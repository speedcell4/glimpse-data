{
  "https://aclanthology.org/2022.aacl-short.42": {
    "title": "Self-Repetition in Abstractive Neural Summarizers",
    "abstract": "We provide a quantitative and qualitative analysis of self-repetition in the output of neural summarizers. We measure self-repetition as the number of n-grams of length four or longer that appear in multiple outputs of the same system. We analyze the behavior of three popular architectures (BART, T5, and Pegasus), fine-tuned on five datasets. In a regression analysis, we find that the three architectures have different propensities for repeating content across output summaries for inputs, with BART being particularly prone to self-repetition. Fine-tuning on more abstractive data, and on data featuring formulaic language is associated with a higher rate of self-repetition. In qualitative analysis, we find systems produce artefacts such as ads and disclaimers unrelated to the content being summarized, as well as formulaic phrases common in the fine-tuning domain. Our approach to corpus-level analysis of self-repetition may help practitioners clean up training data for summarizers and ultimately support methods for minimizing the amount of self-repetition",
    "volume": "short",
    "checked": true,
    "id": "effeb7a31330929dbda68df674a7c29cd0f44c15",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.43": {
    "title": "Domain Specific Sub-network for Multi-Domain Neural Machine Translation",
    "abstract": "This paper presents Domain-Specific Sub-network (DoSS). It uses a set of masks obtained through pruning to define a sub-network for each domain and finetunes the sub-network parameters on domain data. This performs very closely and drastically reduces the number of parameters compared to finetuning the whole network on each domain. Also a method to make masks unique per domain is proposed and shown to greatly improve the generalization to unseen domains. In our experiments on German to English machine translation the proposed method outperforms the strong baseline of continue training on multi-domain (medical, tech and religion) data by 1.47 BLEU points. Also continue training DoSS on new domain (legal) outperforms the multi-domain (medical, tech, religion, legal) baseline by 1.52 BLEU points",
    "volume": "short",
    "checked": true,
    "id": "9fced211efcb0ecfd6119a852964b31134e48c61",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.44": {
    "title": "Modeling Document-level Temporal Structures for Building Temporal Dependency Graphs",
    "abstract": "We propose to leverage news discourse profiling to model document-level temporal structures for building temporal dependency graphs. Our key observation is that the functional roles of sentences used for profiling news discourse signify different time frames relevant to a news story and can, therefore, help to recover the global temporal structure of a document. Our analyses and experiments with the widely used knowledge distillation technique show that discourse profiling effectively identifies distant inter-sentence event and (or) time expression pairs that are temporally related and otherwise difficult to locate",
    "volume": "short",
    "checked": true,
    "id": "3718864bbd0657b09561db62b4898db51a7bc402",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.45": {
    "title": "Evaluating Pre-Trained Sentence-BERT with Class Embeddings in Active Learning for Multi-Label Text Classification",
    "abstract": "The Transformer Language Model is a powerful tool that has been shown to excel at various NLP tasks and has become the de-facto standard solution thanks to its versatility. In this study, we employ pre-trained document embeddings in an Active Learning task to group samples with the same labels in the embedding space on a legal document corpus. We find that the calculated class embeddings are not close to the respective samples and consequently do not partition the embedding space in a meaningful way. In addition, we explore using the class embeddings as an Active Learning strategy with dramatically reduced results compared to all baselines",
    "volume": "short",
    "checked": true,
    "id": "3584649c5b22d2fb9b2ed58f6b03819964fe048b",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.46": {
    "title": "MiQA: A Benchmark for Inference on Metaphorical Questions",
    "abstract": "We propose a benchmark to assess the capability of large language models to reason with conventional metaphors. Our benchmark combines the previously isolated topics of metaphor detection and commonsense reasoning into a single task that requires a model to make inferences by accurately selecting between the literal and metaphorical register. We examine the performance of state-of-the-art pre-trained models on binary-choice tasks and find a large discrepancy between the performance of small and very large models, going from chance to near-human level. We also analyse the largest model in a generative setting and find that although human performance is approached, careful multiple-shot prompting is required",
    "volume": "short",
    "checked": true,
    "id": "777683db4795ff691533c2c4be3244fabd826842",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.47": {
    "title": "GCDT: A Chinese RST Treebank for Multigenre and Multilingual Discourse Parsing",
    "abstract": "A lack of large-scale human-annotated data has hampered the hierarchical discourse parsing of Chinese. In this paper, we present GCDT, the largest hierarchical discourse treebank for Mandarin Chinese in the framework of Rhetorical Structure Theory (RST). GCDT covers over 60K tokens across five genres of freely available text, using the same relation inventory as contemporary RST treebanks for English. We also report on this dataset’s parsing experiments, including state-of-the-art (SOTA) scores for Chinese RST parsing and RST parsing on the English GUM dataset, using cross-lingual training in Chinese and English with multilingual embeddings",
    "volume": "short",
    "checked": true,
    "id": "78e4056235c39f52f0b3023e92a5ea83cafe7d14",
    "citation_count": 1
  },
  "https://aclanthology.org/2022.aacl-short.48": {
    "title": "Assessing Combinational Generalization of Language Models in Biased Scenarios",
    "abstract": "In light of the prominence of Pre-trained Language Models (PLMs) across numerous downstream tasks, shedding light on what they learn is an important endeavor. Whereas previous work focuses on assessing in-domain knowledge, we evaluate the generalization ability in biased scenarios through component combinations where it could be easy for the PLMs to learn shortcuts from the training corpus. This would lead to poor performance on the testing corpus, which is combinationally reconstructed from the training components. The results show that PLMs are able to overcome such distribution shifts for specific tasks and with sufficient data. We further find that overfitting can lead the models to depend more on biases for prediction, thus hurting the combinational generalization ability of PLMs",
    "volume": "short",
    "checked": true,
    "id": "6ada57ec94cd8712cd140f0741c6df0bc01e7509",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.49": {
    "title": "Controllable Text Simplification with Deep Reinforcement Learning",
    "abstract": "We propose a method for controlling the difficulty of a sentence based on deep reinforcement learning. Although existing models are trained based on the word-level difficulty, the sentence-level difficulty has not been taken into account in the loss function. Our proposed method generates sentences of appropriate difficulty for the target audience through reinforcement learning using a reward calculated based on the difference between the difficulty of the output sentence and the target difficulty. Experimental results of English text simplification show that the proposed method achieves a higher performance than existing approaches. Compared to previous studies, the proposed method can generate sentences whose grade-levels are closer to those of human references estimated using a fine-tuned pre-trained model",
    "volume": "short",
    "checked": true,
    "id": "1d9a7e9868dc311b1daed2b9fec18d8c5bea2e8a",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.50": {
    "title": "Vector Space Interpolation for Query Expansion",
    "abstract": "Topic-sensitive query set expansion is an important area of research that aims to improve search results for information retrieval. It is particularly crucial for queries related to sensitive and emerging topics. In this work, we describe a method for query set expansion about emerging topics using vector space interpolation. We use a transformer model called OPTIMUS, which is suitable for vector space manipulation due to its variational autoencoder nature. One of our proposed methods – Dirichlet interpolation shows promising results for query expansion. Our methods effectively generate new queries about the sensitive topic by incorporating set-level diversity, which is not captured by traditional sentence-level augmentation methods such as paraphrasing or back-translation",
    "volume": "short",
    "checked": true,
    "id": "fb72ad9707674c91774c47a2e8572008a5702af9",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.51": {
    "title": "SchAman: Spell-Checking Resources and Benchmark for Endangered Languages from Amazonia",
    "abstract": "Spell-checkers are core applications in language learning and normalisation, which may enormously contribute to language revitalisation and language teaching in the context of indigenous communities. Spell-checking as a generation task, however, requires large amount of data, which is not feasible for endangered languages, such as the languages spoken in Peruvian Amazonia. We propose here augmentation methods for various misspelling types as a strategy to train neural spell-checking models and we create an evaluation resource for four indigenous languages of Peru: Shipibo-Konibo, Asháninka, Yánesha, Yine. We focus on special errors that are significant for learning these languages, such as phoneme-to-grapheme ambiguity, grammatical errors (gender, tense, number, among others), accentuation, punctuation and normalisation in contexts where two or more writing traditions co-exist. We found that an ensemble model, trained with augmented data from various types of error achieves overall better scores in most of the error types and languages. Finally, we released our spell-checkers as a web service to be used by indigenous communities and organisations to develop future language materials",
    "volume": "short",
    "checked": true,
    "id": "56d2fd1245981e5f5dfb42f5697afbac7fbb7354",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.52": {
    "title": "CoFE: A New Dataset of Intra-Multilingual Multi-target Stance Classification from an Online European Participatory Democracy Platform",
    "abstract": "Stance Recognition over proposals is the task of automatically detecting whether a comment on a specific proposal is in favor of this proposal, against this proposal or that neither inference is likely. The dataset that we propose to use is an online debating platform inaugurated in 2021, where users can submit proposals and comment over proposals or over other comments. It contains 4.2k proposals and 20k comments focused on various topics. Every comment and proposal can come written in another language, with more than 40% of the proposal/comment pairs containing at least two languages, creating a unique intra-multilingual setting. A portion of the data (more than 7k comment/proposal pairs, in 26 languages) was annotated by the writers with a self-tag assessing whether they are in favor or against the proposal. Another part of the data (without self-tag) has been manually annotated: 1206 comments in 6 morphologically different languages (fr, de, en, el, it, hu) were tagged, leading to a Krippendorff’s α of 0.69. This setting allows defining an intra-multilingual and multi-target stance classification task over online debates",
    "volume": "short",
    "checked": true,
    "id": "549be8a2e471973d15abb8e93346559bf9f96715",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.53": {
    "title": "Exploring the Effects of Negation and Grammatical Tense on Bias Probes",
    "abstract": "We investigate in this paper how correlations between occupations and gendered-pronouns can be affected and changed by adding negation in bias probes, or changing the grammatical tense of the verbs in the probes. We use a set of simple bias probes in Norwegian and English, and perform 16 different probing analysis, using four Norwegian and four English pre-trained language models. We show that adding negation to probes does not have a considerable effect on the correlations between gendered-pronouns and occupations, supporting other works on negation in language models. We also show that altering the grammatical tense of verbs in bias probes do have some interesting effects on models’ behaviours and correlations. We argue that we should take grammatical tense into account when choosing bias probes, and aggregating results across tenses might be a better representation of the existing correlations",
    "volume": "short",
    "checked": true,
    "id": "2b7718e556eef6a986bb2338d662af5c6776cf55",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.54": {
    "title": "Promoting Pre-trained LM with Linguistic Features on Automatic Readability Assessment",
    "abstract": "Automatic readability assessment (ARA) aims at classifying the readability level of a passage automatically. In the past, manually selected linguistic features are used to classify the passages. However, as the use of deep neural network surges, there is less work focusing on these linguistic features. Recently, many works integrate linguistic features with pre-trained language model (PLM) to make up for the information that PLMs are not good at capturing. Despite their initial success, insufficient analysis of the long passage characteristic of ARA has been done before. To further investigate the promotion of linguistic features on PLMs in ARA from the perspective of passage length, with commonly used linguistic features and abundant experiments, we find that: (1) Linguistic features promote PLMs in ARA mainly on long passages. (2) The promotion of the features on PLMs becomes less significant when the dataset size exceeds 750 passages. (3) By analyzing commonly used ARA datasets, we find Newsela is actually not suitable for ARA. Our code is available at https://github.com/recorderhou/linguistic-features-in-ARA",
    "volume": "short",
    "checked": true,
    "id": "292e7a4ddc743d0d84bc3b4485c932eaa7a0052d",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.55": {
    "title": "An Empirical Study of Pipeline vs. Joint approaches to Entity and Relation Extraction",
    "abstract": "The Entity and Relation Extraction (ERE) task includes two basic sub-tasks: Named Entity Recognition and Relation Extraction. In the last several years, much work focused on joint approaches for the common perception that the pipeline approach suffers from the error propagation problem. Recent work reconsiders the pipeline scheme and shows that it can produce comparable results. To systematically study the pros and cons of these two schemes. We design and test eight pipeline and joint approaches to the ERE task. We find that with the same span representation methods, the best joint approach still outperforms the best pipeline model, but improperly designed joint approaches may have poor performance. We hope our work could shed some light on the pipeline-vs-joint debate of the ERE task and inspire further research",
    "volume": "short",
    "checked": true,
    "id": "25705acd7f0e2cd8790e6ce434e1134b39283b1d",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.56": {
    "title": "CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing",
    "abstract": "A bottleneck to developing Semantic Parsing (SP) models is the need for a large volume of human-labeled training data. Given the complexity and cost of human annotation for SP, labeled data is often scarce, particularly in multilingual settings. Large Language Models (LLMs) excel at SP given only a few examples, however LLMs are unsuitable for runtime systems which require low latency. In this work, we propose CLASP, a simple method to improve low-resource SP for moderate-sized models: we generate synthetic data from AlexaTM 20B to augment the training set for a model 40x smaller (500M parameters). We evaluate on two datasets in low-resource settings: English PIZZA, containing either 348 or 16 real examples, and mTOP cross-lingual zero-shot, where training data is available only in English, and the model must generalize to four new languages. On both datasets, we show significant improvements over strong baseline methods",
    "volume": "short",
    "checked": true,
    "id": "190b831643573cd73d543f620c50051078d8bce9",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.57": {
    "title": "Plug and Play Knowledge Distillation for kNN-LM with External Logits",
    "abstract": "Despite the promising evaluation results by knowledge distillation (KD) in natural language understanding (NLU) and sequence-to-sequence (seq2seq) tasks, KD for causal language modeling (LM) remains a challenge. In this paper, we present a novel perspective of knowledge distillation by proposing plug and play knowledge distillation (PP-KD) to improve a (student) kNN-LM that is the state-of-the-art in causal language modeling by leveraging external logits from either a powerful or a heterogeneous (teacher) LM. Unlike conventional logit-based KD where the teacher’s knowledge is built-in during training, PP-KD is plug and play: it stores the teacher’s knowledge (i.e., logits) externally and uses the teacher’s logits of the retrieved k-nearest neighbors during kNN-LM inference at test time. In contrast to marginal perplexity improvement by logit-based KD in conventional neural (causal) LM, PP-KD achieves a significant improvement, enhancing the kNN-LMs in multiple language modeling datasets, showing a novel and promising perspective for causal LM distillation",
    "volume": "short",
    "checked": true,
    "id": "defa2a63f86cdc40a435aa6d963d863b1f8c0774",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.58": {
    "title": "How Well Do Multi-hop Reading Comprehension Models Understand Date Information?",
    "abstract": "Several multi-hop reading comprehension datasets have been proposed to resolve the issue of reasoning shortcuts by which questions can be answered without performing multi-hop reasoning. However, the ability of multi-hop models to perform step-by-step reasoning when finding an answer to a comparison question remains unclear. It is also unclear how questions about the internal reasoning process are useful for training and evaluating question-answering (QA) systems. To evaluate the model precisely in a hierarchical manner, we first propose a dataset, HieraDate, with three probing tasks in addition to the main question: extraction, reasoning, and robustness. Our dataset is created by enhancing two previous multi-hop datasets, HotpotQA and 2WikiMultiHopQA, focusing on multi-hop questions on date information that involve both comparison and numerical reasoning. We then evaluate the ability of existing models to understand date information. Our experimental results reveal that the multi-hop models do not have the ability to subtract two dates even when they perform well in date comparison and number subtraction tasks. Other results reveal that our probing questions can help to improve the performance of the models (e.g., by +10.3 F1) on the main QA task and our dataset can be used for data augmentation to improve the robustness of the models",
    "volume": "short",
    "checked": true,
    "id": "a9c885434a9c6ec32b61af8075bc979b575d48a6",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-short.59": {
    "title": "Dodging the Data Bottleneck: Automatic Subtitling with Automatically Segmented ST Corpora",
    "abstract": "Speech translation for subtitling (SubST) is the task of automatically translating speech data into well-formed subtitles by inserting subtitle breaks compliant to specific displaying guidelines. Similar to speech translation (ST), model training requires parallel data comprising audio inputs paired with their textual translations. In SubST, however, the text has to be also annotated with subtitle breaks. So far, this requirement has represented a bottleneck for system development, as confirmed by the dearth of publicly available SubST corpora. To fill this gap, we propose a method to convert existing ST corpora into SubST resources without human intervention. We build a segmenter model that automatically segments texts into proper subtitles by exploiting audio and text in a multimodal fashion, achieving high segmentation quality in zero-shot conditions. Comparative experiments with SubST systems respectively trained on manual and automatic segmentations result in similar performance, showing the effectiveness of our approach",
    "volume": "short",
    "checked": true,
    "id": "3f919cf463684c662be5f81004f49ad62a3bbec0",
    "citation_count": 1
  },
  "https://aclanthology.org/2022.aacl-short.60": {
    "title": "How to tackle an emerging topic? Combining strong and weak labels for Covid news NER",
    "abstract": "Being able to train Named Entity Recognition (NER) models for emerging topics is crucial for many real-world applications especially in the medical domain where new topics are continuously evolving out of the scope of existing models and datasets. For a realistic evaluation setup, we introduce a novel COVID-19 news NER dataset (COVIDNEWS-NER) and release 3000 entries of hand annotated strongly labelled sentences and 13000 auto-generated weakly labelled sentences. Besides the dataset, we propose CONTROSTER, a recipe to strategically combine weak and strong labels in improving NER in an emerging topic through transfer learning. We show the effectiveness of CONTROSTER on COVIDNEWS-NER while providing analysis on combining weak and strong labels for training. Our key findings are: (1) Using weak data to formulate an initial backbone before tuning on strong data outperforms methods trained on only strong or weak data. (2) A combination of out-of-domain and in-domain weak label training is crucial and can overcome saturation when being training on weak labels from a single source",
    "volume": "short",
    "checked": true,
    "id": "a3ef4b4cbfc2f1029e2cd3747afb0c5cebe15764",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-demo.1": {
    "title": "VScript: Controllable Script Generation with Visual Presentation",
    "abstract": "In order to offer a customized script tool and inspire professional scriptwriters, we present VScript. It is a controllable pipeline that generates complete scripts, including dialogues and scene descriptions, as well as presents visually using video retrieval. With an interactive interface, our system allows users to select genres and input starting words that control the theme and development of the generated script. We adopt a hierarchical structure, which first generates the plot, then the script and its visual presentation. A novel approach is also introduced to plot-guided dialogue generation by treating it as an inverse dialogue summarization. The experiment results show that our approach outperforms the baselines on both automatic and human evaluations, especially in genre control",
    "volume": "demo",
    "checked": true,
    "id": "6858962fccda2dc4026614fd5d3fd9762ed69468",
    "citation_count": 1
  },
  "https://aclanthology.org/2022.aacl-demo.2": {
    "title": "TexPrax: A Messaging Application for Ethical, Real-time Data Collection and Annotation",
    "abstract": "Collecting and annotating task-oriented dialog data is difficult, especially for highly specific domains that require expert knowledge. At the same time, informal communication channels such as instant messengers are increasingly being used at work. This has led to a lot of work-relevant information that is disseminated through those channels and needs to be post-processed manually by the employees. To alleviate this problem, we present TexPrax, a messaging system to collect and annotate _problems_, _causes_, and _solutions_ that occur in work-related chats. TexPrax uses a chatbot to directly engage the employees to provide lightweight annotations on their conversation and ease their documentation work. To comply with data privacy and security regulations, we use an end-to-end message encryption and give our users full control over their data which has various advantages over conventional annotation tools. We evaluate TexPrax in a user-study with German factory employees who ask their colleagues for solutions on problems that arise during their daily work. Overall, we collect 202 task-oriented German dialogues containing 1,027 sentences with sentence-level expert annotations. Our data analysis also reveals that real-world conversations frequently contain instances with code-switching, varying abbreviations for the same entity, and dialects which NLP systems should be able to handle",
    "volume": "demo",
    "checked": true,
    "id": "f823c24977c746f2fb042c6ceeed9bd75a4dae2f",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-demo.3": {
    "title": "PicTalky: Augmentative and Alternative Communication for Language Developmental Disabilities",
    "abstract": "Children with language disabilities face communication difficulties in daily life. They are often deprived of the opportunity to participate in social activities due to their difficulty in understanding or using natural language. In this regard, Augmentative and Alternative Communication (AAC) can be a practical means of communication for children with language disabilities. In this study, we propose PicTalky, which is an AI-based AAC system that helps children with language developmental disabilities to improve their communication skills and language comprehension abilities. PicTalky can process both text and pictograms more accurately by connecting a series of neural-based NLP modules. Additionally, we perform quantitative and qualitative analyses on the modules of PicTalky. By using this service, it is expected that those suffering from language problems will be able to express their intentions or desires more easily and improve their quality of life. We have made the models freely available alongside a demonstration of the web interface. Furthermore, we implemented robotics AAC for the first time by applying PicTalky to the NAO robot",
    "volume": "demo",
    "checked": true,
    "id": "f9841ca7db2e6aff0024d478ec2b05282bd884ac",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-demo.4": {
    "title": "UKP-SQuARE v2: Explainability and Adversarial Attacks for Trustworthy QA",
    "abstract": "Question Answering (QA) systems are increasingly deployed in applications where they support real-world decisions. However, state-of-the-art models rely on deep neural networks, which are difficult to interpret by humans. Inherently interpretable models or post hoc explainability methods can help users to comprehend how a model arrives at its prediction and, if successful, increase their trust in the system. Furthermore, researchers can leverage these insights to develop new methods that are more accurate and less biased. In this paper, we introduce SQuARE v2, the new version of SQuARE, to provide an explainability infrastructure for comparing models based on methods such as saliency maps and graph-based explanations. While saliency maps are useful to inspect the importance of each input token for the model’s prediction, graph-based explanations from external Knowledge Graphs enable the users to verify the reasoning behind the model prediction. In addition, we provide multiple adversarial attacks to compare the robustness of QA models. With these explainability methods and adversarial attacks, we aim to ease the research on trustworthy QA models. SQuARE is available on https://square.ukp-lab.de",
    "volume": "demo",
    "checked": true,
    "id": "06099fa001d2d5a014fc8c5b17a4cdb492b4e519",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-demo.5": {
    "title": "TaxFree: a Visualization Tool for Candidate-free Taxonomy Enrichment",
    "abstract": "Taxonomies are widely used in a various number of downstream NLP tasks and, therefore, should be kept up-to-date. In this paper, we present TaxFree, an open source system for taxonomy visualisation and automatic Taxonomy Enrichment without pre-defined candidates on the example of WordNet-3.0. As oppose to the traditional task formulation (where the list of new words is provided beforehand), we provide an approach for automatic extension of a taxonomy using a large pre-trained language model. As an advantage to the existing visualisation tools of WordNet, TaxFree also integrates graphic representations of synsets from ImageNet. Such visualisation tool can be used for both updating taxonomies and inspecting them for the required modifications",
    "volume": "demo",
    "checked": true,
    "id": "6def57d3934b5e355c202cdf0db637f822781613",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-demo.6": {
    "title": "F-coref: Fast, Accurate and Easy to Use Coreference Resolution",
    "abstract": "We introduce fastcoref, a python package for fast, accurate, and easy-to-use English coreference resolution. The package is pip-installable, and allows two modes: an accurate mode based on the LingMess architecture, providing state-of-the-art coreference accuracy, and a substantially faster model, F-coref, which is the focus of this work. F-coref allows to process 2.8K OntoNotes documents in 25 seconds on a V100 GPU (compared to 6 minutes for the LingMess model, and to 12 minutes of the popular AllenNLP coreference model) with only a modest drop in accuracy. The fast speed is achieved through a combination of distillation of a compact model from the LingMess model, and an efficient batching implementation using a technique we call leftover batching. https://github.com/shon-otmazgin/fastcoref",
    "volume": "demo",
    "checked": true,
    "id": "c9b0fcfc9470318a56e7dedf4503186e3a343408",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-demo.7": {
    "title": "PIEKM: ML-based Procedural Information Extraction and Knowledge Management System for Materials Science Literature",
    "abstract": "The published materials science literature contains abundant description information about synthesis procedures that can help discover new material areas, deepen the study of materials synthesis, and accelerate its automated planning. Nevertheless, this information is expressed in unstructured text, and manually processing and assimilating useful information is expensive and time-consuming for researchers. To address this challenge, we develop a Machine Learning-based procedural information extraction and knowledge management system (PIEKM) that extracts procedural information recipe steps, figures, and tables from materials science articles, and provides information retrieval capability and the statistics visualization functionality. Our system aims to help researchers to gain insights and quickly understand the connections among massive data. Moreover, we demonstrate that the machine learning-based system performs well in low-resource scenarios (i.e., limited annotated data) for domain adaption",
    "volume": "demo",
    "checked": true,
    "id": "ba8542c3fb348e4c00ba6c7b430ca818110fc0fa",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-demo.8": {
    "title": "BiomedCurator: Data Curation for Biomedical Literature",
    "abstract": "We present BiomedCurator1, a web application that extracts the structured data from scientific articles in PubMed and ClinicalTrials.gov. BiomedCurator uses state-of-the-art natural language processing techniques to fill the fields pre-selected by domain experts in the relevant biomedical area. The BiomedCurator web application includes: text generation based model for relation extraction, entity detection and recognition, text classification model for extracting several fields, information retrieval from external knowledge base to retrieve IDs, and a pattern-based extraction approach that can extract several fields using regular expressions over the PubMed and ClinicalTrials.gov datasets. Evaluation results show that different approaches of BiomedCurator web application system are effective for automatic data curation in the biomedical domain",
    "volume": "demo",
    "checked": true,
    "id": "f7a2100b0635bfb41ef8c7794fd314bdad4331fd",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-demo.9": {
    "title": "Text Characterization Toolkit (TCT)",
    "abstract": "We present a tool, Text Characterization Toolkit (TCT), that researchers can use to study characteristics of large datasets. Furthermore, such properties can lead to understanding the influence of such attributes on models’ behaviour. Traditionally, in most NLP research, models are usually evaluated by reporting single-number performance scores on a number of readily available benchmarks, without much deeper analysis. Here, we argue that – especially given the well-known fact that benchmarks often contain biases, artefacts, and spurious correlations – deeper results analysis should become the de-facto standard when presenting new models or benchmarks. TCT aims at filling this gap by facilitating such deeper analysis for datasets at scale, where datasets can be for training/development/evaluation. TCT includes both an easy-to-use tool, as well as off-the-shelf scripts that can be used for specific analyses. We also present use-cases from several different domains. TCT is used to predict difficult examples for given well-known trained models; TCT is also used to identify (potentially harmful) biases present in a dataset",
    "volume": "demo",
    "checked": true,
    "id": "134fa0adba9d0226f7fb0d4043cff974275d514a",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-demo.10": {
    "title": "Meeting Decision Tracker: Making Meeting Minutes with De-Contextualized Utterances",
    "abstract": "Meetings are a universal process to make decisions in business and project collaboration. The capability to automatically itemize the decisions in daily meetings allows for extensive tracking of past discussions. To that end, we developed Meeting Decision Tracker, a prototype system to construct decision items comprising decision utterance detector (DUD) and decision utterance rewriter (DUR). We show that DUR makes a sizable contribution to improving the user experience by dealing with utterance collapse in natural conversation. An introduction video of our system is also available at https://youtu.be/TG1pJJo0Iqo",
    "volume": "demo",
    "checked": true,
    "id": "d2d7f1925ca72763f0c12200aa00725beda702f1",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-srw.1": {
    "title": "Emotional Intensity Estimation based on Writer's Personality",
    "abstract": "We propose a method for personalized emotional intensity estimation based on a writer’s personality test for Japanese SNS posts. Existing emotion analysis models are difficult to accurately estimate the writer’s subjective emotions behind the text. We personalize the emotion analysis using not only the text but also the writer’s personality information. Experimental results show that personality information improves the performance of emotional intensity estimation. Furthermore, a hybrid model combining the existing personalized method with ours achieved state-of-the-art performance",
    "volume": "student",
    "checked": true,
    "id": "0e699fb4bace75f6d8dd8d2a6e9b5d0e63eedc5b",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-srw.2": {
    "title": "Bipartite-play Dialogue Collection for Practical Automatic Evaluation of Dialogue Systems",
    "abstract": "Automation of dialogue system evaluation is a driving force for the efficient development of dialogue systems. This paper introduces the bipartite-play method, a dialogue collection method for automating dialogue system evaluation. It addresses the limitations of existing dialogue collection methods: (i) inability to compare with systems that are not publicly available, and (ii) vulnerability to cheating by intentionally selecting systems to be compared. Experimental results show that the automatic evaluation using the bipartite-play method mitigates these two drawbacks and correlates as strongly with human subjectivity as existing methods",
    "volume": "student",
    "checked": true,
    "id": "a9a53c28f3b964cf561c05bf204b4c06f6454eec",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-srw.3": {
    "title": "Toward Building a Language Model for Understanding Temporal Commonsense",
    "abstract": "The ability to capture temporal commonsense relationships for time-related events expressed in text is a very important task in natural language understanding. On the other hand, pre-trained language models such as BERT, which have recently achieved great success in a wide range of natural language processing tasks, are still considered to have poor performance in temporal reasoning. In this paper, we focus on the development of language models for temporal commonsense inference over several pre-trained language models. Our model relies on multi-step fine-tuning using multiple corpora, and masked language modeling to predict masked temporal indicators that are crucial for temporal commonsense reasoning. We also experimented with multi-task learning and build a language model that can improve performance on multiple time-related tasks. In our experiments, multi-step fine-tuning using the general commonsense reading task as auxiliary task produced the best results. This result showed a significant improvement in accuracy over standard fine-tuning in the temporal commonsense inference task",
    "volume": "student",
    "checked": true,
    "id": "8b0b5b998d65add5c036b45c36afa3f9df96f4b0",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-srw.4": {
    "title": "Optimal Summaries for Enabling a Smooth Handover in Chat-Oriented Dialogue",
    "abstract": "In dialogue systems, one option for creating a better dialogue experience for the user is to have a human operator take over the dialogue when the system runs into trouble communicating with the user. In this type of handover situation (we call it intervention), it is useful for the operator to have access to the dialogue summary. However, it is not clear exactly what type of summary would be the most useful for a smooth handover. In this study, we investigated the optimal type of summary through experiments in which interlocutors were presented with various summary types during interventions in order to examine their effects. Our findings showed that the best summaries were an abstractive summary plus one utterance immediately before the handover and an extractive summary consisting of five utterances immediately before the handover. From the viewpoint of computational cost, we recommend that extractive summaries consisting of the last five utterances be used",
    "volume": "student",
    "checked": true,
    "id": "04f73c7d8275e1296c536ac37dc50d5dffa2b3ae",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-srw.5": {
    "title": "MUTE: A Multimodal Dataset for Detecting Hateful Memes",
    "abstract": "The exponential surge of social media has enabled information propagation at an unprecedented rate. However, it also led to the generation of a vast amount of malign content, such as hateful memes. To eradicate the detrimental impact of this content, over the last few years hateful memes detection problem has grabbed the attention of researchers. However, most past studies were conducted primarily for English memes, while memes on resource constraint languages (i.e., Bengali) are under-studied. Moreover, current research considers memes with a caption written in monolingual (either English or Bengali) form. However, memes might have code-mixed captions (English+Bangla), and the existing models can not provide accurate inference in such cases. Therefore, to facilitate research in this arena, this paper introduces a multimodal hate speech dataset (named MUTE) consisting of 4158 memes having Bengali and code-mixed captions. A detailed annotation guideline is provided to aid the dataset creation in other resource constraint languages. Additionally, extensive experiments have been carried out on MUTE, considering the only visual, only textual, and both modalities. The result demonstrates that joint evaluation of visual and textual features significantly improves (≈ 3%) the hateful memes classification compared to the unimodal evaluation",
    "volume": "student",
    "checked": true,
    "id": "4104cfd71b219b5a532677fe8eac151ec6bf182a",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-srw.6": {
    "title": "A Simple and Fast Strategy for Handling Rare Words in Neural Machine Translation",
    "abstract": "Neural Machine Translation (NMT) has currently obtained state-of-the-art in machine translation systems. However, dealing with rare words is still a big challenge in translation systems. The rare words are often translated using a manual dictionary or copied from the source to the target with original words. In this paper, we propose a simple and fast strategy for integrating constraints during the training and decoding process to improve the translation of rare words. The effectiveness of our proposal is demonstrated in both high and low-resource translation tasks, including the language pairs: English → Vietnamese, Chinese → Vietnamese, Khmer → Vietnamese, and Lao → Vietnamese. We show the improvements of up to +1.8 BLEU scores over the baseline systems",
    "volume": "student",
    "checked": true,
    "id": "c74a7fdd62c0809a4736889c3fbc35aea03599ba",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-srw.7": {
    "title": "C3PO: A Lightweight Copying Mechanism for Translating Pseudocode to Code",
    "abstract": "Writing computer programs is a skill that remains inaccessible to most due to the barrier of programming language (PL) syntax. While large language models (LLMs) have been proposed to translate natural language pseudocode to PL code, they are costly in terms of data and compute. We propose a lightweight alternative to LLMs that exploits the property of code wherein most tokens can be simply copied from the pseudocode. We divide the problem into three phases: Copy, Generate, and Combine. In the Copy Phase, a binary classifier is employed to determine and mask the pseudocode tokens that can be directly copied into the code. In the Generate Phase, a Sequence-to-Sequence model is used to generate the masked PL code equivalent. In the Combine Phase, the generated sequence is combined with the tokens that the Copy Phase had masked. We show that our C3PO models achieve similar performance to non-C3PO models while reducing the computational cost of training as well as the vocabulary sizes",
    "volume": "student",
    "checked": true,
    "id": "60043104ca33a1fc905af57ead32768e52c69103",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-srw.8": {
    "title": "Outlier-Aware Training for Improving Group Accuracy Disparities",
    "abstract": "Methods addressing spurious correlations such as Just Train Twice (JTT, Liu et al. 2021) involve reweighting a subset of the training set to maximize the worst-group accuracy. However, the reweighted set of examples may potentially contain unlearnable examples that hamper the model’s learning. We propose mitigating this by detecting outliers to the training set and removing them before reweighting. Our experiments show that our method achieves competitive or better accuracy compared with JTT and can detect and remove annotation errors in the subset being reweighted in JTT",
    "volume": "student",
    "checked": true,
    "id": "e7cbb45aa0be2d695ab04724a4367cb3947b3cee",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-srw.9": {
    "title": "An Empirical Study on Topic Preservation in Multi-Document Summarization",
    "abstract": "Multi-document summarization (MDS) is a process of generating an informative and concise summary from multiple topic-related documents. Many studies have analyzed the quality of MDS dataset or models, however no work has been done from the perspective of topic preservation. In this work, we fill the gap by performing an empirical analysis on two MDS datasets and study topic preservation on generated summaries from 8 MDS models.Our key findings include i) Multi-News dataset has better gold summaries compared to Multi-XScience in terms of its topic distribution consistency and ii) Extractive approaches perform better than abstractive approaches in preserving topic information from source documents. We hope our findings could help develop a summarization model that can generate topic-focused summary and also give inspiration to researchers in creating dataset for such challenging task",
    "volume": "student",
    "checked": true,
    "id": "b816f2d9a893275229ad0b2a76ff1540361a399a",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-srw.10": {
    "title": "Detecting Urgency in Multilingual Medical SMS in Kenya",
    "abstract": "Access to mobile phones in many low- and middle-income countries has increased exponentially over the last 20 years, providing an opportunity to connect patients with healthcare interventions through mobile phones (known as mobile health). A barrier to large-scale implementation of interactive mobile health interventions is the human effort needed to manage participant messages. In this study, we explore the use of natural language processing to improve healthcare workers’ management of messages from pregnant and postpartum women in Kenya. Using multilingual, low-resource language text messages from the Mobile solutions for Women and Children’s health (Mobile WACh NEO) study, we developed models to assess urgency of incoming messages. We evaluated models using a novel approach that focuses on clinical usefulness in either triaging or prioritizing messages. Our best-performing models did not reach the threshold for clinical usefulness we set, but have the potential to improve nurse workflow and responsiveness to urgent messages",
    "volume": "student",
    "checked": true,
    "id": "7aaa399947269a02e531707749f1497f900e2e96",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-srw.11": {
    "title": "Language over Labels: Contrastive Language Supervision Exceeds Purely Label-Supervised Classification Performance on Chest X-Rays",
    "abstract": "The multi-modal foundation model CLIP computes representations from texts and images that achieved unprecedented performance on tasks such as zero-shot image classification. However, CLIP was pretrained on public internet data. Thus it lacks highly domain-specific knowledge. We investigate the adaptation of CLIP-based models to the chest radiography domain using the MIMIC-CXR dataset. We show that the features of the pretrained CLIP models do not transfer to this domain. We adapt CLIP to the chest radiography domain using contrastive language supervision and show that this approach yields a model that outperforms supervised learning on labels on the MIMIC-CXR dataset while also generalizing to the CheXpert and RSNA Pneumonia datasets. Furthermore, we do a detailed ablation study of the batch and dataset size. Finally, we show that language supervision allows for better explainability by using the multi-modal model to generate images from texts such that experts can inspect what the model has learned",
    "volume": "student",
    "checked": true,
    "id": "49b43e98c4ffc4ee0383f15940c97e9540a64c9f",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-srw.12": {
    "title": "Dynamic Topic Modeling by Clustering Embeddings from Pretrained Language Models: A Research Proposal",
    "abstract": "A new trend in topic modeling research is to do Neural Topic Modeling by Clustering document Embeddings (NTM-CE) created with a pretrained language model. Studies have evaluated static NTM-CE models and found them performing comparably to, or even better than other topic models. An important extension of static topic modeling is making the models dynamic, allowing the study of topic evolution over time, as well as detecting emerging and disappearing topics. In this research proposal, we present two research questions to understand dynamic topic modeling with NTM-CE theoretically and practically. To answer these, we propose four phases with the aim of establishing evaluation methods for dynamic topic modeling, finding NTM-CE-specific properties, and creating a framework for dynamic NTM-CE. For evaluation, we propose to use both quantitative measurements of coherence and human evaluation supported by our recently developed tool",
    "volume": "student",
    "checked": true,
    "id": "4c41cb7804cfb419127b172d109b9c6876e00a24",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-srw.13": {
    "title": "Concreteness vs. Abstractness: A Selectional Preference Perspective",
    "abstract": "Concrete words refer to concepts that are strongly experienced through human senses (banana, chair, salt, etc.), whereas abstract concepts are less perceptually salient (idea, glory, justice, etc.). A clear definition of abstractness is crucial for the understanding of human cognitive processes and for the development of natural language applications such as figurative language detection. In this study, we investigate selectional preferences as a criterion to distinguish between concrete and abstract concepts and words: we hypothesise that abstract and concrete verbs and nouns differ regarding the semantic classes of their arguments. Our study uses a collection of 5,438 nouns and 1,275 verbs to exploit selectional preferences as a salient characteristic in classifying English abstract vs. concrete words, and in predicting their concreteness scores. We achieve an f1-score of 0.84 for nouns and 0.71 for verbs in classification, and Spearman’s ρ correlation of 0.86 for nouns and 0.59 for verbs",
    "volume": "student",
    "checked": true,
    "id": "6d0925495edb6cade0bd6d4628fee626b8b0665b",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-tutorials.1": {
    "title": "Efficient and Robust Knowledge Graph Construction",
    "abstract": "Knowledge graph construction which aims to extract knowledge from the text corpus, has appealed to the NLP community researchers. Previous decades have witnessed the remarkable progress of knowledge graph construction on the basis of neural models; however, those models often cost massive computation or labeled data resources and suffer from unstable inference accounting for biased or adversarial samples. Recently, numerous approaches have been explored to mitigate the efficiency and robustness issues for knowledge graph construction, such as prompt learning and adversarial training. In this tutorial, we aim to bring interested NLP researchers up to speed on the recent and ongoing techniques for efficient and robust knowledge graph construction. Additionally, our goal is to provide a systematic and up-to-date overview of these methods and reveal new research opportunities to the audience",
    "volume": "tutorial",
    "checked": true,
    "id": "f14ea74a40c40cad10c32fccf5a410144735d683",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-tutorials.2": {
    "title": "Recent Advances in Pre-trained Language Models: Why Do They Work and How Do They Work",
    "abstract": "Pre-trained language models (PLMs) are language models that are pre-trained on large-scaled corpora in a self-supervised fashion. These PLMs have fundamentally changed the natural language processing community in the past few years. In this tutorial, we aim to provide a broad and comprehensive introduction from two perspectives: why those PLMs work, and how to use them in NLP tasks. The first part of the tutorial shows some insightful analysis on PLMs that partially explain their exceptional downstream performance. The second part first focuses on emerging pre-training methods that enable PLMs to perform diverse downstream tasks and then illustrates how one can apply those PLMs to downstream tasks under different circumstances. These circumstances include fine-tuning PLMs when under data scarcity, and using PLMs with parameter efficiency. We believe that attendees of different backgrounds would find this tutorial informative and useful",
    "volume": "tutorial",
    "checked": true,
    "id": "15b0d15a1ef487a14d24544d76f6da1271a4f7ec",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-tutorials.3": {
    "title": "When Cantonese NLP Meets Pre-training: Progress and Challenges",
    "abstract": "Cantonese is an influential Chinese variant with a large population of speakers worldwide. However, it is under-resourced in terms of the data scale and diversity, excluding Cantonese Natural Language Processing (NLP) from the stateof-the-art (SOTA) “pre-training and fine-tuning” paradigm. This tutorial will start with a substantially review of the linguistics and NLP progress for shaping language specificity, resources, and methodologies. It will be followed by an introduction to the trendy transformerbased pre-training methods, which have been largely advancing the SOTA performance of a wide range of downstream NLP tasks in numerous majority languages (e.g., English and Chinese). Based on the above, we will present the main challenges for Cantonese NLP in relation to Cantonese language idiosyncrasies of colloquialism and multilingualism, followed by the future directions to line NLP for Cantonese and other low-resource languages up to the cutting-edge pre-training practice",
    "volume": "tutorial",
    "checked": true,
    "id": "0a410973a43ea8d87eca14e90e40b4e3381c8265",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-tutorials.4": {
    "title": "Grounding Meaning Representation for Situated Reasoning",
    "abstract": "As natural language technology becomes ever-present in everyday life, people will expect artificial agents to understand language use as humans do. Nevertheless, most advanced neural AI systems fail at some types of interactions that are trivial for humans (e.g., ask a smart system “What am I pointing at?”). One critical aspect of human language understanding is situated reasoning, where inferences make reference to the local context, perceptual surroundings, and contextual groundings from the interaction. In this cutting-edge tutorial, we bring to the NLP/CL community a synthesis of multimodal grounding and meaning representation techniques with formal and computational models of embodied reasoning. We will discuss existing approaches to multimodal language grounding and meaning representations, discuss the kind of information each method captures and their relative suitability to situated reasoning tasks, and demon- strate how to construct agents that conduct situated reasoning by embodying a simulated environment. In doing so, these agents also represent their human interlocutor(s) within the simulation, and are represented through their virtual embodiment in the real world, enabling true bidirectional communication with a computer using multiple modalities",
    "volume": "tutorial",
    "checked": true,
    "id": "64ab90619b617d3f7d76a2b4bf95997cd999a3c1",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.aacl-tutorials.5": {
    "title": "The Battlefront of Combating Misinformation and Coping with Media Bias",
    "abstract": "Misinformation is a pressing issue in modern society. It arouses a mixture of anger, distrust, confusion, and anxiety that cause damage on our daily life judgments and public policy decisions. While recent studies have explored various fake news detection and media bias detection techniques in attempts to tackle the problem, there remain many ongoing challenges yet to be addressed, as can be witnessed from the plethora of untrue and harmful content present during the COVID-19 pandemic and the international crises of late. In this tutorial, we provide researchers and practitioners with a systematic overview of the frontier in fighting misinformation. Specifically, we dive into the important research questions of how to (i) develop a robust fake news detection system, which not only fact-check information pieces provable by background knowledge but also reason about the consistency and the reliability of subtle details for emerging events; (ii) uncover the bias and agenda of news sources to better characterize misinformation; as well as (iii) correct false information and mitigate news bias, while allowing diverse opinions to be expressed. Moreover, we discuss the remaining challenges, future research directions, and exciting opportunities to help make this world a better place, with safer and more harmonic information sharing",
    "volume": "tutorial",
    "checked": true,
    "id": "842c9aa5f16a3b91492b3eeaca9e80e4c3dca72d",
    "citation_count": 1
  },
  "https://aclanthology.org/2022.aacl-tutorials.6": {
    "title": "A Tour of Explicit Multilingual Semantics: Word Sense Disambiguation, Semantic Role Labeling and Semantic Parsing",
    "abstract": "The recent advent of modern pretrained language models has sparked a revolution in Natural Language Processing (NLP), especially in multilingual and cross-lingual applications. Today, such language models have become the de facto standard for providing rich input representations to neural systems, achieving unprecedented results in an increasing range of benchmarks. However, questions that often arise are: firstly, whether current language models are, indeed, able to capture explicit, symbolic meaning; secondly, if they are, to what extent; thirdly, and perhaps more importantly, whether current approaches are capable of scaling across languages. In this cutting-edge tutorial, we will review recent efforts that have aimed at shedding light on meaning in NLP, with a focus on three key open problems in lexical and sentence-level semantics: Word Sense Disambiguation, Semantic Role Labeling, and Semantic Parsing. After a brief introduction, we will spotlight how state-of-the-art models tackle these tasks in multiple languages, showing where they excel and where they fail. We hope that this tutorial will broaden the audience interested in multilingual semantics and inspire researchers to further advance the field",
    "volume": "tutorial",
    "checked": true,
    "id": "8baae43106b43e52e26bbc7970f19f798ba7cfd2",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.1": {
    "title": "Efficient Entity Embedding Construction from Type Knowledge for BERT",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "db3047d23f492e9d6cf734831840c5c5e1bf895b",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.2": {
    "title": "Spa: On the Sparsity of Virtual Adversarial Training for Dependency Parsing",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "4277e67c4720597be38ce09e5db66ca1bf84f99f",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.3": {
    "title": "KreolMorisienMT: A Dataset for Mauritian Creole Machine Translation",
    "abstract": null,
    "volume": "findings",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.4": {
    "title": "LEATHER: A Framework for Learning to Generate Human-like Text in Dialogue",
    "abstract": "Algorithms for text-generation in dialogue can be misguided. For example, in task-oriented settings, reinforcement learning that optimizes only task-success can lead to abysmal lexical diversity. We hypothesize this is due to poor theoretical understanding of the objectives in text-generation and their relation to the learning process (i.e., model training). To this end, we propose a new theoretical framework for learning to generate text in dialogue. Compared to existing theories of learning, our framework allows for analysis of the multi-faceted goals inherent to text-generation. We use our framework to develop theoretical guarantees for learners that adapt to unseen data. As an example, we apply our theory to study data-shift within a cooperative learning algorithm proposed for the GuessWhat?! visual dialogue game. From this insight, we propose a new algorithm, and empirically, we demonstrate our proposal improves both task-success and human-likeness of the generated text. Fi-nally, we show statistics from our theory are empirically predictive of multiple qualities of the generated dialogue, suggesting our theory is useful for model-selection when human evaluations are not available",
    "volume": "findings",
    "checked": true,
    "id": "016cbbfeb1803134eea5b18c0fe7d8c448e53449",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.5": {
    "title": "Conceptual Similarity for Subjective Tags",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "5e71672d750d8cd2fad06a0798eed52319b1f5cb",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.6": {
    "title": "TaskMix: Data Augmentation for Meta-Learning of Spoken Intent Understanding",
    "abstract": "Meta-Learning has emerged as a research direction to better transfer knowledge from related tasks to unseen but related tasks. How-ever, Meta-Learning requires many training tasks to learn representations that transfer well to unseen tasks; otherwise, it leads to overﬁtting, and the performance degenerates to worse than Multi-task Learning. We show that a state-of-the-art data augmentation method worsens this problem of overﬁtting when the task diversity is low. We propose a simple method, TaskMix, which synthesizes new tasks by linearly interpolating existing tasks. We compare TaskMix against many baselines on an in-house multilingual intent classiﬁcation dataset of N-Best ASR hypotheses derived from real-life human-machine telephony utterances and two datasets derived from MTOP. We show that TaskMix outperforms baselines, alleviates overﬁtting when task diversity is low, and does not degrade performance even when it is high",
    "volume": "findings",
    "checked": true,
    "id": "2e1418467252a3d08c09e036d070e4396fcd68c0",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.7": {
    "title": "Understanding the Use of Quantifiers in Mandarin",
    "abstract": "We introduce a corpus of short texts in Mandarin, in which quantiﬁed expressions ﬁg-ure prominently. We illustrate the signiﬁ-cance of the corpus by examining the hypothesis (known as Huang’s “coolness” hypothesis) that speakers of East Asian Languages tend to speak more brieﬂy but less informatively than, for example, speakers of West-European languages. The corpus results from an elicitation experiment in which participants were asked to describe abstract visual scenes. We compare the resulting corpus, called MQTUNA , with an English corpus that was collected using the same experimental paradigm. The comparison reveals that some, though not all, aspects of quantiﬁer use support the above-mentioned hypothesis. Implications of these ﬁndings for the generation of quantiﬁed noun phrases are discussed. MQTUNA is available at: https: //github.com/a-quei/qtuna",
    "volume": "findings",
    "checked": true,
    "id": "b2c69438c5aef69c01c3a88af293637ed65a2759",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.8": {
    "title": "Does Representational Fairness Imply Empirical Fairness?",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "b192aa0fbcc5ebe3da60dac26c945e866aadd994",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.9": {
    "title": "SEHY: A Simple yet Effective Hybrid Model for Summarization of Long Scientific Documents",
    "abstract": null,
    "volume": "findings",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.10": {
    "title": "PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation",
    "abstract": null,
    "volume": "findings",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.11": {
    "title": "A Hybrid Architecture for Labelling Bilingual Māori-English Tweets",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "b12eab257d8ca8d8388389f6d0263267e51fde1c",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.12": {
    "title": "Meta-Learning Adaptive Knowledge Distillation for Efficient Biomedical Natural Language Processing",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "708363eb2d97802e373b40355b956163d41dad80",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.13": {
    "title": "The Effects of Surprisal across Languages: Results from Native and Non-native Reading",
    "abstract": null,
    "volume": "findings",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.14": {
    "title": "Assessing How Users Display Self-Disclosure and Authenticity in Conversation with Human-Like Agents: A Case Study of Luda Lee",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "89c2b5f5c1fcbfd3dfee89c5f658b32cb901a60d",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.15": {
    "title": "Block Diagram-to-Text: Understanding Block Diagram Images by Generating Natural Language Descriptors",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "9ea33c4af89b945442ee8013f700f056192ed306",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.16": {
    "title": "Multi-Domain Dialogue State Tracking By Neural-Retrieval Augmentation",
    "abstract": null,
    "volume": "findings",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.17": {
    "title": "TaKG: A New Dataset for Paragraph-level Table-to-Text Generation Enhanced with Knowledge Graphs",
    "abstract": null,
    "volume": "findings",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.18": {
    "title": "Revisiting Checkpoint Averaging for Neural Machine Translation",
    "abstract": null,
    "volume": "findings",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.19": {
    "title": "Modeling Referential Gaze in Task-oriented Settings of Varying Referential Complexity",
    "abstract": null,
    "volume": "findings",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.20": {
    "title": "Automating Interlingual Homograph Recognition with Parallel Sentences",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "201de362b0b81de9d4d011d96d70b8421c2fea04",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.21": {
    "title": "CoRAL: a Context-aware Croatian Abusive Language Dataset",
    "abstract": "In light of unprecedented increases in the pop-ularity of the internet and social media, comment moderation has never been a more rel-evant task. Semi-automated comment moderation systems greatly aid human moderators by either automatically classifying the examples or allowing the moderators to prioritize which comments to consider ﬁrst. However, the concept of inappropriate content is often subjective, and such content can be conveyed in many subtle and indirect ways. In this work, we propose CoRAL 1 – a language and cul-turally aware Croatian Abusive dataset cover-ing phenomena of implicitness and reliance on local and global context. We show exper-imentally that current models degrade when comments are not explicit and further degrade when language skill and context knowledge are required to interpret the comment",
    "volume": "findings",
    "checked": true,
    "id": "a0449cb688e65967a8ec21d4d37fed3ea08d93e3",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.22": {
    "title": "A Copy Mechanism for Handling Knowledge Base Elements in SPARQL Neural Machine Translation",
    "abstract": "Neural Machine Translation (NMT) models from English to SPARQL are a promising development for SPARQL query generation. However, current architectures are unable to integrate the knowledge base (KB) schema and handle questions on knowledge resources, classes, and properties unseen during training, rendering them unusable outside the scope of topics covered in the training set. Inspired by the performance gains in natural language processing tasks, we propose to integrate a copy mechanism for neural SPARQL query generation as a way to tackle this issue. We illus-trate our proposal by adding a copy layer and a dynamic knowledge base vocabulary to two Seq2Seq architectures (CNNs and Transform-ers). This layer makes the models copy KB elements directly from the questions, instead of generating them. We evaluate our approach on state-of-the-art datasets, including datasets referencing unknown KB elements and mea-sure the accuracy of the copy-augmented architectures. Our results show a considerable increase in performance on all datasets compared to non-copy architectures",
    "volume": "findings",
    "checked": true,
    "id": "0b871a9f12e5c2da1b291a8b166c671256ebe1cd",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.23": {
    "title": "A Multilingual Multiway Evaluation Data Set for Structured Document Translation of Asian Languages",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "88c1ee108185c16d086770b784bfeb4e0232c9d4",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.24": {
    "title": "On Measures of Biases and Harms in NLP",
    "abstract": "Recent studies show that Natural Language Processing (NLP) technologies propagate societal biases about demographic groups associated with attributes such as gender, race, and nationality. To create interventions and mitigate these biases and associated harms, it is vital to be able to detect and measure such biases. While existing works propose bias evaluation and mitigation methods for various tasks, there remains a need to cohesively understand the biases and the speciﬁc harms they measure, and how different measures compare with each other. To address this gap, this work presents a practical framework of harms and a series of questions that practitioners can answer to guide the development of bias measures. As a validation of our framework and documentation questions, we also present several case studies of how existing bias measures in NLP—both intrinsic measures of bias in representations and extrinsic measures of bias of downstream applications— can be aligned with different harms and how our proposed documentation questions facilitates more holistic understanding of what bias measures are measuring",
    "volume": "findings",
    "checked": true,
    "id": "beb4f0ef465212c5eae59e85dc838d3ba47dbacc",
    "citation_count": 1
  },
  "https://aclanthology.org/2022.findings-aacl.25": {
    "title": "Logographic Information Aids Learning Better Representations for Natural Language Inference",
    "abstract": "Statistical language models conventionally implement representation learning based on the contextual distribution of words or other for-mal units, whereas any information related to the logographic features of written text are often ignored, assuming they should be retrieved relying on the cooccurence statistics. On the other hand, as language models become larger and require more data to learn reliable representations, such assumptions may start to fall back, especially under conditions of data sparsity. Many languages, including Chinese and Vietnamese, use logographic writing systems where surface forms are represented as a visual organization of smaller graphemic units, which often contain many semantic cues. In this paper, we present a novel study which ex-plores the beneﬁts of providing language models with logographic information in learning better semantic representations. We test our hypothesis in the natural language inference (NLI) task by evaluating the beneﬁt of computing multi-modal representations that combine contextual information with glyph information. Our evaluation results in six languages with different typology and writing systems suggest signiﬁcant beneﬁts of using multi-modal embeddings in languages with logograhic systems, especially for words with less occurence statistics",
    "volume": "findings",
    "checked": true,
    "id": "c9b98d7dd15acfddf8de8448263bfff0feb6c382",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.26": {
    "title": "Cross-domain Analysis on Japanese Legal Pretrained Language Models",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "d7f235239f1c899fc2d3ffff5829fb46627b13c9",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.27": {
    "title": "Multilingual CheckList: Generation and Evaluation",
    "abstract": "Multilingual evaluation benchmarks usually contain limited high-resource languages and do not test models for speciﬁc linguistic capabilities. CheckList (Ribeiro et al., 2020) is a template-based evaluation approach that tests models for speciﬁc capabilities. The CheckList template creation process requires native speakers, posing a challenge in scaling to hundreds of languages. In this work, we explore multiple approaches to generate Multilingual CheckLists. We device an algorithm – T emplate E xtraction A lgorithm (TEA) for automatically extracting target language CheckList templates from machine translated instances of a source language templates. We compare the TEA CheckLists with CheckLists created with different levels of human intervention. We further introduce metrics along the dimensions of cost , diversity , utility , and correctness to compare the CheckLists. We thoroughly analyze different approaches to creating CheckLists in Hindi. Furthermore, we experiment with 9 more different languages. We ﬁnd that TEA followed by human veriﬁcation is ideal for scaling Checklist-based evaluation to multiple languages while TEA gives a good estimates of model performance. We release the code of TEA and the CheckLists created at aka.ms/multilingualchecklist",
    "volume": "findings",
    "checked": true,
    "id": "2a9378a5f0f188fa74fbd3360a26bd378882634f",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.28": {
    "title": "Part Represents Whole: Improving the Evaluation of Machine Translation System Using Entropy Enhanced Metrics",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "7fa53e510ea7e0cf6755e2bdbbb30f7ba3773cc0",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.29": {
    "title": "Memformer: A Memory-Augmented Transformer for Sequence Modeling",
    "abstract": "Transformers have reached remarkable suc- 001 cess in sequence modeling. However, these 002 models have efficiency issues as they need 003 to store all the history token-level represen- 004 tations as memory. We present Memformer, 005 an efficient neural network for sequence mod- 006 eling, that utilizes an external dynamic mem- 007 ory to encode and retrieve past information. 008 Our model achieves linear time complexity and 009 constant memory space complexity when pro- 010 cessing long sequences. We also propose a 011 new optimization scheme, memory replay back- 012 propagation (MRBP), which promotes long- 013 range back-propagation through time with a 014 significantly reduced memory requirement. Ex- 015 perimental results show that Memformer has 016 achieved comparable performance compared 017 against the baselines by using 8.1x less memory 018 space and 3.2x faster on inference. Analysis 019 of the attention pattern shows that our external 020 memory slots can encode and retain important 021 information through timesteps. 022",
    "volume": "findings",
    "checked": true,
    "id": "67ee20536c30a225b86902af2f091e28e5e19b40",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.30": {
    "title": "Open-Domain Conversational Question Answering with Historical Answers",
    "abstract": "Open-domain conversational question answering can be viewed as two tasks: passage retrieval and conversational question answering, where the former relies on selecting candidate passages from a large corpus and the latter requires better understanding of a question with contexts to predict the answers. This paper proposes ConvADR-QA that leverages historical answers to boost retrieval performance and further achieves better answering performance. Our experiments on the benchmark dataset, OR-QuAC, demonstrate that our model outperforms existing baselines in both extractive and generative reader settings, well justifying the effectiveness of historical answers for open-domain conversational question answering. 1",
    "volume": "findings",
    "checked": true,
    "id": "e72a1760a8dbec104045c132bbb8e08fe462bf31",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.31": {
    "title": "Robustness Evaluation of Text Classification Models Using Mathematical Optimization and Its Application to Adversarial Training",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "abd0e489ae53bfadabc3be8693c5f431dd1499fb",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.32": {
    "title": "HERB: Measuring Hierarchical Regional Bias in Pre-trained Language Models",
    "abstract": null,
    "volume": "findings",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.33": {
    "title": "Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models",
    "abstract": null,
    "volume": "findings",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.34": {
    "title": "Chop and Change: Anaphora Resolution in Instructional Cooking Videos",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "39942f2cce067ff065e8ced6041addd9277cdb6d",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.35": {
    "title": "#DisabledOnIndianTwitter\" : A Dataset towards Understanding the Expression of People with Disabilities on Indian Twitter",
    "abstract": null,
    "volume": "findings",
    "checked": false,
    "id": "ea9f769bbf0e75dcb821310940a004aad9e3c41f",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.36": {
    "title": "Topic-aware Multimodal Summarization",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "2246ee915ee46cc499809a2bc3109680ce80caf7",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.37": {
    "title": "ArgGen: Prompting Text Generation Models for Document-Level Event-Argument Aggregation",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "61f49465c0d53663ad5264c8f683c6724d31eef1",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.38": {
    "title": "Hierarchical Processing of Visual and Language Information in the Brain",
    "abstract": null,
    "volume": "findings",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.39": {
    "title": "Differential Bias: On the Perceptibility of Stance Imbalance in Argumentation",
    "abstract": "Most research on natural language processing treats bias as an absolute concept: Based on a (probably complex) algorithmic analysis, a sentence, an article, or a text is classified as biased or not. Given the fact that for humans the question of whether a text is biased can be difficult to answer or is answered contra-dictory, we ask whether an “absolute bias clas-sification” is a promising goal at all. We see the problem not in the complexity of interpret-ing language phenomena but in the diversity of sociocultural backgrounds of the readers, which cannot be handled uniformly: To decide whether a text has crossed the proverbial line between non-biased and biased is subjective. By asking “Is text X more [less, equally] biased than text Y ?” we propose to analyze a simpler problem, which, by its construction, is rather independent of standpoints, views, or sociocultural aspects. In such a model, bias be-comes a preference relation that induces a partial ordering from least biased to most biased texts without requiring a decision on where to draw the line. A prerequisite for this kind of bias model is the ability of humans to perceive relative bias differences in the first place. In our research, we selected a specific type of bias in argumentation, the stance bias, and designed a crowdsourcing study showing that differences in stance bias are perceptible when (light) support is provided through training or visual aid",
    "volume": "findings",
    "checked": true,
    "id": "9de28bdafc45e98e9257e4f0fb12ea7bd3e18260",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.40": {
    "title": "BeamR: Beam Reweighing with Attribute Discriminators for Controllable Text Generation",
    "abstract": "Recent advances in natural language process- ing have led to the availability of large pre- 002 trained language models (LMs), with rich gen- 003 erative capabilities. Although these models 004 are able to produce ﬂuent and coherent text, 005 it remains a challenge to control various at- 006 tributes of the generation, including sentiment, 007 formality, topic and many others. We propose a 008 Beam Reweighing (B EAM R) method, building 009 on top of standard beam search, in order to con- 010 trol different attributes. B EAM R combines any 011 generative LM with any attribute discrimina- 012 tor, offering full ﬂexibility of generation style 013 and attribute, while the beam search backbone 014 maintains ﬂuency across different domains. No- 015 tably, B EAM R allows practitioners to leverage 016 pre-trained models without the need to train 017 generative LMs together with discriminators. 018 We evaluate B EAM R in two diverse tasks: senti- 019 ment steering, and machine translation formal- 020 ity. Our results show that B EAM R performs 021 on par with or better than existing state-of-the- 022 art approaches (including ﬁne-tuned methods), 023 and highlight the ﬂexiblity of B EAM R in both 024 causal and seq2seq language modeling tasks",
    "volume": "findings",
    "checked": true,
    "id": "2ea995de85d6827fa79f0c96452b49bae9ef6bf4",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.41": {
    "title": "R&R: Metric-guided Adversarial Sentence Generation",
    "abstract": null,
    "volume": "findings",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.42": {
    "title": "A Simple yet Effective Learnable Positional Encoding Method for Improving Document Transformer Model",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "3f16c495ab32a4eb7dc880a8d4413e2eda807099",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.43": {
    "title": "MMM: An Emotion and Novelty-aware Approach for Multilingual Multimodal Misinformation Detection",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "7531bf0e0c9c344549a62c7b68c47aa5db9d4358",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.findings-aacl.44": {
    "title": "Adversarial Sample Generation for Aspect based Sentiment Classification",
    "abstract": null,
    "volume": "findings",
    "checked": true,
    "id": "d61316792b12d6e4c577a8afae0055015e09c86e",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.cogalex-1.1": {
    "title": "Patterns of Text Readability in Human and Predicted Eye Movements",
    "abstract": "It has been shown that multilingual transformer models are able to predict human reading behavior when fine-tuned on small amounts of eye tracking data. As the cumulated prediction results do not provide insights into the linguistic cues that the model acquires to predict reading behavior, we conduct a deeper analysis of the predictions from the perspective of readability. We try to disentangle the three-fold relationship between human eye movements, the capability of language models to predict these eye movement patterns, and sentence-level readability measures for English. We compare a range of model configurations to multiple baselines. We show that the models exhibit difficulties with function words and that pre-training only provides limited advantages for linguistic generalization",
    "volume": "workshop",
    "checked": true,
    "id": "b7ce27f8fd9f601a247fc26bfaf4d39d28303466",
    "citation_count": 1
  },
  "https://aclanthology.org/2022.cogalex-1.2": {
    "title": "(In)Alienable Possession in Mandarin Relative Clauses",
    "abstract": "Inalienable possession differs from alienable possession in that, in the former – e.g., kinships and part-whole relations – there is an intrinsic semantic dependency between the possessor and possessum. This paper reports two studies that used acceptability-judgment tasks to investigate whether native Mandarin speakers experienced different levels of interpretational costs while resolving different types of possessive relations, i.e., inalienable possessions (kinship terms and body parts) and alienable ones, expressed within relative clauses. The results show that sentences received higher acceptability ratings when body parts were the possessum as compared to sentences with alienable possessum, indicating that the inherent semantic dependency facilitates the resolution. However, inalienable kinship terms received the lowest acceptability ratings. We argue that this was because the kinship terms, which had the [+human] feature and appeared at the beginning of the experimental sentences, tended to be interpreted as the subject in shallow processing; these features contradicted the semantic-syntactic requirements of the experimental sentences",
    "volume": "workshop",
    "checked": true,
    "id": "8387714e721ea8a3acb28da322b3861a2050d4f0",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.cogalex-1.3": {
    "title": "Do Age of Acquisition and Orthographic Transparency Have the Same Effects in Different Modalities?",
    "abstract": "This paper is intended to study the effects of age of acquisition (AoA) and orthographic transparency on word retrieval in Persian, which is an understudied language. A naming task (both pictures and words) and a recall task (both pictures and words) were used to explore how lexical retrieval and verbal memory are affected by AoA and transparency. Seventy two native speakers of Persian were recruited to participate in two experiments. The results showed that early acquired words are processed faster than late acquired words only when pictures were used as stimuli. Transparency of the words was not an influential factor. However, in the recall experiment a three-way interaction was observed: early acquired pictures and words were processed faster than late acquired stimuli except the words in the transparent condition. The findings speak to the fact that language-specific properties of languages are very important",
    "volume": "workshop",
    "checked": true,
    "id": "01c33568d460f3bbeb3ce8dfb36e77815919fb18",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.cogalex-1.4": {
    "title": "CAT ManyNames: A New Dataset for Object Naming in Catalan",
    "abstract": "Object Naming is an important task within the field of Language and Vision that consists of generating a correct and appropriate name for an object given an image. The ManyNames dataset uses real-world human annotated images with multiple labels, instead of just one. In this work, we describe the adaptation of this dataset (originally in English) to Catalan, by (i) machine-translating the English labels and (ii) collecting human annotations for a subset of the original corpus and comparing both resources. Analyses reveal divergences in the lexical variation of the two sets showing potential problems of directly translated resources, particularly when there is no resource to a proper context, which in this case is conveyed by the image. The analysis also points to the impact of cultural factors in the naming task, which should be accounted for in future cross-lingual naming tasks",
    "volume": "workshop",
    "checked": true,
    "id": "ff3228db41519ea1cc523310c7e254f585826c0c",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.cogalex-1.5": {
    "title": "Finetuning Latin BERT for Word Sense Disambiguation on the Thesaurus Linguae Latinae",
    "abstract": "The Thesaurus Linguae Latinae (TLL) is a comprehensive monolingual dictionary that records contextualized meanings and usages of Latin words in antique sources at an unprecedented scale. We created a new dataset based on a subset of sense representations in the TLL, with which we finetuned the Latin-BERT neural language model (Bamman and Burns, 2020) on a supervised Word Sense Disambiguation task. We observe that the contextualized BERT representations finetuned on TLL data score better than static embeddings used in a bidirectional LSTM classifier on the same dataset, and that our per-lemma BERT models achieve higher and more robust performance than reported by Bamman and Burns (2020) based on data from a bilingual Latin dictionary. We demonstrate the differences in sense organizational principles between these two lexical resources, and report about our dataset construction and improved evaluation methodology",
    "volume": "workshop",
    "checked": true,
    "id": "abc0571c63254fe7e8b81a5cad2274110b5b37b2",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.cogalex-1.6": {
    "title": "Putting WordNet's Dictionary Examples in the Context of Definition Modelling: An Empirical Analysis",
    "abstract": "Definition modeling is the task to generate a valid definition for a given input term. This relatively novel task has been approached either with no context (i.e., given a word embedding alone) and, more recently, as word-in-context modeling. Despite their success, most works make little to no distinction between resources and their specific features (e.g., type and style of definitions, or quality of examples) when used for training. Given the high diversity lexicographic resources exhibit in terms of topic coverage, style and formal structure, it is desirable for downstream definition modeling to better understand which of them are better suited for the task. In this paper, we propose an empirical evaluation of the well-known lexical database WordNet, and specifically, its dictionary examples. We evaluate them both directly, by matching them against criteria for good dictionary writing, and indirectly, in the task of definition modeling. Our results suggest that WordNet’s dictionary examples could be improved by extending them in length, and incorporating prototypicality",
    "volume": "workshop",
    "checked": true,
    "id": "bd42e0c78abb51827713bd16b54f950a63676bfe",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.cogalex-1.7": {
    "title": "Exploring Nominal Coercion in Semantic Spaces with Static and Contextualized Word Embeddings",
    "abstract": "The distinction between mass nouns and count nouns has a long history in formal semantics, and linguists have been trying to identify the semantic properties defining the two classes. However, they also recognized that both can undergo meaning shifts and be used in contexts of a different type, via nominal coercion. In this paper, we present an approach to measure the meaning shift in count-mass coercion in English that makes use of static and contextualized word embedding distance. Our results show that the coercion shifts are detected only by a small subset of the traditional word embedding models, and that the shifts detected by the contextualized embedding of BERT are more pronounced for mass nouns",
    "volume": "workshop",
    "checked": true,
    "id": "7556b2d9f85b97a694c09323f2c90ee5cba33ffe",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.cogalex-1.8": {
    "title": "A Frame-Based Model of Inherent Polysemy, Copredication and Argument Coercion",
    "abstract": "The paper presents a frame-based model of inherently polysemous nouns (such as ‘book’, which denotes both a physical object and an informational content) in which the meaning facets are directly accessible via attributes and which also takes into account the semantic relations between the facets. Predication over meaning facets (as in ‘memorize the book’) is then modeled as targeting the value of the corresponding facet attribute while coercion (as in ‘finish the book’) is modeled via specific patterns that enrich the predication. We use a compositional framework whose basic components are lexicalized syntactic trees paired with semantic frames and in which frame unification is triggered by tree composition. The approach is applied to a variety of combinations of predications over meaning facets and coercions",
    "volume": "workshop",
    "checked": true,
    "id": "a0c4224f7e9d6ccf856e21583a9d4127980e6099",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.cogalex-1.9": {
    "title": "VISCOSE - a Kanji Dictionary Enriched with VISual, COmpositional, and SEmantic Information",
    "abstract": "In this paper, we present a novel approach for building kanji dictionaries by enriching the lexical data of 3,500 kanji with images, structural decompositions, and semantically based cross-media mappings from the textual to the visual dimension. Our kanji dictionary is part of a Web-based contextual language learning environment based on augmented browsing technology. We display our multimodal kanji information as kanji cards in the Web browser, offering a versatile representation that can be integrated into other advanced creative language learning applications, such as memorization puzzles, creative storytelling assignments, or educational games",
    "volume": "workshop",
    "checked": true,
    "id": "d82f51caa2037f52f66f9e6640b7e28c3e3612d5",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.cogalex-1.10": {
    "title": "Compositionality as an Analogical Process: Introducing ANNE",
    "abstract": "Usage-based constructionist approaches consider language a structured inventory of constructions, form-meaning pairings of different schematicity and complexity, and claim that the more a linguistic pattern is encountered, the more it becomes accessible to speakers. However, when an expression is unavailable, what processes underlie the interpretation? While traditional answers rely on the principle of compositionality, for which the meaning is built word-by-word and incrementally, usage-based theories argue that novel utterances are created based on previously experienced ones through analogy, mapping an existing structural pattern onto a novel instance. Starting from this theoretical perspective, we propose here a computational implementation of these assumptions. As the principle of compositionality has been used to generate distributional representations of phrases, we propose a neural network simulating the construction of phrasal embedding as an analogical process. Our framework, inspired by word2vec and computer vision techniques, was evaluated on tasks of generalization from existing vectors",
    "volume": "workshop",
    "checked": true,
    "id": "a8bb67083047be9cd4548bff5275ca20ecfdcdc6",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.eval4nlp-1.1": {
    "title": "A Japanese Corpus of Many Specialized Domains for Word Segmentation and Part-of-Speech Tagging",
    "abstract": "We present a Japanese morphological corpus of sentences from 27 specialized domains for the two tasks of word segmentation and part-of-speech tagging. Experiments on the corpus demonstrated that recent neural models with domain adaptation techniques and pretrained language models achieved accurate performance for the two tasks for many specialized domains",
    "volume": "workshop",
    "checked": true,
    "id": "fee578a89fbc4303b72eb9a71254461dd6f40ab0",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.eval4nlp-1.2": {
    "title": "Assessing Resource-Performance Trade-off of Natural Language Models using Data Envelopment Analysis",
    "abstract": "Natural language models are often summarized through a high-dimensional set of de-scriptive metrics including training corpus size, training time, the number of trainable parameters, inference times, and evaluation statistics that assess performance across tasks. The high dimensional nature of these metrics yields challenges with regard to objectively comparing models; in particular it is challeng-ing to assess the trade-off models make between performance and resources (compute time, memory, etc.). We apply Data Envelopment Analysis (DEA) to this problem of assessing the resource-performance trade-off. DEA is a nonparamet-ric method that measures productive efﬁciency of abstract units that consume one or more inputs and yield at least one output . We recast natural language models as units suitable for DEA, and we show that DEA can be used to create an effective framework for quantifying model performance and efﬁciency. A central feature of DEA is that it identiﬁes a subset of models that live on an efﬁcient frontier of performance. DEA is also scalable, having been applied to problems with thousands of units. We report empirical results of DEA applied to 14 different language models that have a variety of architectures, and we show that DEA can be used to identify a subset of models that effectively balance resource demands against performance",
    "volume": "workshop",
    "checked": true,
    "id": "2429c30f25195bbf6df965aac3827e3610ba2e6c",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.eval4nlp-1.3": {
    "title": "From COMET to COMES – Can Summary Evaluation Benefit from Translation Evaluation?",
    "abstract": "C OMET is a recently proposed trainable neural-based evaluation metric developed to assess the quality of Machine Translation systems. In this paper, we explore the usage of C OMET for evaluating Text Summarization systems – despite being trained on multilingual MT outputs, it performs remarkably well in monolingual settings, when predicting summarization output quality. We introduce a variant of the model – C OMES – trained on the annotated summarization outputs that uses MT data for pre-training. We examine its performance on several datasets with human judgments collected for different notions of summary quality, covering several domains and languages",
    "volume": "workshop",
    "checked": true,
    "id": "054e70254fb2ee1c1cd9cac91096ee1b24e1eef3",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.eval4nlp-1.4": {
    "title": "Better Smatch = Better Parser? AMR evaluation is not so simple anymore",
    "abstract": "Recently, astonishing advances have been observed in AMR parsing, as measured by the structural S MATCH metric. In fact, today’s systems achieve performance levels that seem to surpass estimates of human inter annotator agreement (IAA). Therefore, it is unclear how well S MATCH (still) relates to human estimates of parse quality, as in this situation potentially ﬁne-grained errors of similar weight may impact the AMR’s meaning to different degrees. We conduct an analysis of two popular and strong AMR parsers that – according to S MATCH – reach quality levels on par with human IAA, and assess how human quality rat-ings relate to S MATCH and other AMR metrics. Our main ﬁndings are: i) While high S MATCH scores indicate otherwise, we ﬁnd that AMR parsing is far from being solved : we frequently ﬁnd structurally small, but semanti-cally unacceptable errors that substantially distort sentence meaning. ii) Considering high-performance parsers, better S MATCH scores may not necessarily indicate consistently better parsing quality . To obtain a meaning-ful and comprehensive assessment of quality differences of parse(r)s, we recommend aug-menting evaluations with macro statistics, use of additional metrics, and more human analysis",
    "volume": "workshop",
    "checked": true,
    "id": "413d21d8c9909d6776baa9f9b0ac762fbdccf0ca",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.eval4nlp-1.5": {
    "title": "GLARE: Generative Left-to-right AdversaRial Examples",
    "abstract": "Recently, transformer models (Vaswani et al., 2017) have been applied to adversarial example generation—word-level substitution models utilizing BERT (Devlin et al., 2018; Garg and Ra-makrishnan, 2020; Li et al., 2020a,b) have out-performed previous state-of-the-art approaches. Extending the paradigm of transformer-based generation of adversarial examples, we propose a novel textual adversarial example generation framework based on transformer language models: our method (GLARE) generates word- and span-level perturbations of input examples using ILM (Donahue et al., 2020), a GPT-2 language model finetuned to fill in masked spans. We demonstrate that GLARE achieves a supe-rior performance to CLARE (the current state-of-the-art model) in terms of attack success rate and semantic similarity between the perturbed and original examples. 1",
    "volume": "workshop",
    "checked": true,
    "id": "c6d3d59570ab060e04a812a27f5c7b79776ff6cd",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.eval4nlp-1.6": {
    "title": "Random Text Perturbations Work, but not Always",
    "abstract": "We present three large-scale experiments on binary text matching classiﬁcation task both in Chinese and English to evaluate the effectiveness and generalizability of random text perturbations as a data augmentation approach for NLP. It is found that the augmentation can bring both negative and positive effects to the test set performance of three neural classiﬁcation models, depending on whether the models train on enough original training examples. This remains true no matter whether ﬁve random text editing operations, used to aug-ment text, are applied together or separately. Our study demonstrates with strong implica-tion that the effectiveness of random text perturbations is task speciﬁc and not generally positive",
    "volume": "workshop",
    "checked": true,
    "id": "8cde4e1f1ecec01a28b2117a7ed53707ee3472d5",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.eval4nlp-1.7": {
    "title": "A Comparative Analysis of Stance Detection Approaches and Datasets",
    "abstract": "Various approaches have been proposed for automated stance detection, including those that use machine and deep learning models and natural language processing techniques. However, their cross-dataset performance, the impact of sample size on performance, and experimental aspects such as runtime have yet to be compared, limiting what is known about the generalizability of prominent approaches. This paper presents a replication study of stance detection approaches on current benchmark datasets. Specifically, we compare six existing machine and deep learning stance detection models on three publicly available datasets. We investigate performance as a function of the number of samples, length of samples (word count), representation across targets, type of text data, and the stance detection models themselves. We identify the current limitations of these approaches and categorize their utility for stance detection under varying circumstances (e.g., size of text samples), which provides valuable insight for future research in stance detection",
    "volume": "workshop",
    "checked": true,
    "id": "bae874f8e3ea28ec30a227010bf5e1b43afeeec6",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.eval4nlp-1.8": {
    "title": "Why is sentence similarity benchmark not predictive of application-oriented task performance?",
    "abstract": "Computing the semantic similarity between two texts is crucial in various NLP tasks. For more than a decade, a framework, known as Semantic Textual Similarity (STS) has been used to test computational models of semantic similarity (Agirre et al., 2012). The STS evaluation framework assumes that a model that performs well for the general STS task should also perform well for specific application-oriented tasks. However, does this assumption indeed hold? This study empirically demonstrates that the answer is not always positive. We found a considerable gap between model performance in STS and each specific task. We identified three factors that contributed to the gap, namely, (i) sentence length distribution, (ii) vocabulary coverage, and (iii) granularity of gold-standard similarity scores. We believe that these findings will be considered in future research on semantic similarity",
    "volume": "workshop",
    "checked": true,
    "id": "f6a536af5e78d68fec71cb1c9691d712b7b099a6",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.eval4nlp-1.9": {
    "title": "Chat Translation Error Detection for Assisting Cross-lingual Communications",
    "abstract": "In this paper, we describe the development of a communication support system that detects erroneous translations to facilitate crosslingual communications due to the limitations of current machine chat translation methods. We trained an error detector as the baseline of the system and constructed a new Japanese–English bilingual chat corpus, BPersona-chat, which comprises multi-turn colloquial chats augmented with crowdsourced quality ratings. The error detector can serve as an encouraging foundation for more advanced erroneous translation detection systems",
    "volume": "workshop",
    "checked": true,
    "id": "abbdbb793fdc7c162d44b82d1fc6f45f45a677fb",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.eval4nlp-1.10": {
    "title": "Evaluating the role of non-lexical markers in GPT-2's language modeling behavior",
    "abstract": "Transformer-based language models are often trained on structured text where non-lexical markers of sentence and discourse structure (e.g., punctuation and casing) are present and used consistently. Transformers encode these markers and arguably benefit from the information they convey. Yet, a systematic evalua-tions of the contribution of non-lexical markers to model performance, and of whether models’ behavior changes significantly in their absence, is currently lacking. This knowledge is both relevant from a theoretical standpoint, but also important to understand how well pretrained models may perform in common application scenarios where casing and punctuation are absent or inconsistent. Here, we analyze GPT-2’s language modeling behavior in parallel corpora that differ in the presence vs. absence of consistent punctuation and casing. We compute GPT-2’s precision and uncertainty in next-token prediction for multiple context sizes, and compare the resulting performance distri-butions across corpora. We find that absence of non-lexical markers, especially punctuation, increases model uncertainty, and it affects (but does not catastrophically disrupt) GPT-2’s precision in next-token prediction. Interestingly, the absence of non-lexical markers prevents the model from benefiting from larger contexts in order to reduce the uncertainty of its predictions. Future work will extend this paradigm to a wider range of models and systematically investigate how features of training text affect both language modeling and downstream predictive performance",
    "volume": "workshop",
    "checked": true,
    "id": "070b85de2624146a08a037a766221ff90ee9fd44",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.eval4nlp-1.11": {
    "title": "Assessing Neural Referential Form Selectors on a Realistic Multilingual Dataset",
    "abstract": "Previous work on Neural Referring Expression Generation (REG) all uses W eb NLG , an English dataset that has been shown to reﬂect a very limited range of referring expression (RE) use. To tackle this issue, we build a dataset based on the O nto N otes corpus that contains a broader range of RE use in both English and Chinese (a language that uses zero pro-nouns). We build neural Referential Form Selection (RFS) models accordingly, assess them on the dataset and conduct probing experiments. The experiments suggest that, compared to W eb NLG , O nto N otes is better for assessing REG/RFS models. We compare English and Chinese RFS and conﬁrm that, in line with linguistic theories, Chinese RFS depends more on discourse context than English",
    "volume": "workshop",
    "checked": true,
    "id": "5b5ef1d03f05028b1a9ecd183df2d96a5b67f3ea",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.1": {
    "title": "A Stylometric Analysis of Amadís de Gaula and Sergas de Esplandián",
    "abstract": "Amadís de Gaula (AG) and its sequel Sergas de Esplandián (SE) are masterpieces of medieval Spanish chivalric romances. Much debate has been devoted to the role played by their purported author Garci Rodríguez de Montalvo. According to the prologue of AG, which consists of four books, the author allegedly revised the first three books that were in circulation at that time and added the fourth book and SE. However, the extent to which Montalvo edited the materials at hand to compose the extant works has yet to be explored extensively. To address this question, we applied stylometric techniques for the first time. Specifically, we investigated the stylistic differences (if any) between the first three books of AG and his own extensions. Literary style is represented as usage of parts-of-speech n-grams. We performed principal component analysis and k-means to demonstrate that Montalvo’s retouching on the first book was minimal, while revising the second and third books in such a way that they came to moderately resemble his authentic creation, that is, the fourth book and SE. Our findings empirically corroborate suppositions formulated from philological viewpoints",
    "volume": "workshop",
    "checked": true,
    "id": "61f4667647a9f077406f2897a22381745c6d4902",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.2": {
    "title": "Computational Exploration of the Origin of Mood in Literary Texts",
    "abstract": "This paper is a methodological exploration of the origin of mood in early modern and modern Finnish literary texts using computational methods. We discuss the pre-processing steps as well as the various natural language processing tools used to try to pinpoint where mood can be best detected in text. We also share several tools and resources developed during this process. Our early attempts suggest that overall mood can be computationally detected in the first three paragraphs of a book",
    "volume": "workshop",
    "checked": true,
    "id": "cecd735b053594b3f69c1a78119ad1a01fc000a4",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.3": {
    "title": "Sentiment is all you need to win US Presidential elections",
    "abstract": "Election speeches play an integral role in communicating the vision and mission of the candidates. From lofty promises to mud-slinging, the electoral candidate accounts for all. However, there remains an open question about what exactly wins over the voters. In this work, we used state-of-the-art natural language processing methods to study the speeches and sentiments of the Republican candidates and Democratic candidates fighting for the 2020 US Presidential election. Comparing the racial dichotomy of the United States, we analyze what led to the victory and defeat of the different candidates. We believe this work will inform the election campaigning strategy and provide a basis for communicating to diverse crowds",
    "volume": "workshop",
    "checked": true,
    "id": "a87552dbf5b4c807ef6174c4df7db26f8f9a9ba7",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.4": {
    "title": "Interactive Analysis and Visualisation of Annotated Collocations in Spanish (AVAnCES)",
    "abstract": "Phraseology studies have been enhanced by Corpus Linguistics, which has become an interdisciplinary field where current technologies play an important role in its development. Computational tools have been implemented in the last decades with positive results on the identification of phrases in different languages. One specific technology that has impacted these studies is social media. As researchers, we have turned our attention to collecting data from these platforms, which comes with great advantages and its own challenges. One of the challenges is the way we design and build corpora relevant to the questions emerging in this type of language expression. This has been approached from different angles, but one that has given invaluable outputs is the building of linguistic corpora with the use of online web applications. In this paper, we take a multidimensional approach to the collection, design, and deployment of a phraseology corpus for Latin American Spanish from Twitter data, extracting features using NLP techniques, and presenting it in an interactive online web application. We expect to contribute to the methodologies used for Corpus Linguistics in the current technological age. Finally, we make this tool publicly available to be used by any researcher interested in the data itself and also on the technological tools developed here",
    "volume": "workshop",
    "checked": true,
    "id": "110b7fdda8f277cc161d68a6ad86839c60f64c82",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.5": {
    "title": "Fractality of sentiment arcs for literary quality assessment: The case of Nobel laureates",
    "abstract": "In the few works that have used NLP to study literary quality, sentiment and emotion analysis have often been considered valuable sources of information. At the same time, the idea that the nature and polarity of the sentiments expressed by a novel might have something to do with its perceived quality seems limited at best. In this paper, we argue that the fractality of narratives, specifically the long-term memory of their sentiment arcs, rather than their simple shape or average valence, might play an important role in the perception of literary quality by a human audience. In particular, we argue that such measure can help distinguish Nobel-winning writers from control groups in a recent corpus of English language novels. To test this hypothesis, we present the results from two studies: (i) a probability distribution test, where we compute the probability of seeing a title from a Nobel laureate at different levels of arc fractality; (ii) a classification test, where we use several machine learning algorithms to measure the predictive power of both sentiment arcs and their fractality measure. Our findings seem to indicate that despite the competitive and complex nature of the task, the populations of Nobel and non-Nobel laureates seem to behave differently and can to some extent be told apart by a classifier",
    "volume": "workshop",
    "checked": true,
    "id": "e67e91a1e1b0615573f1b92c313c0211168c12b6",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.6": {
    "title": "Style Classification of Rabbinic Literature for Detection of Lost Midrash Tanhuma Material",
    "abstract": "Midrash collections are complex rabbinic works that consist of text in multiple languages, that evolved through long processes of instable oral and written transmission. Determining the origin of a given passage in such a compilation is not always straightforward and is often a matter disputed by scholars, yet it is essential for scholars’ understanding of the passage and its relationship to other texts in the rabbinic corpus. To help solve this problem, we propose a system for classification of rabbinic literature based on its style, leveraging recently released pretrained Transformer models for Hebrew. Additionally, we demonstrate how our method can be applied to uncover lost material from the Midrash Tanhuma",
    "volume": "workshop",
    "checked": true,
    "id": "10b2b289ff2b87a39495ab9ab2af9825c7c75a81",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.7": {
    "title": "Use the Metadata, Luke! – An Experimental Joint Metadata Search and N-gram Trend Viewer for Personal Web Archives",
    "abstract": "Many digital humanists (philologists, historians, sociologists, librarians, the audience for web archives) design their research around metadata (publication date ranges, sources, authors, etc.). However, current major web archives are limited to technical metadata while lacking high quality, descriptive metadata allowing for faceted queries. As researchers often lack the technical skill necessary to enrich existing web archives with descriptive metadata, they increasingly turn to creating personal web archives that contain such metadata, tailored to their research requirements. Software that enable creating such archives without advanced technical skills have gained popularity, however, tools for examination and querying are currently the missing link. We showcase a solution designed to fill this gap",
    "volume": "workshop",
    "checked": true,
    "id": "38c2f4b22a5e34e679e857c8def72480cfeea791",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.8": {
    "title": "MALM: Mixing Augmented Language Modeling for Zero-Shot Machine Translation",
    "abstract": "Large pre-trained language models have brought remarkable progress in NLP. Pre-training and Fine-tuning have given state-of-art performance across tasks in text processing. Data Augmentation techniques have also helped build state-of-art models on low or zero resource tasks. Many works in the past have attempted at learning a single massively multilingual machine translation model for zero-shot translation. Although those translation models are producing correct translations, the main challenge is those models are producing the wrong languages for zero-shot translation. This work and its results indicate that prompt conditioned large models do not suffer from off-target language errors i.e. errors arising due to translation to wrong languages. We empirically demonstrate the effectiveness of self-supervised pre-training and data augmentation for zero-shot multi-lingual machine translation",
    "volume": "workshop",
    "checked": true,
    "id": "5c6f057a70b658b17902182636bb40a51ce9a887",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.9": {
    "title": "ParsSimpleQA: The Persian Simple Question Answering Dataset and System over Knowledge Graph",
    "abstract": "The simple question answering over the knowledge graph concerns answering single-relation questions by querying the facts in the knowledge graph. This task has drawn significant attention in recent years. However, there is a demand for a simple question dataset in the Persian language to study open-domain simple question answering. In this paper, we present the first Persian single-relation question answering dataset and a model that uses a knowledge graph as a source of knowledge to answer questions. We create the ParsSimpleQA dataset semi-automatically in two steps. First, we build single-relation question templates. Next, we automatically create simple questions and answers using templates, entities, and relations from Farsbase. To present the reliability of the presented dataset, we proposed a simple question-answering system that receives questions and uses deep learning and information retrieval techniques for answering questions. The experimental results presented in this paper show that the ParsSimpleQA dataset is very promising for the Persian simple question-answering task",
    "volume": "workshop",
    "checked": true,
    "id": "f4a841d8b0cb60ee2339ab6da50bca2ccb02122c",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.10": {
    "title": "Enhancing Digital History – Event discovery via Topic Modeling and Change Detection",
    "abstract": "Digital history is the application of computer science techniques to historical data in order to uncover insights into events occurring during specific time periods from the past. This relatively new interdisciplinary field can help identify and record latent information about political, cultural, and economic trends that are not otherwise apparent from traditional historical analysis. This paper presents a method that uses topic modeling and breakpoint detection to observe how extracted topics come in and out of prominence over various time periods. We apply our techniques on British parliamentary speech data from the 19th century. Findings show that some of the events produced are cohesive in topic content (religion, transportation, economics, etc.) and time period (events are focused in the same year or month). Topic content identified should be further analyzed for specific events and undergo external validation to determine the quality and value of the findings to historians specializing in 19th century Britain",
    "volume": "workshop",
    "checked": true,
    "id": "8edbb46f502bee8ee8d12a547c208152af420b92",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.11": {
    "title": "A Parallel Corpus and Dictionary for Amis-Mandarin Translation",
    "abstract": "Amis is an endangered language indigenous to Taiwan with limited data available for computational processing. We thus present an Amis-Mandarin dataset containing a parallel corpus of 5,751 Amis and Mandarin sentences and a dictionary of 7,800 Amis words and phrases with their definitions in Mandarin. Using our dataset, we also established a baseline for machine translation between Amis and Mandarin in both directions. Our dataset can be found at https://github.com/francisdzheng/amis-mandarin",
    "volume": "workshop",
    "checked": true,
    "id": "96109028a1f584d48bbbaaaed84a1171ee66eee8",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.12": {
    "title": "Machines in the media: semantic change in the lexicon of mechanization in 19th-century British newspapers",
    "abstract": "The industrialization process associated with the so-called Industrial Revolution in 19th-century Great Britain was a time of profound changes, including in the English lexicon. An important yet understudied phenomenon is the semantic shift in the lexicon of mechanisation. In this paper we present the first large-scale analysis of terms related to mechanization over the course of the 19th-century in English. We draw on a corpus of historical British newspapers comprising 4.6 billion tokens and train historical word embedding models. We test existing semantic change detection techniques and analyse the results in light of previous historical linguistic scholarship",
    "volume": "workshop",
    "checked": true,
    "id": "9b5530a909315acea5a8247383dc2fc2e5373e8d",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.13": {
    "title": "Optimizing the weighted sequence alignment algorithm for large-scale text similarity computation",
    "abstract": "We present an optimized implementation of the weighted sequence alignment algorithm (a.k.a. weighted edit distance) in a scenario where the items to align are numeric vectors and the substitution weights are determined by their cosine similarity. The optimization relies on using vector and matrix operations provided by numeric computation libraries (including GPU acceleration) instead of loops. The resulting algorithm provides an efficient way of aligning large sets of texts represented as sequences of continuous-space numeric vectors (embeddings). The optimization made it possible to compute alignment-based similarity for all pairs of texts in a large corpus of Finnic oral folk poetry for the purpose of studying intertextuality in the oral tradition",
    "volume": "workshop",
    "checked": true,
    "id": "2a32ff1a4aa7e9280b78f2e955fbc921798536c3",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.14": {
    "title": "Domain-specific Evaluation of Word Embeddings for Philosophical Text using Direct Intrinsic Evaluation",
    "abstract": "We perform a direct intrinsic evaluation of word embeddings trained on the works of a single philosopher. Six models are compared to human judgements elicited using two tasks: a synonym detection task and a coherence task. We apply a method that elicits judgements based on explicit knowledge from experts, as the linguistic intuition of non-expert participants might differ from that of the philosopher. We find that an in-domain SVD model has the best 1-nearest neighbours for target terms, while transfer learning-based Nonce2Vec performs better for low frequency target terms",
    "volume": "workshop",
    "checked": true,
    "id": "f37048f8e3095cc6bbb54169ed17ebab8a4c89f0",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.15": {
    "title": "Towards Bootstrapping a Chatbot on Industrial Heritage through Term and Relation Extraction",
    "abstract": "We describe initial work in developing a methodology for the automatic generation of a conversational agent or ‘chatbot’ through term and relation extraction from a relevant corpus of language data. We develop our approach in the domain of industrial heritage in the 18th and 19th centuries, and more specifically on the industrial history of canals and mills in Ireland. We collected a corpus of relevant newspaper reports and Wikipedia articles, which we deemed representative of a layman’s understanding of this topic. We used the Saffron toolkit to extract relevant terms and relations between the terms from the corpus and leveraged the extracted knowledge to query the British Library Digital Collection and the Project Gutenberg library. We leveraged the extracted terms and relations in identifying possible answers for a constructed set of questions based on the extracted terms, by matching them with sentences in the British Library Digital Collection and the Project Gutenberg library. In a final step, we then took this data set of question-answer pairs to train a chatbot. We evaluate our approach by manually assessing the appropriateness of the generated answers for a random sample, each of which is judged by four annotators",
    "volume": "workshop",
    "checked": true,
    "id": "73dff7b72c4f4a8eef81982bac79a31e2056e08a",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.16": {
    "title": "Non-Parametric Word Sense Disambiguation for Historical Languages",
    "abstract": "Recent approaches to Word Sense Disambiguation (WSD) have profited from the enhanced contextualized word representations coming from contemporary Large Language Models (LLMs). This advancement is accompanied by a renewed interest in WSD applications in Humanities research, where the lack of suitable, specific WSD-annotated resources is a hurdle in developing ad-hoc WSD systems. Because they can exploit sentential context, LLMs are particularly suited for disambiguation tasks. Still, the application of LLMs is often limited to linear classifiers trained on top of the LLM architecture. In this paper, we follow recent developments in non-parametric learning and show how LLMs can be efficiently fine-tuned to achieve strong few-shot performance on WSD for historical languages (English and Dutch, date range: 1450-1950). We test our hypothesis using (i) a large, general evaluation set taken from large lexical databases, and (ii) a small real-world scenario involving an ad-hoc WSD task. Moreover, this paper marks the release of GysBERT, a LLM for historical Dutch",
    "volume": "workshop",
    "checked": true,
    "id": "f2934365e4f71704da1cf2ad473feb080f864816",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.17": {
    "title": "Introducing a Large Corpus of Tokenized Classical Chinese Poems of Tang and Song Dynasties",
    "abstract": "Classical Chinese poems of Tang and Song dynasties are an important part for the studies of Chinese literature. To thoroughly understand the poems, properly segmenting the verses is an important step for human readers and software agents. Yet, due to the availability of data and the costs of annotation, there are still no known large and useful sources that offer classical Chinese poems with annotated word boundaries. In this project, annotators with Chinese literature background labeled 32399 poems. We analyzed the annotated patterns and conducted inter-rater agreement studies about the annotations. The distributions of the annotated patterns for poem lines are very close to some well-known professional heuristics, i.e., that the 2-2-1, 2-1-2, 2-2-1-2, and 2-2-2-1 patterns are very frequent. The annotators agreed well at the line level, but agreed on the segmentations of a whole poem only 43% of the time. We applied a traditional machine-learning approach to segment the poems, and achieved promising results at the line level as well. Using the annotated data as the ground truth, these methods could segment only about 18% of the poems completely right under favorable conditions. Switching to deep-learning methods helped us achieved better than 30%",
    "volume": "workshop",
    "checked": true,
    "id": "7825bf2f009e6e8897fb77f1f0f032643192b49e",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.18": {
    "title": "Creative Text-to-Image Generation: Suggestions for a Benchmark",
    "abstract": "Language models for text-to-image generation can output good quality images when referential aspects of pictures are evaluated. The generation of creative images is not under scrutiny at the moment, but it poses interesting challenges: should we expect more creative images using more creative prompts? What is the relationship between prompts and images in the global process of human evaluation? In this paper, we want to highlight several criteria that should be taken into account for building a creative text-to-image generation benchmark, collecting insights from multiple disciplines (e.g., linguistics, cognitive psychology, philosophy, psychology of art)",
    "volume": "workshop",
    "checked": true,
    "id": "7b72c32c5b11612f4e4d24a1dff263a417dbe1ad",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.19": {
    "title": "The predictability of literary translation",
    "abstract": "Research has shown that the practice of translation exhibits predictable linguistic cues that make translated texts detectable from original-language texts (a phenomenon known as “translationese”). In this paper, we test the extent to which literary translations are subject to the same effects and whether they also exhibit meaningful differences at the level of content. Research into the function of translations within national literary markets using smaller case studies has suggested that translations play a cultural role that is distinct from that of original-language literature, i.e. their differences reside not only at the level of translationese but at the level of content. Using a dataset consisting of original-language fiction in English and translations into English from 120 languages (N=21,302), we find that one of the principal functions of literary translation is to convey predictable geographic identities to local readers that nevertheless extend well beyond the foreignness of persons and places",
    "volume": "workshop",
    "checked": true,
    "id": "e16bfc9fed117873d450f2dd079b2c677191eb5b",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.20": {
    "title": "Emotion Conditioned Creative Dialog Generation",
    "abstract": "We present a DialGPT based model for generating creative dialog responses that are conditioned based on one of the following emotions: anger, disgust, fear, happiness, pain, sadness and surprise. Our model is capable of producing a contextually apt response given an input sentence and a desired emotion label. Our model is capable of expressing the desired emotion with an accuracy of 0.6. The best performing emotions are neutral, fear and disgust. When measuring the strength of the expressed emotion, we find that anger, fear and disgust are expressed in the most strong fashion by the model",
    "volume": "workshop",
    "checked": true,
    "id": "a1245ef24fcc470469881227ed0a145b23e9cd65",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.21": {
    "title": "Integration of Named Entity Recognition and Sentence Segmentation on Ancient Chinese based on Siku-BERT",
    "abstract": "Sentence segmentation and named entity recognition are two significant tasks in ancient Chinese processing since punctuation and named entity information are important for further research on ancient classics. These two are sequence labeling tasks in essence so we can tag the labels of these two tasks for each token simultaneously. Our work is to evaluate whether such a unified way would be better than tagging the label of each task separately with a BERT-based model. The paper adopts a BERT-based model that was pre-trained on ancient Chinese text to conduct experiments on Zuozhuan text. The results show there is no difference between these two tagging approaches without concerning the type of entities and punctuation. The ablation experiments show that the punctuation token in the text is useful for NER tasks, and finer tagging sets such as differentiating the tokens that locate at the end of an entity and those are in the middle of an entity could offer a useful feature for NER while impact negatively sentences segmentation with unified tagging",
    "volume": "workshop",
    "checked": true,
    "id": "19c141c0d44170fb8dcd45db7c7cd919569a8ece",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.nlp4dh-1.22": {
    "title": "(Re-)Digitizing 吳守禮 Ngôo Siú-lé's Mandarin – Taiwanese Dictionary",
    "abstract": "This paper presents the efforts conducted to obtain a usable and open digital version in XML-TEI of one of the major lexicographic work for bilingual Taiwanese dictionaries, namely the 《國臺對照活用辭典》(Practical Mandarin-Taiwanese Dictionary) The original dictionary was published in 2000, after decades of work by Prof. 吳守禮 (Ngôo Siu-le/Wu Shouli)",
    "volume": "workshop",
    "checked": true,
    "id": "d68bddff14a96806e937ecb3ffeca2c8bb274572",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.sumeval-1.1": {
    "title": "The SUMEval 2022 Shared Task on Performance Prediction of Multilingual Pre-trained Language Models",
    "abstract": "The SUMEval Workshop’s shared task in-volved predicting performance of multilingual PLMs across multiple languages when these models are fine-tuned with varying amounts of data in different languages. The training data was provided for performances of two multilingual models on four NLP tasks, and a baseline was shared with the participants to get started. For test data, the task had two variants for evaluation, non-surprise version where the performance was to be predicted for languages seen in the training data but with unseen configurations, and surprise version where the languages were unseen during the training. A total of five teams participated in the shared task with 15 submissions overall. The participants proposed addition of new features, feature engineering techniques and trained an ensemble of regression models for the task. The best performing team had an improvement of 64% in MAE over the shared baseline for the non-surprise variant, and a 17% improvement for the surprise variant",
    "volume": "workshop",
    "checked": true,
    "id": "ea77d7f930761a0a462d70de70318487b06e3d5d",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.sumeval-1.2": {
    "title": "To Train or Not to Train: Predicting the Performance of Massively Multilingual Models",
    "abstract": "Evaluating the performance of Massively Multilingual Language Models (MMLMs) is difficult due to the shortage of evaluation datasets in low-resource languages. Due to computational limitations evaluating MMLMs trained on all possible pivot configurations is not feasible. This paper describes our contribution to the SumEval 2022 shared task, which handles the crucial task of Performance prediction of MMLMs. We build upon Microsoft Research’s Project LITMUS and devise a method to further improve predictions. We develop various machine-learning approaches which outperform the baseline score provided by LITMUS. Our system ranked first with an RMSE score of 0.017 for the non-surprise and 0.109 for the surprise dataset",
    "volume": "workshop",
    "checked": true,
    "id": "f493a82734d6bec6c28a3c78978c5eb95b97fc87",
    "citation_count": 1
  },
  "https://aclanthology.org/2022.sumeval-1.3": {
    "title": "The GMU System Submission for the SUMEval 2022 Shared Task",
    "abstract": "This paper describes the submission of our multilingual NLP model performance evaluation system for the SUMEval 2022 shared task, a system for predict the performance of a model on a set of target languages. The system is based on the LITMUS model (Srinivasan et al., 2022), with the addition of 3 new features and model ensembling. Experimental results show that our system obtains a significant improvement than the baseline on both the test set and the surprised test set. Our system has achieved a 11% MAE reduction on the test set and is the best-performing submission on the surprise test set with 17% MAE reduction compared to the baseline. 1",
    "volume": "workshop",
    "checked": true,
    "id": "ea37ce6f8c0827f19f3b324a3c1003cdfafa2856",
    "citation_count": 1
  },
  "https://aclanthology.org/2022.sumeval-1.4": {
    "title": "NTREX-128 – News Test References for MT Evaluation of 128 Languages",
    "abstract": "We release NTREX-128, a data set for machine translation (MT) evaluation from English into a total of 128 target languages. The paper de-scribes the data creation process and proposes a quality filtering method based on human evaluation. We show experimental results which confirm that the directionality of test sets translation indeed plays an important role wrt. the usefulness of the corresponding metrics’ scores. Thus, we recommend that the NTREX-128 data set should be used for evaluation of English-sourced translation models but not in reverse direction. The test set release introduces another benchmark for the evaluation of massively multilingual machine translation research",
    "volume": "workshop",
    "checked": true,
    "id": "e113f849feab96255b17b73ea5bca064174765e5",
    "citation_count": 0
  },
  "https://aclanthology.org/2022.sumeval-1.5": {
    "title": "IndoRobusta: Towards Robustness Against Diverse Code-Mixed Indonesian Local Languages",
    "abstract": "Signiﬁcant progress has been made on Indonesian NLP. Nevertheless, exploration of the code-mixing phenomenon in Indonesian is limited, despite many languages being frequently mixed with Indonesian in daily conversation. In this work, we explore code-mixing in Indonesian with four embedded languages, i.e., English, Sundanese, Javanese, and Malay; and introduce IndoRobusta 1 , a framework to evaluate and improve the code-mixing robustness. Our analysis shows that the pre-training corpus bias affects the model’s ability to better handle Indonesian-English code-mixing when compared to other local languages, despite having higher language diversity",
    "volume": "workshop",
    "checked": true,
    "id": "75974dcb2b3fdd3143e341227f2bc0f4047dbadb",
    "citation_count": 0
  }
}