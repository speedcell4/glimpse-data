{
  "https://aclanthology.org/D16-1001": {
    "title": "Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "715dde17239c52b3f9924a5a35edc32b0f27830b",
    "semantic_title": "span-based constituency parsing with a structure-label system and provably optimal dynamic oracles",
    "citation_count": 117,
    "authors": [
      "James Cross",
      "Liang Huang"
    ]
  },
  "https://aclanthology.org/D16-1002": {
    "title": "Rule Extraction for Tree-to-Tree Transducers by Cost Minimization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "388fcc03a5babd658c52c1c3be8149b64b5dbbb1",
    "semantic_title": "rule extraction for tree-to-tree transducers by cost minimization",
    "citation_count": 6,
    "authors": [
      "Pascual Martínez-Gómez",
      "Yusuke Miyao"
    ]
  },
  "https://aclanthology.org/D16-1003": {
    "title": "A Neural Network for Coordination Boundary Prediction",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f7e37cd317e037e05e30198b4259032d5f5e4e9e",
    "semantic_title": "a neural network for coordination boundary prediction",
    "citation_count": 20,
    "authors": [
      "Jessica Ficler",
      "Yoav Goldberg"
    ]
  },
  "https://aclanthology.org/D16-1004": {
    "title": "Using Left-corner Parsing to Encode Universal Structural Constraints in Grammar Induction",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "37fcc0d2a57d310b3cfecd161a197221e0ca25ab",
    "semantic_title": "using left-corner parsing to encode universal structural constraints in grammar induction",
    "citation_count": 32,
    "authors": [
      "Hiroshi Noji",
      "Yusuke Miyao",
      "Mark Johnson"
    ]
  },
  "https://aclanthology.org/D16-1005": {
    "title": "Distinguishing Past, On-going, and Future Events: The EventStatus Corpus",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9d1c5959fa42db039f3f0f769da0ae44c9a915a1",
    "semantic_title": "distinguishing past, on-going, and future events: the eventstatus corpus",
    "citation_count": 18,
    "authors": [
      "Ruihong Huang",
      "Ignacio Cases",
      "Dan Jurafsky",
      "Cleo Condoravdi",
      "Ellen Riloff"
    ]
  },
  "https://aclanthology.org/D16-1006": {
    "title": "Nested Propositions in Open Information Extraction",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4c187332ba519e50feb6ca1454dd02682a0a3643",
    "semantic_title": "nested propositions in open information extraction",
    "citation_count": 53,
    "authors": [
      "Nikita Bhutani",
      "H. V. Jagadish",
      "Dragomir Radev"
    ]
  },
  "https://aclanthology.org/D16-1007": {
    "title": "A Position Encoding Convolutional Neural Network Based on Dependency Tree for Relation Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "28fccd407338011bbb553d873bf0757bd131799d",
    "semantic_title": "a position encoding convolutional neural network based on dependency tree for relation classification",
    "citation_count": 39,
    "authors": [
      "Yunlun Yang",
      "Yunhai Tong",
      "Shulei Ma",
      "Zhi-Hong Deng"
    ]
  },
  "https://aclanthology.org/D16-1008": {
    "title": "Learning to Recognize Discontiguous Entities",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "68077d1b6b6185aaa714cc81d70c848efe9754da",
    "semantic_title": "learning to recognize discontiguous entities",
    "citation_count": 31,
    "authors": [
      "Aldrian Obaja Muis",
      "Wei Lu"
    ]
  },
  "https://aclanthology.org/D16-1009": {
    "title": "Modeling Human Reading with Neural Attention",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "29f2fcfc34fa08a7980a29e06d8bc0ed219e478d",
    "semantic_title": "modeling human reading with neural attention",
    "citation_count": 51,
    "authors": [
      "Michael Hahn",
      "Frank Keller"
    ]
  },
  "https://aclanthology.org/D16-1010": {
    "title": "Comparing Computational Cognitive Models of Generalization in a Language Acquisition Task",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c442df2443d2fb87c8483de5112d9f5ab4ac2a5a",
    "semantic_title": "comparing computational cognitive models of generalization in a language acquisition task",
    "citation_count": 15,
    "authors": [
      "Libby Barak",
      "Adele E. Goldberg",
      "Suzanne Stevenson"
    ]
  },
  "https://aclanthology.org/D16-1011": {
    "title": "Rationalizing Neural Predictions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "467d5d8fc766e73bfd3e9415f75479823f92c2f7",
    "semantic_title": "rationalizing neural predictions",
    "citation_count": 696,
    "authors": [
      "Tao Lei",
      "Regina Barzilay",
      "Tommi Jaakkola"
    ]
  },
  "https://aclanthology.org/D16-1012": {
    "title": "Deep Multi-Task Learning with Shared Memory for Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "652a7e6090a6b10cbeb0883ddac2002620aa8e69",
    "semantic_title": "deep multi-task learning with shared memory for text classification",
    "citation_count": 46,
    "authors": [
      "Pengfei Liu",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/D16-1013": {
    "title": "Natural Language Comprehension with the EpiReader",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "75fa915984f1903cd8d0e1ea54b9d008d5a87fe5",
    "semantic_title": "natural language comprehension with the epireader",
    "citation_count": 95,
    "authors": [
      "Adam Trischler",
      "Zheng Ye",
      "Xingdi Yuan",
      "Philip Bachman",
      "Alessandro Sordoni",
      "Kaheer Suleman"
    ]
  },
  "https://aclanthology.org/D16-1014": {
    "title": "Creating Causal Embeddings for Question Answering with Minimal Supervision",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "37118ec6f278a83e8d7fbb057d8ef840fc81f99f",
    "semantic_title": "creating causal embeddings for question answering with minimal supervision",
    "citation_count": 46,
    "authors": [
      "Rebecca Sharp",
      "Mihai Surdeanu",
      "Peter Jansen",
      "Peter Clark",
      "Michael Hammond"
    ]
  },
  "https://aclanthology.org/D16-1015": {
    "title": "Improving Semantic Parsing via Answer Type Inference",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f3594f9d60c98cac88f9033c69c2b666713ed6d6",
    "semantic_title": "improving semantic parsing via answer type inference",
    "citation_count": 49,
    "authors": [
      "Semih Yavuz",
      "Izzeddin Gur",
      "Yu Su",
      "Mudhakar Srivatsa",
      "Xifeng Yan"
    ]
  },
  "https://aclanthology.org/D16-1016": {
    "title": "Semantic Parsing to Probabilistic Programs for Situated Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9940e8b05c808a8dc5a688fa860bcf40ef3b59d3",
    "semantic_title": "semantic parsing to probabilistic programs for situated question answering",
    "citation_count": 20,
    "authors": [
      "Jayant Krishnamurthy",
      "Oyvind Tafjord",
      "Aniruddha Kembhavi"
    ]
  },
  "https://aclanthology.org/D16-1017": {
    "title": "Event participant modelling with neural networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d08d663d7795c76bb008f539b1ac7caf8a9ef26c",
    "semantic_title": "event participant modelling with neural networks",
    "citation_count": 26,
    "authors": [
      "Ottokar Tilk",
      "Vera Demberg",
      "Asad Sayeed",
      "Dietrich Klakow",
      "Stefan Thater"
    ]
  },
  "https://aclanthology.org/D16-1018": {
    "title": "Context-Dependent Sense Embedding",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "af439bd130ee3faab1cc475ff656c09b070907e8",
    "semantic_title": "context-dependent sense embedding",
    "citation_count": 20,
    "authors": [
      "Lin Qiu",
      "Kewei Tu",
      "Yong Yu"
    ]
  },
  "https://aclanthology.org/D16-1019": {
    "title": "Jointly Embedding Knowledge Graphs and Logical Rules",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2cefe5adb11295b830ce27176c6d84b66fb20c2c",
    "semantic_title": "jointly embedding knowledge graphs and logical rules",
    "citation_count": 209,
    "authors": [
      "Shu Guo",
      "Quan Wang",
      "Lihong Wang",
      "Bin Wang",
      "Li Guo"
    ]
  },
  "https://aclanthology.org/D16-1020": {
    "title": "Learning Connective-based Word Representations for Implicit Discourse Relation Identification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "78f4db90f4d115e3b69771903bfd9b15ba661a1d",
    "semantic_title": "learning connective-based word representations for implicit discourse relation identification",
    "citation_count": 29,
    "authors": [
      "Chloé Braud",
      "Pascal Denis"
    ]
  },
  "https://aclanthology.org/D16-1021": {
    "title": "Aspect Level Sentiment Classification with Deep Memory Network",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b28f7e2996b6ee2784dd2dbb8212cfa0c79ba9e7",
    "semantic_title": "aspect level sentiment classification with deep memory network",
    "citation_count": 777,
    "authors": [
      "Duyu Tang",
      "Bing Qin",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/D16-1022": {
    "title": "Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "46c3981eb659db34df5856a0108b4f3ff44048b7",
    "semantic_title": "lifelong-rl: lifelong relaxation labeling for separating entities and aspects in opinion targets",
    "citation_count": 30,
    "authors": [
      "Lei Shu",
      "Bing Liu",
      "Hu Xu",
      "Annice Kim"
    ]
  },
  "https://aclanthology.org/D16-1023": {
    "title": "Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2d38f7aab07d4435b2110602db4138ef20da4cc0",
    "semantic_title": "learning sentence embeddings with auxiliary tasks for cross-domain sentiment classification",
    "citation_count": 168,
    "authors": [
      "Jianfei Yu",
      "Jing Jiang"
    ]
  },
  "https://aclanthology.org/D16-1024": {
    "title": "Attention-based LSTM Network for Cross-Lingual Sentiment Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b31a60f21dae6adfc66e6c1c04bc74b57638b000",
    "semantic_title": "attention-based lstm network for cross-lingual sentiment classification",
    "citation_count": 234,
    "authors": [
      "Xinjie Zhou",
      "Xiaojun Wan",
      "Jianguo Xiao"
    ]
  },
  "https://aclanthology.org/D16-1025": {
    "title": "Neural versus Phrase-Based Machine Translation Quality: a Case Study",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d223fa07e86d6486375119b055d8fec77c21a325",
    "semantic_title": "neural versus phrase-based machine translation quality: a case study",
    "citation_count": 280,
    "authors": [
      "Luisa Bentivogli",
      "Arianna Bisazza",
      "Mauro Cettolo",
      "Marcello Federico"
    ]
  },
  "https://aclanthology.org/D16-1026": {
    "title": "Zero-Resource Translation with Multi-Lingual Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "198ac64703e83d00eb0f51a4c4a7c77cb08a7e5c",
    "semantic_title": "zero-resource translation with multi-lingual neural machine translation",
    "citation_count": 252,
    "authors": [
      "Orhan Firat",
      "Baskaran Sankaran",
      "Yaser Al-onaizan",
      "Fatos T. Yarman Vural",
      "Kyunghyun Cho"
    ]
  },
  "https://aclanthology.org/D16-1027": {
    "title": "Memory-enhanced Decoder for Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6bae362f72f01d0ab08a5e9625b2ae93daefd5b6",
    "semantic_title": "memory-enhanced decoder for neural machine translation",
    "citation_count": 66,
    "authors": [
      "Mingxuan Wang",
      "Zhengdong Lu",
      "Hang Li",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/D16-1028": {
    "title": "Semi-Supervised Learning of Sequence Models with Method of Moments",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4d17f62dec2897ef573bf50754bdcffc7f076ccb",
    "semantic_title": "semi-supervised learning of sequence models with method of moments",
    "citation_count": 4,
    "authors": [
      "Zita Marinho",
      "André F. T. Martins",
      "Shay B. Cohen",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/D16-1029": {
    "title": "Learning from Explicit and Implicit Supervision Jointly For Algebra Word Problems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7b6f61e6067d3fe520e01b19e2941aa122fcd564",
    "semantic_title": "learning from explicit and implicit supervision jointly for algebra word problems",
    "citation_count": 41,
    "authors": [
      "Shyam Upadhyay",
      "Ming-Wei Chang",
      "Kai-Wei Chang",
      "Wen-tau Yih"
    ]
  },
  "https://aclanthology.org/D16-1030": {
    "title": "TweeTime : A Minimally Supervised Method for Recognizing and Normalizing Time Expressions in Twitter",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0641bb2aa9820dc20befa1ff833331d702eb79e9",
    "semantic_title": "tweetime : a minimally supervised method for recognizing and normalizing time expressions in twitter",
    "citation_count": 17,
    "authors": [
      "Jeniya Tabassum",
      "Alan Ritter",
      "Wei Xu"
    ]
  },
  "https://aclanthology.org/D16-1031": {
    "title": "Language as a Latent Variable: Discrete Generative Models for Sentence Compression",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c78468c2f87efabf8845ccd210ced364d45e5eab",
    "semantic_title": "language as a latent variable: discrete generative models for sentence compression",
    "citation_count": 218,
    "authors": [
      "Yishu Miao",
      "Phil Blunsom"
    ]
  },
  "https://aclanthology.org/D16-1032": {
    "title": "Globally Coherent Text Generation with Neural Checklist Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3a0a3fbae91d98597d3d7bf5c33ff3eb818dc0a9",
    "semantic_title": "globally coherent text generation with neural checklist models",
    "citation_count": 208,
    "authors": [
      "Chloé Kiddon",
      "Luke Zettlemoyer",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/D16-1033": {
    "title": "A Dataset and Evaluation Metrics for Abstractive Compression of Sentences and Short Paragraphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": "0b6e670f9bb87b47c4e368733427b0d6db7c97e0",
    "semantic_title": "uva-dare (digital academic repository) a dataset and evaluation metrics for abstractive compression of sentences and short paragraphs",
    "citation_count": 48,
    "authors": [
      "Kristina Toutanova",
      "Chris Brockett",
      "Ke M. Tran",
      "Saleema Amershi"
    ]
  },
  "https://aclanthology.org/D16-1034": {
    "title": "PaCCSS-IT: A Parallel Corpus of Complex-Simple Sentences for Automatic Text Simplification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "db0c79ae2e668b41f4ac9e4d39b7c9f1dc4f911e",
    "semantic_title": "paccss-it: a parallel corpus of complex-simple sentences for automatic text simplification",
    "citation_count": 32,
    "authors": [
      "Dominique Brunato",
      "Andrea Cimino",
      "Felice Dell’Orletta",
      "Giulia Venturi"
    ]
  },
  "https://aclanthology.org/D16-1035": {
    "title": "Discourse Parsing with Attention-based Hierarchical Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6cf767e3afcea890e759d4f07fd1620b3f3684c7",
    "semantic_title": "discourse parsing with attention-based hierarchical neural networks",
    "citation_count": 94,
    "authors": [
      "Qi Li",
      "Tianshi Li",
      "Baobao Chang"
    ]
  },
  "https://aclanthology.org/D16-1036": {
    "title": "Multi-view Response Selection for Human-Computer Conversation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2e0bed618d023cad81eae218e69afce8bef8e4d6",
    "semantic_title": "multi-view response selection for human-computer conversation",
    "citation_count": 216,
    "authors": [
      "Xiangyang Zhou",
      "Daxiang Dong",
      "Hua Wu",
      "Shiqi Zhao",
      "Dianhai Yu",
      "Hao Tian",
      "Xuan Liu",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/D16-1037": {
    "title": "Variational Neural Discourse Relation Recognizer",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2891789ef2413f7979ae62fd3ce296b56059e7f1",
    "semantic_title": "variational neural discourse relation recognizer",
    "citation_count": 18,
    "authors": [
      "Biao Zhang",
      "Deyi Xiong",
      "Jinsong Su",
      "Qun Liu",
      "Rongrong Ji",
      "Hong Duan",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/D16-1038": {
    "title": "Event Detection and Co-reference with Minimal Supervision",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6df8cd4c69e75b286b1ba27417fd41a21d4982e1",
    "semantic_title": "event detection and co-reference with minimal supervision",
    "citation_count": 102,
    "authors": [
      "Haoruo Peng",
      "Yangqiu Song",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/D16-1039": {
    "title": "Learning Term Embeddings for Taxonomic Relation Identification Using Dynamic Weighting Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a27e243d2ef62644e7a2a1fa51878fe7dbca4479",
    "semantic_title": "learning term embeddings for taxonomic relation identification using dynamic weighting neural network",
    "citation_count": 68,
    "authors": [
      "Anh Tuan Luu",
      "Yi Tay",
      "Siu Cheung Hui",
      "See Kiong Ng"
    ]
  },
  "https://aclanthology.org/D16-1040": {
    "title": "Relation Schema Induction using Tensor Factorization with Side Information",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c1c39ebad25d1bcde6587138bd2ffa9b3a250781",
    "semantic_title": "relation schema induction using tensor factorization with side information",
    "citation_count": 24,
    "authors": [
      "Madhav Nimishakavi",
      "Uday Singh Saini",
      "Partha Talukdar"
    ]
  },
  "https://aclanthology.org/D16-1041": {
    "title": "Supervised Distributional Hypernym Discovery via Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "157c7cc5f51c69bfad84b68d43e44470418c3a5c",
    "semantic_title": "supervised distributional hypernym discovery via domain adaptation",
    "citation_count": 55,
    "authors": [
      "Luis Espinosa-Anke",
      "Jose Camacho-Collados",
      "Claudio Delli Bovi",
      "Horacio Saggion"
    ]
  },
  "https://aclanthology.org/D16-1042": {
    "title": "Latent Tree Language Model",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ca9d8946f05448852df1dc7e3e68d76c9f8b6654",
    "semantic_title": "latent tree language model",
    "citation_count": 1,
    "authors": [
      "Tomáš Brychcín"
    ]
  },
  "https://aclanthology.org/D16-1043": {
    "title": "Comparing Data Sources and Architectures for Deep Visual Representation Learning in Semantics",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9e9be88c361df56ae1221f5c4ebbb4ae2cbe0a27",
    "semantic_title": "comparing data sources and architectures for deep visual representation learning in semantics",
    "citation_count": 24,
    "authors": [
      "Douwe Kiela",
      "Anita Lilla Verő",
      "Stephen Clark"
    ]
  },
  "https://aclanthology.org/D16-1044": {
    "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "12f7de07f9b00315418e381b2bd797d21f12b419",
    "semantic_title": "multimodal compact bilinear pooling for visual question answering and visual grounding",
    "citation_count": 1323,
    "authors": [
      "Akira Fukui",
      "Dong Huk Park",
      "Daylen Yang",
      "Anna Rohrbach",
      "Trevor Darrell",
      "Marcus Rohrbach"
    ]
  },
  "https://aclanthology.org/D16-1045": {
    "title": "The Structured Weighted Violations Perceptron Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6b280fc096e6af4295c2e9b8bb81cb4c2d0c2ac1",
    "semantic_title": "the structured weighted violations perceptron algorithm",
    "citation_count": 1,
    "authors": [
      "Rotem Dror",
      "Roi Reichart"
    ]
  },
  "https://aclanthology.org/D16-1046": {
    "title": "How Transferable are Neural Networks in NLP Applications?",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f21b36a5cfba94bbc6ed5c3642e1e46057deccaf",
    "semantic_title": "how transferable are neural networks in nlp applications?",
    "citation_count": 280,
    "authors": [
      "Lili Mou",
      "Zhao Meng",
      "Rui Yan",
      "Ge Li",
      "Yan Xu",
      "Lu Zhang",
      "Zhi Jin"
    ]
  },
  "https://aclanthology.org/D16-1047": {
    "title": "Morphological Priors for Probabilistic Neural Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f690c45d7cb80ce46a438c4dd1f9b50e1a45a84e",
    "semantic_title": "morphological priors for probabilistic neural word embeddings",
    "citation_count": 41,
    "authors": [
      "Parminder Bhatia",
      "Robert Guthrie",
      "Jacob Eisenstein"
    ]
  },
  "https://aclanthology.org/D16-1048": {
    "title": "Automatic Cross-Lingual Similarization of Dependency Grammars for Tree-based Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "961d57fc4bf51f73d8fa6bb30a7d5566255c1f82",
    "semantic_title": "automatic cross-lingual similarization of dependency grammars for tree-based machine translation",
    "citation_count": 1,
    "authors": [
      "Wenbin Jiang",
      "Wen Zhang",
      "Jinan Xu",
      "Rangjia Cai"
    ]
  },
  "https://aclanthology.org/D16-1049": {
    "title": "IRT-based Aggregation Model of Crowdsourced Pairwise Comparison for Evaluating Machine Translations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "162445849d0d14a598d663c7bcdc006974268a53",
    "semantic_title": "irt-based aggregation model of crowdsourced pairwise comparison for evaluating machine translations",
    "citation_count": 15,
    "authors": [
      "Naoki Otani",
      "Toshiaki Nakazawa",
      "Daisuke Kawahara",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/D16-1050": {
    "title": "Variational Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ceb154e8f8ac411914f3327d67257776db3aa413",
    "semantic_title": "variational neural machine translation",
    "citation_count": 188,
    "authors": [
      "Biao Zhang",
      "Deyi Xiong",
      "Jinsong Su",
      "Hong Duan",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/D16-1051": {
    "title": "Towards a Convex HMM Surrogate for Word Alignment",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3b35dfbad2d96265845848a0d24572ca163cda82",
    "semantic_title": "towards a convex hmm surrogate for word alignment",
    "citation_count": 0,
    "authors": [
      "Andrei Simion",
      "Michael Collins",
      "Cliff Stein"
    ]
  },
  "https://aclanthology.org/D16-1052": {
    "title": "Solving Verbal Questions in IQ Test by Knowledge-Powered Word Embedding",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7fffd8c7eebe2365b6ac483ef827ddb30b399c5e",
    "semantic_title": "solving verbal questions in iq test by knowledge-powered word embedding",
    "citation_count": 11,
    "authors": [
      "Huazheng Wang",
      "Fei Tian",
      "Bin Gao",
      "Chengjieren Zhu",
      "Jiang Bian",
      "Tie-Yan Liu"
    ]
  },
  "https://aclanthology.org/D16-1053": {
    "title": "Long Short-Term Memory-Networks for Machine Reading",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "13fe71da009484f240c46f14d9330e932f8de210",
    "semantic_title": "long short-term memory-networks for machine reading",
    "citation_count": 957,
    "authors": [
      "Jianpeng Cheng",
      "Li Dong",
      "Mirella Lapata"
    ]
  },
  "https://aclanthology.org/D16-1054": {
    "title": "On Generating Characteristic-rich Question Sets for QA Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ce23476759263bd3f5e95fc758385eb62b3ab59a",
    "semantic_title": "on generating characteristic-rich question sets for qa evaluation",
    "citation_count": 94,
    "authors": [
      "Yu Su",
      "Huan Sun",
      "Brian Sadler",
      "Mudhakar Srivatsa",
      "Izzeddin Gür",
      "Zenghui Yan",
      "Xifeng Yan"
    ]
  },
  "https://aclanthology.org/D16-1055": {
    "title": "Learning to Translate for Multilingual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "35e406eb52d8097346df41537da354abdd96f775",
    "semantic_title": "learning to translate for multilingual question answering",
    "citation_count": 20,
    "authors": [
      "Ferhan Ture",
      "Elizabeth Boschee"
    ]
  },
  "https://aclanthology.org/D16-1056": {
    "title": "A Semiparametric Model for Bayesian Reader Identification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cc44780f2a12d39736deb055fb3ea2515cae62d8",
    "semantic_title": "a semiparametric model for bayesian reader identification",
    "citation_count": 8,
    "authors": [
      "Ahmed Abdelwahab",
      "Reinhold Kliegl",
      "Niels Landwehr"
    ]
  },
  "https://aclanthology.org/D16-1057": {
    "title": "Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c6fe37beac50446012a87aac9fc2f8c4f34891e1",
    "semantic_title": "inducing domain-specific sentiment lexicons from unlabeled corpora",
    "citation_count": 308,
    "authors": [
      "William L. Hamilton",
      "Kevin Clark",
      "Jure Leskovec",
      "Dan Jurafsky"
    ]
  },
  "https://aclanthology.org/D16-1058": {
    "title": "Attention-based LSTM for Aspect-level Sentiment Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "82bb306038446302cedd20fa986d20640ed88a2e",
    "semantic_title": "attention-based lstm for aspect-level sentiment classification",
    "citation_count": 1645,
    "authors": [
      "Yequan Wang",
      "Minlie Huang",
      "Xiaoyan Zhu",
      "Li Zhao"
    ]
  },
  "https://aclanthology.org/D16-1059": {
    "title": "Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1c72d8090f281f58d5b141a3917e341250ab4c8c",
    "semantic_title": "recursive neural conditional random fields for aspect-based sentiment analysis",
    "citation_count": 333,
    "authors": [
      "Wenya Wang",
      "Sinno Jialin Pan",
      "Daniel Dahlmeier",
      "Xiaokui Xiao"
    ]
  },
  "https://aclanthology.org/D16-1060": {
    "title": "Extracting Aspect Specific Opinion Expressions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "81400418aee6c057019d1902c5e06436428fb29e",
    "semantic_title": "extracting aspect specific opinion expressions",
    "citation_count": 9,
    "authors": [
      "Abhishek Laddha",
      "Arjun Mukherjee"
    ]
  },
  "https://aclanthology.org/D16-1061": {
    "title": "Emotion Distribution Learning from Texts",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ac9eca5ee4485cebb1d634d00f0a42a2510bb8a3",
    "semantic_title": "emotion distribution learning from texts",
    "citation_count": 102,
    "authors": [
      "Deyu Zhou",
      "Xuan Zhang",
      "Yin Zhou",
      "Quan Zhao",
      "Xin Geng"
    ]
  },
  "https://aclanthology.org/D16-1062": {
    "title": "Building an Evaluation Scale using Item Response Theory",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "739bf9a7451712bca3094e626632dce0ae715224",
    "semantic_title": "building an evaluation scale using item response theory",
    "citation_count": 53,
    "authors": [
      "John P. Lalor",
      "Hao Wu",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/D16-1063": {
    "title": "WordRank: Learning Word Embeddings via Robust Ranking",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6bf23e53cbbca9139dd8982f0bfa6072c2029dc0",
    "semantic_title": "wordrank: learning word embeddings via robust ranking",
    "citation_count": 37,
    "authors": [
      "Shihao Ji",
      "Hyokun Yun",
      "Pinar Yanardag",
      "Shin Matsushima",
      "S. V. N. Vishwanathan"
    ]
  },
  "https://aclanthology.org/D16-1064": {
    "title": "Exploring Semantic Representation in Brain Activity Using Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "dc6e0b2096911eb14d4e3e742996ee4a2905bd57",
    "semantic_title": "exploring semantic representation in brain activity using word embeddings",
    "citation_count": 17,
    "authors": [
      "Yu-Ping Ruan",
      "Zhen-Hua Ling",
      "Yu Hu"
    ]
  },
  "https://aclanthology.org/D16-1065": {
    "title": "AMR Parsing with an Incremental Joint Model",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f6bab2ae3bb22d6b91f76d819ac514eb638213e2",
    "semantic_title": "amr parsing with an incremental joint model",
    "citation_count": 47,
    "authors": [
      "Junsheng Zhou",
      "Feiyu Xu",
      "Hans Uszkoreit",
      "Weiguang Qu",
      "Ran Li",
      "Yanhui Gu"
    ]
  },
  "https://aclanthology.org/D16-1066": {
    "title": "Identifying Dogmatism in Social Media: Signals and Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b15d3de8789657542c1045cc2ac90b1b0670e017",
    "semantic_title": "identifying dogmatism in social media: signals and models",
    "citation_count": 13,
    "authors": [
      "Ethan Fast",
      "Eric Horvitz"
    ]
  },
  "https://aclanthology.org/D16-1067": {
    "title": "Enhanced Personalized Search using Social Data",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a97c17a0d2c1dc057810fc2892d436db0b74c135",
    "semantic_title": "enhanced personalized search using social data",
    "citation_count": 6,
    "authors": [
      "Dong Zhou",
      "Séamus Lawless",
      "Xuan Wu",
      "Wenyu Zhao",
      "Jianxun Liu"
    ]
  },
  "https://aclanthology.org/D16-1068": {
    "title": "Effective Greedy Inference for Graph-based Non-Projective Dependency Parsing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b35cdc4e82ed8c7a44147c19c528cbe3f87bf031",
    "semantic_title": "effective greedy inference for graph-based non-projective dependency parsing",
    "citation_count": 2,
    "authors": [
      "Ilan Tchernowitz",
      "Liron Yedidsion",
      "Roi Reichart"
    ]
  },
  "https://aclanthology.org/D16-1069": {
    "title": "Generating Abbreviations for Chinese Named Entities Using Recurrent Neural Network with Dynamic Dictionary",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d0c4328d372e210d7f2536633c5ba3af1274ba78",
    "semantic_title": "generating abbreviations for chinese named entities using recurrent neural network with dynamic dictionary",
    "citation_count": 4,
    "authors": [
      "Qi Zhang",
      "Jin Qian",
      "Ya Guo",
      "Yaqian Zhou",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/D16-1070": {
    "title": "Neural Network for Heterogeneous Annotations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3669cded8d2158cf47cb519ae1d2a5facb122ea6",
    "semantic_title": "neural network for heterogeneous annotations",
    "citation_count": 32,
    "authors": [
      "Hongshen Chen",
      "Yue Zhang",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/D16-1071": {
    "title": "LAMB: A Good Shepherd of Morphologically Rich Languages",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1a26e105c1d0e7fb3eda59f0e1c072a910ec7264",
    "semantic_title": "lamb: a good shepherd of morphologically rich languages",
    "citation_count": 8,
    "authors": [
      "Sebastian Ebert",
      "Thomas Müller",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/D16-1072": {
    "title": "Fast Coupled Sequence Labeling on Heterogeneous Annotations via Context-aware Pruning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2f1bede073ff7c8136e66df22d3feaabef5747ac",
    "semantic_title": "fast coupled sequence labeling on heterogeneous annotations via context-aware pruning",
    "citation_count": 7,
    "authors": [
      "Zhenghua Li",
      "Jiayuan Chao",
      "Min Zhang",
      "Jiwen Yang"
    ]
  },
  "https://aclanthology.org/D16-1073": {
    "title": "Unsupervised Neural Dependency Parsing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b360859eb746963767e554ae32cee1d1f3bcbc22",
    "semantic_title": "unsupervised neural dependency parsing",
    "citation_count": 56,
    "authors": [
      "Yong Jiang",
      "Wenjuan Han",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/D16-1074": {
    "title": "Generating Coherent Summaries of Scientific Articles Using Coherence Patterns",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ded417776c70a885e8c06442b99b9dd49dbff2a8",
    "semantic_title": "generating coherent summaries of scientific articles using coherence patterns",
    "citation_count": 50,
    "authors": [
      "Daraksha Parveen",
      "Mohsen Mesgar",
      "Michael Strube"
    ]
  },
  "https://aclanthology.org/D16-1075": {
    "title": "News Stream Summarization using Burst Information Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "574c8f5ed8c8b2779ce3eeb4a6d31f218a9073cd",
    "semantic_title": "news stream summarization using burst information networks",
    "citation_count": 11,
    "authors": [
      "Tao Ge",
      "Lei Cui",
      "Baobao Chang",
      "Sujian Li",
      "Ming Zhou",
      "Zhifang Sui"
    ]
  },
  "https://aclanthology.org/D16-1076": {
    "title": "Rationale-Augmented Convolutional Neural Networks for Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "db0751478590cf6d9b61045bbd6f8ff4a9cc2294",
    "semantic_title": "rationale-augmented convolutional neural networks for text classification",
    "citation_count": 144,
    "authors": [
      "Ye Zhang",
      "Iain Marshall",
      "Byron C. Wallace"
    ]
  },
  "https://aclanthology.org/D16-1077": {
    "title": "Transferring User Interests Across Websites with Unstructured Text for Cold-Start Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d9669a0aeb9bd118dc03bdf94f1333a11e34c49d",
    "semantic_title": "transferring user interests across websites with unstructured text for cold-start recommendation",
    "citation_count": 5,
    "authors": [
      "Yu-Yang Huang",
      "Shou-De Lin"
    ]
  },
  "https://aclanthology.org/D16-1078": {
    "title": "Speculation and Negation Scope Detection via Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4f2ae80aae386d7cf32be372a411efceea11d2a3",
    "semantic_title": "speculation and negation scope detection via convolutional neural networks",
    "citation_count": 62,
    "authors": [
      "Zhong Qian",
      "Peifeng Li",
      "Qiaoming Zhu",
      "Guodong Zhou",
      "Zhunchen Luo",
      "Wei Luo"
    ]
  },
  "https://aclanthology.org/D16-1079": {
    "title": "Analyzing Linguistic Knowledge in Sequential Model of Sentence",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f14a86307b62232aa2cf68177c24bd71d010615f",
    "semantic_title": "analyzing linguistic knowledge in sequential model of sentence",
    "citation_count": 35,
    "authors": [
      "Peng Qian",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/D16-1080": {
    "title": "Keyphrase Extraction Using Deep Recurrent Neural Networks on Twitter",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2bde9b4149d416614b2866343bb9102a63bd6ae9",
    "semantic_title": "keyphrase extraction using deep recurrent neural networks on twitter",
    "citation_count": 173,
    "authors": [
      "Qi Zhang",
      "Yang Wang",
      "Yeyun Gong",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/D16-1081": {
    "title": "Solving and Generating Chinese Character Riddles",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c8681518bedf9a5cebcf896629fb0be1bd7bb3d0",
    "semantic_title": "solving and generating chinese character riddles",
    "citation_count": 5,
    "authors": [
      "Chuanqi Tan",
      "Furu Wei",
      "Li Dong",
      "Weifeng Lv",
      "Ming Zhou"
    ]
  },
  "https://aclanthology.org/D16-1082": {
    "title": "Structured prediction models for RNN based sequence labeling in clinical text",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4acf3db41f1c34cf8d568ecd198820f9993da03c",
    "semantic_title": "structured prediction models for rnn based sequence labeling in clinical text",
    "citation_count": 187,
    "authors": [
      "Abhyuday Jagannatha",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/D16-1083": {
    "title": "Learning to Represent Review with Tensor Decomposition for Spam Detection",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9abc08a7335700f803ec0ed0fe4f7c754a3b344d",
    "semantic_title": "learning to represent review with tensor decomposition for spam detection",
    "citation_count": 44,
    "authors": [
      "Xuepeng Wang",
      "Kang Liu",
      "Shizhu He",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/D16-1084": {
    "title": "Stance Detection with Bidirectional Conditional Encoding",
    "volume": "main",
    "abstract": "Stance detection is the task of classifying the attitude expressed in a text towards a target such as Hillary Clinton to be \"positive\", negative\" or \"neutral\". Previous work has assumed that either the target is mentioned in the text or that training data for every target is given. This paper considers the more challenging version of this task, where targets are not always mentioned and no training data is available for the test targets. We experiment with conditional LSTM encoding, which builds a representation of the tweet that is dependent on the target, and demonstrate that it outperforms encoding the tweet and the target independently. Performance is improved further when the conditional model is augmented with bidirectional encoding. We evaluate our approach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving performance second best only to a system trained on semi-automatically labelled tweets for the test target. When such weak supervision is added, our approach achieves state-of-the-art results",
    "checked": true,
    "id": "8e7d063c681557c94382ff3da6415d3720fe11a7",
    "semantic_title": "",
    "citation_count": 314,
    "authors": [
      "Isabelle Augenstein",
      "Tim Rocktäschel",
      "Andreas Vlachos",
      "Kalina Bontcheva"
    ]
  },
  "https://aclanthology.org/D16-1085": {
    "title": "Modeling Skip-Grams for Event Detection with Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Convolutional neural networks (CNN) have achieved the top performance for event detection due to their capacity to induce the underlying structures of the k-grams in the sentences. However, the current CNN-based event detectors only model the consecutive k-grams and ignore the non-consecutive kgrams that might involve important structures for event detection. In this work, we propose to improve the current CNN models for ED by introducing the non-consecutive convolution. Our systematic evaluation on both the general setting and the domain adaptation setting demonstrates the effectiveness of the nonconsecutive CNN model, leading to the significant performance improvement over the current state-of-the-art systems",
    "checked": true,
    "id": "fc5b90ec6a64145594e2fa4b2543b0d9a10906ff",
    "semantic_title": "",
    "citation_count": 103,
    "authors": [
      "Thien Huu Nguyen",
      "Ralph Grishman"
    ]
  },
  "https://aclanthology.org/D16-1086": {
    "title": "Porting an Open Information Extraction System from English to German",
    "volume": "main",
    "abstract": "Many downstream NLP tasks can benefit from Open Information Extraction (Open IE) as a semantic representation. While Open IE systems are available for English, many other languages lack such tools. In this paper, we present a straightforward approach for adapting PropS, a rule-based predicate-argument analysis for English, to a new language, German. With this approach, we quickly obtain an Open IE system for German covering 89% of the English rule set. It yields 1.6 extractions per sentence with 60% precision, making it readily usable in downstream applications",
    "checked": true,
    "id": "c9c27bb00f4135ac6db128e3478d705fbc16598e",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Tobias Falke",
      "Gabriel Stanovsky",
      "Iryna Gurevych",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/D16-1087": {
    "title": "Named Entity Recognition for Novel Types by Transfer Learning",
    "volume": "main",
    "abstract": "In named entity recognition, we often don't have a large in-domain training corpus or a knowledge base with adequate coverage to train a model directly. In this paper, we propose a method where, given training data in a related domain with similar (but not identical) named entity (NE) types and a small amount of in-domain training data, we use transfer learning to learn a domain-specific NE model. That is, the novelty in the task setup is that we assume not just domain mismatch, but also label mismatch",
    "checked": true,
    "id": "e3b31ab11bbcc8c9003604292471529a34804400",
    "semantic_title": "",
    "citation_count": 35,
    "authors": [
      "Lizhen Qu",
      "Gabriela Ferraro",
      "Liyuan Zhou",
      "Weiwei Hou",
      "Timothy Baldwin"
    ]
  },
  "https://aclanthology.org/D16-1088": {
    "title": "Extracting Subevents via an Effective Two-phase Approach",
    "volume": "main",
    "abstract": "We present our pilot research on automatically extracting subevents from a domain-specific corpus, focusing on the type of subevents that describe physical actions composing an event. We decompose the challenging problem and propose a two-phase approach that effectively captures sentential and local cues that describe subevents. We extracted a rich set of over 600 novel subevent phrases. Evaluation shows the automatically learned subevents help to discover 10% additional main events (of which the learned subevents are a part) and improve event detection performance",
    "checked": true,
    "id": "659f12c55f9db868946c0117e81b113266d7a8e9",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Allison Badgett",
      "Ruihong Huang"
    ]
  },
  "https://aclanthology.org/D16-1089": {
    "title": "Gaussian Visual-Linguistic Embedding for Zero-Shot Recognition",
    "volume": "main",
    "abstract": "An exciting outcome of research at the inter-section of language and vision is that of zero-shot learning (ZSL). ZSL promises to scale visual recognition by borrowing distributed semantic models learned from linguistic cor-pora and turning them into visual recognition models. However the popular word-vector DSM embeddings are relatively impoverished in their expressivity as they model each word as a single vector point. In this paper we explore word- distribution embeddings for ZSL. We present a visual-linguistic mapping for ZSL in the case where words and visual categories are both represented by distributions. Experiments show improved results on ZSL benchmarks due to this better exploiting of intra-concept variability in each modality",
    "checked": true,
    "id": "492596eb82218085485ad77e7b414250f5289605",
    "semantic_title": "",
    "citation_count": 38,
    "authors": [
      "Tanmoy Mukherjee",
      "Timothy Hospedales"
    ]
  },
  "https://aclanthology.org/D16-1090": {
    "title": "Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions",
    "volume": "main",
    "abstract": "Visual Question Answering (VQA) is the task of answering natural-language questions about images. We introduce the novel problem of determining the relevance of questions to images in VQA. Current VQA models do not reason about whether a question is even related to the given image (e.g. What is the capital of Argentina?) or if it requires information from external resources to answer correctly. This can break the continuity of a dialogue in human-machine interaction. Our approaches for determining relevance are composed of two stages. Given an image and a question, (1) we first determine whether the question is visual or not, (2) if visual, we determine whether the question is relevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA model uncertainty, and caption-question similarity, are able to outperform strong baselines on both relevance tasks. We also present human studies showing that VQA models augmented with such question relevance reasoning are perceived as more intelligent, reasonable, and human-like",
    "checked": true,
    "id": "0eb859d4184476bd80d5f2090b3401c702f66135",
    "semantic_title": "",
    "citation_count": 50,
    "authors": [
      "Arijit Ray",
      "Gordon Christie",
      "Mohit Bansal",
      "Dhruv Batra",
      "Devi Parikh"
    ]
  },
  "https://aclanthology.org/D16-1091": {
    "title": "Sort Story: Sorting Jumbled Images and Captions into Stories",
    "volume": "main",
    "abstract": "Temporal common sense has applications in AI tasks such as QA, multi-document summarization, and human-AI communication. We propose the task of sequencing -- given a jumbled set of aligned image-caption pairs that belong to a story, the task is to sort them such that the output sequence forms a coherent story. We present multiple approaches, via unary (position) and pairwise (order) predictions, and their ensemble-based combinations, achieving strong results on this task. We use both text-based and image-based features, which depict complementary improvements. Using qualitative examples, we demonstrate that our models have learnt interesting aspects of temporal common sense",
    "checked": true,
    "id": "3cc0d9c1f690addd2c82e60f2a460e3c557ff242",
    "semantic_title": "",
    "citation_count": 60,
    "authors": [
      "Harsh Agrawal",
      "Arjun Chandrasekaran",
      "Dhruv Batra",
      "Devi Parikh",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/D16-1092": {
    "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks look at the same regions?",
    "volume": "main",
    "abstract": "We conduct large-scale studies on `human attention' in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Overall, our experiments show that current attention models in VQA do not seem to be looking at the same regions as humans",
    "checked": true,
    "id": "58cb0c24c936b8a14ca7b2d56ba80de733c545b3",
    "semantic_title": "",
    "citation_count": 393,
    "authors": [
      "Abhishek Das",
      "Harsh Agrawal",
      "Larry Zitnick",
      "Devi Parikh",
      "Dhruv Batra"
    ]
  },
  "https://aclanthology.org/D16-1093": {
    "title": "Recurrent Residual Learning for Sequence Classification",
    "volume": "main",
    "abstract": "In this paper, we explore the possibility of leveraging Residual Networks (ResNet), a powerful structure in constructing extremely deep neural network for image understanding, to improve recurrent neural networks (RNN) for modeling sequential data. We show that for sequence classification tasks, incorporating residual connections into recurrent structures yields similar accuracy to Long Short Term Memory (LSTM) RNN with much fewer model parameters. In addition, we propose two novel models which combine the best of both residual learning and LSTM. Experiments show that the new models significantly outperform LSTM",
    "checked": true,
    "id": "b1f3198f2b49f8028cd1cdd2d685c144cd18cb9d",
    "semantic_title": "",
    "citation_count": 94,
    "authors": [
      "Yiren Wang",
      "Fei Tian"
    ]
  },
  "https://aclanthology.org/D16-1094": {
    "title": "Richer Interpolative Smoothing Based on Modified Kneser-Ney Language Modeling",
    "volume": "main",
    "abstract": "In this work we present a generalisation of the Modified Kneser-Ney interpolative smoothing for richer smoothing via additional discount parameters. We provide mathematical underpinning for the estimator of the new discount parameters, and showcase the utility of our rich MKN language models on several European languages. We further explore the interdependency among the training data size, language model order, and number of discount parameters. Our empirical results illustrate that larger number of discount parameters, i) allows for better allocation of mass in the smoothing process, particularly on small data regime where statistical sparsity is severe, and ii) leads to significant reduction in perplexity, particularly for out-of-domain test sets which introduce higher ratio of out-ofvocabulary words.1",
    "checked": true,
    "id": "14fcb7bf8c8b428bc95f2bc1460d65026fba56f9",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Ehsan Shareghi",
      "Trevor Cohn",
      "Gholamreza Haffari"
    ]
  },
  "https://aclanthology.org/D16-1095": {
    "title": "A General Regularization Framework for Domain Adaptation",
    "volume": "main",
    "abstract": "We propose a domain adaptation framework, and formally prove that it generalizes the feature augmentation technique in (Daumé III, 2007) and the multi-task regularization framework in (Evgeniou and Pontil, 2004). We show that our framework is strictly more general than these approaches and allows practitioners to tune hyper-parameters to encourage transfer between close domains and avoid negative transfer between distant ones",
    "checked": true,
    "id": "edd8587921d63c797cd0f2a7acca82852daba8d6",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Wei Lu",
      "Hai Leong Chieu",
      "Jonathan Löfgren"
    ]
  },
  "https://aclanthology.org/D16-1096": {
    "title": "Coverage Embedding Models for Neural Machine Translation",
    "volume": "main",
    "abstract": "In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system",
    "checked": true,
    "id": "f997c2f1f668b942c4cccd425bc192df651ed516",
    "semantic_title": "",
    "citation_count": 137,
    "authors": [
      "Haitao Mi",
      "Baskaran Sankaran",
      "Zhiguo Wang",
      "Abe Ittycheriah"
    ]
  },
  "https://aclanthology.org/D16-1097": {
    "title": "Neural Morphological Analysis: Encoding-Decoding Canonical Segments",
    "volume": "main",
    "abstract": "Canonical morphological segmentation aims to divide words into a sequence of standardized segments. In this work, we propose a character-based neural encoderdecoder model for this task. Additionally, we extend our model to include morphemelevel and lexical information through a neural reranker. We set the new state of the art for the task improving previous results by up to 21% accuracy. Our experiments cover three languages: English, German and Indonesian",
    "checked": true,
    "id": "c75ecddc5eda6b71b54bf8ef40f6fa9da1d8f108",
    "semantic_title": "",
    "citation_count": 40,
    "authors": [
      "Katharina Kann",
      "Ryan Cotterell",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/D16-1098": {
    "title": "Exploiting Mutual Benefits between Syntax and Semantic Roles using Neural Network",
    "volume": "main",
    "abstract": "We investigate mutual benefits between syntax and semantic roles using neural network models, by studying a parsing→SRL pipeline, a SRL→parsing pipeline, and a simple joint model by embedding sharing. The integration of syntactic and semantic features gives promising results in a Chinese Semantic Treebank, demonstrating large potentials of neural models for joint parsing and semantic role labeling",
    "checked": true,
    "id": "18fa452beeb2826f989f053758d007d1e3a78012",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Peng Shi",
      "Zhiyang Teng",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/D16-1099": {
    "title": "The Effects of Data Size and Frequency Range on Distributional Semantic Models",
    "volume": "main",
    "abstract": "This paper investigates the effects of data size and frequency range on distributional semantic models. We compare the performance of a number of representative models for several test settings over data of varying sizes, and over test items of various frequency. Our results show that neural network-based models underperform when the data is small, and that the most reliable model over data of varying sizes and frequency ranges is the inverted factorized model",
    "checked": true,
    "id": "4b63af049b14cb57e0e82a6d214e73036de72a85",
    "semantic_title": "",
    "citation_count": 53,
    "authors": [
      "Magnus Sahlgren",
      "Alessandro Lenci"
    ]
  },
  "https://aclanthology.org/D16-1100": {
    "title": "Multi-Granularity Chinese Word Embedding",
    "volume": "main",
    "abstract": "This paper considers the problem of learning Chinese word embeddings. In contrast to English, a Chinese word is usually composed of characters, and most of the characters themselves can be further divided into components such as radicals. While characters and radicals contain rich information and are capable of indicating semantic meanings of words, they have not been fully exploited by existing word embedding methods. In this work, we propose multi-granularity embedding (MGE) for Chinese words. The key idea is to make full use of such word-character-radical composition, and enrich word embeddings by further incorporating finer-grained semantics from characters and radicals. Quantitative evaluation demonstrates the superiority of MGE in word similarity computation and analogical reasoning. Qualitative analysis further shows its capability to identify finer-grained semantic meanings of words",
    "checked": true,
    "id": "a5134affedeeac45a1123167e0ecbf52aa96cc1e",
    "semantic_title": "",
    "citation_count": 86,
    "authors": [
      "Rongchao Yin",
      "Quan Wang",
      "Peng Li",
      "Rui Li",
      "Bin Wang"
    ]
  },
  "https://aclanthology.org/D16-1101": {
    "title": "Numerically Grounded Language Models for Semantic Error Correction",
    "volume": "main",
    "abstract": "Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction. Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities. Our approach uses language models grounded in numbers within the text. Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases. Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches. Conditioning on a knowledge base yields further improvements",
    "checked": true,
    "id": "7e1eea0235e1902b6e3219fd9de9105e2f08fd65",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Georgios Spithourakis",
      "Isabelle Augenstein",
      "Sebastian Riedel"
    ]
  },
  "https://aclanthology.org/D16-1102": {
    "title": "Towards Semi-Automatic Generation of Proposition Banks for Low-Resource Languages",
    "volume": "main",
    "abstract": "Annotation projection based on parallel corpora has shown great promise in inexpensively creating Proposition Banks for languages for which high-quality parallel corpora and syntactic parsers are available. In this paper, we present an experimental study where we apply this approach to three languages that lack such resources: Tamil, Bengali and Malayalam. We find an average quality difference of 6 to 20 absolute F-measure points vis-avis high-resource languages, which indicates that annotation projection alone is insufficient in low-resource scenarios. Based on these results, we explore the possibility of using annotation projection as a starting point for inexpensive data curation involving both experts and non-experts. We give an outline of what such a process may look like and present an initial study to discuss its potential and challenges",
    "checked": true,
    "id": "cbe85e3e78c4292bf771774f8159b0145e8cdc7b",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Alan Akbik",
      "Vishwajeet Kumar",
      "Yunyao Li"
    ]
  },
  "https://aclanthology.org/D16-1103": {
    "title": "A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis",
    "volume": "main",
    "abstract": "Opinion mining from customer reviews has become pervasive in recent years. Sentences in reviews, however, are usually classified independently, even though they form part of a review's argumentative structure. Intuitively, sentences in a review build and elaborate upon each other; knowledge of the review structure and sentential context should thus inform the classification of each sentence. We demonstrate this hypothesis for the task of aspect-based sentiment analysis by modeling the interdependencies of sentences in a review with a hierarchical bidirectional LSTM. We show that the hierarchical model outperforms two non-hierarchical baselines, obtains results competitive with the state-of-the-art, and outperforms the state-of-the-art on five multilingual, multi-domain datasets without any hand-engineered features or external resources",
    "checked": true,
    "id": "def0ec30fab1594a2686677868e7fed4c11fb7f4",
    "semantic_title": "",
    "citation_count": 227,
    "authors": [
      "Sebastian Ruder",
      "Parsa Ghaffari",
      "John G. Breslin"
    ]
  },
  "https://aclanthology.org/D16-1104": {
    "title": "Are Word Embedding-based Features Useful for Sarcasm Detection?",
    "volume": "main",
    "abstract": "This paper makes a simple increment to state-of-the-art in sarcasm detection research. Existing approaches are unable to capture subtle forms of context incongruity which lies at the heart of sarcasm. We explore if prior work can be enhanced using semantic similarity/discordance between word embeddings. We augment word embedding-based features to four feature sets reported in the past. We also experiment with four types of word embeddings. We observe an improvement in sarcasm detection, irrespective of the word embedding used or the original feature set to which our features are augmented. For example, this augmentation results in an improvement in F-score of around 4\\% for three out of these four feature sets, and a minor degradation in case of the fourth, when Word2Vec embeddings are used. Finally, a comparison of the four embeddings shows that Word2Vec and dependency weight-based features outperform LSA and GloVe, in terms of their benefit to sarcasm detection",
    "checked": true,
    "id": "8c8a7513cfd1e66144a1771bf4e29b7388c57bb3",
    "semantic_title": "",
    "citation_count": 141,
    "authors": [
      "Aditya Joshi",
      "Vaibhav Tripathi",
      "Kevin Patel",
      "Pushpak Bhattacharyya",
      "Mark Carman"
    ]
  },
  "https://aclanthology.org/D16-1105": {
    "title": "Weakly Supervised Tweet Stance Classification by Relational Bootstrapping",
    "volume": "main",
    "abstract": "Supervised stance classification, in such domains as Congressional debates and online forums, has been a topic of interest in the past decade. Approaches have evolved from text classification to structured output prediction, including collective classification and sequence labeling. In this work, we investigate collective classification of stances on Twitter, using hinge-loss Markov random fields (HLMRFs). Given the graph of all posts, users, and their relationships, we constrain the predicted post labels and latent user labels to correspond with the network structure. We focus on a weakly supervised setting, in which only a small set of hashtags or phrases is labeled. Using our relational approach, we are able to go beyond the stance-indicative patterns and harvest more stance-indicative tweets, which can also be used to train any linear text classifier when the network structure is not available or is costly",
    "checked": true,
    "id": "96a8cb7004dc98b25d92f8310834c871e0c19d9b",
    "semantic_title": "",
    "citation_count": 49,
    "authors": [
      "Javid Ebrahimi",
      "Dejing Dou",
      "Daniel Lowd"
    ]
  },
  "https://aclanthology.org/D16-1106": {
    "title": "The Gun Violence Database: A new task and data set for NLP",
    "volume": "main",
    "abstract": "We argue that NLP researchers are especially well-positioned to contribute to the national discussion about gun violence. Reasoning about the causes and outcomes of gun violence is typically dominated by politics and emotion, and data-driven research on the topic is stymied by a shortage of data and a lack of federal funding. However, data abounds in the form of unstructured text from news articles across the country. This is an ideal application of NLP technologies, such as relation extraction, coreference resolution, and event detection. We introduce a new and growing dataset, the Gun Violence Database, in order to facilitate the adaptation of current NLP technologies to the domain of gun violence, thus enabling better social science research on this important and under-resourced problem",
    "checked": true,
    "id": "39cc240b6e359071fe1ddaac22473eb354f85a00",
    "semantic_title": "",
    "citation_count": 27,
    "authors": [
      "Ellie Pavlick",
      "Heng Ji",
      "Xiaoman Pan",
      "Chris Callison-Burch"
    ]
  },
  "https://aclanthology.org/D16-1107": {
    "title": "Fluency detection on communication networks",
    "volume": "main",
    "abstract": "When considering a social media corpus, we often have access to structural information about how messages are flowing between people or organizations. This information is particularly useful when the linguistic evidence is sparse, incomplete, or of dubious quality. In this paper we construct a simple model to leverage the structure of Twitter data to help determine the set of languages each user is fluent in. Our results demonstrate that imposing several intuitive constraints leads to improvements in performance and stability. We release the first annotated data set for exploring this task, and discuss how our approach may be extended to other applications",
    "checked": true,
    "id": "0910320d6a9830211c747fea8633fd9b83fb4981",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Lippincott",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/D16-1108": {
    "title": "Characterizing the Language of Online Communities and its Relation to Community Reception",
    "volume": "main",
    "abstract": "This work investigates style and topic aspects of language in online communities: looking at both utility as an identifier of the community and correlation with community reception of content. Style is characterized using a hybrid word and part-of-speech tag n-gram language model, while topic is represented using Latent Dirichlet Allocation. Experiments with several Reddit forums show that style is a better indicator of community identity than topic, even for communities organized around specific topics. Further, there is a positive correlation between the community reception to a contribution and the style similarity to that community, but not so for topic similarity",
    "checked": true,
    "id": "e9a4d110c2bda1df94ea98ce9713ce5ab64c94d8",
    "semantic_title": "",
    "citation_count": 41,
    "authors": [
      "Trang Tran",
      "Mari Ostendorf"
    ]
  },
  "https://aclanthology.org/D16-1109": {
    "title": "Joint Transition-based Dependency Parsing and Disfluency Detection for Automatic Speech Recognition Texts",
    "volume": "main",
    "abstract": "Joint dependency parsing with disfluency detection is an important task in speech language processing. Recent methods show high performance for this task, although most authors make the unrealistic assumption that input texts are transcribed by human annotators. In real-world applications, the input text is typically the output of an automatic speech recognition (ASR) system, which implies that the text contains not only disfluency noises but also recognition errors from the ASR system. In this work, we propose a parsing method that handles both disfluency and ASR errors using an incremental shift-reduce algorithm with several novel features suited to ASR output texts. Because the gold dependency information is usually annotated only on transcribed texts, we also introduce an alignment-based method for transferring the gold dependency annotation to the ASR output texts to construct training data for our parser. We conducted an experiment on the Switchboard corpus and show that our method outperforms conventional methods in terms of dependency parsing and disfluency detection",
    "checked": true,
    "id": "5399096acb4d3951177e7a254020f7d8c6d72f4d",
    "semantic_title": "",
    "citation_count": 33,
    "authors": [
      "Masashi Yoshikawa",
      "Hiroyuki Shindo",
      "Yuji Matsumoto"
    ]
  },
  "https://aclanthology.org/D16-1110": {
    "title": "Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue Systems",
    "volume": "main",
    "abstract": "In this paper, we describe our approach of enabling an interactive dialogue system to recognize user emotion and sentiment in realtime. These modules allow otherwise conventional dialogue systems to have \"empathy\" and answer to the user while being aware of their emotion and intent. Emotion recognition from speech previously consists of feature engineering and machine learning where the first stage causes delay in decoding time. We describe a CNN model to extract emotion from raw speech input without feature engineering. This approach even achieves an impressive average of 65.7% accuracy on six emotion categories, a 4.5% improvement when compared to the conventional feature based SVM classification. A separate, CNN-based sentiment analysis module recognizes sentiments from speech recognition results, with 82.5 Fmeasure on human-machine dialogues when trained with out-of-domain data",
    "checked": true,
    "id": "2278d52eff38b4d5cb35a15b8e07b892ec6e213b",
    "semantic_title": "",
    "citation_count": 80,
    "authors": [
      "Dario Bertero",
      "Farhad Bin Siddique",
      "Chien-Sheng Wu",
      "Yan Wan",
      "Ricky Ho Yin Chan",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/D16-1111": {
    "title": "A Neural Network Architecture for Multilingual Punctuation Generation",
    "volume": "main",
    "abstract": "This work was supported by the European Commission under the contract numbers FP7-ICT-/n610411 (MULTISENSOR) and H2020-RIA-645012 (KRISTINA)",
    "checked": true,
    "id": "080e5c10e846aab3acb6deca315105d145e02e26",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Miguel Ballesteros",
      "Leo Wanner"
    ]
  },
  "https://aclanthology.org/D16-1112": {
    "title": "Neural Headline Generation on Abstract Meaning Representation",
    "volume": "main",
    "abstract": "Neural network-based encoder-decoder models are among recent attractive methodologies for tackling natural language generation tasks. This paper investigates the usefulness of structural syntactic and semantic information additionally incorporated in a baseline neural attention-based model. We encode results obtained from an abstract meaning representation (AMR) parser using a modified version of Tree-LSTM. Our proposed attention-based AMR encoder-decoder model improves headline generation benchmarks compared with the baseline neural attention-based model",
    "checked": true,
    "id": "4d1f12f1a28afc30aab6f5086b3f2e481cf1f49f",
    "semantic_title": "",
    "citation_count": 156,
    "authors": [
      "Sho Takase",
      "Jun Suzuki",
      "Naoaki Okazaki",
      "Tsutomu Hirao",
      "Masaaki Nagata"
    ]
  },
  "https://aclanthology.org/D16-1113": {
    "title": "Robust Gram Embeddings",
    "volume": "main",
    "abstract": "Word embedding models learn vectorial word representations that can be used in a variety of NLP applications. When training data is scarce, these models risk losing their generalization abilities due to the complexity of the models and the overfitting to finite data. We propose a regularized embedding formulation, called Robust Gram (RG), which penalizes overfitting by suppressing the disparity between target and context embeddings. Our experimental analysis shows that the RG model trained on small datasets generalizes better compared to alternatives, is more robust to variations in the training set, and correlates well to human similarities in a set of word similarity tasks",
    "checked": true,
    "id": "fca669c91d4010f4dce82e9fe3bd610bc96a3612",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Taygun Kekeç",
      "David M. J. Tax"
    ]
  },
  "https://aclanthology.org/D16-1114": {
    "title": "SimpleScience: Lexical Simplification of Scientific Terminology",
    "volume": "main",
    "abstract": "Lexical simplification of scientific terms represents a unique challenge due to the lack of a standard parallel corpora and fast rate at which vocabulary shift along with research. We introduce SimpleScience, a lexical simplification approach for scientific terminology. We use word embeddings to extract simplification rules from a parallel corpora containing scientific publications and Wikipedia. To evaluate our system we construct SimpleSciGold, a novel gold standard set for science-related simplifications. We find that our approach outperforms prior context-aware approaches at generating simplifications for scientific terms",
    "checked": true,
    "id": "a07ce6abdb9edab5bfe32a8650e9f54de987af16",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Yea-Seul Kim",
      "Jessica Hullman",
      "Matthew Burgess",
      "Eytan Adar"
    ]
  },
  "https://aclanthology.org/D16-1115": {
    "title": "Automatic Features for Essay Scoring – An Empirical Study",
    "volume": "main",
    "abstract": "Essay scoring is a complicated processing requiring analyzing, summarizing and judging expertise. Traditional work on essay scoring focused on automatic handcrafted features, which are expensive yet sparse. Neural models offer a way to learn syntactic and semantic features automatically, which can potentially improve upon discrete features. In this paper, we employ convolutional neural network (CNN) for the effect of automatically learning features, and compare the result with the state-of-art discrete baselines. For in-domain and domain-adaptation essay scoring tasks, our neural model empirically outperforms discrete models",
    "checked": true,
    "id": "d63abbe30e5d4bb13b41bf760a5ab09e9e6cc1f7",
    "semantic_title": "",
    "citation_count": 118,
    "authors": [
      "Fei Dong",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/D16-1116": {
    "title": "Semantic Parsing with Semi-Supervised Sequential Autoencoders",
    "volume": "main",
    "abstract": "We present a novel semi-supervised approach for sequence transduction and apply it to semantic parsing. The unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms",
    "checked": true,
    "id": "1d00075feb9f2fecc1d035897b98eb2f45461b2e",
    "semantic_title": "",
    "citation_count": 79,
    "authors": [
      "Tomáš Kočiský",
      "Gábor Melis",
      "Edward Grefenstette",
      "Chris Dyer",
      "Wang Ling",
      "Phil Blunsom",
      "Karl Moritz Hermann"
    ]
  },
  "https://aclanthology.org/D16-1117": {
    "title": "Equation Parsing : Mapping Sentences to Grounded Equations",
    "volume": "main",
    "abstract": "Identifying mathematical relations expressed in text is essential to understanding a broad range of natural language text from election reports, to financial news, to sport commentaries to mathematical word problems. This paper focuses on identifying and understanding mathematical relations described within a single sentence. We introduce the problem of Equation Parsing -- given a sentence, identify noun phrases which represent variables, and generate the mathematical equation expressing the relation described in the sentence. We introduce the notion of projective equation parsing and provide an efficient algorithm to parse text to projective equations. Our system makes use of a high precision lexicon of mathematical expressions and a pipeline of structured predictors, and generates correct equations in $70\\%$ of the cases. In $60\\%$ of the time, it also identifies the correct noun phrase $\\rightarrow$ variables mapping, significantly outperforming baselines. We also release a new annotated dataset for task evaluation",
    "checked": true,
    "id": "786a4e288dddaddd5926a3401b407f034be1f3fd",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Subhro Roy",
      "Shyam Upadhyay",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/D16-1118": {
    "title": "Automatic Extraction of Implicit Interpretations from Modal Constructions",
    "volume": "main",
    "abstract": "This paper presents an approach to extract implicit interpretations from modal constructions. Importantly, our approach uses a deterministic procedure to normalize eventualities and generate potential interpretations. An annotation effort demonstrates that these interpretations are intuitive to humans and most modal constructions convey at least one interpretation. Experimental results show that the task is challenging but can be automated",
    "checked": true,
    "id": "813779418a613d1faecd7b1deb9b4456121a9b7e",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jordan Sanders",
      "Eduardo Blanco"
    ]
  },
  "https://aclanthology.org/D16-1119": {
    "title": "Understanding Negation in Positive Terms Using Syntactic Dependencies",
    "volume": "main",
    "abstract": "This paper presents a two-step procedure to extract positive meaning from verbal negation. We first generate potential positive interpretations manipulating syntactic dependencies. Then, we score them according to their likelihood. Manual annotations show that positive interpretations are ubiquitous and intuitive to humans. Experimental results show that dependencies are better suited than semantic roles for this task, and automation is possible",
    "checked": true,
    "id": "8911933eae2dcf4106ee584e392fb720e5283ae6",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Zahra Sarabi",
      "Eduardo Blanco"
    ]
  },
  "https://aclanthology.org/D16-1120": {
    "title": "Demographic Dialectal Variation in Social Media: A Case Study of African-American English",
    "volume": "main",
    "abstract": "Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE-like language",
    "checked": true,
    "id": "7a4f3a0cfc0cc2aafa4ed1a2924380e82d5e3e4c",
    "semantic_title": "",
    "citation_count": 252,
    "authors": [
      "Su Lin Blodgett",
      "Lisa Green",
      "Brendan O’Connor"
    ]
  },
  "https://aclanthology.org/D16-1121": {
    "title": "Understanding Language Preference for Expression of Opinion and Sentiment: What do Hindi-English Speakers do on Twitter?",
    "volume": "main",
    "abstract": "Linguistic research on multilingual societies has indicated that there is usually a preferred language for expression of emotion and sentiment (Dewaele, 2010). Paucity of data has limited such studies to participant interviews and speech transcriptions from small groups of speakers. In this paper, we report a study on 430,000 unique tweets from Indian users, specifically Hindi-English bilinguals, to understand the language of preference, if any, for expressing opinion and sentiment. To this end, we develop classifiers for opinion detection in these languages, and further classifying opinionated tweets into positive, negative and neutral sentiments. Our study indicates that Hindi (i.e., the native language) is preferred over English for expression of negative opinion and swearing. As an aside, we explore some common pragmatic functions of code-switching through sentiment detection",
    "checked": true,
    "id": "2842c21e879ee581aa50704817454f21b539fc69",
    "semantic_title": "",
    "citation_count": 56,
    "authors": [
      "Koustav Rudra",
      "Shruti Rijhwani",
      "Rafiya Begum",
      "Kalika Bali",
      "Monojit Choudhury",
      "Niloy Ganguly"
    ]
  },
  "https://aclanthology.org/D16-1122": {
    "title": "Detecting and Characterizing Events",
    "volume": "main",
    "abstract": "Significant events are characterized by interactions between entities (such as countries, organizations, or individuals) that deviate from typical interaction patterns. Analysts, including historians, political scientists, and journalists, commonly read large quantities of text to construct an accurate picture of when and where an event happened, who was involved, and in what ways. In this paper, we present the Capsule model for analyzing documents to detect and characterize events of potential significance. Specifically, we develop a model based on topic modeling that distinguishes between topics that describe \"business as usual\" and topics that deviate from these patterns. To demonstrate this model, we analyze a corpus of over two million U.S. State Department cables from the 1970s. We provide an open-source implementation of an inference algorithm for the model and a pipeline for exploring its results",
    "checked": true,
    "id": "56b874006900ade34feb39f1317ebbe6c0b0e1d2",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Allison Chaney",
      "Hanna Wallach",
      "Matthew Connelly",
      "David Blei"
    ]
  },
  "https://aclanthology.org/D16-1123": {
    "title": "Convolutional Neural Network Language Models",
    "volume": "main",
    "abstract": "Convolutional Neural Networks (CNNs) have shown to yield very strong results in several Computer Vision tasks. Their application to language has received much less attention, and it has mainly focused on static classification tasks, such as sentence classification for Sentiment Analysis or relation extraction. In this work, we study the application of CNNs to language modeling, a dynamic, sequential prediction task that needs models to capture local as well as long-range dependency information. Our contribution is twofold. First, we show that CNNs achieve 11-26% better absolute performance than feed-forward neural language models, demonstrating their potential for language representation even in sequential tasks. As for recurrent models, our model outperforms RNNs but is below state of the art LSTM models. Second, we gain some understanding of the behavior of the model, showing that CNNs in language act as feature detectors at a high level of abstraction, like in Computer Vision, and that the model can profitably use information from as far as 16 words before the target",
    "checked": true,
    "id": "b0c5a4f8c833a4bcbe96f17a1e9323a5d46a02f4",
    "semantic_title": "",
    "citation_count": 34,
    "authors": [
      "Ngoc-Quan Pham",
      "German Kruszewski",
      "Gemma Boleda"
    ]
  },
  "https://aclanthology.org/D16-1124": {
    "title": "Generalizing and Hybridizing Count-based and Neural Language Models",
    "volume": "main",
    "abstract": "Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches",
    "checked": true,
    "id": "95cedaeb3178a4671703a05171a144e6b964a819",
    "semantic_title": "",
    "citation_count": 28,
    "authors": [
      "Graham Neubig",
      "Chris Dyer"
    ]
  },
  "https://aclanthology.org/D16-1125": {
    "title": "Reasoning about Pragmatics with Neural Listeners and Speakers",
    "volume": "main",
    "abstract": "We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural \"listener\" and \"speaker\" models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated _without_ demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to a 69% success rate using existing techniques",
    "checked": true,
    "id": "92dbc1509b5641946cc8b524610cb6803d6ee5f6",
    "semantic_title": "",
    "citation_count": 137,
    "authors": [
      "Jacob Andreas",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/D16-1126": {
    "title": "Generating Topical Poetry",
    "volume": "main",
    "abstract": "We describe Hafez, a program that generates any number of distinct poems on a usersupplied topic. Poems obey rhythmic and rhyme constraints. We describe the poetrygeneration algorithm, give experimental data concerning its parameters, and show its generality with respect to language and poetic form",
    "checked": true,
    "id": "a3a8f179b8589ea51aa553722daf82f87210abc4",
    "semantic_title": "",
    "citation_count": 138,
    "authors": [
      "Marjan Ghazvininejad",
      "Xing Shi",
      "Yejin Choi",
      "Kevin Knight"
    ]
  },
  "https://aclanthology.org/D16-1127": {
    "title": "Deep Reinforcement Learning for Dialogue Generation",
    "volume": "main",
    "abstract": "Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues",
    "checked": true,
    "id": "1298dae5751fb06184f6b067d1503bde8037bdb7",
    "semantic_title": "",
    "citation_count": 1188,
    "authors": [
      "Jiwei Li",
      "Will Monroe",
      "Alan Ritter",
      "Dan Jurafsky",
      "Michel Galley",
      "Jianfeng Gao"
    ]
  },
  "https://aclanthology.org/D16-1128": {
    "title": "Neural Text Generation from Structured Data with Application to the Biography Domain",
    "volume": "main",
    "abstract": "This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU",
    "checked": true,
    "id": "604764133befe7a0aaa692919545846197e6e065",
    "semantic_title": "",
    "citation_count": 415,
    "authors": [
      "Rémi Lebret",
      "David Grangier",
      "Michael Auli"
    ]
  },
  "https://aclanthology.org/D16-1129": {
    "title": "What makes a convincing argument? Empirical analysis and detecting attributes of convincingness in Web argumentation",
    "volume": "main",
    "abstract": "This article tackles a new challenging task in computational argumentation. Given a pair of two arguments to a certain controversial topic, we aim to directly assess qualitative properties of the arguments in order to explain why one argument is more convincing than the other one. We approach this task in a fully empirical manner by annotating 26k explanations written in natural language. These explanations describe convincingness of arguments in the given argument pair, such as their strengths or flaws. We create a new crowd-sourced corpus containing 9,111 argument pairs, multi-labeled with 17 classes, which was cleaned and curated by employing several strict quality measures. We propose two tasks on this data set, namely (1) predicting the full label distribution and (2) classifying types of flaws in less convincing arguments. Our experiments with feature-rich SVM learners and Bidirectional LSTM neural networks with convolution and attention mechanism reveal that such a novel fine-grained analysis of Web argument convincingness is a very challenging task. We release the new UKPConvArg2 corpus and software under permissive licenses to the research community",
    "checked": true,
    "id": "4759aaacd71fbb2b5ca253aa13ccceac0bc7fe8a",
    "semantic_title": "",
    "citation_count": 90,
    "authors": [
      "Ivan Habernal",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/D16-1130": {
    "title": "Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention",
    "volume": "main",
    "abstract": "Recognizing implicit discourse relations is a challenging but important task in the field of Natural Language Processing. For such a complex text processing task, different from previous studies, we argue that it is necessary to repeatedly read the arguments and dynamically exploit the efficient features useful for recognizing discourse relations. To mimic the repeated reading strategy, we propose the neural networks with multi-level attention (NNMA), combining the attention mechanism and external memories to gradually fix the attention on some specific words helpful to judging the discourse relations. Experiments on the PDTB dataset show that our proposed method achieves the state-of-art results. The visualization of the attention weights also illustrates the progress that our model observes the arguments on each level and progressively locates the important words",
    "checked": true,
    "id": "eb3d818cf095c3bb8c08b6ff3a19b1bc28f9d698",
    "semantic_title": "",
    "citation_count": 68,
    "authors": [
      "Yang Liu",
      "Sujian Li"
    ]
  },
  "https://aclanthology.org/D16-1131": {
    "title": "Antecedent Selection for Sluicing: Structure and Content",
    "volume": "main",
    "abstract": "Sluicing is an elliptical process where the majority of a question can go unpronounced as long as there is a salient antecedent in previous discourse. This paper considers the task of antecedent selection: finding the correct antecedent for a given case of sluicing. We argue that both syntactic and discourse relationships are important in antecedent selection, and we construct linguistically sophisticated features that describe the relevant relationships. We also define features that describe the relation of the content of the antecedent and the sluice type. We develop a linear model which achieves accuracy of 72.4%, a substantial improvement over a strong manually constructed baseline. Feature analysis confirms that both syntactic and discourse features are important in antecedent selection",
    "checked": true,
    "id": "0d6f007038c1afb9f9cc60045ca188fc707469a0",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Pranav Anand",
      "Daniel Hardt"
    ]
  },
  "https://aclanthology.org/D16-1132": {
    "title": "Intra-Sentential Subject Zero Anaphora Resolution using Multi-Column Convolutional Neural Network",
    "volume": "main",
    "abstract": "This paper proposes a method for intrasentential subject zero anaphora resolution in Japanese. Our proposed method utilizes a Multi-column Convolutional Neural Network (MCNN) for predicting zero anaphoric relations. Motivated by Centering Theory and other previous works, we exploit as clues both the surface word sequence and the dependency tree of a target sentence in our MCNN. Even though the F-score of our method was lower than that of the state-of-the-art method, which achieved relatively high recall and low precision, our method achieved much higher precision (>0.8) in a wide range of recall levels. We believe such high precision is crucial for real-world NLP applications and thus our method is preferable to the state-of-the-art method",
    "checked": true,
    "id": "45b0d948217d4578b71687549fbfdd797dc03c5b",
    "semantic_title": "",
    "citation_count": 27,
    "authors": [
      "Ryu Iida",
      "Kentaro Torisawa",
      "Jong-Hoon Oh",
      "Canasai Kruengkrai",
      "Julien Kloetzer"
    ]
  },
  "https://aclanthology.org/D16-1133": {
    "title": "An Unsupervised Probability Model for Speech-to-Translation Alignment of Low-Resource Languages",
    "volume": "main",
    "abstract": "For many low-resource languages, spoken language resources are more likely to be annotated with translations than with transcriptions. Translated speech data is potentially valuable for documenting endangered languages or for training speech translation systems. A first step towards making use of such data would be to automatically align spoken words with their translations. We present a model that combines Dyer et al.'s reparameterization of IBM Model 2 (fast-align) and k-means clustering using Dynamic Time Warping as a distance metric. The two components are trained jointly using expectation-maximization. In an extremely low-resource scenario, our model performs significantly better than both a neural model and a strong baseline",
    "checked": true,
    "id": "fd83fa5720c7283ff2c935902b956889e3db8a2b",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Antonios Anastasopoulos",
      "David Chiang",
      "Long Duong"
    ]
  },
  "https://aclanthology.org/D16-1134": {
    "title": "HUME: Human UCCA-Based Evaluation of Machine Translation",
    "volume": "main",
    "abstract": "Human evaluation of machine translation normally uses sentence-level measures such as relative ranking or adequacy scales. However, these provide no insight into possible errors, and do not scale well with sentence length. We argue for a semantics-based evaluation, which captures what meaning components are retained in the MT output, thus providing a more fine-grained analysis of translation quality, and enabling the construction and tuning of semantics-based MT. We present a novel human semantic evaluation measure, Human UCCA-based MT Evaluation (HUME), building on the UCCA semantic representation scheme. HUME covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled MT output. We experiment with four language pairs, demonstrating HUME's broad applicability, and report good inter-annotator agreement rates and correlation with human adequacy scores",
    "checked": true,
    "id": "c9e5456e328be30de60a87b742e733b5dca004b9",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Alexandra Birch",
      "Omri Abend",
      "Ondřej Bojar",
      "Barry Haddow"
    ]
  },
  "https://aclanthology.org/D16-1135": {
    "title": "Improving Multilingual Named Entity Recognition with Wikipedia Entity Type Mapping",
    "volume": "main",
    "abstract": "The state-of-the-art named entity recognition (NER) systems are statistical machine learning models that have strong generalization capability (i.e., can recognize unseen entities that do not appear in training data) based on lexical and contextual information. However, such a model could still make mistakes if its features favor a wrong entity type. In this paper, we utilize Wikipedia as an open knowledge base to improve multilingual NER systems. Central to our approach is the construction of high-accuracy, high-coverage multilingual Wikipedia entity type mappings. These mappings are built from weakly annotated data and can be extended to new languages with no human annotation or language-dependent knowledge involved. Based on these mappings, we develop several approaches to improve an NER system. We evaluate the performance of the approaches via experiments on NER systems trained for 6 languages. Experimental results show that the proposed approaches are effective in improving the accuracy of such systems on unseen entities, especially when a system is applied to a new domain or it is trained with little training data (up to 18.3 F1 score improvement)",
    "checked": true,
    "id": "7ee4921f0a81055253fa8d9fe85c4fb82ffff9ae",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Jian Ni",
      "Radu Florian"
    ]
  },
  "https://aclanthology.org/D16-1136": {
    "title": "Learning Crosslingual Word Embeddings without Bilingual Corpora",
    "volume": "main",
    "abstract": "Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task",
    "checked": true,
    "id": "42636e49faeb9d8eefac065979115c685fec1bab",
    "semantic_title": "",
    "citation_count": 117,
    "authors": [
      "Long Duong",
      "Hiroshi Kanayama",
      "Tengfei Ma",
      "Steven Bird",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/D16-1137": {
    "title": "Sequence-to-Sequence Learning as Beam-Search Optimization",
    "volume": "main",
    "abstract": "Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation",
    "checked": true,
    "id": "5507dc32b368c8afd3b9507e9b5888da7bd7d7cd",
    "semantic_title": "",
    "citation_count": 521,
    "authors": [
      "Sam Wiseman",
      "Alexander M. Rush"
    ]
  },
  "https://aclanthology.org/D16-1138": {
    "title": "Online Segment to Segment Neural Transduction",
    "volume": "main",
    "abstract": "We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training, and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixed-length hidden states before producing any output. It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. Experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders",
    "checked": true,
    "id": "f61da0efbb38bd3e6b9a9855809f5288b829f1f0",
    "semantic_title": "",
    "citation_count": 84,
    "authors": [
      "Lei Yu",
      "Jan Buys",
      "Phil Blunsom"
    ]
  },
  "https://aclanthology.org/D16-1139": {
    "title": "Sequence-Level Knowledge Distillation",
    "volume": "main",
    "abstract": "Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model, with a decrease of 0.4 BLEU",
    "checked": true,
    "id": "57a10537978600fd33dcdd48922c791609a4851a",
    "semantic_title": "",
    "citation_count": 496,
    "authors": [
      "Yoon Kim",
      "Alexander M. Rush"
    ]
  },
  "https://aclanthology.org/D16-1140": {
    "title": "Controlling Output Length in Neural Encoder-Decoders",
    "volume": "main",
    "abstract": "Neural encoder-decoder models have shown great success in many sequence generation tasks. However, previous work has not investigated situations in which we would like to control the length of encoder-decoder outputs. This capability is crucial for applications such as text summarization, in which we have to generate concise summaries with a desired length. In this paper, we propose methods for controlling the output sequence length for neural encoder-decoder models: two decoding-based methods and two learning-based methods. Results show that our learning-based methods have the capability to control length without degrading summary quality in a summarization task",
    "checked": true,
    "id": "3cfdec4f1664fcdc20fd5a6d3f86e7b40cf19f70",
    "semantic_title": "",
    "citation_count": 211,
    "authors": [
      "Yuta Kikuchi",
      "Graham Neubig",
      "Ryohei Sasano",
      "Hiroya Takamura",
      "Manabu Okumura"
    ]
  },
  "https://aclanthology.org/D16-1141": {
    "title": "Poet Admits // Mute Cypher: Beam Search to find Mutually Enciphering Poetic Texts",
    "volume": "main",
    "abstract": "The Xenotext Experiment implants poetry into an extremophile's DNA, and uses that DNA to generate new poetry in a protein form. The molecular machinery of life requires that these two poems encipher each other under a symmetric substitution cipher. We search for ciphers which permit writing under the Xenotext constraints, incorporating ideas from cipher-cracking algorithms, and using n-gram data to assess a cipher's \"writability\". Our algorithm, Beam Verse, is a beam search which uses new heuristics to navigate the cipher-space. We find thousands of ciphers which score higher than successful ciphers used to write Xenotext constrained texts",
    "checked": true,
    "id": "da4f0fcef0c91f967e80466fe6433b1f965929b7",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cole Peterson",
      "Alona Fyshe"
    ]
  },
  "https://aclanthology.org/D16-1142": {
    "title": "All Fingers are not Equal: Intensity of References in Scientific Articles",
    "volume": "main",
    "abstract": "Research accomplishment is usually measured by considering all citations with equal importance, thus ignoring the wide variety of purposes an article is being cited for. Here, we posit that measuring the intensity of a reference is crucial not only to perceive better understanding of research endeavor, but also to improve the quality of citation-based applications. To this end, we collect a rich annotated dataset with references labeled by the intensity, and propose a novel graph-based semi-supervised model, GraLap to label the intensity of references. Experiments with AAN datasets show a significant improvement compared to the baselines to achieve the true labels of the references (46% better correlation). Finally, we provide four applications to demonstrate how the knowledge of reference intensity leads to design better real-world applications",
    "checked": true,
    "id": "a59cafd1b42dd3b6e7442933a5390bd70616ab91",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Tanmoy Chakraborty",
      "Ramasuri Narayanam"
    ]
  },
  "https://aclanthology.org/D16-1143": {
    "title": "Improving Users' Demographic Prediction via the Videos They Talk about",
    "volume": "main",
    "abstract": "In this paper, we improve microblog users' demographic prediction by fully utilizing their video related behaviors. First, we collect the describing words of currently popular videos, including video names, actor names and video keywords, from video websites. Secondly, we search these describing words in users' microblogs, and build the direct relationships between users and the appeared words. After that, to make the sparse relationship denser, we propose a Bayesian method to calculate the probability of connections between users and other video describing words. Lastly, we build two models to predict users' demographics with the obtained direct and indirect relationships. Based on a large realworld dataset, experiment results show that our method can significantly improve these words' demographic predictive ability",
    "checked": true,
    "id": "9f297a93decd6defb8df09a6ba622257e90f52ed",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Yuan Wang",
      "Yang Xiao",
      "Chao Ma",
      "Zhen Xiao"
    ]
  },
  "https://aclanthology.org/D16-1144": {
    "title": "AFET: Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding",
    "volume": "main",
    "abstract": "Distant supervision has been widely used in current systems of fine-grained entity typing to automatically assign categories (entity types) to entity mentions. However, the types so obtained from knowledge bases are often incorrect for the entity mention's local context. This paper proposes a novel embedding method to separately model \"clean\" and \"noisy\" mentions, and incorporates the given type hierarchy to induce loss functions. We formulate a joint optimization problem to learn embeddings for mentions and typepaths, and develop an iterative algorithm to solve the problem. Experiments on three public datasets demonstrate the effectiveness and robustness of the proposed method, with an average 15% improvement in accuracy over the next best compared method1",
    "checked": true,
    "id": "ee42c6c3c5db2f0eb40faacf6e3b80035a645287",
    "semantic_title": "",
    "citation_count": 116,
    "authors": [
      "Xiang Ren",
      "Wenqi He",
      "Meng Qu",
      "Lifu Huang",
      "Heng Ji",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/D16-1145": {
    "title": "Mining Inference Formulas by Goal-Directed Random Walks",
    "volume": "main",
    "abstract": "Deep inference on a large-scale knowledge base (KB) needs a mass of formulas, but it is almost impossible to create all formulas manually. Data-driven methods have been proposed to mine formulas from KBs automatically, where random sampling and approximate calculation are common techniques to handle big data. Among a series of methods, Random Walk is believed to be suitable for knowledge graph data. However, a pure random walk without goals still has a poor efficiency of mining useful formulas, and even introduces lots of noise which may mislead inference. Although several heuristic rules have been proposed to direct random walks, they do not work well due to the diversity of formulas. To this end, we propose a novel goaldirected inference formula mining algorithm, which directs random walks by the specific inference target at each step. The algorithm is more inclined to visit benefic structures to infer the target, so it can increase efficiency of random walks and avoid noise simultaneously. The experiments on both WordNet and Freebase prove that our approach is has a high efficiency and performs best on the task",
    "checked": true,
    "id": "45348379df0d40b5c1d4df26587e6b80cfe96565",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Zhuoyu Wei",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  "https://aclanthology.org/D16-1146": {
    "title": "Lifted Rule Injection for Relation Embeddings",
    "volume": "main",
    "abstract": "Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models. A recent approach regularizes relation and entity representations by propositionalization of first-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime",
    "checked": true,
    "id": "a1fb4e2029af4a118a045cf9dc7c00d65047f8df",
    "semantic_title": "",
    "citation_count": 130,
    "authors": [
      "Thomas Demeester",
      "Tim Rocktäschel",
      "Sebastian Riedel"
    ]
  },
  "https://aclanthology.org/D16-1147": {
    "title": "Key-Value Memory Networks for Directly Reading Documents",
    "volume": "main",
    "abstract": "Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, WikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing WikiQA benchmark",
    "checked": true,
    "id": "bba5f2852b1db8a18004eb7328efa5e1d57cc62a",
    "semantic_title": "",
    "citation_count": 783,
    "authors": [
      "Alexander Miller",
      "Adam Fisch",
      "Jesse Dodge",
      "Amir-Hossein Karimi",
      "Antoine Bordes",
      "Jason Weston"
    ]
  },
  "https://aclanthology.org/D16-1148": {
    "title": "Analyzing Framing through the Casts of Characters in the News",
    "volume": "main",
    "abstract": "We present an unsupervised model for the discovery and clustering of latent \"personas\" (characterizations of entities). Our model simultaneously clusters documents featuring similar collections of personas. We evaluate this model on a collection of news articles about immigration, showing that personas help predict the coarse-grained framing annotations in the Media Frames Corpus. We also introduce automated model selection as a fair and robust form of feature evaluation",
    "checked": true,
    "id": "15130cdd46ff7e3a69384e6c62ab1775c4aba96f",
    "semantic_title": "",
    "citation_count": 54,
    "authors": [
      "Dallas Card",
      "Justin Gross",
      "Amber Boydstun",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/D16-1149": {
    "title": "The Teams Corpus and Entrainment in Multi-Party Spoken Dialogues",
    "volume": "main",
    "abstract": "When interacting individuals entrain, they begin to speak more like each other. To support research on entrainment in cooperative multi-party dialogues, we have created a corpus where teams of three or four speakers play two rounds of a cooperative board game. We describe the experimental design and technical infrastructure used to collect our corpus, which consists of audio, video, transcriptions, and questionnaire data for 63 teams (47 hours of audio). We illustrate the use of our corpus as a novel resource for studying team entrainment by 1) developing and evaluating teamlevel acoustic-prosodic entrainment measures that extend existing dyad measures, and 2) investigating relationships between team entrainment and participation dominance",
    "checked": true,
    "id": "5d27112fdfbb3ebd3f4bf701207d016fc379e45c",
    "semantic_title": "",
    "citation_count": 23,
    "authors": [
      "Diane Litman",
      "Susannah Paletz",
      "Zahra Rahimi",
      "Stefani Allegretti",
      "Caitlin Rice"
    ]
  },
  "https://aclanthology.org/D16-1150": {
    "title": "Personalized Emphasis Framing for Persuasive Message Generation",
    "volume": "main",
    "abstract": "In this paper, we present a study on personalized emphasis framing which can be used to tailor the content of a message to enhance its appeal to different individuals. With this framework, we directly model content selection decisions based on a set of psychologically-motivated domain-independent personal traits including personality (e.g., extraversion and conscientiousness) and basic human values (e.g., self-transcendence and hedonism). We also demonstrate how the analysis results can be used in automated personalized content selection for persuasive message generation",
    "checked": true,
    "id": "214e6d4f7ca9e31e751cd0f7de285906d8e0d039",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Tao Ding",
      "Shimei Pan"
    ]
  },
  "https://aclanthology.org/D16-1151": {
    "title": "Cross Sentence Inference for Process Knowledge",
    "volume": "main",
    "abstract": "For AI systems to reason about real world situations, they need to recognize which processes are at play and which entities play key roles in them. Our goal is to extract this kind of rolebased knowledge about processes, from multiple sentence-level descriptions. This knowledge is hard to acquire; while semantic role labeling (SRL) systems can extract sentence level role information about individual mentions of a process, their results are often noisy and they do not attempt create a globally consistent characterization of a process. To overcome this, we extend standard within sentence joint inference to inference across multiple sentences. This cross sentence inference promotes role assignments that are compatible across different descriptions of the same process. When formulated as an Integer Linear Program, this leads to improvements over within-sentence inference by nearly 3% in F1. The resulting role-based knowledge is of high quality (with a F1 of nearly 82)",
    "checked": true,
    "id": "b6f0f0339b964b4a1eedaa24835e583f6eb6955f",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Samuel Louvan",
      "Chetan Naik",
      "Sadhana Kumaravel",
      "Heeyoung Kwon",
      "Niranjan Balasubramanian",
      "Peter Clark"
    ]
  },
  "https://aclanthology.org/D16-1152": {
    "title": "Toward Socially-Infused Information Extraction: Embedding Authors, Mentions, and Entities",
    "volume": "main",
    "abstract": "Entity linking is the task of identifying mentions of entities in text, and linking them to entries in a knowledge base. This task is especially difficult in microblogs, as there is little additional text to provide disambiguating context; rather, authors rely on an implicit common ground of shared knowledge with their readers. In this paper, we attempt to capture some of this implicit context by exploiting the social network structure in microblogs. We build on the theory of homophily, which implies that socially linked individuals share interests, and are therefore likely to mention the same sorts of entities. We implement this idea by encoding authors, mentions, and entities in a continuous vector space, which is constructed so that socially-connected authors have similar vector representations. These vectors are incorporated into a neural structured prediction model, which captures structural constraints that are inherent in the entity linking task. Together, these design decisions yield F1 improvements of 1%-5% on benchmark datasets, as compared to the previous state-of-the-art",
    "checked": true,
    "id": "90610e5ff03f57a26a786ccbd7dfe0b6111eeef2",
    "semantic_title": "",
    "citation_count": 23,
    "authors": [
      "Yi Yang",
      "Ming-Wei Chang",
      "Jacob Eisenstein"
    ]
  },
  "https://aclanthology.org/D16-1153": {
    "title": "Phonologically Aware Neural Model for Named Entity Recognition in Low Resource Transfer Settings",
    "volume": "main",
    "abstract": "Named Entity Recognition is a well established information extraction task with many state of the art systems existing for a variety of languages. Most systems rely on language specific resources, large annotated corpora, gazetteers and feature engineering to perform well monolingually. In this paper, we introduce an attentional neural model which only uses language universal phonological character representations with word embeddings to achieve state of the art performance in a monolingual setting using supervision and which can quickly adapt to a new language with minimal or no data. We demonstrate that phonological character representations facilitate cross-lingual transfer, outperform orthographic representations and incorporating both attention and phonological features improves statistical efficiency of the model in 0-shot and low data transfer settings with no task specific feature engineering in the source or target language",
    "checked": true,
    "id": "f1a8ff5542ffd51c3618953e5cf926c467154f79",
    "semantic_title": "",
    "citation_count": 77,
    "authors": [
      "Akash Bharadwaj",
      "David Mortensen",
      "Chris Dyer",
      "Jaime Carbonell"
    ]
  },
  "https://aclanthology.org/D16-1154": {
    "title": "Long-Short Range Context Neural Networks for Language Modeling",
    "volume": "main",
    "abstract": "The goal of language modeling techniques is to capture the statistical and structural properties of natural languages from training corpora. This task typically involves the learning of short range dependencies, which generally model the syntactic properties of a language and/or long range dependencies, which are semantic in nature. We propose in this paper a new multi-span architecture, which separately models the short and long context information while it dynamically merges them to perform the language modeling task. This is done through a novel recurrent Long-Short Range Context (LSRC) network, which explicitly models the local (short) and global (long) context using two separate hidden states that evolve in time. This new architecture is an adaptation of the Long-Short Term Memory network (LSTM) to take into account the linguistic properties. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art language modeling techniques",
    "checked": true,
    "id": "eb329aab0d7d48b982dde62a5db6deaafa7de07d",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Youssef Oualil",
      "Mittul Singh",
      "Clayton Greenberg",
      "Dietrich Klakow"
    ]
  },
  "https://aclanthology.org/D16-1155": {
    "title": "Jointly Learning Grounded Task Structures from Language Instruction and Visual Demonstration",
    "volume": "main",
    "abstract": "To enable language-based communication and collaboration with cognitive robots, this paper presents an approach where an agent can learn task models jointly from language instruction and visual demonstration using an And-Or Graph (AoG) representation. The learned AoG captures a hierarchical task structure where linguistic labels (for language communication) are grounded to corresponding state changes from the physical environment (for perception and action). Our empirical results on a cloth-folding domain have shown that, although state detection through visual processing is full of uncertainties and error prone, by a tight integration with language the agent is able to learn an effective AoG for task representation. The learned AoG can be further applied to infer and interpret on-going actions from new visual demonstration using linguistic labels at different levels of granularity",
    "checked": true,
    "id": "a3e2d3ce897a28fc35be86ec73fab9d5591f10d4",
    "semantic_title": "",
    "citation_count": 35,
    "authors": [
      "Changsong Liu",
      "Shaohua Yang",
      "Sari Saba-Sadiya",
      "Nishant Shukla",
      "Yunzhong He",
      "Song-Chun Zhu",
      "Joyce Chai"
    ]
  },
  "https://aclanthology.org/D16-1156": {
    "title": "Resolving Language and Vision Ambiguities Together: Joint Segmentation & Prepositional Attachment Resolution in Captioned Scenes",
    "volume": "main",
    "abstract": "We present an approach to simultaneously perform semantic segmentation and prepositional phrase attachment resolution for captioned images. Some ambiguities in language cannot be resolved without simultaneously reasoning about an associated image. If we consider the sentence \"I shot an elephant in my pajamas\", looking at language alone (and not using common sense), it is unclear if it is the person or the elephant wearing the pajamas or both. Our approach produces a diverse set of plausible hypotheses for both semantic segmentation and prepositional phrase attachment resolution that are then jointly reranked to select the most consistent pair. We show that our semantic segmentation and prepositional phrase attachment resolution modules have complementary strengths, and that joint reasoning produces more accurate results than any module operating in isolation. Multiple hypotheses are also shown to be crucial to improved multiple-module reasoning. Our vision and language approach significantly outperforms the Stanford Parser (De Marneffe et al., 2006) by 17.91% (28.69% relative) and 12.83% (25.28% relative) in two different experiments. We also make small improvements over DeepLab-CRF (Chen et al., 2015)",
    "checked": true,
    "id": "26eb2c900814707ae962184ad4173e754247a80a",
    "semantic_title": "",
    "citation_count": 30,
    "authors": [
      "Gordon Christie",
      "Ankit Laddha",
      "Aishwarya Agrawal",
      "Stanislaw Antol",
      "Yash Goyal",
      "Kevin Kochersberger",
      "Dhruv Batra"
    ]
  },
  "https://aclanthology.org/D16-1157": {
    "title": "Charagram: Embedding Words and Sentences via Character n-grams",
    "volume": "main",
    "abstract": "We present Charagram embeddings, a simple approach for learning character-based compositional models to embed textual sequences. A word or sentence is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding. We use three tasks for evaluation: word similarity, sentence similarity, and part-of-speech tagging. We demonstrate that Charagram embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks",
    "checked": true,
    "id": "12e9d005c77f76e344361f79c4b008034ae547eb",
    "semantic_title": "",
    "citation_count": 182,
    "authors": [
      "John Wieting",
      "Mohit Bansal",
      "Kevin Gimpel",
      "Karen Livescu"
    ]
  },
  "https://aclanthology.org/D16-1158": {
    "title": "Length bias in Encoder Decoder Models and a Case for Global Conditioning",
    "volume": "main",
    "abstract": "Encoder-decoder networks are popular for modeling sequences probabilistically in many applications. These models use the power of the Long Short-Term Memory (LSTM) architecture to capture the full dependence among variables, unlike earlier models like CRFs that typically assumed conditional independence among non-adjacent variables. However in practice encoder-decoder models exhibit a bias towards short sequences that surprisingly gets worse with increasing beam size.   In this paper we show that such phenomenon is due to a discrepancy between the full sequence margin and the per-element margin enforced by the locally conditioned training objective of a encoder-decoder model. The discrepancy more adversely impacts long sequences, explaining the bias towards predicting short sequences.   For the case where the predicted sequences come from a closed set, we show that a globally conditioned model alleviates the above problems of encoder-decoder models. From a practical point of view, our proposed model also eliminates the need for a beam-search during inference, which reduces to an efficient dot-product based search in a vector-space",
    "checked": true,
    "id": "4e912f84a498e883d7b15311007e3ac5635af883",
    "semantic_title": "",
    "citation_count": 36,
    "authors": [
      "Pavel Sountsov",
      "Sunita Sarawagi"
    ]
  },
  "https://aclanthology.org/D16-1159": {
    "title": "Does String-Based Neural MT Learn Source Syntax?",
    "volume": "main",
    "abstract": "We investigate whether a neural, encoderdecoder translation system learns syntactic information on the source side as a by-product of training. We propose two methods to detect whether the encoder has learned local and global source syntax. A fine-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing",
    "checked": true,
    "id": "d821ce08da6c0084d5eacbdf65e25556bc1b9bc3",
    "semantic_title": "",
    "citation_count": 305,
    "authors": [
      "Xing Shi",
      "Inkit Padhi",
      "Kevin Knight"
    ]
  },
  "https://aclanthology.org/D16-1160": {
    "title": "Exploiting Source-side Monolingual Data in Neural Machine Translation",
    "volume": "main",
    "abstract": "Neural Machine Translation (NMT) based on the encoder-decoder architecture has recently become a new paradigm. Researchers have proven that the target-side monolingual data can greatly enhance the decoder model of NMT. However, the source-side monolingual data is not fully explored although it should be useful to strengthen the encoder model of NMT, especially when the parallel corpus is far from sufficient. In this paper, we propose two approaches to make full use of the sourceside monolingual data in NMT. The first approach employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. The second approach applies the multi-task learning framework using two NMTs to predict the translation and the reordered source-side monolingual sentences simultaneously. The extensive experiments demonstrate that the proposed methods obtain significant improvements over the strong attention-based NMT",
    "checked": true,
    "id": "d9b03cd97db6255081d1e57983fa673d1f8f2d0e",
    "semantic_title": "",
    "citation_count": 251,
    "authors": [
      "Jiajun Zhang",
      "Chengqing Zong"
    ]
  },
  "https://aclanthology.org/D16-1161": {
    "title": "Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction",
    "volume": "main",
    "abstract": "In this work, we study parameter tuning towards the M^2 metric, the standard metric for automatic grammar error correction (GEC) tasks. After implementing M^2 as a scorer in the Moses tuning framework, we investigate interactions of dense and sparse features, different optimizers, and tuning strategies for the CoNLL-2014 shared task. We notice erratic behavior when optimizing sparse feature weights with M^2 and offer partial solutions. We find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning outperforms all previously published results for the CoNLL-2014 test set by a large margin (46.37% M^2 over previously 41.75%, by an SMT system with neural features) while being trained on the same, publicly available data. Our newly introduced dense and sparse features widen that gap, and we improve the state-of-the-art to 49.49% M^2",
    "checked": true,
    "id": "b19f365aab0bf8c6cf712c07313b919556bfacc0",
    "semantic_title": "",
    "citation_count": 93,
    "authors": [
      "Marcin Junczys-Dowmunt",
      "Roman Grundkiewicz"
    ]
  },
  "https://aclanthology.org/D16-1162": {
    "title": "Incorporating Discrete Translation Lexicons into Neural Machine Translation",
    "volume": "main",
    "abstract": "Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time",
    "checked": true,
    "id": "dc984ea8be018a0244b40468d13f7b734ab55bac",
    "semantic_title": "",
    "citation_count": 198,
    "authors": [
      "Philip Arthur",
      "Graham Neubig",
      "Satoshi Nakamura"
    ]
  },
  "https://aclanthology.org/D16-1163": {
    "title": "Transfer Learning for Low-Resource Neural Machine Translation",
    "volume": "main",
    "abstract": "The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves Bleu scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 Bleu on four low-resource language pairs. Ensembling and unknown word replacement add another 2 Bleu which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 Bleu, improving the state-of-the-art on low-resource machine translation",
    "checked": true,
    "id": "1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6",
    "semantic_title": "",
    "citation_count": 657,
    "authors": [
      "Barret Zoph",
      "Deniz Yuret",
      "Jonathan May",
      "Kevin Knight"
    ]
  },
  "https://aclanthology.org/D16-1164": {
    "title": "MixKMeans: Clustering Question-Answer Archives",
    "volume": "main",
    "abstract": "Community-driven Question Answering (CQA) systems that crowdsource experiential information in the form of questions and answers and have accumulated valuable reusable knowledge. Clustering of QA datasets from CQA systems provides a means of organizing the content to ease tasks such as manual curation and tagging. In this paper, we present a clustering method that exploits the two-part question-answer structure in QA datasets to improve clustering quality. Our method, MixKMeans, composes question and answer space similarities in a way that the space on which the match is higher is allowed to dominate. This construction is motivated by our observation that semantic similarity between question-answer data (QAs) could get localized in either space. We empirically evaluate our method on a variety of real-world labeled datasets. Our results indicate that our method significantly outperforms stateof-the-art clustering methods for the task of clustering question-answer archives",
    "checked": true,
    "id": "39ce92c72c1f3dc884c6d277e8d0113a18f1cad9",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Deepak P"
    ]
  },
  "https://aclanthology.org/D16-1165": {
    "title": "It Takes Three to Tango: Triangulation Approach to Answer Ranking in Community Question Answering",
    "volume": "main",
    "abstract": "We address the problem of answering new questions in community forums, by selecting suitable answers to already asked questions. We approach the task as an answer ranking problem, adopting a pairwise neural network architecture that selects which of two competing answers is better. We focus on the utility of the three types of similarities occurring in the triangle formed by the original question, the related question, and an answer to the related comment, which we call relevance, relatedness, and appropriateness. Our proposed neural network models the interactions among all input components using syntactic and semantic embeddings, lexical matching, and domain-specific features. It achieves state-of-the-art results, showing that the three similarities are important and need to be modeled together. Our experiments demonstrate that all feature types are relevant, but the most important ones are the lexical similarity features, the domain-specific features, and the syntactic and semantic embeddings",
    "checked": true,
    "id": "c60a68dc35d3ec729c9aa922d53d0a02e1c7e109",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Preslav Nakov",
      "Lluís Màrquez",
      "Francisco Guzmán"
    ]
  },
  "https://aclanthology.org/D16-1166": {
    "title": "Character-Level Question Answering with Attention",
    "volume": "main",
    "abstract": "We show that a character-level encoder-decoder framework can be successfully applied to question answering with a structured knowledge base. We use our model for single-relation question answering and demonstrate the effectiveness of our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy from 63.9% to 70.9%, without use of ensembles. Importantly, our character-level model has 16x fewer parameters than an equivalent word-level model, can be learned with significantly less data compared to previous work, which relies on data augmentation, and is robust to new entities in testing",
    "checked": true,
    "id": "698d675ba7134ac701de810c9ca4a6de72cb414b",
    "semantic_title": "",
    "citation_count": 172,
    "authors": [
      "Xiaodong He",
      "David Golub"
    ]
  },
  "https://aclanthology.org/D16-1167": {
    "title": "Learning to Generate Textual Data",
    "volume": "main",
    "abstract": "To learn text understanding models with millions of parameters one needs massive amounts of data. In this work, we argue that generating data can compensate for this need. While defining generic data generators is difficult, we propose to allow generators to be \"weakly\" specified in the sense that a set of parameters controls how the data is generated. Consider for example generators where the example templates, grammar, and/or vocabulary is determined by this set of parameters. Instead of manually tuning these parameters, we learn them from the limited training data at our disposal. To achieve this, we derive an efficient algorithm called GENERE that jointly estimates the parameters of the model and the undetermined generation parameters. We illustrate its benefits by learning to solve math exam questions using a highly parametrized sequence-to-sequence neural network",
    "checked": true,
    "id": "328d1e5dfe956021ac172760242f7fc8743b90ea",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Guillaume Bouchard",
      "Pontus Stenetorp",
      "Sebastian Riedel"
    ]
  },
  "https://aclanthology.org/D16-1168": {
    "title": "A Theme-Rewriting Approach for Generating Algebra Word Problems",
    "volume": "main",
    "abstract": "Texts present coherent stories that have a particular theme or overall setting, for example science fiction or western. In this paper, we present a text generation method called {\\it rewriting} that edits existing human-authored narratives to change their theme without changing the underlying story. We apply the approach to math word problems, where it might help students stay more engaged by quickly transforming all of their homework assignments to the theme of their favorite movie without changing the math concepts that are being taught. Our rewriting method uses a two-stage decoding process, which proposes new words from the target theme and scores the resulting stories according to a number of factors defining aspects of syntactic, semantic, and thematic coherence. Experiments demonstrate that the final stories typically represent the new theme well while still testing the original math concepts, outperforming a number of baselines. We also release a new dataset of human-authored rewrites of math word problems in several themes",
    "checked": true,
    "id": "7e27669e08b38ca03eeadce84086208bcd6e70ed",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Rik Koncel-Kedziorski",
      "Ioannis Konstas",
      "Luke Zettlemoyer",
      "Hannaneh Hajishirzi"
    ]
  },
  "https://aclanthology.org/D16-1169": {
    "title": "Context-Sensitive Lexicon Features for Neural Sentiment Analysis",
    "volume": "main",
    "abstract": "Sentiment lexicons have been leveraged as a useful source of features for sentiment analysis models, leading to the state-of-the-art accuracies. On the other hand, most existing methods use sentiment lexicons without considering context, typically taking the count, sum of strength, or maximum sentiment scores over the whole input. We propose a context-sensitive lexicon-based method based on a simple weighted-sum model, using a recurrent neural network to learn the sentiments strength, intensification and negation of lexicon sentiments in composing the sentiment value of sentences. Results show that our model can not only learn such operation details, but also give significant improvements over state-of-the-art recurrent neural network baselines without lexical features, achieving the best results on a Twitter benchmark",
    "checked": true,
    "id": "f1ee33068dfff064ef4018422a7fb1c6d6710711",
    "semantic_title": "",
    "citation_count": 125,
    "authors": [
      "Zhiyang Teng",
      "Duy-Tin Vo",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/D16-1170": {
    "title": "Event-Driven Emotion Cause Extraction with Corpus Construction",
    "volume": "main",
    "abstract": "In this paper, we present our work in emotion cause extraction. Since there is no open dataset available, the lack of annotated resources has limited the research in this area. Thus, we first present a dataset we built using SINA city news. The annotation is based on the scheme of the W3C Emotion Markup Language. Second, we propose a 7-tuple definition to describe emotion cause events. Based on this general definition, we propose a new event-driven emotion cause extraction method using multi-kernel SVMs where a syntactical tree based approach is used to represent events in text. A convolution kernel based multikernel SVM are used to extract emotion causes. Because traditional convolution kernels do not use lexical information at the terminal nodes of syntactic trees, we modify the kernel function with a synonym based improvement. Even with very limited training data, we can still extract sufficient features for the task. Evaluations show that our approach achieves 11.6% higher F-measure compared to referenced methods. The contributions of our work include resource construction, concept definition and algorithm development",
    "checked": true,
    "id": "120bd71c72f9477dec6b5291c32f73ae4afbf163",
    "semantic_title": "",
    "citation_count": 131,
    "authors": [
      "Lin Gui",
      "Dongyin Wu",
      "Ruifeng Xu",
      "Qin Lu",
      "Yu Zhou"
    ]
  },
  "https://aclanthology.org/D16-1171": {
    "title": "Neural Sentiment Classification with User and Product Attention",
    "volume": "main",
    "abstract": "Document-level sentiment classification aims to predict user's overall sentiment in a document about a product. However, most of existing methods only focus on local text information and ignore the global user preference and product characteristics. Even though some works take such information into account, they usually suffer from high model complexity and only consider wordlevel preference rather than semantic levels. To address this issue, we propose a hierarchical neural network to incorporate global user and product information into sentiment classification. Our model first builds a hierarchical LSTM model to generate sentence and document representations. Afterwards, user and product information is considered via attentions over different semantic levels due to its ability of capturing crucial semantic components. The experimental results show that our model achieves significant and consistent improvements compared to all state-of-theart methods. The source code of this paper can be obtained from https://github. com/thunlp/NSC",
    "checked": true,
    "id": "4ff5cdeeed3dfeeea542d463dc2f5ad64fd38281",
    "semantic_title": "",
    "citation_count": 278,
    "authors": [
      "Huimin Chen",
      "Maosong Sun",
      "Cunchao Tu",
      "Yankai Lin",
      "Zhiyuan Liu"
    ]
  },
  "https://aclanthology.org/D16-1172": {
    "title": "Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification",
    "volume": "main",
    "abstract": "Recently, neural networks have achieved great success on sentiment classification due to their ability to alleviate feature engineering. However, one of the remaining challenges is to model long texts in document-level sentiment classification under a recurrent architecture because of the deficiency of the memory unit. To address this problem, we present a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. CLSTM introduces a cache mechanism, which divides memory into several groups with different forgetting rates and thus enables the network to keep sentiment information better within a recurrent unit. The proposed CLSTM outperforms the state-of-the-art models on three publicly available document-level sentiment analysis datasets",
    "checked": true,
    "id": "9904a69eab0792859108eec6b0578d11264b8e83",
    "semantic_title": "",
    "citation_count": 143,
    "authors": [
      "Jiacheng Xu",
      "Danlu Chen",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/D16-1173": {
    "title": "Deep Neural Networks with Massive Learned Knowledge",
    "volume": "main",
    "abstract": "Regulating deep neural networks (DNNs) with human structured knowledge has shown to be of great benefit for improved accuracy and interpretability. We develop a general framework that enables learning knowledge and its confidence jointly with the DNNs, so that the vast amount of fuzzy knowledge can be incorporated and automatically optimized with little manual efforts. We apply the framework to sentence sentiment analysis, augmenting a DNN with massive linguistic constraints on discourse and polarity structures. Our model substantially enhances the performance using less training data, and shows improved interpretability. The principled framework can also be applied to posterior regularization for regulating other statistical models",
    "checked": true,
    "id": "e770e1b88351280d810967b3633731c6968ea6d0",
    "semantic_title": "",
    "citation_count": 64,
    "authors": [
      "Zhiting Hu",
      "Zichao Yang",
      "Ruslan Salakhutdinov",
      "Eric Xing"
    ]
  },
  "https://aclanthology.org/D16-1174": {
    "title": "De-Conflated Semantic Representations",
    "volume": "main",
    "abstract": "One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have. Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years. However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses. We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge it derives from a semantic network. Our approach provides multiple advantages in comparison to the past work, including its high coverage and the ability to generate accurate representations even for infrequent word senses. We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them",
    "checked": true,
    "id": "b54315a22b825e9ca1b59aa1d3fac98ea4925941",
    "semantic_title": "",
    "citation_count": 84,
    "authors": [
      "Mohammad Taher Pilehvar",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/D16-1175": {
    "title": "Improving Sparse Word Representations with Distributional Inference for Semantic Composition",
    "volume": "main",
    "abstract": "Distributional models are derived from co-occurrences in a corpus, where only a small proportion of all possible plausible co-occurrences will be observed. This results in a very sparse vector space, requiring a mechanism for inferring missing knowledge. Most methods face this challenge in ways that render the resulting word representations uninterpretable, with the consequence that semantic composition becomes hard to model. In this paper we explore an alternative which involves explicitly inferring unobserved co-occurrences using the distributional neighbourhood. We show that distributional inference improves sparse word representations on several word similarity benchmarks and demonstrate that our model is competitive with the state-of-the-art for adjective-noun, noun-noun and verb-object compositions while being fully interpretable",
    "checked": true,
    "id": "b462b3739aee93ec09deca5663aad2631820b9f3",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Thomas Kober",
      "Julie Weeds",
      "Jeremy Reffin",
      "David Weir"
    ]
  },
  "https://aclanthology.org/D16-1176": {
    "title": "Modelling Interaction of Sentence Pair with Coupled-LSTMs",
    "volume": "main",
    "abstract": "Recently, there is rising interest in modelling the interactions of two sentences with deep neural networks. However, most of the existing methods encode two sequences with separate encoders, in which a sentence is encoded with little or no information from the other sentence. In this paper, we propose a deep architecture to model the strong interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architecture and its superiority to state-of-the-art methods",
    "checked": true,
    "id": "10d89efc96beb45676149c8a3237a86a72a2116e",
    "semantic_title": "",
    "citation_count": 48,
    "authors": [
      "Pengfei Liu",
      "Xipeng Qiu",
      "Yaqian Zhou",
      "Jifan Chen",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/D16-1177": {
    "title": "Universal Decompositional Semantics on Universal Dependencies",
    "volume": "main",
    "abstract": "We present a framework for augmenting data sets from the Universal Dependencies project with Universal Decompositional Semantics. Where the Universal Dependencies project aims to provide a syntactic annotation standard that can be used consistently across many languages as well as a collection of corpora that use that standard, our extension has similar aims for semantic annotation. We describe results from annotating the English Universal Dependencies treebank, dealing with word senses, semantic roles, and event properties",
    "checked": true,
    "id": "00dc74ca39fc6630bef824a4768dd214bbd81927",
    "semantic_title": "",
    "citation_count": 131,
    "authors": [
      "Aaron Steven White",
      "Drew Reisinger",
      "Keisuke Sakaguchi",
      "Tim Vieira",
      "Sheng Zhang",
      "Rachel Rudinger",
      "Kyle Rawlins",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/D16-1178": {
    "title": "Friends with Motives: Using Text to Infer Influence on SCOTUS",
    "volume": "main",
    "abstract": "We present a probabilistic model of the influence of language on the behavior of the U.S. Supreme Court, specifically influence of amicus briefs on Court decisions and opinions. The approach assumes that amici are rational, utility-maximizing agents who try to win votes or affect the language of court opinions. Our model leads to improved predictions of justices' votes and perplexity of opinion language. It is amenable to inspection, allowing us to explore inferences about the persuasiveness of different amici and influenceability of different justices; these are consistent with earlier findings. \"Language is the central tool of our trade.\" John G. Roberts, 2007 (Garner, 2010)",
    "checked": true,
    "id": "2dbcfbca560e7fbc6b31ce86acf2a7f08892c449",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Yanchuan Sim",
      "Bryan Routledge",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/D16-1179": {
    "title": "Verb Phrase Ellipsis Resolution Using Discriminative and Margin-Infused Algorithms",
    "volume": "main",
    "abstract": "Verb Phrase Ellipsis (VPE) is an anaphoric construction in which a verb phrase has been elided. It occurs frequently in dialogue and informal conversational settings, but despite its evident impact on event coreference resolution and extraction, there has been relatively little work on computational methods for identifying and resolving VPE. Here, we present a novel approach to detecting and resolving VPE by using supervised discriminative machine learning techniques trained on features extracted from an automatically parsed, publicly available dataset. Our approach yields state-of-the-art results for VPE detection by improving F1 score by over 11%; additionally, we explore an approach to antecedent identification that uses the Margin-Infused-RelaxedAlgorithm, which shows promising results",
    "checked": true,
    "id": "7b33745e9025c08e51fa45238d73374d6f7f92e5",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Kian Kenyon-Dean",
      "Jackie Chi Kit Cheung",
      "Doina Precup"
    ]
  },
  "https://aclanthology.org/D16-1180": {
    "title": "Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser",
    "volume": "main",
    "abstract": "We introduce two first-order graph-based dependency parsers achieving a new state of the art. The first is a consensus parser built from an ensemble of independently trained greedy LSTM transition-based parsers with different random initializations. We cast this approach as minimum Bayes risk decoding (under the Hamming cost) and argue that weaker consensus within the ensemble is a useful signal of difficulty or ambiguity. The second parser is a \"distillation\" of the ensemble into a single model. We train the distillation parser using a structured hinge loss objective with a novel cost that incorporates ensemble uncertainty estimates for each possible attachment, thereby avoiding the intractable cross-entropy computations required by applying standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German",
    "checked": true,
    "id": "d43b4801ea469a71b346698bf41197ef97e97d53",
    "semantic_title": "",
    "citation_count": 74,
    "authors": [
      "Adhiguna Kuncoro",
      "Miguel Ballesteros",
      "Lingpeng Kong",
      "Chris Dyer",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/D16-1181": {
    "title": "LSTM Shift-Reduce CCG Parsing",
    "volume": "main",
    "abstract": "We describe a neural shift-reduce parsing model for CCG, factored into four unidirectional LSTMs and one bidirectional LSTM. This factorization allows the linearization of the complete parsing history, and results in a highly accurate greedy parser that outperforms all previous beam-search shift-reduce parsers for CCG. By further deriving a globally optimized model using a task-based loss, we improve over the state of the art by up to 2.67% labeled F1",
    "checked": true,
    "id": "dc61a283dc672ad9c1bdc539906a86d969a375ce",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Wenduan Xu"
    ]
  },
  "https://aclanthology.org/D16-1182": {
    "title": "An Evaluation of Parser Robustness for Ungrammatical Sentences",
    "volume": "main",
    "abstract": "For many NLP applications that require a parser, the sentences of interest may not be well-formed. If the parser can overlook problems such as grammar mistakes and produce a parse tree that closely resembles the correct analysis for the intended sentence, we say that the parser is robust. This paper compares the performances of eight state-of-the-art dependency parsers on two domains of ungrammatical sentences: learner English and machine translation outputs. We have developed an evaluation metric and conducted a suite of experiments. Our analyses may help practitioners to choose an appropriate parser for their tasks, and help developers to improve parser robustness against ungrammatical sentences",
    "checked": true,
    "id": "f735122727511fe7593401099012e34bf51df4e5",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Homa B. Hashemi",
      "Rebecca Hwa"
    ]
  },
  "https://aclanthology.org/D16-1183": {
    "title": "Neural Shift-Reduce CCG Semantic Parsing",
    "volume": "main",
    "abstract": "We present a shift-reduce CCG semantic parser. Our parser uses a neural network architecture that balances model capacity and computational cost. We train by transferring a model from a computationally expensive loglinear CKY parser. Our learner addresses two challenges: selecting the best parse for learning when the CKY parser generates multiple correct trees, and learning from partial derivations when the CKY parser fails to parse. We evaluate on AMR parsing. Our parser performs comparably to the CKY parser, while doing significantly fewer operations. We also present results for greedy semantic parsing with a relatively small drop in performance",
    "checked": true,
    "id": "bf4dc8112c00cddd87c5f6110dae5efc305b1a27",
    "semantic_title": "",
    "citation_count": 40,
    "authors": [
      "Dipendra Kumar Misra",
      "Yoav Artzi"
    ]
  },
  "https://aclanthology.org/D16-1184": {
    "title": "Syntactic Parsing of Web Queries",
    "volume": "main",
    "abstract": "Syntactic parsing of web queries is important for query understanding. However, web queries usually do not observe the grammar of a written language, and no labeled syntactic trees for web queries are available. In this paper, we focus on a query's clicked sentence, i.e., a well-formed sentence that i) contains all the tokens of the query, and ii) appears in the query's top clicked web pages. We argue such sentences are semantically consistent with the query. We introduce algorithms to derive a query's syntactic structure from the dependency trees of its clicked sentences. This gives us a web query treebank without manual labeling. We then train a dependency parser on the treebank. Our model achieves much better UAS (0.86) and LAS (0.80) scores than state-of-the-art parsers on web queries",
    "checked": true,
    "id": "606f4dea3f2457f7a7fd237c025bb79142490d2a",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Xiangyan Sun",
      "Haixun Wang",
      "Yanghua Xiao",
      "Zhongyuan Wang"
    ]
  },
  "https://aclanthology.org/D16-1185": {
    "title": "Unsupervised Text Recap Extraction for TV Series",
    "volume": "main",
    "abstract": "Sequences found at the beginning of TV shows help the audience absorb the essence of previous episodes, and grab their attention with upcoming plots. In this paper, we propose a novel task, text recap extraction. Compared with conventional summarization, text recap extraction captures the duality of summarization and plot contingency between adjacent episodes. We present a new dataset, TVRecap, for text recap extraction on TV shows. We propose an unsupervised model that identifies text recaps based on plot descriptions. We introduce two contingency factors, concept coverage and sparse reconstruction, that encourage recaps to prompt the upcoming story development. We also propose a multi-view extension of our model which can incorporate dialogues and synopses. We conduct extensive experiments on TVRecap, and conclude that our model outperforms summarization approaches",
    "checked": true,
    "id": "dfff89390ac3bd22e4153e0639d4b7382611fb0c",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Hongliang Yu",
      "Shikun Zhang",
      "Louis-Philippe Morency"
    ]
  },
  "https://aclanthology.org/D16-1186": {
    "title": "On- and Off-Topic Classification and Semantic Annotation of User-Generated Software Requirements",
    "volume": "main",
    "abstract": "Users prefer natural language software requirements because of their usability and accessibility. When they describe their wishes for software development, they often provide off-topic information. We therefore present an automated approach for identifying and semantically annotating the on-topic parts of the given descriptions. It is designed to support requirement engineers in the requirement elicitation process on detecting and analyzing requirements in user-generated content. Since no lexical resources with domain-specific information about requirements are available, we created a corpus of requirements written in controlled language by instructed users and uncontrolled language by uninstructed users. We annotated these requirements regarding predicate-argument structures, conditions, priorities, motivations and semantic roles and used this information to train classifiers for information extraction purposes. The approach achieves an accuracy of 92% for the on- and off-topic classification task and an F1-measure of 72% for the semantic annotation",
    "checked": true,
    "id": "16e4dcf649a17589082f8bedb61c987141ebbb28",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Markus Dollmann",
      "Michaela Geierhos"
    ]
  },
  "https://aclanthology.org/D16-1187": {
    "title": "Deceptive Review Spam Detection via Exploiting Task Relatedness and Unlabeled Data",
    "volume": "main",
    "abstract": "Existing work on detecting deceptive reviews primarily focuses on feature engineering and applies off-the-shelf supervised classification algorithms to the problem. Then, one real challenge would be to manually recognize plentiful ground truth spam review data for model building, which is rather difficult and often requires domain expertise in practice. In this paper, we propose to exploit the relatedness of multiple review spam detection tasks and readily available unlabeled data to address the scarcity of labeled opinion spam data. We first develop a multi-task learning method based on logistic regression (MTL-LR), which can boost the learning for a task by sharing the knowledge contained in the training signals of other related tasks. To leverage the unlabeled data, we introduce a graph Laplacian regularizer into each base model. We then propose a novel semi-supervised multitask learning method via Laplacian regularized logistic regression (SMTL-LLR) to further improve the review spam detection performance. We also develop a stochastic alternating method to cope with the optimization for SMTL-LLR. Experimental results on real-world review data demonstrate the benefit of SMTL-LLR over several well-established baseline methods",
    "checked": true,
    "id": "3b79a2e7e4331b89d5e3feb023908339f1a41a09",
    "semantic_title": "",
    "citation_count": 52,
    "authors": [
      "Zhen Hai",
      "Peilin Zhao",
      "Peng Cheng",
      "Peng Yang",
      "Xiao-Li Li",
      "Guangxia Li"
    ]
  },
  "https://aclanthology.org/D16-1188": {
    "title": "Regularizing Text Categorization with Clusters of Words",
    "volume": "main",
    "abstract": "Regularization is a critical step in supervised learning to not only address overfitting, but also to take into account any prior knowledge we may have on the features and their dependence. In this paper, we explore stateof-the-art structured regularizers and we propose novel ones based on clusters of words from LSI topics, word2vec embeddings and graph-of-words document representation. We show that our proposed regularizers are faster than the state-of-the-art ones and still improve text classification accuracy. Code and data are available online1",
    "checked": true,
    "id": "3a9204c4c41f46954b17a9c682a18a0924917e8e",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Konstantinos Skianis",
      "François Rousseau",
      "Michalis Vazirgiannis"
    ]
  },
  "https://aclanthology.org/D16-1189": {
    "title": "Deep Reinforcement Learning with a Combinatorial Action Space for Predicting Popular Reddit Threads",
    "volume": "main",
    "abstract": "We introduce an online popularity prediction and tracking task as a benchmark task for reinforcement learning with a combinatorial, natural language action space. A specified number of discussion threads predicted to be popular are recommended, chosen from a fixed window of recent comments to track. Novel deep reinforcement learning architectures are studied for effective modeling of the value function associated with actions comprised of interdependent sub-actions. The proposed model, which represents dependence between sub-actions through a bi-directional LSTM, gives the best performance across different experimental configurations and domains, and it also generalizes well with varying numbers of recommendation requests",
    "checked": true,
    "id": "5ebe658f62d02b871df66a0563b9f8f5c82272ca",
    "semantic_title": "",
    "citation_count": 49,
    "authors": [
      "Ji He",
      "Mari Ostendorf",
      "Xiaodong He",
      "Jianshu Chen",
      "Jianfeng Gao",
      "Lihong Li",
      "Li Deng"
    ]
  },
  "https://aclanthology.org/D16-1190": {
    "title": "Non-Literal Text Reuse in Historical Texts: An Approach to Identify Reuse Transformations and its Application to Bible Reuse",
    "volume": "main",
    "abstract": "Text reuse refers to citing, copying or alluding text excerpts from a text resource to a new context. While detecting reuse in contemporary languages is well supported—given extensive research, techniques, and corpora— automatically detecting historical text reuse is much more difficult. Corpora of historical languages are less documented and often encompass various genres, linguistic varieties, and topics. In fact, historical text reuse detection is much less understood and empirical studies are necessary to enable and improve its automation. We present a linguistic analysis of text reuse in two ancient data sets. We contribute an automated approach to analyze how an original text was transformed into its reuse, taking linguistic resources into account to understand how they help characterizing the transformation. It is complemented by a manual analysis of a subset of the reuse. Our results show the limitations of approaches focusing on literal reuse detection. Yet, linguistic resources can effectively support understanding the non-literal text reuse transformation process. Our results support practitioners and researchers working on understanding and detecting historical reuse",
    "checked": true,
    "id": "42a129b0fb95b4a466f68bc9c697a2028d7ef9a3",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Maria Moritz",
      "Andreas Wiederhold",
      "Barbara Pavlek",
      "Yuri Bizzoni",
      "Marco Büchler"
    ]
  },
  "https://aclanthology.org/D16-1191": {
    "title": "A Graph Degeneracy-based Approach to Keyword Extraction",
    "volume": "main",
    "abstract": "We operate a change of paradigm and hypothesize that keywords are more likely to be found among influential nodes of a graph-ofwords rather than among its nodes high on eigenvector-related centrality measures. To test this hypothesis, we introduce unsupervised techniques that capitalize on graph degeneracy. Our methods strongly and significantly outperform all baselines on two datasets (short and medium size documents), and reach best performance on the third one (long documents)",
    "checked": true,
    "id": "0c22933a45594cec626678e828269e85e28294b0",
    "semantic_title": "",
    "citation_count": 76,
    "authors": [
      "Antoine Tixier",
      "Fragkiskos Malliaros",
      "Michalis Vazirgiannis"
    ]
  },
  "https://aclanthology.org/D16-1192": {
    "title": "Predicting the Relative Difficulty of Single Sentences With and Without Surrounding Context",
    "volume": "main",
    "abstract": "The problem of accurately predicting relative reading difficulty across a set of sentences arises in a number of important natural language applications, such as finding and curating effective usage examples for intelligent language tutoring systems. Yet while significant research has explored document- and passage-level reading difficulty, the special challenges involved in assessing aspects of readability for single sentences have received much less attention, particularly when considering the role of surrounding passages. We introduce and evaluate a novel approach for estimating the relative reading difficulty of a set of sentences, with and without surrounding context. Using different sets of lexical and grammatical features, we explore models for predicting pairwise relative difficulty using logistic regression, and examine rankings generated by aggregating pairwise difficulty labels using a Bayesian rating system to form a final ranking. We also compare rankings derived for sentences assessed with and without context, and find that contextual features can help predict differences in relative difficulty judgments across these two conditions",
    "checked": true,
    "id": "c233928d5701816fb5c2ff7409b3b5d7150b136c",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Elliot Schumacher",
      "Maxine Eskenazi",
      "Gwen Frishkoff",
      "Kevyn Collins-Thompson"
    ]
  },
  "https://aclanthology.org/D16-1193": {
    "title": "A Neural Approach to Automated Essay Scoring",
    "volume": "main",
    "abstract": "Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. The performance of such systems is tightly bound to the quality of the underlying features. However, it is laborious to manually design the most informative features for such a system. In this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. The results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6% in terms of quadratic weighted Kappa, without requiring any feature engineering",
    "checked": true,
    "id": "aea14f23a951975f605a981d003386e46bf8acfe",
    "semantic_title": "",
    "citation_count": 293,
    "authors": [
      "Kaveh Taghipour",
      "Hwee Tou Ng"
    ]
  },
  "https://aclanthology.org/D16-1194": {
    "title": "Non-uniform Language Detection in Technical Writing",
    "volume": "main",
    "abstract": "Technical writing in professional environments, such as user manual authoring, requires the use of uniform language. Nonuniform language detection is a novel task, which aims to guarantee the consistency for technical writing by detecting sentences in a document that are intended to have the same meaning within a similar context but use different words or writing style. This paper proposes an approach that utilizes text similarity algorithms at lexical, syntactic, semantic and pragmatic levels. Different features are extracted and integrated by applying a machine learning classification method. We tested our method using smart phone user manuals, and compared its performance against the state-ofthe-art methods in a related area. The experiments demonstrate that our approach achieves the upper bound performance for this task",
    "checked": true,
    "id": "6f3422fc63755a1ea07f667870e4087eb09382b5",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Weibo Wang",
      "Abidalrahman Moh’d",
      "Aminul Islam",
      "Axel Soto",
      "Evangelos Milios"
    ]
  },
  "https://aclanthology.org/D16-1195": {
    "title": "Adapting Grammatical Error Correction Based on the Native Language of Writers with Neural Network Joint Models",
    "volume": "main",
    "abstract": "An important aspect for the task of grammatical error correction (GEC) that has not yet been adequately explored is adaptation based on the native language (L1) of writers, despite the marked influences of L1 on second language (L2) writing. In this paper, we adapt a neural network joint model (NNJM) using L1-specific learner text and integrate it into a statistical machine translation (SMT) based GEC system. Specifically, we train an NNJM on general learner text (not L1-specific) and subsequently train on L1-specific data using a Kullback-Leibler divergence regularized objective function in order to preserve generalization of the model. We incorporate this adapted NNJM as a feature in an SMT-based English GEC system and show that adaptation achieves significant F0.5 score gains on English texts written by L1 Chinese, Russian, and Spanish writers",
    "checked": true,
    "id": "b9067abeda0b6376fc0add64feb432e2b9b1351d",
    "semantic_title": "",
    "citation_count": 37,
    "authors": [
      "Shamil Chollampatt",
      "Duc Tam Hoang",
      "Hwee Tou Ng"
    ]
  },
  "https://aclanthology.org/D16-1196": {
    "title": "Orthographic Syllable as basic unit for SMT between Related Languages",
    "volume": "main",
    "abstract": "We explore the use of the orthographic syllable, a variable-length consonant-vowel sequence, as a basic unit of translation between related languages which use abugida or alphabetic scripts. We show that orthographic syllable level translation significantly outperforms models trained over other basic units (word, morpheme and character) when training over small parallel corpora",
    "checked": true,
    "id": "8da1c2acdb8174f16566606d8d8b7bf3870d649a",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Anoop Kunchukuttan",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/D16-1197": {
    "title": "Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge",
    "volume": "main",
    "abstract": "This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models",
    "checked": true,
    "id": "74157ae408173bf713f1e94f15aca1475c43bd74",
    "semantic_title": "",
    "citation_count": 73,
    "authors": [
      "Nicholas Locascio",
      "Karthik Narasimhan",
      "Eduardo DeLeon",
      "Nate Kushman",
      "Regina Barzilay"
    ]
  },
  "https://aclanthology.org/D16-1198": {
    "title": "Supervised Keyphrase Extraction as Positive Unlabeled Learning",
    "volume": "main",
    "abstract": "The problem of noisy and unbalanced training data for supervised keyphrase extraction results from the subjectivity of keyphrase assignment, which we quantify by crowdsourcing keyphrases for news and fashion magazine articles with many annotators per document. We show that annotators exhibit substantial disagreement, meaning that single annotator data could lead to very different training sets for supervised keyphrase extractors. Thus, annotations from single authors or readers lead to noisy training data and poor extraction performance of the resulting supervised extractor. We provide a simple but effective solution to still work with such data by reweighting the importance of unlabeled candidate phrases in a two stage Positive Unlabeled Learning setting. We show that performance of trained keyphrase extractors approximates a classifier trained on articles labeled by multiple annotators, leading to higher average F1scores and better rankings of keyphrases. We apply this strategy to a variety of test collections from different backgrounds and show improvements over strong baseline models",
    "checked": true,
    "id": "95d8ff7cd82d9626a58339ebb3f59ac08127ebdf",
    "semantic_title": "",
    "citation_count": 44,
    "authors": [
      "Lucas Sterckx",
      "Cornelia Caragea",
      "Thomas Demeester",
      "Chris Develder"
    ]
  },
  "https://aclanthology.org/D16-1199": {
    "title": "Learning to Answer Questions from Wikipedia Infoboxes",
    "volume": "main",
    "abstract": "A natural language interface to answers on the Web can help us access information more efficiently. We start with an interesting source of information—infoboxes in Wikipedia that summarize factoid knowledge—and develop a comprehensive approach to answering questions with high precision. We first build a system to access data in infoboxes in a structured manner. We use our system to construct a crowdsourced dataset of over 15,000 highquality, diverse questions. With these questions, we train a convolutional neural network model that outperforms models that achieve top results in similar answer selection tasks",
    "checked": true,
    "id": "91a95f979f22172746c42ab436391674968c4983",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Alvaro Morales",
      "Varot Premtoon",
      "Cordelia Avery",
      "Sue Felshin",
      "Boris Katz"
    ]
  },
  "https://aclanthology.org/D16-1200": {
    "title": "Timeline extraction using distant supervision and joint inference",
    "volume": "main",
    "abstract": "In timeline extraction the goal is to order all the events in which a target entity is involved in a timeline. Due to the lack of explicitly annotated data, previous work is primarily rule-based and uses pre-trained temporal linking systems. In this work, we propose a distantly supervised approach by heuristically aligning timelines with documents. The noisy training data created allows us to learn models that anchor events to temporal expressions and entities; during testing, the predictions of these models are combined to produce the timeline. Furthermore, we show how to improve performance using joint inference. In experiments in the SemEval-2015 TimeLine task we show that our distantly supervised approach matches the state-of-the-art performance while joint inference further improves on it by 3.2 F-score points",
    "checked": true,
    "id": "520e82c0f35a14ecf78b93de3673bb8b2a3212fc",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Savelie Cornegruta",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/D16-1201": {
    "title": "Combining Supervised and Unsupervised Enembles for Knowledge Base Population",
    "volume": "main",
    "abstract": "We propose an algorithm that combines supervised and unsupervised methods to ensemble multiple systems for two popular Knowledge Base Population (KBP) tasks, Cold Start Slot Filling (CSSF) and Tri-lingual Entity Discovery and Linking (TEDL). We demonstrate that it outperforms the best system for both tasks in the 2015 competition, several ensembling baselines, as well as a state-of-the-art stacking approach. The success of our technique on two different and challenging problems demonstrates the power and generality of our combined approach to ensembling",
    "checked": true,
    "id": "07eece3e33bf5c9ec88710ec5c0ede98a2dae1b1",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Nazneen Fatema Rajani",
      "Raymond Mooney"
    ]
  },
  "https://aclanthology.org/D16-1202": {
    "title": "Character Sequence Models for Colorful Words",
    "volume": "main",
    "abstract": "We present a neural network architecture to predict a point in color space from the sequence of characters in the color's name. Using large scale color--name pairs obtained from an online color design forum, we evaluate our model on a \"color Turing test\" and find that, given a name, the colors predicted by our model are preferred by annotators to color names created by humans. Our datasets and demo system are available online at colorlab.us",
    "checked": true,
    "id": "805fe1ce2e51f21c7116496407907ecfb2ee60a8",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Kazuya Kawakami",
      "Chris Dyer",
      "Bryan Routledge",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/D16-1203": {
    "title": "Analyzing the Behavior of Visual Question Answering Models",
    "volume": "main",
    "abstract": "Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models -- with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016.   Our behavior analysis reveals that despite recent progress, today's VQA models are \"myopic\" (tend to fail on sufficiently novel instances), often \"jump to conclusions\" (converge on a predicted answer after 'listening' to just half the question), and are \"stubborn\" (do not change their answers across images)",
    "checked": true,
    "id": "8e759195eb4b4f0f480a8a2cf1c629bfd881d4e5",
    "semantic_title": "",
    "citation_count": 262,
    "authors": [
      "Aishwarya Agrawal",
      "Dhruv Batra",
      "Devi Parikh"
    ]
  },
  "https://aclanthology.org/D16-1204": {
    "title": "Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text",
    "volume": "main",
    "abstract": "This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality",
    "checked": true,
    "id": "d1ffd519ff274517ec6fd014ae67af0d0c68a969",
    "semantic_title": "",
    "citation_count": 113,
    "authors": [
      "Subhashini Venugopalan",
      "Lisa Anne Hendricks",
      "Raymond Mooney",
      "Kate Saenko"
    ]
  },
  "https://aclanthology.org/D16-1205": {
    "title": "Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity",
    "volume": "main",
    "abstract": "Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, thereby loosing important information for word expectations, such as word interrelations. In this paper, we present a DSM that addresses this issue by defining verb contexts as joint syntactic dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts achieve performances comparable to single dependencies or even better. Moreover, they are able to overcome the data sparsity problem of joint feature spaces, in spite of the limited size of our training corpus",
    "checked": true,
    "id": "11802a2935e46be5a87db0f9bea8c98b1dc1cd54",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Emmanuele Chersoni",
      "Enrico Santus",
      "Alessandro Lenci",
      "Philippe Blache",
      "Chu-Ren Huang"
    ]
  },
  "https://aclanthology.org/D16-1206": {
    "title": "Speed-Accuracy Tradeoffs in Tagging with Variable-Order CRFs and Structured Sparsity",
    "volume": "main",
    "abstract": "We propose a method for learning the structure of variable-order CRFs, a more flexible variant of higher-order linear-chain CRFs. Variableorder CRFs achieve faster inference by including features for only some of the tag ngrams. Our learning method discovers the useful higher-order features at the same time as it trains their weights, by maximizing an objective that combines log-likelihood with a structured-sparsity regularizer. An active-set outer loop allows the feature set to grow as far as needed. On part-of-speech tagging in 5 randomly chosen languages from the Universal Dependencies dataset, our method of shrinking the model achieved a 2–6x speedup over a baseline, with no significant drop in accuracy",
    "checked": true,
    "id": "29dfd68b03505ef981f5b36ea3c5a5a53dff0c0b",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Tim Vieira",
      "Ryan Cotterell",
      "Jason Eisner"
    ]
  },
  "https://aclanthology.org/D16-1207": {
    "title": "Learning Robust Representations of Text",
    "volume": "main",
    "abstract": "Deep neural networks have achieved remarkable results across many language processing tasks, however these methods are highly sensitive to noise and adversarial attacks. We present a regularization based method for limiting network sensitivity to its inputs, inspired by ideas from computer vision, thus learning models that are more robust. Empirical evaluation over a range of sentiment datasets with a convolutional neural network shows that, compared to a baseline model and the dropout method, our method achieves superior performance over noisy inputs and out-of-domain data",
    "checked": true,
    "id": "57567039998db7ef670c6ee6837301402d72c078",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Yitong Li",
      "Trevor Cohn",
      "Timothy Baldwin"
    ]
  },
  "https://aclanthology.org/D16-1208": {
    "title": "Modified Dirichlet Distribution: Allowing Negative Parameters to Induce Stronger Sparsity",
    "volume": "main",
    "abstract": "The Dirichlet distribution (Dir) is one of the most widely used prior distributions in statistical approaches to natural language processing. The parameters of Dir are required to be positive, which significantly limits its strength as a sparsity prior. In this paper, we propose a simple modification to the Dirichlet distribution that allows the parameters to be negative. Our modified Dirichlet distribution (mDir) not only induces much stronger sparsity, but also simultaneously performs smoothing. mDir is still conjugate to the multinomial distribution, which simplifies posterior inference. We introduce two simple and efficient algorithms for finding the mode of mDir. Our experiments on learning Gaussian mixtures and unsupervised dependency parsing demonstrate the advantage of mDir over Dir. © 2016 Association for Computational Linguistics",
    "checked": true,
    "id": "4fc20939aa4b51889d08ae81b817c984e0aa6d36",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/D16-1209": {
    "title": "Gated Word-Character Recurrent Language Model",
    "volume": "main",
    "abstract": "We introduce a recurrent neural network language model (RNN-LM) with long short-term memory (LSTM) units that utilizes both character-level and word-level inputs. Our model has a gate that adaptively finds the optimal mixture of the character-level and word-level inputs. The gate creates the final vector representation of a word by combining two distinct representations of the word. The character-level inputs are converted into vector representations of words using a bidirectional LSTM. The word-level inputs are projected into another high-dimensional space by a word lookup table. The final vector representations of words are used in the LSTM language model which predicts the next word given all the preceding words. Our model with the gating mechanism effectively utilizes the character-level inputs for rare and out-of-vocabulary words and outperforms word-level language models on several English corpora",
    "checked": true,
    "id": "58001259d2f6442b07cc0d716ff99899abbb2bc7",
    "semantic_title": "",
    "citation_count": 98,
    "authors": [
      "Yasumasa Miyamoto",
      "Kyunghyun Cho"
    ]
  },
  "https://aclanthology.org/D16-1210": {
    "title": "Unsupervised Word Alignment by Agreement Under ITG Constraint",
    "volume": "main",
    "abstract": "We propose a novel unsupervised word alignment method that uses a constraint based on Inversion Transduction Grammar (ITG) parse trees to jointly unify two directional models. Previous agreement methods are not helpful for locating alignments with long distances because they do not use any syntactic structures. In contrast, the proposed method symmetrizes alignments in consideration of their structural coherence by using the ITG constraint softly in the posterior regularization framework (Ganchev et al., 2010). The ITG constraint is also compatible with word alignments that are not covered by ITG parse trees. Hence, the proposed method is robust to ITG parse errors compared to other alignment methods that directly use an ITG model. Compared to the HMM (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), and the baseline agreement method (Ganchev et al., 2010), the experimental results show that the proposed method significantly improves alignment performance regarding the Japanese-English KFTT and BTEC corpus, and in translation evaluation, the proposed method shows comparable or statistical significantly better performance on the JapaneseEnglish KFTT and IWSLT 2007 corpus",
    "checked": true,
    "id": "383e8b0c887cce0192b4cd944c61585af59e5d2c",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Hidetaka Kamigaito",
      "Akihiro Tamura",
      "Hiroya Takamura",
      "Manabu Okumura",
      "Eiichiro Sumita"
    ]
  },
  "https://aclanthology.org/D16-1211": {
    "title": "Training with Exploration Improves a Greedy Stack LSTM Parser",
    "volume": "main",
    "abstract": "We adapt the greedy Stack-LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles(Goldberg and Nivre, 2013) instead of cross-entropy minimization. This form of training, which accounts for model predictions at training time rather than assuming an error-free action history, improves parsing accuracies for both English and Chinese, obtaining very strong results for both languages. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural-network",
    "checked": true,
    "id": "6789e0dbd294cccb3b7dd4e001c9e8ba4813f334",
    "semantic_title": "",
    "citation_count": 76,
    "authors": [
      "Miguel Ballesteros",
      "Yoav Goldberg",
      "Chris Dyer",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/D16-1212": {
    "title": "Capturing Argument Relationship for Chinese Semantic Role Labeling",
    "volume": "main",
    "abstract": "In this paper, we capture the argument relationships for Chinese semantic role labeling task, and improve the task's performance with the help of argument relationships. We split the relationship between two candidate arguments into two categories: (1) Compatible arguments: if one candidate argument belongs to a given predicate, then the other is more likely to belong to the same predicate; (2) Incompatible arguments: if one candidate argument belongs to a given predicate, then the other is less likely to belong to the same predicate. However, previous works did not explicitly model argument relationships. We use a simple maximum entropy classifier to capture the two categories of argument relationships and test its performance on the Chinese Proposition Bank (CPB). The experiments show that argument relationships is effective in Chinese semantic role labeling task",
    "checked": true,
    "id": "b991625f3c25aaf5f82eebfa6405a017a16a1695",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Lei Sha",
      "Sujian Li",
      "Baobao Chang",
      "Zhifang Sui",
      "Tingsong Jiang"
    ]
  },
  "https://aclanthology.org/D16-1213": {
    "title": "BrainBench: A Brain-Image Test Suite for Distributional Semantic Models",
    "volume": "main",
    "abstract": "The brain is the locus of our language ability, and so brain images can be used to ground linguistic theories. Here we introduce BrainBench, a lightweight system for testing distributional models of word semantics. We compare the performance of several models, and show that the performance on brain-image tasks differs from the performance on behavioral tasks. We release our benchmark test as part of a web service",
    "checked": true,
    "id": "95e0308c4dbc98f27c0645c7d87204c4d7be33af",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Haoyan Xu",
      "Brian Murphy",
      "Alona Fyshe"
    ]
  },
  "https://aclanthology.org/D16-1214": {
    "title": "Evaluating Induced CCG Parsers on Grounded Semantic Parsing",
    "volume": "main",
    "abstract": "We compare the effectiveness of four different syntactic CCG parsers for a semantic slot-filling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation provides a unique window to explore the strengths and weaknesses of semantics captured by unsupervised grammar induction systems. We release a new Freebase semantic parsing dataset called SPADES (Semantic PArsing of DEclarative Sentences) containing 93K cloze-style questions paired with answers. We evaluate all our models on this dataset. Our code and data are available at this https URL",
    "checked": true,
    "id": "c72cdb5ce7e0911c7f442ab503652d6fdeef35e0",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Yonatan Bisk",
      "Siva Reddy",
      "John Blitzer",
      "Julia Hockenmaier",
      "Mark Steedman"
    ]
  },
  "https://aclanthology.org/D16-1215": {
    "title": "Vector-space models for PPDB paraphrase ranking in context",
    "volume": "main",
    "abstract": "The PPDB is an automatically built database which contains millions of paraphrases in different languages. Paraphrases in this resource are associated with features that serve to their ranking and reflect paraphrase quality. This context-unaware ranking captures the semantic similarity of paraphrases but cannot serve to estimate their adequacy in specific contexts. We propose to use vector-space semantic models for selecting PPDB paraphrases that preserve the meaning of specific text fragments. This is the first work that addresses the substitutability of PPDB paraphrases in context. We show that vector-space models of meaning can be successfully applied to this task and increase the benefit brought by the use of the PPDB resource in applications",
    "checked": true,
    "id": "90f5f51b819f98cf4fd6a5a09caa45844fc06636",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Marianna Apidianaki"
    ]
  },
  "https://aclanthology.org/D16-1216": {
    "title": "Interpreting Neural Networks to Improve Politeness Comprehension",
    "volume": "main",
    "abstract": "We present an interpretable neural network approach to predicting and understanding politeness in natural language requests. Our models are based on simple convolutional neural networks directly on raw text, avoiding any manual identification of complex sentiment or syntactic features, while performing better than such feature-based models from previous work. More importantly, we use the challenging task of politeness prediction as a testbed to next present a much-needed understanding of what these successful networks are actually learning. For this, we present several network visualizations based on activation clusters, first derivative saliency, and embedding space transformations, helping us automatically identify several subtle linguistics markers of politeness theories. Further, this analysis reveals multiple novel, high-scoring politeness strategies which, when added back as new features, reduce the accuracy gap between the original featurized system and the neural model, thus providing a clear quantitative interpretation of the success of these neural networks",
    "checked": true,
    "id": "b3c8d8aa2be2ba5a271c9be98dca2ccbc31e88f1",
    "semantic_title": "",
    "citation_count": 53,
    "authors": [
      "Malika Aubakirova",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/D16-1217": {
    "title": "Does ‘well-being' translate on Twitter?",
    "volume": "main",
    "abstract": "We investigate whether psychological wellbeing translates across English and Spanish Twitter, by building and comparing source language and automatically translated weighted lexica in English and Spanish. We find that the source language models perform substantially better than the machine translated versions. Moreover, manually correcting translation errors does not improve model performance, suggesting that meaningful cultural information is being lost in translation. Further work is needed to clarify when automatic translation of well-being lexica is effective and how it can be improved for crosscultural analysis",
    "checked": true,
    "id": "74afe33b8c2248ff27e7bb9672f9a246e13052b4",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Laura Smith",
      "Salvatore Giorgi",
      "Rishi Solanki",
      "Johannes Eichstaedt",
      "H. Andrew Schwartz",
      "Muhammad Abdul-Mageed",
      "Anneke Buffone",
      "Lyle Ungar"
    ]
  },
  "https://aclanthology.org/D16-1218": {
    "title": "Beyond Canonical Texts: A Computational Analysis of Fanfiction",
    "volume": "main",
    "abstract": "While much computational work on fiction has focused on works in the literary canon, user-created fanfiction presents a unique opportunity to study an ecosystem of literary production and consumption, embodying qualities both of large-scale literary data (55 billion tokens) and also a social network (with over 2 million users). We present several empirical analyses of this data in order to illustrate the range of affordances it presents to research in NLP, computational social science and the digital humanities. We find that fanfiction deprioritizes main protagonists in comparison to canonical texts, has a statistically significant difference in attention allocated to female characters, and offers a framework for developing models of reader reactions to stories",
    "checked": true,
    "id": "55dd3ee73a78819d974176425c3a35dbe9359063",
    "semantic_title": "",
    "citation_count": 23,
    "authors": [
      "Smitha Milli",
      "David Bamman"
    ]
  },
  "https://aclanthology.org/D16-1219": {
    "title": "Using Syntactic and Semantic Context to Explore Psychodemographic Differences in Self-reference",
    "volume": "main",
    "abstract": "Psychological analysis of language has repeatedly shown that an individual's rate of mentioning 1st person singular pronouns predicts a wealth of important demographic and psychological factors. However, these analyses are performed out of context — syntactic and semantic — which may change the magnitude or even direction of such relationships. In this paper, we put \"pronouns in their context\", exploring the relationship between self-reference and age, gender, and depression depending on syntactic position and verbal governor. We find that pronouns are overall more predictive when taking dependency relations and verb semantic categories into account, and, the direction of the relationship can change depending on the semantic class of the verbal governor",
    "checked": true,
    "id": "0d5855a9fb5809c7f715a1c3dd9358f39b9f3ce4",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Masoud Rouhizadeh",
      "Lyle Ungar",
      "Anneke Buffone",
      "H Andrew Schwartz"
    ]
  },
  "https://aclanthology.org/D16-1220": {
    "title": "Learning to Identify Metaphors from a Corpus of Proverbs",
    "volume": "main",
    "abstract": "In this paper, we experiment with a resource consisting of metaphorically annotated proverbs on the task of word-level metaphor recognition. We observe that existing feature sets do not perform well on this data. We design a novel set of features to better capture the peculiar nature of proverbs and we demonstrate that these new features are significantly more effective on the metaphorically dense proverb data",
    "checked": true,
    "id": "9a23eda0f9067bddbabe38a71318ee245494b106",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Gözde Özbal",
      "Carlo Strapparava",
      "Serra Sinem Tekiroğlu",
      "Daniele Pighin"
    ]
  },
  "https://aclanthology.org/D16-1221": {
    "title": "An Embedding Model for Predicting Roll-Call Votes",
    "volume": "main",
    "abstract": "We develop a novel embedding-based model for predicting legislative roll-call votes from bill text. The model introduces multidimensional ideal vectors for legislators as an alternative to single dimensional ideal point models for quantitatively analyzing roll-call data. These vectors are learned to correspond with pre-trained word embeddings which allows us to analyze which features in a bill text are most predictive of political support. Our model is quite simple, while at the same time allowing us to successfully predict legislator votes on specific bills with higher accuracy than past methods",
    "checked": true,
    "id": "bc297b4f88a75ecbfd20c3288e2aca855d02d22a",
    "semantic_title": "",
    "citation_count": 30,
    "authors": [
      "Peter Kraft",
      "Hirsh Jain",
      "Alexander M. Rush"
    ]
  },
  "https://aclanthology.org/D16-1222": {
    "title": "Natural Language Model Re-usability for Scaling to Different Domains",
    "volume": "main",
    "abstract": "Natural language understanding is the coreof the human computer interactions. However,building new domains and tasks thatneed a separate set of models is a bottleneckfor scaling to a large number of domainsand experiences. In this paper, wepropose a practical technique that addressesthis issue in a web-scale language understandingsystem: Microsoft's personal digital assistantCortana. The proposed technique usesa constrained decoding method with a universalslot tagging model sharing the sameschema as the collection of slot taggers builtfor each domain. The proposed approach allowsreusing of slots across different domainsand tasks while achieving virtually the sameperformance as those slot taggers trained perdomain fashion",
    "checked": true,
    "id": "8ab6df876566c2f747bc3757f938299db5c81cca",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Young-Bum Kim",
      "Alexandre Rochette",
      "Ruhi Sarikaya"
    ]
  },
  "https://aclanthology.org/D16-1223": {
    "title": "Leveraging Sentence-level Information with Encoder LSTM for Semantic Slot Filling",
    "volume": "main",
    "abstract": "Recurrent Neural Network (RNN) and one of its specific architectures, Long Short-Term Memory (LSTM), have been widely used for sequence labeling. In this paper, we first enhance LSTM-based sequence labeling to explicitly model label dependencies. Then we propose another enhancement to incorporate the global information spanning over the whole input sequence. The latter proposed method, encoder-labeler LSTM, first encodes the whole input sequence into a fixed length vector with the encoder LSTM, and then uses this encoded vector as the initial state of another LSTM for sequence labeling. Combining these methods, we can predict the label sequence with considering label dependencies and information of whole input sequence. In the experiments of a slot filling task, which is an essential component of natural language understanding, with using the standard ATIS corpus, we achieved the state-of-the-art F1-score of 95.66%",
    "checked": true,
    "id": "58e2c9af296e710740a4268d87ac8cc308ca90a1",
    "semantic_title": "",
    "citation_count": 115,
    "authors": [
      "Gakuto Kurata",
      "Bing Xiang",
      "Bowen Zhou",
      "Mo Yu"
    ]
  },
  "https://aclanthology.org/D16-1224": {
    "title": "AMR-to-text generation as a Traveling Salesman Problem",
    "volume": "main",
    "abstract": "The task of AMR-to-text generation is to generate grammatical text that sustains the semantic meaning for a given AMR graph. We at- tack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution. The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset",
    "checked": true,
    "id": "0e367d898c9701f09ec3205b39bb19aa677c751f",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Linfeng Song",
      "Yue Zhang",
      "Xiaochang Peng",
      "Zhiguo Wang",
      "Daniel Gildea"
    ]
  },
  "https://aclanthology.org/D16-1225": {
    "title": "Learning to Capitalize with Character-Level Recurrent Neural Networks: An Empirical Study",
    "volume": "main",
    "abstract": "In this paper, we investigate case restoration for text without case information. Previous such work operates at the word level. We propose an approach using character-level recurrent neural networks (RNN), which performs competitively compared to language modeling and conditional random fields (CRF) approaches. We further provide quantitative and qualitative analysis on how RNN helps improve truecasing",
    "checked": true,
    "id": "ba7b2c534c749408c3d269f1c7996426576e2959",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Raymond Hendy Susanto",
      "Hai Leong Chieu",
      "Wei Lu"
    ]
  },
  "https://aclanthology.org/D16-1226": {
    "title": "The Effects of the Content of FOMC Communications on US Treasury Rates",
    "volume": "main",
    "abstract": "This study measures the effects of Federal Open Market Committee text content on the direction of shortand medium-term interest rate movements. Because the words relevant to shortand medium-term interest rates differ, we apply a supervised approach to learn distinct sets of topics for each dependent variable being examined. We generate predictions with and without controlling for factors relevant to interest rate movements, and our prediction results average across multiple training-test splits. Using data from 1999-2016, we achieve 93% and 64% accuracy in predicting Target and Effective Federal Funds Rate movements and 38%-40% accuracy in predicting longer term Treasury Rate movements. We obtain lower but comparable accuracies after controlling for other macroeconomic and market factors",
    "checked": true,
    "id": "7983c187c6b42266a459c6de010b54482164258e",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Christopher Rohlfs",
      "Sunandan Chakraborty",
      "Lakshminarayanan Subramanian"
    ]
  },
  "https://aclanthology.org/D16-1227": {
    "title": "Learning to refine text based recommendations",
    "volume": "main",
    "abstract": "We propose a text-based recommendation engine that utilizes recurrent neural networks to flexibly map textual input into continuous vector representations tailored to the recommendation task. Here, the text objects are documents such as Wikipedia articles or question and answer pairs. As neural models require substantial training time, we introduce a sequential component so as to quickly adjust the learned metric over objects as additional evidence accrues. We evaluate the approach on recommending Wikipedia descriptions of ingredients to their associated product categories. We also exemplify the sequential metric adjustment on retrieving similar Stack Exchange AskUbuntu questions. 1",
    "checked": true,
    "id": "16d20ccadd1331c357a6f589df437000f611d69c",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Youyang Gu",
      "Tao Lei",
      "Regina Barzilay",
      "Tommi Jaakkola"
    ]
  },
  "https://aclanthology.org/D16-1228": {
    "title": "There's No Comparison: Reference-less Evaluation Metrics in Grammatical Error Correction",
    "volume": "main",
    "abstract": "Current methods for automatically evaluating grammatical error correction (GEC) systems rely on gold-standard references. However, these methods suffer from penalizing grammatical edits that are correct but not in the gold standard. We show that reference-less grammaticality metrics correlate very strongly with human judgments and are competitive with the leading reference-based evaluation metrics. By interpolating both methods, we achieve state-of-the-art correlation with human judgments. Finally, we show that GEC metrics are much more reliable when they are calculated at the sentence level instead of the corpus level. We have set up a CodaLab site for benchmarking GEC output using a common dataset and different evaluation metrics",
    "checked": true,
    "id": "4ffd3fbbefc45d7847687a82cad166bac14a5fc7",
    "semantic_title": "",
    "citation_count": 43,
    "authors": [
      "Courtney Napoles",
      "Keisuke Sakaguchi",
      "Joel Tetreault"
    ]
  },
  "https://aclanthology.org/D16-1229": {
    "title": "Cultural Shift or Linguistic Drift? Comparing Two Computational Measures of Semantic Change",
    "volume": "main",
    "abstract": "Words shift in meaning for many reasons, including cultural factors like new technologies and regular linguistic processes like subjectification. Understanding the evolution of language and culture requires disentangling these underlying causes. Here we show how two different distributional measures can be used to detect two different types of semantic change. The first measure, which has been used in many previous works, analyzes global shifts in a word's distributional semantics; it is sensitive to changes due to regular processes of linguistic drift, such as the semantic generalization of promise (\"I promise.\" \"It promised to be exciting.\"). The second measure, which we develop here, focuses on local changes to a word's nearest semantic neighbors; it is more sensitive to cultural shifts, such as the change in the meaning of cell (\"prison cell\" \"cell phone\"). Comparing measurements made by these two methods allows researchers to determine whether changes are more cultural or linguistic in nature, a distinction that is essential for work in the digital humanities and historical linguistics",
    "checked": true,
    "id": "f39cb3680b0810fb794c0b52690cb9e3e0b16ced",
    "semantic_title": "",
    "citation_count": 192,
    "authors": [
      "William L. Hamilton",
      "Jure Leskovec",
      "Dan Jurafsky"
    ]
  },
  "https://aclanthology.org/D16-1230": {
    "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
    "volume": "main",
    "abstract": "We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems",
    "checked": true,
    "id": "129cbad01be98ee88a930e31898cb76be79c41c1",
    "semantic_title": "",
    "citation_count": 1083,
    "authors": [
      "Chia-Wei Liu",
      "Ryan Lowe",
      "Iulian Serban",
      "Mike Noseworthy",
      "Laurent Charlin",
      "Joelle Pineau"
    ]
  },
  "https://aclanthology.org/D16-1231": {
    "title": "Addressee and Response Selection for Multi-Party Conversation",
    "volume": "main",
    "abstract": "To create conversational systems working in actual situations, it is crucial to assume that they interact with multiple agents. In this work, we tackle addressee and response selection for multi-party conversation, in which systems are expected to select whom they address as well as what they say. The key challenge of this task is to jointly model who is talking about what in a previous context. For the joint modeling, we propose two modeling frameworks: 1) static modeling and 2) dynamic modeling. To show benchmark results of our frameworks, we created a multi-party conversation corpus. Our experiments on the dataset show that the recurrent neural network based models of our frameworks robustly predict addressees and responses in conversations with a large number of agents",
    "checked": true,
    "id": "811e014002d1e4d1e185fc236cf9e3fafe2aade5",
    "semantic_title": "",
    "citation_count": 40,
    "authors": [
      "Hiroki Ouchi",
      "Yuta Tsuboi"
    ]
  },
  "https://aclanthology.org/D16-1232": {
    "title": "Nonparametric Bayesian Models for Spoken Language Understanding",
    "volume": "main",
    "abstract": "In this paper, we propose a new generative approach for semantic slot filling task in spoken language understanding using a nonparametric Bayesian formalism. Slot filling is typically formulated as a sequential labeling problem, which does not directly deal with the posterior distribution of possible slot values. We present a nonparametric Bayesian model involving the generation of arbitrary natural language phrases, which allows an explicit calculation of the distribution over an infinite set of slot values. We demonstrate that this approach significantly improves slot estimation accuracy compared to the existing sequential labeling algorithm",
    "checked": true,
    "id": "c82fb23b008c11b064570724364d4f52910701bf",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Kei Wakabayashi",
      "Johane Takeuchi",
      "Kotaro Funakoshi",
      "Mikio Nakano"
    ]
  },
  "https://aclanthology.org/D16-1233": {
    "title": "Conditional Generation and Snapshot Learning in Neural Dialogue Systems",
    "volume": "main",
    "abstract": "Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used",
    "checked": true,
    "id": "55289d3feef4bc1e4ff17008120e371eb7f55a24",
    "semantic_title": "",
    "citation_count": 71,
    "authors": [
      "Tsung-Hsien Wen",
      "Milica Gašić",
      "Nikola Mrkšić",
      "Lina M. Rojas-Barahona",
      "Pei-Hao Su",
      "Stefan Ultes",
      "David Vandyke",
      "Steve Young"
    ]
  },
  "https://aclanthology.org/D16-1234": {
    "title": "Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment",
    "volume": "main",
    "abstract": "We consider the task of predicting lexical entailment using distributional vectors. We perform a novel qualitative analysis of one existing model which was previously shown to only measure the prototypicality of word pairs. We find that the model strongly learns to identify hypernyms using Hearst patterns, which are well known to be predictive of lexical relations. We present a novel model which exploits this behavior as a method of feature extraction in an iterative procedure similar to Principal Component Analysis. Our model combines the extracted features with the strengths of other proposed models in the literature, and matches or outperforms prior work on multiple data sets",
    "checked": true,
    "id": "c881d1989fa1a497b404c511a26bf7df9f12936e",
    "semantic_title": "",
    "citation_count": 71,
    "authors": [
      "Stephen Roller",
      "Katrin Erk"
    ]
  },
  "https://aclanthology.org/D16-1235": {
    "title": "SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity",
    "volume": "main",
    "abstract": "Verbs play a critical role in the meaning of sentences, but these ubiquitous words have received little attention in recent distributional semantics research. We introduce SimVerb-3500, an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed verb types from the USF free-association database, providing at least three examples for every VerbNet class. This broad coverage facilitates detailed analyses of how syntactic and semantic phenomena together influence human understanding of verb meaning. Further, with significantly larger development and test sets than existing benchmarks, SimVerb-3500 enables more robust evaluation of representation learning architectures and promotes the development of methods tailored to verbs. We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning",
    "checked": true,
    "id": "a1d1e3f7ee6767712fe14ee1bde11a341775daa3",
    "semantic_title": "",
    "citation_count": 236,
    "authors": [
      "Daniela Gerz",
      "Ivan Vulić",
      "Felix Hill",
      "Roi Reichart",
      "Anna Korhonen"
    ]
  },
  "https://aclanthology.org/D16-1236": {
    "title": "POLY: Mining Relational Paraphrases from Multilingual Sentences",
    "volume": "main",
    "abstract": "Language resources that systematically organize paraphrases for binary relations are of great value for various NLP tasks and have recently been advanced in projects like PATTY, WiseNet and DEFIE. This paper presents a new method for building such a resource and the resource itself, called POLY. Starting with a very large collection of multilingual sentences parsed into triples of phrases, our method clusters relational phrases using probabilistic measures. We judiciously leverage fine-grained semantic typing of relational arguments for identifying synonymous phrases. The evaluation of POLY shows significant improvements in precision and recall over the prior works on PATTY and DEFIE. An extrinsic use case demonstrates the benefits of POLY for question answering",
    "checked": true,
    "id": "76f43ca313f2619af0ce55f5a29425e26918ae11",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Adam Grycner",
      "Gerhard Weikum"
    ]
  },
  "https://aclanthology.org/D16-1237": {
    "title": "Exploiting Sentence Similarities for Better Alignments",
    "volume": "main",
    "abstract": "We study the problem of jointly aligning sentence constituents and predicting their similarities. While extensive sentence similarity data exists, manually generating reference alignments and labeling the similarities of the aligned chunks is comparatively onerous. This prompts the natural question of whether we can exploit easy-to-create sentence level data to train better aligners. In this paper, we present a model that learns to jointly align constituents of two sentences and also predict their similarities. By taking advantage of both sentence and constituent level data, we show that our model achieves state-of-the-art performance at predicting alignments and constituent similarities",
    "checked": true,
    "id": "76d971e45ffe6302a0e4cec5841b384423c872f3",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Tao Li",
      "Vivek Srikumar"
    ]
  },
  "https://aclanthology.org/D16-1238": {
    "title": "Bi-directional Attention with Agreement for Dependency Parsing",
    "volume": "main",
    "abstract": "We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions. The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The proposed parser makes use of {\\it soft} headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity. We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages",
    "checked": true,
    "id": "0c9211b3c08a32bc4d73d04f3c427c7db5e0fe91",
    "semantic_title": "",
    "citation_count": 44,
    "authors": [
      "Hao Cheng",
      "Hao Fang",
      "Xiaodong He",
      "Jianfeng Gao",
      "Li Deng"
    ]
  },
  "https://aclanthology.org/D16-1239": {
    "title": "Anchoring and Agreement in Syntactic Annotations",
    "volume": "main",
    "abstract": "We present a study on two key characteristics of human syntactic annotations: anchoring and agreement. Anchoring is a well known cognitive bias in human decision making, where judgments are drawn towards pre-existing values. We study the influence of anchoring on a standard approach to creation of syntactic resources where syntactic annotations are obtained via human editing of tagger and parser output. Our experiments demonstrate a clear anchoring effect and reveal unwanted consequences, including overestimation of parsing performance and lower quality of annotations in comparison with human-based annotations. Using sentences from the Penn Treebank WSJ, we also report systematically obtained inter-annotator agreement estimates for English dependency parsing. Our agreement results control for parser bias, and are consequential in that they are on par with state of the art parsing performance for English newswire. We discuss the impact of our findings on strategies for future annotation efforts and parser evaluations",
    "checked": true,
    "id": "e993cab5f30c298d5224e2288beb9fd245c1cd34",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Yevgeni Berzak",
      "Yan Huang",
      "Andrei Barbu",
      "Anna Korhonen",
      "Boris Katz"
    ]
  },
  "https://aclanthology.org/D16-1240": {
    "title": "Tense Manages to Predict Implicative Behavior in Verbs",
    "volume": "main",
    "abstract": "Implicative verbs (e.g. manage) entail their complement clauses, while non-implicative verbs (e.g. want) do not. For example, while managing to solve the problem entails solving the problem, no such inference follows from wanting to solve the problem. Differentiating between implicative and non-implicative verbs is therefore an essential component of natural language understanding, relevant to applications such as textual entailment and summarization. We present a simple method for predicting implicativeness which exploits known constraints on the tense of implicative verbs and their complements. We show that this yields an effective, data-driven way of capturing this nuanced property in verbs",
    "checked": true,
    "id": "ae0699d65e00ca9aa793f3e5ea71ab114deb120b",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Ellie Pavlick",
      "Chris Callison-Burch"
    ]
  },
  "https://aclanthology.org/D16-1241": {
    "title": "Who did What: A Large-Scale Person-Centered Cloze Dataset",
    "volume": "main",
    "abstract": "We have constructed a new \"Who-did-What\" dataset of over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus. The WDW dataset has a variety of novel features. First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation. Instead, each problem is formed from two independent articles --- an article given as the passage to be read and a separate article on the same events used to form the question. Second, we avoid anonymization --- each choice is a person named entity. Third, the problems have been filtered to remove a fraction that are easily solved by simple baselines, while remaining 84% solvable by humans. We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community",
    "checked": true,
    "id": "a39ffa57ef8e538b3c6a6c2bbc0b641f7cdc60dc",
    "semantic_title": "",
    "citation_count": 134,
    "authors": [
      "Takeshi Onishi",
      "Hai Wang",
      "Mohit Bansal",
      "Kevin Gimpel",
      "David McAllester"
    ]
  },
  "https://aclanthology.org/D16-1242": {
    "title": "Building compositional semantics and higher-order inference system for a wide-coverage Japanese CCG parser",
    "volume": "main",
    "abstract": "This paper presents a system that compositionally maps outputs of a wide-coverage Japanese CCG parser onto semantic representations and performs automated inference in higher-order logic. The system is evaluated on a textual entailment dataset. It is shown that the system solves inference problems that focus on a variety of complex linguistic phenomena, including those that are difficult to represent in the standard first-order logic",
    "checked": true,
    "id": "7f625b01df244b6fa7f95dc3e995e9d4a3909473",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Koji Mineshima",
      "Ribeka Tanaka",
      "Pascual Martínez-Gómez",
      "Yusuke Miyao",
      "Daisuke Bekki"
    ]
  },
  "https://aclanthology.org/D16-1243": {
    "title": "Learning to Generate Compositional Color Descriptions",
    "volume": "main",
    "abstract": "The production of color language is essential for grounded language generation. Color descriptions have many challenging properties: they can be vague, compositionally complex, and denotationally rich. We present an effective approach to generating color descriptions using recurrent neural networks and a Fourier-transformed color representation. Our model outperforms previous work on a conditional language modeling task over a large corpus of naturalistic color descriptions. In addition, probing the model's output reveals that it can accurately produce not only basic color terms but also descriptors with non-convex denotations (\"greenish\"), bare modifiers (\"bright\", \"dull\"), and compositional phrases (\"faded teal\") not seen in training",
    "checked": true,
    "id": "0ec9ec9e37131ff28efba99c6cab789d08dfff38",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Will Monroe",
      "Noah D. Goodman",
      "Christopher Potts"
    ]
  },
  "https://aclanthology.org/D16-1244": {
    "title": "A Decomposable Attention Model for Natural Language Inference",
    "volume": "main",
    "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements",
    "checked": true,
    "id": "2cd8e8f510c89c7c18268e8ad51c061e459ad321",
    "semantic_title": "",
    "citation_count": 1216,
    "authors": [
      "Ankur Parikh",
      "Oscar Täckström",
      "Dipanjan Das",
      "Jakob Uszkoreit"
    ]
  },
  "https://aclanthology.org/D16-1245": {
    "title": "Deep Reinforcement Learning for Mention-Ranking Coreference Models",
    "volume": "main",
    "abstract": "Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment with two approaches: the REINFORCE policy gradient algorithm and a reward-rescaled max-margin objective. We find the latter to be more effective, resulting in significant improvements over the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task",
    "checked": true,
    "id": "3aa64ea8099f9c1f705f616937f27827c873d77f",
    "semantic_title": "",
    "citation_count": 305,
    "authors": [
      "Kevin Clark",
      "Christopher D. Manning"
    ]
  },
  "https://aclanthology.org/D16-1246": {
    "title": "A Stacking Gated Neural Architecture for Implicit Discourse Relation Classification",
    "volume": "main",
    "abstract": "Discourse parsing is considered as one of the most challenging natural language processing (NLP) tasks. Implicit discourse relation classification is the bottleneck for discourse parsing. Without the guide of explicit discourse connectives, the relation of sentence pairs are very hard to be inferred. This paper proposes a stacking neural network model to solve the classification problem in which a convolutional neural network (CNN) is utilized for sentence modeling and a collaborative gated neural network (CGNN) is proposed for feature transformation. Our evaluation and comparisons show that the proposed model outperforms previous state-of-the-art systems",
    "checked": true,
    "id": "cfd468bf8b138b1eed6b32ad262a1a794f9440b4",
    "semantic_title": "",
    "citation_count": 60,
    "authors": [
      "Lianhui Qin",
      "Zhisong Zhang",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/D16-1247": {
    "title": "Insertion Position Selection Model for Flexible Non-Terminals in Dependency Tree-to-Tree Machine Translation",
    "volume": "main",
    "abstract": "Dependency tree-to-tree translation models are powerful because they can naturally handle long range reorderings which is important for distant language pairs. The translation process is easy if it can be accomplished only by replacing non-terminals in translation rules with other rules. However it is sometimes necessary to adjoin translation rules. Flexible non-terminals have been proposed as a promising solution for this problem. A flexible non-terminal provides several insertion position candidates for the rules to be adjoined, but it increases the computational cost of decoding. In this paper we propose a neural network based insertion position selection model to reduce the computational cost by selecting the appropriate insertion positions. The experimental results show the proposed model can select the appropriate insertion position with a high accuracy. It reduces the decoding time and improves the translation quality owing to reduced search space",
    "checked": true,
    "id": "8e0dccbba2aa4e58b79b419a596775a6fba86a26",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Toshiaki Nakazawa",
      "John Richardson",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/D16-1248": {
    "title": "Why Neural Translations are the Right Length",
    "volume": "main",
    "abstract": "We investigate how neural, encoder-decoder translation systems output target strings of appropriate lengths, finding that a collection of hidden units learns to explicitly implement this functionality",
    "checked": true,
    "id": "2759976e7d8a27c788fb52a24830fc63ce0de570",
    "semantic_title": "",
    "citation_count": 66,
    "authors": [
      "Xing Shi",
      "Kevin Knight",
      "Deniz Yuret"
    ]
  },
  "https://aclanthology.org/D16-1249": {
    "title": "Supervised Attentions for Neural Machine Translation",
    "volume": "main",
    "abstract": "In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the \"true\" alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system",
    "checked": true,
    "id": "d4a887499773ff32aae898711e595654f3f65199",
    "semantic_title": "",
    "citation_count": 123,
    "authors": [
      "Haitao Mi",
      "Zhiguo Wang",
      "Abe Ittycheriah"
    ]
  },
  "https://aclanthology.org/D16-1250": {
    "title": "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance",
    "volume": "main",
    "abstract": "Mapping word embeddings of different languages into a single space has multiple applications. In order to map from a source space into a target space, a common approach is to learn a linear mapping that minimizes the distances between equivalences listed in a bilingual dictionary. In this paper, we propose a framework that generalizes previous work, provides an efficient exact method to learn the optimal linear transformation and yields the best bilingual results in translation induction while preserving monolingual performance in an analogy task",
    "checked": true,
    "id": "9a2eed5f8175275af0d55d4aed39afc8e2b2acf2",
    "semantic_title": "",
    "citation_count": 345,
    "authors": [
      "Mikel Artetxe",
      "Gorka Labaka",
      "Eneko Agirre"
    ]
  },
  "https://aclanthology.org/D16-1251": {
    "title": "Measuring the behavioral impact of machine translation quality improvements with A/B testing",
    "volume": "main",
    "abstract": "In this paper we discuss a process for quantifying the behavioral impact of a domaincustomized machine translation system deployed on a large-scale e-commerce platform. We discuss several machine translation systems that we trained using aligned text from product listing descriptions written in multiple languages. We document the quality improvements of these systems as measured through automated quality measures and crowdsourced human quality assessments. We then measure the effect of these quality improvements on user behavior using an automated A/B testing framework. Through testing we observed an increase in key ecommerce metrics, including a significant increase in purchases",
    "checked": true,
    "id": "2a86deac719aa2ec638ff96ab272f1e0a4e121ff",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ben Russell",
      "Duncan Gillespie"
    ]
  },
  "https://aclanthology.org/D16-1252": {
    "title": "Creating a Large Benchmark for Open Information Extraction",
    "volume": "main",
    "abstract": "Open information extraction (Open IE) was presented as an unrestricted variant of traditional information extraction. It has been gaining substantial attention, manifested by a large number of automatic Open IE extractors and downstream applications. In spite of this broad attention, the Open IE task definition has been lacking – there are no formal guidelines and no large scale gold standard annotation. Subsequently, the various implementations of Open IE resorted to small scale posthoc evaluations, inhibiting an objective and reproducible cross-system comparison. In this work, we develop a methodology that leverages the recent QA-SRL annotation to create a first independent and large scale Open IE annotation,1 and use it to automatically compare the most prominent Open IE systems",
    "checked": true,
    "id": "a32d7aba28ce9f130934b8e892df5bf2cad97e21",
    "semantic_title": "",
    "citation_count": 113,
    "authors": [
      "Gabriel Stanovsky",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/D16-1253": {
    "title": "Bilingually-constrained Synthetic Data for Implicit Discourse Relation Recognition",
    "volume": "main",
    "abstract": "To alleviate the shortage of labeled data, we propose to use bilingually-constrained synthetic implicit data for implicit discourse relation recognition. These data are extracted from a bilingual sentence-aligned corpus according to the implicit/explicit mismatch between different languages. Incorporating these data via a multi-task neural network model achieves significant improvements over baselines, on both the English PDTB and Chinese CDTB data sets",
    "checked": true,
    "id": "c9df8ce459f27059406d05fb05eb8e2e1a57aa55",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Changxing Wu",
      "Xiaodong Shi",
      "Yidong Chen",
      "Yanzhou Huang",
      "Jinsong Su"
    ]
  },
  "https://aclanthology.org/D16-1254": {
    "title": "Transition-Based Dependency Parsing with Heuristic Backtracking",
    "volume": "main",
    "abstract": "We introduce a novel approach to the decoding problem in transition-based parsing: heuristic backtracking. This algorithm uses a series of partial parses on the sentence to locate the best candidate parse, using confidence estimates of transition decisions as a heuristic to guide the starting points of the search. This allows us to achieve a parse accuracy comparable to beam search, despite using fewer transitions. When used to augment a Stack-LSTM transition-based parser, the parser shows an unlabeled attachment score of up to 93.30% for English and 87.61% for Chinese",
    "checked": true,
    "id": "6a932c699e8cc8fb8ef66642b3db701a5fa04c8b",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Jacob Buckman",
      "Miguel Ballesteros",
      "Chris Dyer"
    ]
  },
  "https://aclanthology.org/D16-1255": {
    "title": "Word Ordering Without Syntax",
    "volume": "main",
    "abstract": "Recent work on word ordering has argued that syntactic structure is important, or even required, for effectively recovering the order of a sentence. We find that, in fact, an n-gram language model with a simple heuristic gives strong results on this task. Furthermore, we show that a long short-term memory (LSTM) language model is even more effective at recovering order, with our basic model outperforming a state-of-the-art syntactic model by 11.5 BLEU points. Additional data and larger beams yield further gains, at the expense of training and search time",
    "checked": true,
    "id": "7b5af1758963babf3740a39615b469b70513a413",
    "semantic_title": "",
    "citation_count": 37,
    "authors": [
      "Allen Schmaltz",
      "Alexander M. Rush",
      "Stuart Shieber"
    ]
  },
  "https://aclanthology.org/D16-1256": {
    "title": "Morphological Segmentation Inside-Out",
    "volume": "main",
    "abstract": "Morphological segmentation has traditionally been modeled with non-hierarchical models, which yield flat segmentations as output. In many cases, however, proper morphological analysis requires hierarchical structure -- especially in the case of derivational morphology. In this work, we introduce a discriminative, joint model of morphological segmentation along with the orthographic changes that occur during word formation. To the best of our knowledge, this is the first attempt to approach discriminative segmentation with a context-free model. Additionally, we release an annotated treebank of 7454 English words with constituency parses, encouraging future research in this area",
    "checked": true,
    "id": "8477cc89bd089b79169fc58d7720ba9f1d49bd2a",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Ryan Cotterell",
      "Arun Kumar",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/D16-1257": {
    "title": "Parsing as Language Modeling",
    "volume": "main",
    "abstract": "We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing — 93.8 F1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%",
    "checked": true,
    "id": "39f1b108687f643015f96a0c800585a44621f99c",
    "semantic_title": "",
    "citation_count": 102,
    "authors": [
      "Do Kook Choe",
      "Eugene Charniak"
    ]
  },
  "https://aclanthology.org/D16-1258": {
    "title": "Human-in-the-Loop Parsing",
    "volume": "main",
    "abstract": "This paper demonstrates that it is possible for a parser to improve its performance with a human in the loop, by posing simple questions to non-experts. For example, given the first sentence of this abstract, if the parser is uncertain about the subject of the verb \"pose,\" it could generate the question What would pose something? with candidate answers this paper and a parser. Any fluent speaker can answer this question, and the correct answer resolves the original uncertainty. We apply the approach to a CCG parser, converting uncertain attachment decisions into natural language questions about the arguments of verbs. Experiments show that crowd workers can answer these questions quickly, accurately and cheaply. Our human-in-the-loop parser improves on the state of the art with less than 2 questions per sentence on average, with a gain of 1.7 F1 on the 10% of sentences whose parses are changed",
    "checked": true,
    "id": "fda558136b2d2b812a608b21fe22959d48db1078",
    "semantic_title": "",
    "citation_count": 28,
    "authors": [
      "Luheng He",
      "Julian Michael",
      "Mike Lewis",
      "Luke Zettlemoyer"
    ]
  },
  "https://aclanthology.org/D16-1259": {
    "title": "Unsupervised Timeline Generation for Wikipedia History Articles",
    "volume": "main",
    "abstract": "This paper presents a generic approach to content selection for creating timelines from individual history articles for which no external information about the same topic is available. This scenario is in contrast to existing works on timeline generation, which require the presence of a large corpus of news articles. To identify salient events in a given history article, we exploit lexical cues about the article's subject area, as well as time expressions that are syntactically attached to an event word. We also test different methods of ensuring timeline coverage of the entire historical time span described. Our best-performing method outperforms a new unsupervised baseline and an improved version of an existing supervised approach. We see our work as a step towards more semantically motivated approaches to single-document summarisation",
    "checked": true,
    "id": "04ab5ed3635a75bfd6b0c95598dbf38a1d4f63e8",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Sandro Bauer",
      "Simone Teufel"
    ]
  },
  "https://aclanthology.org/D16-1260": {
    "title": "Encoding Temporal Information for Time-Aware Link Prediction",
    "volume": "main",
    "abstract": "Most existing knowledge base (KB) embedding methods solely learn from time-unknown fact triples but neglect the temporal information in the knowledge base. In this paper, we propose a novel time-aware KB embedding approach taking advantage of the happening time of facts. Specifically, we use temporal order constraints to model transformation between time-sensitive relations and enforce the embeddings to be temporally consistent and more accurate. We empirically evaluate our approach in two tasks of link prediction and triple classification. Experimental results show that our method outperforms other baselines on the two tasks consistently",
    "checked": true,
    "id": "fd458e7109e8ebac2f59d399981054957078c7a4",
    "semantic_title": "",
    "citation_count": 75,
    "authors": [
      "Tingsong Jiang",
      "Tianyu Liu",
      "Tao Ge",
      "Lei Sha",
      "Sujian Li",
      "Baobao Chang",
      "Zhifang Sui"
    ]
  },
  "https://aclanthology.org/D16-1261": {
    "title": "Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning",
    "volume": "main",
    "abstract": "Most successful information extraction systems operate with access to a large collection of documents. In this work, we explore the task of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce. This process entails issuing search queries, extraction from new sources and reconciliation of extracted values, which are repeated until sufficient evidence is collected. We approach the problem using a reinforcement learning framework where our model learns to select optimal actions based on contextual information. We employ a deep Q-network, trained to optimize a reward function that reflects extraction accuracy while penalizing extra effort. Our experiments on two databases -- of shooting incidents, and food adulteration cases -- demonstrate that our system significantly outperforms traditional extractors and a competitive meta-classifier baseline",
    "checked": true,
    "id": "ac50801574f7c97f1cf8f6718a9ff2b3e18c2dc6",
    "semantic_title": "",
    "citation_count": 136,
    "authors": [
      "Karthik Narasimhan",
      "Adam Yala",
      "Regina Barzilay"
    ]
  },
  "https://aclanthology.org/D16-1262": {
    "title": "Global Neural CCG Parsing with Optimality Guarantees",
    "volume": "main",
    "abstract": "We introduce the first global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a new objective that encourages the parser to explore a tiny fraction of the search space. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees",
    "checked": true,
    "id": "8602398403281dae0694b4e0488eb501d6db49ef",
    "semantic_title": "",
    "citation_count": 38,
    "authors": [
      "Kenton Lee",
      "Mike Lewis",
      "Luke Zettlemoyer"
    ]
  },
  "https://aclanthology.org/D16-1263": {
    "title": "Learning a Lexicon and Translation Model from Phoneme Lattices",
    "volume": "main",
    "abstract": "Language documentation begins by gathering speech. Manual or automatic transcription at the word level is typically not possible because of the absence of an orthography or prior lexicon, and though manual phonemic transcription is possible, it is prohibitively slow. On the other hand, translations of the minority language into a major language are more easily acquired. We propose a method to harness such translations to improve automatic phoneme recognition. The method assumes no prior lexicon or translation model, instead learning them from phoneme lattices and translations of the speech being transcribed. Experiments demonstrate phoneme error rate improvements against two baselines and the model's ability to learn useful bilingual lexical entries",
    "checked": true,
    "id": "91e605a125f64207a242693d0dc1c862080f6c27",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Oliver Adams",
      "Graham Neubig",
      "Trevor Cohn",
      "Steven Bird",
      "Quoc Truong Do",
      "Satoshi Nakamura"
    ]
  },
  "https://aclanthology.org/D16-1264": {
    "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
    "volume": "main",
    "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.   The dataset is freely available at this https URL",
    "checked": true,
    "id": "05dd7254b632376973f3a1b4d39485da17814df5",
    "semantic_title": "",
    "citation_count": 5607,
    "authors": [
      "Pranav Rajpurkar",
      "Jian Zhang",
      "Konstantin Lopyrev",
      "Percy Liang"
    ]
  },
  "https://aclanthology.org/D16-2002": {
    "title": "Advanced Markov Logic Techniques for Scalable Joint Inference in NLP",
    "volume": "tutorial",
    "abstract": "In the early days of the statistical NLP era, many language processing tasks were tackled using the so-called pipeline architecture: the given task is broken into a series of sub-tasks such that the output of one sub-task is an input to the next sub-task in the sequence. The pipeline architecture is appealing for various reasons, including modularity, modeling convenience, and manageable computational complexity. However, it suffers from the error propagation problem: errors made in one sub-task are propagated to the next sub-task in the sequence, leading to poor accuracy on that sub-task, which in turn leads to more errors downstream. Another disadvantage associated with it is lack of feedback: errors made in a sub-task are often not corrected using knowledge uncovered while solving another sub-task down the pipeline.Realizing these weaknesses, researchers have turned to joint inference approaches in recent years. One such approach involves the use of Markov logic, which is defined as a set of weighted first-order logic formulas and, at a high level, unifies first-order logic with probabilistic graphical models. It is an ideal modeling language (knowledge representation) for compactly representing relational and uncertain knowledge in NLP. In a typical use case of MLNs in NLP, the application designer describes the background knowledge using a few first-order logic sentences and then uses software packages such as Alchemy, Tuffy, and Markov the beast to perform learning and inference (prediction) over the MLN. However, despite its obvious advantages, over the years, researchers and practitioners have found it difficult to use MLNs effectively in many NLP applications. The main reason for this is that it is hard to scale inference and learning algorithms for MLNs to large datasets and complex models, that are typical in NLP.In this tutorial, we will introduce the audience to recent advances in scaling up inference and learning in MLNs as well as new approaches to make MLNs a \"black-box\" for NLP applications (with only minor tuning required on the part of the user). Specifically, we will introduce attendees to a key idea that has emerged in the MLN research community over the last few years, lifted inference , which refers to inference techniques that take advantage of symmetries (e.g., synonyms), both exact and approximate, in the MLN . We will describe how these next-generation inference techniques can be used to perform effective joint inference. We will also present our new software package for inference and learning in MLNs, Alchemy 2.0, which is based on lifted inference, focusing primarily on how it can be used to scale up inference and learning in large models and datasets for applications such as semantic similarity determination, information extraction and question answering",
    "checked": true,
    "id": "81076e08ee834b3854eb338210ab0c27c549b3b4",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deepak Venugopal",
      "Vibhav Gogate",
      "Vincent Ng"
    ]
  },
  "https://aclanthology.org/D16-2003": {
    "title": "Lifelong Machine Learning for Natural Language Processing",
    "volume": "tutorial",
    "abstract": "Machine learning (ML) has been successfully used as a prevalent approach to solving numerous NLP problems. However, the classic ML paradigm learns in isolation. That is, given a dataset, an ML algorithm is executed on the dataset to produce a model without using any related or prior knowledge. Although this type of isolated learning is very useful, it also has serious limitations as it does not accumulate knowledge learned in the past and use the knowledge to help future learning, which is the hallmark of human learning and human intelligence. Lifelong machine learning (LML) aims to achieve this capability. Specifically, it aims to design and develop computational learning systems and algorithms that learn as humans do, i.e., retaining the results learned in the past, abstracting knowledge from them, and using the knowledge to help future learning. In this tutorial, we will introduce the existing research of LML and to show that LML is very suitable for NLP tasks and has potential to help NLP make major progresses",
    "checked": true,
    "id": "3a54b512a9ecb873480d8bb93caf46494f52bfc2",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zhiyuan Chen",
      "Bing Liu"
    ]
  },
  "https://aclanthology.org/D16-2004": {
    "title": "Neural Networks for Sentiment Analysis",
    "volume": "tutorial",
    "abstract": "Sentiment analysis has been a major research topic in natural language processing (NLP). Traditionally, the problem has been attacked using discrete models and manually-defined sparse features. Over the past few years, neural network models have received increased research efforts in most sub areas of sentiment analysis, giving highly promising results. A main reason is the capability of neural models to automatically learn dense features that capture subtle semantic information over words, sentences and documents, which are difficult to model using traditional discrete features based on words and ngram patterns. This tutorial gives an introduction to neural network models for sentiment analysis, discussing the mathematics of word embeddings, sequence models and tree structured models and their use in sentiment analysis on the word, sentence and document levels, and fine-grained sentiment analysis. The tutorial covers a range of neural network models (e.g. CNN, RNN, RecNN, LSTM) and their extensions, which are employed in four main subtasks of sentiment analysis:Sentiment-oriented embeddings;Sentence-level sentiment;Document-level sentiment;Fine-grained sentiment.The content of the tutorial is divided into 3 sections of 1 hour each. We assume that the audience is familiar with linear algebra and basic neural network structures, introduce the mathematical details of the most typical models. First, we will introduce the sentiment analysis task, basic concepts related to neural network models for sentiment analysis, and show detail approaches to integrate sentiment information into embeddings. Sentence-level models will be described in the second section. Finally, we will discuss neural network models use for document-level and fine-grained sentiment",
    "checked": true,
    "id": "b6ec36454d40ad9ce7e9b9c5804a2824c738a676",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhang",
      "Duy Tin Vo"
    ]
  },
  "https://aclanthology.org/D16-2005": {
    "title": "Continuous Vector Spaces for Cross-language NLP Applications",
    "volume": "tutorial",
    "abstract": "The mathematical metaphor offered by the geometric concept of distance in vector spaces with respect to semantics and meaning has been proven to be useful in many monolingual natural language processing applications. There is also some recent and strong evidence that this paradigm can also be useful in the cross-language setting. In this tutorial, we present and discuss some of the most recent advances on exploiting the vector space model paradigm in specific cross-language natural language processing applications, along with a comprehensive review of the theoretical background behind them.First, the tutorial introduces some fundamental concepts of distributional semantics and vector space models. More specifically, the concepts of distributional hypothesis and term-document matrices are revised, followed by a brief discussion on linear and non-linear dimensionality reduction techniques and their implications to the parallel distributed approach to semantic cognition. Next, some classical examples of using vector space models in monolingual natural language processing applications are presented. Specific examples in the areas of information retrieval, related term identification and semantic compositionality are described.Then, the tutorial focuses its attention on the use of the vector space model paradigm in cross-language applications. To this end, some recent examples are presented and discussed in detail, addressing the specific problems of cross-language information retrieval, cross-language sentence matching, and machine translation. Some of the most recent developments in the area of Neural Machine Translation are also discussed.Finally, the tutorial concludes with a discussion about current and future research problems related to the use of vector space models in cross-language settings. Future avenues for scientific research are described, with major emphasis on the extension from vector and matrix representations to tensors, as well as the problem of encoding word position information into the vector-based representations",
    "checked": true,
    "id": "981c2652e0199cd5fc383be260be89e69060b643",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rafael E. Banchs"
    ]
  },
  "https://aclanthology.org/D16-2006": {
    "title": "Methods and Theories for Large-scale Structured Prediction",
    "volume": "tutorial",
    "abstract": "Many important NLP tasks are casted as structured prediction problems, and try to predict certain forms of structured output from the input. Examples of structured prediction include POS tagging, named entity recognition, PCFG parsing, dependency parsing, machine translation, and many others. When apply structured prediction to a specific NLP task, there are the following challenges:1. Model selection: Among various models/algorithms with different characteristics, which one should we choose for a specific NLP task?2. Training: How to train the model parameters effectively and efficiently?3. Overfitting: To achieve good accuracy on test data, it is important to control the overfitting from the training data. How to control the overfitting risk for structured prediction?This tutorial will provide a clear overview of recent advances in structured prediction methods and theories, and address the above issues when we apply structured prediction to NLP tasks. We will introduce large margin methods (e.g., perceptrons, MIRA), graphical models (e.g., CRFs), and deep learning methods (e.g., RNN, LSTM), and show the respective advantages and disadvantages for NLP applications. For the training algorithms, we will introduce online/ stochastic training methods, and we will introduce parallel online/stochastic learning algorithms and theories to speed up the training (e.g., the Hogwild algorithm). For controlling the overfitting from training data, we will introduce the weight regularization methods, structure regularization, and implicit regularization methods",
    "checked": true,
    "id": "578b8c06530c6083e8b6f8ede6420c6c0e2f424b",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Sun",
      "Yansong Feng"
    ]
  },
  "https://aclanthology.org/D16-2001": {
    "title": "Practical Neural Networks for NLP: From Theory to Code",
    "volume": "tutorial",
    "abstract": "This tutorial aims to bring NLP researchers up to speed with the current techniques in deep learning and neural networks, and show them how they can turn their ideas into practical implementations. We will start with simple classification models (logistic regression and multilayer perceptrons) and cover more advanced patterns that come up in NLP such as recurrent networks for sequence tagging and prediction problems, structured networks (e.g., compositional architectures based on syntax trees), structured output spaces (sequences and trees), attention for sequence-to-sequence transduction, and feature induction for complex algorithm states. A particular emphasis will be on learning to represent complex objects as recursive compositions of simpler objects. This representation will reflect characterize standard objects in NLP, such as the composition of characters and morphemes into words, and words into sentences and documents. In addition, new opportunities such as learning to embed \"algorithm states\" such as those used in transition-based parsing and other sequential structured prediction models (for which effective features may be difficult to engineer by hand) will be covered.Everything in the tutorial will be grounded in code — we will show how to program seemingly complex neural-net models using toolkits based on the computation-graph formalism. Computation graphs decompose complex computations into a DAG, with nodes representing inputs, target outputs, parameters, or (sub)differentiable functions (e.g., \"tanh\", \"matrix multiply\", and \"softmax\"), and edges represent data dependencies. These graphs can be run \"forward\" to make predictions and compute errors (e.g., log loss, squared error) and then \"backward\" to compute derivatives with respect to model parameters. In particular we'll cover the Python bindings of the CNN library. CNN has been designed from the ground up for NLP applications, dynamically structured NNs, rapid prototyping, and a transparent data and execution model",
    "checked": null,
    "id": "fb38451ff87254ac1ff15e79154ef958b4efb6a6",
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  }
}