{
  "https://aclanthology.org/D16-1001": {
    "title": "Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles",
    "abstract": "Parsing accuracy using efficient greedy transition systems has improved dramatically in recent years thanks to neural networks. Despite striking results in dependency parsing, however, neural models have not surpassed state-of-the-art approaches in constituency parsing. To remedy this, we introduce a new shift-reduce system whose stack contains merely sentence spans, represented by a bare minimum of LSTM features. We also design the first provably optimal dynamic oracle for constituency parsing, which runs in amortized O(1) time, compared to O(n^3) oracles for standard dependency parsing. Training with this oracle, we achieve the best F1 scores on both English and French of any parser that does not use reranking or external data",
    "volume": "main",
    "checked": true,
    "id": "715dde17239c52b3f9924a5a35edc32b0f27830b",
    "citation_count": 112
  },
  "https://aclanthology.org/D16-1002": {
    "title": "Rule Extraction for Tree-to-Tree Transducers by Cost Minimization",
    "abstract": "Tree transducers that model expressive linguistic phenomena often require wordalignments and a heuristic rule extractor to induce their grammars. However, when the corpus of tree/string pairs is small compared to the size of the vocabulary or the complexity of the grammar, word-alignments are unreliable. We propose a general rule extraction algorithm that uses cost functions over tree fragments, and formulate the extraction as a cost minimization problem. As a by-product, we are able to introduce back-off states at which some cost functions generate right-hand-sides of previously unseen lefthand-sides, thus creating transducer rules \"on-the-fly\". We test the generalization power of our induced tree transducers on a QA task over a large Knowledge Base, obtaining a reasonable syntactic accuracy and effectively overcoming the typical lack of rule coverage",
    "volume": "main",
    "checked": true,
    "id": "388fcc03a5babd658c52c1c3be8149b64b5dbbb1",
    "citation_count": 6
  },
  "https://aclanthology.org/D16-1003": {
    "title": "A Neural Network for Coordination Boundary Prediction",
    "abstract": "We propose a neural-network based model for coordination boundary prediction. The network is designed to incorporate two signals: the similarity between conjuncts and the observation that replacing the whole coordination phrase with a conjunct tends to produce a coherent sentences. The modeling makes use of several LSTM networks. The model is trained solely on conjunction annotations in a Treebank, without using external resources. We show improvements on predicting coordination boundaries on the PTB compared to two state-of-the-art parsers; as well as improvement over previous coordination boundary prediction systems on the Genia corpus",
    "volume": "main",
    "checked": true,
    "id": "f7e37cd317e037e05e30198b4259032d5f5e4e9e",
    "citation_count": 20
  },
  "https://aclanthology.org/D16-1004": {
    "title": "Using Left-corner Parsing to Encode Universal Structural Constraints in Grammar Induction",
    "abstract": "Center-embedding is difficult to process and is known as a rare syntactic construction across languages. In this paper we describe a method to incorporate this assumption into the grammar induction tasks by restricting the search space of a model to trees with limited centerembedding. The key idea is the tabulation of left-corner parsing, which captures the degree of center-embedding of a parse via its stack depth. We apply the technique to learning of famous generative model, the dependency model with valence (Klein and Manning, 2004). Cross-linguistic experiments on Universal Dependencies show that often our method boosts the performance from the baseline, and competes with the current state-ofthe-art model in a number of languages",
    "volume": "main",
    "checked": true,
    "id": "37fcc0d2a57d310b3cfecd161a197221e0ca25ab",
    "citation_count": 32
  },
  "https://aclanthology.org/D16-1005": {
    "title": "Distinguishing Past, On-going, and Future Events: The EventStatus Corpus",
    "abstract": "Determining whether a major societal event has already happened, is still on-going, or may occur in the future is crucial for event prediction, timeline generation, and news summarization. We introduce a new task and a new corpus, EventStatus, which has 4500 English and Spanish articles about civil unrest events labeled as PAST, ON-GOING, or FUTURE. We show that the temporal status of these events is difficult to classify because local tense and aspect cues are often lacking, time expressions are insufficient, and the linguistic contexts have rich semantic compositionality. We explore two approaches for event status classification: (1) a feature-based SVM classifier augmented with a novel induced lexicon of future-oriented verbs, such as \"threatened\" and \"planned\", and (2) a convolutional neural net. Both types of classifiers improve event status recognition over a state-of-the-art TempEval model, and our analysis offers linguistic insights into the semantic compositionality challenges for this new task",
    "volume": "main",
    "checked": true,
    "id": "9d1c5959fa42db039f3f0f769da0ae44c9a915a1",
    "citation_count": 16
  },
  "https://aclanthology.org/D16-1006": {
    "title": "Nested Propositions in Open Information Extraction",
    "abstract": "The challenges of Machine Reading and Knowledge Extraction at a web scale require a system capable of extracting diverse information from large, heterogeneous corpora. The Open Information Extraction (OIE) paradigm aims at extracting assertions from large corpora without requiring a vocabulary or relation-specific training data. Most systems built on this paradigm extract binary relations from arbitrary sentences, ignoring the context under which the assertions are correct and complete. They lack the expressiveness needed to properly represent and extract complex assertions commonly found in the text. To address the lack of representation power, we propose NESTIE, which uses a nested representation to extract higher-order relations, and complex, interdependent assertions. Nesting the extracted propositions allows NESTIE to more accurately reflect the meaning of the original sentence. Our experimental study on real-world datasets suggests that NESTIE obtains comparable precision with better minimality and informativeness than existing approaches. NESTIE produces 1.7-1.8 times more minimal extractions and achieves 1.1-1.2 times higher informativeness than CLAUSIE",
    "volume": "main",
    "checked": true,
    "id": "4c187332ba519e50feb6ca1454dd02682a0a3643",
    "citation_count": 52
  },
  "https://aclanthology.org/D16-1007": {
    "title": "A Position Encoding Convolutional Neural Network Based on Dependency Tree for Relation Classification",
    "abstract": "With the renaissance of neural network in recent years, relation classification has again become a research hotspot in natural language processing, and leveraging parse trees is a common and effective method of tackling this problem. In this work, we offer a new perspective on utilizing syntactic information of dependency parse tree and present a position encoding convolutional neural network (PECNN) based on dependency parse tree for relation classification. First, treebased position features are proposed to encode the relative positions of words in dependency trees and help enhance the word representations. Then, based on a redefinition of \"context\", we design two kinds of tree-based convolution kernels for capturing the semantic and structural information provided by dependency trees. Finally, the features extracted by convolution module are fed to a classifier for labelling the semantic relations. Experiments on the benchmark dataset show that PECNN outperforms state-of-the-art approaches. We also compare the effect of different position features and visualize the influence of treebased position feature by tracing back the convolution process",
    "volume": "main",
    "checked": true,
    "id": "28fccd407338011bbb553d873bf0757bd131799d",
    "citation_count": 36
  },
  "https://aclanthology.org/D16-1008": {
    "title": "Learning to Recognize Discontiguous Entities",
    "abstract": "This paper focuses on the study of recognizing discontiguous entities. Motivated by a previous work, we propose to use a novel hypergraph representation to jointly encode discontiguous entities of unbounded length, which can overlap with one another. To compare with existing approaches, we first formally introduce the notion of model ambiguity, which defines the difficulty level of interpreting the outputs of a model, and then formally analyze the theoretical advantages of our model over previous existing approaches based on linear-chain CRFs. Our empirical results also show that our model is able to achieve significantly better results when evaluated on standard data with many discontiguous entities",
    "volume": "main",
    "checked": true,
    "id": "68077d1b6b6185aaa714cc81d70c848efe9754da",
    "citation_count": 29
  },
  "https://aclanthology.org/D16-1009": {
    "title": "Modeling Human Reading with Neural Attention",
    "abstract": "When humans read text, they fixate some words and skip others. However, there have been few attempts to explain skipping behavior with computational models, as most existing work has focused on predicting reading times (e.g.,~using surprisal). In this paper, we propose a novel approach that models both skipping and reading, using an unsupervised architecture that combines a neural attention with autoencoding, trained on raw text using reinforcement learning. Our model explains human reading behavior as a tradeoff between precision of language understanding (encoding the input accurately) and economy of attention (fixating as few words as possible). We evaluate the model on the Dundee eye-tracking corpus, showing that it accurately predicts skipping behavior and reading times, is competitive with surprisal, and captures known qualitative features of human reading",
    "volume": "main",
    "checked": true,
    "id": "29f2fcfc34fa08a7980a29e06d8bc0ed219e478d",
    "citation_count": 43
  },
  "https://aclanthology.org/D16-1010": {
    "title": "Comparing Computational Cognitive Models of Generalization in a Language Acquisition Task",
    "abstract": "Natural language acquisition relies on appropriate generalization: the ability to produce novel sentences, while learning to restrict productions to acceptable forms in the language. Psycholinguists have proposed various properties that might play a role in guiding appropriate generalizations, looking at learning of verb alternations as a testbed. Several computational cognitive models have explored aspects of this phenomenon, but their results are hard to compare given the high variability in the linguistic properties represented in their input. In this paper, we directly compare two recent approaches, a Bayesian model and a connectionist model, in their ability to replicate human judgments of appropriate generalizations. We find that the Bayesian model more accurately mimics the judgments due to its richer learning mechanism that can exploit distributional properties of the input in a manner consistent with human behaviour",
    "volume": "main",
    "checked": true,
    "id": "c442df2443d2fb87c8483de5112d9f5ab4ac2a5a",
    "citation_count": 13
  },
  "https://aclanthology.org/D16-1011": {
    "title": "Rationalizing Neural Predictions",
    "abstract": "Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task",
    "volume": "main",
    "checked": true,
    "id": "467d5d8fc766e73bfd3e9415f75479823f92c2f7",
    "citation_count": 619
  },
  "https://aclanthology.org/D16-1012": {
    "title": "Deep Multi-Task Learning with Shared Memory for Text Classification",
    "abstract": "Neural network based models have achieved impressive results on various specific tasks. However, in previous works, most models are learned separately based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks",
    "volume": "main",
    "checked": true,
    "id": "652a7e6090a6b10cbeb0883ddac2002620aa8e69",
    "citation_count": 43
  },
  "https://aclanthology.org/D16-1013": {
    "title": "Natural Language Comprehension with the EpiReader",
    "abstract": "We present the EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model's response to the questions. The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that the EpiReader sets a new state-of-the-art on the CNN and Children's Book Test machine comprehension benchmarks, outperforming previous neural models by a significant margin",
    "volume": "main",
    "checked": true,
    "id": "75fa915984f1903cd8d0e1ea54b9d008d5a87fe5",
    "citation_count": 92
  },
  "https://aclanthology.org/D16-1014": {
    "title": "Creating Causal Embeddings for Question Answering with Minimal Supervision",
    "abstract": "A common model for question answering (QA) is that a good answer is one that is closely related to the question, where relatedness is often determined using general-purpose lexical models such as word embeddings. We argue that a better approach is to look for answers that are related to the question in a relevant way, according to the information need of the question, which may be determined through task-specific embeddings. With causality as a use case, we implement this insight in three steps. First, we generate causal embeddings cost-effectively by bootstrapping cause-effect pairs extracted from free text using a small set of seed patterns. Second, we train dedicated embeddings over this data, by using task-specific contexts, i.e., the context of a cause is its effect. Finally, we extend a state-of-the-art reranking approach for QA to incorporate these causal embeddings. We evaluate the causal embedding models both directly with a casual implication task, and indirectly, in a downstream causal QA task using data from Yahoo! Answers. We show that explicitly modeling causality improves performance in both tasks. In the QA task our best model achieves 37.3% P@1, significantly outperforming a strong baseline by 7.7% (relative)",
    "volume": "main",
    "checked": true,
    "id": "37118ec6f278a83e8d7fbb057d8ef840fc81f99f",
    "citation_count": 44
  },
  "https://aclanthology.org/D16-1015": {
    "title": "Improving Semantic Parsing via Answer Type Inference",
    "abstract": "In this work, we show the possibility of inferring the answer type before solving a factoid question and leveraging the type information to improve semantic parsing. By replacing the topic entity in a question with its type, we are able to generate an abstract form of the question, whose answer corresponds to the answer type of the original question. A bidirectional LSTM model is built to train over the abstract form of questions and infer their answer types. It is also observed that if we convert a question into a statement form, our LSTM model achieves better accuracy. Using the predicted type information to rerank the logical forms returned by AgendaIL, one of the leading semantic parsers, we are able to improve the F1-score from 49.7% to 52.6% on the WEBQUESTIONS data",
    "volume": "main",
    "checked": true,
    "id": "f3594f9d60c98cac88f9033c69c2b666713ed6d6",
    "citation_count": 49
  },
  "https://aclanthology.org/D16-1016": {
    "title": "Semantic Parsing to Probabilistic Programs for Situated Question Answering",
    "abstract": "Situated question answering is the problem of answering questions about an environment such as an image or diagram. This problem requires jointly interpreting a question and an environment using background knowledge to select the correct answer. We present Parsing to Probabilistic Programs (P3), a novel situated question answering model that can use background knowledge and global features of the question/environment interpretation while retaining efficient approximate inference. Our key insight is to treat semantic parses as probabilistic programs that execute nondeterministically and whose possible executions represent environmental uncertainty. We evaluate our approach on a new, publicly-released data set of 5000 science diagram questions, outperforming several competitive classical and neural baselines",
    "volume": "main",
    "checked": true,
    "id": "9940e8b05c808a8dc5a688fa860bcf40ef3b59d3",
    "citation_count": 20
  },
  "https://aclanthology.org/D16-1017": {
    "title": "Event participant modelling with neural networks",
    "abstract": "A common problem in cognitive modelling is lack of access to accurate broad-coverage models of event-level surprisal. As shown in, e.g., Bicknell et al. (2010), event-level knowledge does affect human expectations for verbal arguments. For example, the model should be able to predict that mechanics are likely to check tires, while journalists are more likely to check typos. Similarly, we would like to predict what locations are likely for playing football or playing flute in order to estimate the surprisal of actually-encountered locations. Furthermore, such a model can be used to provide a probability distribution over fillers for a thematic role which is not mentioned in the text at all. To this end, we train two neural network models (an incremental one and a non-incremental one) on large amounts of automatically rolelabelled text. Our models are probabilistic and can handle several roles at once, which also enables them to learn interactions between different role fillers. Evaluation shows a drastic improvement over current state-of-the-art systems on modelling human thematic fit judgements, and we demonstrate via a sentence similarity task that the system learns highly useful embeddings",
    "volume": "main",
    "checked": true,
    "id": "d08d663d7795c76bb008f539b1ac7caf8a9ef26c",
    "citation_count": 24
  },
  "https://aclanthology.org/D16-1018": {
    "title": "Context-Dependent Sense Embedding",
    "abstract": "Word embedding has been widely studied and proven helpful in solving many natural language processing tasks. However, the ambiguity of natural language is always a problem on learning high quality word embeddings. A possible solution is sense embedding which trains embedding for each sense of words instead of each word. Some recent work on sense embedding uses context clustering methods to determine the senses of words, which is heuristic in nature. Other work creates a probabilistic model and performs word sense disambiguation and sense embedding iteratively. However, most of the previous work has the problems of learning sense embeddings based on imperfect word embeddings as well as ignoring the dependency between sense choices of neighboring words. In this paper, we propose a novel probabilistic model for sense embedding that is not based on problematic word embedding of polysemous words and takes into account the dependency between sense choices. Based on our model, we derive a dynamic programming inference algorithm and an Expectation-Maximization style unsupervised learning algorithm. The empirical studies show that our model outperforms the state-of-the-art model on a word sense induction task by a 13% relative gain. © 2016 Association for Computational Linguistics",
    "volume": "main",
    "checked": true,
    "id": "af439bd130ee3faab1cc475ff656c09b070907e8",
    "citation_count": 11
  },
  "https://aclanthology.org/D16-1019": {
    "title": "Jointly Embedding Knowledge Graphs and Logical Rules",
    "abstract": "Embedding knowledge graphs into continuous vector spaces has recently attracted increasing interest. Most existing methods perform the embedding task using only fact triples. Logical rules, although containing rich background information, have not been well studied in this task. This paper proposes a novel method of jointly embedding knowledge graphs and logical rules. The key idea is to represent and model triples and rules in a unified framework. Specifically, triples are represented as atomic formulae and modeled by the translation assumption, while rules represented as complex formulae and modeled by t-norm fuzzy logics. Embedding then amounts to minimizing a global loss over both atomic and complex formulae. In this manner, we learn embeddings compatible not only with triples but also with rules, which will certainly be more predictive for knowledge acquisition and inference. We evaluate our method with link prediction and triple classification tasks. Experimental results show that joint embedding brings significant and consistent improvements over stateof-the-art methods. Particularly, it enhances the prediction of new facts which cannot even be directly inferred by pure logical inference, demonstrating the capability of our method to learn more predictive embeddings",
    "volume": "main",
    "checked": true,
    "id": "2cefe5adb11295b830ce27176c6d84b66fb20c2c",
    "citation_count": 173
  },
  "https://aclanthology.org/D16-1020": {
    "title": "Learning Connective-based Word Representations for Implicit Discourse Relation Identification",
    "abstract": "We introduce a simple semi-supervised approach to improve implicit   discourse relation identification. This approach harnesses large   amounts of automatically extracted discourse connectives along with   their arguments to construct new distributional word   representations. Specifically, we represent words in the space of   discourse connectives as a way to directly encode their rhetorical   function. Experiments on the Penn Discourse Treebank demonstrate the   effectiveness of these task-tailored representations in predicting   implicit discourse relations. Our results indeed show that, despite   their simplicity, these connective-based representations outperform   various off-the-shelf word embeddings, and achieve state-of-the-art   performance on this problem",
    "volume": "main",
    "checked": true,
    "id": "78f4db90f4d115e3b69771903bfd9b15ba661a1d",
    "citation_count": 27
  },
  "https://aclanthology.org/D16-1021": {
    "title": "Aspect Level Sentiment Classification with Deep Memory Network",
    "abstract": "We introduce a deep memory network for aspect level sentiment classification. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation",
    "volume": "main",
    "checked": true,
    "id": "b28f7e2996b6ee2784dd2dbb8212cfa0c79ba9e7",
    "citation_count": 693
  },
  "https://aclanthology.org/D16-1022": {
    "title": "Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets",
    "abstract": "It is well-known that opinions have targets. Extracting such targets is an important problem of opinion mining because without knowing the target of an opinion, the opinion is of limited use. So far many algorithms have been proposed to extract opinion targets. However, an opinion target can be an entity or an aspect (part or attribute) of an entity. An opinion about an entity is an opinion about the entity as a whole, while an opinion about an aspect is just an opinion about that specific attribute or aspect of an entity. Thus, opinion targets should be separated into entities and aspects before use because they represent very different things about opinions. This paper proposes a novel algorithm, called Lifelong-RL, to solve the problem based on lifelong machine learning and relaxation labeling. Extensive experiments show that the proposed algorithm Lifelong-RL outperforms baseline methods markedly",
    "volume": "main",
    "checked": true,
    "id": "46c3981eb659db34df5856a0108b4f3ff44048b7",
    "citation_count": 28
  },
  "https://aclanthology.org/D16-1023": {
    "title": "Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification",
    "abstract": "In this paper, we study cross-domain sentiment classiﬁcation with neural network archi-tectures. We borrow the idea from Structural Correspondence Learning and use two auxiliary tasks to help induce a sentence embedding that supposedly works well across domains for sentiment classiﬁcation. We also propose to jointly learn this sentence embedding together with the sentiment classiﬁer itself. Experiment results demonstrate that our proposed joint model outperforms several state-of-the-art methods on ﬁve benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "2d38f7aab07d4435b2110602db4138ef20da4cc0",
    "citation_count": 156
  },
  "https://aclanthology.org/D16-1024": {
    "title": "Attention-based LSTM Network for Cross-Lingual Sentiment Classification",
    "abstract": "Most of the state-of-the-art sentiment classification methods are based on supervised learning algorithms which require large amounts of manually labeled data. However, the labeled resources are usually imbalanced in different languages. Cross-lingual sentiment classification tackles the problem by adapting the sentiment resources in a resource-rich language to resource-poor languages. In this study, we propose an attention-based bilingual representation learning model which learns the distributed semantics of the documents in both the source and the target languages. In each language, we use Long Short Term Memory (LSTM) network to model the documents, which has been proved to be very effective for word sequences. Meanwhile, we propose a hierarchical attention mechanism for the bilingual LSTM network. The sentence-level attention model learns which sentences of a document are more important for determining the overall sentiment while the word-level attention model learns which words in each sentence are decisive. The proposed model achieves good results on a benchmark dataset using English as the source language and Chinese as the target language",
    "volume": "main",
    "checked": true,
    "id": "b31a60f21dae6adfc66e6c1c04bc74b57638b000",
    "citation_count": 219
  },
  "https://aclanthology.org/D16-1025": {
    "title": "Neural versus Phrase-Based Machine Translation Quality: a Case Study",
    "abstract": "Within the field of Statistical Machine Translation (SMT), the neural approach (NMT) has recently emerged as the first technology able to challenge the long-standing dominance of phrase-based approaches (PBMT). In particular, at the IWSLT 2015 evaluation campaign, NMT outperformed well established state-of-the-art PBMT systems on English-German, a language pair known to be particularly hard because of morphology and syntactic differences. To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural versus phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data. For the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models -- such as the reordering of verbs -- while pointing out other aspects that remain to be improved",
    "volume": "main",
    "checked": true,
    "id": "d223fa07e86d6486375119b055d8fec77c21a325",
    "citation_count": 244
  },
  "https://aclanthology.org/D16-1026": {
    "title": "Zero-Resource Translation with Multi-Lingual Neural Machine Translation",
    "abstract": "In this paper, we propose a novel finetuning algorithm for the recently introduced multi-way, mulitlingual neural machine translate that enables zero-resource machine translation. When used together with novel many-to-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivot-based translation strategy, while keeping only one additional copy of attention-related parameters",
    "volume": "main",
    "checked": true,
    "id": "198ac64703e83d00eb0f51a4c4a7c77cb08a7e5c",
    "citation_count": 237
  },
  "https://aclanthology.org/D16-1027": {
    "title": "Memory-enhanced Decoder for Neural Machine Translation",
    "abstract": "We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory, as a natural but powerful extension to the state in the decoding RNN. This memory-enhanced RNN decoder is called \\textsc{MemDec}. At each time during decoding, \\textsc{MemDec} will read from this memory and write to this memory once, both with content-based addressing. Unlike the unbounded memory in previous work\\cite{RNNsearch} to store the representation of source sentence, the memory in \\textsc{MemDec} is a matrix with pre-determined size designed to better capture the information important for the decoding process at each time step. Our empirical study on Chinese-English translation shows that it can improve by $4.8$ BLEU upon Groundhog and $5.3$ BLEU upon on Moses, yielding the best performance achieved with the same training set",
    "volume": "main",
    "checked": true,
    "id": "6bae362f72f01d0ab08a5e9625b2ae93daefd5b6",
    "citation_count": 67
  },
  "https://aclanthology.org/D16-1028": {
    "title": "Semi-Supervised Learning of Sequence Models with Method of Moments",
    "abstract": "We propose a fast and scalable method for semi-supervised learning of sequence models, based on anchor words and moment matching. Our method can handle hidden Markov models with feature-based log-linear emissions. Unlike other semi-supervised methods, no decoding passes are necessary on the unlabeled data and no graph needs to be constructed— only one pass is necessary to collect moment statistics. The model parameters are estimated by solving a small quadratic program for each feature. Experiments on part-of-speech (POS) tagging for Twitter and for a low-resource language (Malagasy) show that our method can learn from very few annotated sentences",
    "volume": "main",
    "checked": true,
    "id": "4d17f62dec2897ef573bf50754bdcffc7f076ccb",
    "citation_count": 5
  },
  "https://aclanthology.org/D16-1029": {
    "title": "Learning from Explicit and Implicit Supervision Jointly For Algebra Word Problems",
    "abstract": "Automatically solving algebra word problems has raised considerable interest recently. Existing state-of-the-art approaches mainly rely on learning from human annotated equations. In this paper, we demonstrate that it is possible to efficiently mine algebra problems and their numerical solutions with little to no manual effort. To leverage the mined dataset, we propose a novel structured-output learning algorithm that aims to learn from both explicit (e.g., equations) and implicit (e.g., solutions) supervision signals jointly. Enabled by this new algorithm, our model gains 4.6% absolute improvement in accuracy on the ALG514 benchmark compared to the one without using implicit supervision. The final model also outperforms the current state-of-the-art approach by 3%",
    "volume": "main",
    "checked": true,
    "id": "7b6f61e6067d3fe520e01b19e2941aa122fcd564",
    "citation_count": 35
  },
  "https://aclanthology.org/D16-1030": {
    "title": "TweeTime : A Minimally Supervised Method for Recognizing and Normalizing Time Expressions in Twitter",
    "abstract": "We describe TweeTIME, a temporal tagger for recognizing and normalizing time expressions in Twitter. Most previous work in social media analysis has to rely on temporal resolvers that are designed for well-edited text, and therefore suffer from the reduced performance due to domain mismatch. We present a minimally supervised method that learns from large quantities of unlabeled data and requires no hand-engineered rules or hand-annotated training corpora. TweeTIME achieves 0.68 F1 score on the end-to-end task of resolving date expressions, outperforming a broad range of state-of-the-art systems",
    "volume": "main",
    "checked": true,
    "id": "6799de9c30da44975f15980c428867b649e3a454",
    "citation_count": 15
  },
  "https://aclanthology.org/D16-1031": {
    "title": "Language as a Latent Variable: Discrete Generative Models for Sentence Compression",
    "abstract": "In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data",
    "volume": "main",
    "checked": true,
    "id": "c78468c2f87efabf8845ccd210ced364d45e5eab",
    "citation_count": 210
  },
  "https://aclanthology.org/D16-1032": {
    "title": "Globally Coherent Text Generation with Neural Checklist Models",
    "abstract": "Recurrent neural networks can generate locally coherent text but often have difficulties representing what has already been generated and what still needs to be said – especially when constructing long texts. We present the neural checklist model, a recurrent neural network that models global coherence by storing and updating an agenda of text strings which should be mentioned somewhere in the output. The model generates output by dynamically adjusting the interpolation among a language model and a pair of attention models that encourage references to agenda items. Evaluations on cooking recipes and dialogue system responses demonstrate high coherence with greatly improved semantic coverage of the agenda",
    "volume": "main",
    "checked": true,
    "id": "3a0a3fbae91d98597d3d7bf5c33ff3eb818dc0a9",
    "citation_count": 201
  },
  "https://aclanthology.org/D16-1033": {
    "title": "A Dataset and Evaluation Metrics for Abstractive Compression of Sentences and Short Paragraphs",
    "abstract": "We introduce a manually-created, multi-reference dataset for abstractive sentence and short paragraph compression. First, we examine the impact of single- and multi-sentence level editing operations on human compression quality as found in this corpus. We observe that substitution and rephrasing operations are more meaning preserving than other operations, and that compressing in context improves quality. Second, we systematically explore the correlations between automatic evaluation metrics and human judgments of meaning preservation and grammaticality in the compression task, and analyze the impact of the linguistic units used and precision versus recall measures on the quality of the metrics. Multi-reference evaluation metrics are shown to offer significant advantage over single reference-based metrics",
    "volume": "main",
    "checked": true,
    "id": "b795767b3906d4b1c6608f6a5af67172bf6a3955",
    "citation_count": 48
  },
  "https://aclanthology.org/D16-1034": {
    "title": "PaCCSS-IT: A Parallel Corpus of Complex-Simple Sentences for Automatic Text Simplification",
    "abstract": "In this paper we present PaCCSS–IT, a Parallel Corpus of Complex–Simple Sentences for ITalian. To build the resource we develop a new method for automatically acquiring a corpus of complex–simple paired sentences able to intercept structural transformations and particularly suitable for text simplification. The method requires a wide amount of texts that can be easily extracted from the web making it suitable also for less–resourced languages. We test it on the Italian language making available the biggest Italian corpus for automatic text simplification",
    "volume": "main",
    "checked": true,
    "id": "db0c79ae2e668b41f4ac9e4d39b7c9f1dc4f911e",
    "citation_count": 29
  },
  "https://aclanthology.org/D16-1035": {
    "title": "Discourse Parsing with Attention-based Hierarchical Neural Networks",
    "abstract": "RST-style document-level discourse parsing remains a difficult task and efficient deep learning models on this task have rarely been presented. In this paper, we propose an attention-based hierarchical neural network model for discourse parsing. We also incorporate tensor-based transformation function to model complicated feature interactions. Experimental results show that our approach obtains comparable performance to the contemporary state-of-the-art systems with little manual feature engineering",
    "volume": "main",
    "checked": true,
    "id": "6cf767e3afcea890e759d4f07fd1620b3f3684c7",
    "citation_count": 94
  },
  "https://aclanthology.org/D16-1036": {
    "title": "Multi-view Response Selection for Human-Computer Conversation",
    "abstract": "In this paper, we study the task of response selection for multi-turn human-computer conversation. Previous approaches take word as a unit and view context and response as sequences of words. This kind of approaches do not explicitly take each utterance as a unit, therefore it is difficult to catch utterancelevel discourse information and dependencies. In this paper, we propose a multi-view response selection model that integrates information from two different views, i.e., word sequence view and utterance sequence view. We jointly model the two views via deep neural networks. Experimental results on a public corpus for context-sensitive response selection demonstrate the effectiveness of the proposed multi-view model, which significantly outperforms other single-view baselines",
    "volume": "main",
    "checked": true,
    "id": "2e0bed618d023cad81eae218e69afce8bef8e4d6",
    "citation_count": 197
  },
  "https://aclanthology.org/D16-1037": {
    "title": "Variational Neural Discourse Relation Recognizer",
    "abstract": "Implicit discourse relation recognition is a crucial component for automatic discourselevel analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse relation recognizer. We refer to this model as VarNDRR. VarNDRR establishes a directed probabilistic model with a latent continuous variable that generates both a discourse and the relation between the two arguments of the discourse. In order to perform efficient inference and learning, we introduce neural discourse relation models to approximate the prior and posterior distributions of the latent variable, and employ these approximated distributions to optimize a reparameterized variational lower bound. This allows VarNDRR to be trained with standard stochastic gradient methods. Experiments on the benchmark data set show that VarNDRR can achieve comparable results against stateof- the-art baselines without using any manual features",
    "volume": "main",
    "checked": true,
    "id": "2891789ef2413f7979ae62fd3ce296b56059e7f1",
    "citation_count": 18
  },
  "https://aclanthology.org/D16-1038": {
    "title": "Event Detection and Co-reference with Minimal Supervision",
    "abstract": "An important aspect of natural language understanding involves recognizing and categorizing events and the relations among them. However, these tasks are quite subtle and annotating training data for machine learning based approaches is an expensive task, resulting in supervised systems that attempt to learn complex models from small amounts of data, which they over-fit. This paper addresses this challenge by developing an event detection and co-reference system with minimal supervision, in the form of a few event examples. We view these tasks as semantic similarity problems between event mentions or event mentions and an ontology of types, thus facilitating the use of large amounts of out of domain text data. Notably, our semantic relatedness function exploits the structure of the text by making use of a semantic-role-labeling based representation of an event. We show that our approach to event detection is competitive with the top supervised methods. More significantly, we outperform stateof-the-art supervised methods for event coreference on benchmark data sets, and support significantly better transfer across domains",
    "volume": "main",
    "checked": true,
    "id": "6df8cd4c69e75b286b1ba27417fd41a21d4982e1",
    "citation_count": 94
  },
  "https://aclanthology.org/D16-1039": {
    "title": "Learning Term Embeddings for Taxonomic Relation Identification Using Dynamic Weighting Neural Network",
    "abstract": "Taxonomic relation identification aims to recognize the 'is-a' relation between two terms. Previous works on identifying taxonomic relations are mostly based on statistical and linguistic approaches, but the accuracy of these approaches is far from satisfactory. In this paper, we propose a novel supervised learning approach for identifying taxonomic relations using term embeddings. For this purpose, we first design a dynamic weighting neural network to learn term embeddings based on not only the hypernym and hyponym terms, but also the contextual information between them. We then apply such embeddings as features to identify taxonomic relations using a supervised method. The experimental results show that our proposed approach significantly outperforms other state-of-the-art methods by 9% to 13% in terms of accuracy for both general and specific domain datasets",
    "volume": "main",
    "checked": true,
    "id": "a27e243d2ef62644e7a2a1fa51878fe7dbca4479",
    "citation_count": 63
  },
  "https://aclanthology.org/D16-1040": {
    "title": "Relation Schema Induction using Tensor Factorization with Side Information",
    "abstract": "Given a set of documents from a specific domain (e.g., medical research journals), how do we automatically build a Knowledge Graph (KG) for that domain? Automatic identification of relations and their schemas, i.e., type signature of arguments of relations (e.g., undergo(Patient, Surgery)), is an important first step towards this goal. We refer to this problem as Relation Schema Induction (RSI). In this paper, we propose Schema Induction using Coupled Tensor Factorization (SICTF), a novel tensor factorization method for relation schema induction. SICTF factorizes Open Information Extraction (OpenIE) triples extracted from a domain corpus along with additional side information in a principled way to induce relation schemas. To the best of our knowledge, this is the first application of tensor factorization for the RSI problem. Through extensive experiments on multiple real-world datasets, we find that SICTF is not only more accurate than state-of-the-art baselines, but also significantly faster (about 14x faster)",
    "volume": "main",
    "checked": true,
    "id": "c1c39ebad25d1bcde6587138bd2ffa9b3a250781",
    "citation_count": 22
  },
  "https://aclanthology.org/D16-1041": {
    "title": "Supervised Distributional Hypernym Discovery via Domain Adaptation",
    "abstract": "Comunicacio presentada a la Conference on Empirical Methods in Natural Language Processing celebrada els dies 1 a 5 de novembre de 2016 a Austin, Texas",
    "volume": "main",
    "checked": true,
    "id": "157c7cc5f51c69bfad84b68d43e44470418c3a5c",
    "citation_count": 53
  },
  "https://aclanthology.org/D16-1042": {
    "title": "Latent Tree Language Model",
    "abstract": "In this paper we introduce Latent Tree Language Model (LTLM), a novel approach to language modeling that encodes syntax and semantics of a given sentence as a tree of word roles.   The learning phase iteratively updates the trees by moving nodes according to Gibbs sampling. We introduce two algorithms to infer a tree for a given sentence. The first one is based on Gibbs sampling. It is fast, but does not guarantee to find the most probable tree. The second one is based on dynamic programming. It is slower, but guarantees to find the most probable tree. We provide comparison of both algorithms.   We combine LTLM with 4-gram Modified Kneser-Ney language model via linear interpolation. Our experiments with English and Czech corpora show significant perplexity reductions (up to 46% for English and 49% for Czech) compared with standalone 4-gram Modified Kneser-Ney language model",
    "volume": "main",
    "checked": true,
    "id": "ca9d8946f05448852df1dc7e3e68d76c9f8b6654",
    "citation_count": 1
  },
  "https://aclanthology.org/D16-1043": {
    "title": "Comparing Data Sources and Architectures for Deep Visual Representation Learning in Semantics",
    "abstract": "Multi-modal distributional models learn grounded representations for improved performance in semantics. Deep visual representations, learned using convolutional neural networks, have been shown to achieve particularly high performance. In this study, we systematically compare deep visual representation learning techniques, experimenting with three well-known network architectures. In addition, we explore the various data sources that can be used for retrieving relevant images, showing that images from search engines perform as well as, or better than, those from manually crafted resources such as ImageNet. Furthermore, we explore the optimal number of images and the multi-lingual applicability of multi-modal semantics. We hope that these findings can serve as a guide for future research in the field",
    "volume": "main",
    "checked": true,
    "id": "9e9be88c361df56ae1221f5c4ebbb4ae2cbe0a27",
    "citation_count": 23
  },
  "https://aclanthology.org/D16-1044": {
    "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding",
    "abstract": "Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge",
    "volume": "main",
    "checked": true,
    "id": "fddc15480d086629b960be5bff96232f967f2252",
    "citation_count": 1203
  },
  "https://aclanthology.org/D16-1045": {
    "title": "The Structured Weighted Violations Perceptron Algorithm",
    "abstract": "We present the Structured Weighted Violations Perceptron (SWVP) algorithm, a new structured prediction algorithm that generalizes the Collins Structured Perceptron (CSP). Unlike CSP, the update rule of SWVP explicitly exploits the internal structure of the predicted labels. We prove the convergence of SWVP for linearly separable training sets, provide mistake and generalization bounds, and show that in the general case these bounds are tighter than those of the CSP special case. In synthetic data experiments with data drawn from an HMM, various variants of SWVP substantially outperform its CSP special case. SWVP also provides encouraging initial dependency parsing results",
    "volume": "main",
    "checked": true,
    "id": "6b280fc096e6af4295c2e9b8bb81cb4c2d0c2ac1",
    "citation_count": 1
  },
  "https://aclanthology.org/D16-1046": {
    "title": "How Transferable are Neural Networks in NLP Applications?",
    "abstract": "Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. It is particularly important to neural networks, which are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct systematic case studies and provide an illuminating picture on the transferability of neural networks in NLP",
    "volume": "main",
    "checked": true,
    "id": "f21b36a5cfba94bbc6ed5c3642e1e46057deccaf",
    "citation_count": 272
  },
  "https://aclanthology.org/D16-1047": {
    "title": "Morphological Priors for Probabilistic Neural Word Embeddings",
    "abstract": "Word embeddings allow natural language processing systems to share statistical information across related words. These embeddings are typically based on distributional statistics, making it difficult for them to generalize to rare or unseen words. We propose to improve word embeddings by incorporating morphological information, capturing shared sub-word features. Unlike previous work that constructs word embeddings directly from morphemes, we combine morphological and distributional information in a unified probabilistic framework, in which the word embedding is a latent variable. The morphological information provides a prior distribution on the latent word embeddings, which in turn condition a likelihood function over an observed corpus. This approach yields improvements on intrinsic word similarity evaluations, and also in the downstream task of part-of-speech tagging",
    "volume": "main",
    "checked": true,
    "id": "f690c45d7cb80ce46a438c4dd1f9b50e1a45a84e",
    "citation_count": 41
  },
  "https://aclanthology.org/D16-1048": {
    "title": "Automatic Cross-Lingual Similarization of Dependency Grammars for Tree-based Machine Translation",
    "abstract": "Structural isomorphism between languages benefits the performance of cross-lingual applications. We propose an automatic algorithm for cross-lingual similarization of dependency grammars, which automatically learns grammars with high cross-lingual similarity. The algorithm similarizes the annotation styles of the dependency grammars for two languages in the level of classification decisions, and gradually improves the cross-lingual similarity without losing linguistic knowledge resorting to iterative crosslingual cooperative learning. The dependency grammars given by cross-lingual similarization have much higher cross-lingual similarity while maintaining non-triviality. As applications, the cross-lingually similarized grammars significantly improve the performance of dependency tree-based machine translation",
    "volume": "main",
    "checked": true,
    "id": "961d57fc4bf51f73d8fa6bb30a7d5566255c1f82",
    "citation_count": 1
  },
  "https://aclanthology.org/D16-1049": {
    "title": "IRT-based Aggregation Model of Crowdsourced Pairwise Comparison for Evaluating Machine Translations",
    "abstract": "Recent work on machine translation has used crowdsourcing to reduce costs of manual evaluations. However, crowdsourced judgments are often biased and inaccurate. In this paper, we present a statistical model that aggregates many manual pairwise comparisons to robustly measure a machine translation system's performance. Our method applies graded response model from item response theory (IRT), which was originally developed for academic tests. We conducted experiments on a public dataset from the Workshop on Statistical Machine Translation 2013, and found that our approach resulted in highly interpretable estimates and was less affected by noisy judges than previously proposed methods",
    "volume": "main",
    "checked": true,
    "id": "162445849d0d14a598d663c7bcdc006974268a53",
    "citation_count": 11
  },
  "https://aclanthology.org/D16-1050": {
    "title": "Variational Neural Machine Translation",
    "abstract": "Models of neural machine translation are often from a discriminative family of encoderdecoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoderdecoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform efficient posterior inference and large-scale training, we build a neural posterior approximator conditioned on both the source and the target sides, and equip it with a reparameterization technique to estimate the variational lower bound. Experiments on both Chinese-English and English- German translation tasks show that the proposed variational neural machine translation achieves significant improvements over the vanilla neural machine translation baselines",
    "volume": "main",
    "checked": true,
    "id": "ceb154e8f8ac411914f3327d67257776db3aa413",
    "citation_count": 180
  },
  "https://aclanthology.org/D16-1051": {
    "title": "Towards a Convex HMM Surrogate for Word Alignment",
    "abstract": "Among the alignment models used in statistical machine translation (SMT), the hidden Markov model (HMM) is arguably the most elegant: it performs consistently better than IBM Model 3 and is very close in performance to the much more complex IBM Model 4. In this paper we discuss a model which combines the structure of the HMM and IBM Model 2. Using this surrogate, our experiments show that we can attain a similar level of alignment quality as the HMM model implemented in GIZA++ (Och and Ney, 2003). For this model, we derive its convex relaxation and show that it too has strong performance despite not having the local optima problems of non-convex objectives. In particular, the word alignment quality of this new convex model is significantly above that of the standard IBM Models 2 and 3, as well as the popular (and still non-convex) IBM Model 2 variant of (Dyer et al., 2013)",
    "volume": "main",
    "checked": true,
    "id": "3b35dfbad2d96265845848a0d24572ca163cda82",
    "citation_count": 0
  },
  "https://aclanthology.org/D16-1052": {
    "title": "Solving Verbal Questions in IQ Test by Knowledge-Powered Word Embedding",
    "abstract": "Verbal comprehension questions appear very frequently in Intelligence Quotient (IQ) tests, which measure human's verbal ability including the understanding of the words with multiple senses, the synonyms and antonyms, and the analogies among words. In this work, we explore whether such tests can be solved automatically by the deep learning technologies for text data. We found that the task was quite challenging, and simply applying existing technologies like word embedding could not achieve a good performance, due to the multiple senses of words and the complex relations among words. To tackle these challenges, we propose a novel framework to automatically solve the verbal IQ questions by leveraging improved word embedding by jointly considering the multi-sense nature of words and the relational information among words. Experimental results have shown that the proposed framework can not only outperform existing methods for solving verbal comprehension questions but also exceed the average performance of the Amazon Mechanical Turk workers involved in the study",
    "volume": "main",
    "checked": true,
    "id": "ccb46d96515840d11ec4aff214d094dd48cd46ab",
    "citation_count": 10
  },
  "https://aclanthology.org/D16-1053": {
    "title": "Long Short-Term Memory-Networks for Machine Reading",
    "abstract": "In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art",
    "volume": "main",
    "checked": true,
    "id": "13fe71da009484f240c46f14d9330e932f8de210",
    "citation_count": 855
  },
  "https://aclanthology.org/D16-1054": {
    "title": "On Generating Characteristic-rich Question Sets for QA Evaluation",
    "abstract": "We present a semi-automated framework for constructing factoid question answering (QA) datasets, where an array of question characteristics are formalized, including structure complexity, function, commonness, answer cardinality, and paraphrasing. Instead of collecting questions and manually characterizing them, we employ a reverse procedure, first generating a kind of graph-structured logical forms from a knowledge base, and then converting them into questions. Our work is the first to generate questions with explicitly specified characteristics for QA evaluation. We construct a new QA dataset with over 5,000 logical form-question pairs, associated with answers from the knowledge base, and show that datasets constructed in this way enable finegrained analyses of QA systems. The dataset can be found in https://github.com/ ysu1989/GraphQuestions",
    "volume": "main",
    "checked": true,
    "id": "ce23476759263bd3f5e95fc758385eb62b3ab59a",
    "citation_count": 84
  },
  "https://aclanthology.org/D16-1055": {
    "title": "Learning to Translate for Multilingual Question Answering",
    "abstract": "In multilingual question answering, either the question needs to be translated into the document language, or vice versa. In addition to direction, there are multiple methods to perform the translation, four of which we explore in this paper: word-based, 10-best, context-based, and grammar-based. We build a feature for each combination of translation direction and method, and train a model that learns optimal feature weights. On a large forum dataset consisting of posts in English, Arabic, and Chinese, our novel learn-to-translate approach was more effective than a strong baseline (p<0.05): translating all text into English, then training a classifier based only on English (original or translated) text",
    "volume": "main",
    "checked": true,
    "id": "35e406eb52d8097346df41537da354abdd96f775",
    "citation_count": 23
  },
  "https://aclanthology.org/D16-1056": {
    "title": "A Semiparametric Model for Bayesian Reader Identification",
    "abstract": "We study the problem of identifying individuals based on their characteristic gaze patterns during reading of arbitrary text. The motivation for this problem is an unobtrusive biometric setting in which a user is observed during access to a document, but no specific challenge protocol requiring the user's time and attention is carried out. Existing models of individual differences in gaze control during reading are either based on simple aggregate features of eye movements, or rely on parametric density models to describe, for instance, saccade amplitudes or word fixation durations. We develop flexible semiparametric models of eye movements during reading in which densities are inferred under a Gaussian process prior centered at a parametric distribution family that is expected to approximate the true distribution well. An empirical study on reading data from 251 individuals shows significant improvements over the state of the art",
    "volume": "main",
    "checked": true,
    "id": "cc44780f2a12d39736deb055fb3ea2515cae62d8",
    "citation_count": 9
  },
  "https://aclanthology.org/D16-1057": {
    "title": "Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora",
    "abstract": "A word's sentiment depends on the domain in which it is used. Computational social science research thus requires sentiment lexicons that are specific to the domains being studied. We combine domain-specific word embeddings with a label propagation framework to induce accurate domain-specific sentiment lexicons using small sets of seed words. We show that our approach achieves state-of-the-art performance on inducing sentiment lexicons from domain-specific corpora and that our purely corpus-based approach outperforms methods that rely on hand-curated resources (e.g., WordNet). Using our framework, we induce and release historical sentiment lexicons for 150 years of English and community-specific sentiment lexicons for 250 online communities from the social media forum Reddit. The historical lexicons we induce show that more than 5% of sentiment-bearing (non-neutral) English words completely switched polarity during the last 150 years, and the community-specific lexicons highlight how sentiment varies drastically between different communities",
    "volume": "main",
    "checked": true,
    "id": "c6fe37beac50446012a87aac9fc2f8c4f34891e1",
    "citation_count": 292
  },
  "https://aclanthology.org/D16-1058": {
    "title": "Attention-based LSTM for Aspect-level Sentiment Classification",
    "abstract": "Aspect-level sentiment classification is a fine-grained task in sentiment analysis. Since it provides more complete and in-depth results, aspect-level sentiment analysis has received much attention these years. In this paper, we reveal that the sentiment polarity of a sentence is not only determined by the content but is also highly related to the concerned aspect. For instance, \"The appetizers are ok, but the service is slow.\", for aspect taste, the polarity is positive while for service, the polarity is negative. Therefore, it is worthwhile to explore the connection between an aspect and the content of a sentence. To this end, we propose an Attention-based Long Short-Term Memory Network for aspect-level sentiment classification. The attention mechanism can concentrate on different parts of a sentence when different aspects are taken as input. We experiment on the SemEval 2014 dataset and results show that our model achieves state-ofthe-art performance on aspect-level sentiment classification",
    "volume": "main",
    "checked": true,
    "id": "82bb306038446302cedd20fa986d20640ed88a2e",
    "citation_count": 1422
  },
  "https://aclanthology.org/D16-1059": {
    "title": "Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis",
    "abstract": "In aspect-based sentiment analysis, extracting aspect terms along with the opinions being expressed from user-generated content is one of the most important subtasks. Previous studies have shown that exploiting connections between aspect and opinion terms is promising for this task. In this paper, we propose a novel joint model that integrates recursive neural networks and conditional random fields into a unified framework for explicit aspect and opinion terms co-extraction. The proposed model learns high-level discriminative features and double propagate information between aspect and opinion terms, simultaneously. Moreover, it is flexible to incorporate hand-crafted features into the proposed model to further boost its information extraction performance. Experimental results on the SemEval Challenge 2014 dataset show the superiority of our proposed model over several baseline methods as well as the winning systems of the challenge",
    "volume": "main",
    "checked": true,
    "id": "1c72d8090f281f58d5b141a3917e341250ab4c8c",
    "citation_count": 291
  },
  "https://aclanthology.org/D16-1060": {
    "title": "Extracting Aspect Specific Opinion Expressions",
    "abstract": "Opinionated expression extraction is a central problem in fine-grained sentiment analysis. Most existing works focus on either generic subjective expression or aspect expression extraction. However, in opinion mining, it is often desirable to mine the aspect specific opinion expressions (or aspectsentiment phrases) containing both the aspect and the opinion. This paper proposes a hybrid generative-discriminative framework for extracting such expressions. The hybrid model consists of (i) an unsupervised generative component for modeling the semantic coherence of terms (words/phrases) based on their collocations across different documents, and (ii) a supervised discriminative sequence modeling component for opinion phrase extraction. Experimental results using Amazon.com reviews demonstrate the effectiveness of the approach that significantly outperforms several state-of-the-art baselines",
    "volume": "main",
    "checked": true,
    "id": "81400418aee6c057019d1902c5e06436428fb29e",
    "citation_count": 9
  },
  "https://aclanthology.org/D16-1061": {
    "title": "Emotion Distribution Learning from Texts",
    "abstract": "The advent of social media and its prosperity enable users to share their opinions and views. Understanding users' emotional states might provide the potential to create new business opportunities. Automatically identifying users' emotional states from their texts and classifying emotions into finite categories such as joy, anger, disgust, etc., can be considered as a text classification problem. However, it introduces a challenging learning scenario where multiple emotions with different intensities are often found in a single sentence. Moreover, some emotions co-occur more often while other emotions rarely coexist. In this paper, we propose a novel approach based on emotion distribution learning in order to address the aforementioned issues. The key idea is to learn a mapping function from sentences to their emotion distributions describing multiple emotions and their respective intensities. Moreover, the relations of emotions are captured based on the Plutchik's wheel of emotions and are subsequently incorporated into the learning algorithm in order to improve the accuracy of emotion detection. Experimental results show that the proposed approach can effectively deal with the emotion distribution detection problem and perform remarkably better than both the state-of-theart emotion detection method and multi-label learning methods",
    "volume": "main",
    "checked": true,
    "id": "ac9eca5ee4485cebb1d634d00f0a42a2510bb8a3",
    "citation_count": 90
  },
  "https://aclanthology.org/D16-1062": {
    "title": "Building an Evaluation Scale using Item Response Theory",
    "abstract": "Evaluation of NLP methods requires testing against a previously vetted gold-standard test set and reporting standard metrics (accuracy/precision/recall/F1). The current assumption is that all items in a given test set are equal with regards to difficulty and discriminating power. We propose Item Response Theory (IRT) from psychometrics as an alternative means for gold-standard test-set generation and NLP system evaluation. IRT is able to describe characteristics of individual items - their difficulty and discriminating power - and can account for these characteristics in its estimation of human intelligence or ability for an NLP task. In this paper, we demonstrate IRT by generating a gold-standard test set for Recognizing Textual Entailment. By collecting a large number of human responses and fitting our IRT model, we show that our IRT model compares NLP systems with the performance in a human population and is able to provide more insight into system performance than standard evaluation metrics. We show that a high accuracy score does not always imply a high IRT score, which depends on the item characteristics and the response pattern",
    "volume": "main",
    "checked": true,
    "id": "739bf9a7451712bca3094e626632dce0ae715224",
    "citation_count": 43
  },
  "https://aclanthology.org/D16-1063": {
    "title": "WordRank: Learning Word Embeddings via Robust Ranking",
    "abstract": "Embedding words in a vector space has gained a lot of attention in recent years. While state-of-the-art methods provide efficient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear. In this paper, we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics. Then, based on this insight, we propose a novel framework WordRank that efficiently estimates word representations via robust ranking, in which the attention mechanism and robustness to noise are readily achieved via the DCG-like ranking losses. The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques. Our algorithm is very competitive to the state-of-the- arts on large corpora, while outperforms them by a significant margin when the training set is limited (i.e., sparse and noisy). With 17 million tokens, WordRank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark. Our multi-node distributed implementation of WordRank is publicly available for general usage",
    "volume": "main",
    "checked": true,
    "id": "6bf23e53cbbca9139dd8982f0bfa6072c2029dc0",
    "citation_count": 37
  },
  "https://aclanthology.org/D16-1064": {
    "title": "Exploring Semantic Representation in Brain Activity Using Word Embeddings",
    "abstract": "In this paper, we utilize distributed word representations (i.e., word embeddings) to analyse the representation of semantics in brain activity. The brain activity data were recorded using functional magnetic resonance imaging (fMRI) when subjects were viewing words. First, we analysed the functional selectivity of different cortex areas by calculating the correlations between neural responses and several types of word representations, including skipgram word embeddings, visual semantic vectors, and primary visual features. The results demonstrated consistency with existing neuroscientific knowledge. Second, we utilized behavioural data as the semantic ground truth to measure their relevance with brain activity. A method to estimate word embeddings under the constraints of brain activity similarities is further proposed based on the semantic word embedding (SWE) model. The experimental results show that the brain activity data are significantly correlated with the behavioural data of human judgements on semantic similarity. The correlations between the estimated word embeddings and the semantic ground truth can be effectively improved after integrating the brain activity data for learning, which implies that semantic patterns in neural representations may exist that have not been fully captured by state-of-the-art word embeddings derived from text corpora",
    "volume": "main",
    "checked": true,
    "id": "dc6e0b2096911eb14d4e3e742996ee4a2905bd57",
    "citation_count": 12
  },
  "https://aclanthology.org/D16-1065": {
    "title": "AMR Parsing with an Incremental Joint Model",
    "abstract": "To alleviate the error propagation in the traditional pipelined models for Abstract Meaning Representation (AMR) parsing, we formulate AMR parsing as a joint task that performs the two subtasks: concept identification and relation identification simultaneously. To this end, we first develop a novel componentwise beam search algorithm for relation identification in an incremental fashion, and then incorporate the decoder into a unified framework based on multiple-beam search, which allows for the bi-directional information flow between the two subtasks in a single incremental model. Experiments on the public datasets demonstrate that our joint model significantly outperforms the previous pipelined counterparts, and also achieves better or comparable performance than other approaches to AMR parsing, without utilizing external semantic resources",
    "volume": "main",
    "checked": true,
    "id": "f6bab2ae3bb22d6b91f76d819ac514eb638213e2",
    "citation_count": 47
  },
  "https://aclanthology.org/D16-1066": {
    "title": "Identifying Dogmatism in Social Media: Signals and Models",
    "abstract": "We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-confidence), finding they have predictive power. We also find evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we find evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves",
    "volume": "main",
    "checked": true,
    "id": "b15d3de8789657542c1045cc2ac90b1b0670e017",
    "citation_count": 14
  },
  "https://aclanthology.org/D16-1067": {
    "title": "Enhanced Personalized Search using Social Data",
    "abstract": "Search personalization that considers the social dimension of the web has attracted a significant volume of research in recent years. A user profile is usually needed to represent a user's interests in order to tailor future searches. Previous research has typically constructed a profile solely from a user's usage information. When the user has only limited activities in the system, the effect of the user profile on search is also constrained. This research addresses the setting where a user has only a limited amount of usage information. We build enhanced user profiles from a set of annotations and resources that users have marked, together with an external knowledge base constructed according to usage histories. We present two probabilistic latent topic models to simultaneously incorporate social annotations, documents and the external knowledge base. Our web search strategy is achieved using personalized social query expansion. We introduce a topical query expansion model to enhance the search by utilizing individual user profiles. The proposed approaches have been intensively evaluated on a large public social annotation dataset. Results show that our models significantly outperformed existing personalized query expansion methods which use user profiles solely built from past usage information in personalized search",
    "volume": "main",
    "checked": true,
    "id": "a97c17a0d2c1dc057810fc2892d436db0b74c135",
    "citation_count": 4
  },
  "https://aclanthology.org/D16-1068": {
    "title": "Effective Greedy Inference for Graph-based Non-Projective Dependency Parsing",
    "abstract": "Exact inference in high-order graph-based non-projective dependency parsing is intractable. Hence, sophisticated approximation techniques based on algorithms such as belief propagation and dual decomposition have been employed. In contrast, we propose a simple greedy search approximation for this problem which is very intuitive and easy to implement. We implement the algorithm within the second-order TurboParser and experiment with the datasets of the CoNLL 2006 and 2007 shared task on multilingual dependency parsing. Our algorithm improves the run time of the parser by a factor of 1.43 while losing 1% in UAS on average across languages. Moreover, an ensemble method exploiting the joint power of the parsers, achieves an average UAS 0.27% higher than the TurboParser",
    "volume": "main",
    "checked": true,
    "id": "b35cdc4e82ed8c7a44147c19c528cbe3f87bf031",
    "citation_count": 2
  },
  "https://aclanthology.org/D16-1069": {
    "title": "Generating Abbreviations for Chinese Named Entities Using Recurrent Neural Network with Dynamic Dictionary",
    "abstract": "Chinese named entities occur frequently in formal and informal environments. Various approaches have been formalized the problem as a sequence labelling task and utilize a character-based methodology, in which character is treated as the basic classification unit. One of the main drawbacks of these methods is that some of the generated abbreviations may not follow the conventional wisdom of Chinese. To address this problem, we propose a novel neural network architecture to perform task. It combines recurrent neural network (RNN) with an architecture determining whether a given sequence of characters can be a word or not. For demonstrating the effectiveness of the proposed method, we evaluate it on Chinese named entity generation and opinion target extraction tasks. Experimental results show that the proposed method can achieve better performance than state-ofthe-art methods",
    "volume": "main",
    "checked": true,
    "id": "d0c4328d372e210d7f2536633c5ba3af1274ba78",
    "citation_count": 4
  },
  "https://aclanthology.org/D16-1070": {
    "title": "Neural Network for Heterogeneous Annotations",
    "abstract": "Multiple treebanks annotated under heterogeneous standards give rise to the research question of best utilizing multiple resources for improving statistical models. Prior research has focused on discrete models, leveraging stacking and multi-view learning to address the problem. In this paper, we empirically investigate heterogeneous annotations using neural network models, building a neural network counterpart to discrete stacking and multiview learning, respectively, finding that neural models have their unique advantages thanks to the freedom from manual feature engineering. Neural model achieves not only better accuracy improvements, but also an order of magnitude faster speed compared to its discrete baseline, adding little time cost compared to a neural model trained on a single treebank",
    "volume": "main",
    "checked": true,
    "id": "3669cded8d2158cf47cb519ae1d2a5facb122ea6",
    "citation_count": 31
  },
  "https://aclanthology.org/D16-1071": {
    "title": "LAMB: A Good Shepherd of Morphologically Rich Languages",
    "abstract": "This paper introduces STEM and LAMB, embeddings trained for stems and lemmata instead of for surface forms. For morphologically rich languages, they perform significantly better than standard embeddings on word similarity and polarity evaluations. On a new WordNet-based evaluation, STEM and LAMB are up to 50% better than standard embeddings. We show that both embeddings have high quality even for small dimensionality and training corpora",
    "volume": "main",
    "checked": true,
    "id": "1a26e105c1d0e7fb3eda59f0e1c072a910ec7264",
    "citation_count": 8
  },
  "https://aclanthology.org/D16-1072": {
    "title": "Fast Coupled Sequence Labeling on Heterogeneous Annotations via Context-aware Pruning",
    "abstract": "The recently proposed coupled sequence labeling is shown to be able to effectively exploit multiple labeled data with heterogeneous annotations but suffer from severe inefficiency problem due to the large bundled tag space (Li et al., 2015). In their case study of part-ofspeech (POS) tagging, Li et al. (2015) manually design context-free tag-to-tag mapping rules with a lot of effort to reduce the tag space. This paper proposes a context-aware pruning approach that performs token-wise constraints on the tag space based on contextual evidences, making the coupled approach efficient enough to be applied to themore complex task of joint word segmentation (WS) and POS tagging for the first time. Experiments show that using the large-scale People Daily as auxiliary heterogeneous data, the coupled approach can improve F-score by 95.55 − 94.88 = 0.67% on WS, and by 90.58 − 89.49 = 1.09% on joint WS&POS on Penn Chinese Treebank. All codes are released at http://hlt.suda.edu.cn/~zhli",
    "volume": "main",
    "checked": true,
    "id": "2f1bede073ff7c8136e66df22d3feaabef5747ac",
    "citation_count": 6
  },
  "https://aclanthology.org/D16-1073": {
    "title": "Unsupervised Neural Dependency Parsing",
    "abstract": "Unsupervised dependency parsing aims to learn a dependency grammar from text annotated with only POS tags. Various features and inductive biases are often used to incorporate prior knowledge into learning. One useful type of prior information is that there exist correlations between the parameters of grammar rules involving different POS tags. Previous work employed manually designed features or special prior distributions to encode such information. In this paper, we propose a novel approach to unsupervised dependency parsing that uses a neural model to predict grammar rule probabilities based on distributed representation of POS tags. The distributed representation is automatically learned from data and captures the correlations between POS tags. Our experiments show that our approach outperforms previous approaches utilizing POS correlations and is competitive with recent state-of-the-art approaches on nine different languages. \\u00a9 2016 Association for Computational Linguistics",
    "volume": "main",
    "checked": true,
    "id": "b360859eb746963767e554ae32cee1d1f3bcbc22",
    "citation_count": 53
  },
  "https://aclanthology.org/D16-1074": {
    "title": "Generating Coherent Summaries of Scientific Articles Using Coherence Patterns",
    "abstract": "Previous work on automatic summarization does not thoroughly consider coherence while generating the summary. We introduce a graph-based approach to summarize scientific articles. We employ coherence patterns to ensure that the generated summaries are coherent. The novelty of our model is twofold: we mine coherence patterns in a corpus of abstracts, and we propose a method to combine coherence, importance and non-redundancy to generate the summary. We optimize these factors simultaneously using Mixed Integer Programming. Our approach significantly outperforms baseline and state-of-the-art systems in terms of coherence (summary coherence assessment) and relevance (ROUGE scores)",
    "volume": "main",
    "checked": true,
    "id": "ded417776c70a885e8c06442b99b9dd49dbff2a8",
    "citation_count": 44
  },
  "https://aclanthology.org/D16-1075": {
    "title": "News Stream Summarization using Burst Information Networks",
    "abstract": "This paper studies summarizing key information from news streams. We propose simple yet effective models to solve the problem based on a novel and promising representation of text streams – Burst Information Networks (BINets). A BINet can be aware of redundant information, allows global analysis of a text stream, and can be efficiently built and dynamically updated, which perfectly fits the demands of text stream summarization. Extensive experiments show that the BINet-based approaches are not only efficient and can be used in a real-time online summarization setting, but also can generate high-quality summaries, outperforming the state-of-the-art approach",
    "volume": "main",
    "checked": true,
    "id": "574c8f5ed8c8b2779ce3eeb4a6d31f218a9073cd",
    "citation_count": 10
  },
  "https://aclanthology.org/D16-1076": {
    "title": "Rationale-Augmented Convolutional Neural Networks for Text Classification",
    "abstract": "We present a new Convolutional Neural Network (CNN) model for text classification that jointly exploits labels on documents and their constituent sentences. Specifically, we consider scenarios in which annotators explicitly mark sentences (or snippets) that support their overall document categorization, i.e., they provide rationales. Our model exploits such supervision via a hierarchical approach in which each document is represented by a linear combination of the vector representations of its component sentences. We propose a sentence-level convolutional model that estimates the probability that a given sentence is a rationale, and we then scale the contribution of each sentence to the aggregate document representation in proportion to these estimates. Experiments on five classification datasets that have document labels and associated rationales demonstrate that our approach consistently outperforms strong baselines. Moreover, our model naturally provides explanations for its predictions",
    "volume": "main",
    "checked": true,
    "id": "db0751478590cf6d9b61045bbd6f8ff4a9cc2294",
    "citation_count": 131
  },
  "https://aclanthology.org/D16-1077": {
    "title": "Transferring User Interests Across Websites with Unstructured Text for Cold-Start Recommendation",
    "abstract": "In this work, we investigate the possibility of cross-website transfer learning for tackling the cold-start problem. To address the coldstart issues commonly present in a collaborative ltering (CF) system, most existing crossdomain CF models require auxiliary rating data from another domain; nevertheless, under the cross-website scenario, such data is often unobtainable. Therefore, we propose the nearest-neighbor transfer matrix factorization (NT-MF) model, where a topic model is applied to the unstructured user-generated content in the source domain, and the similarity between users in the latent topic space is utilized to guide the target-domain CF model. Speci cally, the latent factors of the nearestneighbors are regarded as a set of pseudo observations, which can be used to estimate the unknown parameters in the model. Improvement over previous methods, especially for the cold-start users, is demonstrated with experiments on a real-world cross-website dataset",
    "volume": "main",
    "checked": true,
    "id": "d9669a0aeb9bd118dc03bdf94f1333a11e34c49d",
    "citation_count": 5
  },
  "https://aclanthology.org/D16-1078": {
    "title": "Speculation and Negation Scope Detection via Convolutional Neural Networks",
    "abstract": "Speculation and negation are important information to identify text factuality. In this paper, we propose a Convolutional Neural Network (CNN)-based model with probabilistic weighted average pooling to address speculation and negation scope detection. In particular, our CNN-based model extracts those meaningful features from various syntactic paths between the cues and the candidate tokens in both constituency and dependency parse trees. Evaluation on BioScope shows that our CNN-based model significantly outperforms the state-ofthe-art systems on Abstracts, a sub-corpus in BioScope, and achieves comparable performances on Clinical Records, another subcorpus in BioScope",
    "volume": "main",
    "checked": true,
    "id": "4f2ae80aae386d7cf32be372a411efceea11d2a3",
    "citation_count": 54
  },
  "https://aclanthology.org/D16-1079": {
    "title": "Analyzing Linguistic Knowledge in Sequential Model of Sentence",
    "abstract": "Sentence modelling is a fundamental topic in computational linguistics. Recently, deep learning-based sequential models of sentence, such as recurrent neural network, have proved to be effective in dealing with the non-sequential properties of human language. However, little is known about how a recurrent neural network captures linguistic knowledge. Here we propose to correlate the neuron activation pattern of a LSTM language model with rich language features at sequential, lexical and compositional level. Qualitative visualization as well as quantitative analysis under multilingual perspective reveals the effectiveness of gate neurons and indicates that LSTM learns to allow different neurons selectively respond to linguistic knowledge at different levels. Cross-language evidence shows that the model captures different aspects of linguistic properties for different languages due to the variance of syntactic complexity. Additionally, we analyze the influence of modelling strategy on linguistic knowledge encoded implicitly in different sequential models",
    "volume": "main",
    "checked": true,
    "id": "f14a86307b62232aa2cf68177c24bd71d010615f",
    "citation_count": 33
  },
  "https://aclanthology.org/D16-1080": {
    "title": "Keyphrase Extraction Using Deep Recurrent Neural Networks on Twitter",
    "abstract": "Keyphrases can provide highly condensed and valuable information that allows users to quickly acquire the main ideas. The task of automatically extracting them have received considerable attention in recent decades. Different from previous studies, which are usually focused on automatically extracting keyphrases from documents or articles, in this study, we considered the problem of automatically extracting keyphrases from tweets. Because of the length limitations of Twitter-like sites, the performances of existing methods usually drop sharply. We proposed a novel deep recurrent neural network (RNN) model to combine keywords and context information to perform this problem. To evaluate the proposed method, we also constructed a large-scale dataset collected from Twitter. The experimental results showed that the proposed method performs significantly better than previous methods",
    "volume": "main",
    "checked": true,
    "id": "2bde9b4149d416614b2866343bb9102a63bd6ae9",
    "citation_count": 155
  },
  "https://aclanthology.org/D16-1081": {
    "title": "Solving and Generating Chinese Character Riddles",
    "abstract": "Chinese character riddle is a riddle game in which the riddle solution is a single Chinese character. It is closely connected with the shape, pronunciation or meaning of Chinese characters. The riddle description (sentence) is usually composed of phrases with rich linguistic phenomena (such as pun, simile, and metaphor), which are associated to different parts (namely radicals) of the solution character. In this paper, we propose a statistical framework to solve and generate Chinese character riddles. Specifically, we learn the alignments and rules to identify the metaphors between phrases in riddles and radicals in characters. Then, in the solving phase, we utilize a dynamic programming method to combine the identified metaphors to obtain candidate solutions. In the riddle generation phase, we use a template-based method and a replacement-based method to obtain candidate riddle descriptions. We then use Ranking SVM to rerank the candidates both in the solving and generation process. Experimental results in the solving task show that the proposed method outperforms baseline methods. We also get very promising results in the generation task according to human judges",
    "volume": "main",
    "checked": true,
    "id": "c8681518bedf9a5cebcf896629fb0be1bd7bb3d0",
    "citation_count": 5
  },
  "https://aclanthology.org/D16-1082": {
    "title": "Structured prediction models for RNN based sequence labeling in clinical text",
    "abstract": "Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In clinical domain one major application of sequence labeling involves extraction of medical entities such as medication, indication, and side-effects from Electronic Health Record narratives. Sequence labeling in this domain, presents its own set of challenges and objectives. In this work we experimented with various CRF based structured learning models with Recurrent Neural Networks. We extend the previously studied LSTM-CRF models with explicit modeling of pairwise potentials. We also propose an approximate version of skip-chain CRF inference with RNN potentials. We use these methodologies for structured prediction in order to improve the exact phrase detection of various medical entities",
    "volume": "main",
    "checked": true,
    "id": "4acf3db41f1c34cf8d568ecd198820f9993da03c",
    "citation_count": 170
  },
  "https://aclanthology.org/D16-1083": {
    "title": "Learning to Represent Review with Tensor Decomposition for Spam Detection",
    "abstract": "Review spam detection is a key task in opinion mining. To accomplish this type of detection, previous work has focused mainly on effectively representing fake and non-fake reviews with discriminative features, which are discovered or elaborately designed by experts or developers. This paper proposes a novel review spam detection method that learns the representation of reviews automatically instead of heavily relying on experts' knowledge in a data-driven manner. More specifically, according to 11 relations (generated automatically from two basic patterns) between reviewers and products, we employ tensor decomposition to learn the embeddings of the reviewers and products in a vector space. We collect relations between any two entities (reviewers and products), which results in much useful and global information. We concatenate the review text, the embeddings of the reviewer and the reviewed product as the representation of a review. Based on such representations, the classifier could identify the opinion spam more precisely. Experimental results on an open Yelp dataset show that our method could effectively enhance the spam detection accuracy compared with the stateof-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "9abc08a7335700f803ec0ed0fe4f7c754a3b344d",
    "citation_count": 37
  },
  "https://aclanthology.org/D16-1084": {
    "title": "Stance Detection with Bidirectional Conditional Encoding",
    "abstract": "Stance detection is the task of classifying the attitude expressed in a text towards a target such as Hillary Clinton to be \"positive\", negative\" or \"neutral\". Previous work has assumed that either the target is mentioned in the text or that training data for every target is given. This paper considers the more challenging version of this task, where targets are not always mentioned and no training data is available for the test targets. We experiment with conditional LSTM encoding, which builds a representation of the tweet that is dependent on the target, and demonstrate that it outperforms encoding the tweet and the target independently. Performance is improved further when the conditional model is augmented with bidirectional encoding. We evaluate our approach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving performance second best only to a system trained on semi-automatically labelled tweets for the test target. When such weak supervision is added, our approach achieves state-of-the-art results",
    "volume": "main",
    "checked": true,
    "id": "8e7d063c681557c94382ff3da6415d3720fe11a7",
    "citation_count": 288
  },
  "https://aclanthology.org/D16-1085": {
    "title": "Modeling Skip-Grams for Event Detection with Convolutional Neural Networks",
    "abstract": "Convolutional neural networks (CNN) have achieved the top performance for event detection due to their capacity to induce the underlying structures of the k-grams in the sentences. However, the current CNN-based event detectors only model the consecutive k-grams and ignore the non-consecutive kgrams that might involve important structures for event detection. In this work, we propose to improve the current CNN models for ED by introducing the non-consecutive convolution. Our systematic evaluation on both the general setting and the domain adaptation setting demonstrates the effectiveness of the nonconsecutive CNN model, leading to the significant performance improvement over the current state-of-the-art systems",
    "volume": "main",
    "checked": true,
    "id": "fc5b90ec6a64145594e2fa4b2543b0d9a10906ff",
    "citation_count": 99
  },
  "https://aclanthology.org/D16-1086": {
    "title": "Porting an Open Information Extraction System from English to German",
    "abstract": "Many downstream NLP tasks can benefit from Open Information Extraction (Open IE) as a semantic representation. While Open IE systems are available for English, many other languages lack such tools. In this paper, we present a straightforward approach for adapting PropS, a rule-based predicate-argument analysis for English, to a new language, German. With this approach, we quickly obtain an Open IE system for German covering 89% of the English rule set. It yields 1.6 extractions per sentence with 60% precision, making it readily usable in downstream applications",
    "volume": "main",
    "checked": true,
    "id": "c9c27bb00f4135ac6db128e3478d705fbc16598e",
    "citation_count": 19
  },
  "https://aclanthology.org/D16-1087": {
    "title": "Named Entity Recognition for Novel Types by Transfer Learning",
    "abstract": "In named entity recognition, we often don't have a large in-domain training corpus or a knowledge base with adequate coverage to train a model directly. In this paper, we propose a method where, given training data in a related domain with similar (but not identical) named entity (NE) types and a small amount of in-domain training data, we use transfer learning to learn a domain-specific NE model. That is, the novelty in the task setup is that we assume not just domain mismatch, but also label mismatch",
    "volume": "main",
    "checked": true,
    "id": "e3b31ab11bbcc8c9003604292471529a34804400",
    "citation_count": 34
  },
  "https://aclanthology.org/D16-1088": {
    "title": "Extracting Subevents via an Effective Two-phase Approach",
    "abstract": "We present our pilot research on automatically extracting subevents from a domain-specific corpus, focusing on the type of subevents that describe physical actions composing an event. We decompose the challenging problem and propose a two-phase approach that effectively captures sentential and local cues that describe subevents. We extracted a rich set of over 600 novel subevent phrases. Evaluation shows the automatically learned subevents help to discover 10% additional main events (of which the learned subevents are a part) and improve event detection performance",
    "volume": "main",
    "checked": true,
    "id": "659f12c55f9db868946c0117e81b113266d7a8e9",
    "citation_count": 8
  },
  "https://aclanthology.org/D16-1089": {
    "title": "Gaussian Visual-Linguistic Embedding for Zero-Shot Recognition",
    "abstract": "An exciting outcome of research at the inter-section of language and vision is that of zero-shot learning (ZSL). ZSL promises to scale visual recognition by borrowing distributed semantic models learned from linguistic cor-pora and turning them into visual recognition models. However the popular word-vector DSM embeddings are relatively impoverished in their expressivity as they model each word as a single vector point. In this paper we explore word- distribution embeddings for ZSL. We present a visual-linguistic mapping for ZSL in the case where words and visual categories are both represented by distributions. Experiments show improved results on ZSL benchmarks due to this better exploiting of intra-concept variability in each modality",
    "volume": "main",
    "checked": true,
    "id": "492596eb82218085485ad77e7b414250f5289605",
    "citation_count": 37
  },
  "https://aclanthology.org/D16-1090": {
    "title": "Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions",
    "abstract": "Visual Question Answering (VQA) is the task of answering natural-language questions about images. We introduce the novel problem of determining the relevance of questions to images in VQA. Current VQA models do not reason about whether a question is even related to the given image (e.g. What is the capital of Argentina?) or if it requires information from external resources to answer correctly. This can break the continuity of a dialogue in human-machine interaction. Our approaches for determining relevance are composed of two stages. Given an image and a question, (1) we first determine whether the question is visual or not, (2) if visual, we determine whether the question is relevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA model uncertainty, and caption-question similarity, are able to outperform strong baselines on both relevance tasks. We also present human studies showing that VQA models augmented with such question relevance reasoning are perceived as more intelligent, reasonable, and human-like",
    "volume": "main",
    "checked": true,
    "id": "0eb859d4184476bd80d5f2090b3401c702f66135",
    "citation_count": 47
  },
  "https://aclanthology.org/D16-1091": {
    "title": "Sort Story: Sorting Jumbled Images and Captions into Stories",
    "abstract": "Temporal common sense has applications in AI tasks such as QA, multi-document summarization, and human-AI communication. We propose the task of sequencing -- given a jumbled set of aligned image-caption pairs that belong to a story, the task is to sort them such that the output sequence forms a coherent story. We present multiple approaches, via unary (position) and pairwise (order) predictions, and their ensemble-based combinations, achieving strong results on this task. We use both text-based and image-based features, which depict complementary improvements. Using qualitative examples, we demonstrate that our models have learnt interesting aspects of temporal common sense",
    "volume": "main",
    "checked": true,
    "id": "3cc0d9c1f690addd2c82e60f2a460e3c557ff242",
    "citation_count": 57
  },
  "https://aclanthology.org/D16-1092": {
    "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks look at the same regions?",
    "abstract": "We conduct large-scale studies on `human attention' in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Overall, our experiments show that current attention models in VQA do not seem to be looking at the same regions as humans",
    "volume": "main",
    "checked": true,
    "id": "58cb0c24c936b8a14ca7b2d56ba80de733c545b3",
    "citation_count": 377
  },
  "https://aclanthology.org/D16-1093": {
    "title": "Recurrent Residual Learning for Sequence Classification",
    "abstract": "In this paper, we explore the possibility of leveraging Residual Networks (ResNet), a powerful structure in constructing extremely deep neural network for image understanding, to improve recurrent neural networks (RNN) for modeling sequential data. We show that for sequence classification tasks, incorporating residual connections into recurrent structures yields similar accuracy to Long Short Term Memory (LSTM) RNN with much fewer model parameters. In addition, we propose two novel models which combine the best of both residual learning and LSTM. Experiments show that the new models significantly outperform LSTM",
    "volume": "main",
    "checked": true,
    "id": "b1f3198f2b49f8028cd1cdd2d685c144cd18cb9d",
    "citation_count": 92
  },
  "https://aclanthology.org/D16-1094": {
    "title": "Richer Interpolative Smoothing Based on Modified Kneser-Ney Language Modeling",
    "abstract": "In this work we present a generalisation of the Modified Kneser-Ney interpolative smoothing for richer smoothing via additional discount parameters. We provide mathematical underpinning for the estimator of the new discount parameters, and showcase the utility of our rich MKN language models on several European languages. We further explore the interdependency among the training data size, language model order, and number of discount parameters. Our empirical results illustrate that larger number of discount parameters, i) allows for better allocation of mass in the smoothing process, particularly on small data regime where statistical sparsity is severe, and ii) leads to significant reduction in perplexity, particularly for out-of-domain test sets which introduce higher ratio of out-ofvocabulary words.1",
    "volume": "main",
    "checked": true,
    "id": "14fcb7bf8c8b428bc95f2bc1460d65026fba56f9",
    "citation_count": 7
  },
  "https://aclanthology.org/D16-1095": {
    "title": "A General Regularization Framework for Domain Adaptation",
    "abstract": "We propose a domain adaptation framework, and formally prove that it generalizes the feature augmentation technique in (Daumé III, 2007) and the multi-task regularization framework in (Evgeniou and Pontil, 2004). We show that our framework is strictly more general than these approaches and allows practitioners to tune hyper-parameters to encourage transfer between close domains and avoid negative transfer between distant ones",
    "volume": "main",
    "checked": true,
    "id": "edd8587921d63c797cd0f2a7acca82852daba8d6",
    "citation_count": 11
  },
  "https://aclanthology.org/D16-1096": {
    "title": "Coverage Embedding Models for Neural Machine Translation",
    "abstract": "In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system",
    "volume": "main",
    "checked": true,
    "id": "f997c2f1f668b942c4cccd425bc192df651ed516",
    "citation_count": 135
  },
  "https://aclanthology.org/D16-1097": {
    "title": "Neural Morphological Analysis: Encoding-Decoding Canonical Segments",
    "abstract": "Canonical morphological segmentation aims to divide words into a sequence of standardized segments. In this work, we propose a character-based neural encoderdecoder model for this task. Additionally, we extend our model to include morphemelevel and lexical information through a neural reranker. We set the new state of the art for the task improving previous results by up to 21% accuracy. Our experiments cover three languages: English, German and Indonesian",
    "volume": "main",
    "checked": true,
    "id": "c75ecddc5eda6b71b54bf8ef40f6fa9da1d8f108",
    "citation_count": 40
  },
  "https://aclanthology.org/D16-1098": {
    "title": "Exploiting Mutual Benefits between Syntax and Semantic Roles using Neural Network",
    "abstract": "We investigate mutual benefits between syntax and semantic roles using neural network models, by studying a parsing→SRL pipeline, a SRL→parsing pipeline, and a simple joint model by embedding sharing. The integration of syntactic and semantic features gives promising results in a Chinese Semantic Treebank, demonstrating large potentials of neural models for joint parsing and semantic role labeling",
    "volume": "main",
    "checked": true,
    "id": "18fa452beeb2826f989f053758d007d1e3a78012",
    "citation_count": 7
  },
  "https://aclanthology.org/D16-1099": {
    "title": "The Effects of Data Size and Frequency Range on Distributional Semantic Models",
    "abstract": "This paper investigates the effects of data size and frequency range on distributional semantic models. We compare the performance of a number of representative models for several test settings over data of varying sizes, and over test items of various frequency. Our results show that neural network-based models underperform when the data is small, and that the most reliable model over data of varying sizes and frequency ranges is the inverted factorized model",
    "volume": "main",
    "checked": true,
    "id": "4b63af049b14cb57e0e82a6d214e73036de72a85",
    "citation_count": 50
  },
  "https://aclanthology.org/D16-1100": {
    "title": "Multi-Granularity Chinese Word Embedding",
    "abstract": "This paper considers the problem of learning Chinese word embeddings. In contrast to English, a Chinese word is usually composed of characters, and most of the characters themselves can be further divided into components such as radicals. While characters and radicals contain rich information and are capable of indicating semantic meanings of words, they have not been fully exploited by existing word embedding methods. In this work, we propose multi-granularity embedding (MGE) for Chinese words. The key idea is to make full use of such word-character-radical composition, and enrich word embeddings by further incorporating finer-grained semantics from characters and radicals. Quantitative evaluation demonstrates the superiority of MGE in word similarity computation and analogical reasoning. Qualitative analysis further shows its capability to identify finer-grained semantic meanings of words",
    "volume": "main",
    "checked": true,
    "id": "a5134affedeeac45a1123167e0ecbf52aa96cc1e",
    "citation_count": 80
  },
  "https://aclanthology.org/D16-1101": {
    "title": "Numerically Grounded Language Models for Semantic Error Correction",
    "abstract": "Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction. Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities. Our approach uses language models grounded in numbers within the text. Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases. Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches. Conditioning on a knowledge base yields further improvements",
    "volume": "main",
    "checked": true,
    "id": "7e1eea0235e1902b6e3219fd9de9105e2f08fd65",
    "citation_count": 13
  },
  "https://aclanthology.org/D16-1102": {
    "title": "Towards Semi-Automatic Generation of Proposition Banks for Low-Resource Languages",
    "abstract": "Annotation projection based on parallel corpora has shown great promise in inexpensively creating Proposition Banks for languages for which high-quality parallel corpora and syntactic parsers are available. In this paper, we present an experimental study where we apply this approach to three languages that lack such resources: Tamil, Bengali and Malayalam. We find an average quality difference of 6 to 20 absolute F-measure points vis-avis high-resource languages, which indicates that annotation projection alone is insufficient in low-resource scenarios. Based on these results, we explore the possibility of using annotation projection as a starting point for inexpensive data curation involving both experts and non-experts. We give an outline of what such a process may look like and present an initial study to discuss its potential and challenges",
    "volume": "main",
    "checked": true,
    "id": "cbe85e3e78c4292bf771774f8159b0145e8cdc7b",
    "citation_count": 15
  },
  "https://aclanthology.org/D16-1103": {
    "title": "A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis",
    "abstract": "Opinion mining from customer reviews has become pervasive in recent years. Sentences in reviews, however, are usually classified independently, even though they form part of a review's argumentative structure. Intuitively, sentences in a review build and elaborate upon each other; knowledge of the review structure and sentential context should thus inform the classification of each sentence. We demonstrate this hypothesis for the task of aspect-based sentiment analysis by modeling the interdependencies of sentences in a review with a hierarchical bidirectional LSTM. We show that the hierarchical model outperforms two non-hierarchical baselines, obtains results competitive with the state-of-the-art, and outperforms the state-of-the-art on five multilingual, multi-domain datasets without any hand-engineered features or external resources",
    "volume": "main",
    "checked": true,
    "id": "def0ec30fab1594a2686677868e7fed4c11fb7f4",
    "citation_count": 220
  },
  "https://aclanthology.org/D16-1104": {
    "title": "Are Word Embedding-based Features Useful for Sarcasm Detection?",
    "abstract": "This paper makes a simple increment to state-of-the-art in sarcasm detection research. Existing approaches are unable to capture subtle forms of context incongruity which lies at the heart of sarcasm. We explore if prior work can be enhanced using semantic similarity/discordance between word embeddings. We augment word embedding-based features to four feature sets reported in the past. We also experiment with four types of word embeddings. We observe an improvement in sarcasm detection, irrespective of the word embedding used or the original feature set to which our features are augmented. For example, this augmentation results in an improvement in F-score of around 4\\% for three out of these four feature sets, and a minor degradation in case of the fourth, when Word2Vec embeddings are used. Finally, a comparison of the four embeddings shows that Word2Vec and dependency weight-based features outperform LSA and GloVe, in terms of their benefit to sarcasm detection",
    "volume": "main",
    "checked": true,
    "id": "8c8a7513cfd1e66144a1771bf4e29b7388c57bb3",
    "citation_count": 136
  },
  "https://aclanthology.org/D16-1105": {
    "title": "Weakly Supervised Tweet Stance Classification by Relational Bootstrapping",
    "abstract": "Supervised stance classification, in such domains as Congressional debates and online forums, has been a topic of interest in the past decade. Approaches have evolved from text classification to structured output prediction, including collective classification and sequence labeling. In this work, we investigate collective classification of stances on Twitter, using hinge-loss Markov random fields (HLMRFs). Given the graph of all posts, users, and their relationships, we constrain the predicted post labels and latent user labels to correspond with the network structure. We focus on a weakly supervised setting, in which only a small set of hashtags or phrases is labeled. Using our relational approach, we are able to go beyond the stance-indicative patterns and harvest more stance-indicative tweets, which can also be used to train any linear text classifier when the network structure is not available or is costly",
    "volume": "main",
    "checked": true,
    "id": "96a8cb7004dc98b25d92f8310834c871e0c19d9b",
    "citation_count": 48
  },
  "https://aclanthology.org/D16-1106": {
    "title": "The Gun Violence Database: A new task and data set for NLP",
    "abstract": "We argue that NLP researchers are especially well-positioned to contribute to the national discussion about gun violence. Reasoning about the causes and outcomes of gun violence is typically dominated by politics and emotion, and data-driven research on the topic is stymied by a shortage of data and a lack of federal funding. However, data abounds in the form of unstructured text from news articles across the country. This is an ideal application of NLP technologies, such as relation extraction, coreference resolution, and event detection. We introduce a new and growing dataset, the Gun Violence Database, in order to facilitate the adaptation of current NLP technologies to the domain of gun violence, thus enabling better social science research on this important and under-resourced problem",
    "volume": "main",
    "checked": true,
    "id": "39cc240b6e359071fe1ddaac22473eb354f85a00",
    "citation_count": 24
  },
  "https://aclanthology.org/D16-1107": {
    "title": "Fluency detection on communication networks",
    "abstract": "When considering a social media corpus, we often have access to structural information about how messages are flowing between people or organizations. This information is particularly useful when the linguistic evidence is sparse, incomplete, or of dubious quality. In this paper we construct a simple model to leverage the structure of Twitter data to help determine the set of languages each user is fluent in. Our results demonstrate that imposing several intuitive constraints leads to improvements in performance and stability. We release the first annotated data set for exploring this task, and discuss how our approach may be extended to other applications",
    "volume": "main",
    "checked": true,
    "id": "0910320d6a9830211c747fea8633fd9b83fb4981",
    "citation_count": 0
  },
  "https://aclanthology.org/D16-1108": {
    "title": "Characterizing the Language of Online Communities and its Relation to Community Reception",
    "abstract": "This work investigates style and topic aspects of language in online communities: looking at both utility as an identifier of the community and correlation with community reception of content. Style is characterized using a hybrid word and part-of-speech tag n-gram language model, while topic is represented using Latent Dirichlet Allocation. Experiments with several Reddit forums show that style is a better indicator of community identity than topic, even for communities organized around specific topics. Further, there is a positive correlation between the community reception to a contribution and the style similarity to that community, but not so for topic similarity",
    "volume": "main",
    "checked": true,
    "id": "e9a4d110c2bda1df94ea98ce9713ce5ab64c94d8",
    "citation_count": 41
  },
  "https://aclanthology.org/D16-1109": {
    "title": "Joint Transition-based Dependency Parsing and Disfluency Detection for Automatic Speech Recognition Texts",
    "abstract": "Joint dependency parsing with disfluency detection is an important task in speech language processing. Recent methods show high performance for this task, although most authors make the unrealistic assumption that input texts are transcribed by human annotators. In real-world applications, the input text is typically the output of an automatic speech recognition (ASR) system, which implies that the text contains not only disfluency noises but also recognition errors from the ASR system. In this work, we propose a parsing method that handles both disfluency and ASR errors using an incremental shift-reduce algorithm with several novel features suited to ASR output texts. Because the gold dependency information is usually annotated only on transcribed texts, we also introduce an alignment-based method for transferring the gold dependency annotation to the ASR output texts to construct training data for our parser. We conducted an experiment on the Switchboard corpus and show that our method outperforms conventional methods in terms of dependency parsing and disfluency detection",
    "volume": "main",
    "checked": true,
    "id": "5399096acb4d3951177e7a254020f7d8c6d72f4d",
    "citation_count": 32
  },
  "https://aclanthology.org/D16-1110": {
    "title": "Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue Systems",
    "abstract": "In this paper, we describe our approach of enabling an interactive dialogue system to recognize user emotion and sentiment in realtime. These modules allow otherwise conventional dialogue systems to have \"empathy\" and answer to the user while being aware of their emotion and intent. Emotion recognition from speech previously consists of feature engineering and machine learning where the first stage causes delay in decoding time. We describe a CNN model to extract emotion from raw speech input without feature engineering. This approach even achieves an impressive average of 65.7% accuracy on six emotion categories, a 4.5% improvement when compared to the conventional feature based SVM classification. A separate, CNN-based sentiment analysis module recognizes sentiments from speech recognition results, with 82.5 Fmeasure on human-machine dialogues when trained with out-of-domain data",
    "volume": "main",
    "checked": true,
    "id": "2278d52eff38b4d5cb35a15b8e07b892ec6e213b",
    "citation_count": 80
  },
  "https://aclanthology.org/D16-1111": {
    "title": "A Neural Network Architecture for Multilingual Punctuation Generation",
    "abstract": "This work was supported by the European Commission under the contract numbers FP7-ICT-/n610411 (MULTISENSOR) and H2020-RIA-645012 (KRISTINA)",
    "volume": "main",
    "checked": true,
    "id": "080e5c10e846aab3acb6deca315105d145e02e26",
    "citation_count": 11
  },
  "https://aclanthology.org/D16-1112": {
    "title": "Neural Headline Generation on Abstract Meaning Representation",
    "abstract": "Neural network-based encoder-decoder models are among recent attractive methodologies for tackling natural language generation tasks. This paper investigates the usefulness of structural syntactic and semantic information additionally incorporated in a baseline neural attention-based model. We encode results obtained from an abstract meaning representation (AMR) parser using a modified version of Tree-LSTM. Our proposed attention-based AMR encoder-decoder model improves headline generation benchmarks compared with the baseline neural attention-based model",
    "volume": "main",
    "checked": true,
    "id": "4d1f12f1a28afc30aab6f5086b3f2e481cf1f49f",
    "citation_count": 148
  },
  "https://aclanthology.org/D16-1113": {
    "title": "Robust Gram Embeddings",
    "abstract": "Word embedding models learn vectorial word representations that can be used in a variety of NLP applications. When training data is scarce, these models risk losing their generalization abilities due to the complexity of the models and the overfitting to finite data. We propose a regularized embedding formulation, called Robust Gram (RG), which penalizes overfitting by suppressing the disparity between target and context embeddings. Our experimental analysis shows that the RG model trained on small datasets generalizes better compared to alternatives, is more robust to variations in the training set, and correlates well to human similarities in a set of word similarity tasks",
    "volume": "main",
    "checked": true,
    "id": "fca669c91d4010f4dce82e9fe3bd610bc96a3612",
    "citation_count": 3
  },
  "https://aclanthology.org/D16-1114": {
    "title": "SimpleScience: Lexical Simplification of Scientific Terminology",
    "abstract": "Lexical simplification of scientific terms represents a unique challenge due to the lack of a standard parallel corpora and fast rate at which vocabulary shift along with research. We introduce SimpleScience, a lexical simplification approach for scientific terminology. We use word embeddings to extract simplification rules from a parallel corpora containing scientific publications and Wikipedia. To evaluate our system we construct SimpleSciGold, a novel gold standard set for science-related simplifications. We find that our approach outperforms prior context-aware approaches at generating simplifications for scientific terms",
    "volume": "main",
    "checked": true,
    "id": "a07ce6abdb9edab5bfe32a8650e9f54de987af16",
    "citation_count": 21
  },
  "https://aclanthology.org/D16-1115": {
    "title": "Automatic Features for Essay Scoring – An Empirical Study",
    "abstract": "Essay scoring is a complicated processing requiring analyzing, summarizing and judging expertise. Traditional work on essay scoring focused on automatic handcrafted features, which are expensive yet sparse. Neural models offer a way to learn syntactic and semantic features automatically, which can potentially improve upon discrete features. In this paper, we employ convolutional neural network (CNN) for the effect of automatically learning features, and compare the result with the state-of-art discrete baselines. For in-domain and domain-adaptation essay scoring tasks, our neural model empirically outperforms discrete models",
    "volume": "main",
    "checked": true,
    "id": "d63abbe30e5d4bb13b41bf760a5ab09e9e6cc1f7",
    "citation_count": 111
  },
  "https://aclanthology.org/D16-1116": {
    "title": "Semantic Parsing with Semi-Supervised Sequential Autoencoders",
    "abstract": "We present a novel semi-supervised approach for sequence transduction and apply it to semantic parsing. The unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms",
    "volume": "main",
    "checked": true,
    "id": "1d00075feb9f2fecc1d035897b98eb2f45461b2e",
    "citation_count": 78
  },
  "https://aclanthology.org/D16-1117": {
    "title": "Equation Parsing : Mapping Sentences to Grounded Equations",
    "abstract": "Identifying mathematical relations expressed in text is essential to understanding a broad range of natural language text from election reports, to financial news, to sport commentaries to mathematical word problems. This paper focuses on identifying and understanding mathematical relations described within a single sentence. We introduce the problem of Equation Parsing -- given a sentence, identify noun phrases which represent variables, and generate the mathematical equation expressing the relation described in the sentence. We introduce the notion of projective equation parsing and provide an efficient algorithm to parse text to projective equations. Our system makes use of a high precision lexicon of mathematical expressions and a pipeline of structured predictors, and generates correct equations in $70\\%$ of the cases. In $60\\%$ of the time, it also identifies the correct noun phrase $\\rightarrow$ variables mapping, significantly outperforming baselines. We also release a new annotated dataset for task evaluation",
    "volume": "main",
    "checked": true,
    "id": "786a4e288dddaddd5926a3401b407f034be1f3fd",
    "citation_count": 22
  },
  "https://aclanthology.org/D16-1118": {
    "title": "Automatic Extraction of Implicit Interpretations from Modal Constructions",
    "abstract": "This paper presents an approach to extract implicit interpretations from modal constructions. Importantly, our approach uses a deterministic procedure to normalize eventualities and generate potential interpretations. An annotation effort demonstrates that these interpretations are intuitive to humans and most modal constructions convey at least one interpretation. Experimental results show that the task is challenging but can be automated",
    "volume": "main",
    "checked": true,
    "id": "813779418a613d1faecd7b1deb9b4456121a9b7e",
    "citation_count": 1
  },
  "https://aclanthology.org/D16-1119": {
    "title": "Understanding Negation in Positive Terms Using Syntactic Dependencies",
    "abstract": "This paper presents a two-step procedure to extract positive meaning from verbal negation. We first generate potential positive interpretations manipulating syntactic dependencies. Then, we score them according to their likelihood. Manual annotations show that positive interpretations are ubiquitous and intuitive to humans. Experimental results show that dependencies are better suited than semantic roles for this task, and automation is possible",
    "volume": "main",
    "checked": true,
    "id": "8911933eae2dcf4106ee584e392fb720e5283ae6",
    "citation_count": 9
  },
  "https://aclanthology.org/D16-1120": {
    "title": "Demographic Dialectal Variation in Social Media: A Case Study of African-American English",
    "abstract": "Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE-like language",
    "volume": "main",
    "checked": true,
    "id": "7a4f3a0cfc0cc2aafa4ed1a2924380e82d5e3e4c",
    "citation_count": 230
  },
  "https://aclanthology.org/D16-1121": {
    "title": "Understanding Language Preference for Expression of Opinion and Sentiment: What do Hindi-English Speakers do on Twitter?",
    "abstract": "Linguistic research on multilingual societies has indicated that there is usually a preferred language for expression of emotion and sentiment (Dewaele, 2010). Paucity of data has limited such studies to participant interviews and speech transcriptions from small groups of speakers. In this paper, we report a study on 430,000 unique tweets from Indian users, specifically Hindi-English bilinguals, to understand the language of preference, if any, for expressing opinion and sentiment. To this end, we develop classifiers for opinion detection in these languages, and further classifying opinionated tweets into positive, negative and neutral sentiments. Our study indicates that Hindi (i.e., the native language) is preferred over English for expression of negative opinion and swearing. As an aside, we explore some common pragmatic functions of code-switching through sentiment detection",
    "volume": "main",
    "checked": true,
    "id": "2842c21e879ee581aa50704817454f21b539fc69",
    "citation_count": 56
  },
  "https://aclanthology.org/D16-1122": {
    "title": "Detecting and Characterizing Events",
    "abstract": "Significant events are characterized by interactions between entities (such as countries, organizations, or individuals) that deviate from typical interaction patterns. Analysts, including historians, political scientists, and journalists, commonly read large quantities of text to construct an accurate picture of when and where an event happened, who was involved, and in what ways. In this paper, we present the Capsule model for analyzing documents to detect and characterize events of potential significance. Specifically, we develop a model based on topic modeling that distinguishes between topics that describe \"business as usual\" and topics that deviate from these patterns. To demonstrate this model, we analyze a corpus of over two million U.S. State Department cables from the 1970s. We provide an open-source implementation of an inference algorithm for the model and a pipeline for exploring its results",
    "volume": "main",
    "checked": true,
    "id": "56b874006900ade34feb39f1317ebbe6c0b0e1d2",
    "citation_count": 14
  },
  "https://aclanthology.org/D16-1123": {
    "title": "Convolutional Neural Network Language Models",
    "abstract": "Convolutional Neural Networks (CNNs) have shown to yield very strong results in several Computer Vision tasks. Their application to language has received much less attention, and it has mainly focused on static classification tasks, such as sentence classification for Sentiment Analysis or relation extraction. In this work, we study the application of CNNs to language modeling, a dynamic, sequential prediction task that needs models to capture local as well as long-range dependency information. Our contribution is twofold. First, we show that CNNs achieve 11-26% better absolute performance than feed-forward neural language models, demonstrating their potential for language representation even in sequential tasks. As for recurrent models, our model outperforms RNNs but is below state of the art LSTM models. Second, we gain some understanding of the behavior of the model, showing that CNNs in language act as feature detectors at a high level of abstraction, like in Computer Vision, and that the model can profitably use information from as far as 16 words before the target",
    "volume": "main",
    "checked": true,
    "id": "b0c5a4f8c833a4bcbe96f17a1e9323a5d46a02f4",
    "citation_count": 34
  },
  "https://aclanthology.org/D16-1124": {
    "title": "Generalizing and Hybridizing Count-based and Neural Language Models",
    "abstract": "Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches",
    "volume": "main",
    "checked": true,
    "id": "95cedaeb3178a4671703a05171a144e6b964a819",
    "citation_count": 28
  },
  "https://aclanthology.org/D16-1125": {
    "title": "Reasoning about Pragmatics with Neural Listeners and Speakers",
    "abstract": "We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural \"listener\" and \"speaker\" models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated _without_ demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to a 69% success rate using existing techniques",
    "volume": "main",
    "checked": true,
    "id": "92dbc1509b5641946cc8b524610cb6803d6ee5f6",
    "citation_count": 126
  },
  "https://aclanthology.org/D16-1126": {
    "title": "Generating Topical Poetry",
    "abstract": "We describe Hafez, a program that generates any number of distinct poems on a usersupplied topic. Poems obey rhythmic and rhyme constraints. We describe the poetrygeneration algorithm, give experimental data concerning its parameters, and show its generality with respect to language and poetic form",
    "volume": "main",
    "checked": true,
    "id": "a3a8f179b8589ea51aa553722daf82f87210abc4",
    "citation_count": 129
  },
  "https://aclanthology.org/D16-1127": {
    "title": "Deep Reinforcement Learning for Dialogue Generation",
    "abstract": "Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues",
    "volume": "main",
    "checked": true,
    "id": "1298dae5751fb06184f6b067d1503bde8037bdb7",
    "citation_count": 1125
  },
  "https://aclanthology.org/D16-1128": {
    "title": "Neural Text Generation from Structured Data with Application to the Biography Domain",
    "abstract": "This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU",
    "volume": "main",
    "checked": true,
    "id": "604764133befe7a0aaa692919545846197e6e065",
    "citation_count": 380
  },
  "https://aclanthology.org/D16-1129": {
    "title": "What makes a convincing argument? Empirical analysis and detecting attributes of convincingness in Web argumentation",
    "abstract": "This article tackles a new challenging task in computational argumentation. Given a pair of two arguments to a certain controversial topic, we aim to directly assess qualitative properties of the arguments in order to explain why one argument is more convincing than the other one. We approach this task in a fully empirical manner by annotating 26k explanations written in natural language. These explanations describe convincingness of arguments in the given argument pair, such as their strengths or flaws. We create a new crowd-sourced corpus containing 9,111 argument pairs, multi-labeled with 17 classes, which was cleaned and curated by employing several strict quality measures. We propose two tasks on this data set, namely (1) predicting the full label distribution and (2) classifying types of flaws in less convincing arguments. Our experiments with feature-rich SVM learners and Bidirectional LSTM neural networks with convolution and attention mechanism reveal that such a novel fine-grained analysis of Web argument convincingness is a very challenging task. We release the new UKPConvArg2 corpus and software under permissive licenses to the research community",
    "volume": "main",
    "checked": true,
    "id": "4759aaacd71fbb2b5ca253aa13ccceac0bc7fe8a",
    "citation_count": 81
  },
  "https://aclanthology.org/D16-1130": {
    "title": "Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention",
    "abstract": "Recognizing implicit discourse relations is a challenging but important task in the field of Natural Language Processing. For such a complex text processing task, different from previous studies, we argue that it is necessary to repeatedly read the arguments and dynamically exploit the efficient features useful for recognizing discourse relations. To mimic the repeated reading strategy, we propose the neural networks with multi-level attention (NNMA), combining the attention mechanism and external memories to gradually fix the attention on some specific words helpful to judging the discourse relations. Experiments on the PDTB dataset show that our proposed method achieves the state-of-art results. The visualization of the attention weights also illustrates the progress that our model observes the arguments on each level and progressively locates the important words",
    "volume": "main",
    "checked": true,
    "id": "eb3d818cf095c3bb8c08b6ff3a19b1bc28f9d698",
    "citation_count": 63
  },
  "https://aclanthology.org/D16-1131": {
    "title": "Antecedent Selection for Sluicing: Structure and Content",
    "abstract": "Sluicing is an elliptical process where the majority of a question can go unpronounced as long as there is a salient antecedent in previous discourse. This paper considers the task of antecedent selection: finding the correct antecedent for a given case of sluicing. We argue that both syntactic and discourse relationships are important in antecedent selection, and we construct linguistically sophisticated features that describe the relevant relationships. We also define features that describe the relation of the content of the antecedent and the sluice type. We develop a linear model which achieves accuracy of 72.4%, a substantial improvement over a strong manually constructed baseline. Feature analysis confirms that both syntactic and discourse features are important in antecedent selection",
    "volume": "main",
    "checked": true,
    "id": "0d6f007038c1afb9f9cc60045ca188fc707469a0",
    "citation_count": 9
  },
  "https://aclanthology.org/D16-1132": {
    "title": "Intra-Sentential Subject Zero Anaphora Resolution using Multi-Column Convolutional Neural Network",
    "abstract": "This paper proposes a method for intrasentential subject zero anaphora resolution in Japanese. Our proposed method utilizes a Multi-column Convolutional Neural Network (MCNN) for predicting zero anaphoric relations. Motivated by Centering Theory and other previous works, we exploit as clues both the surface word sequence and the dependency tree of a target sentence in our MCNN. Even though the F-score of our method was lower than that of the state-of-the-art method, which achieved relatively high recall and low precision, our method achieved much higher precision (>0.8) in a wide range of recall levels. We believe such high precision is crucial for real-world NLP applications and thus our method is preferable to the state-of-the-art method",
    "volume": "main",
    "checked": true,
    "id": "45b0d948217d4578b71687549fbfdd797dc03c5b",
    "citation_count": 26
  },
  "https://aclanthology.org/D16-1133": {
    "title": "An Unsupervised Probability Model for Speech-to-Translation Alignment of Low-Resource Languages",
    "abstract": "For many low-resource languages, spoken language resources are more likely to be annotated with translations than with transcriptions. Translated speech data is potentially valuable for documenting endangered languages or for training speech translation systems. A first step towards making use of such data would be to automatically align spoken words with their translations. We present a model that combines Dyer et al.'s reparameterization of IBM Model 2 (fast-align) and k-means clustering using Dynamic Time Warping as a distance metric. The two components are trained jointly using expectation-maximization. In an extremely low-resource scenario, our model performs significantly better than both a neural model and a strong baseline",
    "volume": "main",
    "checked": true,
    "id": "fd83fa5720c7283ff2c935902b956889e3db8a2b",
    "citation_count": 25
  },
  "https://aclanthology.org/D16-1134": {
    "title": "HUME: Human UCCA-Based Evaluation of Machine Translation",
    "abstract": "Human evaluation of machine translation normally uses sentence-level measures such as relative ranking or adequacy scales. However, these provide no insight into possible errors, and do not scale well with sentence length. We argue for a semantics-based evaluation, which captures what meaning components are retained in the MT output, thus providing a more fine-grained analysis of translation quality, and enabling the construction and tuning of semantics-based MT. We present a novel human semantic evaluation measure, Human UCCA-based MT Evaluation (HUME), building on the UCCA semantic representation scheme. HUME covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled MT output. We experiment with four language pairs, demonstrating HUME's broad applicability, and report good inter-annotator agreement rates and correlation with human adequacy scores",
    "volume": "main",
    "checked": true,
    "id": "c9e5456e328be30de60a87b742e733b5dca004b9",
    "citation_count": 32
  },
  "https://aclanthology.org/D16-1135": {
    "title": "Improving Multilingual Named Entity Recognition with Wikipedia Entity Type Mapping",
    "abstract": "The state-of-the-art named entity recognition (NER) systems are statistical machine learning models that have strong generalization capability (i.e., can recognize unseen entities that do not appear in training data) based on lexical and contextual information. However, such a model could still make mistakes if its features favor a wrong entity type. In this paper, we utilize Wikipedia as an open knowledge base to improve multilingual NER systems. Central to our approach is the construction of high-accuracy, high-coverage multilingual Wikipedia entity type mappings. These mappings are built from weakly annotated data and can be extended to new languages with no human annotation or language-dependent knowledge involved. Based on these mappings, we develop several approaches to improve an NER system. We evaluate the performance of the approaches via experiments on NER systems trained for 6 languages. Experimental results show that the proposed approaches are effective in improving the accuracy of such systems on unseen entities, especially when a system is applied to a new domain or it is trained with little training data (up to 18.3 F1 score improvement)",
    "volume": "main",
    "checked": true,
    "id": "7ee4921f0a81055253fa8d9fe85c4fb82ffff9ae",
    "citation_count": 28
  },
  "https://aclanthology.org/D16-1136": {
    "title": "Learning Crosslingual Word Embeddings without Bilingual Corpora",
    "abstract": "Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task",
    "volume": "main",
    "checked": true,
    "id": "42636e49faeb9d8eefac065979115c685fec1bab",
    "citation_count": 116
  },
  "https://aclanthology.org/D16-1137": {
    "title": "Sequence-to-Sequence Learning as Beam-Search Optimization",
    "abstract": "Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation",
    "volume": "main",
    "checked": true,
    "id": "5507dc32b368c8afd3b9507e9b5888da7bd7d7cd",
    "citation_count": 497
  },
  "https://aclanthology.org/D16-1138": {
    "title": "Online Segment to Segment Neural Transduction",
    "abstract": "We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training, and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixed-length hidden states before producing any output. It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. Experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders",
    "volume": "main",
    "checked": true,
    "id": "f61da0efbb38bd3e6b9a9855809f5288b829f1f0",
    "citation_count": 82
  },
  "https://aclanthology.org/D16-1139": {
    "title": "Sequence-Level Knowledge Distillation",
    "abstract": "Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model, with a decrease of 0.4 BLEU",
    "volume": "main",
    "checked": true,
    "id": "57a10537978600fd33dcdd48922c791609a4851a",
    "citation_count": 453
  },
  "https://aclanthology.org/D16-1140": {
    "title": "Controlling Output Length in Neural Encoder-Decoders",
    "abstract": "Neural encoder-decoder models have shown great success in many sequence generation tasks. However, previous work has not investigated situations in which we would like to control the length of encoder-decoder outputs. This capability is crucial for applications such as text summarization, in which we have to generate concise summaries with a desired length. In this paper, we propose methods for controlling the output sequence length for neural encoder-decoder models: two decoding-based methods and two learning-based methods. Results show that our learning-based methods have the capability to control length without degrading summary quality in a summarization task",
    "volume": "main",
    "checked": true,
    "id": "3cfdec4f1664fcdc20fd5a6d3f86e7b40cf19f70",
    "citation_count": 201
  },
  "https://aclanthology.org/D16-1141": {
    "title": "Poet Admits // Mute Cypher: Beam Search to find Mutually Enciphering Poetic Texts",
    "abstract": "The Xenotext Experiment implants poetry into an extremophile's DNA, and uses that DNA to generate new poetry in a protein form. The molecular machinery of life requires that these two poems encipher each other under a symmetric substitution cipher. We search for ciphers which permit writing under the Xenotext constraints, incorporating ideas from cipher-cracking algorithms, and using n-gram data to assess a cipher's \"writability\". Our algorithm, Beam Verse, is a beam search which uses new heuristics to navigate the cipher-space. We find thousands of ciphers which score higher than successful ciphers used to write Xenotext constrained texts",
    "volume": "main",
    "checked": true,
    "id": "da4f0fcef0c91f967e80466fe6433b1f965929b7",
    "citation_count": 0
  },
  "https://aclanthology.org/D16-1142": {
    "title": "All Fingers are not Equal: Intensity of References in Scientific Articles",
    "abstract": "Research accomplishment is usually measured by considering all citations with equal importance, thus ignoring the wide variety of purposes an article is being cited for. Here, we posit that measuring the intensity of a reference is crucial not only to perceive better understanding of research endeavor, but also to improve the quality of citation-based applications. To this end, we collect a rich annotated dataset with references labeled by the intensity, and propose a novel graph-based semi-supervised model, GraLap to label the intensity of references. Experiments with AAN datasets show a significant improvement compared to the baselines to achieve the true labels of the references (46% better correlation). Finally, we provide four applications to demonstrate how the knowledge of reference intensity leads to design better real-world applications",
    "volume": "main",
    "checked": true,
    "id": "a59cafd1b42dd3b6e7442933a5390bd70616ab91",
    "citation_count": 15
  },
  "https://aclanthology.org/D16-1143": {
    "title": "Improving Users' Demographic Prediction via the Videos They Talk about",
    "abstract": "In this paper, we improve microblog users' demographic prediction by fully utilizing their video related behaviors. First, we collect the describing words of currently popular videos, including video names, actor names and video keywords, from video websites. Secondly, we search these describing words in users' microblogs, and build the direct relationships between users and the appeared words. After that, to make the sparse relationship denser, we propose a Bayesian method to calculate the probability of connections between users and other video describing words. Lastly, we build two models to predict users' demographics with the obtained direct and indirect relationships. Based on a large realworld dataset, experiment results show that our method can significantly improve these words' demographic predictive ability",
    "volume": "main",
    "checked": true,
    "id": "9f297a93decd6defb8df09a6ba622257e90f52ed",
    "citation_count": 15
  },
  "https://aclanthology.org/D16-1144": {
    "title": "AFET: Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding",
    "abstract": "Distant supervision has been widely used in current systems of fine-grained entity typing to automatically assign categories (entity types) to entity mentions. However, the types so obtained from knowledge bases are often incorrect for the entity mention's local context. This paper proposes a novel embedding method to separately model \"clean\" and \"noisy\" mentions, and incorporates the given type hierarchy to induce loss functions. We formulate a joint optimization problem to learn embeddings for mentions and typepaths, and develop an iterative algorithm to solve the problem. Experiments on three public datasets demonstrate the effectiveness and robustness of the proposed method, with an average 15% improvement in accuracy over the next best compared method1",
    "volume": "main",
    "checked": true,
    "id": "ee42c6c3c5db2f0eb40faacf6e3b80035a645287",
    "citation_count": 110
  },
  "https://aclanthology.org/D16-1145": {
    "title": "Mining Inference Formulas by Goal-Directed Random Walks",
    "abstract": "Deep inference on a large-scale knowledge base (KB) needs a mass of formulas, but it is almost impossible to create all formulas manually. Data-driven methods have been proposed to mine formulas from KBs automatically, where random sampling and approximate calculation are common techniques to handle big data. Among a series of methods, Random Walk is believed to be suitable for knowledge graph data. However, a pure random walk without goals still has a poor efficiency of mining useful formulas, and even introduces lots of noise which may mislead inference. Although several heuristic rules have been proposed to direct random walks, they do not work well due to the diversity of formulas. To this end, we propose a novel goaldirected inference formula mining algorithm, which directs random walks by the specific inference target at each step. The algorithm is more inclined to visit benefic structures to infer the target, so it can increase efficiency of random walks and avoid noise simultaneously. The experiments on both WordNet and Freebase prove that our approach is has a high efficiency and performs best on the task",
    "volume": "main",
    "checked": true,
    "id": "45348379df0d40b5c1d4df26587e6b80cfe96565",
    "citation_count": 19
  },
  "https://aclanthology.org/D16-1146": {
    "title": "Lifted Rule Injection for Relation Embeddings",
    "abstract": "Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models. A recent approach regularizes relation and entity representations by propositionalization of first-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime",
    "volume": "main",
    "checked": true,
    "id": "a1fb4e2029af4a118a045cf9dc7c00d65047f8df",
    "citation_count": 125
  },
  "https://aclanthology.org/D16-1147": {
    "title": "Key-Value Memory Networks for Directly Reading Documents",
    "abstract": "Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, WikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing WikiQA benchmark",
    "volume": "main",
    "checked": true,
    "id": "bba5f2852b1db8a18004eb7328efa5e1d57cc62a",
    "citation_count": 733
  },
  "https://aclanthology.org/D16-1148": {
    "title": "Analyzing Framing through the Casts of Characters in the News",
    "abstract": "We present an unsupervised model for the discovery and clustering of latent \"personas\" (characterizations of entities). Our model simultaneously clusters documents featuring similar collections of personas. We evaluate this model on a collection of news articles about immigration, showing that personas help predict the coarse-grained framing annotations in the Media Frames Corpus. We also introduce automated model selection as a fair and robust form of feature evaluation",
    "volume": "main",
    "checked": true,
    "id": "15130cdd46ff7e3a69384e6c62ab1775c4aba96f",
    "citation_count": 49
  },
  "https://aclanthology.org/D16-1149": {
    "title": "The Teams Corpus and Entrainment in Multi-Party Spoken Dialogues",
    "abstract": "When interacting individuals entrain, they begin to speak more like each other. To support research on entrainment in cooperative multi-party dialogues, we have created a corpus where teams of three or four speakers play two rounds of a cooperative board game. We describe the experimental design and technical infrastructure used to collect our corpus, which consists of audio, video, transcriptions, and questionnaire data for 63 teams (47 hours of audio). We illustrate the use of our corpus as a novel resource for studying team entrainment by 1) developing and evaluating teamlevel acoustic-prosodic entrainment measures that extend existing dyad measures, and 2) investigating relationships between team entrainment and participation dominance",
    "volume": "main",
    "checked": true,
    "id": "5d27112fdfbb3ebd3f4bf701207d016fc379e45c",
    "citation_count": 21
  },
  "https://aclanthology.org/D16-1150": {
    "title": "Personalized Emphasis Framing for Persuasive Message Generation",
    "abstract": "In this paper, we present a study on personalized emphasis framing which can be used to tailor the content of a message to enhance its appeal to different individuals. With this framework, we directly model content selection decisions based on a set of psychologically-motivated domain-independent personal traits including personality (e.g., extraversion and conscientiousness) and basic human values (e.g., self-transcendence and hedonism). We also demonstrate how the analysis results can be used in automated personalized content selection for persuasive message generation",
    "volume": "main",
    "checked": true,
    "id": "214e6d4f7ca9e31e751cd0f7de285906d8e0d039",
    "citation_count": 19
  },
  "https://aclanthology.org/D16-1151": {
    "title": "Cross Sentence Inference for Process Knowledge",
    "abstract": "For AI systems to reason about real world situations, they need to recognize which processes are at play and which entities play key roles in them. Our goal is to extract this kind of rolebased knowledge about processes, from multiple sentence-level descriptions. This knowledge is hard to acquire; while semantic role labeling (SRL) systems can extract sentence level role information about individual mentions of a process, their results are often noisy and they do not attempt create a globally consistent characterization of a process. To overcome this, we extend standard within sentence joint inference to inference across multiple sentences. This cross sentence inference promotes role assignments that are compatible across different descriptions of the same process. When formulated as an Integer Linear Program, this leads to improvements over within-sentence inference by nearly 3% in F1. The resulting role-based knowledge is of high quality (with a F1 of nearly 82)",
    "volume": "main",
    "checked": true,
    "id": "b6f0f0339b964b4a1eedaa24835e583f6eb6955f",
    "citation_count": 5
  },
  "https://aclanthology.org/D16-1152": {
    "title": "Toward Socially-Infused Information Extraction: Embedding Authors, Mentions, and Entities",
    "abstract": "Entity linking is the task of identifying mentions of entities in text, and linking them to entries in a knowledge base. This task is especially difficult in microblogs, as there is little additional text to provide disambiguating context; rather, authors rely on an implicit common ground of shared knowledge with their readers. In this paper, we attempt to capture some of this implicit context by exploiting the social network structure in microblogs. We build on the theory of homophily, which implies that socially linked individuals share interests, and are therefore likely to mention the same sorts of entities. We implement this idea by encoding authors, mentions, and entities in a continuous vector space, which is constructed so that socially-connected authors have similar vector representations. These vectors are incorporated into a neural structured prediction model, which captures structural constraints that are inherent in the entity linking task. Together, these design decisions yield F1 improvements of 1%-5% on benchmark datasets, as compared to the previous state-of-the-art",
    "volume": "main",
    "checked": true,
    "id": "90610e5ff03f57a26a786ccbd7dfe0b6111eeef2",
    "citation_count": 21
  },
  "https://aclanthology.org/D16-1153": {
    "title": "Phonologically Aware Neural Model for Named Entity Recognition in Low Resource Transfer Settings",
    "abstract": "Named Entity Recognition is a well established information extraction task with many state of the art systems existing for a variety of languages. Most systems rely on language specific resources, large annotated corpora, gazetteers and feature engineering to perform well monolingually. In this paper, we introduce an attentional neural model which only uses language universal phonological character representations with word embeddings to achieve state of the art performance in a monolingual setting using supervision and which can quickly adapt to a new language with minimal or no data. We demonstrate that phonological character representations facilitate cross-lingual transfer, outperform orthographic representations and incorporating both attention and phonological features improves statistical efficiency of the model in 0-shot and low data transfer settings with no task specific feature engineering in the source or target language",
    "volume": "main",
    "checked": true,
    "id": "f1a8ff5542ffd51c3618953e5cf926c467154f79",
    "citation_count": 74
  },
  "https://aclanthology.org/D16-1154": {
    "title": "Long-Short Range Context Neural Networks for Language Modeling",
    "abstract": "The goal of language modeling techniques is to capture the statistical and structural properties of natural languages from training corpora. This task typically involves the learning of short range dependencies, which generally model the syntactic properties of a language and/or long range dependencies, which are semantic in nature. We propose in this paper a new multi-span architecture, which separately models the short and long context information while it dynamically merges them to perform the language modeling task. This is done through a novel recurrent Long-Short Range Context (LSRC) network, which explicitly models the local (short) and global (long) context using two separate hidden states that evolve in time. This new architecture is an adaptation of the Long-Short Term Memory network (LSTM) to take into account the linguistic properties. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art language modeling techniques",
    "volume": "main",
    "checked": true,
    "id": "eb329aab0d7d48b982dde62a5db6deaafa7de07d",
    "citation_count": 9
  },
  "https://aclanthology.org/D16-1155": {
    "title": "Jointly Learning Grounded Task Structures from Language Instruction and Visual Demonstration",
    "abstract": "To enable language-based communication and collaboration with cognitive robots, this paper presents an approach where an agent can learn task models jointly from language instruction and visual demonstration using an And-Or Graph (AoG) representation. The learned AoG captures a hierarchical task structure where linguistic labels (for language communication) are grounded to corresponding state changes from the physical environment (for perception and action). Our empirical results on a cloth-folding domain have shown that, although state detection through visual processing is full of uncertainties and error prone, by a tight integration with language the agent is able to learn an effective AoG for task representation. The learned AoG can be further applied to infer and interpret on-going actions from new visual demonstration using linguistic labels at different levels of granularity",
    "volume": "main",
    "checked": true,
    "id": "a3e2d3ce897a28fc35be86ec73fab9d5591f10d4",
    "citation_count": 34
  },
  "https://aclanthology.org/D16-1156": {
    "title": "Resolving Language and Vision Ambiguities Together: Joint Segmentation & Prepositional Attachment Resolution in Captioned Scenes",
    "abstract": "We present an approach to simultaneously perform semantic segmentation and prepositional phrase attachment resolution for captioned images. Some ambiguities in language cannot be resolved without simultaneously reasoning about an associated image. If we consider the sentence \"I shot an elephant in my pajamas\", looking at language alone (and not using common sense), it is unclear if it is the person or the elephant wearing the pajamas or both. Our approach produces a diverse set of plausible hypotheses for both semantic segmentation and prepositional phrase attachment resolution that are then jointly reranked to select the most consistent pair. We show that our semantic segmentation and prepositional phrase attachment resolution modules have complementary strengths, and that joint reasoning produces more accurate results than any module operating in isolation. Multiple hypotheses are also shown to be crucial to improved multiple-module reasoning. Our vision and language approach significantly outperforms the Stanford Parser (De Marneffe et al., 2006) by 17.91% (28.69% relative) and 12.83% (25.28% relative) in two different experiments. We also make small improvements over DeepLab-CRF (Chen et al., 2015)",
    "volume": "main",
    "checked": true,
    "id": "26eb2c900814707ae962184ad4173e754247a80a",
    "citation_count": 28
  },
  "https://aclanthology.org/D16-1157": {
    "title": "Charagram: Embedding Words and Sentences via Character n-grams",
    "abstract": "We present Charagram embeddings, a simple approach for learning character-based compositional models to embed textual sequences. A word or sentence is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding. We use three tasks for evaluation: word similarity, sentence similarity, and part-of-speech tagging. We demonstrate that Charagram embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks",
    "volume": "main",
    "checked": true,
    "id": "12e9d005c77f76e344361f79c4b008034ae547eb",
    "citation_count": 178
  },
  "https://aclanthology.org/D16-1158": {
    "title": "Length bias in Encoder Decoder Models and a Case for Global Conditioning",
    "abstract": "Encoder-decoder networks are popular for modeling sequences probabilistically in many applications. These models use the power of the Long Short-Term Memory (LSTM) architecture to capture the full dependence among variables, unlike earlier models like CRFs that typically assumed conditional independence among non-adjacent variables. However in practice encoder-decoder models exhibit a bias towards short sequences that surprisingly gets worse with increasing beam size.   In this paper we show that such phenomenon is due to a discrepancy between the full sequence margin and the per-element margin enforced by the locally conditioned training objective of a encoder-decoder model. The discrepancy more adversely impacts long sequences, explaining the bias towards predicting short sequences.   For the case where the predicted sequences come from a closed set, we show that a globally conditioned model alleviates the above problems of encoder-decoder models. From a practical point of view, our proposed model also eliminates the need for a beam-search during inference, which reduces to an efficient dot-product based search in a vector-space",
    "volume": "main",
    "checked": true,
    "id": "4e912f84a498e883d7b15311007e3ac5635af883",
    "citation_count": 34
  },
  "https://aclanthology.org/D16-1159": {
    "title": "Does String-Based Neural MT Learn Source Syntax?",
    "abstract": "We investigate whether a neural, encoderdecoder translation system learns syntactic information on the source side as a by-product of training. We propose two methods to detect whether the encoder has learned local and global source syntax. A fine-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing",
    "volume": "main",
    "checked": true,
    "id": "d821ce08da6c0084d5eacbdf65e25556bc1b9bc3",
    "citation_count": 291
  },
  "https://aclanthology.org/D16-1160": {
    "title": "Exploiting Source-side Monolingual Data in Neural Machine Translation",
    "abstract": "Neural Machine Translation (NMT) based on the encoder-decoder architecture has recently become a new paradigm. Researchers have proven that the target-side monolingual data can greatly enhance the decoder model of NMT. However, the source-side monolingual data is not fully explored although it should be useful to strengthen the encoder model of NMT, especially when the parallel corpus is far from sufficient. In this paper, we propose two approaches to make full use of the sourceside monolingual data in NMT. The first approach employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. The second approach applies the multi-task learning framework using two NMTs to predict the translation and the reordered source-side monolingual sentences simultaneously. The extensive experiments demonstrate that the proposed methods obtain significant improvements over the strong attention-based NMT",
    "volume": "main",
    "checked": true,
    "id": "d9b03cd97db6255081d1e57983fa673d1f8f2d0e",
    "citation_count": 232
  },
  "https://aclanthology.org/D16-1161": {
    "title": "Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction",
    "abstract": "In this work, we study parameter tuning towards the M^2 metric, the standard metric for automatic grammar error correction (GEC) tasks. After implementing M^2 as a scorer in the Moses tuning framework, we investigate interactions of dense and sparse features, different optimizers, and tuning strategies for the CoNLL-2014 shared task. We notice erratic behavior when optimizing sparse feature weights with M^2 and offer partial solutions. We find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning outperforms all previously published results for the CoNLL-2014 test set by a large margin (46.37% M^2 over previously 41.75%, by an SMT system with neural features) while being trained on the same, publicly available data. Our newly introduced dense and sparse features widen that gap, and we improve the state-of-the-art to 49.49% M^2",
    "volume": "main",
    "checked": true,
    "id": "b19f365aab0bf8c6cf712c07313b919556bfacc0",
    "citation_count": 91
  },
  "https://aclanthology.org/D16-1162": {
    "title": "Incorporating Discrete Translation Lexicons into Neural Machine Translation",
    "abstract": "Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time",
    "volume": "main",
    "checked": true,
    "id": "dc984ea8be018a0244b40468d13f7b734ab55bac",
    "citation_count": 193
  },
  "https://aclanthology.org/D16-1163": {
    "title": "Transfer Learning for Low-Resource Neural Machine Translation",
    "abstract": "The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves Bleu scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 Bleu on four low-resource language pairs. Ensembling and unknown word replacement add another 2 Bleu which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 Bleu, improving the state-of-the-art on low-resource machine translation",
    "volume": "main",
    "checked": true,
    "id": "1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6",
    "citation_count": 615
  },
  "https://aclanthology.org/D16-1164": {
    "title": "MixKMeans: Clustering Question-Answer Archives",
    "abstract": "Community-driven Question Answering (CQA) systems that crowdsource experiential information in the form of questions and answers and have accumulated valuable reusable knowledge. Clustering of QA datasets from CQA systems provides a means of organizing the content to ease tasks such as manual curation and tagging. In this paper, we present a clustering method that exploits the two-part question-answer structure in QA datasets to improve clustering quality. Our method, MixKMeans, composes question and answer space similarities in a way that the space on which the match is higher is allowed to dominate. This construction is motivated by our observation that semantic similarity between question-answer data (QAs) could get localized in either space. We empirically evaluate our method on a variety of real-world labeled datasets. Our results indicate that our method significantly outperforms stateof-the-art clustering methods for the task of clustering question-answer archives",
    "volume": "main",
    "checked": true,
    "id": "39ce92c72c1f3dc884c6d277e8d0113a18f1cad9",
    "citation_count": 9
  },
  "https://aclanthology.org/D16-1165": {
    "title": "It Takes Three to Tango: Triangulation Approach to Answer Ranking in Community Question Answering",
    "abstract": "We address the problem of answering new questions in community forums, by selecting suitable answers to already asked questions. We approach the task as an answer ranking problem, adopting a pairwise neural network architecture that selects which of two competing answers is better. We focus on the utility of the three types of similarities occurring in the triangle formed by the original question, the related question, and an answer to the related comment, which we call relevance, relatedness, and appropriateness. Our proposed neural network models the interactions among all input components using syntactic and semantic embeddings, lexical matching, and domain-specific features. It achieves state-of-the-art results, showing that the three similarities are important and need to be modeled together. Our experiments demonstrate that all feature types are relevant, but the most important ones are the lexical similarity features, the domain-specific features, and the syntactic and semantic embeddings",
    "volume": "main",
    "checked": true,
    "id": "c60a68dc35d3ec729c9aa922d53d0a02e1c7e109",
    "citation_count": 14
  },
  "https://aclanthology.org/D16-1166": {
    "title": "Character-Level Question Answering with Attention",
    "abstract": "We show that a character-level encoder-decoder framework can be successfully applied to question answering with a structured knowledge base. We use our model for single-relation question answering and demonstrate the effectiveness of our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy from 63.9% to 70.9%, without use of ensembles. Importantly, our character-level model has 16x fewer parameters than an equivalent word-level model, can be learned with significantly less data compared to previous work, which relies on data augmentation, and is robust to new entities in testing",
    "volume": "main",
    "checked": true,
    "id": "698d675ba7134ac701de810c9ca4a6de72cb414b",
    "citation_count": 169
  },
  "https://aclanthology.org/D16-1167": {
    "title": "Learning to Generate Textual Data",
    "abstract": "To learn text understanding models with millions of parameters one needs massive amounts of data. In this work, we argue that generating data can compensate for this need. While defining generic data generators is difficult, we propose to allow generators to be \"weakly\" specified in the sense that a set of parameters controls how the data is generated. Consider for example generators where the example templates, grammar, and/or vocabulary is determined by this set of parameters. Instead of manually tuning these parameters, we learn them from the limited training data at our disposal. To achieve this, we derive an efficient algorithm called GENERE that jointly estimates the parameters of the model and the undetermined generation parameters. We illustrate its benefits by learning to solve math exam questions using a highly parametrized sequence-to-sequence neural network",
    "volume": "main",
    "checked": true,
    "id": "328d1e5dfe956021ac172760242f7fc8743b90ea",
    "citation_count": 4
  },
  "https://aclanthology.org/D16-1168": {
    "title": "A Theme-Rewriting Approach for Generating Algebra Word Problems",
    "abstract": "Texts present coherent stories that have a particular theme or overall setting, for example science fiction or western. In this paper, we present a text generation method called {\\it rewriting} that edits existing human-authored narratives to change their theme without changing the underlying story. We apply the approach to math word problems, where it might help students stay more engaged by quickly transforming all of their homework assignments to the theme of their favorite movie without changing the math concepts that are being taught. Our rewriting method uses a two-stage decoding process, which proposes new words from the target theme and scores the resulting stories according to a number of factors defining aspects of syntactic, semantic, and thematic coherence. Experiments demonstrate that the final stories typically represent the new theme well while still testing the original math concepts, outperforming a number of baselines. We also release a new dataset of human-authored rewrites of math word problems in several themes",
    "volume": "main",
    "checked": true,
    "id": "7e27669e08b38ca03eeadce84086208bcd6e70ed",
    "citation_count": 22
  },
  "https://aclanthology.org/D16-1169": {
    "title": "Context-Sensitive Lexicon Features for Neural Sentiment Analysis",
    "abstract": "Sentiment lexicons have been leveraged as a useful source of features for sentiment analysis models, leading to the state-of-the-art accuracies. On the other hand, most existing methods use sentiment lexicons without considering context, typically taking the count, sum of strength, or maximum sentiment scores over the whole input. We propose a context-sensitive lexicon-based method based on a simple weighted-sum model, using a recurrent neural network to learn the sentiments strength, intensification and negation of lexicon sentiments in composing the sentiment value of sentences. Results show that our model can not only learn such operation details, but also give significant improvements over state-of-the-art recurrent neural network baselines without lexical features, achieving the best results on a Twitter benchmark",
    "volume": "main",
    "checked": true,
    "id": "f1ee33068dfff064ef4018422a7fb1c6d6710711",
    "citation_count": 123
  },
  "https://aclanthology.org/D16-1170": {
    "title": "Event-Driven Emotion Cause Extraction with Corpus Construction",
    "abstract": "In this paper, we present our work in emotion cause extraction. Since there is no open dataset available, the lack of annotated resources has limited the research in this area. Thus, we first present a dataset we built using SINA city news. The annotation is based on the scheme of the W3C Emotion Markup Language. Second, we propose a 7-tuple definition to describe emotion cause events. Based on this general definition, we propose a new event-driven emotion cause extraction method using multi-kernel SVMs where a syntactical tree based approach is used to represent events in text. A convolution kernel based multikernel SVM are used to extract emotion causes. Because traditional convolution kernels do not use lexical information at the terminal nodes of syntactic trees, we modify the kernel function with a synonym based improvement. Even with very limited training data, we can still extract sufficient features for the task. Evaluations show that our approach achieves 11.6% higher F-measure compared to referenced methods. The contributions of our work include resource construction, concept definition and algorithm development",
    "volume": "main",
    "checked": true,
    "id": "120bd71c72f9477dec6b5291c32f73ae4afbf163",
    "citation_count": 119
  },
  "https://aclanthology.org/D16-1171": {
    "title": "Neural Sentiment Classification with User and Product Attention",
    "abstract": "Document-level sentiment classification aims to predict user's overall sentiment in a document about a product. However, most of existing methods only focus on local text information and ignore the global user preference and product characteristics. Even though some works take such information into account, they usually suffer from high model complexity and only consider wordlevel preference rather than semantic levels. To address this issue, we propose a hierarchical neural network to incorporate global user and product information into sentiment classification. Our model first builds a hierarchical LSTM model to generate sentence and document representations. Afterwards, user and product information is considered via attentions over different semantic levels due to its ability of capturing crucial semantic components. The experimental results show that our model achieves significant and consistent improvements compared to all state-of-theart methods. The source code of this paper can be obtained from https://github. com/thunlp/NSC",
    "volume": "main",
    "checked": true,
    "id": "4ff5cdeeed3dfeeea542d463dc2f5ad64fd38281",
    "citation_count": 272
  },
  "https://aclanthology.org/D16-1172": {
    "title": "Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification",
    "abstract": "Recently, neural networks have achieved great success on sentiment classification due to their ability to alleviate feature engineering. However, one of the remaining challenges is to model long texts in document-level sentiment classification under a recurrent architecture because of the deficiency of the memory unit. To address this problem, we present a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. CLSTM introduces a cache mechanism, which divides memory into several groups with different forgetting rates and thus enables the network to keep sentiment information better within a recurrent unit. The proposed CLSTM outperforms the state-of-the-art models on three publicly available document-level sentiment analysis datasets",
    "volume": "main",
    "checked": true,
    "id": "9904a69eab0792859108eec6b0578d11264b8e83",
    "citation_count": 142
  },
  "https://aclanthology.org/D16-1173": {
    "title": "Deep Neural Networks with Massive Learned Knowledge",
    "abstract": "Regulating deep neural networks (DNNs) with human structured knowledge has shown to be of great benefit for improved accuracy and interpretability. We develop a general framework that enables learning knowledge and its confidence jointly with the DNNs, so that the vast amount of fuzzy knowledge can be incorporated and automatically optimized with little manual efforts. We apply the framework to sentence sentiment analysis, augmenting a DNN with massive linguistic constraints on discourse and polarity structures. Our model substantially enhances the performance using less training data, and shows improved interpretability. The principled framework can also be applied to posterior regularization for regulating other statistical models",
    "volume": "main",
    "checked": true,
    "id": "e770e1b88351280d810967b3633731c6968ea6d0",
    "citation_count": 56
  },
  "https://aclanthology.org/D16-1174": {
    "title": "De-Conflated Semantic Representations",
    "abstract": "One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have. Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years. However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses. We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge it derives from a semantic network. Our approach provides multiple advantages in comparison to the past work, including its high coverage and the ability to generate accurate representations even for infrequent word senses. We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them",
    "volume": "main",
    "checked": true,
    "id": "b54315a22b825e9ca1b59aa1d3fac98ea4925941",
    "citation_count": 82
  },
  "https://aclanthology.org/D16-1175": {
    "title": "Improving Sparse Word Representations with Distributional Inference for Semantic Composition",
    "abstract": "Distributional models are derived from co-occurrences in a corpus, where only a small proportion of all possible plausible co-occurrences will be observed. This results in a very sparse vector space, requiring a mechanism for inferring missing knowledge. Most methods face this challenge in ways that render the resulting word representations uninterpretable, with the consequence that semantic composition becomes hard to model. In this paper we explore an alternative which involves explicitly inferring unobserved co-occurrences using the distributional neighbourhood. We show that distributional inference improves sparse word representations on several word similarity benchmarks and demonstrate that our model is competitive with the state-of-the-art for adjective-noun, noun-noun and verb-object compositions while being fully interpretable",
    "volume": "main",
    "checked": true,
    "id": "b462b3739aee93ec09deca5663aad2631820b9f3",
    "citation_count": 16
  },
  "https://aclanthology.org/D16-1176": {
    "title": "Modelling Interaction of Sentence Pair with Coupled-LSTMs",
    "abstract": "Recently, there is rising interest in modelling the interactions of two sentences with deep neural networks. However, most of the existing methods encode two sequences with separate encoders, in which a sentence is encoded with little or no information from the other sentence. In this paper, we propose a deep architecture to model the strong interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architecture and its superiority to state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "10d89efc96beb45676149c8a3237a86a72a2116e",
    "citation_count": 44
  },
  "https://aclanthology.org/D16-1177": {
    "title": "Universal Decompositional Semantics on Universal Dependencies",
    "abstract": "We present a framework for augmenting data sets from the Universal Dependencies project with Universal Decompositional Semantics. Where the Universal Dependencies project aims to provide a syntactic annotation standard that can be used consistently across many languages as well as a collection of corpora that use that standard, our extension has similar aims for semantic annotation. We describe results from annotating the English Universal Dependencies treebank, dealing with word senses, semantic roles, and event properties",
    "volume": "main",
    "checked": true,
    "id": "00dc74ca39fc6630bef824a4768dd214bbd81927",
    "citation_count": 126
  },
  "https://aclanthology.org/D16-1178": {
    "title": "Friends with Motives: Using Text to Infer Influence on SCOTUS",
    "abstract": "We present a probabilistic model of the influence of language on the behavior of the U.S. Supreme Court, specifically influence of amicus briefs on Court decisions and opinions. The approach assumes that amici are rational, utility-maximizing agents who try to win votes or affect the language of court opinions. Our model leads to improved predictions of justices' votes and perplexity of opinion language. It is amenable to inspection, allowing us to explore inferences about the persuasiveness of different amici and influenceability of different justices; these are consistent with earlier findings. \"Language is the central tool of our trade.\" John G. Roberts, 2007 (Garner, 2010)",
    "volume": "main",
    "checked": true,
    "id": "2dbcfbca560e7fbc6b31ce86acf2a7f08892c449",
    "citation_count": 10
  },
  "https://aclanthology.org/D16-1179": {
    "title": "Verb Phrase Ellipsis Resolution Using Discriminative and Margin-Infused Algorithms",
    "abstract": "Verb Phrase Ellipsis (VPE) is an anaphoric construction in which a verb phrase has been elided. It occurs frequently in dialogue and informal conversational settings, but despite its evident impact on event coreference resolution and extraction, there has been relatively little work on computational methods for identifying and resolving VPE. Here, we present a novel approach to detecting and resolving VPE by using supervised discriminative machine learning techniques trained on features extracted from an automatically parsed, publicly available dataset. Our approach yields state-of-the-art results for VPE detection by improving F1 score by over 11%; additionally, we explore an approach to antecedent identification that uses the Margin-Infused-RelaxedAlgorithm, which shows promising results",
    "volume": "main",
    "checked": true,
    "id": "7b33745e9025c08e51fa45238d73374d6f7f92e5",
    "citation_count": 15
  },
  "https://aclanthology.org/D16-1180": {
    "title": "Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser",
    "abstract": "We introduce two first-order graph-based dependency parsers achieving a new state of the art. The first is a consensus parser built from an ensemble of independently trained greedy LSTM transition-based parsers with different random initializations. We cast this approach as minimum Bayes risk decoding (under the Hamming cost) and argue that weaker consensus within the ensemble is a useful signal of difficulty or ambiguity. The second parser is a \"distillation\" of the ensemble into a single model. We train the distillation parser using a structured hinge loss objective with a novel cost that incorporates ensemble uncertainty estimates for each possible attachment, thereby avoiding the intractable cross-entropy computations required by applying standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German",
    "volume": "main",
    "checked": true,
    "id": "d43b4801ea469a71b346698bf41197ef97e97d53",
    "citation_count": 73
  },
  "https://aclanthology.org/D16-1181": {
    "title": "LSTM Shift-Reduce CCG Parsing",
    "abstract": "We describe a neural shift-reduce parsing model for CCG, factored into four unidirectional LSTMs and one bidirectional LSTM. This factorization allows the linearization of the complete parsing history, and results in a highly accurate greedy parser that outperforms all previous beam-search shift-reduce parsers for CCG. By further deriving a globally optimized model using a task-based loss, we improve over the state of the art by up to 2.67% labeled F1",
    "volume": "main",
    "checked": true,
    "id": "dc61a283dc672ad9c1bdc539906a86d969a375ce",
    "citation_count": 21
  },
  "https://aclanthology.org/D16-1182": {
    "title": "An Evaluation of Parser Robustness for Ungrammatical Sentences",
    "abstract": "For many NLP applications that require a parser, the sentences of interest may not be well-formed. If the parser can overlook problems such as grammar mistakes and produce a parse tree that closely resembles the correct analysis for the intended sentence, we say that the parser is robust. This paper compares the performances of eight state-of-the-art dependency parsers on two domains of ungrammatical sentences: learner English and machine translation outputs. We have developed an evaluation metric and conducted a suite of experiments. Our analyses may help practitioners to choose an appropriate parser for their tasks, and help developers to improve parser robustness against ungrammatical sentences",
    "volume": "main",
    "checked": true,
    "id": "f735122727511fe7593401099012e34bf51df4e5",
    "citation_count": 18
  },
  "https://aclanthology.org/D16-1183": {
    "title": "Neural Shift-Reduce CCG Semantic Parsing",
    "abstract": "We present a shift-reduce CCG semantic parser. Our parser uses a neural network architecture that balances model capacity and computational cost. We train by transferring a model from a computationally expensive loglinear CKY parser. Our learner addresses two challenges: selecting the best parse for learning when the CKY parser generates multiple correct trees, and learning from partial derivations when the CKY parser fails to parse. We evaluate on AMR parsing. Our parser performs comparably to the CKY parser, while doing significantly fewer operations. We also present results for greedy semantic parsing with a relatively small drop in performance",
    "volume": "main",
    "checked": true,
    "id": "bf4dc8112c00cddd87c5f6110dae5efc305b1a27",
    "citation_count": 40
  },
  "https://aclanthology.org/D16-1184": {
    "title": "Syntactic Parsing of Web Queries",
    "abstract": "Syntactic parsing of web queries is important for query understanding. However, web queries usually do not observe the grammar of a written language, and no labeled syntactic trees for web queries are available. In this paper, we focus on a query's clicked sentence, i.e., a well-formed sentence that i) contains all the tokens of the query, and ii) appears in the query's top clicked web pages. We argue such sentences are semantically consistent with the query. We introduce algorithms to derive a query's syntactic structure from the dependency trees of its clicked sentences. This gives us a web query treebank without manual labeling. We then train a dependency parser on the treebank. Our model achieves much better UAS (0.86) and LAS (0.80) scores than state-of-the-art parsers on web queries",
    "volume": "main",
    "checked": true,
    "id": "606f4dea3f2457f7a7fd237c025bb79142490d2a",
    "citation_count": 7
  },
  "https://aclanthology.org/D16-1185": {
    "title": "Unsupervised Text Recap Extraction for TV Series",
    "abstract": "Sequences found at the beginning of TV shows help the audience absorb the essence of previous episodes, and grab their attention with upcoming plots. In this paper, we propose a novel task, text recap extraction. Compared with conventional summarization, text recap extraction captures the duality of summarization and plot contingency between adjacent episodes. We present a new dataset, TVRecap, for text recap extraction on TV shows. We propose an unsupervised model that identifies text recaps based on plot descriptions. We introduce two contingency factors, concept coverage and sparse reconstruction, that encourage recaps to prompt the upcoming story development. We also propose a multi-view extension of our model which can incorporate dialogues and synopses. We conduct extensive experiments on TVRecap, and conclude that our model outperforms summarization approaches",
    "volume": "main",
    "checked": true,
    "id": "dfff89390ac3bd22e4153e0639d4b7382611fb0c",
    "citation_count": 1
  },
  "https://aclanthology.org/D16-1186": {
    "title": "On- and Off-Topic Classification and Semantic Annotation of User-Generated Software Requirements",
    "abstract": "Users prefer natural language software requirements because of their usability and accessibility. When they describe their wishes for software development, they often provide off-topic information. We therefore present an automated approach for identifying and semantically annotating the on-topic parts of the given descriptions. It is designed to support requirement engineers in the requirement elicitation process on detecting and analyzing requirements in user-generated content. Since no lexical resources with domain-specific information about requirements are available, we created a corpus of requirements written in controlled language by instructed users and uncontrolled language by uninstructed users. We annotated these requirements regarding predicate-argument structures, conditions, priorities, motivations and semantic roles and used this information to train classifiers for information extraction purposes. The approach achieves an accuracy of 92% for the on- and off-topic classification task and an F1-measure of 72% for the semantic annotation",
    "volume": "main",
    "checked": true,
    "id": "16e4dcf649a17589082f8bedb61c987141ebbb28",
    "citation_count": 16
  },
  "https://aclanthology.org/D16-1187": {
    "title": "Deceptive Review Spam Detection via Exploiting Task Relatedness and Unlabeled Data",
    "abstract": "Existing work on detecting deceptive reviews primarily focuses on feature engineering and applies off-the-shelf supervised classification algorithms to the problem. Then, one real challenge would be to manually recognize plentiful ground truth spam review data for model building, which is rather difficult and often requires domain expertise in practice. In this paper, we propose to exploit the relatedness of multiple review spam detection tasks and readily available unlabeled data to address the scarcity of labeled opinion spam data. We first develop a multi-task learning method based on logistic regression (MTL-LR), which can boost the learning for a task by sharing the knowledge contained in the training signals of other related tasks. To leverage the unlabeled data, we introduce a graph Laplacian regularizer into each base model. We then propose a novel semi-supervised multitask learning method via Laplacian regularized logistic regression (SMTL-LLR) to further improve the review spam detection performance. We also develop a stochastic alternating method to cope with the optimization for SMTL-LLR. Experimental results on real-world review data demonstrate the benefit of SMTL-LLR over several well-established baseline methods",
    "volume": "main",
    "checked": true,
    "id": "3b79a2e7e4331b89d5e3feb023908339f1a41a09",
    "citation_count": 49
  },
  "https://aclanthology.org/D16-1188": {
    "title": "Regularizing Text Categorization with Clusters of Words",
    "abstract": "Regularization is a critical step in supervised learning to not only address overfitting, but also to take into account any prior knowledge we may have on the features and their dependence. In this paper, we explore stateof-the-art structured regularizers and we propose novel ones based on clusters of words from LSI topics, word2vec embeddings and graph-of-words document representation. We show that our proposed regularizers are faster than the state-of-the-art ones and still improve text classification accuracy. Code and data are available online1",
    "volume": "main",
    "checked": true,
    "id": "3a9204c4c41f46954b17a9c682a18a0924917e8e",
    "citation_count": 13
  },
  "https://aclanthology.org/D16-1189": {
    "title": "Deep Reinforcement Learning with a Combinatorial Action Space for Predicting Popular Reddit Threads",
    "abstract": "We introduce an online popularity prediction and tracking task as a benchmark task for reinforcement learning with a combinatorial, natural language action space. A specified number of discussion threads predicted to be popular are recommended, chosen from a fixed window of recent comments to track. Novel deep reinforcement learning architectures are studied for effective modeling of the value function associated with actions comprised of interdependent sub-actions. The proposed model, which represents dependence between sub-actions through a bi-directional LSTM, gives the best performance across different experimental configurations and domains, and it also generalizes well with varying numbers of recommendation requests",
    "volume": "main",
    "checked": true,
    "id": "5ebe658f62d02b871df66a0563b9f8f5c82272ca",
    "citation_count": 46
  },
  "https://aclanthology.org/D16-1190": {
    "title": "Non-Literal Text Reuse in Historical Texts: An Approach to Identify Reuse Transformations and its Application to Bible Reuse",
    "abstract": "Text reuse refers to citing, copying or alluding text excerpts from a text resource to a new context. While detecting reuse in contemporary languages is well supported—given extensive research, techniques, and corpora— automatically detecting historical text reuse is much more difficult. Corpora of historical languages are less documented and often encompass various genres, linguistic varieties, and topics. In fact, historical text reuse detection is much less understood and empirical studies are necessary to enable and improve its automation. We present a linguistic analysis of text reuse in two ancient data sets. We contribute an automated approach to analyze how an original text was transformed into its reuse, taking linguistic resources into account to understand how they help characterizing the transformation. It is complemented by a manual analysis of a subset of the reuse. Our results show the limitations of approaches focusing on literal reuse detection. Yet, linguistic resources can effectively support understanding the non-literal text reuse transformation process. Our results support practitioners and researchers working on understanding and detecting historical reuse",
    "volume": "main",
    "checked": true,
    "id": "42a129b0fb95b4a466f68bc9c697a2028d7ef9a3",
    "citation_count": 9
  },
  "https://aclanthology.org/D16-1191": {
    "title": "A Graph Degeneracy-based Approach to Keyword Extraction",
    "abstract": "We operate a change of paradigm and hypothesize that keywords are more likely to be found among influential nodes of a graph-ofwords rather than among its nodes high on eigenvector-related centrality measures. To test this hypothesis, we introduce unsupervised techniques that capitalize on graph degeneracy. Our methods strongly and significantly outperform all baselines on two datasets (short and medium size documents), and reach best performance on the third one (long documents)",
    "volume": "main",
    "checked": true,
    "id": "0c22933a45594cec626678e828269e85e28294b0",
    "citation_count": 73
  },
  "https://aclanthology.org/D16-1192": {
    "title": "Predicting the Relative Difficulty of Single Sentences With and Without Surrounding Context",
    "abstract": "The problem of accurately predicting relative reading difficulty across a set of sentences arises in a number of important natural language applications, such as finding and curating effective usage examples for intelligent language tutoring systems. Yet while significant research has explored document- and passage-level reading difficulty, the special challenges involved in assessing aspects of readability for single sentences have received much less attention, particularly when considering the role of surrounding passages. We introduce and evaluate a novel approach for estimating the relative reading difficulty of a set of sentences, with and without surrounding context. Using different sets of lexical and grammatical features, we explore models for predicting pairwise relative difficulty using logistic regression, and examine rankings generated by aggregating pairwise difficulty labels using a Bayesian rating system to form a final ranking. We also compare rankings derived for sentences assessed with and without context, and find that contextual features can help predict differences in relative difficulty judgments across these two conditions",
    "volume": "main",
    "checked": true,
    "id": "c233928d5701816fb5c2ff7409b3b5d7150b136c",
    "citation_count": 13
  },
  "https://aclanthology.org/D16-1193": {
    "title": "A Neural Approach to Automated Essay Scoring",
    "abstract": "Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. The performance of such systems is tightly bound to the quality of the underlying features. However, it is laborious to manually design the most informative features for such a system. In this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. The results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6% in terms of quadratic weighted Kappa, without requiring any feature engineering",
    "volume": "main",
    "checked": true,
    "id": "aea14f23a951975f605a981d003386e46bf8acfe",
    "citation_count": 264
  },
  "https://aclanthology.org/D16-1194": {
    "title": "Non-uniform Language Detection in Technical Writing",
    "abstract": "Technical writing in professional environments, such as user manual authoring, requires the use of uniform language. Nonuniform language detection is a novel task, which aims to guarantee the consistency for technical writing by detecting sentences in a document that are intended to have the same meaning within a similar context but use different words or writing style. This paper proposes an approach that utilizes text similarity algorithms at lexical, syntactic, semantic and pragmatic levels. Different features are extracted and integrated by applying a machine learning classification method. We tested our method using smart phone user manuals, and compared its performance against the state-ofthe-art methods in a related area. The experiments demonstrate that our approach achieves the upper bound performance for this task",
    "volume": "main",
    "checked": true,
    "id": "6f3422fc63755a1ea07f667870e4087eb09382b5",
    "citation_count": 1
  },
  "https://aclanthology.org/D16-1195": {
    "title": "Adapting Grammatical Error Correction Based on the Native Language of Writers with Neural Network Joint Models",
    "abstract": "An important aspect for the task of grammatical error correction (GEC) that has not yet been adequately explored is adaptation based on the native language (L1) of writers, despite the marked influences of L1 on second language (L2) writing. In this paper, we adapt a neural network joint model (NNJM) using L1-specific learner text and integrate it into a statistical machine translation (SMT) based GEC system. Specifically, we train an NNJM on general learner text (not L1-specific) and subsequently train on L1-specific data using a Kullback-Leibler divergence regularized objective function in order to preserve generalization of the model. We incorporate this adapted NNJM as a feature in an SMT-based English GEC system and show that adaptation achieves significant F0.5 score gains on English texts written by L1 Chinese, Russian, and Spanish writers",
    "volume": "main",
    "checked": true,
    "id": "b9067abeda0b6376fc0add64feb432e2b9b1351d",
    "citation_count": 33
  },
  "https://aclanthology.org/D16-1196": {
    "title": "Orthographic Syllable as basic unit for SMT between Related Languages",
    "abstract": "We explore the use of the orthographic syllable, a variable-length consonant-vowel sequence, as a basic unit of translation between related languages which use abugida or alphabetic scripts. We show that orthographic syllable level translation significantly outperforms models trained over other basic units (word, morpheme and character) when training over small parallel corpora",
    "volume": "main",
    "checked": true,
    "id": "8da1c2acdb8174f16566606d8d8b7bf3870d649a",
    "citation_count": 29
  },
  "https://aclanthology.org/D16-1197": {
    "title": "Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge",
    "abstract": "This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models",
    "volume": "main",
    "checked": true,
    "id": "74157ae408173bf713f1e94f15aca1475c43bd74",
    "citation_count": 70
  },
  "https://aclanthology.org/D16-1198": {
    "title": "Supervised Keyphrase Extraction as Positive Unlabeled Learning",
    "abstract": "The problem of noisy and unbalanced training data for supervised keyphrase extraction results from the subjectivity of keyphrase assignment, which we quantify by crowdsourcing keyphrases for news and fashion magazine articles with many annotators per document. We show that annotators exhibit substantial disagreement, meaning that single annotator data could lead to very different training sets for supervised keyphrase extractors. Thus, annotations from single authors or readers lead to noisy training data and poor extraction performance of the resulting supervised extractor. We provide a simple but effective solution to still work with such data by reweighting the importance of unlabeled candidate phrases in a two stage Positive Unlabeled Learning setting. We show that performance of trained keyphrase extractors approximates a classifier trained on articles labeled by multiple annotators, leading to higher average F1scores and better rankings of keyphrases. We apply this strategy to a variety of test collections from different backgrounds and show improvements over strong baseline models",
    "volume": "main",
    "checked": true,
    "id": "95d8ff7cd82d9626a58339ebb3f59ac08127ebdf",
    "citation_count": 44
  },
  "https://aclanthology.org/D16-1199": {
    "title": "Learning to Answer Questions from Wikipedia Infoboxes",
    "abstract": "A natural language interface to answers on the Web can help us access information more efficiently. We start with an interesting source of information—infoboxes in Wikipedia that summarize factoid knowledge—and develop a comprehensive approach to answering questions with high precision. We first build a system to access data in infoboxes in a structured manner. We use our system to construct a crowdsourced dataset of over 15,000 highquality, diverse questions. With these questions, we train a convolutional neural network model that outperforms models that achieve top results in similar answer selection tasks",
    "volume": "main",
    "checked": true,
    "id": "91a95f979f22172746c42ab436391674968c4983",
    "citation_count": 16
  },
  "https://aclanthology.org/D16-1200": {
    "title": "Timeline extraction using distant supervision and joint inference",
    "abstract": "In timeline extraction the goal is to order all the events in which a target entity is involved in a timeline. Due to the lack of explicitly annotated data, previous work is primarily rule-based and uses pre-trained temporal linking systems. In this work, we propose a distantly supervised approach by heuristically aligning timelines with documents. The noisy training data created allows us to learn models that anchor events to temporal expressions and entities; during testing, the predictions of these models are combined to produce the timeline. Furthermore, we show how to improve performance using joint inference. In experiments in the SemEval-2015 TimeLine task we show that our distantly supervised approach matches the state-of-the-art performance while joint inference further improves on it by 3.2 F-score points",
    "volume": "main",
    "checked": true,
    "id": "520e82c0f35a14ecf78b93de3673bb8b2a3212fc",
    "citation_count": 12
  },
  "https://aclanthology.org/D16-1201": {
    "title": "Combining Supervised and Unsupervised Enembles for Knowledge Base Population",
    "abstract": "We propose an algorithm that combines supervised and unsupervised methods to ensemble multiple systems for two popular Knowledge Base Population (KBP) tasks, Cold Start Slot Filling (CSSF) and Tri-lingual Entity Discovery and Linking (TEDL). We demonstrate that it outperforms the best system for both tasks in the 2015 competition, several ensembling baselines, as well as a state-of-the-art stacking approach. The success of our technique on two different and challenging problems demonstrates the power and generality of our combined approach to ensembling",
    "volume": "main",
    "checked": true,
    "id": "07eece3e33bf5c9ec88710ec5c0ede98a2dae1b1",
    "citation_count": 11
  },
  "https://aclanthology.org/D16-1202": {
    "title": "Character Sequence Models for Colorful Words",
    "abstract": "We present a neural network architecture to predict a point in color space from the sequence of characters in the color's name. Using large scale color--name pairs obtained from an online color design forum, we evaluate our model on a \"color Turing test\" and find that, given a name, the colors predicted by our model are preferred by annotators to color names created by humans. Our datasets and demo system are available online at colorlab.us",
    "volume": "main",
    "checked": true,
    "id": "805fe1ce2e51f21c7116496407907ecfb2ee60a8",
    "citation_count": 14
  },
  "https://aclanthology.org/D16-1203": {
    "title": "Analyzing the Behavior of Visual Question Answering Models",
    "abstract": "Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models -- with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016.   Our behavior analysis reveals that despite recent progress, today's VQA models are \"myopic\" (tend to fail on sufficiently novel instances), often \"jump to conclusions\" (converge on a predicted answer after 'listening' to just half the question), and are \"stubborn\" (do not change their answers across images)",
    "volume": "main",
    "checked": true,
    "id": "8e759195eb4b4f0f480a8a2cf1c629bfd881d4e5",
    "citation_count": 246
  },
  "https://aclanthology.org/D16-1204": {
    "title": "Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text",
    "abstract": "This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality",
    "volume": "main",
    "checked": true,
    "id": "d1ffd519ff274517ec6fd014ae67af0d0c68a969",
    "citation_count": 111
  },
  "https://aclanthology.org/D16-1205": {
    "title": "Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity",
    "abstract": "Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, thereby loosing important information for word expectations, such as word interrelations. In this paper, we present a DSM that addresses this issue by defining verb contexts as joint syntactic dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts achieve performances comparable to single dependencies or even better. Moreover, they are able to overcome the data sparsity problem of joint feature spaces, in spite of the limited size of our training corpus",
    "volume": "main",
    "checked": true,
    "id": "11802a2935e46be5a87db0f9bea8c98b1dc1cd54",
    "citation_count": 14
  },
  "https://aclanthology.org/D16-1206": {
    "title": "Speed-Accuracy Tradeoffs in Tagging with Variable-Order CRFs and Structured Sparsity",
    "abstract": "We propose a method for learning the structure of variable-order CRFs, a more flexible variant of higher-order linear-chain CRFs. Variableorder CRFs achieve faster inference by including features for only some of the tag ngrams. Our learning method discovers the useful higher-order features at the same time as it trains their weights, by maximizing an objective that combines log-likelihood with a structured-sparsity regularizer. An active-set outer loop allows the feature set to grow as far as needed. On part-of-speech tagging in 5 randomly chosen languages from the Universal Dependencies dataset, our method of shrinking the model achieved a 2–6x speedup over a baseline, with no significant drop in accuracy",
    "volume": "main",
    "checked": true,
    "id": "29dfd68b03505ef981f5b36ea3c5a5a53dff0c0b",
    "citation_count": 10
  },
  "https://aclanthology.org/D16-1207": {
    "title": "Learning Robust Representations of Text",
    "abstract": "Deep neural networks have achieved remarkable results across many language processing tasks, however these methods are highly sensitive to noise and adversarial attacks. We present a regularization based method for limiting network sensitivity to its inputs, inspired by ideas from computer vision, thus learning models that are more robust. Empirical evaluation over a range of sentiment datasets with a convolutional neural network shows that, compared to a baseline model and the dropout method, our method achieves superior performance over noisy inputs and out-of-domain data",
    "volume": "main",
    "checked": true,
    "id": "57567039998db7ef670c6ee6837301402d72c078",
    "citation_count": 13
  },
  "https://aclanthology.org/D16-1208": {
    "title": "Modified Dirichlet Distribution: Allowing Negative Parameters to Induce Stronger Sparsity",
    "abstract": "The Dirichlet distribution (Dir) is one of the most widely used prior distributions in statistical approaches to natural language processing. The parameters of Dir are required to be positive, which significantly limits its strength as a sparsity prior. In this paper, we propose a simple modification to the Dirichlet distribution that allows the parameters to be negative. Our modified Dirichlet distribution (mDir) not only induces much stronger sparsity, but also simultaneously performs smoothing. mDir is still conjugate to the multinomial distribution, which simplifies posterior inference. We introduce two simple and efficient algorithms for finding the mode of mDir. Our experiments on learning Gaussian mixtures and unsupervised dependency parsing demonstrate the advantage of mDir over Dir. © 2016 Association for Computational Linguistics",
    "volume": "main",
    "checked": true,
    "id": "4fc20939aa4b51889d08ae81b817c984e0aa6d36",
    "citation_count": 5
  },
  "https://aclanthology.org/D16-1209": {
    "title": "Gated Word-Character Recurrent Language Model",
    "abstract": "We introduce a recurrent neural network language model (RNN-LM) with long short-term memory (LSTM) units that utilizes both character-level and word-level inputs. Our model has a gate that adaptively finds the optimal mixture of the character-level and word-level inputs. The gate creates the final vector representation of a word by combining two distinct representations of the word. The character-level inputs are converted into vector representations of words using a bidirectional LSTM. The word-level inputs are projected into another high-dimensional space by a word lookup table. The final vector representations of words are used in the LSTM language model which predicts the next word given all the preceding words. Our model with the gating mechanism effectively utilizes the character-level inputs for rare and out-of-vocabulary words and outperforms word-level language models on several English corpora",
    "volume": "main",
    "checked": true,
    "id": "58001259d2f6442b07cc0d716ff99899abbb2bc7",
    "citation_count": 94
  },
  "https://aclanthology.org/D16-1210": {
    "title": "Unsupervised Word Alignment by Agreement Under ITG Constraint",
    "abstract": "We propose a novel unsupervised word alignment method that uses a constraint based on Inversion Transduction Grammar (ITG) parse trees to jointly unify two directional models. Previous agreement methods are not helpful for locating alignments with long distances because they do not use any syntactic structures. In contrast, the proposed method symmetrizes alignments in consideration of their structural coherence by using the ITG constraint softly in the posterior regularization framework (Ganchev et al., 2010). The ITG constraint is also compatible with word alignments that are not covered by ITG parse trees. Hence, the proposed method is robust to ITG parse errors compared to other alignment methods that directly use an ITG model. Compared to the HMM (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), and the baseline agreement method (Ganchev et al., 2010), the experimental results show that the proposed method significantly improves alignment performance regarding the Japanese-English KFTT and BTEC corpus, and in translation evaluation, the proposed method shows comparable or statistical significantly better performance on the JapaneseEnglish KFTT and IWSLT 2007 corpus",
    "volume": "main",
    "checked": true,
    "id": "383e8b0c887cce0192b4cd944c61585af59e5d2c",
    "citation_count": 2
  },
  "https://aclanthology.org/D16-1211": {
    "title": "Training with Exploration Improves a Greedy Stack LSTM Parser",
    "abstract": "We adapt the greedy Stack-LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles(Goldberg and Nivre, 2013) instead of cross-entropy minimization. This form of training, which accounts for model predictions at training time rather than assuming an error-free action history, improves parsing accuracies for both English and Chinese, obtaining very strong results for both languages. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural-network",
    "volume": "main",
    "checked": true,
    "id": "6789e0dbd294cccb3b7dd4e001c9e8ba4813f334",
    "citation_count": 74
  },
  "https://aclanthology.org/D16-1212": {
    "title": "Capturing Argument Relationship for Chinese Semantic Role Labeling",
    "abstract": "In this paper, we capture the argument relationships for Chinese semantic role labeling task, and improve the task's performance with the help of argument relationships. We split the relationship between two candidate arguments into two categories: (1) Compatible arguments: if one candidate argument belongs to a given predicate, then the other is more likely to belong to the same predicate; (2) Incompatible arguments: if one candidate argument belongs to a given predicate, then the other is less likely to belong to the same predicate. However, previous works did not explicitly model argument relationships. We use a simple maximum entropy classifier to capture the two categories of argument relationships and test its performance on the Chinese Proposition Bank (CPB). The experiments show that argument relationships is effective in Chinese semantic role labeling task",
    "volume": "main",
    "checked": true,
    "id": "b991625f3c25aaf5f82eebfa6405a017a16a1695",
    "citation_count": 8
  },
  "https://aclanthology.org/D16-1213": {
    "title": "BrainBench: A Brain-Image Test Suite for Distributional Semantic Models",
    "abstract": "The brain is the locus of our language ability, and so brain images can be used to ground linguistic theories. Here we introduce BrainBench, a lightweight system for testing distributional models of word semantics. We compare the performance of several models, and show that the performance on brain-image tasks differs from the performance on behavioral tasks. We release our benchmark test as part of a web service",
    "volume": "main",
    "checked": true,
    "id": "95e0308c4dbc98f27c0645c7d87204c4d7be33af",
    "citation_count": 23
  },
  "https://aclanthology.org/D16-1214": {
    "title": "Evaluating Induced CCG Parsers on Grounded Semantic Parsing",
    "abstract": "We compare the effectiveness of four different syntactic CCG parsers for a semantic slot-filling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation provides a unique window to explore the strengths and weaknesses of semantics captured by unsupervised grammar induction systems. We release a new Freebase semantic parsing dataset called SPADES (Semantic PArsing of DEclarative Sentences) containing 93K cloze-style questions paired with answers. We evaluate all our models on this dataset. Our code and data are available at this https URL",
    "volume": "main",
    "checked": true,
    "id": "c72cdb5ce7e0911c7f442ab503652d6fdeef35e0",
    "citation_count": 20
  },
  "https://aclanthology.org/D16-1215": {
    "title": "Vector-space models for PPDB paraphrase ranking in context",
    "abstract": "The PPDB is an automatically built database which contains millions of paraphrases in different languages. Paraphrases in this resource are associated with features that serve to their ranking and reflect paraphrase quality. This context-unaware ranking captures the semantic similarity of paraphrases but cannot serve to estimate their adequacy in specific contexts. We propose to use vector-space semantic models for selecting PPDB paraphrases that preserve the meaning of specific text fragments. This is the first work that addresses the substitutability of PPDB paraphrases in context. We show that vector-space models of meaning can be successfully applied to this task and increase the benefit brought by the use of the PPDB resource in applications",
    "volume": "main",
    "checked": true,
    "id": "90f5f51b819f98cf4fd6a5a09caa45844fc06636",
    "citation_count": 7
  },
  "https://aclanthology.org/D16-1216": {
    "title": "Interpreting Neural Networks to Improve Politeness Comprehension",
    "abstract": "We present an interpretable neural network approach to predicting and understanding politeness in natural language requests. Our models are based on simple convolutional neural networks directly on raw text, avoiding any manual identification of complex sentiment or syntactic features, while performing better than such feature-based models from previous work. More importantly, we use the challenging task of politeness prediction as a testbed to next present a much-needed understanding of what these successful networks are actually learning. For this, we present several network visualizations based on activation clusters, first derivative saliency, and embedding space transformations, helping us automatically identify several subtle linguistics markers of politeness theories. Further, this analysis reveals multiple novel, high-scoring politeness strategies which, when added back as new features, reduce the accuracy gap between the original featurized system and the neural model, thus providing a clear quantitative interpretation of the success of these neural networks",
    "volume": "main",
    "checked": true,
    "id": "b3c8d8aa2be2ba5a271c9be98dca2ccbc31e88f1",
    "citation_count": 48
  },
  "https://aclanthology.org/D16-1217": {
    "title": "Does ‘well-being' translate on Twitter?",
    "abstract": "We investigate whether psychological wellbeing translates across English and Spanish Twitter, by building and comparing source language and automatically translated weighted lexica in English and Spanish. We find that the source language models perform substantially better than the machine translated versions. Moreover, manually correcting translation errors does not improve model performance, suggesting that meaningful cultural information is being lost in translation. Further work is needed to clarify when automatic translation of well-being lexica is effective and how it can be improved for crosscultural analysis",
    "volume": "main",
    "checked": true,
    "id": "74afe33b8c2248ff27e7bb9672f9a246e13052b4",
    "citation_count": 9
  },
  "https://aclanthology.org/D16-1218": {
    "title": "Beyond Canonical Texts: A Computational Analysis of Fanfiction",
    "abstract": "While much computational work on fiction has focused on works in the literary canon, user-created fanfiction presents a unique opportunity to study an ecosystem of literary production and consumption, embodying qualities both of large-scale literary data (55 billion tokens) and also a social network (with over 2 million users). We present several empirical analyses of this data in order to illustrate the range of affordances it presents to research in NLP, computational social science and the digital humanities. We find that fanfiction deprioritizes main protagonists in comparison to canonical texts, has a statistically significant difference in attention allocated to female characters, and offers a framework for developing models of reader reactions to stories",
    "volume": "main",
    "checked": true,
    "id": "55dd3ee73a78819d974176425c3a35dbe9359063",
    "citation_count": 21
  },
  "https://aclanthology.org/D16-1219": {
    "title": "Using Syntactic and Semantic Context to Explore Psychodemographic Differences in Self-reference",
    "abstract": "Psychological analysis of language has repeatedly shown that an individual's rate of mentioning 1st person singular pronouns predicts a wealth of important demographic and psychological factors. However, these analyses are performed out of context — syntactic and semantic — which may change the magnitude or even direction of such relationships. In this paper, we put \"pronouns in their context\", exploring the relationship between self-reference and age, gender, and depression depending on syntactic position and verbal governor. We find that pronouns are overall more predictive when taking dependency relations and verb semantic categories into account, and, the direction of the relationship can change depending on the semantic class of the verbal governor",
    "volume": "main",
    "checked": true,
    "id": "0d5855a9fb5809c7f715a1c3dd9358f39b9f3ce4",
    "citation_count": 5
  },
  "https://aclanthology.org/D16-1220": {
    "title": "Learning to Identify Metaphors from a Corpus of Proverbs",
    "abstract": "In this paper, we experiment with a resource consisting of metaphorically annotated proverbs on the task of word-level metaphor recognition. We observe that existing feature sets do not perform well on this data. We design a novel set of features to better capture the peculiar nature of proverbs and we demonstrate that these new features are significantly more effective on the metaphorically dense proverb data",
    "volume": "main",
    "checked": true,
    "id": "9a23eda0f9067bddbabe38a71318ee245494b106",
    "citation_count": 6
  },
  "https://aclanthology.org/D16-1221": {
    "title": "An Embedding Model for Predicting Roll-Call Votes",
    "abstract": "We develop a novel embedding-based model for predicting legislative roll-call votes from bill text. The model introduces multidimensional ideal vectors for legislators as an alternative to single dimensional ideal point models for quantitatively analyzing roll-call data. These vectors are learned to correspond with pre-trained word embeddings which allows us to analyze which features in a bill text are most predictive of political support. Our model is quite simple, while at the same time allowing us to successfully predict legislator votes on specific bills with higher accuracy than past methods",
    "volume": "main",
    "checked": true,
    "id": "bc297b4f88a75ecbfd20c3288e2aca855d02d22a",
    "citation_count": 28
  },
  "https://aclanthology.org/D16-1222": {
    "title": "Natural Language Model Re-usability for Scaling to Different Domains",
    "abstract": "Natural language understanding is the coreof the human computer interactions. However,building new domains and tasks thatneed a separate set of models is a bottleneckfor scaling to a large number of domainsand experiences. In this paper, wepropose a practical technique that addressesthis issue in a web-scale language understandingsystem: Microsoft's personal digital assistantCortana. The proposed technique usesa constrained decoding method with a universalslot tagging model sharing the sameschema as the collection of slot taggers builtfor each domain. The proposed approach allowsreusing of slots across different domainsand tasks while achieving virtually the sameperformance as those slot taggers trained perdomain fashion",
    "volume": "main",
    "checked": true,
    "id": "8ab6df876566c2f747bc3757f938299db5c81cca",
    "citation_count": 13
  },
  "https://aclanthology.org/D16-1223": {
    "title": "Leveraging Sentence-level Information with Encoder LSTM for Semantic Slot Filling",
    "abstract": "Recurrent Neural Network (RNN) and one of its specific architectures, Long Short-Term Memory (LSTM), have been widely used for sequence labeling. In this paper, we first enhance LSTM-based sequence labeling to explicitly model label dependencies. Then we propose another enhancement to incorporate the global information spanning over the whole input sequence. The latter proposed method, encoder-labeler LSTM, first encodes the whole input sequence into a fixed length vector with the encoder LSTM, and then uses this encoded vector as the initial state of another LSTM for sequence labeling. Combining these methods, we can predict the label sequence with considering label dependencies and information of whole input sequence. In the experiments of a slot filling task, which is an essential component of natural language understanding, with using the standard ATIS corpus, we achieved the state-of-the-art F1-score of 95.66%",
    "volume": "main",
    "checked": true,
    "id": "58e2c9af296e710740a4268d87ac8cc308ca90a1",
    "citation_count": 109
  },
  "https://aclanthology.org/D16-1224": {
    "title": "AMR-to-text generation as a Traveling Salesman Problem",
    "abstract": "The task of AMR-to-text generation is to generate grammatical text that sustains the semantic meaning for a given AMR graph. We at- tack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution. The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset",
    "volume": "main",
    "checked": true,
    "id": "0e367d898c9701f09ec3205b39bb19aa677c751f",
    "citation_count": 31
  },
  "https://aclanthology.org/D16-1225": {
    "title": "Learning to Capitalize with Character-Level Recurrent Neural Networks: An Empirical Study",
    "abstract": "In this paper, we investigate case restoration for text without case information. Previous such work operates at the word level. We propose an approach using character-level recurrent neural networks (RNN), which performs competitively compared to language modeling and conditional random fields (CRF) approaches. We further provide quantitative and qualitative analysis on how RNN helps improve truecasing",
    "volume": "main",
    "checked": true,
    "id": "ba7b2c534c749408c3d269f1c7996426576e2959",
    "citation_count": 25
  },
  "https://aclanthology.org/D16-1226": {
    "title": "The Effects of the Content of FOMC Communications on US Treasury Rates",
    "abstract": "This study measures the effects of Federal Open Market Committee text content on the direction of shortand medium-term interest rate movements. Because the words relevant to shortand medium-term interest rates differ, we apply a supervised approach to learn distinct sets of topics for each dependent variable being examined. We generate predictions with and without controlling for factors relevant to interest rate movements, and our prediction results average across multiple training-test splits. Using data from 1999-2016, we achieve 93% and 64% accuracy in predicting Target and Effective Federal Funds Rate movements and 38%-40% accuracy in predicting longer term Treasury Rate movements. We obtain lower but comparable accuracies after controlling for other macroeconomic and market factors",
    "volume": "main",
    "checked": true,
    "id": "7983c187c6b42266a459c6de010b54482164258e",
    "citation_count": 6
  },
  "https://aclanthology.org/D16-1227": {
    "title": "Learning to refine text based recommendations",
    "abstract": "We propose a text-based recommendation engine that utilizes recurrent neural networks to flexibly map textual input into continuous vector representations tailored to the recommendation task. Here, the text objects are documents such as Wikipedia articles or question and answer pairs. As neural models require substantial training time, we introduce a sequential component so as to quickly adjust the learned metric over objects as additional evidence accrues. We evaluate the approach on recommending Wikipedia descriptions of ingredients to their associated product categories. We also exemplify the sequential metric adjustment on retrieving similar Stack Exchange AskUbuntu questions. 1",
    "volume": "main",
    "checked": true,
    "id": "16d20ccadd1331c357a6f589df437000f611d69c",
    "citation_count": 10
  },
  "https://aclanthology.org/D16-1228": {
    "title": "There's No Comparison: Reference-less Evaluation Metrics in Grammatical Error Correction",
    "abstract": "Current methods for automatically evaluating grammatical error correction (GEC) systems rely on gold-standard references. However, these methods suffer from penalizing grammatical edits that are correct but not in the gold standard. We show that reference-less grammaticality metrics correlate very strongly with human judgments and are competitive with the leading reference-based evaluation metrics. By interpolating both methods, we achieve state-of-the-art correlation with human judgments. Finally, we show that GEC metrics are much more reliable when they are calculated at the sentence level instead of the corpus level. We have set up a CodaLab site for benchmarking GEC output using a common dataset and different evaluation metrics",
    "volume": "main",
    "checked": true,
    "id": "4ffd3fbbefc45d7847687a82cad166bac14a5fc7",
    "citation_count": 42
  },
  "https://aclanthology.org/D16-1229": {
    "title": "Cultural Shift or Linguistic Drift? Comparing Two Computational Measures of Semantic Change",
    "abstract": "Words shift in meaning for many reasons, including cultural factors like new technologies and regular linguistic processes like subjectification. Understanding the evolution of language and culture requires disentangling these underlying causes. Here we show how two different distributional measures can be used to detect two different types of semantic change. The first measure, which has been used in many previous works, analyzes global shifts in a word's distributional semantics; it is sensitive to changes due to regular processes of linguistic drift, such as the semantic generalization of promise (\"I promise.\" \"It promised to be exciting.\"). The second measure, which we develop here, focuses on local changes to a word's nearest semantic neighbors; it is more sensitive to cultural shifts, such as the change in the meaning of cell (\"prison cell\" \"cell phone\"). Comparing measurements made by these two methods allows researchers to determine whether changes are more cultural or linguistic in nature, a distinction that is essential for work in the digital humanities and historical linguistics",
    "volume": "main",
    "checked": true,
    "id": "f39cb3680b0810fb794c0b52690cb9e3e0b16ced",
    "citation_count": 180
  },
  "https://aclanthology.org/D16-1230": {
    "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
    "abstract": "We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems",
    "volume": "main",
    "checked": true,
    "id": "129cbad01be98ee88a930e31898cb76be79c41c1",
    "citation_count": 1024
  },
  "https://aclanthology.org/D16-1231": {
    "title": "Addressee and Response Selection for Multi-Party Conversation",
    "abstract": "To create conversational systems working in actual situations, it is crucial to assume that they interact with multiple agents. In this work, we tackle addressee and response selection for multi-party conversation, in which systems are expected to select whom they address as well as what they say. The key challenge of this task is to jointly model who is talking about what in a previous context. For the joint modeling, we propose two modeling frameworks: 1) static modeling and 2) dynamic modeling. To show benchmark results of our frameworks, we created a multi-party conversation corpus. Our experiments on the dataset show that the recurrent neural network based models of our frameworks robustly predict addressees and responses in conversations with a large number of agents",
    "volume": "main",
    "checked": true,
    "id": "811e014002d1e4d1e185fc236cf9e3fafe2aade5",
    "citation_count": 37
  },
  "https://aclanthology.org/D16-1232": {
    "title": "Nonparametric Bayesian Models for Spoken Language Understanding",
    "abstract": "In this paper, we propose a new generative approach for semantic slot filling task in spoken language understanding using a nonparametric Bayesian formalism. Slot filling is typically formulated as a sequential labeling problem, which does not directly deal with the posterior distribution of possible slot values. We present a nonparametric Bayesian model involving the generation of arbitrary natural language phrases, which allows an explicit calculation of the distribution over an infinite set of slot values. We demonstrate that this approach significantly improves slot estimation accuracy compared to the existing sequential labeling algorithm",
    "volume": "main",
    "checked": true,
    "id": "c82fb23b008c11b064570724364d4f52910701bf",
    "citation_count": 2
  },
  "https://aclanthology.org/D16-1233": {
    "title": "Conditional Generation and Snapshot Learning in Neural Dialogue Systems",
    "abstract": "Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used",
    "volume": "main",
    "checked": true,
    "id": "55289d3feef4bc1e4ff17008120e371eb7f55a24",
    "citation_count": 69
  },
  "https://aclanthology.org/D16-1234": {
    "title": "Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment",
    "abstract": "We consider the task of predicting lexical entailment using distributional vectors. We perform a novel qualitative analysis of one existing model which was previously shown to only measure the prototypicality of word pairs. We find that the model strongly learns to identify hypernyms using Hearst patterns, which are well known to be predictive of lexical relations. We present a novel model which exploits this behavior as a method of feature extraction in an iterative procedure similar to Principal Component Analysis. Our model combines the extracted features with the strengths of other proposed models in the literature, and matches or outperforms prior work on multiple data sets",
    "volume": "main",
    "checked": true,
    "id": "c881d1989fa1a497b404c511a26bf7df9f12936e",
    "citation_count": 69
  },
  "https://aclanthology.org/D16-1235": {
    "title": "SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity",
    "abstract": "Verbs play a critical role in the meaning of sentences, but these ubiquitous words have received little attention in recent distributional semantics research. We introduce SimVerb-3500, an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed verb types from the USF free-association database, providing at least three examples for every VerbNet class. This broad coverage facilitates detailed analyses of how syntactic and semantic phenomena together influence human understanding of verb meaning. Further, with significantly larger development and test sets than existing benchmarks, SimVerb-3500 enables more robust evaluation of representation learning architectures and promotes the development of methods tailored to verbs. We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning",
    "volume": "main",
    "checked": true,
    "id": "a1d1e3f7ee6767712fe14ee1bde11a341775daa3",
    "citation_count": 228
  },
  "https://aclanthology.org/D16-1236": {
    "title": "POLY: Mining Relational Paraphrases from Multilingual Sentences",
    "abstract": "Language resources that systematically organize paraphrases for binary relations are of great value for various NLP tasks and have recently been advanced in projects like PATTY, WiseNet and DEFIE. This paper presents a new method for building such a resource and the resource itself, called POLY. Starting with a very large collection of multilingual sentences parsed into triples of phrases, our method clusters relational phrases using probabilistic measures. We judiciously leverage fine-grained semantic typing of relational arguments for identifying synonymous phrases. The evaluation of POLY shows significant improvements in precision and recall over the prior works on PATTY and DEFIE. An extrinsic use case demonstrates the benefits of POLY for question answering",
    "volume": "main",
    "checked": true,
    "id": "76f43ca313f2619af0ce55f5a29425e26918ae11",
    "citation_count": 16
  },
  "https://aclanthology.org/D16-1237": {
    "title": "Exploiting Sentence Similarities for Better Alignments",
    "abstract": "We study the problem of jointly aligning sentence constituents and predicting their similarities. While extensive sentence similarity data exists, manually generating reference alignments and labeling the similarities of the aligned chunks is comparatively onerous. This prompts the natural question of whether we can exploit easy-to-create sentence level data to train better aligners. In this paper, we present a model that learns to jointly align constituents of two sentences and also predict their similarities. By taking advantage of both sentence and constituent level data, we show that our model achieves state-of-the-art performance at predicting alignments and constituent similarities",
    "volume": "main",
    "checked": true,
    "id": "76d971e45ffe6302a0e4cec5841b384423c872f3",
    "citation_count": 8
  },
  "https://aclanthology.org/D16-1238": {
    "title": "Bi-directional Attention with Agreement for Dependency Parsing",
    "abstract": "We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions. The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The proposed parser makes use of {\\it soft} headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity. We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages",
    "volume": "main",
    "checked": true,
    "id": "0c9211b3c08a32bc4d73d04f3c427c7db5e0fe91",
    "citation_count": 44
  },
  "https://aclanthology.org/D16-1239": {
    "title": "Anchoring and Agreement in Syntactic Annotations",
    "abstract": "We present a study on two key characteristics of human syntactic annotations: anchoring and agreement. Anchoring is a well known cognitive bias in human decision making, where judgments are drawn towards pre-existing values. We study the influence of anchoring on a standard approach to creation of syntactic resources where syntactic annotations are obtained via human editing of tagger and parser output. Our experiments demonstrate a clear anchoring effect and reveal unwanted consequences, including overestimation of parsing performance and lower quality of annotations in comparison with human-based annotations. Using sentences from the Penn Treebank WSJ, we also report systematically obtained inter-annotator agreement estimates for English dependency parsing. Our agreement results control for parser bias, and are consequential in that they are on par with state of the art parsing performance for English newswire. We discuss the impact of our findings on strategies for future annotation efforts and parser evaluations",
    "volume": "main",
    "checked": true,
    "id": "e993cab5f30c298d5224e2288beb9fd245c1cd34",
    "citation_count": 25
  },
  "https://aclanthology.org/D16-1240": {
    "title": "Tense Manages to Predict Implicative Behavior in Verbs",
    "abstract": "Implicative verbs (e.g. manage) entail their complement clauses, while non-implicative verbs (e.g. want) do not. For example, while managing to solve the problem entails solving the problem, no such inference follows from wanting to solve the problem. Differentiating between implicative and non-implicative verbs is therefore an essential component of natural language understanding, relevant to applications such as textual entailment and summarization. We present a simple method for predicting implicativeness which exploits known constraints on the tense of implicative verbs and their complements. We show that this yields an effective, data-driven way of capturing this nuanced property in verbs",
    "volume": "main",
    "checked": true,
    "id": "ae0699d65e00ca9aa793f3e5ea71ab114deb120b",
    "citation_count": 8
  },
  "https://aclanthology.org/D16-1241": {
    "title": "Who did What: A Large-Scale Person-Centered Cloze Dataset",
    "abstract": "We have constructed a new \"Who-did-What\" dataset of over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus. The WDW dataset has a variety of novel features. First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation. Instead, each problem is formed from two independent articles --- an article given as the passage to be read and a separate article on the same events used to form the question. Second, we avoid anonymization --- each choice is a person named entity. Third, the problems have been filtered to remove a fraction that are easily solved by simple baselines, while remaining 84% solvable by humans. We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community",
    "volume": "main",
    "checked": true,
    "id": "a39ffa57ef8e538b3c6a6c2bbc0b641f7cdc60dc",
    "citation_count": 130
  },
  "https://aclanthology.org/D16-1242": {
    "title": "Building compositional semantics and higher-order inference system for a wide-coverage Japanese CCG parser",
    "abstract": "This paper presents a system that compositionally maps outputs of a wide-coverage Japanese CCG parser onto semantic representations and performs automated inference in higher-order logic. The system is evaluated on a textual entailment dataset. It is shown that the system solves inference problems that focus on a variety of complex linguistic phenomena, including those that are difficult to represent in the standard first-order logic",
    "volume": "main",
    "checked": true,
    "id": "7f625b01df244b6fa7f95dc3e995e9d4a3909473",
    "citation_count": 17
  },
  "https://aclanthology.org/D16-1243": {
    "title": "Learning to Generate Compositional Color Descriptions",
    "abstract": "The production of color language is essential for grounded language generation. Color descriptions have many challenging properties: they can be vague, compositionally complex, and denotationally rich. We present an effective approach to generating color descriptions using recurrent neural networks and a Fourier-transformed color representation. Our model outperforms previous work on a conditional language modeling task over a large corpus of naturalistic color descriptions. In addition, probing the model's output reveals that it can accurately produce not only basic color terms but also descriptors with non-convex denotations (\"greenish\"), bare modifiers (\"bright\", \"dull\"), and compositional phrases (\"faded teal\") not seen in training",
    "volume": "main",
    "checked": true,
    "id": "0ec9ec9e37131ff28efba99c6cab789d08dfff38",
    "citation_count": 20
  },
  "https://aclanthology.org/D16-1244": {
    "title": "A Decomposable Attention Model for Natural Language Inference",
    "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements",
    "volume": "main",
    "checked": true,
    "id": "2cd8e8f510c89c7c18268e8ad51c061e459ad321",
    "citation_count": 1168
  },
  "https://aclanthology.org/D16-1245": {
    "title": "Deep Reinforcement Learning for Mention-Ranking Coreference Models",
    "abstract": "Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment with two approaches: the REINFORCE policy gradient algorithm and a reward-rescaled max-margin objective. We find the latter to be more effective, resulting in significant improvements over the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task",
    "volume": "main",
    "checked": true,
    "id": "3aa64ea8099f9c1f705f616937f27827c873d77f",
    "citation_count": 290
  },
  "https://aclanthology.org/D16-1246": {
    "title": "A Stacking Gated Neural Architecture for Implicit Discourse Relation Classification",
    "abstract": "Discourse parsing is considered as one of the most challenging natural language processing (NLP) tasks. Implicit discourse relation classification is the bottleneck for discourse parsing. Without the guide of explicit discourse connectives, the relation of sentence pairs are very hard to be inferred. This paper proposes a stacking neural network model to solve the classification problem in which a convolutional neural network (CNN) is utilized for sentence modeling and a collaborative gated neural network (CGNN) is proposed for feature transformation. Our evaluation and comparisons show that the proposed model outperforms previous state-of-the-art systems",
    "volume": "main",
    "checked": true,
    "id": "cfd468bf8b138b1eed6b32ad262a1a794f9440b4",
    "citation_count": 56
  },
  "https://aclanthology.org/D16-1247": {
    "title": "Insertion Position Selection Model for Flexible Non-Terminals in Dependency Tree-to-Tree Machine Translation",
    "abstract": "Dependency tree-to-tree translation models are powerful because they can naturally handle long range reorderings which is important for distant language pairs. The translation process is easy if it can be accomplished only by replacing non-terminals in translation rules with other rules. However it is sometimes necessary to adjoin translation rules. Flexible non-terminals have been proposed as a promising solution for this problem. A flexible non-terminal provides several insertion position candidates for the rules to be adjoined, but it increases the computational cost of decoding. In this paper we propose a neural network based insertion position selection model to reduce the computational cost by selecting the appropriate insertion positions. The experimental results show the proposed model can select the appropriate insertion position with a high accuracy. It reduces the decoding time and improves the translation quality owing to reduced search space",
    "volume": "main",
    "checked": true,
    "id": "8e0dccbba2aa4e58b79b419a596775a6fba86a26",
    "citation_count": 1
  },
  "https://aclanthology.org/D16-1248": {
    "title": "Why Neural Translations are the Right Length",
    "abstract": "We investigate how neural, encoder-decoder translation systems output target strings of appropriate lengths, finding that a collection of hidden units learns to explicitly implement this functionality",
    "volume": "main",
    "checked": true,
    "id": "2759976e7d8a27c788fb52a24830fc63ce0de570",
    "citation_count": 64
  },
  "https://aclanthology.org/D16-1249": {
    "title": "Supervised Attentions for Neural Machine Translation",
    "abstract": "In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the \"true\" alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system",
    "volume": "main",
    "checked": true,
    "id": "d4a887499773ff32aae898711e595654f3f65199",
    "citation_count": 123
  },
  "https://aclanthology.org/D16-1250": {
    "title": "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance",
    "abstract": "Mapping word embeddings of different languages into a single space has multiple applications. In order to map from a source space into a target space, a common approach is to learn a linear mapping that minimizes the distances between equivalences listed in a bilingual dictionary. In this paper, we propose a framework that generalizes previous work, provides an efficient exact method to learn the optimal linear transformation and yields the best bilingual results in translation induction while preserving monolingual performance in an analogy task",
    "volume": "main",
    "checked": true,
    "id": "9a2eed5f8175275af0d55d4aed39afc8e2b2acf2",
    "citation_count": 333
  },
  "https://aclanthology.org/D16-1251": {
    "title": "Measuring the behavioral impact of machine translation quality improvements with A/B testing",
    "abstract": "In this paper we discuss a process for quantifying the behavioral impact of a domaincustomized machine translation system deployed on a large-scale e-commerce platform. We discuss several machine translation systems that we trained using aligned text from product listing descriptions written in multiple languages. We document the quality improvements of these systems as measured through automated quality measures and crowdsourced human quality assessments. We then measure the effect of these quality improvements on user behavior using an automated A/B testing framework. Through testing we observed an increase in key ecommerce metrics, including a significant increase in purchases",
    "volume": "main",
    "checked": true,
    "id": "2a86deac719aa2ec638ff96ab272f1e0a4e121ff",
    "citation_count": 1
  },
  "https://aclanthology.org/D16-1252": {
    "title": "Creating a Large Benchmark for Open Information Extraction",
    "abstract": "Open information extraction (Open IE) was presented as an unrestricted variant of traditional information extraction. It has been gaining substantial attention, manifested by a large number of automatic Open IE extractors and downstream applications. In spite of this broad attention, the Open IE task definition has been lacking – there are no formal guidelines and no large scale gold standard annotation. Subsequently, the various implementations of Open IE resorted to small scale posthoc evaluations, inhibiting an objective and reproducible cross-system comparison. In this work, we develop a methodology that leverages the recent QA-SRL annotation to create a first independent and large scale Open IE annotation,1 and use it to automatically compare the most prominent Open IE systems",
    "volume": "main",
    "checked": true,
    "id": "a32d7aba28ce9f130934b8e892df5bf2cad97e21",
    "citation_count": 110
  },
  "https://aclanthology.org/D16-1253": {
    "title": "Bilingually-constrained Synthetic Data for Implicit Discourse Relation Recognition",
    "abstract": "To alleviate the shortage of labeled data, we propose to use bilingually-constrained synthetic implicit data for implicit discourse relation recognition. These data are extracted from a bilingual sentence-aligned corpus according to the implicit/explicit mismatch between different languages. Incorporating these data via a multi-task neural network model achieves significant improvements over baselines, on both the English PDTB and Chinese CDTB data sets",
    "volume": "main",
    "checked": true,
    "id": "c9df8ce459f27059406d05fb05eb8e2e1a57aa55",
    "citation_count": 22
  },
  "https://aclanthology.org/D16-1254": {
    "title": "Transition-Based Dependency Parsing with Heuristic Backtracking",
    "abstract": "We introduce a novel approach to the decoding problem in transition-based parsing: heuristic backtracking. This algorithm uses a series of partial parses on the sentence to locate the best candidate parse, using confidence estimates of transition decisions as a heuristic to guide the starting points of the search. This allows us to achieve a parse accuracy comparable to beam search, despite using fewer transitions. When used to augment a Stack-LSTM transition-based parser, the parser shows an unlabeled attachment score of up to 93.30% for English and 87.61% for Chinese",
    "volume": "main",
    "checked": true,
    "id": "6a932c699e8cc8fb8ef66642b3db701a5fa04c8b",
    "citation_count": 8
  },
  "https://aclanthology.org/D16-1255": {
    "title": "Word Ordering Without Syntax",
    "abstract": "Recent work on word ordering has argued that syntactic structure is important, or even required, for effectively recovering the order of a sentence. We find that, in fact, an n-gram language model with a simple heuristic gives strong results on this task. Furthermore, we show that a long short-term memory (LSTM) language model is even more effective at recovering order, with our basic model outperforming a state-of-the-art syntactic model by 11.5 BLEU points. Additional data and larger beams yield further gains, at the expense of training and search time",
    "volume": "main",
    "checked": true,
    "id": "7b5af1758963babf3740a39615b469b70513a413",
    "citation_count": 37
  },
  "https://aclanthology.org/D16-1256": {
    "title": "Morphological Segmentation Inside-Out",
    "abstract": "Morphological segmentation has traditionally been modeled with non-hierarchical models, which yield flat segmentations as output. In many cases, however, proper morphological analysis requires hierarchical structure -- especially in the case of derivational morphology. In this work, we introduce a discriminative, joint model of morphological segmentation along with the orthographic changes that occur during word formation. To the best of our knowledge, this is the first attempt to approach discriminative segmentation with a context-free model. Additionally, we release an annotated treebank of 7454 English words with constituency parses, encouraging future research in this area",
    "volume": "main",
    "checked": true,
    "id": "8477cc89bd089b79169fc58d7720ba9f1d49bd2a",
    "citation_count": 10
  },
  "https://aclanthology.org/D16-1257": {
    "title": "Parsing as Language Modeling",
    "abstract": "We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing — 93.8 F1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%",
    "volume": "main",
    "checked": true,
    "id": "39f1b108687f643015f96a0c800585a44621f99c",
    "citation_count": 101
  },
  "https://aclanthology.org/D16-1258": {
    "title": "Human-in-the-Loop Parsing",
    "abstract": "This paper demonstrates that it is possible for a parser to improve its performance with a human in the loop, by posing simple questions to non-experts. For example, given the first sentence of this abstract, if the parser is uncertain about the subject of the verb \"pose,\" it could generate the question What would pose something? with candidate answers this paper and a parser. Any fluent speaker can answer this question, and the correct answer resolves the original uncertainty. We apply the approach to a CCG parser, converting uncertain attachment decisions into natural language questions about the arguments of verbs. Experiments show that crowd workers can answer these questions quickly, accurately and cheaply. Our human-in-the-loop parser improves on the state of the art with less than 2 questions per sentence on average, with a gain of 1.7 F1 on the 10% of sentences whose parses are changed",
    "volume": "main",
    "checked": true,
    "id": "fda558136b2d2b812a608b21fe22959d48db1078",
    "citation_count": 28
  },
  "https://aclanthology.org/D16-1259": {
    "title": "Unsupervised Timeline Generation for Wikipedia History Articles",
    "abstract": "This paper presents a generic approach to content selection for creating timelines from individual history articles for which no external information about the same topic is available. This scenario is in contrast to existing works on timeline generation, which require the presence of a large corpus of news articles. To identify salient events in a given history article, we exploit lexical cues about the article's subject area, as well as time expressions that are syntactically attached to an event word. We also test different methods of ensuring timeline coverage of the entire historical time span described. Our best-performing method outperforms a new unsupervised baseline and an improved version of an existing supervised approach. We see our work as a step towards more semantically motivated approaches to single-document summarisation",
    "volume": "main",
    "checked": true,
    "id": "04ab5ed3635a75bfd6b0c95598dbf38a1d4f63e8",
    "citation_count": 3
  },
  "https://aclanthology.org/D16-1260": {
    "title": "Encoding Temporal Information for Time-Aware Link Prediction",
    "abstract": "Most existing knowledge base (KB) embedding methods solely learn from time-unknown fact triples but neglect the temporal information in the knowledge base. In this paper, we propose a novel time-aware KB embedding approach taking advantage of the happening time of facts. Specifically, we use temporal order constraints to model transformation between time-sensitive relations and enforce the embeddings to be temporally consistent and more accurate. We empirically evaluate our approach in two tasks of link prediction and triple classification. Experimental results show that our method outperforms other baselines on the two tasks consistently",
    "volume": "main",
    "checked": true,
    "id": "fd458e7109e8ebac2f59d399981054957078c7a4",
    "citation_count": 66
  },
  "https://aclanthology.org/D16-1261": {
    "title": "Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning",
    "abstract": "Most successful information extraction systems operate with access to a large collection of documents. In this work, we explore the task of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce. This process entails issuing search queries, extraction from new sources and reconciliation of extracted values, which are repeated until sufficient evidence is collected. We approach the problem using a reinforcement learning framework where our model learns to select optimal actions based on contextual information. We employ a deep Q-network, trained to optimize a reward function that reflects extraction accuracy while penalizing extra effort. Our experiments on two databases -- of shooting incidents, and food adulteration cases -- demonstrate that our system significantly outperforms traditional extractors and a competitive meta-classifier baseline",
    "volume": "main",
    "checked": true,
    "id": "ac50801574f7c97f1cf8f6718a9ff2b3e18c2dc6",
    "citation_count": 133
  },
  "https://aclanthology.org/D16-1262": {
    "title": "Global Neural CCG Parsing with Optimality Guarantees",
    "abstract": "We introduce the first global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a new objective that encourages the parser to explore a tiny fraction of the search space. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees",
    "volume": "main",
    "checked": true,
    "id": "8602398403281dae0694b4e0488eb501d6db49ef",
    "citation_count": 36
  },
  "https://aclanthology.org/D16-1263": {
    "title": "Learning a Lexicon and Translation Model from Phoneme Lattices",
    "abstract": "Language documentation begins by gathering speech. Manual or automatic transcription at the word level is typically not possible because of the absence of an orthography or prior lexicon, and though manual phonemic transcription is possible, it is prohibitively slow. On the other hand, translations of the minority language into a major language are more easily acquired. We propose a method to harness such translations to improve automatic phoneme recognition. The method assumes no prior lexicon or translation model, instead learning them from phoneme lattices and translations of the speech being transcribed. Experiments demonstrate phoneme error rate improvements against two baselines and the model's ability to learn useful bilingual lexical entries",
    "volume": "main",
    "checked": true,
    "id": "91e605a125f64207a242693d0dc1c862080f6c27",
    "citation_count": 14
  },
  "https://aclanthology.org/D16-1264": {
    "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
    "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.   The dataset is freely available at this https URL",
    "volume": "main",
    "checked": true,
    "id": "05dd7254b632376973f3a1b4d39485da17814df5",
    "citation_count": 5068
  },
  "https://aclanthology.org/D16-2001": {
    "title": "Practical Neural Networks for NLP: From Theory to Code",
    "abstract": "This tutorial aims to bring NLP researchers up to speed with the current techniques in deep learning and neural networks, and show them how they can turn their ideas into practical implementations. We will start with simple classification models (logistic regression and multilayer perceptrons) and cover more advanced patterns that come up in NLP such as recurrent networks for sequence tagging and prediction problems, structured networks (e.g., compositional architectures based on syntax trees), structured output spaces (sequences and trees), attention for sequence-to-sequence transduction, and feature induction for complex algorithm states. A particular emphasis will be on learning to represent complex objects as recursive compositions of simpler objects. This representation will reflect characterize standard objects in NLP, such as the composition of characters and morphemes into words, and words into sentences and documents. In addition, new opportunities such as learning to embed \"algorithm states\" such as those used in transition-based parsing and other sequential structured prediction models (for which effective features may be difficult to engineer by hand) will be covered.Everything in the tutorial will be grounded in code — we will show how to program seemingly complex neural-net models using toolkits based on the computation-graph formalism. Computation graphs decompose complex computations into a DAG, with nodes representing inputs, target outputs, parameters, or (sub)differentiable functions (e.g., \"tanh\", \"matrix multiply\", and \"softmax\"), and edges represent data dependencies. These graphs can be run \"forward\" to make predictions and compute errors (e.g., log loss, squared error) and then \"backward\" to compute derivatives with respect to model parameters. In particular we'll cover the Python bindings of the CNN library. CNN has been designed from the ground up for NLP applications, dynamically structured NNs, rapid prototyping, and a transparent data and execution model",
    "volume": "tutorial",
    "checked": true,
    "id": "fb38451ff87254ac1ff15e79154ef958b4efb6a6",
    "citation_count": 0
  },
  "https://aclanthology.org/D16-2002": {
    "title": "Advanced Markov Logic Techniques for Scalable Joint Inference in NLP",
    "abstract": "In the early days of the statistical NLP era, many language processing tasks were tackled using the so-called pipeline architecture: the given task is broken into a series of sub-tasks such that the output of one sub-task is an input to the next sub-task in the sequence. The pipeline architecture is appealing for various reasons, including modularity, modeling convenience, and manageable computational complexity. However, it suffers from the error propagation problem: errors made in one sub-task are propagated to the next sub-task in the sequence, leading to poor accuracy on that sub-task, which in turn leads to more errors downstream. Another disadvantage associated with it is lack of feedback: errors made in a sub-task are often not corrected using knowledge uncovered while solving another sub-task down the pipeline.Realizing these weaknesses, researchers have turned to joint inference approaches in recent years. One such approach involves the use of Markov logic, which is defined as a set of weighted first-order logic formulas and, at a high level, unifies first-order logic with probabilistic graphical models. It is an ideal modeling language (knowledge representation) for compactly representing relational and uncertain knowledge in NLP. In a typical use case of MLNs in NLP, the application designer describes the background knowledge using a few first-order logic sentences and then uses software packages such as Alchemy, Tuffy, and Markov the beast to perform learning and inference (prediction) over the MLN. However, despite its obvious advantages, over the years, researchers and practitioners have found it difficult to use MLNs effectively in many NLP applications. The main reason for this is that it is hard to scale inference and learning algorithms for MLNs to large datasets and complex models, that are typical in NLP.In this tutorial, we will introduce the audience to recent advances in scaling up inference and learning in MLNs as well as new approaches to make MLNs a \"black-box\" for NLP applications (with only minor tuning required on the part of the user). Specifically, we will introduce attendees to a key idea that has emerged in the MLN research community over the last few years, lifted inference , which refers to inference techniques that take advantage of symmetries (e.g., synonyms), both exact and approximate, in the MLN . We will describe how these next-generation inference techniques can be used to perform effective joint inference. We will also present our new software package for inference and learning in MLNs, Alchemy 2.0, which is based on lifted inference, focusing primarily on how it can be used to scale up inference and learning in large models and datasets for applications such as semantic similarity determination, information extraction and question answering",
    "volume": "tutorial",
    "checked": true,
    "id": "81076e08ee834b3854eb338210ab0c27c549b3b4",
    "citation_count": 0
  },
  "https://aclanthology.org/D16-2003": {
    "title": "Lifelong Machine Learning for Natural Language Processing",
    "abstract": "Machine learning (ML) has been successfully used as a prevalent approach to solving numerous NLP problems. However, the classic ML paradigm learns in isolation. That is, given a dataset, an ML algorithm is executed on the dataset to produce a model without using any related or prior knowledge. Although this type of isolated learning is very useful, it also has serious limitations as it does not accumulate knowledge learned in the past and use the knowledge to help future learning, which is the hallmark of human learning and human intelligence. Lifelong machine learning (LML) aims to achieve this capability. Specifically, it aims to design and develop computational learning systems and algorithms that learn as humans do, i.e., retaining the results learned in the past, abstracting knowledge from them, and using the knowledge to help future learning. In this tutorial, we will introduce the existing research of LML and to show that LML is very suitable for NLP tasks and has potential to help NLP make major progresses",
    "volume": "tutorial",
    "checked": true,
    "id": "3a54b512a9ecb873480d8bb93caf46494f52bfc2",
    "citation_count": 2
  },
  "https://aclanthology.org/D16-2004": {
    "title": "Neural Networks for Sentiment Analysis",
    "abstract": "Sentiment analysis has been a major research topic in natural language processing (NLP). Traditionally, the problem has been attacked using discrete models and manually-defined sparse features. Over the past few years, neural network models have received increased research efforts in most sub areas of sentiment analysis, giving highly promising results. A main reason is the capability of neural models to automatically learn dense features that capture subtle semantic information over words, sentences and documents, which are difficult to model using traditional discrete features based on words and ngram patterns. This tutorial gives an introduction to neural network models for sentiment analysis, discussing the mathematics of word embeddings, sequence models and tree structured models and their use in sentiment analysis on the word, sentence and document levels, and fine-grained sentiment analysis. The tutorial covers a range of neural network models (e.g. CNN, RNN, RecNN, LSTM) and their extensions, which are employed in four main subtasks of sentiment analysis:Sentiment-oriented embeddings;Sentence-level sentiment;Document-level sentiment;Fine-grained sentiment.The content of the tutorial is divided into 3 sections of 1 hour each. We assume that the audience is familiar with linear algebra and basic neural network structures, introduce the mathematical details of the most typical models. First, we will introduce the sentiment analysis task, basic concepts related to neural network models for sentiment analysis, and show detail approaches to integrate sentiment information into embeddings. Sentence-level models will be described in the second section. Finally, we will discuss neural network models use for document-level and fine-grained sentiment",
    "volume": "tutorial",
    "checked": true,
    "id": "b6ec36454d40ad9ce7e9b9c5804a2824c738a676",
    "citation_count": 0
  },
  "https://aclanthology.org/D16-2005": {
    "title": "Continuous Vector Spaces for Cross-language NLP Applications",
    "abstract": "The mathematical metaphor offered by the geometric concept of distance in vector spaces with respect to semantics and meaning has been proven to be useful in many monolingual natural language processing applications. There is also some recent and strong evidence that this paradigm can also be useful in the cross-language setting. In this tutorial, we present and discuss some of the most recent advances on exploiting the vector space model paradigm in specific cross-language natural language processing applications, along with a comprehensive review of the theoretical background behind them.First, the tutorial introduces some fundamental concepts of distributional semantics and vector space models. More specifically, the concepts of distributional hypothesis and term-document matrices are revised, followed by a brief discussion on linear and non-linear dimensionality reduction techniques and their implications to the parallel distributed approach to semantic cognition. Next, some classical examples of using vector space models in monolingual natural language processing applications are presented. Specific examples in the areas of information retrieval, related term identification and semantic compositionality are described.Then, the tutorial focuses its attention on the use of the vector space model paradigm in cross-language applications. To this end, some recent examples are presented and discussed in detail, addressing the specific problems of cross-language information retrieval, cross-language sentence matching, and machine translation. Some of the most recent developments in the area of Neural Machine Translation are also discussed.Finally, the tutorial concludes with a discussion about current and future research problems related to the use of vector space models in cross-language settings. Future avenues for scientific research are described, with major emphasis on the extension from vector and matrix representations to tensors, as well as the problem of encoding word position information into the vector-based representations",
    "volume": "tutorial",
    "checked": true,
    "id": "981c2652e0199cd5fc383be260be89e69060b643",
    "citation_count": 0
  },
  "https://aclanthology.org/D16-2006": {
    "title": "Methods and Theories for Large-scale Structured Prediction",
    "abstract": "Many important NLP tasks are casted as structured prediction problems, and try to predict certain forms of structured output from the input. Examples of structured prediction include POS tagging, named entity recognition, PCFG parsing, dependency parsing, machine translation, and many others. When apply structured prediction to a specific NLP task, there are the following challenges:1. Model selection: Among various models/algorithms with different characteristics, which one should we choose for a specific NLP task?2. Training: How to train the model parameters effectively and efficiently?3. Overfitting: To achieve good accuracy on test data, it is important to control the overfitting from the training data. How to control the overfitting risk for structured prediction?This tutorial will provide a clear overview of recent advances in structured prediction methods and theories, and address the above issues when we apply structured prediction to NLP tasks. We will introduce large margin methods (e.g., perceptrons, MIRA), graphical models (e.g., CRFs), and deep learning methods (e.g., RNN, LSTM), and show the respective advantages and disadvantages for NLP applications. For the training algorithms, we will introduce online/ stochastic training methods, and we will introduce parallel online/stochastic learning algorithms and theories to speed up the training (e.g., the Hogwild algorithm). For controlling the overfitting from training data, we will introduce the weight regularization methods, structure regularization, and implicit regularization methods",
    "volume": "tutorial",
    "checked": true,
    "id": "578b8c06530c6083e8b6f8ede6420c6c0e2f424b",
    "citation_count": 0
  }
}