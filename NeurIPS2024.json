{
  "https://papers.nips.cc/paper_files/paper/2024/hash/000f947dcaff8fbffcc3f53a1314f358-Abstract-Conference.html": {
    "title": "MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and Provable Convergence",
    "volume": "main",
    "abstract": "We propose a new variant of the Adam optimizer called MicroAdam that specifically minimizes memory overheads, while maintaining theoretical convergence guarantees. We achieve this by compressing the gradient information before it is fed into the optimizer state, thereby reducing its memory footprint significantly. We control the resulting compression error via a novel instance of the classical error feedback mechanism from distributed optimization in which the error correction information is itself compressed to allow for practical memory gains. We prove that the resulting approach maintains theoretical convergence guarantees competitive to those of AMSGrad, while providing good practical performance. Specifically, we show that MicroAdam can be implemented efficiently on GPUs: on both million-scale (BERT) and billion-scale (LLaMA) models, MicroAdam provides practical convergence competitive to that of the uncompressed Adam baseline, with lower memory usage and similar running time. Our code is available at https://github.com/IST-DASLab/MicroAdam",
    "checked": true,
    "id": "db3e88973dc0a1c137de739e1f24160f96ff1b47",
    "semantic_title": "microadam: accurate adaptive optimization with low space overhead and provable convergence",
    "citation_count": 17,
    "authors": [
      "Ionut-Vlad Modoranu",
      "Mher Safaryan",
      "Grigory Malinovsky",
      "Eldar Kurtić",
      "Thomas Robert",
      "Peter Richtarik",
      "Dan Alistarh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/00295cede6e1600d344b5cd6d9fd4640-Abstract-Conference.html": {
    "title": "GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are increasingly used for various tasks with graph structures. Though LLMs can process graph information in a textual format, they overlook the rich vision modality, which is an intuitive way for humans to comprehend structural information and conduct general graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., $\\textit{visual graph}$) are still unexplored. To fill the gap, we innovatively propose an end-to-end framework, called $\\textbf{G}$raph to v$\\textbf{I}$sual and $\\textbf{T}$extual Integr$\\textbf{A}$tion (GITA), which firstly incorporates visual graphs into general graph reasoning. Besides, we establish $\\textbf{G}$raph-based $\\textbf{V}$ision-$\\textbf{L}$anguage $\\textbf{Q}$uestion $\\textbf{A}$nswering (GVLQA) dataset from existing graph data, which is the first vision-language dataset for general graph reasoning purposes. Extensive experiments on the GVLQA dataset and five real-world datasets show that GITA outperforms mainstream LLMs in terms of general graph reasoning capabilities. Moreover, We highlight the effectiveness of the layout augmentation on visual graphs and pretraining on the GVLQA dataset",
    "checked": true,
    "id": "b2561fe791760431e2e8c4263a1e58fb594e6288",
    "semantic_title": "gita: graph to visual and textual integration for vision-language graph reasoning",
    "citation_count": 17,
    "authors": [
      "Yanbin Wei",
      "Shuai Fu",
      "Weisen Jiang",
      "Zejian Zhang",
      "Zhixiong Zeng",
      "Qi Wu",
      "James T. Kwok",
      "Yu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/00532321a253959cedc4f971b5524131-Abstract-Conference.html": {
    "title": "How does PDE order affect the convergence of PINNs?",
    "volume": "main",
    "abstract": "This paper analyzes the inverse relationship between the order of partial differential equations (PDEs) and the convergence of gradient descent in physics-informed neural networks (PINNs) with the power of ReLU activation. The integration of the PDE into a loss function endows PINNs with a distinctive feature to require computing derivatives of model up to the PDE order. Although it has been empirically observed that PINNs encounter difficulties in convergence when dealing with high-order or high-dimensional PDEs, a comprehensive theoretical understanding of this issue remains elusive. This paper offers theoretical support for this pathological behavior by demonstrating that the gradient flow converges in a lower probability when the PDE order is higher. In addition, we show that PINNs struggle to address high-dimensional problems because the influence of dimensionality on convergence is exacerbated with increasing PDE order. To address the pathology, we use the insights garnered to consider variable splitting that decomposes the high-order PDE into a system of lower-order PDEs. We prove that by reducing the differential order, the gradient flow of variable splitting is more likely to converge to the global optimum. Furthermore, we present numerical experiments in support of our theoretical claims",
    "checked": true,
    "id": "dfb67a59df15f52be34e1c80c94b9b69e731a4c3",
    "semantic_title": "how does pde order affect the convergence of pinns?",
    "citation_count": 1,
    "authors": [
      "Chang hoon Song",
      "Yesom Park",
      "Myungjoo Kang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/005413e90d003d13886019607b037f52-Abstract-Conference.html": {
    "title": "Fair Wasserstein Coresets",
    "volume": "main",
    "abstract": "Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. While current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples, their impact on downstream learning processes has yet to be explored. In this work, we present fair Wasserstein coresets ($\\texttt{FWC}$), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. $\\texttt{FWC}$ uses an efficient majority minimization algorithm to minimize the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of $\\texttt{FWC}$ is equivalent to Lloyd's algorithm for k-medians and k-means clustering. Experiments conducted on both synthetic and real datasets show that $\\texttt{FWC}$: (i) achieves a competitive fairness-performance tradeoff in downstream models compared to existing approaches, (ii) improves downstream fairness when added to the existing training data and (iii) can be used to reduce biases in predictions from large language models (GPT-3.5 and GPT-4)",
    "checked": true,
    "id": "01b82619ecdd591d8ec66d6f90e55c4de3bd13e2",
    "semantic_title": "fair wasserstein coresets",
    "citation_count": 2,
    "authors": [
      "Zikai Xiong",
      "Niccolo Dalmasso",
      "Shubham Sharma",
      "Freddy Lecue",
      "Daniele Magazzeni",
      "Vamsi Potluru",
      "Tucker Balch",
      "Manuela M. Veloso"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/00616a2d48f5716f3d6f783491149364-Abstract-Conference.html": {
    "title": "Improved Regret for Bandit Convex Optimization with Delayed Feedback",
    "volume": "main",
    "abstract": "We investigate bandit convex optimization (BCO) with delayed feedback, where only the loss value of the action is revealed under an arbitrary delay. Let $n,T,\\bar{d}$ denote the dimensionality, time horizon, and average delay, respectively. Previous studies have achieved an $O(\\sqrt{n}T^{3/4}+(n\\bar{d})^{1/3}T^{2/3})$ regret bound for this problem, whose delay-independent part matches the regret of the classical non-delayed bandit gradient descent algorithm. However, there is a large gap between its delay-dependent part, i.e., $O((n\\bar{d})^{1/3}T^{2/3})$, and an existing $\\Omega(\\sqrt{\\bar{d}T})$ lower bound. In this paper, we illustrate that this gap can be filled in the worst case, where $\\bar{d}$ is very close to the maximum delay $d$. Specifically, we first develop a novel algorithm, and prove that it enjoys a regret bound of $O(\\sqrt{n}T^{3/4}+\\sqrt{dT})$ in general. Compared with the previous result, our regret bound is better for $d=O((n\\bar{d})^{2/3}T^{1/3})$, and the delay-dependent part is tight in the worst case. The primary idea is to decouple the joint effect of the delays and the bandit feedback on the regret by carefully incorporating the delayed bandit feedback with a blocking update mechanism. Furthermore, we show that the proposed algorithm can improve the regret bound to $O((nT)^{2/3}\\log^{1/3}T+d\\log T)$ for strongly convex functions. Finally, if the action sets are unconstrained, we demonstrate that it can be simply extended to achieve an $O(n\\sqrt{T\\log T}+d\\log T)$ regret bound for strongly convex and smooth functions",
    "checked": true,
    "id": "18747113ceb1154010e676b0fa86bab1b666fd9b",
    "semantic_title": "improved regret for bandit convex optimization with delayed feedback",
    "citation_count": 5,
    "authors": [
      "Yuanyu Wan",
      "Chang Yao",
      "Mingli Song",
      "Lijun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/008a16ead32f932b711788c276890456-Abstract-Conference.html": {
    "title": "Enhancing Chess Reinforcement Learning with Graph Representation",
    "volume": "main",
    "abstract": "Mastering games is a hard task, as games can be extremely complex, and still fundamentally different in structure from one another. While the AlphaZero algorithm has demonstrated an impressive ability to learn the rules and strategy of a large variety of games, ranging from Go and Chess, to Atari games, its reliance on extensive computational resources and rigid Convolutional Neural Network (CNN) architecture limits its adaptability and scalability. A model trained to play on a $19\\times 19$ Go board cannot be used to play on a smaller $13\\times 13$ board, despite the similarity between the two Go variants.In this paper, we focus on Chess, and explore using a more generic Graph-based Representation of a game state, rather than a grid-based one, to introduce a more general architecture based on Graph Neural Networks (GNN). We also expand the classical Graph Attention Network (GAT) layer to incorporate edge-features, to naturally provide a generic policy output format.Our experiments, performed on smaller networks than the initial AlphaZero paper, show that this new architecture outperforms previous architectures with a similar number of parameters, being able to increase playing strength an order of magnitude faster. We also show that the model, when trained on a smaller $5\\times 5$ variant of chess, is able to be quickly fine-tuned to play on regular $8\\times 8$ chess, suggesting that this approach yields promising generalization abilities.Our code is available at https://github.com/akulen/AlphaGateau",
    "checked": true,
    "id": "b22ed23ba984556f428185a3d231b3b99a44b415",
    "semantic_title": "enhancing chess reinforcement learning with graph representation",
    "citation_count": 1,
    "authors": [
      "Tomas Rigaux",
      "Hisashi Kashima"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/009729d26288b9a8826023692a876107-Abstract-Conference.html": {
    "title": "Mixtures of Experts for Audio-Visual Learning",
    "volume": "main",
    "abstract": "With the rapid development of multimedia technology, audio-visual learning has emerged as a promising research topic within the field of multimodal analysis. In this paper, we explore parameter-efficient transfer learning for audio-visual learning and propose the Audio-Visual Mixture of Experts (\\ourmethodname) to inject adapters into pre-trained models flexibly. Specifically, we introduce unimodal and cross-modal adapters as multiple experts to specialize in intra-modal and inter-modal information, respectively, and employ a lightweight router to dynamically allocate the weights of each expert according to the specific demands of each task. Extensive experiments demonstrate that our proposed approach \\ourmethodname achieves superior performance across multiple audio-visual tasks, including AVE, AVVP, AVS, and AVQA. Furthermore, visual-only experimental results also indicate that our approach can tackle challenging scenes where modality information is missing.The source code is available at \\url{https://github.com/yingchengy/AVMOE}",
    "checked": true,
    "id": "f8834915914efa31bb9d56e3f8bf7219340f35e9",
    "semantic_title": "mixtures of experts for audio-visual learning",
    "citation_count": 8,
    "authors": [
      "Ying Cheng",
      "Yang Li",
      "Junjie He",
      "Rui Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0098a92f5f4e2d96c6db471e0c5507a8-Abstract-Conference.html": {
    "title": "Learning Place Cell Representations and Context-Dependent Remapping",
    "volume": "main",
    "abstract": "Hippocampal place cells are known for their spatially selective firing patterns, which has led to the suggestion that they encode an animal's location. However, place cells also respond to contextual cues, such as smell. Furthermore, they have the ability to remap, wherein the firing fields and rates of cells change in response to changes in the environment. How place cell responses emerge, and how these representations remap is not fully understood. In this work, we propose a similarity-based objective function that translates proximity in space, to proximity in representation. We show that a neural network trained to minimize the proposed objective learns place-like representations. We also show that the proposed objective is easily extended to include other sources of information, such as context information, in the same way. When trained to encode multiple contexts, networks learn distinct representations, exhibiting remapping behaviors between contexts. The proposed objective is invariant to orthogonal transformations. Such transformations of the original trained representation (e.g. rotations), therefore yield new representations distinct from the original, without explicit relearning, akin to remapping. Our findings shed new light on the formation and encoding properties of place cells, and also demonstrate an interesting case of representational reuse",
    "checked": true,
    "id": "c23309c05b4e1cf3e7620e9df42985345ef2913a",
    "semantic_title": "learning place cell representations and context-dependent remapping",
    "citation_count": 2,
    "authors": [
      "Markus Pettersen",
      "Frederik Rogge",
      "Mikkel Lepperød"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/00a0db6d26a58c996ff915bd765eb969-Abstract-Conference.html": {
    "title": "Robust Sparse Regression with Non-Isotropic Designs",
    "volume": "main",
    "abstract": "We develop a technique to design efficiently computable estimators for sparse linear regression in the simultaneous presence of two adversaries: oblivious and adaptive.Consider the model $y^*=X^*\\beta^*+ \\eta$ where $X^*$ is an $n\\times d$ random design matrix, $\\beta^*\\in \\mathbb{R}^d$ is a $k$-sparse vector, and the noise $\\eta$ is independent of $X^*$ and chosen by the \\emph{oblivious adversary}. Apart from the independence of $X^*$, we only require a small fraction entries of $\\eta$ to have magnitude at most $1$. The \\emph{adaptive adversary} is allowed to arbitrarily corrupt an $\\varepsilon$-fraction of the samples $(X_1^*, y_1^*),\\ldots, (X_n^*, y_n^*)$.Given the $\\varepsilon$-corrupted samples $(X_1, y_1),\\ldots, (X_n, y_n)$, the goal is to estimate $\\beta^*$. We assume that the rows of $X^*$ are iid samples from some $d$-dimensional distribution $\\mathcal{D}$ with zero mean and (unknown) covariance matrix $\\Sigma$ with bounded condition number.We design several robust algorithms that outperform the state of the art even in the special case of Gaussian noise $\\eta \\sim N(0,1)^n$. In particular, we provide a polynomial-time algorithm that with high probability recovers $\\beta^*$ up to error $O(\\sqrt{\\varepsilon})$ as long as $n \\ge \\tilde{O}(k^2/\\varepsilon)$, only assuming some bounds on the third and the fourth moments of $\\mathcal{D}$. In addition, prior to this work, even in the special case of Gaussian design $\\mathcal{D} = N(0,\\Sigma)$ and noise $\\eta \\sim N(0,1)$, no polynomial time algorithm was known to achieve error $o(\\sqrt{\\varepsilon})$ in the sparse setting $n < d^2$. We show that under some assumptions on the fourth and the eighth moments of $\\mathcal{D}$, there is a polynomial-time algorithm that achieves error $o(\\sqrt{\\varepsilon})$ as long as $n \\ge \\tilde{O}(k^4 / \\varepsilon^3)$. For Gaussian distribution $\\mathcal{D} = N(0,\\Sigma)$, this algorithm achieves error $O(\\varepsilon^{3/4})$. Moreover, our algorithm achieves error $o(\\sqrt{\\varepsilon})$ for all log-concave distributions if $\\varepsilon \\le 1/\\text{polylog(d)}$. Our algorithms are based on the filtering of the covariates that uses sum-of-squares relaxations, and weighted Huber loss minimization with $\\ell_1$ regularizer. We provide a novel analysis of weighted penalized Huber loss that is suitable for heavy-tailed designs in the presence of two adversaries. Furthermore, we complement our algorithmic results with Statistical Query lower bounds, providing evidence that our estimators are likely to have nearly optimal sample complexity",
    "checked": true,
    "id": "0418db47ff95e6e708047c16fae073b68eb76062",
    "semantic_title": "robust sparse regression with non-isotropic designs",
    "citation_count": 2,
    "authors": [
      "Chih-Hung Liu",
      "Gleb Novikov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/00d1f03b87a401b1c7957e0cc785d0bc-Abstract-Conference.html": {
    "title": "Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy",
    "volume": "main",
    "abstract": "Diffusion models have recently achieved great success in the synthesis of high-quality images and videos. However, the existing denoising techniques in diffusion models are commonly based on step-by-step noise predictions, which suffers from high computation cost, resulting in a prohibitive latency for interactive applications. In this paper, we propose AdaptiveDiffusion to relieve this bottleneck by adaptively reducing the noise prediction steps during the denoising process. Our method considers the potential of skipping as many noise prediction steps as possible while keeping the final denoised results identical to the original full-step ones. Specifically, the skipping strategy is guided by the third-order latent difference that indicates the stability between timesteps during the denoising process, which benefits the reusing of previous noise prediction results. Extensive experiments on image and video diffusion models demonstrate that our method can significantly speed up the denoising process while generating identical results to the original process, achieving up to an average 2-5x speedup without quality degradation. The code is available at https://github.com/UniModal4Reasoning/AdaptiveDiffusion",
    "checked": true,
    "id": "46c2adf01ceadca12cf5e9ce0312b881414a48a8",
    "semantic_title": "training-free adaptive diffusion with bounded difference approximation strategy",
    "citation_count": 5,
    "authors": [
      "Hancheng Ye",
      "Jiakang Yuan",
      "Renqiu Xia",
      "Xiangchao Yan",
      "Tao Chen",
      "Junchi Yan",
      "Botian Shi",
      "Bo Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/00d80722b756de0166523a87805dd00f-Abstract-Conference.html": {
    "title": "Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs",
    "volume": "main",
    "abstract": "The recent development of chain-of-thought (CoT) decoding has enabled large language models (LLMs) to generate explicit logical reasoning paths for complex problem-solving. However, research indicates that these paths are not always deliberate and optimal. The tree-of-thought (ToT) method employs tree-searching to extensively explore the reasoning space and find better reasoning paths that CoT decoding might overlook. This deliberation, however, comes at the cost of significantly increased inference complexity. In this work, we demonstrate that fine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to achieve similar or better performance, thereby avoiding the substantial inference burden. This is achieved through \\emph{Chain of Preference Optimization} (CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths with those of ToT using the inherent preference information in the tree-search process. Extensive experimental results show that CPO significantly improves LLM performance in solving a variety of complex problems, including question answering, fact verification, and arithmetic reasoning, demonstrating its effectiveness. Our code is available at https://github.com/sail-sg/CPO",
    "checked": true,
    "id": "3fb26c0cf930b04635540e4815c4b8ca0581155c",
    "semantic_title": "chain of preference optimization: improving chain-of-thought reasoning in llms",
    "citation_count": 78,
    "authors": [
      "Xuan Zhang",
      "Chao Du",
      "Tianyu Pang",
      "Qian Liu",
      "Wei Gao",
      "Min Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/01025a4e79355bb37a10ba39605944b5-Abstract-Conference.html": {
    "title": "Decompose, Analyze and Rethink: Solving Intricate Problems with Human-like Reasoning Cycle",
    "volume": "main",
    "abstract": "In this paper, we introduce DeAR (Decompose-Analyze-Rethink), a framework that iteratively builds a reasoning tree to tackle intricate problems within a single large language model (LLM). Unlike approaches that extend or search for rationales, DeAR is featured by 1) adopting a tree-based question decomposition manner to plan the organization of rationales, which mimics the logical planning inherentin human cognition; 2) globally updating the rationales at each reasoning step through natural language feedback. Specifically, the Decompose stage decomposes the question into simpler sub-questions, storing them as new nodes; the Analyze stage generates and self-checks rationales for sub-questions at each node evel; and the Rethink stage updates parent-node rationales based on feedback from their child nodes. By generating and updating the reasoning process from a more global perspective, DeAR constructs more adaptive and accurate logical structures for complex problems, facilitating timely error correction compared to rationale-extension and search-based approaches such as Tree-of-Thoughts (ToT) and Graph-of-Thoughts (GoT). We conduct extensive experiments on three reasoning benchmarks, including ScienceQA, StrategyQA, and GSM8K, which cover a variety of reasoning tasks, demonstrating that our approach significantly reduces logical errors and enhances performance across various LLMs. Furthermore, we validate that DeAR is an efficient method that achieves a superior trade-off between accuracy and reasoning time compared to ToT and GoT",
    "checked": true,
    "id": "2a0f45671b0f97683be97b330b9c3e2daec285bc",
    "semantic_title": "decompose, analyze and rethink: solving intricate problems with human-like reasoning cycle",
    "citation_count": 19,
    "authors": [
      "Shangzi Xue",
      "Zhenya Huang",
      "Jiayu Liu",
      "Xin Lin",
      "Yuting Ning",
      "Binbin Jin",
      "Xin Li",
      "Qi Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/010c5ba0cafc743fece8be02e7adb8dd-Abstract-Conference.html": {
    "title": "UQ-Guided Hyperparameter Optimization for Iterative Learners",
    "volume": "main",
    "abstract": "Hyperparameter Optimization (HPO) plays a pivotal role in unleashing the potential of iterative machine learning models. This paper addresses a crucial aspect that has largely been overlooked in HPO: the impact of uncertainty in ML model training. The paper introduces the concept of uncertainty-aware HPO and presents a novel approach called the UQ-guided scheme for quantifying uncertainty. This scheme offers a principled and versatile method to empower HPO techniques in handling model uncertainty during their exploration of the candidate space.By constructing a probabilistic model and implementing probability-driven candidate selection and budget allocation, this approach enhances the quality of the resulting model hyperparameters. It achieves a notable performance improvement of over 50\\% in terms of accuracy regret and exploration time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiesong Liu",
      "Feng Zhang",
      "Jiawei Guan",
      "Xipeng Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/010c855df402b443e0c16e5b7434e74c-Abstract-Conference.html": {
    "title": "Occupancy-based Policy Gradient: Estimation, Convergence, and Optimality",
    "volume": "main",
    "abstract": "Occupancy functions play an instrumental role in reinforcement learning (RL) for guiding exploration, handling distribution shift, and optimizing general objectives beyond the expected return. Yet, computationally efficient policy optimization methods that use (only) occupancy functions are virtually non-existent. In this paper, we establish the theoretical foundations of model-free policy gradient (PG) methods that compute the gradient through the occupancy for both online and offline RL, without modeling value functions. Our algorithms reduce gradient estimation to squared-loss regression and are computationally oracle-efficient. We characterize the sample complexities of both local and global convergence, accounting for both finite-sample estimation error and the roles of exploration (online) and data coverage (offline). Occupancy-based PG naturally handles arbitrary offline data distributions, and, with one-line algorithmic changes, can be adapted to optimize any differentiable objective functional",
    "checked": true,
    "id": "613c324423264b8cc946a774b963fde69a614ac9",
    "semantic_title": "occupancy-based policy gradient: estimation, convergence, and optimality",
    "citation_count": 1,
    "authors": [
      "Audrey Huang",
      "Nan Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0113ef4642264adc2e6924a3cbbdf532-Abstract-Conference.html": {
    "title": "TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables",
    "volume": "main",
    "abstract": "Deep models have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous variables can provide valuable external information for endogenous variables. Thus, unlike well-established multivariate or univariate forecasting paradigms that either treat all the variables equally or ignore exogenous information, this paper focuses on a more practical setting: time series forecasting with exogenous variables. We propose a novel approach, TimeXer, to ingest external information to enhance the forecasting of endogenous variables. With deftly designed embedding layers, TimeXer empowers the canonical Transformer with the ability to reconcile endogenous and exogenous information, where patch-wise self-attention and variate-wise cross-attention are used simultaneously. Moreover, global endogenous tokens are learned to effectively bridge the causal information underlying exogenous series into endogenous temporal patches. Experimentally, TimeXer achieves consistent state-of-the-art performance on twelve real-world forecasting benchmarks and exhibits notable generality and scalability. Code is available at this repository: https://github.com/thuml/TimeXer",
    "checked": true,
    "id": "db5cb5354d8c1ff9cccf2a1c9ef5c218c9323cc1",
    "semantic_title": "timexer: empowering transformers for time series forecasting with exogenous variables",
    "citation_count": 105,
    "authors": [
      "Yuxuan Wang",
      "Haixu Wu",
      "Jiaxiang Dong",
      "Guo Qin",
      "Haoran Zhang",
      "Yong Liu",
      "Yunzhong Qiu",
      "Jianmin Wang",
      "Mingsheng Long"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/013d743db3c684957305d32017f13339-Abstract-Conference.html": {
    "title": "DomainGallery: Few-shot Domain-driven Image Generation by Attribute-centric Finetuning",
    "volume": "main",
    "abstract": "The recent progress in text-to-image models pretrained on large-scale datasets has enabled us to generate various images as long as we provide a text prompt describing what we want. Nevertheless, the availability of these models is still limited when we expect to generate images that fall into a specific domain either hard to describe or just unseen to the models. In this work, we propose DomainGallery, a few-shot domain-driven image generation method which aims at finetuning pretrained Stable Diffusion on few-shot target datasets in an attribute-centric manner. Specifically, DomainGallery features prior attribute erasure, attribute disentanglement, regularization and enhancement. These techniques are tailored to few-shot domain-driven generation in order to solve key issues that previous works have failed to settle. Extensive experiments are given to validate the superior performance of DomainGallery on a variety of domain-driven generation scenarios",
    "checked": true,
    "id": "d53909d7bc575935a879c6600524ec4cf91047b3",
    "semantic_title": "domaingallery: few-shot domain-driven image generation by attribute-centric finetuning",
    "citation_count": 0,
    "authors": [
      "Yuxuan Duan",
      "Yan Hong",
      "Bo Zhang",
      "jun lan",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Jianfu Zhang",
      "Li Niu",
      "Liqing Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/013f9cd52b38e3e53475605d2b8e7c23-Abstract-Conference.html": {
    "title": "Collaborative Cognitive Diagnosis with Disentangled Representation Learning for Learner Modeling",
    "volume": "main",
    "abstract": "Learners sharing similar implicit cognitive states often display comparable observable problem-solving performances. Leveraging collaborative connections among such similar learners proves valuable in comprehending human learning. Motivated by the success of collaborative modeling in various domains, such as recommender systems, we aim to investigate how collaborative signals among learners contribute to the diagnosis of human cognitive states (i.e., knowledge proficiency) in the context of intelligent education.The primary challenges lie in identifying implicit collaborative connections and disentangling the entangled cognitive factors of learners for improved explainability and controllability in learner Cognitive Diagnosis (CD). However, there has been no work on CD capable of simultaneously modeling collaborative and disentangled cognitive states. To address this gap, we present Coral, a $\\underline{Co}$llabo$\\underline{ra}$tive cognitive diagnosis model with disentang$\\underline{l}$ed representation learning. Specifically, Coral first introduces a disentangled state encoder to achieve the initial disentanglement of learners' states.Subsequently, a meticulously designed collaborative representation learning procedure captures collaborative signals. It dynamically constructs a collaborative graph of learners by iteratively searching for optimal neighbors in a context-aware manner. Using the constructed graph, collaborative information is extracted through node representation learning. Finally, a decoding process aligns the initial cognitive states and collaborative states, achieving co-disentanglement with practice performance reconstructions.Extensive experiments demonstrate the superior performance of Coral, showcasing significant improvements over state-of-the-art methods across several real-world datasets.Our code is available at https://github.com/bigdata-ustc/Coral",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weibo Gao",
      "Qi Liu",
      "Linan Yue",
      "Fangzhou Yao",
      "Hao Wang",
      "Yin Gu",
      "zheng zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0142921fad7ef9192bd87229cdafa9d4-Abstract-Conference.html": {
    "title": "AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning",
    "volume": "main",
    "abstract": "Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation. However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability. We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments. AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment. 2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention. To mitigate hallucinations in managing rules, we introduce a case-conditioned prompting strategy for the Builder. Finally, the Formulator agent compiles these rules into a comprehensive manual. The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable. Given only one simple demonstration, AutoManual significantly improves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with GPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at https://github.com/minghchen/automanual",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghao Chen",
      "Yihang Li",
      "Yanting Yang",
      "Shiyu Yu",
      "Binbin Lin",
      "Xiaofei He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0147d967a5db3b8dde08d2a327b24568-Abstract-Conference.html": {
    "title": "Slot-VLM: Object-Event Slots for Video-Language Modeling",
    "volume": "main",
    "abstract": "Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video understanding. A pivotal challenge is the development of an effective method to encapsulate video content into a set of representative tokens to align with LLMs. In this work, we introduce Slot-VLM, a new framework designed to generate semantically decomposed video tokens, in terms of object-wise and event-wise visual representations, to facilitate LLM inference. Particularly, we design an Object-Event Slots module, i.e., OE-Slots, that adaptively aggregates the dense video tokens from the vision encoder to a set of representative slots. In order to take into account both the spatial object details and the varied temporal dynamics, we build OE-Slots with two branches: the Object-Slots branch and the Event-Slots branch. The Object-Slots branch focuses on extracting object-centric slots from features of high spatial resolution but low frame sample rate, emphasizing detailed object information. The Event-Slots branch is engineered to learn event-centric slots from high temporal sample rate but low spatial resolution features. These complementary slots are combined to form the vision context, serving as the input to the LLM for effective video reasoning. Our experimental results demonstrate the effectiveness of our Slot-VLM, which achieves the state-of-the-art performance on video question-answering",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Xu",
      "Cuiling Lan",
      "Wenxuan Xie",
      "Xuejin Chen",
      "Yan Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/014fe398da515cd552fa6e1f33e0565e-Abstract-Conference.html": {
    "title": "VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time",
    "volume": "main",
    "abstract": "We introduce VASA, a framework for generating lifelike talking faces with appealing visual affective skills (VAS) given a single static image and a speech audio clip. Our premiere model, VASA-1, is capable of not only generating lip movements that are exquisitely synchronized with the audio, but also producing a large spectrum of facial nuances and natural head motions that contribute to the perception of authenticity and liveliness. The core innovations include a diffusion-based holistic facial dynamics and head movement generation model that works in a face latent space, and the development of such an expressive and disentangled face latent space using videos.Through extensive experiments including evaluation on a set of new metrics, we show that our method significantly outperforms previous methods along various dimensions comprehensively. Our method delivers high video quality with realistic facial and head dynamics and also supports the online generation of 512$\\times$512 videos at up to 40 FPS with negligible starting latency.It paves the way for real-time engagements with lifelike avatars that emulate human conversational behaviors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sicheng Xu",
      "Guojun Chen",
      "Yu-Xiao Guo",
      "Jiaolong Yang",
      "Chong Li",
      "Zhenyu Zang",
      "Yizhong Zhang",
      "Xin Tong",
      "Baining Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/015a8c69bedcb0a7b2ed2e1678f34399-Abstract-Conference.html": {
    "title": "Community Detection Guarantees using Embeddings Learned by Node2Vec",
    "volume": "main",
    "abstract": "Embedding the nodes of a large network into an Euclidean space is a common objective in modernmachine learning, with a variety of tools available. These embeddings can then be used as features fortasks such as community detection/node clustering or link prediction, where they achieve state of the artperformance. With the exception of spectral clustering methods, there is little theoretical understandingfor commonly used approaches to learning embeddings. In this work we examine the theoreticalproperties of the embeddings learned by node2vec. Our main result shows that the use of k-meansclustering on the embedding vectors produced by node2vec gives weakly consistent community recoveryfor the nodes in (degree corrected) stochastic block models. We also discuss the use of these embeddingsfor node and link prediction tasks. We demonstrate this result empirically for bothreal and simulated networks, and examine how this relatesto other embedding tools for network data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Davison",
      "S. Carlyle Morgan",
      "Owen G. Ward"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0164f863e66a4272b680ecc4e561bf9d-Abstract-Conference.html": {
    "title": "Physics-Informed Regularization for Domain-Agnostic Dynamical System Modeling",
    "volume": "main",
    "abstract": "Learning complex physical dynamics purely from data is challenging due to the intrinsic properties of systems to be satisfied. Incorporating physics-informed priors, such as in Hamiltonian Neural Networks (HNNs), achieves high-precision modeling for energy-conservative systems. However, real-world systems often deviate from strict energy conservation and follow different physical priors. To address this, we present a framework that achieves high-precision modeling for a wide range of dynamical systems from the numerical aspect, by enforcing Time-Reversal Symmetry (TRS) via a novel regularization term. It helps preserve energies for conservative systems while serving as a strong inductive bias for non-conservative, reversible systems. While TRS is a domain-specific physical prior, we present the first theoretical proof that TRS loss can universally improve modeling accuracy by minimizing higher-order Taylor terms in ODE integration, which is numerically beneficial to various systems regardless of their properties, even for irreversible systems. By integrating the TRS loss within neural ordinary differential equation models, the proposed model TREAT demonstrates superior performance on diverse physical systems. It achieves a significant 11.5% MSE improvement in a challenging chaotic triple-pendulum scenario, underscoring TREAT's broad applicability and effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Huang",
      "Wanjia Zhao",
      "Jingdong Gao",
      "Ziniu Hu",
      "Xiao Luo",
      "Yadi Cao",
      "Yuanzhou Chen",
      "Yizhou Sun",
      "Wei Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/01772a8b0420baec00c4d59fe2fbace6-Abstract-Conference.html": {
    "title": "Improving Sparse Decomposition of Language Model Activations with Gated Sparse Autoencoders",
    "volume": "main",
    "abstract": "Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of those activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage -- systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity",
    "checked": true,
    "id": "39b391659bf214e155d77c8090f513d36706ec10",
    "semantic_title": "improving sparse decomposition of language model activations with gated sparse autoencoders",
    "citation_count": 14,
    "authors": [
      "Senthooran Rajamanoharan",
      "Arthur Conmy",
      "Lewis Smith",
      "Tom Lieberum",
      "Vikrant Varma",
      "Janos Kramar",
      "Rohin Shah",
      "Neel Nanda"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/019ef89617d539b15ed610ce8d1b76e1-Abstract-Conference.html": {
    "title": "Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers",
    "volume": "main",
    "abstract": "Modern deep policy gradient methods achieve effective performance on simulated robotic tasks, but they all require large replay buffers or expensive batch updates, or both, making them incompatible for real systems with resource-limited computers. We show that these methods fail catastrophically when limited to small replay buffers or during incremental learning, where updates only use the most recent sample without batch updates or a replay buffer. We propose a novel incremental deep policy gradient method --- Action Value Gradient (AVG) and a set of normalization and scaling techniques to address the challenges of instability in incremental learning. On robotic simulation benchmarks, we show that AVG is the only incremental method that learns effectively, often achieving final performance comparable to batch policy gradient methods. This advancement enabled us to show for the first time effective deep reinforcement learning with real robots using only incremental updates, employing a robotic manipulator and a mobile robot",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gautham Vasan",
      "Mohamed Elsayed",
      "Seyed Alireza Azimi",
      "Jiamin He",
      "Fahim Shahriar",
      "Colin Bellinger",
      "Martha White",
      "Rupam Mahmood"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/01a8d63f9cb6dcbaa3092ccddd2075ac-Abstract-Conference.html": {
    "title": "Towards Understanding How Transformers Learn In-context Through a Representation Learning Lens",
    "volume": "main",
    "abstract": "Pre-trained large language models based on Transformers have demonstrated remarkable in-context learning (ICL) abilities. With just a few demonstration examples, the models can implement new tasks without any parameter updates. However, it is still an open question to understand the mechanism of ICL. In this paper, we attempt to explore the ICL process in Transformers through a lens of representation learning. Initially, leveraging kernel methods, we figure out a dual model for one softmax attention layer. The ICL inference process of the attention layer aligns with the training procedure of its dual model, generating token representation predictions that are equivalent to the dual model's test outputs. We delve into the training process of this dual model from a representation learning standpoint and further derive a generalization error bound related to the quantity of demonstration tokens. Subsequently, we extend our theoretical conclusions to more complicated scenarios, including one Transformer layer and multiple attention layers. Furthermore, drawing inspiration from existing representation learning methods especially contrastive learning, we propose potential modifications for the attention layer. Finally, experiments are designed to support our findings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruifeng Ren",
      "Yong Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/01b3dea1871f7cea1e0e6be1f2f085bc-Abstract-Conference.html": {
    "title": "FINALLY: fast and universal speech enhancement with studio-like quality",
    "volume": "main",
    "abstract": "In this paper, we address the challenge of speech enhancement in real-world recordings, which often contain various forms of distortion, such as background noise, reverberation, and microphone artifacts.We revisit the use of Generative Adversarial Networks (GANs) for speech enhancement and theoretically show that GANs are naturally inclined to seek the point of maximum density within the conditional clean speech distribution, which, as we argue, is essential for speech enhancement task.We study various feature extractors for perceptual loss to facilitate the stability of adversarial training, developing a methodology for probing the structure of the feature space.This leads us to integrate WavLM-based perceptual loss into MS-STFT adversarial training pipeline, creating an effective and stable training procedure for the speech enhancement model.The resulting speech enhancement model, which we refer to as FINALLY, builds upon the HiFi++ architecture, augmented with a WavLM encoder and a novel training pipeline.Empirical results on various datasets confirm our model's ability to produce clear, high-quality speech at 48 kHz, achieving state-of-the-art performance in the field of speech enhancement. Demo page: https://samsunglabs.github.io/FINALLY-page/",
    "checked": true,
    "id": "c1c008b426bda84dfc196cc7115caa7f93e02fe5",
    "semantic_title": "finally: fast and universal speech enhancement with studio-like quality",
    "citation_count": 6,
    "authors": [
      "Nicholas Babaev",
      "Kirill Tamogashev",
      "Azat Saginbaev",
      "Ivan Shchekotov",
      "Hanbin Bae",
      "Hosang Sung",
      "WonJun Lee",
      "Hoon-Young Cho",
      "Pavel Andreev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/01ce1ae7f94d139e4917f9e4425a4f38-Abstract-Conference.html": {
    "title": "Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate",
    "volume": "main",
    "abstract": "Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to imitate. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and, in turn, provide feedback to help the main model capture more generalizable and imitable correlations. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and methodologies like Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to training models on the original dataset. The results suggest the effectiveness and efficiency of LoT in identifying generalizable information at the right scales while discarding spurious data correlations, thus making LoT a valuable addition to current machine learning. Code is available at https://github.com/jincan333/LoT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Can Jin",
      "Tong Che",
      "Hongwu Peng",
      "Yiyuan Li",
      "Dimitris Metaxas",
      "Marco Pavone"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/01ecd39ca49ddecc5729ca996304781b-Abstract-Conference.html": {
    "title": "Prospective Representation Learning for Non-Exemplar Class-Incremental Learning",
    "volume": "main",
    "abstract": "Non-exemplar class-incremental learning (NECIL) is a challenging task that requires recognizing both old and new classes without retaining any old class samples. Current works mainly deal with the conflicts between old and new classes retrospectively as a new task comes in. However, the lack of old task data makes balancing old and new classes difficult. Instead, we propose a Prospective Representation Learning (PRL) approach to prepare the model for handling conflicts in advance. In the base phase, we squeeze the embedding distribution of the current classes to reserve space for forward compatibility with future classes. In the incremental phase, we make the new class features away from the saved prototypes of old classes in a latent space while aligning the current embedding space with the latent space when updating the model. Thereby, the new class features are clustered in the reserved space to minimize the shock of the new classes on the former classes. Our approach can help existing NECIL baselines to balance old and new classes in a plug-and-play manner. Extensive experiments on several benchmarks demonstrate that our approach outperforms the state-of-the-art methods",
    "checked": true,
    "id": "a76589da5a5761194b9b69461a9876fe73a6def5",
    "semantic_title": "prospective representation learning for non-exemplar class-incremental learning",
    "citation_count": 1,
    "authors": [
      "Wuxuan Shi",
      "Mang Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/01fb6de3360f9e32862665580e2c5853-Abstract-Conference.html": {
    "title": "SPO: Sequential Monte Carlo Policy Optimisation",
    "volume": "main",
    "abstract": "Leveraging planning during learning and decision-making is central to the long-term development of intelligent agents. Recent works have successfully combined tree-based search methods and self-play learning mechanisms to this end. However, these methods typically face scaling challenges due to the sequential nature of their search. While practical engineering solutions can partly overcome this, they often result in a negative impact on performance. In this paper, we introduce SPO: Sequential Monte Carlo Policy Optimisation, a model-based reinforcement learning algorithm grounded within the Expectation Maximisation (EM) framework. We show that SPO provides robust policy improvement and efficient scaling properties. The sample-based search makes it directly applicable to both discrete and continuous action spaces without modifications. We demonstrate statistically significant improvements in performance relative to model-free and model-based baselines across both continuous and discrete environments. Furthermore, the parallel nature of SPO's search enables effective utilisation of hardware accelerators, yielding favourable scaling laws",
    "checked": true,
    "id": "947242eacf8cdea55ef35833431dd3eea7166c23",
    "semantic_title": "spo: sequential monte carlo policy optimisation",
    "citation_count": 1,
    "authors": [
      "Matthew Macfarlane",
      "Edan Toledo",
      "Donal Byrne",
      "Paul Duckworth",
      "Alexandre Laterre"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0232cafe8d1909a01019abe8af32f3e1-Abstract-Conference.html": {
    "title": "Differentiable Modal Synthesis for Physical Modeling of Planar String Sound and Motion Simulation",
    "volume": "main",
    "abstract": "While significant advancements have been made in music generation and differentiable sound synthesis within machine learning and computer audition, the simulation of instrument vibration guided by physical laws has been underexplored. To address this gap, we introduce a novel model for simulating the spatio-temporal motion of nonlinear strings, integrating modal synthesis and spectral modeling within a neural network framework. Our model leverages mechanical properties and fundamental frequencies as inputs, outputting string states across time and space that solve the partial differential equation characterizing the nonlinear string. Empirical evaluations demonstrate that the proposed architecture achieves superior accuracy in string motion simulation compared to existing baseline architectures. The code and demo are available online",
    "checked": true,
    "id": "8ea0b72d0bb28c0add2db47233fbb8c1a776170c",
    "semantic_title": "differentiable modal synthesis for physical modeling of planar string sound and motion simulation",
    "citation_count": 2,
    "authors": [
      "Jin Woo Lee",
      "Jaehyun Park",
      "Min Jun Choi",
      "Kyogu Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0252a434b18962c94910c07cd9a7fecc-Abstract-Conference.html": {
    "title": "Unified Lexical Representation for Interpretable Visual-Language Alignment",
    "volume": "main",
    "abstract": "Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's groundbreaking work. Although CLIP performs well, the typical direct latent feature alignment lacks clarity in its representation and similarity scores. On the other hand, lexical representation, a vector whose element represents the similarity between the sample and a word from the vocabulary, is a natural sparse representation and interpretable, providing exact matches for individual words.However, lexical representations are difficult to learn due to no ground-truth supervision and false-discovery issues, and thus requires complex design to train effectively.In this paper, we introduce LexVLA, a more interpretable VLA framework by learning a unified lexical representation for both modalities without complex design. We use DINOv2 as our visual model for its local-inclined features and Llama 2, a generative language model, to leverage its in-context lexical prediction ability.To avoid the false discovery, we propose an overuse penalty to refrain the lexical representation from falsely frequently activating meaningless words.We demonstrate that these two pre-trained uni-modal models can be well-aligned by fine-tuning on the modest multi-modal dataset and avoid intricate training configurations. On cross-modal retrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset, outperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and those trained from scratch on even bigger datasets (e.g., 1.1B data, including CC-12M).We conduct extensive experiments to analyze LexVLA. Codes are available at https://github.com/Clementine24/LexVLA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Li",
      "Yikai Wang",
      "Yanwei Fu",
      "Dongyu Ru",
      "Zheng Zhang",
      "Tong He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/026211b600a8d54760a87d78b16d9153-Abstract-Conference.html": {
    "title": "LLaNA: Large Language and NeRF Assistant",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated an excellent understanding of images and 3D data. However, both modalities have shortcomings in holistically capturing the appearance and geometry of objects. Meanwhile, Neural Radiance Fields (NeRFs), which encode information within the weights of a simple Multi-Layer Perceptron (MLP), have emerged as an increasingly widespread modality that simultaneously encodes the geometry and photorealistic appearance of objects. This paper investigates the feasibility and effectiveness of ingesting NeRF into MLLM. We create LLaNA, the first general-purpose NeRF-languageassistant capable of performing new tasks such as NeRF captioning and Q&A. Notably, our method directly processes the weights of the NeRF's MLP to extract information about the represented objects without the need to render images or materialize 3D data structures. Moreover, we build a dataset of NeRFs with text annotations for various NeRF-language tasks with no human intervention.Based on this dataset, we develop a benchmark to evaluate the NeRF understanding capability of our method. Results show that processing NeRF weights performs favourably against extracting 2D or 3D representations from NeRFs",
    "checked": true,
    "id": "83c7067eaaba7c78945e7fa2ee21967c21d90061",
    "semantic_title": "llana: large language and nerf assistant",
    "citation_count": 7,
    "authors": [
      "Andrea Amaduzzi",
      "Pierluigi Zama Ramirez",
      "Giuseppe Lisanti",
      "Samuele Salti",
      "Luigi Di Stefano"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0267925e3c276e79189251585b4100bf-Abstract-Conference.html": {
    "title": "DiTFastAttn: Attention Compression for Diffusion Transformer Models",
    "volume": "main",
    "abstract": "Diffusion Transformers (DiT) excel at image and video generation but face computational challenges due to the quadratic complexity of self-attention operators. We propose DiTFastAttn, a post-training compression method to alleviate the computational bottleneck of DiT.We identify three key redundancies in the attention computation during DiT inference: (1) spatial redundancy, where many attention heads focus on local information; (2) temporal redundancy, with high similarity between the attention outputs of neighboring steps; (3) conditional redundancy, where conditional and unconditional inferences exhibit significant similarity. We propose three techniques to reduce these redundancies: (1) $\\textit{Window Attention with Residual Sharing}$ to reduce spatial redundancy; (2) $\\textit{Attention Sharing across Timesteps}$ to exploit the similarity between steps; (3) $\\textit{Attention Sharing across CFG}$ to skip redundant computations during conditional generation",
    "checked": true,
    "id": "887d44184b40aa654b77c4474f70fffc534067e9",
    "semantic_title": "ditfastattn: attention compression for diffusion transformer models",
    "citation_count": 38,
    "authors": [
      "Zhihang Yuan",
      "Hanling Zhang",
      "Lu Pu",
      "Xuefei Ning",
      "Linfeng Zhang",
      "Tianchen Zhao",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/02802e3df178cce7b13e8f63dd29ad9f-Abstract-Conference.html": {
    "title": "Open LLMs are Necessary for Current Private Adaptations and Outperform their Closed Alternatives",
    "volume": "main",
    "abstract": "While open Large Language Models (LLMs) have made significant progress, they still fall short of matching the performance of their closed, proprietary counterparts, making the latter attractive even for the use on highly private data. Recently, various new methods have been proposed to adapt closed LLMs to private data without leaking private information to third parties and/or the LLM provider. In this work, we analyze the privacy protection and performance of the four most recent methods for private adaptation of closed LLMs. By examining their threat models and thoroughly comparing their performance under different privacy levels according to differential privacy (DP), various LLM architectures, and multiple datasets for classification and generation tasks, we find that: (1) all the methods leak query data, i.e., the (potentially sensitive) user data that is queried at inference time, to the LLM provider, (2) three out of four methods also leak large fractions of private training data to the LLM provider while the method that protects private data requires a local open LLM, (3) all the methods exhibit lower performance compared to three private gradient-based adaptation methods for local open LLMs, and (4) the private adaptation methods for closed LLMs incur higher monetary training and query costs than running the alternative methods on local open LLMs.This yields the conclusion that, to achieve truly privacy-preserving LLM adaptations that yield high performance and more privacy at lower costs, taking into account current methods and models, one should use open LLMs",
    "checked": true,
    "id": "27424264819e9bc372560902569b35a5bbf5dd4d",
    "semantic_title": "open llms are necessary for current private adaptations and outperform their closed alternatives",
    "citation_count": 9,
    "authors": [
      "Vincent Hanke",
      "Tom Blanchard",
      "Franziska Boenisch",
      "Iyiola Olatunji",
      "Michael Backes",
      "Adam Dziedzic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/028ef7e68a5ea25fc26cd6abf3a5c147-Abstract-Conference.html": {
    "title": "SpelsNet: Surface Primitive Elements Segmentation by B-Rep Graph Structure Supervision",
    "volume": "main",
    "abstract": "Within the realm of Computer-Aided Design (CAD), Boundary-Representation (B-Rep) is the standard option for modeling shapes. We present SpelsNet, a neural architecture for the segmentation of 3D point clouds into surface primitive elements under topological supervision of its B-Rep graph structure. We also propose a point-to-BRep adjacency representation that allows for adapting conventional Linear Algebraic Representation of B-Rep graph structure to the point cloud domain. Thanks to this representation, SpelsNet learns from both spatial and topological domains to enable accurate and topologically consistent surface primitive element segmentation. In particular, SpelsNet is composed of two main components; (1) a supervised 3D spatial segmentation head that outputs B-Rep element types and memberships; (2) a graph-based head that leverages the proposed topological supervision. To enable the learning of SpelsNet with the proposed point-to-BRep adjacency supervision, we extend two existing CAD datasets with the required annotations, and conduct a thorough experimental validation on them. The obtained results showcase the efficacy of SpelsNet and its topological supervision compared to a set of baselines and state-of-the-art approaches",
    "checked": true,
    "id": "37a1e9bcc6f696bc2ea02f01a06b54f43436c3ca",
    "semantic_title": "spelsnet: surface primitive elements segmentation by b-rep graph structure supervision",
    "citation_count": 2,
    "authors": [
      "Kseniya Cherenkova",
      "Elona Dupont",
      "Anis Kacem",
      "Gleb Gusev",
      "Djamila Aouada"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/028fcbcf85435d39a40c4d61b42c99a4-Abstract-Conference.html": {
    "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
    "volume": "main",
    "abstract": "LLMs are seeing growing use for applications which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facilitates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve < 0.1 perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for KVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model",
    "checked": true,
    "id": "b085968c4362fb286ad6c5ef71a5db9630da0498",
    "semantic_title": "kvquant: towards 10 million context length llm inference with kv cache quantization",
    "citation_count": 251,
    "authors": [
      "Coleman Hooper",
      "Sehoon Kim",
      "Hiva Mohammadzadeh",
      "Michael W. Mahoney",
      "Sophia Shao",
      "Kurt Keutzer",
      "Amir Gholami"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/02c1d1d33dbfbaf03b3971bb542e72e2-Abstract-Conference.html": {
    "title": "DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion",
    "volume": "main",
    "abstract": "Diffusion-based methods have achieved remarkable achievements in 2D image or 3D object generation, however, the generation of 3D scenes and even $360^{\\circ}$ images remains constrained, due to the limited number of scene datasets, the complexity of 3D scenes themselves, and the difficulty of generating consistent multi-view images. To address these issues, we first establish a large-scale panoramic video-text dataset containing millions of consecutive panoramic keyframes with corresponding panoramic depths, camera poses, and text descriptions. Then, we propose a novel text-driven panoramic generation framework, termed DiffPano, to achieve scalable, consistent, and diverse panoramic scene generation. Specifically, benefiting from the powerful generative capabilities of stable diffusion, we fine-tune a single-view text-to-panorama diffusion model with LoRA on the established panoramic video-text dataset. We further design a spherical epipolar-aware multi-view diffusion model to ensure the multi-view consistency of the generated panoramic images. Extensive experiments demonstrate that DiffPano can generate scalable, consistent, and diverse panoramic images with given unseen text descriptions and camera poses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weicai Ye",
      "Chenhao Ji",
      "Zheng Chen",
      "Junyao Gao",
      "Xiaoshui Huang",
      "Song-Hai Zhang",
      "Wanli Ouyang",
      "Tong He",
      "Cairong Zhao",
      "Guofeng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/02cef2ae63853724eb99e70721d3bc65-Abstract-Conference.html": {
    "title": "Causal Contrastive Learning for Counterfactual Regression Over Time",
    "volume": "main",
    "abstract": "Estimating treatment effects over time holds significance in various domains, including precision medicine, epidemiology, economy, and marketing. This paper introduces a unique approach to counterfactual regression over time, emphasizing long-term predictions. Distinguishing itself from existing models like Causal Transformer, our approach highlights the efficacy of employing RNNs for long-term forecasting, complemented by Contrastive Predictive Coding (CPC) and Information Maximization (InfoMax). Emphasizing efficiency, we avoid the need for computationally expensive transformers. Leveraging CPC, our method captures long-term dependencies within time-varying confounders. Notably, recent models have disregarded the importance of invertible representation, compromising identification assumptions. To remedy this, we employ the InfoMax principle, maximizing a lower bound of mutual information between sequence data and its representation. Our method achieves state-of-the-art counterfactual estimation results using both synthetic and real-world data, marking the pioneering incorporation of Contrastive Predictive Encoding in causal inference",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mouad EL Bouchattaoui",
      "Myriam Tami",
      "BENOIT LEPETIT",
      "Paul-Henry Cournède"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/02cf868040aa0f0a1e1121cf255fdcfb-Abstract-Conference.html": {
    "title": "Certified Robustness for Deep Equilibrium Models via Serialized Random Smoothing",
    "volume": "main",
    "abstract": "Implicit models such as Deep Equilibrium Models (DEQs) have emerged as promising alternative approaches for building deep neural networks. Their certified robustness has gained increasing research attention due to security concerns. Existing certified defenses for DEQs employing interval bound propagation and Lipschitz-bounds not only offer conservative certification bounds but also are restricted to specific forms of DEQs. In this paper, we provide the first randomized smoothing certified defense for DEQs to solve these limitations. Our study reveals that simply applying randomized smoothing to certify DEQs provides certified robustness generalized to large-scale datasets but incurs extremely expensive computation costs. To reduce computational redundancy, we propose a novel Serialized Randomized Smoothing (SRS) approach that leverages historical information. Additionally, we derive a new certified radius estimation for SRS to theoretically ensure the correctness of our algorithm. Extensive experiments and ablation studies on image recognition demonstrate that our algorithm can significantly accelerate the certification of DEQs by up to 7x almost without sacrificing the certified accuracy. The implementation will be publicly available upon the acceptance of this work. Our code is available at https://github.com/WeizhiGao/Serialized-Randomized-Smoothing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhi Gao",
      "Zhichao Hou",
      "Han Xu",
      "Xiaorui Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/02d425a464e48bda5e810f8f4914b77e-Abstract-Conference.html": {
    "title": "Causal vs. Anticausal merging of predictors",
    "volume": "main",
    "abstract": "We study the differences arising from merging predictors in the causal and anticausal directions using the same data.In particular we study the asymmetries that arise in a simple model where we merge the predictors using one binary variable as target and two continuous variables as predictors.We use Causal Maximum Entropy (CMAXENT) as inductive bias to merge the predictors, however, we expect similar differences to hold also when we use other merging methods that take into account asymmetries between cause and effect.We show that if we observe all bivariate distributions, the CMAXENT solution reduces to a logistic regression in the causal direction and Linear Discriminant Analysis (LDA) in the anticausal direction.Furthermore, we study how the decision boundaries of these two solutions differ whenever we observe only some of the bivariate distributions implications for Out-Of-Variable (OOV) generalisation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergio Garrido Mejia",
      "Patrick Blöbaum",
      "Bernhard Schölkopf",
      "Dominik Janzing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0302fb83c62991efbccf0a003e4f5a92-Abstract-Conference.html": {
    "title": "Efficient Adversarial Training in LLMs with Continuous Attacks",
    "volume": "main",
    "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on five models from different families (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sophie Xhonneux",
      "Alessandro Sordoni",
      "Stephan Günnemann",
      "Gauthier Gidel",
      "Leo Schwinn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/030cf55d506515f39c042e63ba0376dd-Abstract-Conference.html": {
    "title": "Context and Geometry Aware Voxel Transformer for Semantic Scene Completion",
    "volume": "main",
    "abstract": "Vision-based Semantic Scene Completion (SSC) has gained much attention due to its widespread applications in various 3D perception tasks. Existing sparse-to-dense approaches typically employ shared context-independent queries across various input images, which fails to capture distinctions among them as the focal regions of different inputs vary and may result in undirected feature aggregation of cross-attention. Additionally, the absence of depth information may lead to points projected onto the image plane sharing the same 2D position or similar sampling points in the feature map, resulting in depth ambiguity. In this paper, we present a novel context and geometry aware voxel transformer. It utilizes a context aware query generator to initialize context-dependent queries tailored to individual input images, effectively capturing their unique characteristics and aggregating information within the region of interest. Furthermore, it extend deformable cross-attention from 2D to 3D pixel space, enabling the differentiation of points with similar image coordinates based on their depth coordinates. Building upon this module, we introduce a neural network named CGFormer to achieve semantic scene completion. Simultaneously, CGFormer leverages multiple 3D representations (i.e., voxel and TPV) to boost the semantic and geometric representation abilities of the transformed 3D volume from both local and global perspectives. Experimental results demonstrate that CGFormer achieves state-of-the-art performance on the SemanticKITTI and SSCBench-KITTI-360 benchmarks, attaining a mIoU of 16.87 and 20.05, as well as an IoU of 45.99 and 48.07, respectively. Remarkably, CGFormer even outperforms approaches employing temporal images as inputs or much larger image backbone networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhu Yu",
      "Runmin Zhang",
      "Jiacheng Ying",
      "Junchen Yu",
      "Xiaohai Hu",
      "Lun Luo",
      "Si-Yuan Cao",
      "Hui-liang Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0318de478e18308a5f64297f618299d3-Abstract-Conference.html": {
    "title": "MIDGArD: Modular Interpretable Diffusion over Graphs for Articulated Designs",
    "volume": "main",
    "abstract": "Providing functionality through articulation and interaction with objects is a key objective in 3D generation. We introduce MIDGArD (Modular Interpretable Diffusion over Graphs for Articulated Designs), a novel diffusion-based framework for articulated 3D asset generation. MIDGArD improves over foundational work in the field by enhancing quality, consistency, and controllability in the generation process. This is achieved through MIDGArD's modular approach that separates the problem into two primary components: structure generation and shape generation. The structure generation module of MIDGArD aims at producing coherent articulation features from noisy or incomplete inputs. It acts on the object's structural and kinematic attributes, represented as features of a graph that are being progressively denoised to issue coherent and interpretable articulation solutions. This denoised graph then serves as an advanced conditioning mechanism for the shape generation module, a 3D generative model that populates each link of the articulated structure with consistent 3D meshes. Experiments show the superiority of MIDGArD on the quality, consistency, and interpretability of the generated assets. Importantly, the generated models are fully simulatable, i.e., can be seamlessly integrated into standard physics engines such as MuJoCo, broadening MIDGArD's applicability to fields such as digital content creation, meta realities, and robotics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quentin Leboutet",
      "Nina Wiedemann",
      "zhipeng cai",
      "Michael Paulitsch",
      "Kai Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/031b5fd7d847f69ed33378a9a1117b4b-Abstract-Conference.html": {
    "title": "Provably Safe Neural Network Controllers via Differential Dynamic Logic",
    "volume": "main",
    "abstract": "While neural networks (NNs) have a large potential as autonomous controllers for Cyber-Physical Systems, verifying the safety of neural network based control systems (NNCSs) poses significant challenges for the practical use of NNs— especially when safety is needed for unbounded time horizons. One reason for this is the intractability of analyzing NNs, ODEs and hybrid systems. To this end, we introduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The first general approach that allows reusing control theory literature for NNCS verification. By joining forces, we can exploit the efficiency of NN verification tools while retaining the rigor of differential dynamic logic (dL). Based on a provably safe control envelope in dL, we derive a specification for the NN which is proven with NN verification tools. We show that a proof of the NN's adherence to the specification is then mirrored by a dL proof on the infinite-time safety of the NNCS.The NN verification properties resulting from hybrid systems typically contain nonlinear arithmetic over formulas with arbitrary logical structure while efficient NN verification tools merely support linear constraints. To overcome this divide, we present Mosaic: An efficient, sound and complete verification approach for polynomial real arithmetic properties on piece-wise linear NNs. Mosaic partitions complex NN verification queries into simple queries and lifts off-the-shelf linear constraint tools to the nonlinear setting in a completeness-preserving manner by combining approximation with exact reasoning for counterexample regions. In our evaluation we demonstrate the versatility of VerSAILLE and Mosaic: We prove infinite-time safety on the classical Vertical Airborne Collision Avoidance NNCS verification benchmark for some scenarios while (exhaustively) enumerating counterexample regions in unsafe scenarios. We also show that our approach significantly outperforms the State-of-the-Art tools in closed-loop NNV",
    "checked": true,
    "id": "6f22f8989bc034baa12ee90b775812f20b61c3c0",
    "semantic_title": "provably safe neural network controllers via differential dynamic logic",
    "citation_count": 11,
    "authors": [
      "Samuel Teuber",
      "Stefan Mitsch",
      "André Platzer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/03261886741f1f21f52f2a2d570616a2-Abstract-Conference.html": {
    "title": "Learning Image Priors Through Patch-Based Diffusion Models for Solving Inverse Problems",
    "volume": "main",
    "abstract": "Diffusion models can learn strong image priors from underlying data distribution and use them to solve inverse problems,but the training process is computationally expensive and requires lots of data.Such bottlenecks prevent most existing works from being feasible for high-dimensional and high-resolution data such as 3D images.This paper proposes a method to learn an efficient data prior for the entire image by training diffusion models only on patches of images.Specifically, we propose a patch-based position-aware diffusion inverse solver, called PaDIS, where we obtain the score function of the whole image through scores of patches and their positional encoding and utilize this as the prior for solving inverse problems.First of all, we show that this diffusion model achieves an improved memory efficiency and data efficiencywhile still maintaining the capability to generate entire images via positional encoding.Additionally, the proposed PaDIS model is highly flexible and can be plugged in with different diffusion inverse solvers (DIS).We demonstrate that the proposed PaDIS approach enables solving various inverse problems in both natural and medical image domains, including CT reconstruction, deblurring, and superresolution, given only patch-based priors.Notably, PaDIS outperforms previous DIS methods trained on entire image priors in the case of limited training data, demonstrating the data efficiency of our proposed approach by learning patch-based prior",
    "checked": true,
    "id": "42f87a530c7ff038021d57530749fa46be906a70",
    "semantic_title": "learning image priors through patch-based diffusion models for solving inverse problems",
    "citation_count": 11,
    "authors": [
      "Jason Hu",
      "Bowen Song",
      "Xiaojian Xu",
      "Liyue Shen",
      "Jeffrey Fessler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/03341fe3c3f848f699b201ce72c13e11-Abstract-Conference.html": {
    "title": "Seek Commonality but Preserve Differences: Dissected Dynamics Modeling for Multi-modal Visual RL",
    "volume": "main",
    "abstract": "Accurate environment dynamics modeling is crucial for obtaining effective state representations in visual reinforcement learning (RL) applications. However, when facing multiple input modalities, existing dynamics modeling methods (e.g., DeepMDP) usually stumble in addressing the complex and volatile relationship between different modalities. In this paper, we study the problem of efficient dynamics modeling for multi-modal visual RL. We find that under the existence of modality heterogeneity, modality-correlated and distinct features are equally important but play different roles in reflecting the evolution of environmental dynamics. Motivated by this fact, we propose Dissected Dynamics Modeling (DDM), a novel multi-modal dynamics modeling method for visual RL. Unlike existing methods, DDM explicitly distinguishes consistent and inconsistent information across modalities and treats them separately with a divide-and-conquer strategy. This is done by dispatching the features carrying different information into distinct dynamics modeling pathways, which naturally form a series of implicit regularizations along the learning trajectories. In addition, a reward predictive function is further introduced to filter task-irrelevant information in both modality-consistent and inconsistent features, ensuring information integrity while avoiding potential distractions. Extensive experiments show that DDM consistently achieves competitive performance in challenging multi-modal visual environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangru Huang",
      "Peixi Peng",
      "Yifan Zhao",
      "Guangyao Chen",
      "Yonghong Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0337b41b4e8b2eb5d7ab161ffd42cf3b-Abstract-Conference.html": {
    "title": "Reciprocal Learning",
    "volume": "main",
    "abstract": "We demonstrate that numerous machine learning algorithms are specific instances of one single paradigm: reciprocal learning. These instances range from active learning over multi-armed bandits to self-training. We show that all these algorithms not only learn parameters from data but also vice versa: They iteratively alter training data in a way that depends on the current model fit. We introduce reciprocal learning as a generalization of these algorithms using the language of decision theory. This allows us to study under what conditions they converge. The key is to guarantee that reciprocal learning contracts such that the Banach fixed-point theorem applies. In this way, we find that reciprocal learning converges at linear rates to an approximately optimal model under some assumptions on the loss function, if their predictions are probabilistic and the sample adaption is both non-greedy and either randomized or regularized. We interpret these findings and provide corollaries that relate them to active learning, self-training, and bandits",
    "checked": true,
    "id": "8908dfb369c56955132a425dda4570af279cee95",
    "semantic_title": "reciprocal learning",
    "citation_count": 0,
    "authors": [
      "Julian Rodemann",
      "Christoph Jansen",
      "Georg Schollmeyer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/03469b1a66e351b18272be23baf3b809-Abstract-Conference.html": {
    "title": "D-LLM: A Token Adaptive Computing Resource Allocation Strategy for Large Language Models",
    "volume": "main",
    "abstract": "Large language models have shown an impressive societal impact owing to their excellent understanding and logical reasoning skills. However, such strong ability relies on a huge amount of computing resources, which makes it difficult to deploy LLMs on computing resource-constrained platforms. Currently, LLMs process each token equivalently, but we argue that not every word is equally important. Some words should not be allocated excessive computing resources, particularly for dispensable terms in simple questions. In this paper, we propose a novel dynamic inference paradigm for LLMs, namely D-LLMs, which adaptively allocate computing resources in token processing. We design a dynamic decision module for each transformer layer that decides whether a network unit should be executed or skipped. Moreover, we tackle the issue of adapting D-LLMs to real-world applications, specifically concerning the missing KV-cache when layers are skipped. To overcome this, we propose a simple yet effective eviction policy to exclude the skipped layers from subsequent attention calculations. The eviction policy not only enables D-LLMs to be compatible with prevalent applications but also reduces considerable storage resources. Experimentally, D-LLMs show superior performance, in terms of computational cost and KV storage utilization. It can reduce up to 45\\% computational cost and KV storage on Q\\&A, summarization, and math solving tasks, 50\\% on commonsense reasoning tasks",
    "checked": true,
    "id": "005fc6908a12f8d460aa06c1eb6c8176ca9b1199",
    "semantic_title": "d-llm: a token adaptive computing resource allocation strategy for large language models",
    "citation_count": 8,
    "authors": [
      "Yikun Jiang",
      "Huanyu Wang",
      "Lei Xie",
      "Hanbin Zhao",
      "zhang chao",
      "Hui Qian",
      "John C. S. Lui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0346c8a510dd15971566a97a241c5e6a-Abstract-Conference.html": {
    "title": "Near-Optimal Distributionally Robust Reinforcement Learning with General $L_p$ Norms",
    "volume": "main",
    "abstract": "To address the challenges of sim-to-real gap and sample efficiency in reinforcement learning (RL), this work studies distributionally robust Markov decision processes (RMDPs) --- optimize the worst-case performance when the deployed environment is within an uncertainty set around some nominal MDP. Despite recent efforts, the sample complexity of RMDPs has remained largely undetermined. While the statistical implications of distributional robustness in RL have been explored in some specific cases, the generalizability of the existing findings remains unclear, especially in comparison to standard RL. Assuming access to a generative model that samples from the nominal MDP, we examine the sample complexity of RMDPs using a class of generalized $L_p$ norms as the 'distance' function for the uncertainty set, under two commonly adopted $sa$-rectangular and $s$-rectangular conditions. Our results imply that RMDPs can be more sample-efficient to solve than standard MDPs using generalized $L_p$ norms in both $sa$- and $s$-rectangular cases, potentially inspiring more empirical research. We provide a near-optimal upper bound and a matching minimax lower bound for the $sa$-rectangular scenarios. For $s$-rectangular cases, we improve the state-of-the-art upper bound and also derive a lower bound using $L_\\infty$ norm that verifies the tightness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Clavier",
      "Laixi Shi",
      "Erwan Le Pennec",
      "Eric Mazumdar",
      "Adam Wierman",
      "Matthieu Geist"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/034b4a2860d4d170ce663584bc78cb32-Abstract-Conference.html": {
    "title": "AdaNovo: Towards Robust \\emph{De Novo} Peptide Sequencing in Proteomics against Data Biases",
    "volume": "main",
    "abstract": "Tandem mass spectrometry has played a pivotal role in advancing proteomics, enabling the high-throughput analysis of protein composition in biological tissues. Despite the development of several deep learning methods for predicting amino acid sequences (peptides) responsible for generating the observed mass spectra, training data biases hinder further advancements of \\emph{de novo} peptide sequencing. Firstly, prior methods struggle to identify amino acids with Post-Translational Modifications (PTMs) due to their lower frequency in training data compared to canonical amino acids, further resulting in unsatisfactory peptide sequencing performance. Secondly, various noise and missing peaks in mass spectra reduce the reliability of training data (Peptide-Spectrum Matches, PSMs). To address these challenges, we propose AdaNovo, a novel and domain knowledge-inspired framework that calculates Conditional Mutual Information (CMI) between the mass spectra and amino acids or peptides, using CMI for robust training against above biases. Extensive experiments indicate that AdaNovo outperforms previous competitors on the widely-used 9-species benchmark, meanwhile yielding 3.6\\% - 9.4\\% improvements in PTMs identification. The supplements contain the code",
    "checked": true,
    "id": "6f8fcc1b648e57925076636057ec94a3f22ff692",
    "semantic_title": "adanovo: towards robust \\emph{de novo} peptide sequencing in proteomics against data biases",
    "citation_count": 1,
    "authors": [
      "Jun Xia",
      "Shaorong Chen",
      "Jingbo Zhou",
      "Shan Xiaojun",
      "Wenjie Du",
      "Zhangyang Gao",
      "Cheng Tan",
      "Bozhen Hu",
      "Jiangbin Zheng",
      "Stan Z. Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/034cd49870f1cc253fc08686049ae7eb-Abstract-Conference.html": {
    "title": "MimicTalk: Mimicking a personalized and expressive 3D talking face in minutes",
    "volume": "main",
    "abstract": "Talking face generation (TFG) aims to animate a target identity's face to create realistic talking videos. Personalized TFG is a variant that emphasizes the perceptual identity similarity of the synthesized result (from the perspective of appearance and talking style). While previous works typically solve this problem by learning an individual neural radiance field (NeRF) for each identity to implicitly store its static and dynamic information, we find it inefficient and non-generalized due to the per-identity-per-training framework and the limited training data. To this end, we propose MimicTalk, the first attempt that exploits the rich knowledge from a NeRF-based person-agnostic generic model for improving the efficiency and robustness of personalized TFG. To be specific, (1) we first come up with a person-agnostic 3D TFG model as the base model and propose to adapt it into a specific identity; (2) we propose a static-dynamic-hybrid adaptation pipeline to help the model learn the personalized static appearance and facial dynamic features; (3) To generate the facial motion of the personalized talking style, we propose an in-context stylized audio-to-motion model that mimics the implicit talking style provided in the reference video without information loss by an explicit style representation. The adaptation process to an unseen identity can be performed in 15 minutes, which is 47 times faster than previous person-dependent methods. Experiments show that our MimicTalk surpasses previous baselines regarding video quality, efficiency, and expressiveness. Video samples are available at https://mimictalk.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenhui Ye",
      "Tianyun Zhong",
      "Yi Ren",
      "Ziyue Jiang",
      "Jiawei Huang",
      "Rongjie Huang",
      "Jinglin Liu",
      "Jinzheng He",
      "Chen Zhang",
      "Zehan Wang",
      "Xize Cheng",
      "Xiang Yin",
      "Zhou Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0356216f73660e15670510f5e42b5fa6-Abstract-Conference.html": {
    "title": "JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models",
    "volume": "main",
    "abstract": "Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications.To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis.To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data.To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM.Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels.Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts.The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM.We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model. The whole process only needs to invoke GPT-4 API 9.3k times and use 4.6B data for training.Experimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings.Our code and data will be publicly released in \\url{https://github.com/RUCAIBox/JiuZhang3.0}",
    "checked": true,
    "id": "9f5b8d5310a2bafad2838dcf91c88a02b0aca106",
    "semantic_title": "jiuzhang3.0: efficiently improving mathematical reasoning by training small data synthesis models",
    "citation_count": 37,
    "authors": [
      "Kun Zhou",
      "Beichen Zhang",
      "jiapeng wang",
      "Zhipeng Chen",
      "Xin Zhao",
      "Jing Sha",
      "Zhichao Sheng",
      "Shijin Wang",
      "Ji-Rong Wen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/03738e5f26967582eeb3b57eef82f1f0-Abstract-Conference.html": {
    "title": "Voila-A: Aligning Vision-Language Models with User's Gaze Attention",
    "volume": "main",
    "abstract": "In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by ubiquitous wearable devices such as MR glasses, as a proxy for human attention to guide VLMs. We propose a novel approach, Voila-A, for gaze alignment to enhance the effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we introduce a new model VOILA-A that integrate gaze information into VLMs while maintain pretrained knowledge from webscale dataset. We evaluate Voila-A using a hold-out validation set and a newly collected VOILA-GAZE testset, which features real-life scenarios captured with a gaze-tracking device. Our experimental results demonstrate that Voila-A significantly outperforms several baseline models. By aligning model attention with human gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and fosters engaging human-AI interaction across a wide range of applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Yan",
      "Zeyu Wang",
      "Lei Ji",
      "Yuntao Wang",
      "Nan Duan",
      "Shuai Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0382cb76309820f71c6eacd47b36ce71-Abstract-Conference.html": {
    "title": "einspace: Searching for Neural Architectures from Fundamental Operations",
    "volume": "main",
    "abstract": "Neural architecture search (NAS) finds high performing networks for a given task. Yet the results of NAS are fairly prosaic; they did not e.g. create a shift from convolutional structures to transformers. This is not least because the search spaces in NAS often aren't diverse enough to include such transformations a priori. Instead, for NAS to provide greater potential for fundamental design shifts, we need a novel expressive search space design which is built from more fundamental operations. To this end, we introduce einspace, a search space based on a parameterised probabilistic context-free grammar. Our space is versatile, supporting architectures of various sizes and complexities, while also containing diverse network operations which allow it to model convolutions, attention components and more. It contains many existing competitive architectures, and provides flexibility for discovering new ones. Using this search space, we perform experiments to find novel architectures as well as improvements on existing ones on the diverse Unseen NAS datasets. We show that competitive architectures can be obtained by searching from scratch, and we consistently find large improvements when initialising the search with strong baselines. We believe that this work is an important advancement towards a transformative NAS paradigm where search space expressivity and strategic search initialisation play key roles",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linus Ericsson",
      "Miguel Espinosa Minano",
      "Chenhongyi Yang",
      "Antreas Antoniou",
      "Amos J. Storkey",
      "Shay B. Cohen",
      "Steven McDonagh",
      "Elliot J. Crowley"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/03bec5d9f651c1fb89be07a4120238a0-Abstract-Conference.html": {
    "title": "Structured flexibility in recurrent neural networks via neuromodulation",
    "volume": "main",
    "abstract": "A core aim in theoretical and systems neuroscience is to develop models which help us better understand biological intelligence. Such models range broadly in both complexity and biological plausibility. One widely-adopted example is task-optimized recurrent neural networks (RNNs), which have been used to generate hypotheses about how the brain's neural dynamics may organize to accomplish tasks. However, task-optimized RNNs typically have a fixed weight matrix representing the synaptic connectivity between neurons. From decades of neuroscience research, we know that synaptic weights are constantly changing, controlled in part by chemicals such as neuromodulators. In this work we explore the computational implications of synaptic gain scaling, a form of neuromodulation, using task-optimized low-rank RNNs.In our neuromodulated RNN (NM-RNN) model, a neuromodulatory subnetwork outputs a low-dimensional neuromodulatory signal that dynamically scales the low-rank recurrent weights of an output-generating RNN. In empirical experiments, we find that the structured flexibility in the NM-RNN allows it to both train and generalize with a higher degree of accuracy than low-rank RNNs on a set of canonical tasks.Additionally, via theoretical analyses we show how neuromodulatory gain scaling endows networks with gating mechanisms commonly found in artificial RNNs. We end by analyzing the low-rank dynamics of trained NM-RNNs, to show how task computations are distributed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julia Costacurta",
      "Shaunak Bhandarkar",
      "David Zoltowski",
      "Scott Linderman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/03cd3cf3f74d4f9ce5958de269960884-Abstract-Conference.html": {
    "title": "DU-Shapley: A Shapley Value Proxy for Efficient Dataset Valuation",
    "volume": "main",
    "abstract": "We consider the dataset valuation problem, that is the problem of quantifying the incremental gain, to some relevant pre-defined utility of a machine learning task, of aggregating an individual dataset to others.The Shapley value is a natural tool to perform dataset valuation due to its formal axiomatic justification, which can be combined with Monte Carlo integration to overcome the computational tractability challenges. Such generic approximation methods, however, remain expensive in some cases. In this paper, we exploit the knowledge about the structure of the dataset valuation problem to devise more efficient Shapley value estimators. We propose a novel approximation, referred to as discrete uniform Shapley, which is expressed as an expectation under a discrete uniform distribution with support of reasonable size. We justify the relevancy of the proposed framework via asymptotic and non-asymptotic theoretical guarantees and illustrate its benefits via an extensive set of numerical experiments",
    "checked": true,
    "id": "e5514b8532ed2988d22b91c4bdc5bbedc9fa98e5",
    "semantic_title": "du-shapley: a shapley value proxy for efficient dataset valuation",
    "citation_count": 5,
    "authors": [
      "Felipe Garrido Lucero",
      "Benjamin Heymann",
      "Maxime Vono",
      "Patrick Loiseau",
      "Vianney Perchet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/03cdf8e212ba92a3f36bffe1391928bd-Abstract-Conference.html": {
    "title": "Enhancing Zero-Shot Vision Models by Label-Free Prompt Distribution Learning and Bias Correcting",
    "volume": "main",
    "abstract": "Vision-language models, such as CLIP, have shown impressive generalization capacities when using appropriate text descriptions. While optimizing prompts on downstream labeled data has proven effective in improving performance, these methods entail labor costs for annotations and are limited by their quality. Additionally, since CLIP is pre-trained on highly imbalanced Web-scale data, it suffers from inherent label bias that leads to suboptimal performance. To tackle the above challenges, we propose a label-**F**ree p**ro**mpt distribution **l**earning and b**i**as **c**orrection framework, dubbed as **Frolic**, which boosts zero-shot performance without the need for labeled data. Specifically, our Frolic learns distributions over prompt prototypes to capture diverse visual representations and adaptively fuses these with the original CLIP through confidence matching.This fused model is further enhanced by correcting label bias via a label-free logit adjustment. Notably, our method is not only training-free but also circumvents the necessity for hyper-parameter tuning. Extensive experimental results across 16 datasets demonstrate the efficacy of our approach, particularly outperforming the state-of-the-art by an average of $2.6\\%$ on 10 datasets with CLIP ViT-B/16 and achieving an average margin of $1.5\\%$ on ImageNet and its five distribution shifts with CLIP ViT-B/16. Codes are available in [https://github.com/zhuhsingyuu/Frolic](https://github.com/zhuhsingyuu/Frolic)",
    "checked": true,
    "id": "4e6a85f656de178f913790f9755b6cb9ee2cd3ca",
    "semantic_title": "enhancing zero-shot vision models by label-free prompt distribution learning and bias correcting",
    "citation_count": 4,
    "authors": [
      "Xingyu Zhu",
      "Beier Zhu",
      "Yi Tan",
      "Shuo Wang",
      "Yanbin Hao",
      "Hanwang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/03d113a060c0ac93a5859517a0f07271-Abstract-Conference.html": {
    "title": "Diffusion PID: Interpreting Diffusion via Partial Information Decomposition",
    "volume": "main",
    "abstract": "Text-to-image diffusion models have made significant progress in generating naturalistic images from textual inputs, and demonstrate the capacity to learn and represent complex visual-semantic relationships. While these diffusion models have achieved remarkable success, the underlying mechanisms driving their performance are not yet fully accounted for, with many unanswered questions surrounding what they learn, how they represent visual-semantic relationships, and why they sometimes fail to generalize. Our work presents Diffusion Partial Information Decomposition (DiffusionPID), a novel technique that applies information-theoretic principles to decompose the input text prompt into its elementary components, enabling a detailed examination of how individual tokens and their interactions shape the generated image. We introduce a formal approach to analyze the uniqueness, redundancy, and synergy terms by applying PID to the denoising model at both the image and pixel level. This approach enables us to characterize how individual tokens and their interactions affect the model output. We first present a fine-grained analysis of characteristics utilized by the model to uniquely localize specific concepts, we then apply our approach in bias analysis and show it can recover gender and ethnicity biases. Finally, we use our method to visually characterize word ambiguity and similarity from the model's perspective and illustrate the efficacy of our method for prompt intervention. Our results show that PID is a potent tool for evaluating and diagnosing text-to-image diffusion models. Link to project page: https://rbz-99.github.io/Diffusion-PID/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaurya Dewan",
      "Rushikesh Zawar",
      "Prakanshul Saxena",
      "Yingshan CHANG",
      "Andrew Luo",
      "Yonatan Bisk"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/03dbc11a22e79cd38bea53cf518c2371-Abstract-Conference.html": {
    "title": "Test-Time Dynamic Image Fusion",
    "volume": "main",
    "abstract": "The inherent challenge of image fusion lies in capturing the correlation of multi-source images and comprehensively integrating effective information from different sources. Most existing techniques fail to perform dynamic image fusion while notably lacking theoretical guarantees, leading to potential deployment risks in this field. Is it possible to conduct dynamic image fusion with a clear theoretical justification? In this paper, we give our solution from a generalization perspective. We proceed to reveal the generalized form of image fusion and derive a new test-time dynamic image fusion paradigm. It provably reduces the upper bound of generalization error. Specifically, we decompose the fused image into multiple components corresponding to its source data. The decomposed components represent the effective information from the source data, thus the gap between them reflects the \\textit{Relative Dominability} (RD) of the uni-source data in constructing the fusion image. Theoretically, we prove that the key to reducing generalization error hinges on the negative correlation between the RD-based fusion weight and the uni-source reconstruction loss. Intuitively, RD dynamically highlights the dominant regions of each source and can be naturally converted to the corresponding fusion weight, achieving robust results. Extensive experiments and discussions with in-depth analysis on multiple benchmarks confirm our findings and superiority. Our code is available at https://github.com/Yinan-Xia/TTD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bing Cao",
      "Yinan Xia",
      "Yi Ding",
      "Changqing Zhang",
      "Qinghua Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/03e7eaa586f0990c633f8a8e57e08ca6-Abstract-Conference.html": {
    "title": "An End-To-End Graph Attention Network Hashing for Cross-Modal Retrieval",
    "volume": "main",
    "abstract": "Due to its low storage cost and fast search speed, cross-modal retrieval based on hashing has attracted widespread attention and is widely used in real-world applications of social media search. However, most existing hashing methods are often limited by uncomprehensive feature representations and semantic associations, which greatly restricts their performance and applicability in practical applications. To deal with this challenge, in this paper, we propose an end-to-end graph attention network hashing (EGATH) for cross-modal retrieval, which can not only capture direct semantic associations between images and texts but also match semantic content between different modalities. We adopt the contrastive language image pretraining (CLIP) combined with the Transformer to improve understanding and generalization ability in semantic consistency across different data modalities. The classifier based on graph attention network is applied to obtain predicted labels to enhance cross-modal feature representation. We construct hash codes using an optimization strategy and loss function to preserve the semantic information and compactness of the hash code. Comprehensive experiments on the NUS-WIDE, MIRFlickr25K, and MS-COCO benchmark datasets show that our EGATH significantly outperforms against several state-of-the-art methods",
    "checked": true,
    "id": "6553fe992cde041fa4439e5e2ae21525095fb2db",
    "semantic_title": "an end-to-end graph attention network hashing for cross-modal retrieval",
    "citation_count": 2,
    "authors": [
      "Huilong Jin",
      "Yingxue Zhang",
      "Lei Shi",
      "Shuang Zhang",
      "Feifei Kou",
      "Jiapeng Yang",
      "Chuangying Zhu",
      "Jia Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/03e9a69e5b686c316a07d73f0cf5e225-Abstract-Conference.html": {
    "title": "Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba",
    "volume": "main",
    "abstract": "3D Hand reconstruction from a single RGB image is challenging due to the articulated motion, self-occlusion, and interaction with objects. Existing SOTA methods employ attention-based transformers to learn the 3D hand pose and shape, yet they do not fully achieve robust and accurate performance, primarily due to inefficiently modeling spatial relations between joints. To address this problem, we propose a novel graph-guided Mamba framework, named Hamba, which bridges graph learning and state space modeling. Our core idea is to reformulate Mamba's scanning into graph-guided bidirectional scanning for 3D reconstruction using a few effective tokens. This enables us to efficiently learn the spatial relationships between joints for improving reconstruction performance. Specifically, we design a Graph-guided State Space (GSS) block that learns the graph-structured relations and spatial sequences of joints and uses 88.5\\% fewer tokens than attention-based methods. Additionally, we integrate the state space features and the global features using a fusion module. By utilizing the GSS block and the fusion module, Hamba effectively leverages the graph-guided state space features and jointly considers global and local features to improve performance. Experiments on several benchmarks and in-the-wild tests demonstrate that Hamba significantly outperforms existing SOTAs, achieving the PA-MPVPE of 5.3mm and F@15mm of 0.992 on FreiHAND. At the time of this paper's acceptance, Hamba holds the top position, Rank 1, in two competition leaderboards on 3D hand reconstruction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoye Dong",
      "Aviral Chharia",
      "Wenbo Gou",
      "Francisco Vicente Carrasco",
      "Fernando D De la Torre"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/040ace837dd270a87055bb10dd7c0392-Abstract-Conference.html": {
    "title": "Omnigrasp: Grasping Diverse Objects with Simulated Humanoids",
    "volume": "main",
    "abstract": "We present a method for controlling a simulated humanoid to grasp an object and move it to follow an object's trajectory. Due to the challenges in controlling a humanoid with dexterous hands, prior methods often use a disembodied hand and only consider vertical lifts or short trajectories. This limited scope hampers their applicability for object manipulation required for animation and simulation. To close this gap, we learn a controller that can pick up a large number (>1200) of objects and carry them to follow randomly generated trajectories. Our key insight is to leverage a humanoid motion representation that provides human-like motor skills and significantly speeds up training. Using only simplistic reward, state, and object representations, our method shows favorable scalability on diverse objects and trajectories. For training, we do not need a dataset of paired full-body motion and object trajectories. At test time, we only require the object mesh and desired trajectories for grasping and transporting. To demonstrate the capabilities of our method, we show state-of-the-art success rates in following object trajectories and generalizing to unseen objects. Code and models will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyi Luo",
      "Jinkun Cao",
      "Sammy Christen",
      "Alexander Winkler",
      "Kris Kitani",
      "Weipeng Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/040c816286b3844fd78f2124eec75f2e-Abstract-Conference.html": {
    "title": "AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data",
    "volume": "main",
    "abstract": "Open-source Large Language Models (LLMs) and their specialized variants, particularly Code LLMs, have recently delivered impressive performance. However, previous Code LLMs are typically fine-tuned on single-source data with limited quality and diversity, which may insufficiently elicit the potential of pre-trained Code LLMs. In this paper, we present AlchemistCoder, a series of Code LLMs with enhanced code generation and generalization capabilities fine-tuned on multi-source data. To achieve this, we pioneer to unveil inherent conflicts among the various styles and qualities in multi-source code corpora and introduce data-specific prompts with hindsight relabeling, termed AlchemistPrompts, to harmonize different data sources and instruction-response pairs. Additionally, we propose incorporating the data construction process into the fine-tuning data as code comprehension tasks, including instruction evolution, data filtering, and code review. Extensive experiments demonstrate that AlchemistCoder holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing the efficacy of our method in refining instruction-following capabilities and advancing the boundaries of code intelligence. Source code and models are available at https://github.com/InternLM/AlchemistCoder",
    "checked": true,
    "id": "1371a9170f802d56c60940693ddeab60b71d08bb",
    "semantic_title": "alchemistcoder: harmonizing and eliciting code capability by hindsight tuning on multi-source data",
    "citation_count": 2,
    "authors": [
      "Zifan Song",
      "Yudong Wang",
      "Wenwei Zhang",
      "Kuikun Liu",
      "Chengqi Lyu",
      "Demin Song",
      "Qipeng Guo",
      "Hang Yan",
      "Dahua Lin",
      "Kai Chen",
      "Cairong Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/040d3b6af368bf71f952c18da5713b48-Abstract-Conference.html": {
    "title": "Gliding over the Pareto Front with Uniform Designs",
    "volume": "main",
    "abstract": "Multiobjective optimization (MOO) plays a critical role in various real-world domains. A major challenge therein is generating $K$ uniform Pareto-optimal solutions to represent the entire Pareto front. To address this issue, this paper firstly introduces \\emph{fill distance} to evaluate the $K$ design points, which provides a quantitative metric for the representativeness of the design. However, directly specifying the optimal design that minimizes the fill distance is nearly intractable due to the nested $\\min-\\max-\\min$ optimization problem. To address this, we propose a surrogate ``max-packing'' design for the fill distance design, which is easier to optimize and leads to a rate-optimal design with a fill distance at most $4\\times$ the minimum value. Extensive experiments on synthetic and real-world benchmarks demonstrate that our proposed paradigm efficiently produces high-quality, representative solutions and outperforms baseline methods",
    "checked": true,
    "id": "dca2a1acb22522298a6b55b04aff74ccfe794de9",
    "semantic_title": "gliding over the pareto front with uniform designs",
    "citation_count": 3,
    "authors": [
      "Xiaoyuan Zhang",
      "Genghui Li",
      "Xi Lin",
      "Yichi Zhang",
      "Yifan Chen",
      "Qingfu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/047682108c3b053c61ad2da5a6057b4e-Abstract-Conference.html": {
    "title": "SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization",
    "volume": "main",
    "abstract": "Social relation reasoning aims to identify relation categories such as friends, spouses, and colleagues from images. While current methods adopt the paradigm of training a dedicated network end-to-end using labeled image data, they are limited in terms of generalizability and interpretability. To address these issues, we first present a simple yet well-crafted framework named SocialGPT, which combines the perception capability of Vision Foundation Models (VFMs) and the reasoning capability of Large Language Models (LLMs) within a modular framework, providing a strong baseline for social relation recognition. Specifically, we instruct VFMs to translate image content into a textual social story, and then utilize LLMs for text-based reasoning. SocialGPT introduces systematic design principles to adapt VFMs and LLMs separately and bridge their gaps. Without additional model training, it achieves competitive zero-shot results on two databases while offering interpretable answers, as LLMs can generate language-based explanations for the decisions. The manual prompt design process for LLMs at the reasoning phase is tedious and an automated prompt optimization method is desired. As we essentially convert a visual classification task into a generative task of LLMs, automatic prompt optimization encounters a unique long prompt optimization issue. To address this issue, we further propose the Greedy Segment Prompt Optimization (GSPO), which performs a greedy search by utilizing gradient information at the segment level. Experimental results show that GSPO significantly improves performance, and our method also generalizes to different image styles. The code is available at https://github.com/Mengzibin/SocialGPT",
    "checked": true,
    "id": "3c87fa3623408799c9ef304cd0b1554607c029e7",
    "semantic_title": "socialgpt: prompting llms for social relation reasoning via greedy segment optimization",
    "citation_count": 8,
    "authors": [
      "Wanhua Li",
      "Zibin Meng",
      "Jiawei Zhou",
      "Donglai Wei",
      "Chuang Gan",
      "Hanspeter Pfister"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/047bf3f8aa5a050351de38df589cc6af-Abstract-Conference.html": {
    "title": "Online Learning of Delayed Choices",
    "volume": "main",
    "abstract": "Choice models are essential for understanding decision-making processes in domains like online advertising, product recommendations, and assortment optimization. The Multinomial Logit (MNL) model is particularly versatile in selecting products or advertisements for display. However, challenges arise with unknown MNL parameters and delayed feedback, requiring sellers to learn customers' choice behavior and make dynamic decisions with biased knowledge due to delays. We address these challenges by developing an algorithm that handles delayed feedback, balancing exploration and exploitation using confidence bounds and optimism. We first consider a censored setting where a threshold for considering feedback is imposed by business requirements. Our algorithm demonstrates a $\\tilde{O}(\\sqrt{NT})$ regret, with a matching lower bound up to a logarithmic term. Furthermore, we extend our analysis to environments with non-thresholded delays, achieving a $\\tilde{O}(\\sqrt{NT})$ regret. To validate our approach, we conduct experiments that confirm the effectiveness of our algorithm",
    "checked": true,
    "id": "f1587f77cfdb9cd44414e85e6362f3f4af2c4cf1",
    "semantic_title": "online learning of delayed choices",
    "citation_count": 1,
    "authors": [
      "Recep Yusuf Bekci"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/047c84ec50bd8ea29349b996fc64af4b-Abstract-Conference.html": {
    "title": "Overcoming Common Flaws in the Evaluation of Selective Classification Systems",
    "volume": "main",
    "abstract": "Selective Classification, wherein models can reject low-confidence predictions, promises reliable translation of machine-learning based classification systems to real-world scenarios such as clinical diagnostics. While current evaluation of these systems typically assumes fixed working points based on pre-defined rejection thresholds, methodological progress requires benchmarking the general performance of systems akin to the $\\mathrm{AUROC}$ in standard classification. In this work, we define 5 requirements for multi-threshold metrics in selective classification regarding task alignment, interpretability, and flexibility, and show how current approaches fail to meet them. We propose the Area under the Generalized Risk Coverage curve ($\\mathrm{AUGRC}$), which meets all requirements and can be directly interpreted as the average risk of undetected failures. We empirically demonstrate the relevance of $\\mathrm{AUGRC}$ on a comprehensive benchmark spanning 6 data sets and 13 confidence scoring functions. We find that the proposed metric substantially changes metric rankings on 5 out of the 6 data sets",
    "checked": true,
    "id": "aa0d360e558bcc1a0ba8c979bbf9856051328c88",
    "semantic_title": "overcoming common flaws in the evaluation of selective classification systems",
    "citation_count": 5,
    "authors": [
      "Jeremias Traub",
      "Till Bungert",
      "Carsten Lüth",
      "Michael Baumgartner",
      "Klaus Maier-Hein",
      "Lena Maier-Hein",
      "Paul Jaeger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/04a80267ad46fc730011f8760f265054-Abstract-Conference.html": {
    "title": "Connecting Joint-Embedding Predictive Architecture with Contrastive Self-supervised Learning",
    "volume": "main",
    "abstract": "In recent advancements in unsupervised visual representation learning, the Joint-Embedding Predictive Architecture (JEPA) has emerged as a significant method for extracting visual features from unlabeled imagery through an innovative masking strategy. Despite its success, two primary limitations have been identified: the inefficacy of Exponential Moving Average (EMA) from I-JEPA in preventing entire collapse and the inadequacy of I-JEPA prediction in accurately learning the mean of patch representations. Addressing these challenges, this study introduces a novel framework, namely C-JEPA (Contrastive-JEPA), which integrates the Image-based Joint-Embedding Predictive Architecture with the Variance-Invariance-Covariance Regularization (VICReg) strategy. This integration is designed to effectively learn the variance/covariance for preventing entire collapse and ensuring invariance in the mean of augmented views, thereby overcoming the identified limitations. Through empirical and theoretical evaluations, our work demonstrates that C-JEPA significantly enhances the stability and quality of visual representation learning. When pre-trained on the ImageNet-1K dataset, C-JEPA exhibits rapid and improved convergence in both linear probing and fine-tuning performance metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shentong Mo",
      "Peter Tong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/04ad66d02234541aac5143de7876e880-Abstract-Conference.html": {
    "title": "Multi-Label Learning with Stronger Consistency Guarantees",
    "volume": "main",
    "abstract": "We present a detailed study of surrogate losses and algorithms for multi-label learning, supported by $H$-consistency bounds. We first show that, for the simplest form of multi-label loss (the popular Hamming loss), the well-known consistent binary relevance surrogate suffers from a sub-optimal dependency on the number of labels in terms of $H$-consistency bounds, when using smooth losses such as logistic losses. Furthermore, this loss function fails to account for label correlations. To address these drawbacks, we introduce a novel surrogate loss, *multi-label logistic loss*, that accounts for label correlations and benefits from label-independent $H$-consistency bounds. We then broaden our analysis to cover a more extensive family of multi-label losses, including all common ones and a new extension defined based on linear-fractional functions with respect to the confusion matrix. We also extend our multi-label logistic losses to more comprehensive multi-label comp-sum losses, adapting comp-sum losses from standard classification to the multi-label learning. We prove that this family of surrogate losses benefits from $H$-consistency bounds, and thus Bayes-consistency, across any general multi-label loss. Our work thus proposes a unified surrogate loss framework benefiting from strong consistency guarantees for any multi-label loss, significantly expanding upon previous work which only established Bayes-consistency and for specific loss functions. Additionally, we adapt constrained losses from standard classification to multi-label constrained losses in a similar way, which also benefit from $H$-consistency bounds and thus Bayes-consistency for any multi-label loss. We further describe efficient gradient computation algorithms for minimizing the multi-label logistic loss",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Mao",
      "Mehryar Mohri",
      "Yutao Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/04b98fd38bd42810d0764cb6c46d10d8-Abstract-Conference.html": {
    "title": "Data Distribution Valuation",
    "volume": "main",
    "abstract": "Data valuation is a class of techniques for quantitatively assessing the value of data for applications like pricing in data marketplaces. Existing data valuation methods define a value for a discrete dataset. However, in many use cases, users are interested in not only the value of the dataset, but that of the distribution from which the dataset was sampled. For example, consider a buyer trying to evaluate whether to purchase data from different vendors. The buyer may observe (and compare) only a small preview sample from each vendor, to decide which vendor's data distribution is most useful to the buyer and purchase. The core question is how should we compare the values of data distributions from their samples? Under a Huber characterization of the data heterogeneity across vendors, we propose a maximum mean discrepancy (MMD)-based valuation method which enables theoretically principled and actionable policies for comparing data distributions from samples. We empirically demonstrate that our method is sample-efficient and effective in identifying valuable data distributions against several existing baselines, on multiple real-world datasets (e.g., network intrusion detection, credit card fraud detection) and downstream applications (classification, regression)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Xu",
      "Shuaiqi Wang",
      "Chuan Sheng Foo",
      "Bryan Kian Hsiang Low",
      "Giulia Fanti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/04badd3b048315c8c3a0ca17eff723d7-Abstract-Conference.html": {
    "title": "AdjointDEIS: Efficient Gradients for Diffusion Models",
    "volume": "main",
    "abstract": "The optimization of the latents and parameters of diffusion models with respect to some differentiable metric defined on the output of the model is a challenging and complex problem. The sampling for diffusion models is done by solving either the probability flow ODE or diffusion SDE wherein a neural network approximates the score function allowing a numerical ODE/SDE solver to be used. However, naive backpropagation techniques are memory intensive, requiring the storage of all intermediate states, and face additional complexity in handling the injected noise from the diffusion term of the diffusion SDE. We propose a novel family of bespoke ODE solvers to the continuous adjoint equations for diffusion models, which we call AdjointDEIS. We exploit the unique construction of diffusion SDEs to further simplify the formulation of the continuous adjoint equations using exponential integrators. Moreover, we provide convergence order guarantees for our bespoke solvers. Significantly, we show that continuous adjoint equations for diffusion SDEs actually simplify to a simple ODE. Lastly, we demonstrate the effectiveness of AdjointDEIS for guided generation with an adversarial attack in the form of the face morphing problem. Our code will be released on our project page https://zblasingame.github.io/AdjointDEIS/",
    "checked": true,
    "id": "14aa1c04b3f9356ef7cdace24a238796e4ea5b31",
    "semantic_title": "adjointdeis: efficient gradients for diffusion models",
    "citation_count": 6,
    "authors": [
      "Zander W. Blasingame",
      "Chen Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/04cc90ec6868b97b7423dc38ced1e35c-Abstract-Conference.html": {
    "title": "Differentially Private Equivalence Testing for Continuous Distributions and Applications",
    "volume": "main",
    "abstract": "We present the first algorithm for testing equivalence between two continuous distributions using differential privacy (DP). Our algorithm is a private version of the algorithm of Diakonikolas et al. The algorithm of Diakonikolas et al uses the data itself to repeatedly discretize the real line so that --- when the two distributions are far apart in ${\\cal A}_k$-norm --- one of the discretized distributions exhibits large $L_2$-norm difference; and upon repeated sampling such large gap would be detected. Designing its private analogue poses two difficulties. First, our DP algorithm can not resample new datapoints as a change to a single datapoint may lead to a very large change in the descretization of the real line. In contrast, the (sorted) index of the discretization point changes only by $1$ between neighboring instances, and so we use a novel algorithm that set the discretization points using random Bernoulli noise, resulting in only a few buckets being affected under the right coupling. Second, our algorithm, which doesn't resample data, requires we also revisit the utility analysis of the original algorithm and prove its correctness w.r.t. the original sorted data; a problem we tackle using sampling a subset of Poisson-drawn size from each discretized bin. Lastly, since any distribution can be reduced to a continuous distribution, our algorithm is successfully carried to multiple other families of distributions and thus has numerous applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Or Sheffet",
      "Daniel Omer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/04d212c4eeeb710f170d47f8d5b9b88a-Abstract-Conference.html": {
    "title": "Reinforcement Learning with Adaptive Regularization for Safe Control of Critical Systems",
    "volume": "main",
    "abstract": "Reinforcement Learning (RL) is a powerful method for controlling dynamic systems, but its learning mechanism can lead to unpredictable actions that undermine the safety of critical systems. Here, we propose RL with Adaptive Regularization (RL-AR), an algorithm that enables safe RL exploration by combining the RL policy with a policy regularizer that hard-codes the safety constraints. RL-AR performs policy combination via a \"focus module,\" which determines the appropriate combination depending on the state—relying more on the safe policy regularizer for less-exploited states while allowing unbiased convergence for well-exploited states. In a series of critical control applications, we demonstrate that RL-AR not only ensures safety during training but also achieves a return competitive with the standards of model-free RL that disregards safety",
    "checked": true,
    "id": "a6af2113ca7a98f299433b17dc7a3365a57e384d",
    "semantic_title": "reinforcement learning with adaptive regularization for safe control of critical systems",
    "citation_count": 3,
    "authors": [
      "Haozhe Tian",
      "Homayoun Hamedmoghadam",
      "Robert Shorten",
      "Pietro Ferraro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/04ea184dfb5f1babb78c093e850a83f9-Abstract-Conference.html": {
    "title": "BERTs are Generative In-Context Learners",
    "volume": "main",
    "abstract": "While in-context learning is commonly associated with causal language models, such as GPT, we demonstrate that this capability also 'emerges' in masked language models. Through an embarrassingly simple inference technique, we enable an existing masked model, DeBERTa, to perform generative tasks without additional training or architectural changes. Our evaluation reveals that the masked and causal language models behave very differently, as they clearly outperform each other on different categories of tasks. These complementary strengths suggest that the field's focus on causal models for in-context learning may be limiting – both architectures can develop these capabilities, but with distinct advantages; pointing toward promising hybrid approaches that combine the strengths of both objectives",
    "checked": true,
    "id": "152d6954e20ffbd879fd831d8647405fe219f370",
    "semantic_title": "berts are generative in-context learners",
    "citation_count": 10,
    "authors": [
      "David Samuel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0506ad3d1bcc8398a920db9340f27fe4-Abstract-Conference.html": {
    "title": "Faster Local Solvers for Graph Diffusion Equations",
    "volume": "main",
    "abstract": "Efficient computation of graph diffusion equations (GDEs), such as Personalized PageRank, Katz centrality, and the Heat kernel, is crucial for clustering, training neural networks, and many other graph-related problems. Standard iterative methods require accessing the whole graph per iteration, making them time-consuming for large-scale graphs. While existing local solvers approximate diffusion vectors through heuristic local updates, they often operate sequentially and are typically designed for specific diffusion types, limiting their applicability. Given that diffusion vectors are highly localizable, as measured by the participation ratio, this paper introduces a novel framework for approximately solving GDEs using a local diffusion process. This framework reveals the suboptimality of existing local solvers. Furthermore, our approach effectively localizes standard iterative solvers by designing simple and provably sublinear time algorithms. These new local solvers are highly parallelizable, making them well-suited for implementation on GPUs. We demonstrate the effectiveness of our framework in quickly obtaining approximate diffusion vectors, achieving up to a hundred-fold speed improvement, and its applicability to large-scale dynamic graphs. Our framework could also facilitate more efficient local message-passing mechanisms for GNNs",
    "checked": true,
    "id": "bd3ef29c2d234ada3d2da25ee6126a72509dc695",
    "semantic_title": "faster local solvers for graph diffusion equations",
    "citation_count": 1,
    "authors": [
      "Jiahe Bai",
      "Baojian Zhou",
      "Deqing Yang",
      "Yanghua Xiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/050f8591be3874b52fdac4e1060eeb29-Abstract-Conference.html": {
    "title": "Beyond Accuracy: Tracking more like Human via Visual Search",
    "volume": "main",
    "abstract": "Human visual search ability enables efficient and accurate tracking of an arbitrary moving target, which is a significant research interest in cognitive neuroscience. The recently proposed Central-Peripheral Dichotomy (CPD) theory sheds light on how humans effectively process visual information and track moving targets in complex environments. However, existing visual object tracking algorithms still fall short of matching human performance in maintaining tracking over time, particularly in complex scenarios requiring robust visual search skills. These scenarios often involve Spatio-Temporal Discontinuities (i.e., STDChallenge), prevalent in long-term tracking and global instance tracking. To address this issue, we conduct research from a human-like modeling perspective: (1) Inspired by the CPD, we pro- pose a new tracker named CPDTrack to achieve human-like visual search ability. The central vision of CPDTrack leverages the spatio-temporal continuity of videos to introduce priors and enhance localization precision, while the peripheral vision improves global awareness and detects object movements. (2) To further evaluate and analyze STDChallenge, we create the STDChallenge Benchmark. Besides, by incorporating human subjects, we establish a human baseline, creating a high- quality environment specifically designed to assess trackers' visual search abilities in videos across STDChallenge. (3) Our extensive experiments demonstrate that the proposed CPDTrack not only achieves state-of-the-art (SOTA) performance in this challenge but also narrows the behavioral differences with humans. Additionally, CPDTrack exhibits strong generalizability across various challenging benchmarks. In summary, our research underscores the importance of human-like modeling and offers strategic insights for advancing intelligent visual target tracking. Code and models are available at https://github.com/ZhangDailing8/CPDTrack",
    "checked": true,
    "id": "97d328352fc2f0099eaea52bf84823949b5174a0",
    "semantic_title": "beyond accuracy: tracking more like human via visual search",
    "citation_count": 3,
    "authors": [
      "Dailing Zhang",
      "Shiyu Hu",
      "Xiaokun Feng",
      "Xuchen Li",
      "wu meiqi",
      "Jing Zhang",
      "Kaiqi Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/051f3997af1dd65da8e14397b6a72f8e-Abstract-Conference.html": {
    "title": "Credit Attribution and Stable Compression",
    "volume": "main",
    "abstract": "Credit attribution is crucial across various fields. In academic research, proper citation acknowledges prior work and establishes original contributions. Similarly, in generative models, such as those trained on existing artworks or music, it is important to ensure that any generated content influenced by these works appropriately credits the original creators.We study credit attribution by machine learning algorithms. We propose new definitions--relaxations of Differential Privacy--that weaken the stability guarantees for a designated subset of $k$ datapoints. These $k$ datapoints can be used non-stably with permission from their owners, potentially in exchange for compensation. Meanwhile, the remaining datapoints are guaranteed to have no significant influence on the algorithm's output.Our framework extends well-studied notions of stability, including Differential Privacy ($k = 0$), differentially private learning with public data (where the $k$ public datapoints are fixed in advance),and stable sample compression (where the $k$ datapoints are selected adaptively by the algorithm).We examine the expressive power of these stability notions within the PAC learning framework, provide a comprehensive characterization of learnability for algorithms adhering to these principles, and propose directions and questions for future research",
    "checked": true,
    "id": "1f59eaf7d57f5cc1e8e4744f934af34670c40729",
    "semantic_title": "credit attribution and stable compression",
    "citation_count": 1,
    "authors": [
      "Roi Livni",
      "Shay Moran",
      "Kobbi Nissim",
      "Chirag Pabbaraju"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0520537ba799d375b8ff5523295c337a-Abstract-Conference.html": {
    "title": "Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration",
    "volume": "main",
    "abstract": "Mobile device operation tasks are increasingly becoming a popular multi-modal AI application scenario. Current Multi-modal Large Language Models (MLLMs), constrained by their training data, lack the capability to function effectively as operation assistants. Instead, MLLM-based agents, which enhance capabilities through tool invocation, are gradually being applied to this scenario. However, the two major navigation challenges in mobile device operation tasks — task progress navigation and focus content navigation — are difficult to effectively solve under the single-agent architecture of existing work. This is due to the overly long token sequences and the interleaved text-image data format, which limit performance. To address these navigation challenges effectively, we propose Mobile-Agent-v2, a multi-agent architecture for mobile device operation assistance. The architecture comprises three agents: planning agent, decision agent, and reflection agent. The planning agent condenses lengthy, interleaved image-text history operations and screens summaries into a pure-text task progress, which is then passed on to the decision agent. This reduction in context length makes it easier for decision agent to navigate the task progress. To retain focus content, we design a memory unit that updates with task progress by decision agent. Additionally, to correct erroneous operations, the reflection agent observes the outcomes of each operation and handles any mistake accordingly. Experimental results indicate that Mobile-Agent-v2 achieves over a 30% improvement in task completion compared to the single-agent architecture of Mobile-Agent. The code is open-sourced at https://github.com/X-PLUG/MobileAgent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyang Wang",
      "Haiyang Xu",
      "Haitao Jia",
      "Xi Zhang",
      "Ming Yan",
      "Weizhou Shen",
      "Ji Zhang",
      "Fei Huang",
      "Jitao Sang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0525fa17a8dbea687359116d01732e12-Abstract-Conference.html": {
    "title": "A Walsh Hadamard Derived Linear Vector Symbolic Architecture",
    "volume": "main",
    "abstract": "Vector Symbolic Architectures (VSAs) are one approach to developing Neuro-symbolic AI, where two vectors in $\\mathbb{R}^d$ are 'bound' together to produce a new vector in the same space. VSAs support the commutativity and associativity of this binding operation, along with an inverse operation, allowing one to construct symbolic-style manipulations over real-valued vectors. Most VSAs were developed before deep learning and automatic differentiation became popular and instead focused on efficacy in hand-designed systems. In this work, we introduce the Hadamard-derived linear Binding (HLB), which is designed to have favorable computational efficiency, and efficacy in classic VSA tasks, and perform well in differentiable systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Mahmudul Alam",
      "Alexander Oberle",
      "Edward Raff",
      "Stella Biderman",
      "Tim Oates",
      "James Holt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0534abc9e6db91683d82186ef0d68202-Abstract-Conference.html": {
    "title": "Autonomous Agents for Collaborative Task under Information Asymmetry",
    "volume": "main",
    "abstract": "Large Language Model Multi-Agent Systems (LLM-MAS) have greatly progressed in solving complex tasks. It communicates among agents within the system to collaboratively solve tasks, under the premise of shared information. However, when agents' collaborations are leveraged to perform multi-person tasks, a new challenge arises due to information asymmetry, since each agent can only access the information of its human user. Previous MAS struggle to complete tasks under this condition. To address this, we propose a new MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems. In iAgents, the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry. iAgents employs a novel agent reasoning mechanism, InfoNav, to navigate agents' communication towards effective information exchange. Together with InfoNav, iAgents organizes human information in a mixed memory to provide agents with accurate and comprehensive information for exchange. Additionally, we introduce InformativeBench, the first benchmark tailored for evaluating LLM agents' task-solving ability under information asymmetry. Experimental results show that iAgents can collaborate within a social network of 140 individuals and 588 relationships, autonomously communicate over 30 turns, and retrieve information from nearly 70,000 messages to complete tasks within 3 minutes",
    "checked": true,
    "id": "c2f0c080588f6622410ef02ef034eae373b43529",
    "semantic_title": "autonomous agents for collaborative task under information asymmetry",
    "citation_count": 6,
    "authors": [
      "Wei Liu",
      "Chenxi Wang",
      "YiFei Wang",
      "Zihao Xie",
      "Rennai Qiu",
      "Yufan Dang",
      "Zhuoyun Du",
      "Weize Chen",
      "Cheng Yang",
      "Chen Qian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/053ee34c0971568bfa5c773015c10502-Abstract-Conference.html": {
    "title": "Retrieval-Augmented Diffusion Models for Time Series Forecasting",
    "volume": "main",
    "abstract": "While time series diffusion models have received considerable focus from many recent works, the performance of existing models remains highly unstable. Factors limiting time series diffusion models include insufficient time series datasets and the absence of guidance. To address these limitations, we propose a Retrieval-Augmented Time series Diffusion model (RATD). The framework of RATD consists of two parts: an embedding-based retrieval process and a reference-guided diffusion model. In the first part, RATD retrieves the time series that are most relevant to historical time series from the database as references. The references are utilized to guide the denoising process in the second part. Our approach allows leveraging meaningful samples within the database to aid in sampling, thus maximizing the utilization of datasets. Meanwhile, this reference-guided mechanism also compensates for the deficiencies of existing time series diffusion models in terms of guidance. Experiments and visualizations on multiple datasets demonstrate the effectiveness of our approach, particularly in complicated prediction tasks. Our code is available at https://github.com/stanliu96/RATD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwei Liu",
      "Ling Yang",
      "Hongyan Li",
      "Shenda Hong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/054e9f9a286671ababa3213d6e59c1c2-Abstract-Conference.html": {
    "title": "The Surprising Effectiveness of SP Voting with Partial Preferences",
    "volume": "main",
    "abstract": "We consider the problem of recovering the ground truth ordering (ranking, top-$k$, or others) over a large number of alternatives. The wisdom of crowd is a heuristic approach based on Condorcet's Jury theorem to address this problem through collective opinions.This approach fails to recover the ground truth when the majority of the crowd is misinformed. The \\emph{surprisingly popular} (SP) algorithm~\\citep{prelec2017solution} is an alternative approach that is able to recover the ground truth even when experts are in minority. The SP algorithm requires the voters to predict other voters' report in the form of a full probability distribution over all rankings of alternatives. However, when the number of alternatives, $m$, is large, eliciting the prediction report or even the vote over $m$ alternatives might be too costly. In this paper, we design a scalable alternative of the SP algorithm which only requires eliciting partial preferences from the voters, and propose new variants of the SP algorithm. In particular, we propose two versions---\\emph{Aggregated-SP} and \\emph{Partial-SP}---that ask voters to report vote and prediction on a subset of size $k$ ($\\ll m$) in terms of top alternative, partial rank, or an approval set. Through a large-scale crowdsourcing experiment on MTurk, we show that both of our approaches outperform conventional preference aggregation algorithms for the recovery of ground truth rankings, when measured in terms of Kendall-Tau distance and Spearman's $\\rho$. We further analyze the collected data and demonstrate that voters' behavior in the experiment, including the minority of the experts, and the SP phenomenon, can be correctly simulated by a concentric mixtures of Mallows model. Finally, we provide theoretical bounds on the sample complexity of SP algorithms with partial rankings to demonstrate the theoretical guarantees of the proposed methods",
    "checked": true,
    "id": "e8feaaaef87f582b66daac5841f0c3c19f9e4989",
    "semantic_title": "the surprising effectiveness of sp voting with partial preferences",
    "citation_count": 2,
    "authors": [
      "Hadi Hosseini",
      "Debmalya Mandal",
      "Amrit Puhan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/054f771d614df12fe8def8ecdbe4e8e1-Abstract-Conference.html": {
    "title": "Flatten Anything: Unsupervised Neural Surface Parameterization",
    "volume": "main",
    "abstract": "Surface parameterization plays an essential role in numerous computer graphics and geometry processing applications. Traditional parameterization approaches are designed for high-quality meshes laboriously created by specialized 3D modelers, thus unable to meet the processing demand for the current explosion of ordinary 3D data. Moreover, their working mechanisms are typically restricted to certain simple topologies, thus relying on cumbersome manual efforts (e.g., surface cutting, part segmentation) for pre-processing. In this paper, we introduce the Flatten Anything Model (FAM), an unsupervised neural architecture to achieve global free-boundary surface parameterization via learning point-wise mappings between 3D points on the target geometric surface and adaptively-deformed UV coordinates within the 2D parameter domain. To mimic the actual physical procedures, we ingeniously construct geometrically-interpretable sub-networks with specific functionalities of surface cutting, UV deforming, unwrapping, and wrapping, which are assembled into a bi-directional cycle mapping framework. Compared with previous methods, our FAM directly operates on discrete surface points without utilizing connectivity information, thus significantly reducing the strict requirements for mesh quality and even applicable to unstructured point cloud data. More importantly, our FAM is fully-automated without the need for pre-cutting and can deal with highly-complex topologies, since its learning process adaptively finds reasonable cutting seams and UV boundaries. Extensive experiments demonstrate the universality, superiority, and inspiring potential of our proposed neural surface parameterization paradigm. Our code is available at https://github.com/keeganhk/FlattenAnything",
    "checked": true,
    "id": "e390b1104c26e4b8fa42e214298776868b7f5b03",
    "semantic_title": "flatten anything: unsupervised neural surface parameterization",
    "citation_count": 4,
    "authors": [
      "Qijian Zhang",
      "Junhui Hou",
      "Wenping Wang",
      "Ying He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/056521a35eacd9d2127b66a7d3c499c5-Abstract-Conference.html": {
    "title": "BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling",
    "volume": "main",
    "abstract": "This paper concerns the problem of aligning samples from large language models to human preferences using *best-of-$n$* sampling, where we draw $n$ samples, rank them, and return the best one. We consider two fundamental problems. First: what is the relationship between best-of-$n$ and other (RLHF-type) approaches to aligning LLMs? In particular, when should one be preferred to the other? We show that the best-of-$n$ sampling distribution is essentially equivalent to the policy learned by RLHF if we apply a particular monotone transformation to the reward function. Moreover, we show that this transformation yields the best possible trade-off between win-rate against the base model vs KL distance from the base model. Then, best-of-$n$ is a Pareto-optimal win-rate vs KL solution.The second problem we consider is how to fine-tune a model to mimic the best-of-$n$ sampling distribution, to avoid drawing $n$ samples for each inference. We derive *BonBon Alignment* as a method for achieving this. Experiments show that BonBon alignment yields a model that achieves high win rates while minimally affecting off-target aspects of the generations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Gui",
      "Cristina Garbacea",
      "Victor Veitch"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/058373e239c542eae6cb796e4dc521da-Abstract-Conference.html": {
    "title": "ReMoDetect: Reward Models Recognize Aligned LLM's Generations",
    "volume": "main",
    "abstract": "The remarkable capabilities and easy accessibility of large language models (LLMs) have significantly increased societal risks (e.g., fake news generation), necessitating the development of LLM-generated text (LGT) detection methods for safe usage. However, detecting LGTs is challenging due to the vast number of LLMs, making it impractical to account for each LLM individually; hence, it is crucial to identify the common characteristics shared by these models. In this paper, we draw attention to a common feature of recent powerful LLMs, namely the alignment training, i.e., training LLMs to generate human-preferable texts. Our key finding is that as these aligned LLMs are trained to maximize the human preferences, they generate texts with higher estimated preferences even than human-written texts; thus, such texts are easily detected by using the reward model (i.e., an LLM trained to model human preference distribution). Based on this finding, we propose two training schemes to further improve the detection ability of the reward model, namely (i) continual preference fine-tuning to make reward model prefer aligned LGTs even further and (ii) reward modeling of Human/LLM mixed texts (a rephrased texts from human-written texts using aligned LLMs), which serves as a median preference text corpus between LGTs and human-written texts to learn the decision boundary better. We provide an extensive evaluation by considering six text domains across twelve aligned LLMs, where our method demonstrates state-of-the-art results",
    "checked": true,
    "id": "33fe80c0857b2eac6466d1b31997e2f1ee879589",
    "semantic_title": "remodetect: reward models recognize aligned llm's generations",
    "citation_count": 1,
    "authors": [
      "Hyunseok Lee",
      "Jihoon Tack",
      "Jinwoo Shin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/059445c2d5b3ef918079851628fef1d6-Abstract-Conference.html": {
    "title": "Analyzing & Reducing the Need for Learning Rate Warmup in GPT Training",
    "volume": "main",
    "abstract": "Learning Rate Warmup is a popular heuristic for training neural networks, especially at larger batch sizes, despite limited understanding of its benefits. Warmup decreases the update size $\\Delta \\mathbf{w}_t = \\eta_t \\mathbf{u}_t$ early in training by using lower values for the learning rate $\\eta_t$. In this work we argue that warmup benefits training by keeping the overall size of $\\Delta \\mathbf{w}_t$ limited, counteracting large initial values of $\\mathbf{u}_t$. Focusing on small-scale GPT training with AdamW/Lion, we explore the following question: *Why and by which criteria are early updates $\\mathbf{u}_t$ too large?* We analyze different metrics for the update size including the $\\ell_2$-norm, resulting directional change, and impact on the representations of the network, providing a new perspective on warmup. In particular, we find that warmup helps counteract large angular updates as well as a limited critical batch size early in training. Finally, we show that the need for warmup can be significantly reduced or eliminated by modifying the optimizer to explicitly normalize $\\mathbf{u}_t$ based on the aforementioned metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atli Kosson",
      "Bettina Messmer",
      "Martin Jaggi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/05a0a9fc9566b7dbf471f2f6497c74b7-Abstract-Conference.html": {
    "title": "Meta-Reinforcement Learning with Universal Policy Adaptation: Provable Near-Optimality under All-task Optimum Comparator",
    "volume": "main",
    "abstract": "Meta-reinforcement learning (Meta-RL) has attracted attention due to its capability to enhance reinforcement learning (RL) algorithms, in terms of data efficiency and generalizability. In this paper, we develop a bilevel optimization framework for meta-RL (BO-MRL) to learn the meta-prior for task-specific policy adaptation, which implements multiple-step policy optimization on one-time data collection. Beyond existing meta-RL analyses, we provide upper bounds of the expected optimality gap over the task distribution. This metric measures the distance of the policy adaptation from the learned meta-prior to the task-specific optimum, and quantifies the model's generalizability to the task distribution. We empirically validate the correctness of the derived upper bounds and demonstrate the superior effectiveness of the proposed algorithm over benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Xu",
      "Minghui Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/05a2d9ef0ae6f249737c1e4cce724a0c-Abstract-Conference.html": {
    "title": "Mutual Information Estimation via Normalizing Flows",
    "volume": "main",
    "abstract": "We propose a novel approach to the problem of mutual information (MI) estimation via introducing a family of estimators based on normalizing flows. The estimator maps original data to the target distribution, for which MI is easier to estimate. We additionally explore the target distributions with known closed-form expressions for MI. Theoretical guarantees are provided to demonstrate that our approach yields MI estimates for the original data. Experiments with high-dimensional data are conducted to highlight the practical advantages of the proposed method",
    "checked": true,
    "id": "b769565643cc6c36d9d3cf5ba48bc15e5a3c4dea",
    "semantic_title": "mutual information estimation via normalizing flows",
    "citation_count": 8,
    "authors": [
      "Ivan Butakov",
      "Aleksandr Tolmachev",
      "Sofia Malanchuk",
      "Anna Neopryatnaya",
      "Alexey Frolov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/05aedcaf4bc6e78a5e22b4cf9114c5e8-Abstract-Conference.html": {
    "title": "Few-Shot Adversarial Prompt Learning on Vision-Language Models",
    "volume": "main",
    "abstract": "The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by aligning adversarial visual features with text supervision. However, in practice, they are still unsatisfactory due to several issues, including heavy adaptation cost, suboptimal text supervision, and uncontrolled natural generalization capacity. In this paper, to address these issues, we propose a few-shot adversarial prompt framework where adapting input sequences with limited data makes significant adversarial robustness improvement. Specifically, we achieve this by providing adversarially correlated text supervision that is end-to-end learned from adversarial examples. We also propose a novel training objective that enhances the consistency of multi-modal features while encourages differentiated uni-modal features between natural and adversarial examples. The proposed framework gives access to learn adversarial text supervision, which provides superior cross-modal adversarial alignment and matches state-of-the-art zero-shot adversarial robustness with only 1\\% training data. Code is available at: https://github.com/lionel-w2/FAP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Zhou",
      "Xiaobo Xia",
      "Zhiwei Lin",
      "Bo Han",
      "Tongliang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/05b12f103c9e613efc4c85674cdc9066-Abstract-Conference.html": {
    "title": "UniSDF: Unifying Neural Representations for High-Fidelity 3D Reconstruction of Complex Scenes with Reflections",
    "volume": "main",
    "abstract": "Neural 3D scene representations have shown great potential for 3D reconstruction from 2D images. However, reconstructing real-world captures of complex scenes still remains a challenge. Existing generic 3D reconstruction methods often struggle to represent fine geometric details and do not adequately model reflective surfaces of large-scale scenes. Techniques that explicitly focus on reflective surfaces can model complex and detailed reflections by exploiting better reflection parameterizations. However, we observe that these methods are often not robust in real scenarios where non-reflective as well as reflective components are present. In this work, we propose UniSDF, a general purpose 3D reconstruction method that can reconstruct large complex scenes with reflections. We investigate both camera view as well as reflected view-based color parameterization techniques and find that explicitly blending these representations in 3D space enables reconstruction of surfaces that are more geometrically accurate, especially for reflective surfaces. We further combine this representation with a multi-resolution grid backbone that is trained in a coarse-to-fine manner, enabling faster reconstructions than prior methods. Extensive experiments on object-level datasets DTU, Shiny Blender as well as unbounded datasets Mip-NeRF 360 and Ref-NeRF real demonstrate that our method is able to robustly reconstruct complex large-scale scenes with fine details and reflective surfaces, leading to the best overall performance. Project page: https://fangjinhuawang.github.io/UniSDF",
    "checked": true,
    "id": "5a570c661ef6aa4492b37ce329686683355c573c",
    "semantic_title": "unisdf: unifying neural representations for high-fidelity 3d reconstruction of complex scenes with reflections",
    "citation_count": 18,
    "authors": [
      "Fangjinhua Wang",
      "Marie-Julie Rakotosaona",
      "Michael Niemeyer",
      "Richard Szeliski",
      "Marc Pollefeys",
      "Federico Tombari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/05b2ea5d57ed04c1dc105658e87b137e-Abstract-Conference.html": {
    "title": "Goal Reduction with Loop-Removal Accelerates RL and Models Human Brain Activity in Goal-Directed Learning",
    "volume": "main",
    "abstract": "Goal-directed planning presents a challenge for classical RL algorithms due to the vastness of the combinatorial state and goal spaces, while humans and animals adapt to complex environments, especially with diverse, non-stationary objectives, often employing intermediate goals for long-horizon tasks.Here, we propose a goal reduction mechanism for effectively deriving subgoals from arbitrary and distant original goals, using a novel loop-removal technique.The product of the method, called goal-reducer, distills high-quality subgoals from a replay buffer, all without the need for prior global environmental knowledge.Simulations show that the goal-reducer can be integrated into RL frameworks like Deep Q-learning and Soft Actor-Critic.It accelerates performance in both discrete and continuous action space tasks, such as grid world navigation and robotic arm manipulation, relative to the corresponding standard RL models.Moreover, the goal-reducer, when combined with a local policy, without iterative training, outperforms its integrated deep RL counterparts in solving a navigation task.This goal reduction mechanism also models human problem-solving.Comparing the model's performance and activation with human behavior and fMRI data in a treasure hunting task, we found matching representational patterns between an goal-reducer agent's components and corresponding human brain areas, particularly the vmPFC and basal ganglia. The results suggest that humans may use a similar computational framework for goal-directed behaviors",
    "checked": true,
    "id": "8b94bf9260f849bb8e614909858775e30a6c847d",
    "semantic_title": "goal reduction with loop-removal accelerates rl and models human brain activity in goal-directed learning",
    "citation_count": 0,
    "authors": [
      "Huzi Cheng",
      "Joshua Brown"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/05b69cc4c8ff6e24c5de1ecd27223d37-Abstract-Conference.html": {
    "title": "QBB: Quantization with Binary Bases for LLMs",
    "volume": "main",
    "abstract": "Current post-training quantization methods for LLMs compress the weights down to 4-bits, with moderate to low degradation in accuracy. However, further reducing the number of bits or accelerating the network while avoiding large accuracy drops, especially for smaller, sub 7B models, remains an actively researched and open problem. To address this, in this work, we introduce Quantization with Binary Bases (QBB), a new approach for low-bit quantization that effectively removes (nearly) all multiplications, reducing the implementation to summations. Our novel approach works by decomposing the original weights into a set of binary (1-bit) matrices using an iterative process. For a given layer, starting from a weight matrix, we first construct an initial approximation using an analytical solution, where each new binary matrix, paired with a scaling vector, approximates the residual error of the previous estimation. Secondly, using gradient descent and a progressive learning curriculum, we find the optimal set of binary matrices and scaling vectors that minimize the $\\ell_2$ distance between the produced approximation and original weights. Thirdly, as previous steps are input agnostic, we holistically optimize the scaling vectors alone, calibrating them in student-teacher fashion, with the teacher providing both the data, by autoregressive generation starting from a random token, and the target logits. When evaluated across multiple LLM families, our approach matches and outperforms all prior works, setting a new state-of-the-art result using a summation-only based approach",
    "checked": true,
    "id": "0630efa78f0fc0fc303e873e09005a1a6007a020",
    "semantic_title": "qbb: quantization with binary bases for llms",
    "citation_count": 3,
    "authors": [
      "Adrian Bulat",
      "Yassine Ouali",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/05cdc7feee41e3572a9a3f4acb773891-Abstract-Conference.html": {
    "title": "Disentangling and mitigating the impact of task similarity for continual learning",
    "volume": "main",
    "abstract": "Continual learning of partially similar tasks poses a challenge for artificial neural networks, as task similarity presents both an opportunity for knowledge transfer and a risk of interference and catastrophic forgetting.However, it remains unclear how task similarity in input features and readout patterns influences knowledge transfer and forgetting, as well as how they interact with common algorithms for continual learning.Here, we develop a linear teacher-student model with latent structure and show analytically that high input feature similarity coupled with low readout similarity is catastrophic for both knowledge transfer and retention. Conversely, the opposite scenario is relatively benign. Our analysis further reveals that task-dependent activity gating improves knowledge retention at the expense of transfer, while task-dependent plasticity gating does not affect either retention or transfer performance at the over-parameterized limit. In contrast, weight regularization based on the Fisher information metric significantly improves retention, regardless of task similarity, without compromising transfer performance. Nevertheless, its diagonal approximation and regularization in the Euclidean space are much less robust against task similarity. We demonstrate consistent results in a permuted MNIST task with latent variables. Overall, this work provides insights into when continual learning is difficult and how to mitigate it",
    "checked": true,
    "id": "26c4c095b424e73e11b0d6a9ef43892589362194",
    "semantic_title": "disentangling and mitigating the impact of task similarity for continual learning",
    "citation_count": 6,
    "authors": [
      "Naoki Hiratani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/05d42b7bd130ecbc57fdf02cbdcd370e-Abstract-Conference.html": {
    "title": "Warm-up Free Policy Optimization: Improved Regret in Linear Markov Decision Processes",
    "volume": "main",
    "abstract": "Policy Optimization (PO) methods are among the most popular Reinforcement Learning (RL) algorithms in practice. Recently, Sherman et al. [2023a] proposed a PO-based algorithm with rate-optimal regret guarantees under the linear Markov Decision Process (MDP) model. However, their algorithm relies on a costly pure exploration warm-up phase that is hard to implement in practice. This paper eliminates this undesired warm-up phase, replacing it with a simple and efficient contraction mechanism. Our PO algorithm achieves rate-optimal regret with improved dependence on the other parameters of the problem (horizon and function approximation dimension) in two fundamental settings: adversarial losses with full-information feedback and stochastic losses with bandit feedback",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asaf Cassel",
      "Aviv Rosenberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/05d6b5b6901fb57d2c287e1d3ce6d63c-Abstract-Conference.html": {
    "title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization",
    "volume": "main",
    "abstract": "Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As batch size, context length, or model size increases, the size of key and value (KV) cache quickly becomes the main contributor to GPU memory usage and the bottleneck of inference latency and throughput. Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths. Currently, KV cache quantization is performed per-channel or per-token independently. Our analysis shows that distinct channels of a key/value activation embedding are highly interdependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropy, which implies that per-channel independent quantization is sub-optimal. To mitigate this sub-optimality, we propose Coupled Quantization (CQ), which couples multiple key/value channels together for quantization to exploit their interdependence and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ compares favorably with existing baselines in preserving model quality, and improves inference throughput by 1.4–3.5$\\times$ relative to the uncompressed baseline. Furthermore, we demonstrate that CQ can preserve model quality reasonably with KV cache quantized down to 1 bit",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Zhang",
      "Jonah Yi",
      "Zhaozhuo Xu",
      "Anshumali Shrivastava"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/05fbf28602c5c2c994499db18363fbbb-Abstract-Conference.html": {
    "title": "Unified Insights: Harnessing Multi-modal Data for Phenotype Imputation via View Decoupling",
    "volume": "main",
    "abstract": "Phenotype imputation plays a crucial role in improving comprehensive and accurate medical evaluation, which in turn can optimize patient treatment and bolster the reliability of clinical research. Despite the adoption of various techniques, multi-modal biological data, which can provide crucial insights into a patient's overall health, is often overlooked. With multi-modal biological data, patient characterization can be enriched from two distinct views: the biological view and the phenotype view. However, the heterogeneity and imprecise nature of the multimodal data still pose challenges in developing an effective method to model from two views. In this paper, we propose a novel framework to incorporate multi-modal biological data via view decoupling. Specifically, we segregate the modeling of biological data from phenotype data in a graph-based learning framework. From the biological view, the latent factors in biological data are discovered to model patient correlation. From the phenotype view, phenotype co-occurrence can be modeled to reveal patterns across patients. Then patients are encoded from these two distinct views. To mitigate the influence of noise and irrelevant information in biological data, we devise a cross-view contrastive knowledge distillation aimed at distilling insights from the biological view to enhance phenotype imputation. We show that phenotype imputation with the proposed model significantly outperforms the state-of-the-art models on the real-world biomedical database",
    "checked": true,
    "id": "5cb09b3b2eb5e3e1cfc5efd957ed13d86ffdb0c6",
    "semantic_title": "unified insights: harnessing multi-modal data for phenotype imputation via view decoupling",
    "citation_count": 0,
    "authors": [
      "Qiannan Zhang",
      "Weishen Pan",
      "Zilong Bai",
      "Chang Su",
      "Fei Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/060f64f690417a5cc6a882479478fd96-Abstract-Conference.html": {
    "title": "Optimal Private and Communication Constraint Distributed Goodness-of-Fit Testing for Discrete Distributions in the Large Sample Regime",
    "volume": "main",
    "abstract": "We study distributed goodness-of-fit testing for discrete distribution under bandwidth and differential privacy constraints. Information constraint distributed goodness-of-fit testing is a problem that has received considerable attention recently. The important case of discrete distributions is theoretically well understood in the classical case where all data is available in one \"central\" location. In a federated setting, however, data is distributed across multiple \"locations\" (e.g. servers) and cannot readily be shared due to e.g. bandwidth or privacy constraints that each server needs to satisfy. We show how recently derived results for goodness-of-fit testing for the mean of a multivariate Gaussian model extend to the discrete distributions, by leveraging Le Cam's theory of statistical equivalence. In doing so, we derive matching minimax upper- and lower-bounds for the goodness-of-fit testing for discrete distributions under bandwidth or privacy constraints in the regime where number of samples held locally are large",
    "checked": true,
    "id": "cdf5372f216125a95cb21af16463522ce53d2e62",
    "semantic_title": "optimal private and communication constraint distributed goodness-of-fit testing for discrete distributions in the large sample regime",
    "citation_count": 1,
    "authors": [
      "Lasse Vuursteen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/061d5d1b7d97117764f205d4e038f9eb-Abstract-Conference.html": {
    "title": "Concentrate Attention: Towards Domain-Generalizable Prompt Optimization for Language Models",
    "volume": "main",
    "abstract": "Recent advances in prompt optimization have notably enhanced the performance of pre-trained language models (PLMs) on downstream tasks. However, the potential of optimized prompts on domain generalization has been under-explored. To explore the nature of prompt generalization on unknown domains, we conduct pilot experiments and find that (i) Prompts gaining more attention weight from PLMs' deep layers are more generalizable and (ii) Prompts with more stable attention distributions in PLMs' deep layers are more generalizable. Thus, we offer a fresh objective towards domain-generalizable prompts optimization named ''Concentration'', which represents the ''lookback'' attention from the current decoding token to the prompt tokens, to increase the attention strength on prompts and reduce the fluctuation of attention distribution.We adapt this new objective to popular soft prompt and hard prompt optimization methods, respectively. Extensive experiments demonstrate that our idea improves comparison prompt optimization methods by 1.42% for soft prompt generalization and 2.16% for hard prompt generalization in accuracy on the multi-source domain generalization setting, while maintaining satisfying in-domain performance. The promising results validate the effectiveness of our proposed prompt optimization objective and provide key insights into domain-generalizable prompts",
    "checked": true,
    "id": "a412903303d73ff6ae617e32b85678704085432e",
    "semantic_title": "concentrate attention: towards domain-generalizable prompt optimization for language models",
    "citation_count": 4,
    "authors": [
      "Chengzhengxu Li",
      "Xiaoming Liu",
      "Zhaohan Zhang",
      "Yichen Wang",
      "Chen Liu",
      "Yu Lan",
      "Chao Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0626822954674a06ccd9c234e3f0d572-Abstract-Conference.html": {
    "title": "Training for Stable Explanation for Free",
    "volume": "main",
    "abstract": "To foster trust in machine learning models, explanations must be faithful and stable for consistent insights. Existing relevant works rely on the $\\ell_p$ distance for stability assessment, which diverges from human perception. Besides, existing adversarial training (AT) associated with intensive computations may lead to an arms race. To address these challenges, we introduce a novel metric to assess the stability of top-$k$ salient features. We introduce R2ET which trains for stable explanation by efficient and effective regularizer,and analyze R2ET by multi-objective optimization to prove numerical and statistical stability of explanations. Moreover, theoretical connections between R2ET and certified robustness justify R2ET's stability in all attacks. Extensive experiments across various data modalities and model architectures show that R2ET achieves superior stability against stealthy attacks, and generalizes effectively across different explanation methods. The code can be found at https://github.com/ccha005/R2ET",
    "checked": true,
    "id": "05c8ecb2d2cf26ad84cf02eec12e9d0943cd6845",
    "semantic_title": "training for stable explanation for free",
    "citation_count": 0,
    "authors": [
      "Chao Chen",
      "Chenghua Guo",
      "Rufeng Chen",
      "Guixiang Ma",
      "Ming Zeng",
      "Xiangwen Liao",
      "Xi Zhang",
      "Sihong Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/06297213eca57fa6eadf8d87caa21b3c-Abstract-Conference.html": {
    "title": "NeuralSolver: Learning Algorithms For Consistent and Efficient Extrapolation Across General Tasks",
    "volume": "main",
    "abstract": "We contribute NeuralSolver, a novel recurrent solver that can efficiently and consistently extrapolate, i.e., learn algorithms from smaller problems (in terms of observation size) and execute those algorithms in large problems. Contrary to previous recurrent solvers, NeuralSolver can be naturally applied in both same-size problems, where the input and output sizes are the same, and in different-size problems, where the size of the input and output differ. To allow for this versatility, we design NeuralSolver with three main components: a recurrent module, that iteratively processes input information at different scales, a processing module, responsible for aggregating the previously processed information, and a curriculum-based training scheme, that improves the extrapolation performance of the method.To evaluate our method we introduce a set of novel different-size tasks and we show that NeuralSolver consistently outperforms the prior state-of-the-art recurrent solvers in extrapolating to larger problems, considering smaller training problems and requiring less parameters than other approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bernardo Esteves",
      "Miguel Vasco",
      "Francisco S. Melo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/064ae24cdbb3eaacc801ee7f4fe0e4f2-Abstract-Conference.html": {
    "title": "Feint Behaviors and Strategies: Formalization, Implementation and Evaluation",
    "volume": "main",
    "abstract": "Feint behaviors refer to a set of deceptive behaviors in a nuanced manner, which enable players to obtain temporal and spatial advantages over opponents in competitive games. Such behaviors are crucial tactics in most competitive multi-player games (e.g., boxing, fencing, basketball, motor racing, etc.). However, existing literature does not provide a comprehensive (and/or concrete) formalization for Feint behaviors, and their implications on game strategies. In this work, we introduce the first comprehensive formalization of Feint behaviors at both action-level and strategy-level, and provide concrete implementation and quantitative evaluation of them in multi-player games. The key idea of our work is to (1) allow automatic generation of Feint behaviors via Palindrome-directed templates, combine them into meaningful behavior sequences via a Dual-Behavior Model; (2) concertize the implications from our formalization of Feint on game strategies, in terms of temporal, spatial, and their collective impacts respectively; and (3) provide a unified implementation scheme of Feint behaviors in existing MARL frameworks. The experimental results show that our design of Feint behaviors can (1) greatly improve the game reward gains; (2) significantly improve the diversity of Multi-Player Games; and (3) only incur negligible overheads in terms of time consumption",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Liu",
      "Xiangjun Peng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/064f6bcd7d3c72fb187bfca35ba2bfd4-Abstract-Conference.html": {
    "title": "LT-Defense: Searching-free Backdoor Defense via Exploiting the Long-tailed Effect",
    "volume": "main",
    "abstract": "Language models have shown vulnerability against backdoor attacks, threatening the security of services based on them. To mitigate the threat, existing solutions attempted to search for backdoor triggers, which can be time-consuming when handling a large search space. Looking into the attack process, we observe that poisoned data will create a long-tailed effect in the victim model, causing the decision boundary to shift towards the attack targets. Inspired by this observation, we introduce LT-Defense, the first searching-free backdoor defense via exploiting the long-tailed effect. Specifically, LT-Defense employs a small set of clean examples and two metrics to distinguish backdoor-related features in the target model. Upon detecting a backdoor model, LT-Defense additionally provides test-time backdoor freezing and attack target prediction. Extensive experiments demonstrate the effectiveness of LT-Defense in both detection accuracy and efficiency, e.g., in task-agnostic scenarios, LT-Defense achieves 98% accuracy across 1440 models with less than 1% of the time cost of state-of-the-art solutions",
    "checked": true,
    "id": "4d69376b13c9fbb5a6357b07ab6020053f35d9eb",
    "semantic_title": "lt-defense: searching-free backdoor defense via exploiting the long-tailed effect",
    "citation_count": 4,
    "authors": [
      "Yixiao Xu",
      "Binxing Fang",
      "Mohan Li",
      "Keke Tang",
      "Zhihong Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/067437c6d5d0369b6d09200bef89715b-Abstract-Conference.html": {
    "title": "HyperLogic: Enhancing Diversity and Accuracy in Rule Learning with HyperNets",
    "volume": "main",
    "abstract": "Exploring the integration of if-then logic rules within neural network architectures presents an intriguing area. This integration seamlessly transforms the rule learning task into neural network training using backpropagation and stochastic gradient descent. From a well-trained sparse and shallow neural network, one can interpret each layer and neuron through the language of logic rules, and a global explanatory rule set can be directly extracted. However, ensuring interpretability may impose constraints on the flexibility, depth, and width of neural networks. In this paper, we propose HyperLogic: a novel framework leveraging hypernetworks to generate weights of the main network. HyperLogic can unveil multiple diverse rule sets, each capable of capturing heterogeneous patterns in data. This provides a simple yet effective method to increase model flexibility and preserve interpretability. We theoretically analyzed the benefits of the HyperLogic by examining the approximation error and generalization capabilities under two types of regularization terms: sparsity and diversity regularizations. Experiments on real data demonstrate that our method can learn more diverse, accurate, and concise rules",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yang",
      "Wendi Ren",
      "Shuang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/06a52a54c8ee03cd86771136bc91eb1f-Abstract-Conference.html": {
    "title": "CAT: Coordinating Anatomical-Textual Prompts for Multi-Organ and Tumor Segmentation",
    "volume": "main",
    "abstract": "Existing promptable segmentation methods in the medical imaging field primarily consider either textual or visual prompts to segment relevant objects, yet they often fall short when addressing anomalies in medical images, like tumors, which may vary greatly in shape, size, and appearance. Recognizing the complexity of medical scenarios and the limitations of textual or visual prompts, we propose a novel dual-prompt schema that leverages the complementary strengths of visual and textual prompts for segmenting various organs and tumors. Specifically, we introduce $\\textbf{\\textit{CAT}}$, an innovative model that $\\textbf{C}$oordinates $\\textbf{A}$natomical prompts derived from 3D cropped images with $\\textbf{T}$extual prompts enriched by medical domain knowledge. The model architecture adopts a general query-based design, where prompt queries facilitate segmentation queries for mask prediction. To synergize two types of prompts within a unified framework, we implement a ShareRefiner, which refines both segmentation and prompt queries while disentangling the two types of prompts. Trained on a consortium of 10 public CT datasets, $\\textbf{\\textit{CAT}}$ demonstrates superior performance in multiple segmentation tasks. Further validation on a specialized in-house dataset reveals the remarkable capacity of segmenting tumors across multiple cancer stages. This approach confirms that coordinating multimodal prompts is a promising avenue for addressing complex scenarios in the medical domain",
    "checked": true,
    "id": "c0176b1ed6c63ff1ecbf6dbaa6b7430ddee73116",
    "semantic_title": "cat: coordinating anatomical-textual prompts for multi-organ and tumor segmentation",
    "citation_count": 7,
    "authors": [
      "Zhongzhen Huang",
      "Yankai Jiang",
      "Rongzhao Zhang",
      "Shaoting Zhang",
      "Xiaofan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/06cbd2e81dfbd3bb4cb0abce95b32584-Abstract-Conference.html": {
    "title": "Optimizing Automatic Differentiation with Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "Computing Jacobians with automatic differentiation is ubiquitous in many scientific domains such as machine learning, computational fluid dynamics, robotics and finance. Even small savings in the number of computations or memory usage in Jacobian computations can already incur massive savings in energy consumption and runtime. While there exist many methods that allow for such savings, they generally trade computational efficiency for approximations of the exact Jacobian.In this paper, we present a novel method to optimize the number of necessary multiplications for Jacobian computation by leveraging deep reinforcement learning (RL) and a concept called cross-country elimination while still computing the exact Jacobian. Cross-country elimination is a framework for automatic differentiation that phrases Jacobian accumulation as ordered elimination of all vertices on the computational graph where every elimination incurs a certain computational cost.Finding the optimal elimination order that minimizes the number of necessary multiplications can be seen as a single player game which in our case is played by an RL agent.We demonstrate that this method achieves up to 33% improvements over state-of-the-art methods on several relevant tasks taken from relevant domains.Furthermore, we show that these theoretical gains translate into actual runtime improvements by providing a cross-country elimination interpreter in JAX that can execute the obtained elimination orders",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jamie Lohoff",
      "Emre Neftci"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/06cf4bae7ccb6ea37b968a394edc2e33-Abstract-Conference.html": {
    "title": "DiGRAF: Diffeomorphic Graph-Adaptive Activation Function",
    "volume": "main",
    "abstract": "In this paper, we propose a novel activation function tailored specifically for graph data in Graph Neural Networks (GNNs). Motivated by the need for graph-adaptive and flexible activation functions, we introduce DiGRAF, leveraging Continuous Piecewise-Affine Based (CPAB) transformations, which we augment with an additional GNN to learn a graph-adaptive diffeomorphic activation function in an end-to-end manner. In addition to its graph-adaptivity and flexibility, DiGRAF also possesses properties that are widely recognized as desirable for activation functions, such as differentiability, boundness within the domain, and computational efficiency. We conduct an extensive set of experiments across diverse datasets and tasks, demonstrating a consistent and superior performance of DiGRAF compared to traditional and graph-specific activation functions, highlighting its effectiveness as an activation function for GNNs. Our code is available at https://github.com/ipsitmantri/DiGRAF",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krishna Sri Ipsit Mantri",
      "Xinzhi Wang",
      "Carola-Bibiane Schönlieb",
      "Bruno Ribeiro",
      "Beatrice Bevilacqua",
      "Moshe Eliasof"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/06dc3d10e257563416d838f377d31b86-Abstract-Conference.html": {
    "title": "Fair Secretaries with Unfair Predictions",
    "volume": "main",
    "abstract": "Algorithms with predictions is a recent framework for decision-making under uncertainty that leverages the power of machine-learned predictions without making any assumption about their quality. The goal in this framework is for algorithms to achieve an improved performance when the predictions are accurate while maintaining acceptable guarantees when the predictions are erroneous. A serious concern with algorithms that use predictions is that these predictions can be biased and, as a result, cause the algorithm to make decisions that are deemed unfair. We show that this concern manifests itself in the classical secretary problem in the learning-augmented setting---the state-of-the-art algorithm can have zero probability of accepting the best candidate, which we deem unfair, despite promising to accept a candidate whose expected value is at least $\\max\\{\\Omega (1) , 1 - O(\\varepsilon)\\}$ times the optimal value, where $\\varepsilon$ is the prediction error.We show how to preserve this promise while also guaranteeing to accept the best candidate with probability $\\Omega(1)$. Our algorithm and analysis are based on a new ``pegging'' idea that diverges from existing works and simplifies/unifies some of their results. Finally, we extend to the $k$-secretary problem and complement our theoretical analysis with experiments",
    "checked": true,
    "id": "0f7e427b9c70adf021d0e235b4b6cc3de6a14d78",
    "semantic_title": "fair secretaries with unfair predictions",
    "citation_count": 1,
    "authors": [
      "Eric Balkanski",
      "Will Ma",
      "Andreas Maggiori"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/06e2dd57e90a736a5a1fd3bb2bf95c6c-Abstract-Conference.html": {
    "title": "Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity",
    "volume": "main",
    "abstract": "We consider nonconvex stochastic optimization problems in the asynchronous centralized distributed setup where the communication times from workers to a server can not be ignored, and the computation and communication times are potentially different for all workers. Using an unbiassed compression technique, we develop a new method—Shadowheart SGD—that provably improves the time complexities of all previous centralized methods. Moreover, we show that the time complexity of Shadowheart SGD is optimal in the family of centralized methods with compressed communication. We also consider the bidirectional setup, where broadcasting from the server to the workers is non-negligible, and develop a corresponding method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Tyurin",
      "Marta Pozzi",
      "Ivan Ilin",
      "Peter Richtarik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/06e9029d3d4d6cee71c5d9b8502f891b-Abstract-Conference.html": {
    "title": "Testably Learning Polynomial Threshold Functions",
    "volume": "main",
    "abstract": "Rubinfeld \\& Vasilyan recently introduced the framework of *testable learning* as an extension of the classical agnostic model. It relaxes distributional assumptions which are difficult to verify by conditions that can be checked efficiently by a *tester*. The tester has to accept whenever the data truly satisfies the original assumptions, and the learner has to succeed whenever the tester accepts. We focus on the setting where the tester has to accept standard Gaussian data. There, it is known that basic concept classes such as halfspaces can be learned testably with the same time complexity as in the (distribution-specific) agnostic model. In this work, we ask whether there is a price to pay for testably learning more complex concept classes. In particular, we consider polynomial threshold functions (PTFs), which naturally generalize halfspaces. We show that PTFs of arbitrary constant degree can be testably learned up to excess error $\\varepsilon > 0$ in time $n^{\\mathrm{poly}(1/\\varepsilon)}$. This qualitatively matches the best known guarantees in the agnostic model. Our results build on a connection between testable learning and *fooling*. In particular, we show that distributions that approximately match at least $\\mathrm{poly}(1/\\varepsilon)$ moments of the standard Gaussian fool constant-degree PTFs (up to error $\\varepsilon$). As a secondary result, we prove that a direct approach to show testable learning (without fooling), which was successfully used for halfspaces, cannot work for PTFs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Slot",
      "Stefan Tiegel",
      "Manuel Wiedmer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/072769405a3c1b60171d09c0ade96ebf-Abstract-Conference.html": {
    "title": "Flexible Context-Driven Sensory Processing in Dynamical Vision Models",
    "volume": "main",
    "abstract": "Visual representations become progressively more abstract along the cortical hierarchy. These abstract representations define notions like objects and shapes, but at the cost of spatial specificity. By contrast, low-level regions represent spatially local but simple input features. How do spatially non-specific representations of abstract concepts in high-level areas flexibly modulate the low-level sensory representations in appropriate ways to guide context-driven and goal-directed behaviors across a range of tasks? We build a biologically motivated and trainable neural network model of dynamics in the visual pathway, incorporating local, lateral, and feedforward synaptic connections, excitatory and inhibitory neurons, and long-range top-down inputs conceptualized as low-rank modulations of the input-driven sensory responses by high-level areas. We study this ${\\bf D}$ynamical ${\\bf C}$ortical ${\\bf net}$work ($DCnet$) in a visual cue-delay-search task and show that the model uses its own cue representations to adaptively modulate its perceptual responses to solve the task, outperforming state-of-the-art DNN vision and LLM models. The model's population states over time shed light on the nature of contextual modulatory dynamics, generating predictions for experiments. We fine-tune the same model on classic psychophysics attention tasks, and find that the model closely replicates known reaction time results. This work represents a promising new foundation for understanding and making predictions about perturbations to visual processing in the brain",
    "checked": true,
    "id": "b9147c7c1d6ddf7b4be99727726a49574de7b435",
    "semantic_title": "flexible context-driven sensory processing in dynamical vision models",
    "citation_count": 1,
    "authors": [
      "Lakshmi Narasimhan Govindarajan",
      "Abhiram Iyer",
      "Valmiki Kothare",
      "Ila Fiete"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0729c38b421cd66f2313ab397d099e37-Abstract-Conference.html": {
    "title": "Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices",
    "volume": "main",
    "abstract": "Dense linear layers are the dominant computational bottleneck in large neural networks, presenting a critical need for more efficient alternatives. Previous efforts to develop alternatives have focused on a small number of hand-crafted structured matrices, and have neglected to investigate whether these structures can surpass dense layers in terms of compute-optimal scaling laws when both the model size and training examples are optimally allocated. In this work, we present a unifying framework that enables searching among all linear operators expressible via an Einstein summation. This framework encompasses many previously proposed structures, such as low-rank, Kronecker, Tensor-Train, and Monarch, along with many novel structures. We develop a taxonomy of all such operators based on their computational and algebraic properties, which provides insights into their scaling laws. Combining these insights with empirical evaluation, we identify a subset of structures that achieve equal or better performance than dense layers as a function of training compute. To further improve their compute efficiency, we develop a natural extension of these performant structures that convert them into a sparse Mixture-of-Experts layer. The resulting layer significantly outperforms dense layers in compute-optimal training efficiency for GPT-2 language models",
    "checked": true,
    "id": "e0eb0d60074f0863c7effbae0796c919e1ce7f56",
    "semantic_title": "searching for efficient linear layers over a continuous space of structured matrices",
    "citation_count": 5,
    "authors": [
      "Andres Potapczynski",
      "Shikai Qiu",
      "Marc Finzi",
      "Christopher Ferri",
      "Charlie Chen",
      "Micah Goldblum",
      "C. Bayan Bruss",
      "Christopher M De Sa",
      "Andrew G Wilson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0731f0e65559059eb9cd9d6f44ce2dd8-Abstract-Conference.html": {
    "title": "Diffusion-Inspired Truncated Sampler for Text-Video Retrieval",
    "volume": "main",
    "abstract": "Prevalent text-to-video retrieval methods represent multimodal text-video data in a joint embedding space, aiming at bridging the relevant text-video pairs and pulling away irrelevant ones. One main challenge in state-of-the-art retrieval methods lies in the modality gap, which stems from the substantial disparities between text and video and can persist in the joint space. In this work, we leverage the potential of Diffusion models to address the text-video modality gap by progressively aligning text and video embeddings in a unified space. However, we identify two key limitations of existing Diffusion models in retrieval tasks: The L2 loss does not fit the ranking problem inherent in text-video retrieval, and the generation quality heavily depends on the varied initial point drawn from the isotropic Gaussian, causing inaccurate retrieval. To this end, we introduce a new Diffusion-Inspired Truncated Sampler (DITS) that jointly performs progressive alignment and modality gap modeling in the joint embedding space. The key innovation of DITS is to leverage the inherent proximity of text and video embeddings, defining a truncated diffusion flow from the fixed text embedding to the video embedding, enhancing controllability compared to adopting the isotropic Gaussian. Moreover, DITS adopts the contrastive loss to jointly consider the relevant and irrelevant pairs, not only facilitating alignment but also yielding a discriminatively structured embedding. Experiments on five benchmark datasets suggest the state-of-the-art performance of DITS. We empirically find that DITS can also improve the structure of the CLIP embedding space. Code is available at https://github.com/Jiamian- Wang/DITS-text-video-retrieval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JIAMIAN WANG",
      "Pichao WANG",
      "Dongfang Liu",
      "Qiang Guan",
      "Sohail Dianat",
      "MAJID RABBANI",
      "Raghuveer Rao",
      "Zhiqiang Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0735ab358bf9bf0f30af6c871b666ec8-Abstract-Conference.html": {
    "title": "LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent Diffusion Models",
    "volume": "main",
    "abstract": "Advances in latent diffusion models (LDMs) have revolutionized high-resolution image generation, but the design space of the autoencoder that is central to these systems remains underexplored. In this paper, we introduce LiteVAE, a new autoencoder design for LDMs, which leverages the 2D discrete wavelet transform to enhance scalability and computational efficiency over standard variational autoencoders (VAEs) with no sacrifice in output quality. We investigate the training methodologies and the decoder architecture of LiteVAE and propose several enhancements that improve the training dynamics and reconstruction quality. Our base LiteVAE model matches the quality of the established VAEs in current LDMs with a six-fold reduction in encoder parameters, leading to faster training and lower GPU memory requirements, while our larger model outperforms VAEs of comparable complexity across all evaluated metrics (rFID, LPIPS, PSNR, and SSIM)",
    "checked": true,
    "id": "16cb92a0178195fd70e285f5153160d264f5d49b",
    "semantic_title": "litevae: lightweight and efficient variational autoencoders for latent diffusion models",
    "citation_count": 12,
    "authors": [
      "Seyedmorteza Sadat",
      "Jakob Buhmann",
      "Derek Bradley",
      "Otmar Hilliges",
      "Romann M Weber"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/073c8584ef86bee26fe9d639ec648e28-Abstract-Conference.html": {
    "title": "ROBIN: Robust and Invisible Watermarks for Diffusion Models with Adversarial Optimization",
    "volume": "main",
    "abstract": "Watermarking generative content serves as a vital tool for authentication, ownership protection, and mitigation of potential misuse. Existing watermarking methods face the challenge of balancing robustness and concealment. They empirically inject a watermark that is both invisible and robust and passively achieve concealment by limiting the strength of the watermark, thus reducing the robustness. In this paper, we propose to explicitly introduce a watermark hiding process to actively achieve concealment, thus allowing the embedding of stronger watermarks. To be specific, we implant a robust watermark in an intermediate diffusion state and then guide the model to hide the watermark in the final generated image. We employ an adversarial optimization algorithm to produce the optimal hiding prompt guiding signal for each watermark. The prompt embedding is optimized to minimize artifacts in the generated image, while the watermark is optimized to achieve maximum strength. The watermark can be verified by reversing the generation process. Experiments on various diffusion models demonstrate the watermark remains verifiable even under significant image tampering and shows superior invisibility compared to other state-of-the-art robust watermarking methods",
    "checked": true,
    "id": "f427ecb2862b986e222c5842ae2c01f765a39290",
    "semantic_title": "robin: robust and invisible watermarks for diffusion models with adversarial optimization",
    "citation_count": 12,
    "authors": [
      "Huayang Huang",
      "Yu Wu",
      "Qian Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/074f42212be2c8ee651db00f17965ec4-Abstract-Conference.html": {
    "title": "A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health",
    "volume": "main",
    "abstract": "Restless multi-armed bandits (RMAB) have demonstrated success in optimizing resource allocation for large beneficiary populations in public health settings. Unfortunately, RMAB models lack flexibility to adapt to evolving public health policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept automated planners across domains of robotic control and navigation. In this paper, we propose a Decision Language Model (DLM) for RMABs, enabling dynamic fine-tuning of RMAB policies in public health settings using human-language commands. We propose using LLMs as automated planners to (1) interpret human policy preference prompts, (2) propose reward functions as code for a multi-agent RMAB environment, and (3) iterate on the generated reward functions using feedback from grounded RMAB simulations. We illustrate the application of DLM in collaboration with ARMMAN, an India-based non-profit promoting preventative care for pregnant mothers, that currently relies on RMAB policies to optimally allocate health worker calls to low-resource populations. We conduct a technology demonstration in simulation using the Gemini Pro model, showing DLM can dynamically shape policy outcomes using only human prompts as input",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Behari",
      "Edwin Zhang",
      "YUNFAN ZHAO",
      "Aparna Taneja",
      "Dheeraj Nagaraj",
      "Milind Tambe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/075b7d4bd7fc32d9cf468a7b67c38d15-Abstract-Conference.html": {
    "title": "FUG: Feature-Universal Graph Contrastive Pre-training for Graphs with Diverse Node Features",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs), known for their effective graph encoding, are extensively used across various fields. Graph self-supervised pre-training, which trains GNN encoders without manual labels to generate high-quality graph representations, has garnered widespread attention. However, due to the inherent complex characteristics in graphs, GNNs encoders pre-trained on one dataset struggle to directly adapt to others that have different node feature shapes. This typically necessitates either model rebuilding or data alignment. The former results in non-transferability as each dataset need to rebuild a new model, while the latter brings serious knowledge loss since it forces features into a uniform shape by preprocessing such as Principal Component Analysis (PCA). To address this challenge, we propose a new Feature-Universal Graph contrastive pre-training strategy (FUG) that naturally avoids the need for model rebuilding and data reshaping. Specifically, inspired by discussions in existing work on the relationship between contrastive Learning and PCA, we conducted a theoretical analysis and discovered that PCA's optimization objective is a special case of that in contrastive Learning. We designed an encoder with contrastive constraints to emulate PCA's generation of basis transformation matrix, which is utilized to losslessly adapt features in different datasets. Furthermore, we introduced a global uniformity constraint to replace negative sampling, reducing the time complexity from $O(n^2)$ to $O(n)$, and by explicitly defining positive samples, FUG avoids the substantial memory requirements of data augmentation. In cross domain experiments, FUG has a performance close to the re-trained new models. The source code is available at: https://github.com/hedongxiao-tju/FUG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jitao Zhao",
      "Di Jin",
      "Meng Ge",
      "Lianze Shan",
      "Xin Wang",
      "Dongxiao He",
      "Zhiyong Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/076c1fa639a7190e216e734f0a1b3e7b-Abstract-Conference.html": {
    "title": "ReGS: Reference-based Controllable Scene Stylization with Gaussian Splatting",
    "volume": "main",
    "abstract": "Referenced-based scene stylization that edits the appearance based on a content-aligned reference image is an emerging research area. Starting with a pretrained neural radiance field (NeRF), existing methods typically learn a novel appearance that matches the given style. Despite their effectiveness, they inherently suffer from time-consuming volume rendering, and thus are impractical for many real-time applications. In this work, we propose ReGS, which adapts 3D Gaussian Splatting (3DGS) for reference-based stylization to enable real-time stylized view synthesis. Editing the appearance of a pretrained 3DGS is challenging as it uses discrete Gaussians as 3D representation, which tightly bind appearance with geometry. Simply optimizing the appearance as prior methods do is often insufficient for modeling continuous textures in the given reference image. To address this challenge, we propose a novel texture-guided control mechanism that adaptively adjusts local responsible Gaussians to a new geometric arrangement, serving for desired texture details. The proposed process is guided by texture clues for effective appearance editing, and regularized by scene depth for preserving original geometric structure. With these novel designs, we show ReGs can produce state-of-the-art stylization results that respect the reference texture while embracing real-time rendering speed for free-view navigation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqun Mei",
      "Jiacong Xu",
      "Vishal Patel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/076c3e48fa502c660902105965fdd9f6-Abstract-Conference.html": {
    "title": "QueST: Self-Supervised Skill Abstractions for Learning Continuous Control",
    "volume": "main",
    "abstract": "Generalization capabilities, or rather a lack thereof, is one of the most important unsolved problems in the field of robot learning, and while several large scale efforts have set out to tackle this problem, unsolved it remains. In this paper, we hypothesize that learning temporal action abstractions using latent variable models (LVMs), which learn to map data to a compressed latent space and back, is apromising direction towards low-level skills that can readily be used for new tasks. Although several works have attempted to show this, they have generally been limited by architectures that do not faithfully capture sharable representations. To address this we present Quantized Skill Transformer (QueST), which learns a larger and more flexible latent encoding that is more capable of modeling the breadth of low-level skills necessary for a variety of tasks. To make use of this extra flexibility, QueST imparts causal inductive bias from the action sequence data into the latent space, leading to more semantically useful and transferable representations. We compare to state-of-the-art imitation learning and LVM baselines and see that QueST's architecture leads to strong performance on several multitask and few-shot learning benchmarks. Further results and videos are available at https://quest-model.github.io",
    "checked": true,
    "id": "b716cf9315b682045a9b1dc4c7f522a0c5c5e2af",
    "semantic_title": "quest: self-supervised skill abstractions for learning continuous control",
    "citation_count": 23,
    "authors": [
      "Atharva Mete",
      "Haotian Xue",
      "Albert Wilcox",
      "Yongxin Chen",
      "Animesh Garg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/07a363fd2263091c2063998e0034999c-Abstract-Conference.html": {
    "title": "DiffPhyCon: A Generative Approach to Control Complex Physical Systems",
    "volume": "main",
    "abstract": "Controlling the evolution of complex physical systems is a fundamental task across science and engineering. Classical techniques suffer from limited applicability or huge computational costs. On the other hand, recent deep learning and reinforcement learning-based approaches often struggle to optimize long-term control sequences under the constraints of system dynamics. In this work, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class of method to address the physical systems control problem. DiffPhyCon excels by simultaneously minimizing both the learned generative energy function and the predefined control objectives across the entire trajectory and control sequence. Thus, it can explore globally and plan near-optimal control sequences. Moreover, we enhance DiffPhyCon with prior reweighting, enabling the discovery of control sequences that significantly deviate from the training distribution. We test our method on three tasks: 1D Burgers' equation, 2D jellyfish movement control, and 2D high-dimensional smoke control, where our generated jellyfish dataset is released as a benchmark for complex physical system control research. Our method outperforms widely applied classical approaches and state-of-the-art deep learning and reinforcement learning methods. Notably, DiffPhyCon unveils an intriguing fast-close-slow-open pattern observed in the jellyfish, aligning with established findings in the field of fluid dynamics. The project website, jellyfish dataset, and code can be found at https://github.com/AI4Science-WestlakeU/diffphycon",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Wei",
      "Peiyan Hu",
      "Ruiqi Feng",
      "Haodong Feng",
      "Yixuan Du",
      "Tao Zhang",
      "Rui Wang",
      "Yue Wang",
      "Zhi-Ming Ma",
      "Tailin Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/07c256a163a7559186ec1c71e95b9ec9-Abstract-Conference.html": {
    "title": "FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity Refiner",
    "volume": "main",
    "abstract": "Building on the success of diffusion models in visual generation, flow-based models reemerge as another prominent family of generative models that have achieved competitive or better performance in terms of both visual quality and inference speed. By learning the velocity field through flow-matching, flow-based models tend to produce a straighter sampling trajectory, which is advantageous during the sampling process. However, unlike diffusion models for which fast samplers are well-developed, efficient sampling of flow-based generative models has been rarely explored. In this paper, we propose a framework called FlowTurbo to accelerate the sampling of flow-based models while still enhancing the sampling quality. Our primary observation is that the velocity predictor's outputs in the flow-based models will become stable during the sampling, enabling the estimation of velocity via a lightweight velocity refiner. Additionally, we introduce several techniques including a pseudo corrector and sample-aware compilation to further reduce inference time. Since FlowTurbo does not change the multi-step sampling paradigm, it can be effectively applied for various tasks such as image editing, inpainting, etc. By integrating FlowTurbo into different flow-based models, we obtain an acceleration ratio of 53.1\\%$\\sim$58.3\\% on class-conditional generation and 29.8\\%$\\sim$38.5\\% on text-to-image generation. Notably, FlowTurbo reaches an FID of 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img), achieving the real-time image generation and establishing the new state-of-the-art. Code is available at https://github.com/shiml20/FlowTurbo",
    "checked": true,
    "id": "f61acf84a72687ae054e673065eeb4dc0c27da78",
    "semantic_title": "flowturbo: towards real-time flow-based image generation with velocity refiner",
    "citation_count": 3,
    "authors": [
      "Wenliang Zhao",
      "Minglei Shi",
      "Xumin Yu",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/07e278a120830b10aae20cc600a8c07b-Abstract-Conference.html": {
    "title": "MADiff: Offline Multi-agent Learning with Diffusion Models",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) aims to learn policies from pre-existing datasets without further interactions, making it a challenging task. Q-learning algorithms struggle with extrapolation errors in offline settings, while supervised learning methods are constrained by model expressiveness. Recently, diffusion models (DMs) have shown promise in overcoming these limitations in single-agent learning, but their application in multi-agent scenarios remains unclear. Generating trajectories for each agent with independent DMs may impede coordination, while concatenating all agents' information can lead to low sample efficiency. Accordingly, we propose MADiff, which is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple agents. To our knowledge, MADiff is the first diffusion-based multi-agent learning framework, functioning as both a decentralized policy and a centralized controller. During decentralized executions, MADiff simultaneously performs teammate modeling, and the centralized controller can also be applied in multi-agent trajectory predictions. Our experiments demonstrate that MADiff outperforms baseline algorithms across various multi-agent learning tasks, highlighting its effectiveness in modeling complex multi-agent interactions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengbang Zhu",
      "Minghuan Liu",
      "Liyuan Mao",
      "Bingyi Kang",
      "Minkai Xu",
      "Yong Yu",
      "Stefano Ermon",
      "Weinan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/07e5845260f6f1e22c9b4e1471493056-Abstract-Conference.html": {
    "title": "Polynomial-Time Computation of Exact $\\Phi$-Equilibria in Polyhedral Games",
    "volume": "main",
    "abstract": "It is a well-known fact that correlated equilibria can be computed in polynomial time in a large class of concisely represented games using the celebrated Ellipsoid Against Hope algorithm \\citep{Papadimitriou2008:Computing, Jiang2015:Polynomial}. However, the landscape of efficiently computable equilibria in sequential (extensive-form) games remains unknown. The Ellipsoid Against Hope does not apply directly to these games, because they do not have the required ``polynomial type'' property. Despite this barrier, \\citet{Huang2008:Computing} altered the algorithm to compute exact extensive-form correlated equilibria.In this paper, we generalize the Ellipsoid Against Hope and develop a simple algorithmic framework for efficiently computing saddle-points in bilinear zero-sum games, even when one of the dimensions is exponentially large. Moreover, the framework only requires a ``good-enough-response'' oracle, which is a weakened notion of a best-response oracle.Using this machinery, we develop a general algorithmic framework for computing exact linear $\\Phi$-equilibria in any polyhedral game (under mild assumptions), including correlated equilibria in normal-form games, and extensive-form correlated equilibria in extensive-form games. This enables us to give the first polynomial-time algorithm for computing exact linear-deviation correlated equilibria in extensive-form games, thus resolving an open question by \\citet{Farina2023:Polynomial}. Furthermore, even for the cases for which a polynomial time algorithm for exact equilibria was already known, our framework provides a conceptually simpler solution",
    "checked": true,
    "id": "916f9921376d08b09b4d4bfd5573fd83bed6b5cc",
    "semantic_title": "polynomial-time computation of exact $\\phi$-equilibria in polyhedral games",
    "citation_count": 5,
    "authors": [
      "Gabriele Farina",
      "Charilaos Pipis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/07fbde96bee50f4e09303fd4f877c2f3-Abstract-Conference.html": {
    "title": "Data Attribution for Text-to-Image Models by Unlearning Synthesized Images",
    "volume": "main",
    "abstract": "The goal of data attribution for text-to-image models is to identify the training images that most influence the generation of a new image. Influence is defined such that, for a given output, if a model is retrained from scratch without the most influential images, the model would fail to reproduce the same output. Unfortunately, directly searching for these influential images is computationally infeasible, since it would require repeatedly retraining models from scratch. In our work, we propose an efficient data attribution method by simulating unlearning the synthesized image. We achieve this by increasing the training loss on the output image, without catastrophic forgetting of other, unrelated concepts. We then identify training images with significant loss deviations after the unlearning process and label these as influential. We evaluate our method with a computationally intensive but \"gold-standard\" retraining from scratch and demonstrate our method's advantages over previous methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng-Yu Wang",
      "Aaron Hertzmann",
      "Alexei Efros",
      "Jun-Yan Zhu",
      "Richard Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/080be5eb7e887319ff30c792c2cbc28c-Abstract-Conference.html": {
    "title": "CLIPCEIL: Domain Generalization through CLIP via Channel rEfinement and Image-text aLignment",
    "volume": "main",
    "abstract": "Domain generalization (DG) is a fundamental yet challenging topic in machine learning. Recently, the remarkable zero-shot capabilities of the large pre-trained vision-language model (e.g., CLIP) have made it popular for various downstream tasks. However, the effectiveness of this capacity often degrades when there are shifts in data distribution during testing compared to the training data. In this paper, we propose a novel method, known as CLIPCEIL, a model that utilizes Channel rEfinement and Image-text aLignment to facilitate the CLIP to the inaccessible $\\textit{out-of-distribution}$ test datasets that exhibit domain shifts. Specifically, we refine the feature channels in the visual domain to ensure they contain domain-invariant and class-relevant features by using a lightweight adapter. This is achieved by minimizing the inter-domain variance while maximizing the inter-class variance. In the meantime, we ensure the image-text alignment by aligning text embeddings of the class descriptions and their corresponding image embedding while further removing the domain-specific features. Moreover, our model integrates multi-scale CLIP features by utilizing a self-attention fusion module, technically implemented through one Transformer layer. Extensive experiments on five widely used benchmark datasets demonstrate that CLIPCEIL outperforms the existing state-of-the-art methods. The source code is available at \\url{https://github.com/yuxi120407/CLIPCEIL}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Yu",
      "Shinjae Yoo",
      "Yuewei Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0814a342597d65e0832fc7ec9b42c317-Abstract-Conference.html": {
    "title": "Verifiably Robust Conformal Prediction",
    "volume": "main",
    "abstract": "Conformal Prediction (CP) is a popular uncertainty quantification method that provides distribution-free, statistically valid prediction sets, assuming that training and test data are exchangeable. In such a case, CP's prediction sets are guaranteed to cover the (unknown) true test output with a user-specified probability. Nevertheless, this guarantee is violated when the data is subjected to adversarial attacks, which often result in a significant loss of coverage. Recently, several approaches have been put forward to recover CP guarantees in this setting. These approaches leverage variations of randomised smoothing to produce conservative sets which account for the effect of the adversarial perturbations. They are, however, limited in that they only support $\\ell_2$-bounded perturbations and classification tasks. This paper introduces VRCP (Verifiably Robust Conformal Prediction), a new framework that leverages recent neural network verification methods to recover coverage guarantees under adversarial attacks. Our VRCP method is the first to support perturbations bounded by arbitrary norms including $\\ell_1$, $\\ell_2$, and $\\ell_\\infty$, as well as regression tasks. We evaluate and compare our approach on image classification tasks (CIFAR10, CIFAR100, and TinyImageNet) and regression tasks for deep reinforcement learning environments. In every case, VRCP achieves above nominal coverage and yields significantly more efficient and informative prediction regions than the SotA",
    "checked": true,
    "id": "cd4baf00b33d89dd731748e6add0362367343107",
    "semantic_title": "verifiably robust conformal prediction",
    "citation_count": 6,
    "authors": [
      "Linus Jeary",
      "Tom Kuipers",
      "Mehran Hosseini",
      "Nicola Paoletti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/081b08068e4733ae3e7ad019fe8d172f-Abstract-Conference.html": {
    "title": "Ordering-Based Causal Discovery for Linear and Nonlinear Relations",
    "volume": "main",
    "abstract": "Identifying causal relations from purely observational data typically requires additional assumptions on relations and/or noise. Most current methods restrict their analysis to datasets that are assumed to have pure linear or nonlinear relations, which is often not reflective of real-world datasets that contain a combination of both. This paper presents CaPS, an ordering-based causal discovery algorithm that effectively handles linear and nonlinear relations. CaPS introduces a novel identification criterion for topological ordering and incorporates the concept of \"parent score\" during the post-processing optimization stage. These scores quantify the strength of the average causal effect, helping to accelerate the pruning process and correct inaccurate predictions in the pruning step. Experimental results demonstrate that our proposed solutions outperform state-of-the-art baselines on synthetic data with varying ratios of linear and nonlinear relations. The results obtained from real-world data also support the competitiveness of CaPS. Code and datasets are available at https://github.com/E2real/CaPS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuopeng Xu",
      "Yujie Li",
      "Cheng Liu",
      "Ning Gui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/08342dc6ab69f23167b4123086ad4d38-Abstract-Conference.html": {
    "title": "Revisiting Score Propagation in Graph Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "The field of graph learning has been substantially advanced by the development of deep learning models, in particular graph neural networks. However, one salient yet largely under-explored challenge is detecting Out-of-Distribution (OOD) nodes on graphs. Prevailing OOD detection techniques developed in other domains like computer vision, do not cater to the interconnected nature of graphs. This work aims to fill this gap by exploring the potential of a simple yet effective method -- OOD score propagation, which propagates OOD scores among neighboring nodes along the graph structure. This post hoc solution can be easily integrated with existing OOD scoring functions, showcasing its excellent flexibility and effectiveness in most scenarios. However, the conditions under which score propagation proves beneficial remain not fully elucidated. Our study meticulously derives these conditions and, inspired by this discovery, introduces an innovative edge augmentation strategy with theoretical guarantee. Empirical evaluations affirm the superiority of our proposed method, outperforming strong OOD detection baselines in various scenarios and settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longfei Ma",
      "Yiyou Sun",
      "Kaize Ding",
      "Zemin Liu",
      "Fei Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/084252ec5f05f13bf565843c1873686d-Abstract-Conference.html": {
    "title": "Stochastic Amortization: A Unified Approach to Accelerate Feature and Data Attribution",
    "volume": "main",
    "abstract": "Many tasks in explainable machine learning, such as data valuation and feature attribution, perform expensive computation for each data point and are intractable for large datasets. These methods require efficient approximations, and although amortizing the process by learning a network to directly predict the desired output is a promising solution, training such models with exact labels is often infeasible. We therefore explore training amortized models with noisy labels, and we find that this is inexpensive and surprisingly effective. Through theoretical analysis of the label noise and experiments with various models and datasets, we show that this approach tolerates high noise levels and significantly accelerates several feature attribution and data valuation methods, often yielding an order of magnitude speedup over existing approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian Covert",
      "Chanwoo Kim",
      "Su-In Lee",
      "James Y Zou",
      "Tatsunori B Hashimoto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0845e3f7447ff9e36a26033bb7334674-Abstract-Conference.html": {
    "title": "Explicit Eigenvalue Regularization Improves Sharpness-Aware Minimization",
    "volume": "main",
    "abstract": "Sharpness-Aware Minimization (SAM) has attracted significant attention for its effectiveness in improving generalization across various tasks. However, its underlying principles remain poorly understood. In this work, we analyze SAM's training dynamics using the maximum eigenvalue of the Hessian as a measure of sharpness and propose a third-order stochastic differential equation (SDE), which reveals that the dynamics are driven by a complex mixture of second- and third-order terms. We show that alignment between the perturbation vector and the top eigenvector is crucial for SAM's effectiveness in regularizing sharpness, but find that this alignment is often inadequate in practice, which limits SAM's efficiency. Building on these insights, we introduce Eigen-SAM, an algorithm that explicitly aims to regularize the top Hessian eigenvalue by aligning the perturbation vector with the leading eigenvector. We validate the effectiveness of our theory and the practical advantages of our proposed approach through comprehensive experiments. Code is available at https://github.com/RitianLuo/EigenSAM",
    "checked": true,
    "id": "faf99b95096835591e74b176f8dc4d4365edcf86",
    "semantic_title": "explicit eigenvalue regularization improves sharpness-aware minimization",
    "citation_count": 0,
    "authors": [
      "Haocheng Luo",
      "Tuan Truong",
      "Tung Pham",
      "Mehrtash Harandi",
      "Dinh Phung",
      "Trung Le"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/084727e8abf90a8365b940036329cb6f-Abstract-Conference.html": {
    "title": "LAM3D: Large Image-Point Clouds Alignment Model for 3D Reconstruction from Single Image",
    "volume": "main",
    "abstract": "Large Reconstruction Models have made significant strides in the realm of automated 3D content generation from single or multiple input images. Despite their success, these models often produce 3D meshes with geometric inaccuracies, stemming from the inherent challenges of deducing 3D shapes solely from image data. In this work, we introduce a novel framework, the Large Image and Point Cloud Alignment Model (LAM3D), which utilizes 3D point cloud data to enhance the fidelity of generated 3D meshes. Our methodology begins with the development of a point-cloud-based network that effectively generates precise and meaningful latent tri-planes, laying the groundwork for accurate 3D mesh reconstruction. Building upon this, our Image-Point-Cloud Feature Alignment technique processes a single input image, aligning to the latent tri-planes to imbue image features with robust 3D information. This process not only enriches the image features but also facilitates the production of high-fidelity 3D meshes without the need for multi-view input, significantly reducing geometric distortions. Our approach achieves state-of-the-art high-fidelity 3D mesh reconstruction from a single image in just 6 seconds, and experiments on various datasets demonstrate its effectiveness",
    "checked": true,
    "id": "c270f9a3ff15d53b1563c5ddf222b9e3dc01c7b4",
    "semantic_title": "lam3d: large image-point clouds alignment model for 3d reconstruction from single image",
    "citation_count": 1,
    "authors": [
      "Ruikai Cui",
      "Xibin Song",
      "Weixuan Sun",
      "Senbo Wang",
      "Weizhe Liu",
      "Shenzhou Chen",
      "Taizhang Shang",
      "YANG LI",
      "Nick Barnes",
      "Hongdong Li",
      "Pan Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/084a67fb91826028f555e288f3adc9a4-Abstract-Conference.html": {
    "title": "Weight decay induces low-rank attention layers",
    "volume": "main",
    "abstract": "The effect of regularizers such as weight decay when training deep neural networks is not well understood. We study the influence of weight decay as well as $L2$-regularization when training neural network models in which parameter matrices interact multiplicatively. This combination is of particular interest as this parametrization is common in attention layers, the workhorse of transformers. Here, key-query, as well as value-projection parameter matrices, are multiplied directly with each other: $W_K^TW_Q$ and $PW_V$. We extend previous results and show on one hand that any local minimum of a $L2$-regularized loss of the form $L(AB^\\top) + \\lambda (\\|A\\|^2 + \\|B\\|^2)$ coincides with a minimum of the nuclear norm-regularized loss $L(AB^\\top) + \\lambda\\|AB^\\top\\|_*$, and on the other hand that the 2 losses become identical exponentially quickly during training. We thus complement existing works linking $L2$-regularization with low-rank regularization, and in particular, explain why such regularization on the matrix product affects early stages of training.Based on these theoretical insights, we verify empirically that the key-query and value-projection matrix products $W_K^TW_Q, PW_V$ within attention layers, when optimized with weight decay, as usually done in vision tasks and language modelling, indeed induce a significant reduction in the rank of $W_K^TW_Q$ and $PW_V$, even in fully online training.We find that, in accordance with existing work, inducing low rank in attention matrix products can damage language model performance, and observe advantages when decoupling weight decay in attention layers from the rest of the parameters",
    "checked": true,
    "id": "7fadcaca88918b31b1664603a9b4dd89e8b162f1",
    "semantic_title": "weight decay induces low-rank attention layers",
    "citation_count": 12,
    "authors": [
      "Seijin Kobayashi",
      "Yassir Akram",
      "Johannes von Oswald"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/084cf2b3d73abafa1705336a0e9ebb1c-Abstract-Conference.html": {
    "title": "Association Pattern-aware Fusion for Biological Entity Relationship Prediction",
    "volume": "main",
    "abstract": "Deep learning-based methods significantly advance the exploration of associations among triple-wise biological entities (e.g., drug-target protein-adverse reaction), thereby facilitating drug discovery and safeguarding human health. However, existing researches only focus on entity-centric information mapping and aggregation, neglecting the crucial role of potential association patterns among different entities. To address the above limitation, we propose a novel association pattern-aware fusion method for biological entity relationship prediction, which effectively integrates the related association pattern information into entity representation learning. Additionally, to enhance the missing information of the low-order message passing, we devise a bind-relation module that considers the strong bind of low-order entity associations. Extensive experiments conducted on three biological datasets quantitatively demonstrate that the proposed method achieves about 4%-23% hit@1 improvements compared with state-of-the-art baselines. Furthermore, the interpretability of association patterns is elucidated in detail, thus revealing the intrinsic biological mechanisms and promoting it to be deployed in real-world scenarios. Our data and code are available at https://github.com/hry98kki/PatternBERP",
    "checked": true,
    "id": "03e6a60822b6a7817f337f92ab14e62e5eb1cb29",
    "semantic_title": "association pattern-aware fusion for biological entity relationship prediction",
    "citation_count": 0,
    "authors": [
      "Lingxiang Jia",
      "Yuchen Ying",
      "Zunlei Feng",
      "Zipeng Zhong",
      "Shaolun Yao",
      "Jiacong Hu",
      "Mingjiang Duan",
      "Xingen Wang",
      "Jie Song",
      "Mingli Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0852b88e96d973bd4e21b673f51621d0-Abstract-Conference.html": {
    "title": "Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning",
    "volume": "main",
    "abstract": "The efficacy of large language models (LLMs) on downstream tasks usually hinges on instruction tuning, which relies critically on the quality of training data. Unfortunately, collecting high-quality and diverse data is both expensive and time-consuming. To mitigate this issue, we propose a novel Star-Agents framework, which automates the enhancement of data quality across datasets through multi-agent collaboration and assessment. The framework adopts a three-pronged strategy. It initially generates diverse instruction data with multiple LLM agents through a bespoke sampling method. Subsequently, the generated data undergo a rigorous evaluation using a dual-model method that assesses both difficulty and quality. Finaly, the above process evolves in a dynamic refinement phase, where more effective LLMs are prioritized, enhancing the overall data quality. Our empirical studies, including instruction tuning experiments with models such as Pythia and LLaMA, demonstrate the effectiveness of the proposed framework. Optimized datasets have achieved substantial improvements, with an average increase of 12\\% and notable gains in specific metrics, such as a 40\\% improvement in Fermi, as evidenced by benchmarks like MT-bench, Vicuna bench, and WizardLM testset. Codes will be released soon",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Zhou",
      "Yehui Tang",
      "Haochen Qin",
      "Yujie Yang",
      "Renren Jin",
      "Deyi Xiong",
      "Kai Han",
      "Yunhe Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0856bc553d3e3b9827e5140d0ad3bf8d-Abstract-Conference.html": {
    "title": "On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) lack the precise semantics and definitive probabilistic interpretation of probabilistic graphical models (PGMs). In this paper, we propose an innovative solution by constructing infinite tree-structured PGMs that correspond exactly to neural networks. Our research reveals that DNNs, during forward propagation, indeed perform approximations of PGM inference that are precise in this alternative PGM structure. Not only does our research complement existing studies that describe neural networks as kernel machines or infinite-sized Gaussian processes, it also elucidates a more direct approximation that DNNs make to exact inference in PGMs. Potential benefits include improved pedagogy and interpretation of DNNs, and algorithms that can merge the strengths of PGMs and DNNs",
    "checked": true,
    "id": "44be0a3bf083fb2d135d467467a9d7a346b121fc",
    "semantic_title": "on neural networks as infinite tree-structured probabilistic graphical models",
    "citation_count": 0,
    "authors": [
      "Boyao Li",
      "Alexander Thomson",
      "houssam nassif",
      "Matthew Engelhard",
      "David Page"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0857833a490eff6b49ce43eba1d01e8e-Abstract-Conference.html": {
    "title": "Sparse Bayesian Generative Modeling for Compressive Sensing",
    "volume": "main",
    "abstract": "This work addresses the fundamental linear inverse problem in compressive sensing (CS) by introducing a new type of regularizing generative prior. Our proposed method utilizes ideas from classical dictionary-based CS and, in particular, sparse Bayesian learning (SBL), to integrate a strong regularization towards sparse solutions. At the same time, by leveraging the notion of conditional Gaussianity, it also incorporates the adaptability from generative models to training data. However, unlike most state-of-the-art generative models, it is able to learn from a few compressed and noisy data samples and requires no optimization algorithm for solving the inverse problem. Additionally, similar to Dirichlet prior networks, our model parameterizes a conjugate prior enabling its application for uncertainty quantification. We support our approach theoretically through the concept of variational inference and validate it empirically using different types of compressible signals",
    "checked": true,
    "id": "e0ece12dd02b3aa4ebd13cec62409405fd99acd0",
    "semantic_title": "sparse bayesian generative modeling for compressive sensing",
    "citation_count": 1,
    "authors": [
      "Benedikt Böck",
      "Sadaf Syed",
      "Wolfgang Utschick"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/085b4b5d1f81ad9e057ad2b3de922ad4-Abstract-Conference.html": {
    "title": "Generative Semi-supervised Graph Anomaly Detection",
    "volume": "main",
    "abstract": "This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the extensively explored unsupervised setting with a fully unlabeled graph. We reveal that having access to the normal nodes, even just a small percentage of normal nodes, helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (namely GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate pseudo anomaly nodes, referred to as 'outlier nodes', for providing effective negative node samples in training a discriminative one-class classifier. The main challenge here lies in the lack of ground truth information about real anomaly nodes. To address this challenge, GGAD is designed to leverage two important priors about the anomaly nodes -- asymmetric local affinity and egocentric closeness -- to generate reliable outlier nodes that assimilate anomaly nodes in both graph structure and feature representations. Comprehensive experiments on six real-world GAD datasets are performed to establish a benchmark for semi-supervised GAD and show that GGAD substantially outperforms state-of-the-art unsupervised and semi-supervised GAD methods with varying numbers of training normal nodes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hezhe Qiao",
      "Qingsong Wen",
      "Xiaoli Li",
      "Ee-peng Lim",
      "Guansong Pang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0877af85978e9e630b77f6221db47876-Abstract-Conference.html": {
    "title": "Building on Efficient Foundations: Effective Training of LLMs with Structured Feedforward Layers",
    "volume": "main",
    "abstract": "State-of-the-art results in large language models (LLMs) often rely on scale, whichbecomes computationally expensive. This has sparked a research agenda to reducethese models' parameter counts and computational costs without significantlyimpacting their performance. Our study focuses on transformer-based LLMs,specifically targeting the computationally intensive feedforward networks (FFNs),which are less studied than attention blocks. We consider three structured linearparameterizations of the FFN using efficient low-rank and block-diagonal matrices.In contrast to many previous works that examined these approximations, our studyi) explores these structures from a training-from-scratch perspective, ii) scales upto 1.3B parameters, and iii) is conducted within recent Transformer-based LLMsrather than convolutional architectures. We demonstrate that these structures canlead to actual computational gains in various scenarios, including online decodingwhen using a pre-merge technique. Additionally, we propose a novel trainingregime, called self-guided training, aimed at improving the poor training dynamicsthat these approximations exhibit when used from initialization. Interestingly,the scaling performance of structured matrices is explored, revealing steepercurves in scaling training FLOPs, along with a favorable scaling trend in theovertraining regime. Specifically, we show that wide and structured networkscan utilize training FLOPs more efficiently, with fewer parameters and lowerloss than dense models at their optimal trade-off. Our code is available athttps://github.com/CLAIRE-Labo/StructuredFFN/tree/main",
    "checked": true,
    "id": "e4e52c4a5611d10c17115e5f302cbb122ecafdee",
    "semantic_title": "building on efficient foundations: effective training of llms with structured feedforward layers",
    "citation_count": 1,
    "authors": [
      "Xiuying Wei",
      "Skander Moalla",
      "Razvan Pascanu",
      "Caglar Gulcehre"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0885cd8bf11e9ca58302992ddcfd3652-Abstract-Conference.html": {
    "title": "Optimizing over Multiple Distributions under Generalized Quasar-Convexity Condition",
    "volume": "main",
    "abstract": "We study a typical optimization model where the optimization variable is composed of multiple probability distributions. Though the model appears frequently in practice, such as for policy problems, it lacks specific analysis in the general setting. For this optimization problem, we propose a new structural condition/landscape description named generalized quasar-convexity (GQC) beyond the realms of convexity. In contrast to original quasar-convexity \\citep{hinder2020near}, GQC allows an individual quasar-convex parameter $\\gamma_i$ for each variable block $i$ and the smaller of $\\gamma_i$ implies less block-convexity. To minimize the objective function, we consider a generalized oracle termed as the internal function that includes the standard gradient oracle as a special case. We provide optimistic mirror descent (OMD) for multiple distributions and prove that the algorithm can achieve an adaptive $\\tilde{\\mathcal{O}}((\\sum_{i=1}^d1/\\gamma_i)\\epsilon^{-1})$ iteration complexity to find an $\\varepsilon$-suboptimal global solution without pre-known the exact values of $\\gamma_i$ when the objective admits ``polynomial-like'' structural. Notably, it achieves iteration complexity that does not explicitly depend on the number of distributions and strictly faster $(\\sum_{i=1}^d 1/\\gamma_i \\text{ v.s. } d\\max_{i\\in[1:d]} 1/\\gamma_i)$ than mirror decent methods. We also extend GQC to the minimax optimization problem proposing the generalized quasar-convexity-concavity (GQCC) condition and a decentralized variant of OMD with regularization. Finally, we show the applications of our algorithmic framework on discounted Markov Decision Processes problem and Markov games, which bring new insights on the landscape analysis of reinforcement learning",
    "checked": true,
    "id": "9059beff87ef7514768fe19d85cab2bc5ce2e5a7",
    "semantic_title": "optimizing over multiple distributions under generalized quasar-convexity condition",
    "citation_count": 0,
    "authors": [
      "Ding Shihong",
      "Long Yang",
      "Luo Luo",
      "Cong Fang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0886e50806c3faf55e557bd63ba3e70c-Abstract-Conference.html": {
    "title": "Topological Generalization Bounds for Discrete-Time Stochastic Optimization Algorithms",
    "volume": "main",
    "abstract": "We present a novel set of rigorous and computationally efficient topology-based complexity notions that exhibit a strong correlation with the generalization gap in modern deep neural networks (DNNs). DNNs show remarkable generalization properties, yet the source of these capabilities remains elusive, defying the established statistical learning theory. Recent studies have revealed that properties of training trajectories can be indicative of generalization. Building on this insight, state-of-the-art methods have leveraged the topology of these trajectories, particularly their fractal dimension, to quantify generalization. Most existing works compute this quantity by assuming continuous- or infinite-time training dynamics, complicating the development of practical estimators capable of accurately predicting generalization without access to test data. In this paper, we respect the discrete-time nature of training trajectories and investigate the underlying topological quantities that can be amenable to topological data analysis tools. This leads to a new family of reliable topological complexity measures that provably bound the generalization error, eliminating the need for restrictive geometric assumptions. These measures are computationally friendly, enabling us to propose simple yet effective algorithms for computing generalization indices. Moreover, our flexible framework can be extended to different domains, tasks, and architectures. Our experimental results demonstrate that our new complexity measures exhibit a strong correlation with generalization error in industry-standard architectures such as transformers and deep graph networks. Our approach consistently outperforms existing topological bounds across a wide range of datasets, models, and optimizers, highlighting the practical relevance and effectiveness of our complexity measures",
    "checked": true,
    "id": "bf5f393d8b008a9465b01b425c093698f670c7c7",
    "semantic_title": "topological generalization bounds for discrete-time stochastic optimization algorithms",
    "citation_count": 8,
    "authors": [
      "Rayna Andreeva",
      "Benjamin Dupuis",
      "Rik Sarkar",
      "Tolga Birdal",
      "Umut Simsekli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/088d99765bc121c6df215da7d45bc4e9-Abstract-Conference.html": {
    "title": "Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models",
    "volume": "main",
    "abstract": "Large language models are usually fine-tuned to align with human preferences. However, fine-tuning a large language model can be challenging. In this work, we introduce $\\textit{weak-to-strong search}$, framing the alignment of a large language model as a test-time greedy search to maximize the log-probability difference between small tuned and untuned models while sampling from the frozen large model. This method serves both as (1) a compute-efficient model up-scaling strategy that avoids directly tuning the large model and as (2) an instance of weak-to-strong generalization that enhances a strong model with weak test-time guidance.Empirically, we demonstrate the flexibility of weak-to-strong search across different tasks. In controlled-sentiment generation and summarization, we use tuned and untuned $\\texttt{gpt2}$s to improve the alignment of large models without additional training. Crucially, in a more difficult instruction-following benchmark, AlpacaEval 2.0, we show that reusing off-the-shelf small models (e.g., $\\texttt{zephyr-7b-beta}$ and its untuned version) can improve the length-controlled win rates of both white-box and black-box large models against $\\texttt{gpt-4-turbo}$ (e.g., $34.4\\% \\rightarrow 37.9\\%$ for $\\texttt{Llama-3-70B-Instruct}$ and $16.0\\% \\rightarrow 20.1\\%$ for $\\texttt{gpt-3.5-turbo-instruct}$), despite the small models' low win rates $\\approx 10.0\\%$",
    "checked": true,
    "id": "120780c2bf33793f871d1b706288203d618f2d1f",
    "semantic_title": "weak-to-strong search: align large language models via searching over small language models",
    "citation_count": 31,
    "authors": [
      "Zhanhui Zhou",
      "Zhixuan Liu",
      "Jie Liu",
      "Zhichen Dong",
      "Chao Yang",
      "Yu Qiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0895e2538a68e786475410c6acb1c30f-Abstract-Conference.html": {
    "title": "HORSE: Hierarchical Representation for Large-Scale Neural Subset Selection",
    "volume": "main",
    "abstract": "Subset selection tasks, such as anomaly detection and compound selection in AI-assisted drug discovery, are crucial for a wide range of applications. Learning subset-valued functions with neural networks has achieved great success by incorporating permutation invariance symmetry into the architecture. However, existing neural set architectures often struggle to either capture comprehensive information from the superset or address complex interactions within the input. Additionally, they often fail to perform in scenarios where superset sizes surpass available memory capacity. To address these challenges, we introduce the novel concept of the Identity Property, which requires models to integrate information from the originating set, resulting in the development of neural networks that excel at performing effective subset selection from large supersets. Moreover, we present the Hierarchical Representation of Neural Subset Selection (HORSE), an attention-based method that learns complex interactions and retains information from both the input set and the optimal subset supervision signal. Specifically, HORSE enables the partitioning of the input ground set into manageable chunks that can be processed independently and then aggregated, ensuring consistent outcomes across different partitions. Through extensive experimentation, we demonstrate that HORSE significantly enhances neural subset selection performance by capturing more complex information and surpasses state-of-the-art methods in handling large-scale inputs by a margin of up to 20%",
    "checked": true,
    "id": "edac27a558152dbcd00e120d7925fac2d5d4ef24",
    "semantic_title": "horse: hierarchical representation for large-scale neural subset selection",
    "citation_count": 0,
    "authors": [
      "Binghui Xie",
      "Yixuan Wang",
      "Yongqiang Chen",
      "Kaiwen Zhou",
      "Yu Li",
      "Wei Meng",
      "James Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0898f05f6c1d247be3eab8da93d33da1-Abstract-Conference.html": {
    "title": "Segment, Shuffle, and Stitch: A Simple Layer for Improving Time-Series Representations",
    "volume": "main",
    "abstract": "Existing approaches for learning representations of time-series keep the temporal arrangement of the time-steps intact with the presumption that the original order is the most optimal for learning. However, non-adjacent sections of real-world time-series may have strong dependencies. Accordingly, we raise the question: Is there an alternative arrangement for time-series which could enable more effective representation learning? To address this, we propose a simple plug-and-play neural network layer called Segment, Shuffle, and Stitch (S3) designed to improve representation learning in time-series models. S3 works by creating non-overlapping segments from the original sequence and shuffling them in a learned manner that is optimal for the task at hand. It then re-attaches the shuffled segments back together and performs a learned weighted sum with the original input to capture both the newly shuffled sequence along with the original sequence. S3 is modular and can be stacked to achieve different levels of granularity, and can be added to many forms of neural architectures including CNNs or Transformers with negligible computation overhead. Through extensive experiments on several datasets and state-of-the-art baselines, we show that incorporating S3 results in significant improvements for the tasks of time-series classification, forecasting, and anomaly detection, improving performance on certain datasets by up to 68\\%. We also show that S3 makes the learning more stable with a smoother training loss curve and loss landscape compared to the original baseline. The code is available at https://github.com/shivam-grover/S3-TimeSeries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivam Grover",
      "Amin Jalali",
      "Ali Etemad"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/08a362bd4ae1934e099ce025f06039fe-Abstract-Conference.html": {
    "title": "TARSS-Net: Temporal-Aware Radar Semantic Segmentation Network",
    "volume": "main",
    "abstract": "Radar signal interpretation plays a crucial role in remote detection and ranging. With the gradual display of the advantages of neural network technology in signal processing, learning-based radar signal interpretation is becoming a research hot-spot and made great progress. And since radar semantic segmentation (RSS) can provide more fine-grained target information, it has become a more concerned direction in this field. However, the temporal information, which is an important clue for analyzing radar data, has not been exploited sufficiently in present RSS frameworks. In this work, we propose a novel temporal information learning paradigm, i.e., data-driven temporal information aggregation with learned target-history relations. Following this idea, a flexible learning module, called Temporal Relation-Aware Module (TRAM) is carefully designed. TRAM contains two main blocks: i) an encoder for capturing the target-history temporal relations (TH-TRE) and ii) a learnable temporal relation attentive pooling (TRAP) for aggregating temporal information. Based on TRAM, an end-to-end Temporal-Aware RSS Network (TARSS-Net) is presented, which has outstanding performance on publicly available and our collected real-measured datasets. Code and supplementary materials are available at https://github.com/zlw9161/TARSS-Net",
    "checked": true,
    "id": "9bb80cea96f8431ed5f9d3a40c1eb81d5999a260",
    "semantic_title": "tarss-net: temporal-aware radar semantic segmentation network",
    "citation_count": 0,
    "authors": [
      "Youcheng Zhang",
      "Liwen Zhang",
      "ZijunHu",
      "Pengcheng Pi",
      "Teng Li",
      "Yuanpei Chen",
      "Shi Peng",
      "Zhe Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/08a4e10cb5b950971a64cf2155871f51-Abstract-Conference.html": {
    "title": "ProEdit: Simple Progression is All You Need for High-Quality 3D Scene Editing",
    "volume": "main",
    "abstract": "This paper proposes ProEdit - a simple yet effective framework for high-quality 3D scene editing guided by diffusion distillation in a novel progressive manner. Inspired by the crucial observation that multi-view inconsistency in scene editing is rooted in the diffusion model's large feasible output space (FOS), our framework controls the size of FOS and reduces inconsistency by decomposing the overall editing task into several subtasks, which are then executed progressively on the scene. Within this framework, we design a difficulty-aware subtask decomposition scheduler and an adaptive 3D Gaussian splatting (3DGS) training strategy, ensuring high efficiency in performing each subtask. Extensive evaluation shows that our ProEdit achieves state-of-the-art results in various scenes and challenging editing tasks, all through a simple framework without any expensive or sophisticated add-ons like distillation losses, components, or training procedures. Notably, ProEdit also provides a new way to preview, control, and select the aggressivity of editing operation during the editing process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junkun Chen",
      "Yu-Xiong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/08a9e28c96d016dd63903ab51cd085b0-Abstract-Conference.html": {
    "title": "IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons",
    "volume": "main",
    "abstract": "It is widely acknowledged that large language models (LLMs) encode a vast reservoir of knowledge after being trained on mass data. Recent studies disclose knowledge conflicts in LLM generation, wherein outdated or incorrect parametric knowledge (i.e., encoded knowledge) contradicts new knowledge provided in the context. To mitigate such knowledge conflicts, we propose a novel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to capitalize on neurons that are crucial in processing contextual cues. Specifically, IRCAN first identifies neurons that significantly contribute to context processing, utilizing a context-aware attribution score derived from integrated gradients. Subsequently, the identified context-aware neurons are strengthened via reweighting. In doing so, we steer LLMs to generate context-sensitive outputs with respect to the new knowledge provided in the context. Extensive experiments conducted across a variety of models and tasks demonstrate that IRCAN not only achieves remarkable improvements in handling knowledge conflicts but also offers a scalable, plug-and-play solution that can be integrated seamlessly with existing models. Our codes are released at https://github.com/danshi777/IRCAN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Shi",
      "Renren Jin",
      "Tianhao Shen",
      "Weilong Dong",
      "Xinwei Wu",
      "Deyi Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/08bd07d567d77d6dd8d82a4474706a5e-Abstract-Conference.html": {
    "title": "Utilizing Human Behavior Modeling to Manipulate Explanations in AI-Assisted Decision Making: The Good, the Bad, and the Scary",
    "volume": "main",
    "abstract": "Recent advances in AI models have increased the integration of AI-based decision aids into the human decision making process. To fully unlock the potential of AI-assisted decision making, researchers have computationally modeled how humans incorporate AI recommendations into their final decisions, and utilized these models to improve human-AI team performance. Meanwhile, due to the ``black-box'' nature of AI models, providing AI explanations to human decision makers to help them rely on AI recommendations more appropriately has become a common practice. In this paper, we explore whether we can quantitatively model how humans integrate both AI recommendations and explanations into their decision process, and whether this quantitative understanding of human behavior from the learned model can be utilized to manipulate AI explanations, thereby nudging individuals towards making targeted decisions. Our extensive human experiments across various tasks demonstrate that human behavior can be easily influenced by these manipulated explanations towards targeted outcomes, regardless of the intent being adversarial or benign. Furthermore, individuals often fail to detect any anomalies in these explanations, despite their decisions being affected by them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoyan Li",
      "Ming Yin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/08f0b541b6eb47bdafebea3a09ad79e5-Abstract-Conference.html": {
    "title": "HairDiffusion: Vivid Multi-Colored Hair Editing via Latent Diffusion",
    "volume": "main",
    "abstract": "Hair editing is a critical image synthesis task that aims to edit hair color and hairstyle using text descriptions or reference images, while preserving irrelevant attributes (e.g., identity, background, cloth). Many existing methods are based on StyleGAN to address this task. However, due to the limited spatial distribution of StyleGAN, it struggles with multiple hair color editing and facial preservation. Considering the advancements in diffusion models, we utilize Latent Diffusion Models (LDMs) for hairstyle editing. Our approach introduces Multi-stage Hairstyle Blend (MHB), effectively separating control of hair color and hairstyle in diffusion latent space. Additionally, we train a warping module to align the hair color with the target region. To further enhance multi-color hairstyle editing, we fine-tuned a CLIP model using a multi-color hairstyle dataset. Our method not only tackles the complexity of multi-color hairstyles but also addresses the challenge of preserving original colors during diffusion editing. Extensive experiments showcase the superiority of our method in editing multi-color hairstyles while preserving facial attributes given textual descriptions and reference images",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zeng",
      "Yang Zhang",
      "Liu Jiachen",
      "Linlin Shen",
      "Kaijun Deng",
      "Weizhao He",
      "Jinbao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/091166620a04a289c555f411d8899049-Abstract-Conference.html": {
    "title": "PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression",
    "volume": "main",
    "abstract": "There has been significant interest in \"extreme\" compression of large language models (LLMs), i.e. to 1-2 bits per parameter, which allows such models to be executed efficiently on resource-constrained devices. Existing work focused on improved one-shot quantization techniques and weight representations; yet, purely post-training approaches are reaching diminishing returns in terms of the accuracy-vs-bit-width trade-off. State-of-the-art quantization methods such as QuIP# and AQLM include fine-tuning (part of) the compressed parameters over a limited amount of calibration data; however, such fine-tuning techniques over compressed weights often make exclusive use of straight-through estimators (STE), whose performance is not well-understood in this setting. In this work, we question the use of STE for extreme LLM compression, showing that it can be sub-optimal, and perform a systematic study of quantization-aware fine-tuning strategies for LLMs.We propose PV-Tuning - a representation-agnostic framework that generalizes and improves upon existing fine-tuning strategies, and provides convergence guarantees in restricted cases.On the practical side, when used for 1-2 bit vector quantization, PV-Tuning outperforms prior techniques for highly-performant models such as Llama and Mistral. Using PV-Tuning, we achieve the first Pareto-optimal quantization for Llama-2 family models at 2 bits per parameter",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladimir Malinovskii",
      "Denis Mazur",
      "Ivan Ilin",
      "Denis Kuznedelev",
      "Konstantin Burlachenko",
      "Kai Yi",
      "Dan Alistarh",
      "Peter Richtarik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/09236f27bad623511341362f26ffcabb-Abstract-Conference.html": {
    "title": "Feedback control guides credit assignment in recurrent neural networks",
    "volume": "main",
    "abstract": "How do brain circuits learn to generate behaviour? While significant strides have been made in understanding learning in artificial neural networks, applying this knowledge to biological networks remains challenging. For instance, while backpropagation is known to perform accurate credit assignment of error in artificial neural networks, how a similarly powerful process can be realized within the constraints of biological circuits remains largely unclear. One of the major challenges is that the brain's extensive recurrent connectivity requires the propagation of error through both space and time, a problem that is notoriously difficult to solve in vanilla recurrent neural networks. Moreover, the extensive feedback connections in the brain are known to influence forward network activity, but the interaction between feedback-driven activity changes and local, synaptic plasticity-based learning is not fully understood. Building on our previous work modelling motor learning, this work investigates the mechanistic properties of pre-trained networks with feedback control on a standard motor task. We show that feedback control of the ongoing recurrent network dynamics approximates the optimal first-order gradient with respect to the network activities, allowing for rapid, ongoing movement correction. Moreover, we show that trial-by-trial adaptation to a persistent perturbation using a local, biologically plausible learning rule that integrates recent activity and error feedback is both more accurate and more efficient with feedback control during learning, due to the decoupling of the recurrent network dynamics and the injection of an adaptive, second-order gradient into the network dynamics. Thus, our results suggest that feedback control may guide credit assignment in biological recurrent neural networks, enabling both rapid and efficient learning in the brain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Klara Kaleb",
      "Barbara Feulner",
      "Juan Gallego",
      "Claudia Clopath"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/09265e2568cf7a6ff47b506acbc2c6eb-Abstract-Conference.html": {
    "title": "COLD: Causal reasOning in cLosed Daily activities",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have shown state-of-the-art performance in a variety of tasks, including arithmetic and reasoning; however, to gauge the intellectual capabilities of LLMs, causal reasoning has become a reliable proxy for validating a general understanding of the mechanics and intricacies of the world similar to humans. Previous works in natural language processing (NLP) have either focused on open-ended causal reasoning via causal commonsense reasoning (CCR) or framed a symbolic representation-based question answering for theoretically backed-up analysis via a causal inference engine. The former adds an advantage of real-world grounding but lacks theoretically backed-up analysis/validation, whereas the latter is far from real-world grounding. In this work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed Daily activities) framework, which is built upon human understanding of daily real-world activities to reason about the causal nature of events. We show that the proposed framework facilitates the creation of enormous causal queries (∼ 9 million) and comes close to the mini-turing test, simulating causal reasoning to evaluate the understanding of a daily real-world task. We evaluate multiple LLMs on the created causal queries and find that causal reasoning is challenging even for activities trivial to humans. We further explore (the causal reasoning abilities of LLMs) using the backdoor criterion to determine the causal strength between events",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhinav Joshi",
      "areeb ahmad",
      "Ashutosh Modi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0939f13ffce3ff487509d902ddba4571-Abstract-Conference.html": {
    "title": "SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (LLaMA 2, LLaMA 3, Gemma) and scales (from 2B to 70B), including more advanced architectural configurations such as the mixture of experts (MoE). Our evaluation spans a wide variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thought reasoning tasks. The results demonstrate that SLED consistently improves factual accuracy by up to 20\\% compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance",
    "checked": true,
    "id": "99f753f6cae65c01c490d488bec6dc51b42543e9",
    "semantic_title": "sled: self logits evolution decoding for improving factuality in large language models",
    "citation_count": 6,
    "authors": [
      "Jianyi Zhang",
      "Da-Cheng Juan",
      "Cyrus Rashtchian",
      "Chun-Sung Ferng",
      "Heinrich Jiang",
      "Yiran Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/094324f386c836c75d4a26f3499d2ede-Abstract-Conference.html": {
    "title": "BackdoorAlign: Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment",
    "volume": "main",
    "abstract": "Despite the general capabilities of Large Language Models (LLMs) like GPT-4, these models still request fine-tuning or adaptation with customized data when meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack) under the setting of Language-Model-as-a-Service (LMaaS), where the model's safety has been significantly compromised by fine-tuning on users' uploaded examples that contain just a few harmful examples. Though potential defenses have been proposed that the service providers of LMaaS can integrate safety examples into the fine-tuning dataset to reduce safety issues, such approaches require incorporating a substantial amount of data, making it inefficient. To effectively defend against the FJAttack with limited safety examples under LMaaS, we propose the Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In particular, service providers will construct prefixed safety examples with a secret prompt, acting as a \"backdoor trigger\". By integrating prefixed safety examples into the fine-tuning dataset, the subsequent fine-tuning process effectively acts as the \"backdoor attack\", establishing a strong correlation between the secret prompt and safety generations. Consequently, safe responses are ensured once service providers prepend this secret prompt ahead of any user input during inference. Our comprehensive experiments demonstrate that through the Backdoor Enhanced Safety Alignment with adding as few as 11 prefixed safety examples, the maliciously fine-tuned LLMs will achieve similar safety performance as the original aligned models without harming the benign performance. Furthermore, we also present the effectiveness of our method in a more practical setting where the fine-tuning data consists of both FJAttack examples and the fine-tuning task data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiongxiao Wang",
      "Jiazhao LI",
      "Yiquan Li",
      "Xiangyu Qi",
      "Junjie Hu",
      "Sharon Li",
      "Patrick McDaniel",
      "Muhao Chen",
      "Bo Li",
      "Chaowei Xiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/097c514162ea7126d40671d23e12f51b-Abstract-Conference.html": {
    "title": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents",
    "volume": "main",
    "abstract": "We introduce a novel reinforcement learning framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments) designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts. The agent possesses capabilities beyond conversation, including reflection, tool usage, and expert consultation. We formulate the construction of such an LLM agent as a reinforcement learning (RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm. We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping. Our extensive experiments on ProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs trained with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance. Datasets and code are available at https://github.com/bytarnish/AGILE",
    "checked": true,
    "id": "82a8b7956661a077d41f58bee59aadd7ff9c79a0",
    "semantic_title": "agile: a novel reinforcement learning framework of llm agents",
    "citation_count": 15,
    "authors": [
      "Feng Peiyuan",
      "Yichen He",
      "Guanhua Huang",
      "Yuan Lin",
      "Hanchong Zhang",
      "Yuchen Zhang",
      "Hang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/098491b37deebbe6c007e69815729e09-Abstract-Conference.html": {
    "title": "SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation",
    "volume": "main",
    "abstract": "In this paper, we propose a new framework for zero-shot object navigation.Existing zero-shot object navigation methods prompt LLM with the text of spatially closed objects, which lacks enough scene context for in-depth reasoning.To better preserve the information of environment and fully exploit the reasoning ability of LLM, we propose to represent the observed scene with 3D scene graph. The scene graph encodes the relationships between objects, groups and rooms with a LLM-friendly structure, for which we design a hierarchical chain-of-thought prompt to help LLM reason the goal location according to scene context by traversing the nodes and edges.Moreover, benefit from the scene graph representation, we further design a re-perception mechanism to empower the object navigation framework with the ability to correct perception error.We conduct extensive experiments on MP3D, HM3D and RoboTHOR environments, where SG-Nav surpasses previous state-of-the-art zero-shot methods by more than \\textbf{10\\%} SR on all benchmarks, while the decision process is explainable. To the best of our knowledge, SG-Nav is the first zero-shot method that achieves even higher performance than supervised object navigation methods on the challenging MP3D benchmark.Code of this project will be released in the final version",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Yin",
      "Xiuwei Xu",
      "Zhenyu Wu",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/09887fac6cb071922e870090ce32aeff-Abstract-Conference.html": {
    "title": "Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound",
    "volume": "main",
    "abstract": "Current PAC-Bayes generalisation bounds are restricted to scalar metrics of performance, such as the loss or error rate. However, one ideally wants more information-rich certificates that control the entire distribution of possible outcomes, such as the distribution of the test loss in regression, or the probabilities of different mis-classifications. We provide the first PAC-Bayes bound capable of providing such rich information by bounding the Kullback-Leibler divergence between the empirical and true probabilities of a set of $M$ error types, which can either be discretized loss values for regression, or the elements of the confusion matrix (or a partition thereof) for classification. We transform our bound into a differentiable training objective. Our bound is especially useful in cases where the severity of different mis-classifications may change over time; existing PAC-Bayes bounds can only bound a particular pre-decided weighting of the error types. In contrast our bound implicitly controls all uncountably many weightings simultaneously",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reuben Adams",
      "John Shawe-Taylor",
      "Benjamin Guedj"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/098d1bd3eb6156a4c2f834563cdcf617-Abstract-Conference.html": {
    "title": "Can Graph Learning Improve Planning in LLM-based Agents?",
    "volume": "main",
    "abstract": "Task planning in language agents is emerging as an important research topic alongside the development of large language models (LLMs). It aims to break down complex user requests in natural language into solvable sub-tasks, thereby fulfilling the original requests. In this context, the sub-tasks can be naturally viewed as a graph, where the nodes represent the sub-tasks, and the edges denote the dependencies among them. Consequently, task planning is a decision-making problem that involves selecting a connected path or subgraph within the corresponding graph and invoking it. In this paper, we explore graph learning-based methods for task planning, a direction that is orthogonal to the prevalent focus on prompt design. Our interest in graph learning stems from a theoretical discovery: the biases of attention and auto-regressive loss impede LLMs' ability to effectively navigate decision-making on graphs, which is adeptly addressed by graph neural networks (GNNs). This theoretical insight led us to integrate GNNs with LLMs to enhance overall performance. Extensive experiments demonstrate that GNN-based methods surpass existing solutions even without training, and minimal training can further enhance their performance. The performance gain increases with a larger task graph size",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xixi Wu",
      "Yifei Shen",
      "Caihua Shan",
      "Kaitao Song",
      "Siwei Wang",
      "Bohang Zhang",
      "Jiarui Feng",
      "Hong Cheng",
      "Wei Chen",
      "Yun Xiong",
      "Dongsheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/09b47a77997b7dd7d2b26bd8ff769392-Abstract-Conference.html": {
    "title": "Fully Explicit Dynamic Gaussian Splatting",
    "volume": "main",
    "abstract": "3D Gaussian Splatting has shown fast and high-quality rendering results in static scenes by leveraging dense 3D prior and explicit representations. Unfortunately, the benefits of the prior and representation do not involve novel view synthesis for dynamic motions. Ironically, this is because the main barrier is the reliance on them, which requires increasing training and rendering times to account for dynamic motions. In this paper, we design Explicit 4D Gaussian Splatting (Ex4DGS).Our key idea is to firstly separate static and dynamic Gaussians during training, and to explicitly sample positions and rotations of the dynamic Gaussians at sparse timestamps. The sampled positions and rotations are then interpolated to represent both spatially and temporally continuous motions of objects in dynamic scenes as well as reducing computational cost. Additionally, we introduce a progressive training scheme and a point-backtracking technique that improves Ex4DGS's convergence. We initially train Ex4DGS using short timestamps and progressively extend timestamps, which makes it work well with a few point clouds. The point-backtracking is used to quantify the cumulative error of each Gaussian over time, enabling the detection and removal of erroneous Gaussians in dynamic scenes. Comprehensive experiments on various scenes demonstrate the state-of-the-art rendering quality from our method, achieving fast rendering of 62 fps on a single 2080Ti GPU",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junoh Lee",
      "Changyeon Won",
      "Hyunjun Jung",
      "Inhwan Bae",
      "Hae-Gon Jeon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/09bf6a87e80d099cf17c6347301c6120-Abstract-Conference.html": {
    "title": "Graph Neural Networks and Arithmetic Circuits",
    "volume": "main",
    "abstract": "We characterize the computational power of neural networks that follow the graph neural network (GNN) architecture, not restricted to aggregate-combine GNNs or other particular types. We establish an exact correspondence between the expressivity of GNNs using diverse activation functions and arithmetic circuits over real numbers. In our results the activation function of the network becomes a gate type in the circuit. Our result holds for families of constant depth circuits and networks, both uniformly and non-uniformly, for all common activation functions",
    "checked": true,
    "id": "bdab2a6a7a46b5fd6cdac5fbd48a00d9c4ab720c",
    "semantic_title": "graph neural networks and arithmetic circuits",
    "citation_count": 1,
    "authors": [
      "Timon Barlag",
      "Vivian Holzapfel",
      "Laura Strieker",
      "Jonni Virtema",
      "Heribert Vollmer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/09d320a5b92d74bbde3d0c4f52e680a9-Abstract-Conference.html": {
    "title": "Self-Refining Diffusion Samplers: Enabling Parallelization via Parareal Iterations",
    "volume": "main",
    "abstract": "In diffusion models, samples are generated through an iterative refinement process, requiring hundreds of sequential model evaluations. Several recent methods have introduced approximations (fewer discretization steps or distillation) to trade off speed at the cost of sample quality. In contrast, we introduce Self-Refining Diffusion Samplers (SRDS) that retain sample quality and can improve latency at the cost of additional parallel compute. We take inspiration from the Parareal algorithm, a popular numerical method for parallel-in-time integration of differential equations. In SRDS, a quick but rough estimate of a sample is first created and then iteratively refined in parallel through Parareal iterations. SRDS is not only guaranteed to accurately solve the ODE and converge to the serial solution but also benefits from parallelization across the diffusion trajectory, enabling batched inference and pipelining. As we demonstrate for pre-trained diffusion models, the early convergence of this refinement procedure drastically reduces the number of steps required to produce a sample, speeding up generation for instance by up to 1.7x on a 25-step StableDiffusion-v2 benchmark and up to 4.3x on longer trajectories",
    "checked": true,
    "id": "6dd0f87ecd7dfcb86f1b19a65d57faf4f9476062",
    "semantic_title": "self-refining diffusion samplers: enabling parallelization via parareal iterations",
    "citation_count": 5,
    "authors": [
      "Nikil Selvam",
      "Amil Merchant",
      "Stefano Ermon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/09e1944b7f2372f9f81866470c59b663-Abstract-Conference.html": {
    "title": "Can Learned Optimization Make Reinforcement Learning Less Difficult?",
    "volume": "main",
    "abstract": "While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration. In particular: it is highly non-stationary; suffers from high degrees of plasticity loss; and requires exploration to prevent premature convergence to local optima and maximize return. In this paper, we consider whether learned optimization can help overcome these problems. Our method, Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties. We show that our parameterization is flexible enough to enable meta-learning in diverse learning contexts, including the ability to use stochasticity for exploration. Our experiments demonstrate that when meta-trained on single and small sets of environments, OPEN outperforms or equals traditionally used optimizers. Furthermore, OPEN shows strong generalization characteristics across a range of environments and agent architectures",
    "checked": true,
    "id": "5ecf53ab083f72f10421225a7dde25eb51cb6b22",
    "semantic_title": "can learned optimization make reinforcement learning less difficult?",
    "citation_count": 7,
    "authors": [
      "Alexander D. Goldie",
      "Chris Lu",
      "Matthew T Jackson",
      "Shimon Whiteson",
      "Jakob Foerster"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0a02c2bc2e2148b803c4ade1d71e1d25-Abstract-Conference.html": {
    "title": "Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling",
    "volume": "main",
    "abstract": "Diffusion models have emerged as a powerful tool for generating high-quality images from textual descriptions. Despite their successes, these models often exhibit limited diversity in the sampled images, particularly when sampling with a high classifier-free guidance weight. To address this issue, we present Kaleido, a novel approach that enhances the diversity of samples by incorporating autoregressive latent priors. Kaleido integrates an autoregressive language model that encodes the original caption and generates latent variables, serving as abstract and intermediary representations for guiding and facilitating the image generation process.In this paper, we explore a variety of discrete latent representations, including textual descriptions, detection bounding boxes, object blobs, and visual tokens. These representations diversify and enrich the input conditions to the diffusion models, enabling more diverse outputs.Our experimental results demonstrate that Kaleido effectively broadens the diversity of the generated image samples from a given textual description while maintaining high image quality. Furthermore, we show that Kaleido adheres closely to the guidance provided by the generated latent variables, demonstrating its capability to effectively control and direct the image generation process",
    "checked": true,
    "id": "f2ade71bf8cfdef0ac82d50ad9e99b6bb8aa076a",
    "semantic_title": "kaleido diffusion: improving conditional diffusion models with autoregressive latent modeling",
    "citation_count": 12,
    "authors": [
      "Jiatao Gu",
      "Ying Shen",
      "Shuangfei Zhai",
      "Yizhe Zhang",
      "Navdeep Jaitly",
      "Joshua Susskind"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0a0e2c6a487314f821346bdc04869e36-Abstract-Conference.html": {
    "title": "Schur Nets: exploiting local structure for equivariance in higher order graph neural networks",
    "volume": "main",
    "abstract": "Recent works have shown that extending the message passing paradigm to subgraphs communicating with other subgraphs, especially via higher order messages, can boost the expressivity of graph neural networks. In such architectures, to faithfully account for local structure such as cycles, the local operations must be equivariant to the automorphism group of the local environment. However, enumerating the automorphism groups of all subgraphs of interest and finding appropriate equivariant operations for each one of them separately is generally not feasible. In this paper we propose a solution to this problem based on spectral graph theory that bypasses having to determine the automorphism group entirely and constructs a basis for equivariant operations directly from the graph Laplacian. We show that this approach can boost the performance of GNNs on some standard benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "QINGQI ZHANG",
      "Ruize Xu",
      "Risi Kondor"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0a0eba34ab2ff40ca2d2843324dcc4ab-Abstract-Conference.html": {
    "title": "Visual Fourier Prompt Tuning",
    "volume": "main",
    "abstract": "With the scale of vision Transformer-based models continuing to grow, finetuning these large-scale pretrained models for new tasks has become increasingly parameter-intensive. Visual prompt tuning is introduced as a parameter-efficient finetuning (PEFT) method to this trend. Despite its successes, a notable research challenge persists within almost all PEFT approaches: significant performance degradation is observed when there is a substantial disparity between the datasets applied in pretraining and finetuning phases. To address this challenge, we draw inspiration from human visual cognition, and propose the Visual Fourier Prompt Tuning (VFPT) method as a general and effective solution for adapting large-scale transformer-based models. Our approach innovatively incorporates the Fast Fourier Transform into prompt embeddings and harmoniously considers both spatial and frequency domain information. Apart from its inherent simplicity and intuitiveness, VFPT exhibits superior performance across all datasets, offering a general solution to dataset challenges, irrespective of data disparities. Empirical results demonstrate that our approach outperforms current state-of-the-art baselines on two benchmarks, with low parameter usage (e.g., 0.57% of model parameters on VTAB-1k) and notable performance enhancements (e.g., 73.20% of mean accuracy on VTAB-1k). Our code is avaliable at https://github.com/runtsang/VFPT",
    "checked": true,
    "id": "6e8490d951de04002706b2f4cc43a338124af12a",
    "semantic_title": "visual fourier prompt tuning",
    "citation_count": 8,
    "authors": [
      "Runjia Zeng",
      "Cheng Han",
      "Qifan Wang",
      "Chunshu Wu",
      "Tong Geng",
      "Lifu Huangg",
      "Ying Nian Wu",
      "Dongfang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0a38bff5278f4554313b6ebe87ab3cd8-Abstract-Conference.html": {
    "title": "Learning Representations for Hierarchies with Minimal Support",
    "volume": "main",
    "abstract": "When training node embedding models to represent large directed graphs (digraphs), it is impossible to observe all entries of the adjacency matrix during training. As a consequence most methods employ sampling. For very large digraphs, however, this means many (most) entries may be unobserved during training. In general, observing every entry would be necessary to uniquely identify a graph, however if we know the graph has a certain property some entries can be omitted - for example, only half the entries would be required for a symmetric graph. In this work, we develop a novel framework to identify a subset of entries required to uniquely distinguish a graph among all transitively-closed DAGs. We give an explicit algorithm to compute the provably minimal set of entries, and demonstrate empirically that one can train node embedding models with greater efficiency and performance, provided the energy function has an appropriate inductive bias. We achieve robust performance on synthetic hierarchies and a larger real-world taxonomy, observing improved convergence rates in a resource-constrained setting while reducing the set of training examples by as much as 99%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Rozonoyer",
      "Michael Boratko",
      "Dhruvesh Patel",
      "Wenlong Zhao",
      "Shib Dasgupta",
      "Hung Le",
      "Andrew McCallum"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0a4e065fb5ee13441c90bb4b4d6072d0-Abstract-Conference.html": {
    "title": "Continuous Partitioning for Graph-Based Semi-Supervised Learning",
    "volume": "main",
    "abstract": "Laplace learning algorithms for graph-based semi-supervised learning have been shown to produce degenerate predictions at low label rates and in imbalanced class regimes, particularly near class boundaries. We propose CutSSL: a framework for graph-based semi-supervised learning based on continuous nonconvex quadratic programming, which provably obtains \\emph{integer} solutions. Our framework is naturally motivated by an \\emph{exact} quadratic relaxation of a cardinality-constrained minimum-cut graph partitioning problem. Furthermore, we show our formulation is related to an optimization problem whose approximate solution is the mean-shifted Laplace learning heuristic, thus providing new insight into the performance of this heuristic. We demonstrate that CutSSL significantly surpasses the current state-of-the-art on k-nearest neighbor graphs and large real-world graph benchmarks across a variety of label rates, class imbalance, and label imbalance regimes. Our implementation is available on Colab\\footnote{\\url{https://colab.research.google.com/drive/1tGU5rxE1N5d0KGcNzlvZ0BgRc7_vob7b?usp=sharing}}",
    "checked": true,
    "id": "4c84710be65e546ee1e3d68b9b310ec455d66374",
    "semantic_title": "continuous partitioning for graph-based semi-supervised learning",
    "citation_count": 1,
    "authors": [
      "Chester Holtz",
      "Pengwen Chen",
      "Zhengchao Wan",
      "Chung-Kuan Cheng",
      "Gal Mishne"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0a85f2414e354f9d61ffea5705a8bbf4-Abstract-Conference.html": {
    "title": "Expressive Gaussian Human Avatars from Monocular RGB Video",
    "volume": "main",
    "abstract": "Nuanced expressiveness, especially through detailed hand and facial expressions, is pivotal for enhancing the realism and vitality of digital human representations.In this work, we aim to learn expressive human avatars from a monocular RGB video; a setting that introduces new challenges in capturing and animating fine-grained details.To this end, we introduce EVA, a drivable human model that can recover fine details based on 3D Gaussians and an expressive parametric human model, SMPL-X.Focused on enhancing expressiveness, our work makes three key contributions.First, we highlight the importance of aligning the SMPL-X model with the video frames for effective avatar learning.Recognizing the limitations of current methods for estimating SMPL-X parameters from in-the-wild videos, we introduce a reconstruction module that significantly improves the image-model alignment.Second, we propose a context-aware adaptive density control strategy, which is adaptively adjusting the gradient thresholds to accommodate the varied granularity across body parts.Third, we develop a feedback mechanism that predicts per-pixel confidence to better guide the optimization of 3D Gaussians.Extensive experiments on two benchmarks demonstrate the superiority of our approach both quantitatively and qualitatively, especially on the fine-grained hand and facial details. We make our code available at the project website: https://evahuman.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hezhen Hu",
      "Zhiwen Fan",
      "Tianhao Wu",
      "Yihan Xi",
      "Seoyoung Lee",
      "Georgios Pavlakos",
      "Zhangyang &quot;Atlas&quot; Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0ad6ebd11593822b8a6d5873ca9c5b0b-Abstract-Conference.html": {
    "title": "Hardness of Learning Neural Networks under the Manifold Hypothesis",
    "volume": "main",
    "abstract": "The manifold hypothesis presumes that high-dimensional data lies on or near a low-dimensional manifold. While the utility of encoding geometric structure has been demonstrated empirically, rigorous analysis of its impact on the learnability of neural networks is largely missing. Several recent results have established hardness results for learning feedforward and equivariant neural networks under i.i.d. Gaussian or uniform Boolean data distributions. In this paper, we investigate the hardness of learning under the manifold hypothesis. We ask, which minimal assumptions on the curvature and regularity of the manifold, if any, render the learning problem efficiently learnable. We prove that learning is hard under input manifolds of bounded curvature by extending proofs of hardness in the SQ and cryptographic settings for boolean data inputs to the geometric setting. On the other hand, we show that additional assumptions on the volume of the data manifold alleviate these fundamental limitations and guarantee learnability via a simple interpolation argument. Notable instances of this regime are manifolds which can be reliably reconstructed via manifold learning. Looking forward, we comment on and empirically explore intermediate regimes of manifolds, which have heterogeneous features commonly found in real world data",
    "checked": true,
    "id": "4a45e6d6184d44fd1163a0a93af273ed4ab41fff",
    "semantic_title": "hardness of learning neural networks under the manifold hypothesis",
    "citation_count": 8,
    "authors": [
      "Bobak Kiani",
      "Jason Wang",
      "Melanie Weber"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0ae94013da7cd459402fd77874e09ee3-Abstract-Conference.html": {
    "title": "TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment",
    "volume": "main",
    "abstract": "Recent advancements in image understanding have benefited from the extensive use of web image-text pairs. However, video understanding remains a challenge despite the availability of substantial web video-text data. This difficulty primarily arises from the inherent complexity of videos and the inefficient language supervision in recent web-collected video-text datasets. In this paper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend large language models (LLMs) for video understanding, without the need for pre-training on real video data. Specifically, we first employ an advanced LLM to automatically generate Textual Videos comprising continuous textual frames, along with corresponding annotations to simulate real video-text data. Then, these annotated textual videos are used to pre-align a language-only LLM with the video modality. To bridge the gap between textual and real videos, we employ the CLIP model as the feature extractor to align image and text modalities. During text-only pre-alignment, the continuous textual frames, encoded as a sequence of CLIP text features, are analogous to continuous CLIP image features, thus aligning the LLM with real video representation. Extensive experiments, including zero-shot evaluation and finetuning on various video understanding tasks, demonstrate that TOPA is an effective and efficient framework for aligning video content with LLMs. In particular, without training on any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of 51.0% on the challenging long-form video understanding benchmark, Egoschema. This performance surpasses previous video-text pre-training approaches and proves competitive with recent GPT-3.5 based video agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Li",
      "Hehe Fan",
      "Yongkang Wong",
      "Mohan S Kankanhalli",
      "Yi Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0aee38a6fe9fffc8b658cfb1d872c1d5-Abstract-Conference.html": {
    "title": "Multi-Label Open Set Recognition",
    "volume": "main",
    "abstract": "In multi-label learning, each training instance is associated with multiple labels simultaneously. Traditional multi-label learning studies primarily focus on closed set scenario, i.e. the class label set of test data is identical to those used in training phase. Nevertheless, in numerous real-world scenarios, the environment is open and dynamic where unknown labels may emerge gradually during testing. In this paper, the problem of multi-label open set recognition (MLOSR) is investigated, which poses significant challenges in classifying and recognizing instances with unknown labels in multi-label setting. To enable open set multi-label prediction, a novel approach named SLAN is proposed by leveraging sub-labeling information enriched by structural information in the feature space. Accordingly, unknown labels are recognized by differentiating the sub-labeling information from holistic supervision. Experimental results on various datasets validate the effectiveness of the proposed approach in dealing with the MLOSR problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibo Wang",
      "Jun-Yi Hang",
      "Min-Ling Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0b081a44ed0b8c0c4aa6bd886a60bea4-Abstract-Conference.html": {
    "title": "Med-Real2Sim: Non-Invasive Medical Digital Twins using Physics-Informed Self-Supervised Learning",
    "volume": "main",
    "abstract": "A digital twin is a virtual replica of a real-world physical phenomena that uses mathematical modeling to characterize and simulate its defining features. By constructing digital twins for disease processes, we can perform in-silico simulations that mimic patients' health conditions and counterfactual outcomes under hypothetical interventions in a virtual setting. This eliminates the need for invasive procedures or uncertain treatment decisions. In this paper, we propose a method to identify digital twin model parameters using only noninvasive patient health data. We approach the digital twin modeling as a composite inverse problem, and observe that its structure resembles pretraining and finetuning in self-supervised learning (SSL). Leveraging this, we introduce a physics-informed SSL algorithm that initially pretrains a neural network on the pretext task of learning a differentiable simulator of a physiological process. Subsequently, the model is trained to reconstruct physiological measurements from noninvasive modalities while being constrained by the physical equations learned in pretraining. We apply our method to identify digital twins of cardiac hemodynamics using noninvasive echocardiogram videos, and demonstrate its utility in unsupervised disease detection and in-silico clinical trials",
    "checked": true,
    "id": "555b9e789989b5ddc50ae1236315e5e1de6c43e2",
    "semantic_title": "med-real2sim: non-invasive medical digital twins using physics-informed self-supervised learning",
    "citation_count": 2,
    "authors": [
      "Keying Kuang",
      "Frances Dean",
      "Jack B. Jedlicki",
      "David Ouyang",
      "Anthony Philippakis",
      "David Sontag",
      "Ahmed M. Alaa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0b135d408253205ba501d55c6539bfc7-Abstract-Conference.html": {
    "title": "SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition",
    "volume": "main",
    "abstract": "Visual place recognition (VPR) is an essential task for multiple applications such as augmented reality and robot localization. Over the past decade, mainstream methods in the VPR area have been to use feature representation based on global aggregation, as exemplified by NetVLAD. These features are suitable for large-scale VPR and robust against viewpoint changes. However, the VLAD-based aggregation methods usually learn a large number of (e.g., 64) clusters and their corresponding cluster centers, which directly leads to a high dimension of the yielded global features. More importantly, when there is a domain gap between the data in training and inference, the cluster centers determined on the training set are usually improper for inference, resulting in a performance drop. To this end, we first attempt to improve NetVLAD by removing the cluster center and setting only a small number of (e.g., only 4) clusters. The proposed method not only simplifies NetVLAD but also enhances the generalizability across different domains. We name this method SuperVLAD. In addition, by introducing ghost clusters that will not be retained in the final output, we further propose a very low-dimensional 1-Cluster VLAD descriptor, which has the same dimension as the output of GeM pooling but performs notably better. Experimental results suggest that, when paired with a transformer-based backbone, our SuperVLAD shows better domain generalization performance than NetVLAD with significantly fewer parameters. The proposed method also surpasses state-of-the-art methods with lower feature dimensions on several benchmark datasets. The code is available at https://github.com/lu-feng/SuperVLAD",
    "checked": true,
    "id": "61265315ed6fd5628852a1eed2b3ab3556db62b5",
    "semantic_title": "supervlad: compact and robust image descriptors for visual place recognition",
    "citation_count": 2,
    "authors": [
      "Feng Lu",
      "Xinyao Zhang",
      "Canming Ye",
      "Shuting Dong",
      "Lijun Zhang",
      "Xiangyuan Lan",
      "Chun Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0b2b199fdd52089b31d3a0120e400b2a-Abstract-Conference.html": {
    "title": "Towards Scalable and Stable Parallelization of Nonlinear RNNs",
    "volume": "main",
    "abstract": "Transformers and linear state space models can be evaluated in parallel on modern hardware, but evaluating nonlinear RNNs appears to be an inherently sequential problem. Recently, however, Lim et al. '24 developed an approach called DEER, which evaluates nonlinear RNNs in parallel by posing the states as the solution to a fixed-point problem. They derived a parallel form of Newton's method to solve the fixed-point problem and achieved significant speedups over sequential evaluation. However, the computational complexity of DEER is cubic in the state size, and the algorithm can suffer from numerical instability. We address these limitations with two novel contributions. To reduce the computational complexity, we apply quasi-Newton approximations and show they converge comparably to Newton, use less memory, and are faster. To stabilize DEER, we leverage a connection between the Levenberg-Marquardt algorithm and Kalman smoothing, which we call ELK. This connection allows us to stabilize Newton's method while using efficient parallelized Kalman smoothing algorithms to retain performance. Through several experiments, we show that these innovations allow for parallel evaluation of nonlinear RNNs at larger scales and with greater stability",
    "checked": true,
    "id": "de5b72c3f8b0802fb860b4d3369dcfc864848944",
    "semantic_title": "towards scalable and stable parallelization of nonlinear rnns",
    "citation_count": 12,
    "authors": [
      "Xavier Gonzalez",
      "Andrew Warrington",
      "Jimmy Smith",
      "Scott Linderman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0b2de71212384ffcaf80ad9fd1a21fe3-Abstract-Conference.html": {
    "title": "3D Gaussian Rendering Can Be Sparser: Efficient Rendering via Learned Fragment Pruning",
    "volume": "main",
    "abstract": "3D Gaussian splatting has recently emerged as a promising technique for novel view synthesis from sparse image sets, yet comes at the cost of requiring millions of 3D Gaussian primitives to reconstruct each 3D scene. This largely limits its application to resource-constrained devices and applications.Despite advances in Gaussian pruning techniques that aim to remove individual 3D Gaussian primitives, the significant reduction in primitives often fails to translate into commensurate increases in rendering speed, impeding efficiency and practical deployment. We identify that this discrepancy arises due to the overlooked impact of fragment count per Gaussian (i.e., the number of pixels each Gaussian is projected onto). To bridge this gap and meet the growing demands for efficient on-device 3D Gaussian rendering, we propose fragment pruning, an orthogonal enhancement to existing pruning methods that can significantly accelerate rendering by selectively pruning fragments within each Gaussian. Our pruning framework dynamically optimizes the pruning threshold for each Gaussian, markedly improving rendering speed and quality. Extensive experiments in both static and dynamic scenes validate the effectiveness of our approach. For instance, by integrating our fragment pruning technique with state-of-the-art Gaussian pruning methods, we achieve up to a 1.71$\\times$ speedup on an edge GPU device, the Jetson Orin NX, and enhance rendering quality by an average of 0.16 PSNR on the Tanks\\&Temples dataset. Our code is available at https://github.com/GATECH-EIC/Fragment-Pruning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhifan Ye",
      "Chenxi Wan",
      "Chaojian Li",
      "Jihoon Hong",
      "Sixu Li",
      "Leshu Li",
      "Yongan Zhang",
      "Yingyan (Celine) Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0b43289db08ed60edc6451cb2132e203-Abstract-Conference.html": {
    "title": "Trade-Offs of Diagonal Fisher Information Matrix Estimators",
    "volume": "main",
    "abstract": "The Fisher information matrix can be used to characterize the local geometry ofthe parameter space of neural networks. It elucidates insightful theories anduseful tools to understand and optimize neural networks. Given its highcomputational cost, practitioners often use random estimators and evaluate onlythe diagonal entries. We examine two popular estimators whose accuracy and samplecomplexity depend on their associated variances. We derive bounds of thevariances and instantiate them in neural networks for regression andclassification. We navigate trade-offs for both estimators based on analyticaland numerical studies. We find that the variance quantities depend on thenon-linearity w.r.t. different parameter groups and should not be neglected whenestimating the Fisher information",
    "checked": false,
    "id": "1f0d29c9a64044d085049e667044edd4abc126d2",
    "semantic_title": "tradeoffs of diagonal fisher information matrix estimators",
    "citation_count": 3,
    "authors": [
      "Alexander Soen",
      "Ke Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0b5669c3b07bb8429af19a7919376ff5-Abstract-Conference.html": {
    "title": "End-to-end Learnable Clustering for Intent Learning in Recommendation",
    "volume": "main",
    "abstract": "Intent learning, which aims to learn users' intents for user understanding and item recommendation, has become a hot research spot in recent years. However, existing methods suffer from complex and cumbersome alternating optimization, limiting performance and scalability. To this end, we propose a novel intent learning method termed \\underline{ELCRec}, by unifying behavior representation learning into an \\underline{E}nd-to-end \\underline{L}earnable \\underline{C}lustering framework, for effective and efficient \\underline{Rec}ommendation. Concretely, we encode user behavior sequences and initialize the cluster centers (latent intents) as learnable neurons. Then, we design a novel learnable clustering module to separate different cluster centers, thus decoupling users' complex intents. Meanwhile, it guides the network to learn intents from behaviors by forcing behavior embeddings close to cluster centers. This allows simultaneous optimization of recommendation and clustering via mini-batch data. Moreover, we propose intent-assisted contrastive learning by using cluster centers as self-supervision signals, further enhancing mutual promotion. Both experimental results and theoretical analyses demonstrate the superiority of ELCRec from six perspectives. Compared to the runner-up, ELCRec improves NDCG@5 by 8.9\\% and reduces computational costs by 22.5\\% on the Beauty dataset. Furthermore, due to the scalability and universal applicability, we deploy this method on the industrial recommendation system with 130 million page views and achieve promising results. The codes are available on GitHub\\footnote{https://github.com/yueliu1999/ELCRec}. A collection (papers, codes, datasets) of deep group recommendation/intent learning methods is available on GitHub\\footnote{https://github.com/yueliu1999/Awesome-Deep-Group-Recommendation}",
    "checked": true,
    "id": "9c2717a6c75c30ad12e705717e5e122e56b1f60d",
    "semantic_title": "end-to-end learnable clustering for intent learning in recommendation",
    "citation_count": 15,
    "authors": [
      "Yue Liu",
      "Shihao Zhu",
      "Jun Xia",
      "YINGWEI MA",
      "Jian Ma",
      "Xinwang Liu",
      "Shengju Yu",
      "Kejun Zhang",
      "Wenliang Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0b77d3a82b59e9d9899370b378087faf-Abstract-Conference.html": {
    "title": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings",
    "volume": "main",
    "abstract": "Zero-shot graph machine learning, especially with graph neural networks (GNNs), has garnered significant interest due to the challenge of scarce labeled data. While methods like self-supervised learning and graph prompt learning have been extensively explored, they often rely on fine-tuning with task-specific labels, limiting their effectiveness in zero-shot scenarios. Inspired by the zero-shot capabilities of instruction-fine-tuned large language models (LLMs), we introduce a novel framework named Token Embedding-Aligned Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and cross-task zero-shot learners for graph machine learning. Concretely, we pretrain a GNN, aligning its representations with token embeddings of an LLM. We then train a linear projector that transforms the GNN's representations into a fixed number of graph token embeddings without tuning the LLM. A unified instruction is designed for various graph tasks at different levels, such as node classification (node-level) and link prediction (edge-level). These design choices collectively enhance our method's effectiveness in zero-shot learning, setting it apart from existing methods. Experiments show that our graph token embeddings help the LLM predictor achieve state-of-the-art performance on unseen datasets and tasks compared to other methods using LLMs as predictors. Our code is available at https://github.com/W-rudder/TEA-GLM",
    "checked": true,
    "id": "3ba86f9723c7f1185c05a38231b4a2d3765e6a3d",
    "semantic_title": "llms as zero-shot graph learners: alignment of gnn representations with llm token embeddings",
    "citation_count": 25,
    "authors": [
      "Duo Wang",
      "Yuan Zuo",
      "Fengzhi Li",
      "Junjie Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0b7f639ef28a9035a71f7e0c04c1d681-Abstract-Conference.html": {
    "title": "Exploiting Representation Curvature for Boundary Detection in Time Series",
    "volume": "main",
    "abstract": "Boundaries are the timestamps at which a class in a time series changes. Recently, representation-based boundary detection has gained popularity, but its emphasis on consecutive distance difference backfires, especially when the changes are gradual. In this paper, we propose a boundary detection method, RECURVE, based on a novel change metric, the curvature of a representation trajectory, to accommodate both gradual and abrupt changes. Here, a sequence of representations in the representation space is interpreted as a trajectory, and a curvature at each timestamp can be computed. Using the theory of random walk, we formally show that the mean curvature is lower near boundaries than at other points. Extensive experiments using diverse real-world time-series datasets confirm the superiority of RECURVE over state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yooju Shin",
      "Jaehyun Park",
      "Susik Yoon",
      "Hwanjun Song",
      "Byung Suk Lee",
      "Jae-Gil Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0b8705a611ed1ce19cdb759031078705-Abstract-Conference.html": {
    "title": "KnowGPT: Knowledge Graph based Prompting for Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in many real-world applications. Nonetheless, LLMs are often criticized for their tendency to produce hallucinations, wherein the models fabricate incorrect statements on tasks beyond their knowledge and perception. To alleviate this issue, graph retrieval-augmented generation (GraphRAG) has been extensively explored which leverages the factual knowledge in knowledge graphs (KGs) to ground the LLM's responses in established facts and principles. However, most state-of-the-art LLMs are closed-source, making it challenging to develop a prompting framework that can efficiently and effectively integrate KGs into LLMs with hard prompts only. Generally, existing KG-enhanced LLMs usually suffer from three critical issues, including huge search space, high API costs, and laborious prompt engineering, that impede their widespread application in practice. To this end, we introduce a novel Knowledge Graph based PrompTing framework, namely KnowGPT, to enhance LLMs with domain knowledge. KnowGPT contains a knowledge extraction module to extract the most informative knowledge from KGs, and a context-aware prompt construction module to automatically convert extracted knowledge into effective prompts. Experiments on three benchmarks demonstrate that KnowGPT significantly outperforms all competitors. Notably, KnowGPT achieves a 92.6% accuracy on OpenbookQA leaderboard, comparable to human-level performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinggang Zhang",
      "Junnan Dong",
      "Hao Chen",
      "Daochen Zha",
      "Zailiang Yu",
      "Xiao Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0b8e4c8468273ee3bafb288229c0acbc-Abstract-Conference.html": {
    "title": "UDC: A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems",
    "volume": "main",
    "abstract": "Single-stage neural combinatorial optimization solvers have achieved near-optimal results on various small-scale combinatorial optimization (CO) problems without requiring expert knowledge. However, these solvers exhibit significant performance degradation when applied to large-scale CO problems. Recently, two-stage neural methods motivated by divide-and-conquer strategies have shown efficiency in addressing large-scale CO problems. Nevertheless, the performance of these methods highly relies on problem-specific heuristics in either the dividing or the conquering procedure, which limits their applicability to general CO problems. Moreover, these methods employ separate training schemes and ignore the interdependencies between the dividing and conquering strategies, often leading to sub-optimal solutions. To tackle these drawbacks, this article develops a unified neural divide-and-conquer framework (i.e., UDC) for solving general large-scale CO problems. UDC offers a Divide-Conquer-Reunion (DCR) training method to eliminate the negative impact of a sub-optimal dividing policy. Employing a high-efficiency Graph Neural Network (GNN) for global instance dividing and a fixed-length sub-path solver for conquering divided sub-problems, the proposed UDC framework demonstrates extensive applicability, achieving superior performance in 10 representative large-scale CO problems. The code is available at https://github.com/CIAM-Group/NCOcode/tree/main/singleobjective/UDC-Large-scale-CO-master",
    "checked": true,
    "id": "3beb3da69b0828e638c6162261412e5b1b726575",
    "semantic_title": "udc: a unified neural divide-and-conquer framework for large-scale combinatorial optimization problems",
    "citation_count": 22,
    "authors": [
      "Zhi Zheng",
      "Changliang Zhou",
      "Tong Xialiang",
      "Mingxuan Yuan",
      "Zhenkun Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0b9536e186a77feff516893a5f393f7a-Abstract-Conference.html": {
    "title": "Eye-gaze Guided Multi-modal Alignment for Medical Representation Learning",
    "volume": "main",
    "abstract": "In the medical multi-modal frameworks, the alignment of cross-modality features presents a significant challenge. However, existing works have learned features that are implicitly aligned from the data, without considering the explicit relationships in the medical context. This data-reliance may lead to low generalization of the learned alignment relationships. In this work, we propose the Eye-gaze Guided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for better alignment of medical visual and textual features. We explore the natural auxiliary role of radiologists' eye-gaze data in aligning medical images and text, and introduce a novel approach by using eye-gaze data, collected synchronously by radiologists during diagnostic evaluations. We conduct downstream tasks of image classification and image-text retrieval on four medical datasets, where EGMA achieved state-of-the-art performance and stronger generalization across different datasets. Additionally, we explore the impact of varying amounts of eye-gaze data on model performance, highlighting the feasibility and utility of integrating this auxiliary data into multi-modal alignment framework",
    "checked": true,
    "id": "78dfc26b803efbdd1b837354a10f54aaff4a4ecb",
    "semantic_title": "eye-gaze guided multi-modal alignment for medical representation learning",
    "citation_count": 3,
    "authors": [
      "Chong Ma",
      "Hanqi Jiang",
      "Wenting Chen",
      "Yiwei Li",
      "Zihao Wu",
      "Xiaowei Yu",
      "Zhengliang Liu",
      "Lei Guo",
      "Dajiang Zhu",
      "Tuo Zhang",
      "Dinggang Shen",
      "Tianming Liu",
      "Xiang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0b99315234cc95e6ef281f9155b68832-Abstract-Conference.html": {
    "title": "Federated Ensemble-Directed Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "We consider the problem of federated offline reinforcement learning (RL), a scenario under which distributed learning agents must collaboratively learn a high-quality control policy only using small pre-collected datasets generated according to different unknown behavior policies. Na\\\"{i}vely combining a standard offline RL approach with a standard federated learning approach to solve this problem can lead to poorly performing policies. In response, we develop the Federated Ensemble-Directed Offline Reinforcement Learning Algorithm (FEDORA), which distills the collective wisdom of the clients using an ensemble learning approach. We develop the FEDORA codebase to utilize distributed compute resources on a federated learning platform. We show that FEDORA significantly outperforms other approaches, including offline RL over the combined data pool, in various complex continuous control environments and real-world datasets. Finally, we demonstrate the performance of FEDORA in the real-world on a mobile robot. We provide our code and a video of our experiments at \\url{https://github.com/DesikRengarajan/FEDORA}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Desik Rengarajan",
      "Nitin Ragothaman",
      "Dileep Kalathil",
      "Srinivas Shakkottai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0ba385c3ea3bb417ac6d6a33e24411bc-Abstract-Conference.html": {
    "title": "Linking In-context Learning in Transformers to Human Episodic Memory",
    "volume": "main",
    "abstract": "Understanding connections between artificial and biological intelligent systems can reveal fundamental principles of general intelligence. While many artificial intelligence models have a neuroscience counterpart, such connections are largely missing in Transformer models and the self-attention mechanism. Here, we examine the relationship between interacting attention heads and human episodic memory. We focus on induction heads, which contribute to in-context learning in Transformer-based large language models (LLMs). We demonstrate that induction heads are behaviorally, functionally, and mechanistically similar to the contextual maintenance and retrieval (CMR) model of human episodic memory. Our analyses of LLMs pre-trained on extensive text data show that CMR-like heads often emerge in the intermediate and late layers, qualitatively mirroring human memory biases. The ablation of CMR-like heads suggests their causal role in in-context learning. Our findings uncover a parallel between the computational mechanisms of LLMs and human memory, offering valuable insights into both research fields",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji-An Li",
      "Corey Zhou",
      "Marcus Benna",
      "Marcelo G Mattar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0bac492172db3311c7e116098cfcf521-Abstract-Conference.html": {
    "title": "Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning",
    "volume": "main",
    "abstract": "Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insights for solving scientific problems based on partial differential equations (PDEs). However, being data-intensive, these methods still require a large amount of PDE data. This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding these expensive simulations. In this work, seeking data efficiency, we design unsupervised pretraining for PDE operator learning. To reduce the need for training data with heavy simulation costs, we mine unlabeled PDE data without simulated solutions,and we pretrain neural operators with physics-inspired reconstruction-based proxy tasks. To improve out-of-distribution performance, we further assist neural operators in flexibly leveraging a similarity-based method that learns in-context examples, without incurring extra training costs or designs. Extensive empirical evaluations on a diverse set of PDEs demonstrate that our method is highly data-efficient, more generalizable, and even outperforms conventional vision-pretrained models. We provide our code at https://github.com/delta-lab-ai/dataefficientnopt",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wuyang Chen",
      "Jialin Song",
      "Pu Ren",
      "Shashank Subramanian",
      "Dmitriy Morozov",
      "Michael W. Mahoney"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0bd32794b26cfc99214b89313764da8e-Abstract-Conference.html": {
    "title": "IMAGPose: A Unified Conditional Framework for Pose-Guided Person Generation",
    "volume": "main",
    "abstract": "Diffusion models represent a promising avenue for image generation, having demonstrated competitive performance in pose-guided person image generation. However, existing methods are limited to generating target images from a source image and a target pose, overlooking two critical user scenarios: generating multiple target images with different poses simultaneously and generating target images from multi-view source images.To overcome these limitations, we propose IMAGPose, a unified conditional framework for pose-guided image generation, which incorporates three pivotal modules: a feature-level conditioning (FLC) module, an image-level conditioning (ILC) module, and a cross-view attention (CVA) module. Firstly, the FLC module combines the low-level texture feature from the VAE encoder with the high-level semantic feature from the image encoder, addressing the issue of missing detail information due to the absence of a dedicated person image feature extractor. Then, the ILC module achieves an alignment of images and poses to adapt to flexible and diverse user scenarios by injecting a variable number of source image conditions and introducing a masking strategy.Finally, the CVA module introduces decomposing global and local cross-attention, ensuring local fidelity and global consistency of the person image when multiple source image prompts. The three modules of IMAGPose work together to unify the task of person image generation under various user scenarios.Extensive experiment results demonstrate the consistency and photorealism of our proposed IMAGPose under challenging user scenarios. The code and model will be available at https://github.com/muzishen/IMAGPose",
    "checked": true,
    "id": "81c24ad3a942011f135f74deb29d150997240e2f",
    "semantic_title": "imagpose: a unified conditional framework for pose-guided person generation",
    "citation_count": 59,
    "authors": [
      "Fei Shen",
      "Jinhui Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0bd7c1a579459520e4d731a14b7bda7d-Abstract-Conference.html": {
    "title": "Supervised Kernel Thinning",
    "volume": "main",
    "abstract": "The kernel thinning algorithm of Dwivedi & Mackey (2024) provides a better-than-i.i.d. compression of a generic set of points. By generating high-fidelity coresets of size significantly smaller than the input points, KT is known to speed up unsupervised tasks like Monte Carlo integration, uncertainty quantification, and non-parametric hypothesis testing, with minimal loss in statistical accuracy. In this work, we generalize the KT algorithm to speed up supervised learning problems involving kernel methods. Specifically, we combine two classical algorithms---Nadaraya-Watson (NW) regression or kernel smoothing, and kernel ridge regression (KRR)---with KT to provide a quadratic speed-up in both training and inference times. We show how distribution compression with KT in each setting reduces to constructing an appropriate kernel, and introduce the Kernel-Thinned NW and Kernel-Thinned KRR estimators. We prove that KT-based regression estimators enjoy significantly superior computational efficiency over the full-data estimators and improved statistical efficiency over i.i.d. subsampling of the training data. En route, we also provide a novel multiplicative error guarantee for compressing with KT. We validate our design choices with both simulations and real data experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Albert Gong",
      "Kyuseong Choi",
      "Raaz Dwivedi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0be40478ab6ee0006ee3b38b158bbc8f-Abstract-Conference.html": {
    "title": "Cell ontology guided transcriptome foundation model",
    "volume": "main",
    "abstract": "Transcriptome foundation models (TFMs) hold great promises of deciphering the transcriptomic language that dictate diverse cell functions by self-supervised learning on large-scale single-cell gene expression data, and ultimately unraveling the complex mechanisms of human diseases. However, current TFMs treat cells as independent samples and ignore the taxonomic relationships between cell types, which are available in cell ontology graphs. We argue that effectively leveraging this ontology information during the TFM pre-training can improve learning biologically meaningful gene co-expression patterns while preserving TFM as a general purpose foundation model for downstream zero-shot and fine-tuning tasks. To this end, we present single cell, Cell-ontology guided TFM (scCello). We introduce cell-type coherence loss and ontology alignment loss, which are minimized along with the masked gene expression prediction loss during the pre-training. The novel loss component guide scCello to learn the cell-type-specific representation and the structural relation between cell types from the cell ontology graph, respectively. We pre-trained scCello on 22 million cells from CellxGene database leveraging their cell-type labels mapped to the cell ontology graph from Open Biological and Biomedical Ontology Foundry. Our TFM demonstrates competitive generalization and transferability performance over the existing TFMs on biologically important tasks including identifying novel cell types of unseen cells, prediction of cell-type-specific marker genes, and cancer drug responses. Source code and modelweights are available at https://github.com/DeepGraphLearning/scCello",
    "checked": false,
    "id": "1cb60f9050c7553fcff65980cdeb228d7d50f4b8",
    "semantic_title": "cell-ontology guided transcriptome foundation model",
    "citation_count": 3,
    "authors": [
      "XINYU YUAN",
      "Zhihao Zhan",
      "Zuobai Zhang",
      "Manqi Zhou",
      "Jianan Zhao",
      "Boyu Han",
      "Yue Li",
      "Jian Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0c1124bd3be769dacf491d92d499c7d8-Abstract-Conference.html": {
    "title": "Weight Diffusion for Future: Learn to Generalize in Non-Stationary Environments",
    "volume": "main",
    "abstract": "Enabling deep models to generalize in non-stationary environments is vital for real-world machine learning, as data distributions are often found to continually change. Recently, evolving domain generalization (EDG) has emerged to tackle the domain generalization in a time-varying system, where the domain gradually evolves over time in an underlying continuous structure. Nevertheless, it typically assumes multiple source domains simultaneously ready. It still remains an open problem to address EDG in the domain-incremental setting, where source domains are non-static and arrive sequentially to mimic the evolution of training domains. To this end, we propose Weight Diffusion (W-Diff), a novel framework that utilizes the conditional diffusion model in the parameter space to learn the evolving pattern of classifiers during the domain-incremental training process. Specifically, the diffusion model is conditioned on the classifier weights of different historical domain (regarded as a reference point) and the prototypes of current domain, to learn the evolution from the reference point to the classifier weights of current domain (regarded as the anchor point). In addition, a domain-shared feature encoder is learned by enforcing prediction consistency among multiple classifiers, so as to mitigate the overfitting problem and restrict the evolving pattern to be reflected in the classifier as much as possible. During inference, we adopt the ensemble manner based on a great number of target domain-customized classifiers, which are cheaply obtained via the conditional diffusion model, for robust prediction. Comprehensive experiments on both synthetic and real-world datasets show the superior generalization performance of W-Diff on unseen domains in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mixue Xie",
      "Shuang Li",
      "Binhui Xie",
      "Chi Liu",
      "Jian Liang",
      "Zixun Sun",
      "Ke Feng",
      "Chengwei Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0c38f54740062529aa4117a04b583f3c-Abstract-Conference.html": {
    "title": "Interpretable Lightweight Transformer via Unrolling of Learned Graph Smoothness Priors",
    "volume": "main",
    "abstract": "We build interpretable and lightweight transformer-like neural networks by unrolling iterative optimization algorithms that minimize graph smoothness priors---the quadratic graph Laplacian regularizer (GLR) and the $\\ell_1$-norm graph total variation (GTV)---subject to an interpolation constraint. The crucial insight is that a normalized signal-dependent graph learning module amounts to a variant of the basic self-attention mechanism in conventional transformers. Unlike \"black-box\" transformers that require learning of large key, query and value matrices to compute scaled dot products as affinities and subsequent output embeddings, resulting in huge parameter sets, our unrolled networks employ shallow CNNs to learn low-dimensional features per node to establish pairwise Mahalanobis distances and construct sparse similarity graphs. At each layer, given a learned graph, the target interpolated signal is simply a low-pass filtered output derived from the minimization of an assumed graph smoothness prior, leading to a dramatic reduction in parameter count. Experiments for two image interpolation applications verify the restoration performance, parameter efficiency and robustness to covariate shift of our graph-based unrolled networks compared to conventional transformers",
    "checked": true,
    "id": "afa4ce590aa558c47a6ef246b0f4b07ad2f44ed0",
    "semantic_title": "interpretable lightweight transformer via unrolling of learned graph smoothness priors",
    "citation_count": 4,
    "authors": [
      "VIET HO TAM THUC DO",
      "Parham Eftekhar",
      "Seyed Alireza Hosseini",
      "Gene Cheung",
      "Philip A. Chou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0c433d14179ee4d6598f0e3ed85d6bc2-Abstract-Conference.html": {
    "title": "Sketching for Distributed Deep Learning: A Sharper Analysis",
    "volume": "main",
    "abstract": "The high communication cost between the server and the clients is a significant bottleneck in scaling distributed learning for overparametrized deep models. One popular approach for reducing this communication overhead is randomized sketching. However, existing theoretical analyses for sketching-based distributed learning (sketch-DL) either incur a prohibitive dependence on the ambient dimension or need additional restrictive assumptions such as heavy-hitters. Nevertheless, despite existing pessimistic analyses, empirical evidence suggests that sketch-DL is competitive with its uncompressed counterpart, thus motivating a sharper analysis. In this work, we introduce a sharper ambient dimension-independent convergence analysis for sketch-DL using the second-order geometry specified by the loss Hessian. Our results imply ambient dimension-independent communication complexity for sketch-DL. We present empirical results both on the loss Hessian and overall accuracy of sketch-DL supporting our theoretical results. Taken together, our results provide theoretical justification for the observed empirical success of sketch-DL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mayank Shrivastava",
      "Berivan Isik",
      "Qiaobo Li",
      "Sanmi Koyejo",
      "Arindam Banerjee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0c4bc137edaf0eb7f66a87275a8be706-Abstract-Conference.html": {
    "title": "Unified Covariate Adjustment for Causal Inference",
    "volume": "main",
    "abstract": "Causal effect identification and estimation are two crucial tasks in causal inference. Although causal effect identification has been theoretically resolved, many existing estimators only address a subset of scenarios, known as the sequential back-door adjustment (SBD) (Pearl and Robins, 1995) or g-formula (Robins, 1986). Recent efforts for developing general-purpose estimators with broader coverage, incorporating the front-door adjustment (FD) (Pearl, 2000) and more, lack scalability due to the high computational cost of summing over high-dimensional variables. In this paper, we introduce a novel approach that achieves broad coverage of causal estimands beyond the SBD, incorporating various sum-product functionals like the FD, while maintaining scalability -- estimated in polynomial time relative to the number of variables and samples. Specifically, we present the class of UCA for which a scalable and doubly robust estimator is developed. In particular, we illustrate the expressiveness of UCA for a wide spectrum of causal estimands (e.g., SBD, FD, and more) in causal inference. We then develop an estimator that exhibits computational efficiency and doubly robustness. The scalability and robustness of the proposed framework are verified through simulations",
    "checked": true,
    "id": "e0c6d440beab22b36278e06cb216694321911b76",
    "semantic_title": "unified covariate adjustment for causal inference",
    "citation_count": 2,
    "authors": [
      "Yonghan Jung",
      "Jin Tian",
      "Elias Bareinboim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0c7ca207a051228f978971447a56464a-Abstract-Conference.html": {
    "title": "The High Line: Exact Risk and Learning Rate Curves of Stochastic Adaptive Learning Rate Algorithms",
    "volume": "main",
    "abstract": "We develop a framework for analyzing the training and learning rate dynamics on a large class of high-dimensional optimization problems, which we call the high line, trained using one-pass stochastic gradient descent (SGD) with adaptive learning rates. We give exact expressions for the risk and learning rate curves in terms of a deterministic solution to a system of ODEs. We then investigate in detail two adaptive learning rates -- an idealized exact line search and AdaGrad-Norm -- on the least squares problem. When the data covariance matrix has strictly positive eigenvalues, this idealized exact line search strategy can exhibit arbitrarily slower convergence when compared to the optimal fixed learning rate with SGD. Moreover we exactly characterize the limiting learning rate (as time goes to infinity) for line search in the setting where the data covariance has only two distinct eigenvalues. For noiseless targets, we further demonstrate that the AdaGrad-Norm learning rate converges to a deterministic constant inversely proportional to the average eigenvalue of the data covariance matrix, and identify a phase transition when the covariance density of eigenvalues follows a power law distribution. We provideour code for evaluation at https://github.com/amackenzie1/highline2024",
    "checked": true,
    "id": "00350e238223d5d61e432827d13e01923a093d51",
    "semantic_title": "the high line: exact risk and learning rate curves of stochastic adaptive learning rate algorithms",
    "citation_count": 2,
    "authors": [
      "Elizabeth Collins-Woodfin",
      "Inbar Seroussi",
      "Begoña García Malaxechebarría",
      "Andrew W. Mackenzie",
      "Elliot Paquette",
      "Courtney Paquette"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0c8bbdf2657b58fa0a620f650fbdd457-Abstract-Conference.html": {
    "title": "Exploring Adversarial Robustness of Deep State Space Models",
    "volume": "main",
    "abstract": "Deep State Space Models (SSMs) have proven effective in numerous task scenarios but face significant security challenges due to Adversarial Perturbations (APs) in real-world deployments. Adversarial Training (AT) is a mainstream approach to enhancing Adversarial Robustness (AR) and has been validated on various traditional DNN architectures. However, its effectiveness in improving the AR of SSMs remains unclear.While many enhancements in SSM components, such as integrating Attention mechanisms and expanding to data-dependent SSM parameterizations, have brought significant gains in Standard Training (ST) settings, their potential benefits in AT remain unexplored. To investigate this, we evaluate existing structural variants of SSMs with AT to assess their AR performance. We observe that pure SSM structures struggle to benefit from AT, whereas incorporating Attention yields a markedly better trade-off between robustness and generalization for SSMs in AT compared to other components. Nonetheless, the integration of Attention also leads to Robust Overfitting (RO) issues.To understand these phenomena, we empirically and theoretically analyze the output error of SSMs under AP. We find that fixed-parameterized SSMs have output error bounds strictly related to their parameters, limiting their AT benefits, while input-dependent SSMs may face the problem of error explosion. Furthermore, we show that the Attention component effectively scales the output error of SSMs during training, enabling them to benefit more from AT, but at the cost of introducing RO due to its high model complexity.Inspired by this, we propose a simple and effective Adaptive Scaling (AdS) mechanism that brings AT performance close to Attention-integrated SSMs without introducing the issue of RO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biqing Qi",
      "Yiang Luo",
      "Junqi Gao",
      "Pengfei Li",
      "Kai Tian",
      "Zhiyuan Ma",
      "Bowen Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0caf026b344e6c455efc12fe3d254e9f-Abstract-Conference.html": {
    "title": "EASI: Evolutionary Adversarial Simulator Identification for Sim-to-Real Transfer",
    "volume": "main",
    "abstract": "Reinforcement Learning (RL) controllers have demonstrated remarkable performance in complex robot control tasks. However, the presence of reality gap often leads to poor performance when deploying policies trained in simulation directly onto real robots. Previous sim-to-real algorithms like Domain Randomization (DR) requires domain-specific expertise and suffers from issues such as reduced control performance and high training costs. In this work, we introduce Evolutionary Adversarial Simulator Identification (EASI), a novel approach that combines Generative Adversarial Network (GAN) and Evolutionary Strategy (ES) to address sim-to-real challenges. Specifically, we consider the problem of sim-to-real as a search problem, where ES acts as a generator in adversarial competition with a neural network discriminator, aiming to find physical parameter distributions that make the state transitions between simulation and reality as similar as possible. The discriminator serves as the fitness function, guiding the evolution of the physical parameter distributions. EASI features simplicity, low cost, and high fidelity, enabling the construction of a more realistic simulator with minimal requirements for real-world data, thus aiding in transferring simulated-trained policies to the real world. We demonstrate the performance of EASI in both sim-to-sim and sim-to-real tasks, showing superior performance compared to existing sim-to-real algorithms",
    "checked": true,
    "id": "20e66cc5a66790f7e2b19ec365c5a0e0fffd4701",
    "semantic_title": "easi: evolutionary adversarial simulator identification for sim-to-real transfer",
    "citation_count": 1,
    "authors": [
      "Haoyu Dong",
      "Huiqiao Fu",
      "Wentao Xu",
      "Zhehao Zhou",
      "Chunlin Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0cbbdfb0a4098af8dc7a497a5e59aff7-Abstract-Conference.html": {
    "title": "On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games",
    "volume": "main",
    "abstract": "In sequential decision-making problems, the information structure describes the causal dependencies between system variables, encompassing the dynamics of the environment and the agents' actions. Classical models of reinforcement learning (e.g., MDPs, POMDPs) assume a restricted and highly regular information structure, while more general models like predictive state representations do not explicitly model the information structure. By contrast, real-world sequential decision-making problems typically involve a complex and time-varying interdependence of system variables, requiring a rich and flexible representation of information structure. In this paper, we formalize a novel reinforcement learning model which explicitly represents the information structure.We then use this model to carry out an information-structural analysis of the statistical complexity of general sequential decision-making problems, obtaining a characterization via a graph-theoretic quantity of the DAG representation of the information structure. We prove an upper bound on the sample complexity of learning a general sequential decision-making problem in terms of its information structure by exhibiting an algorithm achieving the upper bound. This recovers known tractability results and gives a novel perspective on reinforcement learning in general sequential decision-making problems, providing a systematic way of identifying new tractable classes of problems",
    "checked": true,
    "id": "5961033e6fad52a4fc72db017b5496547425cb72",
    "semantic_title": "on the role of information structure in reinforcement learning for partially-observable sequential teams and games",
    "citation_count": 0,
    "authors": [
      "Awni Altabaa",
      "Zhuoran Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0ccd06ff26fd6a7829293ce90e0e7f7d-Abstract-Conference.html": {
    "title": "On $f$-Divergence Principled Domain Adaptation: An Improved Framework",
    "volume": "main",
    "abstract": "Unsupervised domain adaptation (UDA) plays a crucial role in addressing distribution shifts in machine learning. In this work, we improve the theoretical foundations of UDA proposed in Acuna et al. (2021) by refining their $f$-divergence-based discrepancy and additionally introducing a new measure, $f$-domain discrepancy ($f$-DD). By removing the absolute value function and incorporating a scaling parameter, $f$-DD obtains novel target error and sample complexity bounds, allowing us to recover previous KL-based results and bridging the gap between algorithms and theory presented in Acuna et al. (2021). Using a localization technique, we also develop a fast-rate generalization bound. Empirical results demonstrate the superior performance of $f$-DD-based learning algorithms over previous works in popular UDA benchmarks",
    "checked": false,
    "id": "55c8f423dcc17d779042b0fe6f540d82309d9908",
    "semantic_title": "on f-divergence principled domain adaptation: an improved framework",
    "citation_count": 4,
    "authors": [
      "Ziqiao Wang",
      "Yongyi Mao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0ccd800d59e94679246ec79d4b19587e-Abstract-Conference.html": {
    "title": "LLM-based Skill Diffusion for Zero-shot Policy Adaptation",
    "volume": "main",
    "abstract": "Recent advances in data-driven imitation learning and offline reinforcement learning have highlighted the use of expert data for skill acquisition and the development of hierarchical policies based on these skills. However, these approaches have not significantly advanced in adapting these skills to unseen contexts, which may involve changing environmental conditions or different user requirements. In this paper, we present a novel LLM-based policy adaptation framework LDuS which leverages an LLM to guide the generation process of a skill diffusion model upon contexts specified in language, facilitating zero-shot skill-based policy adaptation to different contexts. To implement the skill diffusion model, we adapt the loss-guided diffusion with a sequential in-painting technique, where target trajectories are conditioned by masking them with past state-action sequences, thereby enabling the robust and controlled generation of skill trajectories in test-time. To have a loss function for a given context, we employ the LLM-based code generation with iterative refinement, by which the code and controlled trajectory are validated to align with the context in a closed-loop manner. Through experiments, we demonstrate the zero-shot adaptability of LDuS to various context types including different specification levels, multi-modality, and varied temporal conditions for several robotic manipulation tasks, outperforming other language-conditioned imitation and planning methods",
    "checked": true,
    "id": "ba6eb892407fece0fff17c628810a8d9a10ea8e0",
    "semantic_title": "llm-based skill diffusion for zero-shot policy adaptation",
    "citation_count": 1,
    "authors": [
      "Woo Kyung Kim",
      "Youngseok Lee",
      "Jooyoung Kim",
      "Honguk Woo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0cdbb43e66ea08c718bc1377ce629128-Abstract-Conference.html": {
    "title": "TARP-VP: Towards Evaluation of Transferred Adversarial Robustness and Privacy on Label Mapping Visual Prompting Models",
    "volume": "main",
    "abstract": "Adversarial robustness and privacy of deep learning (DL) models are two widely studied topics in AI security. Adversarial training (AT) is an effective approach to improve the robustness of DL models against adversarial attacks. However, while models with AT demonstrate enhanced robustness, they become more susceptible to membership inference attacks (MIAs), thus increasing the risk of privacy leakage. This indicates a negative trade-off between adversarial robustness and privacy in general deep learning models. Visual prompting is a novel model reprogramming (MR) technique used for fine-tuning pre-trained models, achieving good performance in vision tasks, especially when combined with the label mapping technique. However, the performance of label-mapping-based visual prompting (LM-VP) under adversarial attacks and MIAs lacks evaluation. In this work, we regard the MR of LM-VP as a unified entity, referred to as the LM-VP model, and take a step toward jointly evaluating the adversarial robustness and privacy of LM-VP models. Experimental results show that the choice of pre-trained models significantly affects the white-box adversarial robustness of LM-VP, and standard AT even substantially degrades its performance. In contrast, transfer AT-trained LM-VP achieves a good trade-off between transferred adversarial robustness and privacy, a finding that has been consistently validated across various pre-trained models",
    "checked": true,
    "id": "0f25bf59c226969a4bfa548b75ed96d284441f86",
    "semantic_title": "tarp-vp: towards evaluation of transferred adversarial robustness and privacy on label mapping visual prompting models",
    "citation_count": 0,
    "authors": [
      "Zhen Chen",
      "Yi Zhang",
      "Fu Wang",
      "Xingyu Zhao",
      "Xiaowei Huang",
      "Wenjie Ruan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0ce1eb87dbb03fdfa872a93d15cfe333-Abstract-Conference.html": {
    "title": "Learning diffusion at lightspeed",
    "volume": "main",
    "abstract": "Diffusion regulates numerous natural processes and the dynamics of many successful generative models. Existing models to learn the diffusion terms from observational data rely on complex bilevel optimization problems and model only the drift of the system.We propose a new simple model, JKOnet, which bypasses the complexity of existing architectures while presenting significantly enhanced representational capabilities: JKOnet recovers the potential, interaction, and internal energy components of the underlying diffusion process. JKOnet* minimizes a simple quadratic loss and outperforms other baselines in terms of sample efficiency, computational complexity, and accuracy. Additionally, JKOnet* provides a closed-form optimal solution for linearly parametrized functionals, and, when applied to predict the evolution of cellular processes from real-world data, it achieves state-of-the-art accuracy at a fraction of the computational cost of all existing methods.Our methodology is based on the interpretation of diffusion processes as energy-minimizing trajectories in the probability space via the so-called JKO scheme, which we study via its first-order optimality conditions",
    "checked": true,
    "id": "e136b8e7261e1835acc89bf0402e956bd4ee5225",
    "semantic_title": "learning diffusion at lightspeed",
    "citation_count": 12,
    "authors": [
      "Antonio Terpin",
      "Nicolas Lanzetti",
      "Martín Gadea",
      "Florian Dorfler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0cf3e7eefb9d643e93e16ff1d94090a7-Abstract-Conference.html": {
    "title": "One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos",
    "volume": "main",
    "abstract": "We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designed token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA",
    "checked": true,
    "id": "4f4f4014979fe35d28444509ad96e8af1effa85d",
    "semantic_title": "one token to seg them all: language instructed reasoning segmentation in videos",
    "citation_count": 42,
    "authors": [
      "Zechen Bai",
      "Tong He",
      "Haiyang Mei",
      "Pichao WANG",
      "Ziteng Gao",
      "Joya Chen",
      "liulei",
      "Zheng Zhang",
      "Mike Zheng Shou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0d0dac08f4199f0c348dd2feace0305a-Abstract-Conference.html": {
    "title": "Rethinking Imbalance in Image Super-Resolution for Efficient Inference",
    "volume": "main",
    "abstract": "Existing super-resolution (SR) methods optimize all model weights equally using $\\mathcal{L}_1$ or $\\mathcal{L}_2$ losses by uniformly sampling image patches without considering dataset imbalances or parameter redundancy, which limits their performance. To address this, we formulate the image SR task as an imbalanced distribution transfer learning problem from a statistical probability perspective, proposing a plug-and-play Weight-Balancing framework (WBSR) to achieve balanced model learning without changing the original model structure and training data. Specifically, we develop a Hierarchical Equalization Sampling (HES) strategy to address data distribution imbalances, enabling better feature representation from texture-rich samples. To tackle model optimization imbalances, we propose a Balanced Diversity Loss (BDLoss) function, focusing on learning texture regions while disregarding redundant computations in smooth regions. After joint training of HES and BDLoss to rectify these imbalances, we present a gradient projection dynamic inference strategy to facilitate accurate and efficient inference. Extensive experiments across various models, datasets, and scale factors demonstrate that our method achieves comparable or superior performance to existing approaches with about 34\\% reduction in computational cost",
    "checked": true,
    "id": "d53c9f096fda8c179872e3325cd1ceeeaa89f0fc",
    "semantic_title": "rethinking imbalance in image super-resolution for efficient inference",
    "citation_count": 1,
    "authors": [
      "Wei Yu",
      "Bowen Yang",
      "Liu Qinglin",
      "Jianing Li",
      "Shengping Zhang",
      "Xiangyang Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0d11b4035d54e449320fc36b0f64ebb7-Abstract-Conference.html": {
    "title": "How Control Information Influences Multilingual Text Image Generation and Editing?",
    "volume": "main",
    "abstract": "Visual text generation has significantly advanced through diffusion models aimed at producing images with readable and realistic text. Recent works primarily use a ControlNet-based framework, employing standard font text images to control diffusion models. Recognizing the critical role of control information in generating high-quality text, we investigate its influence from three perspectives: input encoding, role at different stages, and output features. Our findings reveal that: 1) Input control information has unique characteristics compared to conventional inputs like Canny edges and depth maps. 2) Control information plays distinct roles at different stages of the denoising process. 3) Output control features significantly differ from the base and skip features of the U-Net decoder in the frequency domain. Based on these insights, we propose TextGen, a novel framework designed to enhance generation quality by optimizing control information. We improve input and output features using Fourier analysis to emphasize relevant information and reduce noise. Additionally, we employ a two-stage generation framework to align the different roles of control information at different stages. Furthermore, we introduce an effective and lightweight dataset for training. Our method achieves state-of-the-art performance in both Chinese and English text generation. The code and dataset are available at https://github.com/CyrilSterling/TextGen",
    "checked": true,
    "id": "2c3e846310ba3e36ba0ce494176d884a39c4cd4d",
    "semantic_title": "how control information influences multilingual text image generation and editing?",
    "citation_count": 6,
    "authors": [
      "Boqiang Zhang",
      "Zuan Gao",
      "Yadong Qu",
      "Hongtao Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0d18ab3b5fabfa6fe47c62e711af02f0-Abstract-Conference.html": {
    "title": "SplitNeRF: Split Sum Approximation Neural Field for Joint Geometry, Illumination, and Material Estimation",
    "volume": "main",
    "abstract": "We present a novel approach for digitizing real-world objects by estimating their geometry, material properties, and environmental lighting from a set of posed images with fixed lighting. Our method incorporates into Neural Radiance Field (NeRF) pipelines the split sum approximation used with image-based lighting for real-time physically based rendering. We propose modeling the scene's lighting with a single scene-specific MLP representing pre-integrated image-based lighting at arbitrary resolutions. We accurately model pre-integrated lighting by exploiting a novel regularizer based on efficient Monte Carlo sampling. Additionally, we propose a new method of supervising self-occlusion predictions by exploiting a similar regularizer based on Monte Carlo sampling. Experimental results demonstrate the efficiency and effectiveness of our approach in estimating scene geometry, material properties, and lighting. Our method attains state-of-the-art relighting quality after only ${\\sim}1$ hour of training in a single NVIDIA A100 GPU",
    "checked": true,
    "id": "7e7f3fed72208ad99eb64129d9fb5a571c4c5ce1",
    "semantic_title": "splitnerf: split sum approximation neural field for joint geometry, illumination, and material estimation",
    "citation_count": 1,
    "authors": [
      "Jesus Zarzar",
      "Bernard Ghanem"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0d288dbe7a757672151aa5fb5423ef2e-Abstract-Conference.html": {
    "title": "Flexible task abstractions emerge in linear networks with fast and bounded units",
    "volume": "main",
    "abstract": "Animals survive in dynamic environments changing at arbitrary timescales, but such data distribution shifts are a challenge to neural networks. To adapt to change, neural systems may change a large number of parameters, which is a slow process involving forgetting past information. In contrast, animals leverage distribution changes to segment their stream of experience into tasks and associate them with internal task abstracts. Animals can then respond flexibly by selecting the appropriate task abstraction. However, how such flexible task abstractions may arise in neural systems remains unknown. Here, we analyze a linear gated network where the weights and gates are jointly optimized via gradient descent, but with neuron-like constraints on the gates including a faster timescale, non-negativity, and bounded activity. We observe that the weights self-organize into modules specialized for tasks or sub-tasks encountered, while the gates layer forms unique representations that switch the appropriate weight modules (task abstractions). We analytically reduce the learning dynamics to an effective eigenspace, revealing a virtuous cycle: fast adapting gates drive weight specialization by protecting previous knowledge, while weight specialization in turn increases the update rate of the gating layer. Task switching in the gating layer accelerates as a function of curriculum block size and task training, mirroring key findings in cognitive neuroscience. We show that the discovered task abstractions support generalization through both task and subtask composition, and we extend our findings to a non-linear network switching between two tasks. Overall, our work offers a theory of cognitive flexibility in animals as arising from joint gradient descent on synaptic and neural gating in a neural network architecture",
    "checked": true,
    "id": "bdbc96020be0509df0e19b72ec5f099110d1ed5a",
    "semantic_title": "flexible task abstractions emerge in linear networks with fast and bounded units",
    "citation_count": 2,
    "authors": [
      "Kai Sandbrink",
      "Jan Bauer",
      "Alexandra Proca",
      "Andrew Saxe",
      "Christopher Summerfield",
      "Ali Hummos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0d3090c00d6e877c6e7267efe2734e1b-Abstract-Conference.html": {
    "title": "Adapting Diffusion Models for Improved Prompt Compliance and Controllable Image Synthesis",
    "volume": "main",
    "abstract": "Recent advances in generative modeling with diffusion processes (DPs) enabled breakthroughs in image synthesis. Despite impressive image quality, these models have various prompt compliance problems, including low recall in generating multiple objects, difficulty in generating text in images, and meeting constraints like object locations and pose. For fine-grained editing and manipulation, they also require fine-grained semantic or instance maps that are tedious to produce manually. While prompt compliance can be enhanced by addition of loss functions at inference, this is time consuming and does not scale to complex scenes. To overcome these limitations, this work introduces a new family of $\\textit{Factor Graph Diffusion Models}$ (FG-DMs) that models the joint distribution of images and conditioning variables, such as semantic, sketch, depth or normal maps via a factor graph decomposition. This joint structure has several advantages, including support for efficient sampling based prompt compliance schemes, which produce images of high object recall, semi-automated fine-grained editing, explainability at intermediate levels, ability to produce labeled datasets for the training of downstream models such as segmentation or depth, training with missing data, and continual learning where new conditioning variables can be added with minimal or no modifications to the existing structure. We propose an implementation of FG-DMs by adapting a pre-trained Stable Diffusion (SD) model to implement all FG-DM factors, using only COCO dataset, and show that it is effective in generating images with 15\\% higher recall than SD while retaining its generalization ability. We introduce an attention distillation loss that encourages consistency among the attention maps of all factors, improving the fidelity of the generated conditions and image. We also show that training FG-DMs from scratch on MM-CelebA-HQ, Cityscapes, ADE20K, and COCO produce images of high quality (FID) and diversity (LPIPS)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deepak Sridhar",
      "Abhishek Peri",
      "Rohith Rachala",
      "Nuno Vasconcelos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0d3496dd0cec77a999c98d35003203ca-Abstract-Conference.html": {
    "title": "On the Saturation Effects of Spectral Algorithms in Large Dimensions",
    "volume": "main",
    "abstract": "The saturation effects, which originally refer to the fact that kernel ridge regression (KRR) fails to achieve the information-theoretical lower bound when the regression function is over-smooth, have been observed for almost 20 years and were rigorously proved recently for kernel ridge regression and some other spectral algorithms over a fixed dimensional domain. The main focus of this paper is to explore the saturation effects for a large class of spectral algorithms (including the KRR, gradient descent, etc.) in large dimensional settings where $n \\asymp d^{\\gamma}$. More precisely, we first propose an improved minimax lower bound for the kernel regression problem in large dimensional settings and show that the gradient flow with early stopping strategy will result in an estimator achieving this lower bound (up to a logarithmic factor). Similar to the results in KRR, we can further determine the exact convergence rates (both upper and lower bounds) of a large class of (optimal tuned) spectral algorithms with different qualification $\\tau$'s. In particular, we find that these exact rate curves (varying along $\\gamma$) exhibit the periodic plateau behavior and the polynomial approximation barrier. Consequently, we can fully depict the saturation effects of the spectral algorithms and reveal a new phenomenon in large dimensional settings (i.e., the saturation effect occurs in large dimensional setting as long as the source condition $s>\\tau$ while it occurs in fixed dimensional setting as long as $s>2\\tau$)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihao Lu",
      "haobo Zhang",
      "Yicheng Li",
      "Qian Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0d67ec04032cccf4a21d04c0ae4ab268-Abstract-Conference.html": {
    "title": "Cross-Scale Self-Supervised Blind Image Deblurring via Implicit Neural Representation",
    "volume": "main",
    "abstract": "Blind image deblurring (BID) is an important yet challenging image recovery problem. Most existing deep learning methods require supervised training with ground truth (GT) images. This paper introduces a self-supervised method for BID that does not require GT images. The key challenge is to regularize the training to prevent over-fitting due to the absence of GT images. By leveraging an exact relationship among the blurred image, latent image, and blur kernel across consecutive scales, we propose an effective cross-scale consistency loss. This is implemented by representing the image and kernel with implicit neural representations (INRs), whose resolution-free property enables consistent yet efficient computation for network training across multiple scales. Combined with a progressively coarse-to-fine training scheme, the proposed method significantly outperforms existing self-supervised methods in extensive experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianjing Zhang",
      "Yuhui Quan",
      "Hui Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0d70af566e69f1dfb687791ecf955e28-Abstract-Conference.html": {
    "title": "A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context Reasoning",
    "volume": "main",
    "abstract": "Extensive knowledge graphs (KGs) have been constructed to facilitate knowledge-driven tasks across various scenarios. However, existing work usually develops separate reasoning models for different KGs, lacking the ability to generalize and transfer knowledge across diverse KGs and reasoning settings. In this paper, we propose a prompt-based KG foundation model via in-context learning, namely KG-ICL, to achieve a universal reasoning ability. Specifically, we introduce a prompt graph centered with a query-related example fact as context to understand the query relation. To encode prompt graphs with the generalization ability to unseen entities and relations in queries, we first propose a unified tokenizer that maps entities and relations in prompt graphs to predefined tokens. Then, we propose two message passing neural networks to perform prompt encoding and KG reasoning, respectively. We conduct evaluation on 43 different KGs in both transductive and inductive settings. Results indicate that the proposed KG-ICL outperforms baselines on most datasets, showcasing its outstanding generalization and universal reasoning capabilities. The source code is accessible on GitHub: https://github.com/nju-websoft/KG-ICL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanning Cui",
      "Zequn Sun",
      "Wei Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0d724619af0c483f9effa8ec23707953-Abstract-Conference.html": {
    "title": "Learning to be Smooth: An End-to-End Differentiable Particle Smoother",
    "volume": "main",
    "abstract": "For challenging state estimation problems arising in domains like vision and robotics, particle-based representations attractively enable temporal reasoning about multiple posterior modes. Particle smoothers offer the potential for more accurate offline data analysis by propagating information both forward and backward in time, but have classically required human-engineered dynamics and observation models. Extending recent advances in discriminative training of particle filters, we develop a framework for low-variance propagation of gradients across long time sequences when training particle smoothers. Our \"two-filter\" smoother integrates particle streams that are propagated forward and backward in time, while incorporating stratification and importance weights in the resampling step to provide low-variance gradient estimates for neural network dynamics and observation models. The resulting mixture density particle smoother is substantially more accurate than state-of-the-art particle filters, as well as search-based baselines, for city-scale global vehicle localization from real-world videos and maps",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Younis",
      "Erik B. Sudderth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0d89cf183391e12063cb63ff0d75ed95-Abstract-Conference.html": {
    "title": "Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation",
    "volume": "main",
    "abstract": "Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely un-ordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN (Maronet al., 2019). Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules",
    "checked": true,
    "id": "b8e59871669c0c4f77c414e8b413b1de10078072",
    "semantic_title": "pard: permutation-invariant autoregressive diffusion for graph generation",
    "citation_count": 11,
    "authors": [
      "Lingxiao Zhao",
      "Xueying Ding",
      "Leman Akoglu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0d97fe65d7a1dc12a05642d9fa4cd578-Abstract-Conference.html": {
    "title": "TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy",
    "volume": "main",
    "abstract": "Tables contain factual and quantitative data accompanied by various structures and contents that pose challenges for machine comprehension. Previous methods generally design task-specific architectures and objectives for individual tasks, resulting in modal isolation and intricate workflows. In this paper, we present a novel large vision-language model, TabPedia, equipped with a concept synergy mechanism. In this mechanism, all the involved diverse visual table understanding (VTU) tasks and multi-source visual embeddings are abstracted as concepts. This unified framework allows TabPedia to seamlessly integrate VTU tasks, such as table detection, table structure recognition, table querying, and table question answering, by leveraging the capabilities of large language models (LLMs). Moreover, the concept synergy mechanism enables table perception-related and comprehension-related tasks to work in harmony, as they can effectively leverage the needed clues from the corresponding source perception embeddings. Furthermore, to better evaluate the VTU task in real-world scenarios, we establish a new and comprehensive table VQA benchmark, ComTQA, featuring approximately 9,000 QA pairs. Extensive quantitative and qualitative experiments on both table perception and comprehension tasks, conducted across various public benchmarks, validate the effectiveness of our TabPedia. The superior performance further confirms the feasibility of using LLMs for understanding visual tables when all concepts work in synergy. The benchmark ComTQA has been open-sourced at https://huggingface.co/datasets/ByteDance/ComTQA. The source code and model also have been released at https://github.com/zhaowc-ustc/TabPedia",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weichao Zhao",
      "Hao Feng",
      "Qi Liu",
      "Jingqun Tang",
      "Binghong Wu",
      "Lei Liao",
      "Shu Wei",
      "Yongjie Ye",
      "Hao Liu",
      "Wengang Zhou",
      "Houqiang Li",
      "Can Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0d99a8c048befb6dd6e17d7684adacac-Abstract-Conference.html": {
    "title": "HonestLLM: Toward an Honest and Helpful Large Language Model",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have achieved remarkable success across various industries and applications, owing to their exceptional generative capabilities. Nevertheless, honesty and helpfulness, which ensure safe and useful real-world deployments, have been considered as the longstanding cornerstones in practice. In this paper, we first established comprehensive principles for honesty LLM and further created the HoneSet with 930 queries across six categories, which is designed to evaluate LLMs' ability to maintain honesty. Then, we improved the honesty and helpfulness of LLMs in both training-free and fine-tuning settings. Specifically, we propose a training-free method named Curiosity-Driven Prompting, which enables LLMs to express their internal confusion and uncertainty about the given query and then optimize their responses. Moreover, we also propose a two-stage fine-tuning approach, inspired by curriculum learning, to enhance the honesty and helpfulness of LLMs. The method first teaches LLMs to distinguish between honest and dishonest, and then LLMs are trained to learn to respond more helpfully. Experimental results demonstrated that both of the two proposed methods improve the helpfulness of LLMs while making them maintain honesty. Our research has paved the way for more reliable and trustworthy LLMs in real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gao Chujie",
      "Siyuan Wu",
      "Yue Huang",
      "Dongping Chen",
      "Qihui Zhang",
      "Zhengyan Fu",
      "Yao Wan",
      "Lichao Sun",
      "Xiangliang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0dbc204928f1e111aff6a8cb5b148151-Abstract-Conference.html": {
    "title": "Neural Isometries: Taming Transformations for Equivariant ML",
    "volume": "main",
    "abstract": "Real-world geometry and 3D vision tasks are replete with challenging symmetries that defy tractable analytical expression. In this paper, we introduce Neural Isometries, an autoencoder framework which learns to map the observation space to a general-purpose latent space wherein encodings are related by isometries whenever their corresponding observations are geometrically related in world space. Specifically, we regularize the latent space such that maps between encodings preserve a learned inner product and commute with a learned functional operator, in the same manner as rigid-body transformations commute with the Laplacian. This approach forms an effective backbone for self-supervised representation learning, and we demonstrate that a simple off-the-shelf equivariant network operating in the pre-trained latent space can achieve results on par with meticulously-engineered, handcrafted networks designed to handle complex, nonlinear symmetries. Furthermore, isometric maps capture information about the respective transformations in world space, and we show that this allows us to regress camera poses directly from the coefficients of the maps between encodings of adjacent views of a scene",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Mitchel",
      "Michael J. Taylor",
      "Vincent Sitzmann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0df38cd13520747e1e64e5b123a78ef8-Abstract-Conference.html": {
    "title": "You Only Cache Once: Decoder-Decoder Architectures for Language Models",
    "volume": "main",
    "abstract": "We introduce a decoder-decoder architecture, YOCO, for large language models, which only caches key-value pairs once. It consists of two components, i.e., a cross-decoder stacked upon a self-decoder. The self-decoder efficiently encodes global key-value (KV) caches that are reused by the cross-decoder via cross-attention. The overall model behaves like a decoder-only Transformer, although YOCO only caches once. The design substantially reduces GPU memory demands, yet retains global attention capability. Additionally, the computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage. Experimental results demonstrate that YOCO achieves favorable performance compared to Transformer in various settings of scaling up model size and number of training tokens. We also extend YOCO to 1M context length with near-perfect needle retrieval accuracy. The profiling results show that YOCO improves inference memory, prefill latency, and throughput by orders of magnitude across context lengths and model sizes",
    "checked": true,
    "id": "1d4c48335d841014d0145256c3c4e7f6c426b8fb",
    "semantic_title": "you only cache once: decoder-decoder architectures for language models",
    "citation_count": 78,
    "authors": [
      "Yutao Sun",
      "Li Dong",
      "Yi Zhu",
      "Shaohan Huang",
      "Wenhui Wang",
      "Shuming Ma",
      "Quanlu Zhang",
      "Jianyong Wang",
      "Furu Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0dfa4b8a2eda4521992e2e1672001e2c-Abstract-Conference.html": {
    "title": "Locally Private and Robust Multi-Armed Bandits",
    "volume": "main",
    "abstract": "We study the interplay between local differential privacy (LDP) and robustness to Huber corruption and possibly heavy-tailed rewards in the context of multi-armed bandits (MABs). We consider two different practical settings: LDP-then-Corruption (LTC) where each user's locally private response might be further corrupted during the data collection process, and Corruption-then-LDP (CTL) where each user's raw data may be corrupted such that the LDP mechanism will only be applied to the corrupted data. To start with, we present the first tight characterization of the mean estimation error in high probability under both LTC and CTL settings. Leveraging this new result, we then present an almost tight characterization (up to log factor) of the minimax regret in online MABs and sub-optimality in offline MABs under both LTC and CTL settings, respectively. Our theoretical results in both settings are also corroborated by a set of systematic simulations. One key message in this paper is that LTC is a more difficult setting that leads to a worse performance guarantee compared to the CTL setting (in the minimax sense). Our sharp understanding of LTC and CTL also naturally allows us to give the first tight performance bounds for the most practical setting where corruption could happen both before and after the LDP mechanism. As an important by-product, we also give the first correct and tight regret bound for locally private and heavy-tailed online MABs, i.e., without Huber corruption, by identifying a fundamental flaw in the state-of-the-art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Zhou",
      "Komo(Wei) ZHANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0dfe31d6e703e138d46a7d2fced38b7c-Abstract-Conference.html": {
    "title": "Understanding Information Storage and Transfer in Multi-Modal Large Language Models",
    "volume": "main",
    "abstract": "Understanding the mechanisms of information storage and transfer in Transformer-based models is important for driving model understanding progress. Recent work has studied these mechanisms for Large Language Models (LLMs), revealing insights on how information is stored in a model's parameters and how information flows to and from these parameters in response to specific prompts. However, these studies have not yet been extended to Multi-modal Large Language Models (MLLMs). Given their expanding capabilities and real-world use, we start by studying one aspect of these models -- how MLLMs process information in a factual visual question answering task. We use a constraint-based formulation which views a visual question as having a set of visual or textual constraints that the model's generated answer must satisfy to be correct (e.g. What movie directed by \\emph{the director in this photo} has won a \\emph{Golden Globe}?). Under this setting, we contribute i) a method that extends causal information tracing from pure language to the multi-modal setting, and ii) \\emph{VQA-Constraints}, a test-bed of 9.7K visual questions annotated with constraints. We use these tools to study two open-source MLLMs, LLaVa and multi-modal Phi-2. Our key findings show that these MLLMs rely on MLP and self-attention blocks in much earlier layers for information storage, compared to LLMs whose mid-layer MLPs are more important. We also show that a consistent small subset of visual tokens output by the vision encoder are responsible for transferring information from the image to these causal blocks. We validate these mechanisms by introducing MultEdit a model-editing algorithm that can correct errors and insert new long-tailed information into MLLMs by targeting these causal blocks. We will publicly release our dataset and code",
    "checked": true,
    "id": "828be35293d29e1c2e926bf711f1fc862e5680e9",
    "semantic_title": "understanding information storage and transfer in multi-modal large language models",
    "citation_count": 17,
    "authors": [
      "Samyadeep Basu",
      "Martin Grayson",
      "Cecily Morrison",
      "Besmira Nushi",
      "Soheil Feizi",
      "Daniela Massiceti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0e0a6539d1270ae8cd3a722eaff247ab-Abstract-Conference.html": {
    "title": "Sharing Key Semantics in Transformer Makes Efficient Image Restoration",
    "volume": "main",
    "abstract": "Image Restoration (IR), a classic low-level vision task, has witnessed significant advancements through deep models that effectively model global information. Notably, the emergence of Vision Transformers (ViTs) has further propelled these advancements. When computing, the self-attention mechanism, a cornerstone of ViTs, tends to encompass all global cues, even those from semantically unrelated objects or regions. This inclusivity introduces computational inefficiencies, particularly noticeable with high input resolution, as it requires processing irrelevant information, thereby impeding efficiency. Additionally, for IR, it is commonly noted that small segments of a degraded image, particularly those closely aligned semantically, provide particularly relevant information to aid in the restoration process, as they contribute essential contextual cues crucial for accurate reconstruction. To address these challenges, we propose boosting IR's performance by sharing the key semantics via Transformer for IR (i.e., SemanIR) in this paper. Specifically, SemanIR initially constructs a sparse yet comprehensive key-semantic dictionary within each transformer stage by establishing essential semantic connections for every degraded patch. Subsequently, this dictionary is shared across all subsequent transformer blocks within the same stage. This strategy optimizes attention calculation within each block by focusing exclusively on semantically related components stored in the key-semantic dictionary. As a result, attention calculation achieves linear computational complexity within each window. Extensive experiments across 6 IR tasks confirm the proposed SemanIR's state-of-the-art performance, quantitatively and qualitatively showcasing advancements. The visual results, code, and trained models are available at: https://github.com/Amazingren/SemanIR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Ren",
      "Yawei Li",
      "Jingyun Liang",
      "Rakesh Ranjan",
      "Mengyuan Liu",
      "Rita Cucchiara",
      "Luc V Gool",
      "Ming-Hsuan Yang",
      "Nicu Sebe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0e21697f3b9385f6c4100aad94c26bed-Abstract-Conference.html": {
    "title": "Breaking Long-Tailed Learning Bottlenecks: A Controllable Paradigm with Hypernetwork-Generated Diverse Experts",
    "volume": "main",
    "abstract": "Traditional long-tailed learning methods often perform poorly when dealing with inconsistencies between training and test data distributions, and they cannot flexibly adapt to different user preferences for trade-offs between head and tail classes. To address this issue, we propose a novel long-tailed learning paradigm that aims to tackle distribution shift in real-world scenarios and accommodate different user preferences for the trade-off between head and tail classes. We generate a set of diverse expert models via hypernetworks to cover all possible distribution scenarios, and optimize the model ensemble to adapt to any test distribution. Crucially, in any distribution scenario, we can flexibly output a dedicated model solution that matches the user's preference. Extensive experiments demonstrate that our method not only achieves higher performance ceilings but also effectively overcomes distribution shift while allowing controllable adjustments according to user preferences. We provide new insights and a paradigm for the long-tailed learning problem, greatly expanding its applicability in practical scenarios. The code can be found here: https://github.com/DataLab-atom/PRL",
    "checked": true,
    "id": "36d7a07fd47744f5fa38cba16c370cbf07627a4c",
    "semantic_title": "breaking long-tailed learning bottlenecks: a controllable paradigm with hypernetwork-generated diverse experts",
    "citation_count": 2,
    "authors": [
      "Zhe Zhao",
      "HaiBin Wen",
      "Zikang Wang",
      "Pengkun Wang",
      "Fanfu Wang",
      "Song Lai",
      "Qingfu Zhang",
      "Yang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0e4d695de4c606494ba9b0f3dac3b57a-Abstract-Conference.html": {
    "title": "Optimal Multiclass U-Calibration Error and Beyond",
    "volume": "main",
    "abstract": "We consider the problem of online multiclass U-calibration, where a forecaster aims to make sequential distributional predictions over $K$ classes with low U-calibration error, that is, low regret with respect to all bounded proper losses simultaneously. Kleinberg et al. (2023) developed an algorithm with U-calibration error $\\mathcal{O}(K\\sqrt{T})$ after $T$ rounds and raised the open question of what the optimal bound is. We resolve this question by showing that the optimal U-calibration error is $\\Theta(\\sqrt{KT})$ --- we start with a simple observation that the Follow-the-Perturbed-Leader algorithm of Daskalakis and Syrgkanis (2016) achieves this upper bound, followed by a matching lower bound constructed with a specific proper loss (which, as a side result, also proves the optimality of the algorithm of Daskalakis and Syrgkanis (2016) in the context of online learning against an adversary with finite choices). We also strengthen our results under natural assumptions on the loss functions, including $\\Theta(\\log T)$ U-calibration error for Lipschitz proper losses, $\\mathcal{O}(\\log T)$ U-calibration error for a certain class of decomposable proper losses, U-calibration error bounds for proper losses with a low covering number, and others",
    "checked": true,
    "id": "cced8f8d8bc1f3223d417a124ef388a1c21d73c8",
    "semantic_title": "optimal multiclass u-calibration error and beyond",
    "citation_count": 6,
    "authors": [
      "Haipeng Luo",
      "Spandan Senapati",
      "Vatsal Sharan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0e5b96f97c1813bb75f6c28532c2ecc7-Abstract-Conference.html": {
    "title": "Text2CAD: Generating Sequential CAD Designs from Beginner-to-Expert Level Text Prompts",
    "volume": "main",
    "abstract": "Prototyping complex computer-aided design (CAD) models in modern softwares can be very time-consuming. This is due to the lack of intelligent systems that can quickly generate simpler intermediate parts. We propose Text2CAD, the first AI framework for generating text-to-parametric CAD models using designer-friendly instructions for all skill levels. Furthermore, we introduce a data annotation pipeline for generating text prompts based on natural language instructions for the DeepCAD dataset using Mistral and LLaVA-NeXT. The dataset contains $\\sim170$K models and $\\sim660$K text annotations, from abstract CAD descriptions (e.g., _generate two concentric cylinders_) to detailed specifications (e.g., _draw two circles with center_ $(x,y)$ and _radius_ $r_{1}$, $r_{2}$, \\textit{and extrude along the normal by} $d$...). Within the Text2CAD framework, we propose an end-to-end transformer-based auto-regressive network to generate parametric CAD models from input texts. We evaluate the performance of our model through a mixture of metrics, including visual quality, parametric precision, and geometrical accuracy. Our proposed framework shows great potential in AI-aided design applications. Project page is available at https://sadilkhan.github.io/text2cad-project/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Sadil Khan",
      "Sankalp Sinha",
      "Talha Uddin",
      "Didier Stricker",
      "Sk Aziz Ali",
      "Muhammad Zeshan Afzal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0e5cce15e1bfc6b3d7b71f24cc5da821-Abstract-Conference.html": {
    "title": "MVGamba: Unify 3D Content Generation as State Space Sequence Modeling",
    "volume": "main",
    "abstract": "Recent 3D large reconstruction models (LRMs) can generate high-quality 3D content in sub-seconds by integrating multi-view diffusion models with scalable multi-view reconstructors. Current works further leverage 3D Gaussian Splatting as 3D representation for improved visual quality and rendering efficiency. However, we observe that existing Gaussian reconstruction models often suffer from multi-view inconsistency and blurred textures. We attribute this to the compromise of multi-view information propagation in favor of adopting powerful yet computationally intensive architectures (\\eg, Transformers). To address this issue, we introduce MVGamba, a general and lightweight Gaussian reconstruction model featuring a multi-view Gaussian reconstructor based on the RNN-like State Space Model (SSM). Our Gaussian reconstructor propagates causal context containing multi-view information for cross-view self-refinement while generating a long sequence of Gaussians for fine-detail modeling with linear complexity.With off-the-shelf multi-view diffusion models integrated, MVGamba unifies 3D generation tasks from a single image, sparse images, or text prompts. Extensive experiments demonstrate that MVGamba outperforms state-of-the-art baselines in all 3D content generation scenarios with approximately only $0.1\\times$ of the model size. The codes are available at \\url{https://github.com/SkyworkAI/MVGamba}",
    "checked": true,
    "id": "abfab6468fc589ce65f5c065ed29d7a2da83cb73",
    "semantic_title": "mvgamba: unify 3d content generation as state space sequence modeling",
    "citation_count": 15,
    "authors": [
      "Xuanyu Yi",
      "Zike Wu",
      "Qiuhong Shen",
      "Qingshan Xu",
      "Pan Zhou",
      "Joo-Hwee Lim",
      "Shuicheng Yan",
      "Xinchao Wang",
      "Hanwang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0e705ac30e573d1526f81a0fd071a151-Abstract-Conference.html": {
    "title": "Make Continual Learning Stronger via C-Flat",
    "volume": "main",
    "abstract": "How to balance the learning 'sensitivity-stability' upon new task training and memory preserving is critical in CL to resolve catastrophic forgetting. Improving model generalization ability within each learning phase is one solution to help CL learning overcome the gap in the joint knowledge space. Zeroth-order loss landscape sharpness-aware minimization is a strong training regime improving model generalization in transfer learning compared with optimizer like SGD. It has also been introduced into CL to improve memory representation or learning efficiency. However, zeroth-order sharpness alone could favors sharper over flatter minima in certain scenarios, leading to a rather sensitive minima rather than a global optima. To further enhance learning stability, we propose a Continual Flatness (C-Flat) method featuring a flatter loss landscape tailored for CL. C-Flat could be easily called with only one line of code and is plug-and-play to any CL methods. A general framework of C-Flat applied to all CL categories and a thorough comparison with loss minima optimizer and flat minima based CL approaches is presented in this paper, showing that our method can boost CL performance in almost all cases. Code is available at https://github.com/WanNaa/C-Flat",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ang Bian",
      "Wei Li",
      "Hangjie Yuan",
      "yu chengrong",
      "Mang Wang",
      "Zixiang Zhao",
      "Aojun Lu",
      "Pengliang Ji",
      "Tao Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0e797d5139ad94fc2dc2080c09119f29-Abstract-Conference.html": {
    "title": "Limits of Transformer Language Models on Learning to Compose Algorithms",
    "volume": "main",
    "abstract": "We analyze the capabilities of Transformer language models in learning compositional discrete tasks. To this end, we evaluate training LLaMA models and prompting GPT-4 and Gemini on four tasks demanding to learn a composition of several discrete sub-tasks. In particular, we measure how well these models can reuse primitives observable in the sub-tasks to learn the composition task. Our results indicate that compositional learning in state-of-the-art Transformer language models is highly sample inefficient: LLaMA requires more data samples than relearning all sub-tasks from scratch to learn the compositional task; in-context prompting with few samples is unreliable and fails at executing the sub-tasks or correcting the errors in multi-round code generation. Further, by leveraging complexity theory, we support these findings with a theoretical analysis focused on the sample inefficiency of gradient descent in memorizing feedforward models. We open source our code at https://github.com/IBM/limitations-lm-algorithmic-compositional-learning",
    "checked": true,
    "id": "9d3815b098f318d704ad2b69bbf88f5acabd121b",
    "semantic_title": "limits of transformer language models on learning to compose algorithms",
    "citation_count": 10,
    "authors": [
      "Jonathan Thomm",
      "Giacomo Camposampiero",
      "Aleksandar Terzic",
      "Michael Hersche",
      "Bernhard Schölkopf",
      "Abbas Rahimi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0e7e2af2e5ba822c9ad35a37b31b5dd4-Abstract-Conference.html": {
    "title": "Stability and Generalization of Asynchronous SGD: Sharper Bounds Beyond Lipschitz and Smoothness",
    "volume": "main",
    "abstract": "Asynchronous stochastic gradient descent (ASGD) has evolved into an indispensable optimization algorithm for training modern large-scale distributed machine learning tasks. Therefore, it is imperative to explore the generalization performance of the ASGD algorithm. However, the existing results are either pessimistic and vacuous or restricted by strict assumptions that fail to reveal the intrinsic impact of asynchronous training on generalization. In this study, we establish sharper stability and generalization bounds for ASGD under much weaker assumptions. Firstly, this paper studies the on-average model stability of ASGD and provides a non-vacuous upper bound on the generalization error, without relying on the Lipschitz assumption. Furthermore, we investigate the excess generalization error of the ASGD algorithm, revealing the effects of asynchronous delay, model initialization, number of training samples and iterations on generalization performance. Secondly, for the first time, this study explores the generalization performance of ASGD in the non-smooth case. We replace smoothness with the much weaker Hölder continuous assumption and achieve similar generalization results as in the smooth case. Finally, we validate our theoretical findings by training numerous machine learning models, including convex problems and non-convex tasks in computer vision and natural language processing",
    "checked": true,
    "id": "62566bb3cfa5944042b76239f1172c7a2a9b9e60",
    "semantic_title": "stability and generalization of asynchronous sgd: sharper bounds beyond lipschitz and smoothness",
    "citation_count": 2,
    "authors": [
      "Xiaoge Deng",
      "Tao Sun",
      "Shengwei Li",
      "Dongsheng Li",
      "Xicheng Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0e8909cae8248c98279f6cd82074aa6d-Abstract-Conference.html": {
    "title": "On the Identifiability of Hybrid Deep Generative Models: Meta-Learning as a Solution",
    "volume": "main",
    "abstract": "The interest in leveraging physics-based inductive bias in deep learning has resulted in recent development of hybrid deep generative models (hybrid-DGMs) that integrates known physics-based mathematical expressions in neural generative models. To identify these hybrid-DGMs requires inferring parameters of the physics-based component along with their neural component. The identifiability of these hybrid-DGMs, however, has not yet been theoretically probed or established. How does the existing theory of the un-identifiability of general DGMs apply to hybrid-DGMs? What may be an effective approach to consutrct a hybrid-DGM with theoretically-proven identifiability? This paper provides the first theoretical probe into the identifiability of hybrid-DGMs, and present meta-learning as a novel solution to construct identifiable hybrid-DGMs. On synthetic and real-data benchmarks, we provide strong empirical evidence for the un-identifiability of existing hybrid-DGMs using unconditional priors, and strong identifiability results of the presented meta-formulations of hybrid-DGMs",
    "checked": true,
    "id": "8f66ede02c93a1c906aa3b2f5955bb6306923146",
    "semantic_title": "on the identifiability of hybrid deep generative models: meta-learning as a solution",
    "citation_count": 0,
    "authors": [
      "Yubo Ye",
      "Maryam Tolou",
      "Sumeet Vadhavkar",
      "Xiajun Jiang",
      "Huafeng Liu",
      "Linwei Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0e9a05f5ce62284c91e4a33498899124-Abstract-Conference.html": {
    "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy. This work introduces MaskLLM, a learnable pruning method that establishes Semi-structured (or ``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks - our method effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks. We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM's learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains. Code is available at https://github.com/NVlabs/MaskLLM",
    "checked": true,
    "id": "60263d3ef1604274e5f095803e9c437f860c511f",
    "semantic_title": "maskllm: learnable semi-structured sparsity for large language models",
    "citation_count": 15,
    "authors": [
      "Gongfan Fang",
      "Hongxu Yin",
      "Saurav Muralidharan",
      "Greg Heinrich",
      "Jeff Pool",
      "Jan Kautz",
      "Pavlo Molchanov",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0eb1ac7551ddbae575415aa5183a88be-Abstract-Conference.html": {
    "title": "Understanding the Gains from Repeated Self-Distillation",
    "volume": "main",
    "abstract": "Self-Distillation is a special type of knowledge distillation where the student model has the same architecture as the teacher model. Despite using the same architecture and the same training data, self-distillation has been empirically observed to improve performance, especially when applied repeatedly. For such a process, there is a fundamental question of interest: How much gain is possible by applying multiple steps of self-distillation? To investigate this relative gain, we propose using the simple but canonical task of linear regression. Our analysis shows that the excess risk achieved by multi-step self-distillation can significantly improve upon a single step of self-distillation, reducing the excess risk by a factor of $d$, where $d$ is the input dimension. Empirical results on regression tasks from the UCI repository show a reduction in the learnt model's risk (MSE) by up to $47$%",
    "checked": true,
    "id": "e3622f469279f3884959957bcd7c7c80babfc138",
    "semantic_title": "understanding the gains from repeated self-distillation",
    "citation_count": 7,
    "authors": [
      "Divyansh Pareek",
      "Simon S Du",
      "Sewoong Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0ed52d7f6f641f228405d48a611e0684-Abstract-Conference.html": {
    "title": "In-Context Learning State Vector with Inner and Momentum Optimization",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have exhibited an impressive ability to perform In-Context Learning (ICL) from only a few examples. Recent works have indicated that the functions learned by ICL can be represented through compressed vectors derived from the transformer. However, the working mechanisms and optimization of these vectors are yet to be thoroughly explored. In this paper, we address this gap by presenting a comprehensive analysis of these compressed vectors, drawing parallels to the parameters trained with gradient descent, and introducing the concept of state vector. Inspired by the works on model soup and momentum-based gradient descent, we propose inner and momentum optimization methods that are applied to refine the state vector progressively as test-time adaptation. Moreover, we simulate state vector aggregation in the multiple example setting, where demonstrations comprising numerous examples are usually too lengthy for regular ICL, and further propose a divide-and-conquer aggregation method to address this challenge. We conduct extensive experiments using Llama-2 and GPT-J in both zero-shot setting and few-shot setting. The experimental results show that our optimization method effectively enhances the state vector and achieves the state-of-the-art performance on diverse tasks",
    "checked": true,
    "id": "b2827857c6d8792622ddc48c829cbd2dcb0981f7",
    "semantic_title": "in-context learning state vector with inner and momentum optimization",
    "citation_count": 9,
    "authors": [
      "Dongfang Li",
      "zhenyu liu",
      "Xinshuo Hu",
      "Zetian Sun",
      "Baotian Hu",
      "Min Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0ef1afa0daa888d695dcd5e9513bafa3-Abstract-Conference.html": {
    "title": "DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving",
    "volume": "main",
    "abstract": "Solving mathematical problems requires advanced reasoning abilities and presents notable challenges for large language models. Previous works usually synthesize data from proprietary models to augment existing datasets, followed by instruction tuning to achieve top-tier results. However, our analysis of these datasets reveals severe biases towards easy queries, with frequent failures to generate any correct response for the most challenging queries.Hypothesizing that difficult queries are crucial to learning complex reasoning, we propose Difficulty-Aware Rejection Tuning (DART), a method that allocates difficult queries more trials during the synthesis phase, enabling more extensive training on difficult samples.Utilizing DART, we have created new datasets for mathematical problem-solving that focus more on difficult queries and are substantially smaller than previous ones. Remarkably, our synthesis process solely relies on a 7B-sized open-weight model, without reliance on the commonly used proprietary GPT-4.We fine-tune various base models on our datasets ranging from 7B to 70B in size, resulting in a series of strong models called DART-Math.In comprehensive in-domain and out-of-domain evaluation on 6 mathematical benchmarks, DART-Math outperforms vanilla rejection tuning significantly, being superior or comparable to previous arts, despite using much smaller datasets and no proprietary models. Furthermore, our results position our synthetic datasets as the most effective and cost-efficient publicly available resources for advancing mathematical problem-solving. Our datasets, models and code are publicly available at https://github.com/hkust-nlp/dart-math",
    "checked": true,
    "id": "c45925acea6a94d881542acb47e43f8924d0b9d3",
    "semantic_title": "dart-math: difficulty-aware rejection tuning for mathematical problem-solving",
    "citation_count": 52,
    "authors": [
      "Yuxuan Tong",
      "Xiwen Zhang",
      "Rui Wang",
      "Ruidong Wu",
      "Junxian He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0f06be0008bc568c88d76206aa17954f-Abstract-Conference.html": {
    "title": "Visual Prompt Tuning in Null Space for Continual Learning",
    "volume": "main",
    "abstract": "Existing prompt-tuning methods have demonstrated impressive performances in continual learning (CL), by selecting and updating relevant prompts in the vision-transformer models. On the contrary, this paper aims to learn each task by tuning the prompts in the direction orthogonal to the subspace spanned by previous tasks' features, so as to ensure no interference on tasks that have been learned to overcome catastrophic forgetting in CL. However, different from the orthogonal projection in the traditional CNN architecture, the prompt gradient orthogonal projection in the ViT architecture shows completely different and greater challenges, i.e., 1) the high-order and non-linear self-attention operation; 2) the drift of prompt distribution brought by the LayerNorm in the transformer block. Theoretically, we have finally deduced two consistency conditions to achieve the prompt gradient orthogonal projection, which provide a theoretical guarantee of eliminating interference on previously learned knowledge via the self-attention mechanism in visual prompt tuning. In practice, an effective null-space-based approximation solution has been proposed to implement the prompt gradient orthogonal projection. Extensive experimental results demonstrate the effectiveness of anti-forgetting on four class-incremental benchmarks with diverse pre-trained baseline models, and our approach achieves superior performances to state-of-the-art methods. Our code is available at https://github.com/zugexiaodui/VPTinNSforCL",
    "checked": true,
    "id": "4eaa9ceb30c0117c6c2a8526f629a5b9da95475a",
    "semantic_title": "visual prompt tuning in null space for continual learning",
    "citation_count": 19,
    "authors": [
      "Yue Lu",
      "Shizhou Zhang",
      "De Cheng",
      "Yinghui Xing",
      "Nannan Wang",
      "PENG WANG",
      "Yanning Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0f25eb6e9dc26c933a5d7516abf1eb8c-Abstract-Conference.html": {
    "title": "The Unmet Promise of Synthetic Training Images: Using Retrieved Real Images Performs Better",
    "volume": "main",
    "abstract": "Generative text-to-image models enable us to synthesize unlimited amounts of images in a controllable manner, spurring many recent efforts to train vision models with synthetic data. However, every synthetic image ultimately originates from the upstream data used to train the generator. Does the intermediate generator provide additional information over directly training on relevant parts of the upstream data? Grounding this question in the setting of image classification, we compare finetuning on task-relevant, targeted synthetic data generated by Stable Diffusion---a generative model trained on the LAION-2B dataset---against finetuning on targeted real images retrieved directly from LAION-2B. We show that while synthetic data can benefit some downstream tasks, it is universally matched or outperformed by real data from the simple retrieval baseline. Our analysis suggests that this underperformance is partially due to generator artifacts and inaccurate task-relevant visual details in the synthetic images. Overall, we argue that targeted retrieval is a critical baseline to consider when training with synthetic data---a baseline that current methods do not yet surpass. We release code, data, and models at https://github.com/scottgeng00/unmet-promise/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Scott Geng",
      "Cheng-Yu Hsieh",
      "Vivek Ramanujan",
      "Matthew Wallingford",
      "Chun-Liang Li",
      "Pang Wei W Koh",
      "Ranjay Krishna"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0f293260c9e3e9527c06920316326114-Abstract-Conference.html": {
    "title": "Reproducibility of predictive networks for mouse visual cortex",
    "volume": "main",
    "abstract": "Deep predictive models of neuronal activity have recently enabled several new discoveries about the selectivity and invariance of neurons in the visual cortex.These models learn a shared set of nonlinear basis functions, which are linearly combined via a learned weight vector to represent a neuron's function.Such weight vectors, which can be thought as embeddings of neuronal function, have been proposed to define functional cell types via unsupervised clustering.However, as deep models are usually highly overparameterized, the learning problem is unlikely to have a unique solution, which raises the question if such embeddings can be used in a meaningful way for downstream analysis.In this paper, we investigate how stable neuronal embeddings are with respect to changes in model architecture and initialization. We find that $L_1$ regularization to be an important ingredient for structured embeddings and develop an adaptive regularization that adjusts the strength of regularization per neuron. This regularization improves both predictive performance and how consistently neuronal embeddings cluster across model fits compared to uniform regularization.To overcome overparametrization, we propose an iterative feature pruning strategy which reduces the dimensionality of performance-optimized models by half without loss of performance and improves the consistency of neuronal embeddings with respect to clustering neurons.Our results suggest that to achieve an objective taxonomy of cell types or a compact representation of the functional landscape, we need novel architectures or learning techniques that improve identifiability. The code is available https://github.com/pollytur/readout_reproducibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Polina Turishcheva",
      "Max Burg",
      "Fabian H. Sinz",
      "Alexander Ecker"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0f4d1fc085b7504c140e66bb26ed8842-Abstract-Conference.html": {
    "title": "Kraken: Inherently Parallel Transformers For Efficient Multi-Device Inference",
    "volume": "main",
    "abstract": "Large Transformer networks are increasingly used in settings where low inference latency is necessary to enable new applications and improve the end-user experience.However, autoregressive inference is resource intensive and requires parallelism for efficiency.Parallelism introduces collective communication that is both expensive and represents a phase when hardware resources are underutilized.Towards mitigating this, Kraken is an evolution of the standard Transformer architecture that is designed to complement existing tensor parallelism schemes for efficient inference on multi-device systems.By introducing a fixed degree of intra-layer model parallelism, the architecture allows collective operations to be overlapped with compute, decreasing latency and increasing hardware utilization.When trained on OpenWebText, Kraken models reach a similar perplexity as standard Transformers while also preserving their language modeling capabilities as evaluated on the SuperGLUE benchmark.Importantly, when tested on multi-GPU systems using TensorRT-LLM engines, Kraken speeds up Time To First Token by a mean of 35.6% across a range of model sizes, context lengths, and degrees of tensor parallelism",
    "checked": true,
    "id": "ec4be85aae5f7d9cfb81e35153e4f6bd672db7a6",
    "semantic_title": "kraken: inherently parallel transformers for efficient multi-device inference",
    "citation_count": 0,
    "authors": [
      "Rohan Baskar Prabhakar",
      "Hengrui Zhang",
      "David Wentzlaff"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0f53ecc0d36a5d5d3d3e94d42c4b23ca-Abstract-Conference.html": {
    "title": "DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection",
    "volume": "main",
    "abstract": "Model-X knockoff has garnered significant attention among various feature selection methods due to its guarantees for controlling the false discovery rate (FDR). Since its introduction in parametric design, knockoff techniques have evolved to handle arbitrary data distributions using deep learning-based generative models. However, we have observed limitations in the current implementations of the deep Model-X knockoff framework. Notably, the \"swap property\" that knockoffs require often faces challenges at the sample level, resulting in diminished selection power. To address these issues, we develop \"Deep Dependency Regularized Knockoff (DeepDRK),\" a distribution-free deep learning method that effectively balances FDR and power. In DeepDRK, we introduce a novel formulation of the knockoff model as a learning problem under multi-source adversarial attacks. By employing an innovative perturbation technique, we achieve lower FDR and higher power. Our model outperforms existing benchmarks across synthetic, semi-synthetic, and real-world datasets, particularly when sample sizes are small and data distributions are non-Gaussian",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Shen",
      "Yici Yan",
      "Zhizhen Jane Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0f588c9d9fa34762362697c1dd463294-Abstract-Conference.html": {
    "title": "Incorporating Surrogate Gradient Norm to Improve Offline Optimization Techniques",
    "volume": "main",
    "abstract": "Offline optimization has recently emerged as an increasingly popular approach to mitigate the prohibitively expensive cost of online experimentation. The key idea is to learn a surrogate of the black-box function that underlines the target experiment using a static (offline) dataset of its previous input-output queries. Such an approach is, however, fraught with an out-of-distribution issue where the learned surrogate becomes inaccurate outside the offline data regimes. To mitigate this, existing offline optimizers have proposed numerous conditioning techniques to prevent the learned surrogate from being too erratic. Nonetheless, such conditioning strategies are often specific to particular surrogate or search models, which might not generalize to a different model choice. This motivates us to develop a model-agnostic approach instead, which incorporates a notion of model sharpness into the training loss of the surrogate as a regularizer. Our approach is supported by a new theoretical analysis demonstrating that reducing surrogate sharpness on the offline dataset provably reduces its generalized sharpness on unseen data. Our analysis extends existing theories from bounding generalized prediction loss (on unseen data) with loss sharpness to bounding the worst-case generalized surrogate sharpness with its empirical estimate on training data, providing a new perspective on sharpness regularization. Our extensive experimentation on a diverse range of optimization tasks also shows that reducing surrogate sharpness often leads to significant improvement, marking (up to) a noticeable 9.6% performance boost. Our code is publicly available at https://github.com/cuong-dm/IGNITE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cuong Dao",
      "Phi Le Nguyen",
      "Truong Thao Nguyen",
      "Nghia Hoang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0f5cb62a8e3331b253c232e229cd551e-Abstract-Conference.html": {
    "title": "Revisiting Adversarial Patches for Designing Camera-Agnostic Attacks against Person Detection",
    "volume": "main",
    "abstract": "Physical adversarial attacks can deceive deep neural networks (DNNs), leading to erroneous predictions in real-world scenarios. To uncover potential security risks, attacking the safety-critical task of person detection has garnered significant attention. However, we observe that existing attack methods overlook the pivotal role of the camera, involving capturing real-world scenes and converting them into digital images, in the physical adversarial attack workflow. This oversight leads to instability and challenges in reproducing these attacks. In this work, we revisit patch-based attacks against person detectors and introduce a camera-agnostic physical adversarial attack to mitigate this limitation. Specifically, we construct a differentiable camera Image Signal Processing (ISP) proxy network to compensate for the physical-to-digital transition gap. Furthermore, the camera ISP proxy network serves as a defense module, forming an adversarial optimization framework with the attack module. The attack module optimizes adversarial patches to maximize effectiveness, while the defense module optimizes the conditional parameters of the camera ISP proxy network to minimize attack effectiveness. These modules engage in an adversarial game, enhancing cross-camera stability. Experimental results demonstrate that our proposed Camera-Agnostic Patch (CAP) attack effectively conceals persons from detectors across various imaging hardware, including two distinct cameras and four smartphones",
    "checked": true,
    "id": "41aac6e7165ea0b3a024ecfae6849c6df38b6ff4",
    "semantic_title": "revisiting adversarial patches for designing camera-agnostic attacks against person detection",
    "citation_count": 4,
    "authors": [
      "Hui Wei",
      "Zhixiang Wang",
      "Kewei Zhang",
      "Jiaqi Hou",
      "Yuanwei Liu",
      "Hao Tang",
      "Zheng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0f6931a9e339a012a9909306d7c758b4-Abstract-Conference.html": {
    "title": "Graph Diffusion Transformers for Multi-Conditional Molecular Generation",
    "volume": "main",
    "abstract": "Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecule generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We present the Graph Diffusion Transformer (Graph DiT) for multi-conditional molecular generation. Graph DiT has a condition encoder to learn the representation of numerical and categorical properties and utilizes a Transformer-based graph denoiser to achieve molecular graph denoising under conditions. Unlike previous graph diffusion models that add noise separately on the atoms and bonds in the forward diffusion process, we propose a graph-dependent noise model for training Graph DiT, designed to accurately estimate graph-related noise in molecules. We extensively validate the Graph DiT for multi-conditional polymer and small molecule generation. Results demonstrate our superiority across metrics from distribution learning to condition control for molecular properties. A polymer inverse design task for gas separation with feedback from domain experts further demonstrates its practical utility. The code is available at https://github.com/liugangcode/Graph-DiT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang Liu",
      "Jiaxin Xu",
      "Tengfei Luo",
      "Meng Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0f934dd2030f5740cde0aa2697a105a9-Abstract-Conference.html": {
    "title": "Reparameterization invariance in approximate Bayesian inference",
    "volume": "main",
    "abstract": "Current approximate posteriors in Bayesian neural networks (BNNs) exhibit a crucial limitation: they fail to maintain invariance under reparameterization, i.e. BNNs assign different posterior densities to different parametrizations of identical functions. This creates a fundamental flaw in the application of Bayesian principles as it breaks the correspondence between uncertainty over the parameters with uncertainty over the parametrized function. In this paper, we investigate this issue in the context of the increasingly popular linearized Laplace approximation. Specifically, it has been observed that linearized predictives alleviate the common underfitting problems of the Laplace approximation. We develop a new geometric view of reparametrizations from which we explain the success of linearization. Moreover, we demonstrate that these reparameterization invariance properties can be extended to the original neural network predictive using a Riemannian diffusion process giving a straightforward algorithm for approximate posterior sampling, which empirically improves posterior fit",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hrittik Roy",
      "Marco Miani",
      "Carl Henrik Ek",
      "Philipp Hennig",
      "Marvin Pförtner",
      "Lukas Tatzel",
      "Søren Hauberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0f93c3e9b557980d93016671acd94bd2-Abstract-Conference.html": {
    "title": "Localized Adaptive Risk Control",
    "volume": "main",
    "abstract": "Adaptive Risk Control (ARC) is an online calibration strategy based on set prediction that offers worst-case deterministic long-term risk control, as well as statistical marginal coverage guarantees. ARC adjusts the size of the prediction set by varying a single scalar threshold based on feedback from past decisions. In this work, we introduce Localized Adaptive Risk Control (L-ARC), an online calibration scheme that targets statistical localized risk guarantees ranging from conditional risk to marginal risk, while preserving the worst-case performance of ARC. L-ARC updates a threshold function within a reproducing kernel Hilbert space (RKHS), with the kernel determining the level of localization of the statistical risk guarantee. The theoretical results highlight a trade-off between localization of the statistical risk and convergence speed to the long-term risk target. Thanks to localization, L-ARC is demonstrated via experiments to produce prediction sets with risk guarantees across different data subpopulations, significantly improving the fairness of the calibrated model for tasks such as image segmentation and beam selection in wireless networks",
    "checked": true,
    "id": "f7f28bf030e5d0e0870b0c515269bfb57af4c137",
    "semantic_title": "localized adaptive risk control",
    "citation_count": 10,
    "authors": [
      "Matteo Zecchin",
      "Osvaldo Simeone"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0f94c552e5fe82bc152494985e34bd48-Abstract-Conference.html": {
    "title": "Theoretical and Empirical Insights into the Origins of Degree Bias in Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) often perform better for high-degree nodes than low-degree nodes on node classification tasks. This degree bias can reinforce social marginalization by, e.g., privileging celebrities and other high-degree actors in social networks during social and content recommendation. While researchers have proposed numerous hypotheses for why GNN degree bias occurs, we find via a survey of 38 degree bias papers that these hypotheses are often not rigorously validated, and can even be contradictory. Thus, we provide an analysis of the origins of degree bias in message-passing GNNs with different graph filters. We prove that high-degree test nodes tend to have a lower probability of misclassification regardless of how GNNs are trained. Moreover, we show that degree bias arises from a variety of factors that are associated with a node's degree (e.g., homophily of neighbors, diversity of neighbors). Furthermore, we show that during training, some GNNs may adjust their loss on low-degree nodes more slowly than on high-degree nodes; however, with sufficiently many epochs of training, message-passing GNNs can achieve their maximum possible training accuracy, which is not significantly limited by their expressive power. Throughout our analysis, we connect our findings to previously-proposed hypotheses for the origins of degree bias, supporting and unifying some while drawing doubt to others. We validate our theoretical findings on 8 common real-world networks, and based on our theoretical and empirical insights, describe a roadmap to alleviate degree bias",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arjun Subramonian",
      "Jian Kang",
      "Yizhou Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0f98645119923217a245735c2c4d23f4-Abstract-Conference.html": {
    "title": "Federated Learning under Periodic Client Participation and Heterogeneous Data: A New Communication-Efficient Algorithm and Analysis",
    "volume": "main",
    "abstract": "In federated learning, it is common to assume that clients are always available to participate in training, which may not be feasible with user devices in practice. Recent works analyze federated learning under more realistic participation patterns, such as cyclic client availability or arbitrary participation. However, all such works either require strong assumptions (e.g., all clients participate almost surely within a bounded window), do not achieve linear speedup and reduced communication rounds, or are not applicable in the general non-convex setting. In this work, we focus on nonconvex optimization and consider participation patterns in which the chance of participation over a fixed window of rounds is equal among all clients, which includes cyclic client availability as a special case. Under this setting, we propose a new algorithm, named Amplified SCAFFOLD, and prove that it achieves linear speedup, reduced communication, and resilience to data heterogeneity simultaneously. In particular, for cyclic participation, our algorithm is proved to enjoy $\\mathcal{O}(\\epsilon^{-2})$ communication rounds to find an $\\epsilon$-stationary point in the non-convex stochastic setting. In contrast, the prior work under the same setting requires $\\mathcal{O}(\\kappa^2 \\epsilon^{-4})$ communication rounds, where $\\kappa$ denotes the data heterogeneity. Therefore, our algorithm significantly reduces communication rounds due to better dependency in terms of $\\epsilon$ and $\\kappa$. Our analysis relies on a fine-grained treatment of the nested dependence between client participation and errors in the control variates, which results in tighter guarantees than previous work. We also provide experimental results with (1) synthetic data and (2) real-world data with a large number of clients $(N = 250)$, demonstrating the effectiveness of our algorithm under periodic client participation",
    "checked": true,
    "id": "ba0e224b22fb2446cc0508eda55b7edd269a7500",
    "semantic_title": "federated learning under periodic client participation and heterogeneous data: a new communication-efficient algorithm and analysis",
    "citation_count": 2,
    "authors": [
      "Michael Crawshaw",
      "Mingrui Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0f9e0309d8a947ca44463a9b7e8b6a3f-Abstract-Conference.html": {
    "title": "xMIL: Insightful Explanations for Multiple Instance Learning in Histopathology",
    "volume": "main",
    "abstract": "Multiple instance learning (MIL) is an effective and widely used approach for weakly supervised machine learning. In histopathology, MIL models have achieved remarkable success in tasks like tumor detection, biomarker prediction, and outcome prognostication. However, MIL explanation methods are still lagging behind, as they are limited to small bag sizes or disregard instance interactions. We revisit MIL through the lens of explainable AI (XAI) and introduce xMIL, a refined framework with more general assumptions. We demonstrate how to obtain improved MIL explanations using layer-wise relevance propagation (LRP) and conduct extensive evaluation experiments on three toy settings and four real-world histopathology datasets. Our approach consistently outperforms previous explanation attempts with particularly improved faithfulness scores on challenging biomarker prediction tasks. Finally, we showcase how xMIL explanations enable pathologists to extract insights from MIL models, representing a significant advance for knowledge discovery and model debugging in digital histopathology",
    "checked": true,
    "id": "20942dc958a73f2b2e3892332fd39c01b8c01b1f",
    "semantic_title": "xmil: insightful explanations for multiple instance learning in histopathology",
    "citation_count": 3,
    "authors": [
      "Julius Hense",
      "Mina Jamshidi Idaji",
      "Oliver Eberle",
      "Thomas Schnake",
      "Jonas Dippel",
      "Laure Ciernik",
      "Oliver Buchstab",
      "Andreas Mock",
      "Frederick Klauschen",
      "Klaus-Robert Müller"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0faa0019b0a8fcab8e6476bc43078e2e-Abstract-Conference.html": {
    "title": "Generalized Linear Bandits with Limited Adaptivity",
    "volume": "main",
    "abstract": "We study the generalized linear contextual bandit problem within the constraints of limited adaptivity. In this paper, we present two algorithms, B-GLinCB and RS-GLinCB, that address, respectively, two prevalent limited adaptivity settings. Given a budget $M$ on the number of policy updates, in the first setting, the algorithm needs to decide upfront $M$ rounds at which it will update its policy, while in the second setting it can adaptively perform $M$ policy updates during its course. For the first setting, we design an algorithm B-GLinCB, that incurs $\\tilde{O}(\\sqrt{T})$ regret when $M = \\Omega( \\log{\\log T} )$ and the arm feature vectors are generated stochastically. For the second setting, we design an algorithm RS-GLinCB that updates its policy $\\tilde{O}(\\log^2 T)$ times and achieves a regret of $\\tilde{O}(\\sqrt{T})$ even when the arm feature vectors are adversarially generated. Notably, in these bounds, we manage to eliminate the dependence on a key instance dependent parameter $\\kappa$, that captures non-linearity of the underlying reward model. Our novel approach for removing this dependence for generalized linear contextual bandits might be of independent interest",
    "checked": true,
    "id": "e1b59e2355f49165118b3af8c68caef84e7b7a09",
    "semantic_title": "generalized linear bandits with limited adaptivity",
    "citation_count": 8,
    "authors": [
      "Ayush Sawarni",
      "Nirjhar Das",
      "Siddharth Barman",
      "Gaurav Sinha"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0fbbc5129cafcee8530223b8565561ac-Abstract-Conference.html": {
    "title": "Image-aware Evaluation of Generated Medical Reports",
    "volume": "main",
    "abstract": "The paper proposes a novel evaluation metric for automatic medical report generation from X-ray images, VLScore. It aims to overcome the limitations of existing evaluation methods, which either focus solely on textual similarities, ignoring clinical aspects, or concentrate only on a single clinical aspect, the pathology, neglecting all other factors. The key idea of our metric is to measure the similarity between radiology reports while considering the corresponding image. We demonstrate the benefit of our metric through evaluation on a dataset where radiologists marked errors in pairs of reports, showing notable alignment with radiologists' judgments. In addition, we provide a new dataset for evaluating metrics. This dataset includes well-designed perturbations that distinguish between significant modifications (e.g., removal of a diagnosis) and insignificant ones. It highlights the weaknesses in current evaluation metrics and provides a clear framework for analysis",
    "checked": true,
    "id": "49c0b2db85ec43b05e75c17642c5f6427d22082e",
    "semantic_title": "image-aware evaluation of generated medical reports",
    "citation_count": 1,
    "authors": [
      "Gefen Dawidowicz",
      "Elad Hirsch",
      "Ayellet Tal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0fd17409385ab9304e5019c6a6eb327a-Abstract-Conference.html": {
    "title": "Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning",
    "volume": "main",
    "abstract": "Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks",
    "checked": true,
    "id": "4362edfd3907204cf1b7ec8e3c16c56db5cd14cf",
    "semantic_title": "large language models-guided dynamic adaptation for temporal knowledge graph reasoning",
    "citation_count": 16,
    "authors": [
      "Jiapu Wang",
      "Sun Kai",
      "LINHAO LUO",
      "Wei Wei",
      "Yongli Hu",
      "Alan Wee-Chung Liew",
      "Shirui Pan",
      "Baocai Yin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0fd3d8093b3ba73d19b393a1326fdba7-Abstract-Conference.html": {
    "title": "An effective framework for estimating individualized treatment rules",
    "volume": "main",
    "abstract": "Estimating individualized treatment rules (ITRs) is fundamental in causal inference, particularly for precision medicine applications. Traditional ITR estimation methods rely on inverse probability weighting (IPW) to address confounding factors and $L_{1}$-penalization for simplicity and interpretability. However, IPW can introduce statistical bias without precise propensity score modeling, while $L_1$-penalization makes the objective non-smooth, leading to computational bias and requiring subgradient methods. In this paper, we propose a unified ITR estimation framework formulated as a constrained, weighted, and smooth convex optimization problem. The optimal ITR can be robustly and effectively computed by projected gradient descent. Our comprehensive theoretical analysis reveals that weights that balance the spectrum of a `weighted design matrix' improve both the optimization and likelihood landscapes, yielding improved computational and statistical estimation guarantees. In particular, this is achieved by distributional covariate balancing weights, which are model-free alternatives to IPW. Extensive simulations and applications demonstrate that our framework achieves significant gains in both robustness and effectiveness for ITR learning against existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joowon Lee",
      "Jared Huling",
      "Guanhua Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0fd489e5e393f61b355be86ed4c24a54-Abstract-Conference.html": {
    "title": "A Simple and Adaptive Learning Rate for FTRL in Online Learning with Minimax Regret of $\\Theta(T^{2/3})$ and its Application to Best-of-Both-Worlds",
    "volume": "main",
    "abstract": "Follow-the-Regularized-Leader (FTRL) is a powerful framework for various online learning problems. By designing its regularizer and learning rate to be adaptive to past observations, FTRL is known to work adaptively to various properties of an underlying environment. However, most existing adaptive learning rates are for online learning problems with a minimax regret of $\\Theta(\\sqrt{T})$ for the number of rounds $T$, and there are only a few studies on adaptive learning rates for problems with a minimax regret of $\\Theta(T^{2/3})$, which include several important problems dealing with indirect feedback. To address this limitation, we establish a new adaptive learning rate framework for problems with a minimax regret of $\\Theta(T^{2/3})$. Our learning rate is designed by matching the stability, penalty, and bias terms that naturally appear in regret upper bounds for problems with a minimax regret of $\\Theta(T^{2/3})$. As applications of this framework, we consider three major problems with a minimax regret of $\\Theta(T^{2/3})$: partial monitoring, graph bandits, and multi-armed bandits with paid observations. We show that FTRL with our learning rate and the Tsallis entropy regularizer improves existing Best-of-Both-Worlds (BOBW) regret upper bounds, which achieve simultaneous optimality in the stochastic and adversarial regimes. The resulting learning rate is surprisingly simple compared to the existing learning rates for BOBW algorithms for problems with a minimax regret of $\\Theta(T^{2/3})$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taira Tsuchiya",
      "Shinji Ito"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0fd4ce94d29be88a5a262a2c77a18f47-Abstract-Conference.html": {
    "title": "Rethinking Fourier Transform from A Basis Functions Perspective for Long-term Time Series Forecasting",
    "volume": "main",
    "abstract": "The interaction between Fourier transform and deep learning opens new avenues for long-term time series forecasting (LTSF). We propose a new perspective to reconsider the Fourier transform from a basis functions perspective. Specifically, the real and imaginary parts of the frequency components can be viewed as the coefficients of cosine and sine basis functions at tiered frequency levels, respectively. We argue existing Fourier-based methods do not involve basis functions thus fail to interpret frequency coefficients precisely and consider the time-frequency relationship sufficiently, leading to inconsistent starting cycles and inconsistent series length issues. Accordingly, a novel Fourier basis mapping (FBM) method addresses these issues by mixing time and frequency domain features through Fourier basis expansion. Differing from existing approaches, FBM (i) embeds the discrete Fourier transform with basis functions, and then (ii) can enable plug-and-play in various types of neural networks for better performance. FBM extracts explicit frequency features while preserving temporal characteristics, enabling the mapping network to capture the time-frequency relationships. By incorporating our unique time-frequency features, the FBM variants can enhance any type of networks like linear, multilayer-perceptron-based, transformer-based, and Fourier-based networks, achieving state-of-the-art LTSF results on diverse real-world datasets with just one or three fully connected layers. The code is available at: https://github.com/runze1223/Fourier-Basis-Mapping",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runze Yang",
      "Longbing Cao",
      "JIE YANG",
      "li jianxun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0fd5675f49141c79ad22d7a533c89b12-Abstract-Conference.html": {
    "title": "Beyond Primal-Dual Methods in Bandits with Stochastic and Adversarial Constraints",
    "volume": "main",
    "abstract": "We address a generalization of the bandit with knapsacks problem, where a learner aims to maximize rewards while satisfying an arbitrary set of long-term constraints. Our goal is to design best-of-both-worlds algorithms that perform optimally under both stochastic and adversarial constraints. Previous works address this problem via primal-dual methods, and require some stringent assumptions, namely the Slater's condition, and in adversarial settings, they either assume knowledge of a lower bound on the Slater's parameter, or impose strong requirements on the primal and dual regret minimizers such as requiring weak adaptivity. We propose an alternative and more natural approach based on optimistic estimations of the constraints. Surprisingly, we show that estimating the constraints with an UCB-like approach guarantees optimal performances.Our algorithm consists of two main components: (i) a regret minimizer working on moving strategy sets and (ii) an estimate of the feasible set as an optimistic weighted empirical mean of previous samples. The key challenge in this approach is designing adaptive weights that meet the different requirements for stochastic and adversarial constraints. Our algorithm is significantly simpler than previous approaches, and has a cleaner analysis. Moreover, ours is the first best-of-both-worlds algorithm providing bounds logarithmic in the number of constraints. Additionally, in stochastic settings, it provides $\\widetilde O(\\sqrt{T})$ regret without Slater's condition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martino Bernasconi",
      "Matteo Castiglioni",
      "Andrea Celli",
      "Federico Fusco"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/0fe6a18be9491139fb759e2f645374b1-Abstract-Conference.html": {
    "title": "Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity",
    "volume": "main",
    "abstract": "Reinforcement Learning (RL) encompasses diverse paradigms, including model-based RL, policy-based RL, and value-based RL, each tailored to approximate the model, optimal policy, and optimal value function, respectively. This work investigates the potential hierarchy of representation complexity among these RL paradigms. By utilizing computational complexity measures, including time complexity and circuit complexity, we theoretically unveil a potential representation complexity hierarchy within RL. We find that representing the model emerges as the easiest task, followed by the optimal policy, while representing the optimal value function presents the most intricate challenge. Additionally, we reaffirm this hierarchy from the perspective of the expressiveness of Multi-Layer Perceptrons (MLPs), which align more closely with practical deep RL and contribute to a completely new perspective in theoretical studying representation complexity in RL. Finally, we conduct deep RL experiments to validate our theoretical findings",
    "checked": true,
    "id": "0e355076c4f0f561d84b7b33e9cebeca5852fa53",
    "semantic_title": "rethinking model-based, policy-based, and value-based reinforcement learning via the lens of representation complexity",
    "citation_count": 4,
    "authors": [
      "Guhao Feng",
      "Han Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/10272bfd0371ef960ec557ed6c866058-Abstract-Conference.html": {
    "title": "Invisible Image Watermarks Are Provably Removable Using Generative AI",
    "volume": "main",
    "abstract": "Invisible watermarks safeguard images' copyrights by embedding hidden messages only detectable by owners. They also prevent people from misusing images, especially those generated by AI models.We propose a family of regeneration attacks to remove these invisible watermarks. The proposed attack method first adds random noise to an image to destroy the watermark and then reconstructs the image. This approach is flexible and can be instantiated with many existing image-denoising algorithms and pre-trained generative models such as diffusion models. Through formal proofs and extensive empirical evaluations, we demonstrate that pixel-level invisible watermarks are vulnerable to this regeneration attack.Our results reveal that, across four different pixel-level watermarking schemes, the proposed method consistently achieves superior performance compared to existing attack techniques, with lower detection rates and higher image quality.However, watermarks that keep the image semantically similar can be an alternative defense against our attacks.Our finding underscores the need for a shift in research/industry emphasis from invisible watermarks to semantic-preserving watermarks. Code is available at https://github.com/XuandongZhao/WatermarkAttacker",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuandong Zhao",
      "Kexun Zhang",
      "Zihao Su",
      "Saastha Vasan",
      "Ilya Grishchenko",
      "Christopher Kruegel",
      "Giovanni Vigna",
      "Yu-Xiang Wang",
      "Lei Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1055c730c7098c04579beb526c8cd4ba-Abstract-Conference.html": {
    "title": "Stochastic Optimization Schemes for Performative Prediction with Nonconvex Loss",
    "volume": "main",
    "abstract": "This paper studies a risk minimization problem with decision dependent data distribution. The problem pertains to the performative prediction setting in which a trained model can affect the outcome estimated by the model. Such dependency creates a feedback loop that influences the stability of optimization algorithms such as stochastic gradient descent (SGD). We present the first study on performative prediction with smooth but possibly non-convex loss. We analyze a greedy deployment scheme with SGD (SGD-GD). Note that in the literature, SGD-GD is often studied with strongly convex loss. We first propose the definition of stationary performative stable (SPS) solutions through relaxing the popular performative stable condition. We then prove that SGD-GD converges to a biased SPS solution in expectation. We consider two conditions of sensitivity on the distribution shifts: (i) the sensitivity is characterized by Wasserstein-1 distance and the loss is Lipschitz w.r.t.~data samples, or (ii) the sensitivity is characterized by total variation (TV) divergence and the loss is bounded. In both conditions, the bias levels are proportional to the stochastic gradient's variance and sensitivity level. Our analysis is extended to a lazy deployment scheme where models are deployed once per several SGD updates, and we show that it converges to an SPS solution with reduced bias. Numerical experiments corroborate our theories",
    "checked": true,
    "id": "9b9fff9d1ffab7fb5811b61caffe6842a1cd0aeb",
    "semantic_title": "stochastic optimization schemes for performative prediction with nonconvex loss",
    "citation_count": 4,
    "authors": [
      "Qiang LI",
      "Hoi-To Wai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/10826a1a80f816ea98d559d7c7a97973-Abstract-Conference.html": {
    "title": "SDP4Bit: Toward 4-bit Communication Quantization in Sharded Data Parallelism for LLM Training",
    "volume": "main",
    "abstract": "Recent years have witnessed a clear trend towards language models with an ever-increasing number of parameters, as well as the growing training overhead and memory usage. Distributed training, particularly through Sharded Data Parallelism (ShardedDP) which partitions optimizer states among workers, has emerged as a crucial technique to mitigate training time and memory usage. Yet, a major challenge in the scalability of ShardedDP is the intensive communication of weights and gradients. While compression techniques can alleviate this issue, they often result in worse accuracy. Driven by this limitation, we propose SDP4Bit (Toward 4Bit Communication Quantization in Sharded Data Parallelism for LLM Training), which effectively reduces the communication of weights and gradients to nearly 4 bits via two novel techniques: quantization on weight differences, and two-level gradient smooth quantization. Furthermore, SDP4Bit presents an algorithm-system co-design with runtime optimization to minimize the computation overhead of compression. Additional to the theoretical guarantees of convergence, we empirically evaluate the accuracy of SDP4Bit on the pre-training of GPT models with up to 6.7 billion parameters, and the results demonstrate a negligible impact on training loss. Furthermore, speed experiments show that SDP4Bit achieves up to 4.08× speedup in end-to-end throughput on a scale of 128 GPUs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinda Jia",
      "Cong Xie",
      "Hanlin Lu",
      "Daoce Wang",
      "Hao Feng",
      "Chengming Zhang",
      "Baixi Sun",
      "Haibin Lin",
      "Zhi Zhang",
      "Xin Liu",
      "Dingwen Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/10a3b1c30b8cceb507b9e8ddcc9a1a6a-Abstract-Conference.html": {
    "title": "How Does Message Passing Improve Collaborative Filtering?",
    "volume": "main",
    "abstract": "Collaborative filtering (CF) has exhibited prominent results for recommender systems and been broadly utilized for real-world applications.A branch of research enhances CF methods by message passing (MP) used in graph neural networks, due to its strong capabilities of extracting knowledge from graph-structured data, like user-item bipartite graphs that naturally exist in CF. They assume that MP helps CF methods in a manner akin to its benefits for graph-based learning tasks in general (e.g., node classification). However, even though MP empirically improves CF, whether or not this assumption is correct still needs verification. To address this gap, we formally investigate why MP helps CF from multiple perspectives and show that many assumptions made by previous works are not entirely accurate. With our curated ablation studies and theoretical analyses, we discover that (i) MP improves the CF performance primarily by additional representations passed from neighbors during the forward pass instead of additional gradient updates to neighbor representations during the model back-propagation and (ii) MP usually helps low-degree nodes more than high-degree nodes.}Utilizing these novel findings, we present Test-time Aggregation for Collaborative Filtering, namely TAG-CF, a test-time augmentation framework that only conducts MP once at inference time. The key novelty of TAG-CF is that it effectively utilizes graph knowledge while circumventing most of notorious computational overheads of MP. Besides, TAG-CF is extremely versatile can be used as a plug-and-play module to enhance representations trained by different CF supervision signals. Evaluated on six datasets (i.e., five academic benchmarks and one real-world industrial dataset), TAG-CF consistently improves the recommendation performance of CF methods without graph by up to 39.2% on cold users and 31.7% on all users, with little to no extra computational overheads. Furthermore, compared with trending graph-enhanced CF methods, TAG-CF delivers comparable or even better performance with less than 1% of their total training times. Our code is publicly available at https://github.com/snap-research/Test-time-Aggregation-for-CF",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingxuan Ju",
      "William Shiao",
      "Zhichun Guo",
      "Yanfang Ye",
      "Yozen Liu",
      "Neil Shah",
      "Tong Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/10a6bdcabbd5a3d36b760daa295f63c1-Abstract-Conference.html": {
    "title": "An Adaptive Approach for Infinitely Many-armed Bandits under Generalized Rotting Constraints",
    "volume": "main",
    "abstract": "In this study, we consider the infinitely many-armed bandit problems in a rested rotting setting, where the mean reward of an arm may decrease with each pull, while otherwise, it remains unchanged. We explore two scenarios regarding the rotting of rewards: one in which the cumulative amount of rotting is bounded by $V_T$, referred to as the slow-rotting case, and the other in which the cumulative number of rotting instances is bounded by $S_T$, referred to as the abrupt-rotting case. To address the challenge posed by rotting rewards, we introduce an algorithm that utilizes UCB with an adaptive sliding window, designed to manage the bias and variance trade-off arising due to rotting rewards. Our proposed algorithm achieves tight regret bounds for both slow and abrupt rotting scenarios. Lastly, we demonstrate the performance of our algorithm using numerical experiments",
    "checked": true,
    "id": "dc0cb0e63764a76ebf94536c5025d60d5323d8be",
    "semantic_title": "an adaptive approach for infinitely many-armed bandits under generalized rotting constraints",
    "citation_count": 2,
    "authors": [
      "Jung-hun Kim",
      "Milan Vojnovic",
      "Se-Young Yun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/10bf96894abaf4c293b205709a98fc74-Abstract-Conference.html": {
    "title": "Near-Optimal Streaming Heavy-Tailed Statistical Estimation with Clipped SGD",
    "volume": "main",
    "abstract": "$\\newcommand{\\Tr}{\\mathsf{Tr}}$We consider the problem of high-dimensional heavy-tailed statistical estimation in the streaming setting, which is much harder than the traditional batch setting due to memory constraints. We cast this problem as stochastic convex optimization with heavy tailed stochastic gradients, and prove that the widely used Clipped-SGD algorithm attains near-optimal sub-Gaussian statistical rates whenever the second moment of the stochastic gradient noise is finite. More precisely, with $T$ samples, we show that Clipped-SGD, for smooth and strongly convex objectives, achieves an error of $\\sqrt{\\frac{\\Tr(\\Sigma)+\\sqrt{\\Tr(\\Sigma)\\\\|\\Sigma\\\\|_2}\\ln(\\tfrac{\\ln(T)}{\\delta})}{T}}$ with probability $1-\\delta$, where $\\Sigma$ is the covariance of the clipped gradient. Note that the fluctuations (depending on $\\tfrac{1}{\\delta}$) are of lower order than the term $\\Tr(\\Sigma)$.This improves upon the current best rate of$\\sqrt{\\frac{\\Tr(\\Sigma)\\ln(\\tfrac{1}{\\delta})}{T}}$ for Clipped-SGD, known \\emph{only} for smooth and strongly convex objectives. Our results also extend to smooth convex and lipschitz convex objectives. Key to our result is a novel iterative refinement strategy for martingale concentration, improving upon the PAC-Bayes approach of \\citet{catoni2018dimension}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniket Das",
      "Dheeraj Nagaraj",
      "Soumyabrata Pal",
      "Arun Suggala",
      "Prateek Varshney"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/10c456d2160517581a234dfde15a7505-Abstract-Conference.html": {
    "title": "Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities",
    "volume": "main",
    "abstract": "Uncertainty quantification in Large Language Models (LLMs) is crucial for applications where safety and reliability are important. In particular, uncertainty can be used to improve the trustworthiness of LLMs by detecting factually incorrect model responses, commonly called hallucinations. Critically, one should seek to capture the model's semantic uncertainty, i.e., the uncertainty over the meanings of LLM outputs, rather than uncertainty over lexical or syntactic variations that do not affect answer correctness.To address this problem, we propose Kernel Language Entropy (KLE), a novel method for uncertainty estimation in white- and black-box LLMs. KLE defines positive semidefinite unit trace kernels to encode the semantic similarities of LLM outputs and quantifies uncertainty using the von Neumann entropy. It considers pairwise semantic dependencies between answers (or semantic clusters), providing more fine-grained uncertainty estimates than previous methods based on hard clustering of answers. We theoretically prove that KLE generalizes the previous state-of-the-art method called semantic entropy and empirically demonstrate that it improves uncertainty quantification performance across multiple natural language generation datasets and LLM architectures",
    "checked": true,
    "id": "53de9f135d5e2590491952862f4f58cd17342ab2",
    "semantic_title": "kernel language entropy: fine-grained uncertainty quantification for llms from semantic similarities",
    "citation_count": 51,
    "authors": [
      "Alexander Nikitin",
      "Jannik Kossen",
      "Yarin Gal",
      "Pekka Marttinen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/10d52f5d2ef0f69ac10da7c962fb6db9-Abstract-Conference.html": {
    "title": "Fine Tuning Out-of-Vocabulary Item Recommendation with User Sequence Imagination",
    "volume": "main",
    "abstract": "Recommending out-of-vocabulary (OOV) items is a challenging problem since the in-vocabulary (IV) items have well-trained behavioral embeddings but the OOV items only have content features. Current OOV recommendation models often generate 'makeshift' embeddings for OOV items from content features and then jointly recommend with the `makeshift' OOV item embeddings and the behavioral IV item embeddings. However, merely using the 'makeshift' embedding will result in suboptimal recommendation performance due to the substantial gap between the content feature and the behavioral embeddings. To bridge the gap, we propose a novel User Sequence IMagination (USIM) fine-tuning framework, which first imagines the user sequences and then refines the generated OOV embeddings with the user behavioral embeddings. Specifically, we frame the user sequence imagination as a reinforcement learning problem and develop a recommendation-focused reward function to evaluate to what extent a user can help recommend the OOV items. Besides, we propose an embedding-driven transition function to model the embedding transition after imaging a user. USIM has been deployed on a prominent e-commerce platform for months, offering recommendations for millions of OOV items and billions of users. Extensive experiments demonstrate that USIM outperforms traditional generative models in OOV item recommendation performance across traditional collaborative filtering and GNN-based collaborative filtering models",
    "checked": true,
    "id": "f9a72f9041ac2a88944a79a5bf42920b81644ac7",
    "semantic_title": "fine tuning out-of-vocabulary item recommendation with user sequence imagination",
    "citation_count": 5,
    "authors": [
      "Ruochen Liu",
      "Hao Chen",
      "Yuanchen Bei",
      "Qijie Shen",
      "Fangwei Zhong",
      "Senzhang Wang",
      "Jianxin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/10e6dfea9a673bef4a7b1cb9234891bc-Abstract-Conference.html": {
    "title": "Data Mixture Inference Attack: BPE Tokenizers Reveal Training Data Compositions",
    "volume": "main",
    "abstract": "The pretraining data of today's strongest language models remains opaque, even when their parameters are open-sourced.In particular, little is known about the proportions of different domains, languages, or code represented in the data. While a long line of membership inference attacks aim to identify training examples on an instance level, they do not extend easily to global statistics about the corpus. In this work, we tackle a task which we call data mixture inference, which aims to uncover the distributional make-up of the pretraining data. We introduce a novel attack based on a previously overlooked source of information — byte-pair encoding (BPE) tokenizers, used by the vast majority of modern language models. Our key insight is that the ordered vocabulary learned by a BPE tokenizer naturally reveals information about the token frequencies in its training data: the first token is the most common byte pair, the second is the most common pair after merging the first token, and so on. Given a tokenizer's merge list along with data samples for each category of interest (e.g., different natural languages), we formulate a linear program that solves for the relative proportion of each category in the tokenizer's training set. Importantly, to the extent to which tokenizer training data is representative of the pretraining data, we indirectly learn about the pretraining data. In controlled experiments, we show that our attack can recover mixture ratios with high precision for tokenizers trained on known mixtures of natural languages, programming languages, and data sources. We then apply our approach to off-the-shelf tokenizers released alongside recent LMs. We confirm much publicly disclosed information about these models, and also make several new inferences: GPT-4o is much more multilingual than its predecessors, training on 10x more non-English data than GPT-3.5, Llama 3 and Claude are trained on predominantly code, and many recent models are trained on 7-16% books. We hope our work sheds light on current design practices for pretraining data, and inspires continued research into data mixture inference for LMs",
    "checked": true,
    "id": "7c58abcba87e304324c762d6936ae1d50aef69bd",
    "semantic_title": "data mixture inference attack: bpe tokenizers reveal training data compositions",
    "citation_count": 4,
    "authors": [
      "Jonathan Hayase",
      "Alisa Liu",
      "Yejin Choi",
      "Sewoong Oh",
      "Noah A. Smith"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/10e9204f14c4daa08041343455435308-Abstract-Conference.html": {
    "title": "Improving Deep Learning Optimization through Constrained Parameter Regularization",
    "volume": "main",
    "abstract": "Regularization is a critical component in deep learning. The most commonly used approach, weight decay, applies a constant penalty coefficient uniformly across all parameters. This may be overly restrictive for some parameters, while insufficient for others. To address this, we present Constrained Parameter Regularization (CPR) as an alternative to traditional weight decay. Unlike the uniform application of a single penalty, CPR enforces an upper bound on a statistical measure, such as the L$_2$-norm, of individual parameter matrices. Consequently, learning becomes a constraint optimization problem, which we tackle using an adaptation of the augmented Lagrangian method. CPR introduces only a minor runtime overhead and only requires setting an upper bound. We propose simple yet efficient mechanisms for initializing this bound, making CPR rely on no hyperparameter or one, akin to weight decay. Our empirical studies on computer vision and language modeling tasks demonstrate CPR's effectiveness. The results show that CPR can outperform traditional weight decay and increase performance in pre-training and fine-tuning",
    "checked": true,
    "id": "ba76e3fe105d0dd3446eab9094cfce54c7f77a5b",
    "semantic_title": "improving deep learning optimization through constrained parameter regularization",
    "citation_count": 2,
    "authors": [
      "Jörg Franke",
      "Michael Hefenbrock",
      "Gregor Koehler",
      "Frank Hutter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/10f1737aa6347ccc555ac068e1b45523-Abstract-Conference.html": {
    "title": "A generalized neural tangent kernel for surrogate gradient learning",
    "volume": "main",
    "abstract": "State-of-the-art neural network training methods depend on the gradient of the network function. Therefore, they cannot be applied to networks whose activation functions do not have useful derivatives, such as binary and discrete-time spiking neural networks. To overcome this problem, the activation function's derivative is commonly substituted with a surrogate derivative, giving rise to surrogate gradient learning (SGL). This method works well in practice but lacks theoretical foundation.The neural tangent kernel (NTK) has proven successful in the analysis of gradient descent. Here, we provide a generalization of the NTK, which we call the surrogate gradient NTK, that enables the analysis of SGL. First, we study a naive extension of the NTK to activation functions with jumps, demonstrating that gradient descent for such activation functions is also ill-posed in the infinite-width limit. To address this problem, we generalize the NTK to gradient descent with surrogate derivatives, i.e., SGL. We carefully define this generalization and expand the existing key theorems on the NTK with mathematical rigor. Further, we illustrate our findings with numerical experiments. Finally, we numerically compare SGL in networks with sign activation function and finite width to kernel regression with the surrogate gradient NTK; the results confirm that the surrogate gradient NTK provides a good characterization of SGL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luke Eilers",
      "Raoul-Martin Memmesheimer",
      "Sven Goedeke"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/10f34ee79b62627b7ebf6279d35ea480-Abstract-Conference.html": {
    "title": "Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space",
    "volume": "main",
    "abstract": "Current research in adversarial robustness of LLMs focuses on \\textit{discrete} input manipulations in the natural language space, which can be directly transferred to \\textit{closed-source} models. However, this approach neglects the steady progression of \\textit{open-source} models. As open-source models advance in capability, ensuring their safety becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose the \\textit{embedding space attack}, which directly attacks the \\textit{continuous} embedding representation of input tokens.We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Additionally, we demonstrate that models compromised by embedding attacks can be used to create discrete jailbreaks in natural language. Lastly, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models. Our findings highlight embedding space attacks as an important threat model in open-source LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leo Schwinn",
      "David Dobre",
      "Sophie Xhonneux",
      "Gauthier Gidel",
      "Stephan Günnemann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/10fc83943b4540a9524af6fc67a23fef-Abstract-Conference.html": {
    "title": "AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models",
    "volume": "main",
    "abstract": "Recent work on pruning large language models (LLMs) has shown that one can eliminate a large number of parameters without compromising performance, making pruning a promising strategy to reduce LLM model size. Existing LLM pruning strategies typically assign uniform pruning ratios across layers, limiting overall pruning ability; and recent work on layerwise pruning of LLMs is often based on heuristics that can easily lead to suboptimal performance. In this paper, we leverage Heavy-Tailed Self-Regularization (HT-SR) Theory, in particular the shape of empirical spectral densities (ESDs) of weight matrices, to design improved layerwise pruning ratios for LLMs. Our analysis reveals a wide variability in how well-trained, and thus relatedly how prunable, different layers of an LLM are. Based on this, we propose AlphaPruning, which uses shape metrics to allocate layerwise sparsity ratios in a more theoretically-principled manner. AlphaPruning can be used in conjunction with multiple existing LLM pruning methods. Our empirical results show that AlphaPruning prunes LLaMA-7B to 80% sparsity while maintaining reasonable perplexity, marking a first in the literature on LLMs",
    "checked": true,
    "id": "46daae2a932aa02e307fd3317e9f3719c56d6010",
    "semantic_title": "alphapruning: using heavy-tailed self regularization theory for improved layer-wise pruning of large language models",
    "citation_count": 13,
    "authors": [
      "Haiquan Lu",
      "Yefan Zhou",
      "Shiwei Liu",
      "Zhangyang &quot;Atlas&quot; Wang",
      "Michael W. Mahoney",
      "Yaoqing Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1119587863e78451f080da2a768c4935-Abstract-Conference.html": {
    "title": "Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels",
    "volume": "main",
    "abstract": "Large-scale vision-language models like CLIP have demonstrated impressive open-vocabulary capabilities for image-level tasks, excelling in recognizing what objects are present. However, they struggle with pixel-level recognition tasks like semantic segmentation, which require understanding where the objects are located. In this work, we propose a novel method, PixelCLIP, to adapt the CLIP image encoder for pixel-level understanding by guiding the model on where, which is achieved using unlabeled images and masks generated from vision foundation models such as SAM and DINO. To address the challenges of leveraging masks without semantic labels, we devise an online clustering algorithm using learnable class names to acquire general semantic concepts. PixelCLIP shows significant performance improvements over CLIP and competitive results compared to caption-supervised methods in open-vocabulary semantic segmentation",
    "checked": true,
    "id": "1b1f1cf8192b44c66b6c8a948bb98560ecce15ac",
    "semantic_title": "towards open-vocabulary semantic segmentation without semantic labels",
    "citation_count": 4,
    "authors": [
      "Heeseong Shin",
      "Chaehyun Kim",
      "Sunghwan Hong",
      "Seokju Cho",
      "Anurag Arnab",
      "Paul Hongsuck Seo",
      "Seungryong Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/113e6f1d94af5df4f306fbcb3f82339f-Abstract-Conference.html": {
    "title": "Sample Efficient Bayesian Learning of Causal Graphs from Interventions",
    "volume": "main",
    "abstract": "Causal discovery is a fundamental problem with applications spanning various areas in science and engineering. It is well understood that solely using observational data, one can only orient the causal graph up to its Markov equivalence class, necessitating interventional data to learn the complete causal graph. Most works in the literature design causal discovery policies with perfect interventions, i.e., they have access to infinite interventional samples. This study considers a Bayesian approach for learning causal graphs with limited interventional samples, mirroring real-world scenarios where such samples are usually costly to obtain. By leveraging the recent result of Wienöbst et al. [2023] on uniform DAG sampling in polynomial time, we can efficiently enumerate all the cut configurations and their corresponding interventional distributions of a target set, and further track their posteriors. Given any number of interventional samples, our proposed algorithm randomly intervenes on a set of target vertices that cut all the edges in the graph and returns a causal graph according to the posterior of each target set. When the number of interventional samples is large enough, we show theoretically that our proposed algorithm will return the true causal graph with high probability. We compare our algorithm against various baseline methods on simulated datasets, demonstrating its superior accuracy measured by the structural Hamming distance between the learned DAG and the ground truth. Additionally, we present a case study showing how this algorithm could be modified to answer more general causal questions without learning the whole graph. As an example, we illustrate that our method can be used to estimate the causal effect of a variable that cannot be intervened",
    "checked": true,
    "id": "cabaf5c0cbb71d96b19e8ea37772eed5cf48a833",
    "semantic_title": "sample efficient bayesian learning of causal graphs from interventions",
    "citation_count": 0,
    "authors": [
      "Zihan Zhou",
      "Muhammad Qasim Elahi",
      "Murat Kocaoglu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/116a952bc0a8cb03113408c1a215be7c-Abstract-Conference.html": {
    "title": "StreamFlow: Streamlined Multi-Frame Optical Flow Estimation for Video Sequences",
    "volume": "main",
    "abstract": "Prior multi-frame optical flow methods typically estimate flow repeatedly in a pair-wise manner, leading to significant computational redundancy. To mitigate this, we implement a Streamlined In-batch Multi-frame (SIM) pipeline, specifically tailored to video inputs to minimize redundant calculations. It enables the simultaneous prediction of successive unidirectional flows in a single forward pass, boosting processing speed by 44.43% and reaching efficiencies on par with two-frame networks. Moreover, we investigate various spatiotemporal modeling methods for optical flow estimation within this pipeline. Notably, we propose a simple yet highly effective parameter-efficient Integrative spatiotemporal Coherence (ISC) modeling method, alongside a lightweight Global Temporal Regressor (GTR) to harness temporal cues. The proposed ISC and GTR bring powerful spatiotemporal modeling capabilities and significantly enhance accuracy, including in occluded areas, while adding modest computations to the SIM pipeline. Compared to the baseline, our approach, StreamFlow, achieves performance enhancements of 15.45% and 11.37% on the Sintel clean and final test sets respectively, with gains of 15.53% and 10.77% on occluded regions and only a 1.11% rise in latency. Furthermore, StreamFlow exhibits state-of-the-art cross-dataset testing results on Sintel and KITTI, demonstrating its robust cross-domain generalization capabilities. The code is available here",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SHANGKUN SUN",
      "Jiaming Liu",
      "Huaxia Li",
      "Guoqing Liu",
      "Thomas Li",
      "Wei Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/11715d433f6f8b9106baae0df023deb3-Abstract-Conference.html": {
    "title": "Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) with billions of parameters excel at predicting the next token in a sequence. Recent work computes non-vacuous compression-based generalization bounds for LLMs, but these bounds are vacuous for large models at the billion-parameter scale. Moreover, these bounds are obtained through restrictive compression techniques, bounding compressed models that generate low-quality text. Additionally, the tightness of these existing bounds depends on the number of IID documents in a training set rather than the much larger number of non-IID constituent tokens, leaving untapped potential for tighter bounds. In this work, we instead use properties of martingales to derive generalization bounds that benefit from the vast number of tokens in LLM training sets. Since a dataset contains far more tokens than documents, our generalization bounds not only tolerate but actually benefit from far less restrictive compression schemes. With Monarch matrices, Kronecker factorizations, and post-training quantization, we achieve non-vacuous generalization bounds for LLMs as large as LLaMA2-70B. Unlike previous approaches, our work achieves the first non-vacuous bounds for models that are deployed in practice and generate high-quality text",
    "checked": true,
    "id": "8ff35d975be71e63ed145ebeb6e9d75be83c7c04",
    "semantic_title": "unlocking tokens as data points for generalization bounds on larger language models",
    "citation_count": 13,
    "authors": [
      "Sanae Lotfi",
      "Yilun Kuang",
      "Marc Finzi",
      "Brandon Amos",
      "Micah Goldblum",
      "Andrew G Wilson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/119b45b5c2020d6bc9bca1e42826a2b3-Abstract-Conference.html": {
    "title": "A Unified Principle of Pessimism for Offline Reinforcement Learning under Model Mismatch",
    "volume": "main",
    "abstract": "In this paper, we address the challenges of offline reinforcement learning (RL) under model mismatch, where the agent aims to optimize its performance through an offline dataset that may not accurately represent the deployment environment. We identify two primary challenges under the setting: inaccurate model estimation due to limited data and performance degradation caused by the model mismatch between the dataset-collecting environment and the target deployment one. To tackle these issues, we propose a unified principle of pessimism using distributionally robust Markov decision processes. We carefully construct a robust MDP with a single uncertainty set to tackle both data sparsity and model mismatch, and demonstrate that the optimal robust policy enjoys a near-optimal sub-optimality gap under the target environment across three widely used uncertainty models: total variation, $\\chi^2$ divergence, and KL divergence. Our results improve upon or match the state-of-the-art performance under the total variation and KL divergence models, and provide the first result for the $\\chi^2$ divergence model",
    "checked": true,
    "id": "593b54b5063ae634a2262dca02b6f2ed2b96bdf2",
    "semantic_title": "a unified principle of pessimism for offline reinforcement learning under model mismatch",
    "citation_count": 4,
    "authors": [
      "Yue Wang",
      "Zhongchang Sun",
      "Shaofeng Zou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/11c6625b0481a7d5625831369f6b7c82-Abstract-Conference.html": {
    "title": "Overcoming Brittleness in Pareto-Optimal Learning Augmented Algorithms",
    "volume": "main",
    "abstract": "The study of online algorithms with machine-learned predictions has gained considerable prominence in recent years. One of the common objectives in the design and analysis of such algorithms is to attain (Pareto) optimal tradeoffs between the {\\em consistency} of the algorithm, i.e., its performance assuming perfect predictions, and its {\\em robustness}, i.e., the performance of the algorithm under adversarial predictions. In this work, we demonstrate that this optimization criterion can be extremely brittle, in that the performance of Pareto-optimal algorithms may degrade dramatically even in the presence of imperceptive prediction error. To remedy this drawback, we propose a new framework in which the smoothness in the performance of the algorithm is enforced by means of a {\\em user-specified profile}. This allows us to regulate the performance of the algorithm as a function of the prediction error, while simultaneouslymaintaining the analytical notion of consistency/robustness tradeoffs, adapted to the profile setting. We apply this new approach to a well-studied online problem, namely the {\\em one-way trading} problem. For this problem, we further address another limitation of the state-of-the-art Pareto-optimal algorithms, namely the fact that they are tailored to worst-case, and extremely pessimistic inputs. We propose a new Pareto-optimal algorithm that leverages any deviation from the worst-case input to its benefit, and introduce a new metric that allows us to compare any two Pareto-optimal algorithms via a {\\em dominance} relation",
    "checked": false,
    "id": "3a4206859690bf297ab4176a7dfa77b7ec51a3ea",
    "semantic_title": "overcoming brittleness in pareto-optimal learning-augmented algorithms",
    "citation_count": 2,
    "authors": [
      "Alex Elenter",
      "Spyros Angelopoulos",
      "Christoph Dürr",
      "Yanni LEFKI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/11c892a9fcc430cc0f4c7d457e5d60ea-Abstract-Conference.html": {
    "title": "GraphMETRO: Mitigating Complex Graph Distribution Shifts via Mixture of Aligned Experts",
    "volume": "main",
    "abstract": "Graph data are inherently complex and heterogeneous, leading to a high natural diversity of distributional shifts. However, it remains unclear how to build machine learning architectures that generalize to the complex distributional shifts naturally occurring in the real world. Here, we develop GraphMETRO, a Graph Neural Network architecture that models natural diversity and captures complex distributional shifts. GraphMETRO employs a Mixture-of-Experts (MoE) architecture with a gating model and multiple expert models, where each expert model targets a specific distributional shift to produce a referential representation w.r.t. a reference model, and the gating model identifies shift components. Additionally, we design a novel objective that aligns the representations from different expert models to ensure reliable optimization. GraphMETRO achieves state-of-the-art results on four datasets from the GOOD benchmark, which is comprised of complex and natural real-world distribution shifts, improving by 67% and 4.2% on the WebKB and Twitch datasets. Code and data are available at https://github.com/Wuyxin/GraphMETRO",
    "checked": true,
    "id": "5e3ff6a96ea3dd715c80aa7d65bc8a6d5abab791",
    "semantic_title": "graphmetro: mitigating complex graph distribution shifts via mixture of aligned experts",
    "citation_count": 4,
    "authors": [
      "Shirley Wu",
      "Kaidi Cao",
      "Bruno Ribeiro",
      "James Y Zou",
      "Jure Leskovec"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/11f5520daf9132775e8604e89f53925a-Abstract-Conference.html": {
    "title": "CHASE: Learning Convex Hull Adaptive Shift for Skeleton-based Multi-Entity Action Recognition",
    "volume": "main",
    "abstract": "Skeleton-based multi-entity action recognition is a challenging task aiming to identify interactive actions or group activities involving multiple diverse entities. Existing models for individuals often fall short in this task due to the inherent distribution discrepancies among entity skeletons, leading to suboptimal backbone optimization. To this end, we introduce a Convex Hull Adaptive Shift based multi-Entity action recognition method (CHASE), which mitigates inter-entity distribution gaps and unbiases subsequent backbones. Specifically, CHASE comprises a learnable parameterized network and an auxiliary objective. The parameterized network achieves plausible, sample-adaptive repositioning of skeleton sequences through two key components. First, the Implicit Convex Hull Constrained Adaptive Shift ensures that the new origin of the coordinate system is within the skeleton convex hull. Second, the Coefficient Learning Block provides a lightweight parameterization of the mapping from skeleton sequences to their specific coefficients in convex combinations. Moreover, to guide the optimization of this network for discrepancy minimization, we propose the Mini-batch Pair-wise Maximum Mean Discrepancy as the additional objective. CHASE operates as a sample-adaptive normalization method to mitigate inter-entity distribution discrepancies, thereby reducing data bias and improving the subsequent classifier's multi-entity action recognition performance. Extensive experiments on six datasets, including NTU Mutual 11/26, H2O, Assembly101, Collective Activity and Volleyball, consistently verify our approach by seamlessly adapting to single-entity backbones and boosting their performance in multi-entity scenarios. Our code is publicly available at https://github.com/Necolizer/CHASE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Wen",
      "Mengyuan Liu",
      "Songtao Wu",
      "Beichen Ding"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/11faf17bf7e5412d9cded369f97db23d-Abstract-Conference.html": {
    "title": "Continual Counting with Gradual Privacy Expiration",
    "volume": "main",
    "abstract": "Differential privacy with gradual expiration models the setting where data items arrive in a stream and at a given time $t$ the privacy loss guaranteed for a data item seen at time $(t-d)$ is $\\epsilon g(d)$, where $g$ is a monotonically non-decreasing function. We study the fundamental *continual (binary) counting* problem where each data item consists of a bit and the algorithm needs to output at each time step the sum of all the bits streamed so far. For a stream of length $T$ and privacy *without* expiration continual counting is possible with maximum (over all time steps) additive error $O(\\log^2(T)/\\varepsilon)$ and the best known lower bound is $\\Omega(\\log(T)/\\varepsilon)$; closing this gap is a challenging open problem. We show that the situation is very different for privacy with gradual expiration by giving upper and lower bounds for a large set of expiration functions $g$. Specifically, our algorithm achieves an additive error of $O(\\log(T)/\\epsilon)$ for a large set of privacy expiration functions. We also give a lower bound that shows that if $C$ is the additive error of any $\\epsilon$-DP algorithm for this problem, then the product of $C$ and the privacy expiration function after $2C$ steps must be $\\Omega(\\log(T)/\\epsilon)$. Our algorithm matches this lower bound as its additive error is $O(\\log(T)/\\epsilon)$, even when $g(2C) = O(1)$.Our empirical evaluation shows that we achieve a slowly growing privacy loss that has significantly smaller empirical privacy loss for large values of $d$ than a natural baseline algorithm",
    "checked": true,
    "id": "d846e722695296bda7c438fb2a893f4c32f1cc1f",
    "semantic_title": "continual counting with gradual privacy expiration",
    "citation_count": 2,
    "authors": [
      "Joel Daniel Andersson",
      "Monika Henzinger",
      "Rasmus Pagh",
      "Teresa Steiner",
      "Jalaj Upadhyay"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/12143893d9d37c3569dda800b95cabd9-Abstract-Conference.html": {
    "title": "Clustering then Propagation: Select Better Anchors for Knowledge Graph Embedding",
    "volume": "main",
    "abstract": "Traditional knowledge graph embedding (KGE) models map entities and relations to unique embedding vectors in a shallow lookup manner. As the scale of data becomes larger, this manner will raise unaffordable computational costs. Anchor-based strategies have been treated as effective ways to alleviate such efficiency problems by propagation on representative entities instead of the whole graph. However, most existing anchor-based KGE models select the anchors in a primitive manner, which limits their performance. To this end, we propose a novel anchor-based strategy for KGE, i.e., a relational clustering-based anchor selection strategy (RecPiece), where two characteristics are leveraged, i.e., (1) representative ability of the cluster centroids and (2) descriptive ability of relation types in KGs. Specifically, we first perform clustering over features of factual triplets instead of entities, where cluster number is naturally set as number of relation types since each fact can be characterized by its relation in KGs. Then, representative triplets are selected around the clustering centroids, further mapped into corresponding anchor entities. Extensive experiments on six datasets show that RecPiece achieves higher performances but comparable or even fewer parameters compared to previous anchor-based KGE models, indicating that our model can select better anchors in a more scalable way",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "KE LIANG",
      "Yue Liu",
      "Hao Li",
      "Lingyuan Meng",
      "Suyuan Liu",
      "Siwei Wang",
      "sihang zhou",
      "Xinwang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/122ea6470232ee5e79a2649243348005-Abstract-Conference.html": {
    "title": "LoFiT: Localized Fine-tuning on LLM Representations",
    "volume": "main",
    "abstract": "Recent work in interpretability shows that large language models (LLMs) can be adapted for new tasks in a learning-free way: it is possible to intervene on LLM representations to elicit desired behaviors for alignment. For instance, adding certain bias vectors to the outputs of certain attention heads is reported to boost the truthfulness of models. In this work, we show that localized fine-tuning serves as an effective alternative to such representation intervention methods. We introduce a framework called Localized Fine-Tuning on LLM Representations (LoFiT), which identifies a subset of attention heads that are most important for learning a specific task, then trains offset vectors to add to the model's hidden representations at those selected heads. LoFiT localizes to a sparse set of heads (3%-10%) and learns the offset vectors from limited training data, comparable to the settings used for representation intervention. For truthfulness and reasoning tasks, we find that LoFiT's intervention vectors are more effective for LLM adaptation than vectors from representation intervention methods such as Inference-time Intervention. We also find that the localization step is important: selecting a task-specific set of attention heads can lead to higher performance than intervening on heads selected for a different task. Finally, across 7 tasks we study, LoFiT achieves comparable performance to other parameter-efficient fine-tuning methods such as LoRA, despite modifying 20x-200x fewer parameters than these methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangcong Yin",
      "Xi Ye",
      "Greg Durrett"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/123a18dfd821c8b440f42a00a27648d6-Abstract-Conference.html": {
    "title": "Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question Answering",
    "volume": "main",
    "abstract": "Audio-Visual Question Answering (AVQA) is a complex multi-modal reasoning task, demanding intelligent systems to accurately respond to natural language queries based on audio-video input pairs. Nevertheless, prevalent AVQA approaches are prone to overlearning dataset biases, resulting in poor robustness. Furthermore, current datasets may not provide a precise diagnostic for these methods. To tackle these challenges, firstly, we propose a novel dataset, MUSIC-AVQA-R, crafted in two steps: rephrasing questions within the test split of a public dataset (MUSIC-AVQA) and subsequently introducing distribution shifts to split questions. The former leads to a large, diverse test space, while the latter results in a comprehensive robustness evaluation on rare, frequent, and overall questions. Secondly, we propose a robust architecture that utilizes a multifaceted cycle collaborative debiasing strategy to overcome bias learning. Experimental results show that this architecture achieves state-of-the-art performance on MUSIC-AVQA-R, notably obtaining a significant improvement of 9.32\\%. Extensive ablation experiments are conducted on the two datasets mentioned to analyze the component effectiveness within the debiasing strategy. Additionally, we highlight the limited robustness of existing multi-modal QA methods through the evaluation on our dataset. We also conduct experiments combining various baselines with our proposed strategy on two datasets to verify its plug-and-play capability. Our dataset and code are available at https://github.com/reml-group/MUSIC-AVQA-R",
    "checked": true,
    "id": "508de298b9aede434d4233f4e4dc937c1818ff64",
    "semantic_title": "look, listen, and answer: overcoming biases for audio-visual question answering",
    "citation_count": 9,
    "authors": [
      "Jie Ma",
      "Min Hu",
      "Pinghui Wang",
      "Wangchun Sun",
      "Lingyun Song",
      "Hongbin Pei",
      "Jun Liu",
      "Youtian Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/123cfe7d8b7702ac97aaf4468fc05fa5-Abstract-Conference.html": {
    "title": "Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials",
    "volume": "main",
    "abstract": "We present Meta 3D AssetGen (AssetGen), a significant advancement in text-to-3D generation which produces faithful, high-quality meshes with texture and material control. Compared to works that bake shading in the 3D object's appearance, AssetGen outputs physically-based rendering (PBR) materials, supporting realistic relighting. AssetGen generates first several views of the object with separate shaded and albedo appearance channels, and then reconstructs colours, metalness and roughness in 3D, using a deferred shading loss for efficient supervision. It also uses a sign-distance function to represent 3D shape more reliably and introduces acorresponding loss for direct shape supervision. This is implemented using fused kernels for high memory efficiency. After mesh extraction, a texture refinement transformer operating in UV space significantly improves sharpness and details. AssetGen achieves 17% improvement in Chamfer Distance and 40% in LPIPS over the best concurrent work for few-view reconstruction, and a human preference of 72% over the best industry competitors of comparable speed, including those that support PBR. Project page with generated assets: https://assetgen.github.io",
    "checked": true,
    "id": "cca2a4ec5920cd72d772a38534f1b68463438b02",
    "semantic_title": "meta 3d assetgen: text-to-mesh generation with high-quality geometry, texture, and pbr materials",
    "citation_count": 45,
    "authors": [
      "Yawar Siddiqui",
      "Tom Monnier",
      "Filippos Kokkinos",
      "Mahendra Kariya",
      "Yanir Kleiman",
      "Emilien Garreau",
      "Oran Gafni",
      "Natalia Neverova",
      "Andrea Vedaldi",
      "Roman Shapovalov",
      "David Novotny"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/123fd8a56501194823c8e0dca00733df-Abstract-Conference.html": {
    "title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "Adapting Large Language Models (LLMs) to new tasks through fine-tuning has been made more efficient by the introduction of Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA. However, these methods often underperform compared to full fine-tuning, particularly in scenarios involving complex datasets. This issue becomes even more pronounced in complex domains, highlighting the need for improved PEFT approaches that can achieve better performance. Through a series of experiments, we have uncovered two critical insights that shed light on the training and parameter inefficiency of LoRA. Building on these insights, we have developed HydraLoRA, a LoRA framework with an asymmetric structure that eliminates the need for domain expertise. Our experiments demonstrate that HydraLoRA outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases. Our anonymous codes are submitted with the paper and will be publicly available. Code is available: https://github.com/Clin0212/HydraLoRA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunlin Tian",
      "Zhan Shi",
      "Zhijiang Guo",
      "Li Li",
      "Cheng-Zhong Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/124256ed80af5d4bf4c4de17b66c4298-Abstract-Conference.html": {
    "title": "Graph Diffusion Policy Optimization",
    "volume": "main",
    "abstract": "Recent research has made significant progress in optimizing diffusion models for downstream objectives, which is an important pursuit in fields such as graph generation for drug design. However, directly applying these models to graph presents challenges, resulting in suboptimal performance. This paper introduces graph diffusion policy optimization (GDPO), a novel approach to optimize graph diffusion models for arbitrary (e.g., non-differentiable) objectives using reinforcement learning. GDPO is based on an eager policy gradient tailored for graph diffusion models, developed through meticulous analysis and promising improved performance. Experimental results show that GDPO achieves state-of-the-art performance in various graph generation tasks with complex and diverse objectives. Code is available at https://github.com/sail-sg/GDPO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijing Liu",
      "Chao Du",
      "Tianyu Pang",
      "Chongxuan LI",
      "Min Lin",
      "Wei Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/128911cc894d57bcae78074a9551c132-Abstract-Conference.html": {
    "title": "Understanding Representation of Deep Equilibrium Models from Neural Collapse Perspective",
    "volume": "main",
    "abstract": "Deep Equilibrium Model (DEQ), which serves as a typical implicit neural network, emphasizes their memory efficiency and competitive performance compared to explicit neural networks. However, there has been relatively limited theoretical analysis on the representation of DEQ. In this paper, we utilize the Neural Collapse ($\\mathcal{NC}$) as a tool to systematically analyze the representation of DEQ under both balanced and imbalanced conditions. $\\mathcal{NC}$ is an interesting phenomenon in the neural network training process that characterizes the geometry of class features and classifier weights. While extensively studied in traditional explicit neural networks, the $\\mathcal{NC}$ phenomenon has not received substantial attention in the context of implicit neural networks. We theoretically show that $\\mathcal{NC}$ exists in DEQ under balanced conditions. Moreover, in imbalanced settings, despite the presence of minority collapse, DEQ demonstrated advantages over explicit neural networks. These advantages include the convergence of extracted features to the vertices of a simplex equiangular tight frame and self-duality properties under mild conditions, highlighting DEQ's superiority in handling imbalanced datasets. Finally, we validate our theoretical analyses through experiments in both balanced and imbalanced scenarios",
    "checked": true,
    "id": "d91a15e7beaaca38e55b10532af98409930aa32b",
    "semantic_title": "understanding representation of deep equilibrium models from neural collapse perspective",
    "citation_count": 0,
    "authors": [
      "Haixiang Sun",
      "Ye Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/12c118ef87fde56a10bd858842781b34-Abstract-Conference.html": {
    "title": "A Recipe for Charge Density Prediction",
    "volume": "main",
    "abstract": "In density functional theory, charge density is the core attribute of atomic systems from which all chemical properties can be derived. Machine learning methods are promising in significantly accelerating charge density prediction, yet existing approaches either lack accuracy or scalability. We propose a recipe that can achieve both. In particular, we identify three key ingredients: (1) representing the charge density with atomic and virtual orbitals (spherical fields centered at atom/virtual coordinates); (2) using expressive and learnable orbital basis sets (basis function for the spherical fields); and (3) using high-capacity equivariant neural network architecture. Our method achieves state-of-the-art accuracy while being more than an order of magnitude faster than existing methods. Furthermore, our method enables flexible efficiency-accuracy trade-offs by adjusting the model/basis sizes",
    "checked": true,
    "id": "fa51265d3a846110bdb4955f4de1be9b1feb487f",
    "semantic_title": "a recipe for charge density prediction",
    "citation_count": 7,
    "authors": [
      "Xiang Fu",
      "Andrew Rosen",
      "Kyle Bystrom",
      "Rui Wang",
      "Albert Musaelian",
      "Boris Kozinsky",
      "Tess Smidt",
      "Tommi Jaakkola"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/12d286282e1be5431ea05262a21f415c-Abstract-Conference.html": {
    "title": "Deep Graph Mating",
    "volume": "main",
    "abstract": "In this paper, we introduce the first learning-free model reuse task within the non-Euclidean domain, termed as Deep Graph Mating (Grama). We strive to create a child Graph Neural Network (GNN) that integrates knowledge from pre-trained parent models without requiring re-training, fine-tuning, or annotated labels. To this end, we begin by investigating the permutation invariance property of GNNs, which leads us to develop two vanilla approaches for Grama: Vanilla Parameter Interpolation (VPI) and Vanilla Alignment Prior to Interpolation (VAPI), both employing topology-independent interpolation in the parameter space. However, neither approach has achieved the anticipated results. Through theoretical analysis of VPI and VAPI, we identify critical challenges unique to Grama, including increased sensitivity to parameter misalignment and further the inherent topology-dependent complexities. Motivated by these findings, we propose the Dual-Message Coordination and Calibration (DuMCC) methodology, comprising the Parent Message Coordination (PMC) scheme to optimise the permutation matrices for parameter interpolation by coordinating aggregated messages, and the Child Message Calibration (CMC) scheme to mitigate over-smoothing identified in PMC by calibrating the message statistics within child GNNs. Experiments across diverse domains, including node and graph property prediction, 3D object recognition, and large-scale semantic parsing, demonstrate that the proposed DuMCC effectively enables training-free knowledge transfer, yielding results on par with those of pre-trained models",
    "checked": true,
    "id": "e636338a509267bfbc48a887c1bb5f523ee95d23",
    "semantic_title": "deep graph mating",
    "citation_count": 1,
    "authors": [
      "Yongcheng Jing",
      "Seok-Hee Hong",
      "Dacheng Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/12d3e63be5574088f7c1bbc9162060bf-Abstract-Conference.html": {
    "title": "LIVE: Learnable In-Context Vector for Visual Question Answering",
    "volume": "main",
    "abstract": "As language models continue to scale, Large Language Models (LLMs) have exhibited emerging capabilities in In-Context Learning (ICL), enabling them to solve language tasks by prefixing a few in-context demonstrations (ICDs) as context. Inspired by these advancements, researchers have extended these techniques to develop Large Multimodal Models (LMMs) with ICL capabilities. However, applying ICL usually faces two major challenges: 1) using more ICDs will largely increase the inference time and 2) the performance is sensitive to the selection of ICDs. These challenges are further exacerbated in LMMs due to the integration of multiple data types and the combinational complexity of multimodal ICDs. Recently, to address these challenges, some NLP studies introduce non-learnable In-Context Vectors (ICVs) which extract useful task information from ICDs into a single vector and then insert it into the LLM to help solve the corresponding task. However, although useful in simple NLP tasks, these non-learnable methods fail to handle complex multimodal tasks like Visual Question Answering (VQA). In this study, we propose \\underline{\\textbf{L}}earnable \\underline{\\textbf{I}}n-Context \\underline{\\textbf{Ve}}ctor (LIVE) to distill essential task information from demonstrations, improving ICL performance in LMMs. Experiments show that LIVE can significantly reduce computational costs while enhancing accuracy in VQA tasks compared to traditional ICL and other non-learnable ICV methods",
    "checked": false,
    "id": "ba01ab9afa7ef6fe488f19b428d099891c8ccce1",
    "semantic_title": "learnable in-context vector for visual question answering",
    "citation_count": 4,
    "authors": [
      "Yingzhe Peng",
      "chenduo hao",
      "Xinting Hu",
      "Jiawei Peng",
      "Xin Geng",
      "Xu Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/12d7ba753894ed348904df1bf0ce02ec-Abstract-Conference.html": {
    "title": "Online Relational Inference for Evolving Multi-agent Interacting Systems",
    "volume": "main",
    "abstract": "We introduce a novel framework, Online Relational Inference (ORI), designed to efficiently identify hidden interaction graphs in evolving multi-agent interacting systems using streaming data. Unlike traditional offline methods that rely on a fixed training set, ORI employs online backpropagation, updating the model with each new data point, thereby allowing it to adapt to changing environments in real-time. A key innovation is the use of an adjacency matrix as a trainable parameter, optimized through a new adaptive learning rate technique called AdaRelation, which adjusts based on the historical sensitivity of the decoder to changes in the interaction graph. Additionally, a data augmentation method named Trajectory Mirror (TM) is introduced to improve generalization by exposing the model to varied trajectory patterns. Experimental results on both synthetic datasets and real-world data (CMU MoCap for human motion) demonstrate that ORI significantly improves the accuracy and adaptability of relational inference in dynamic settings compared to existing methods. This approach is model-agnostic, enabling seamless integration with various neural relational inference (NRI) architectures, and offers a robust solution for real-time applications in complex, evolving systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beomseok Kang",
      "Priyabrata Saha",
      "Sudarshan Sharma",
      "Biswadeep Chakraborty",
      "Saibal Mukhopadhyay"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/12ffe4499085e9a51beb02441212e26b-Abstract-Conference.html": {
    "title": "Open-Book Neural Algorithmic Reasoning",
    "volume": "main",
    "abstract": "Neural algorithmic reasoning is an emerging area of machine learning that focuses on building neural networks capable of solving complex algorithmic tasks. Recent advancements predominantly follow the standard supervised learning paradigm -- feeding an individual problem instance into the network each time and training it to approximate the execution steps of a classical algorithm. We challenge this mode and propose a novel open-book learning framework. In this framework, whether during training or testing, the network can access and utilize all instances in the training dataset when reasoning for a given instance.Empirical evaluation is conducted on the challenging CLRS Algorithmic Reasoning Benchmark, which consists of 30 diverse algorithmic tasks. Our open-book learning framework exhibits a significant enhancement in neural reasoning capabilities. Further, we notice that there is recent literature suggesting that multi-task training on CLRS can improve the reasoning accuracy of certain tasks, implying intrinsic connections between different algorithmic tasks. We delve into this direction via the open-book framework. When the network reasons for a specific task, we enable it to aggregate information from training instances of other tasks in an attention-based manner. We show that this open-book attention mechanism offers insights into the inherent relationships among various tasks in the benchmark and provides a robust tool for interpretable multi-task training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hefei Li",
      "Peng Chao",
      "Chenyang Xu",
      "Zhengfeng Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/13183a224208671a6fc33ba1aa661ec4-Abstract-Conference.html": {
    "title": "Classification Diffusion Models: Revitalizing Density Ratio Estimation",
    "volume": "main",
    "abstract": "A prominent family of methods for learning data distributions relies on density ratio estimation (DRE), where a model is trained to classify between data samples and samples from some reference distribution. DRE-based models can directly output the likelihood for any given input, a highly desired property that is lacking in most generative techniques. Nevertheless, to date, DRE methods have failed in accurately capturing the distributions of complex high-dimensional data, like images, and have thus been drawing reduced research attention in recent years. In this work we present classification diffusion models (CDMs), a DRE-based generative method that adopts the formalism of denoising diffusion models (DDMs) while making use of a classifier that predicts the level of noise added to a clean signal. Our method is based on an analytical connection that we derive between the MSE-optimal denoiser for removing white Gaussian noise and the cross-entropy-optimal classifier for predicting the noise level. Our method is the first DRE-based technique that can successfully generate images beyond the MNIST dataset. Furthermore, it can output the likelihood of any input in a single forward pass, achieving state-of-the-art negative log likelihood (NLL) among methods with this property",
    "checked": false,
    "id": "69ac152e50bfe13380e99c49413e108b1edbd4ca",
    "semantic_title": "classification diffusion models",
    "citation_count": 2,
    "authors": [
      "Shahar Yadin",
      "Noam Elata",
      "Tomer Michaeli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/13250eb13871b3c2c0a0667b54bad165-Abstract-Conference.html": {
    "title": "Text-Infused Attention and Foreground-Aware Modeling for Zero-Shot Temporal Action Detection",
    "volume": "main",
    "abstract": "Zero-Shot Temporal Action Detection (ZSTAD) aims to classify and localize action segments in untrimmed videos for unseen action categories. Most existing ZSTAD methods utilize a foreground-based approach, limiting the integration of text and visual features due to their reliance on pre-extracted proposals. In this paper, we introduce a cross-modal ZSTAD baseline with mutual cross-attention, integrating both text and visual information throughout the detection process. Our simple approach results in superior performance compared to previous methods. Despite this improvement, we further identify a common-action bias issue that the cross-modal baseline over-focus on common sub-actions due to a lack of ability to discriminate text-related visual parts. To address this issue, we propose Text-infused attention and Foreground-aware Action Detection (Ti-FAD), which enhances the ability to focus on text-related sub-actions and distinguish relevant action segments from the background. Our extensive experiments demonstrate that Ti-FAD outperforms the state-of-the-art methods on ZSTAD benchmarks by a large margin: 41.2\\% (+ 11.0\\%) on THUMOS14 and 32.0\\% (+ 5.4\\%) on ActivityNet v1.3. Code is available at: https://github.com/YearangLee/Ti-FAD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yearang Lee",
      "Ho-Joong Kim",
      "Seong-Whan Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1326b385e39647c733e4e81edc1d6670-Abstract-Conference.html": {
    "title": "Online Budgeted Matching with General Bids",
    "volume": "main",
    "abstract": "Online Budgeted Matching (OBM) is a classic problem with important applications in online advertising, online service matching, revenue management, and beyond. Traditional online algorithms typically assume a small bid setting, where the maximum bid-to-budget ratio ($\\kappa$) is infinitesimally small. While recent algorithms have tried to address scenarios with non-small or general bids, they often rely on the Fractional Last Matching (FLM) assumption, which allows for accepting partial bids when the remaining budget is insufficient. This assumption, however, does not hold for many applications with indivisible bids. In this paper, we remove the FLM assumption and tackle the open problem of OBM with general bids. We first establish an upper bound of $1-\\kappa$ on the competitive ratio for any deterministic online algorithm. We then propose a novel meta algorithm, called MetaAd, which reduces to different algorithms with first known provable competitive ratios parameterized by the maximum bid-to-budget ratio $\\kappa\\in [0,1]$. As a by-product, we extend MetaAd to the FLM setting and get provable competitive algorithms. Finally, we apply our competitive analysis to the design learning- augmented algorithms",
    "checked": true,
    "id": "d0e9dad745df09a71c6112df89cf12856e9f3cee",
    "semantic_title": "online budgeted matching with general bids",
    "citation_count": 1,
    "authors": [
      "Jianyi Yang",
      "Pengfei Li",
      "Adam Wierman",
      "Shaolei Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1343e23bc2d34c054040e73ad86582cf-Abstract-Conference.html": {
    "title": "Marginal Causal Flows for Validation and Inference",
    "volume": "main",
    "abstract": "Investigating the marginal causal effect of an intervention on an outcome from complex data remains challenging due to the inflexibility of employed models and the lack of complexity in causal benchmark datasets, which often fail to reproduce intricate real-world data patterns. In this paper we introduce Frugal Flows, a likelihood-based machine learning model that uses normalising flows to flexibly learn the data-generating process, while also directly targeting the marginal causal quantities inferred from observational data. We provide a novel algorithm for fitting a model to observational data with a parametrically specified causal distribution, and propose that these models are exceptionally well suited for synthetic data generation to validate causal methods. Unlike existing data generation methods, Frugal Flows generate synthetic data that closely resembles the empirical dataset, while also automatically and exactly satisfying a user-defined average treatment effect. To our knowledge, Frugal Flows are the first generative model to both learn flexible data representations and also \\textit{exactly} parameterise quantities such as the average treatment effect and the degree of unobserved confounding. We demonstrate the above with experiments on both simulated and real-world datasets",
    "checked": true,
    "id": "ef3f39812c4fc0fb4979c007f5f2e6b6eac08773",
    "semantic_title": "marginal causal flows for validation and inference",
    "citation_count": 4,
    "authors": [
      "Daniel de Vassimon Manela",
      "Laura Battaglia",
      "Robin Evans"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/134ed7a477770f227f12450ef0cbb8f4-Abstract-Conference.html": {
    "title": "CRAYM: Neural Field Optimization via Camera RAY Matching",
    "volume": "main",
    "abstract": "We introduce camera ray matching (CRAYM) into the joint optimization of camera poses and neural fields from multi-view images. The optimized field, referred to as a feature volume, can be \"probed\" by the camera rays for novel view synthesis (NVS) and 3D geometry reconstruction. One key reason for matching camera rays, instead of pixels as in prior works, is that the camera rays can be parameterized by the feature volume to carry both geometric and photometric information. Multi-view consistencies involving the camera rays and scene rendering can be naturally integrated into the joint optimization and network training, to impose physically meaningful constraints to improve the final quality of both the geometric reconstruction and photorealistic rendering. We formulate our per-ray optimization and matched ray coherence by focusing on camera rays passing through keypoints in the input images to elevate both the efficiency and accuracy of scene correspondences. Accumulated ray features along the feature volume provide a means to discount the coherence constraint amid erroneous ray matching. We demonstrate the effectiveness of CRAYM for both NVS and geometry reconstruction, over dense- or sparse-view settings, with qualitative and quantitative comparisons to state-of-the-art alternatives",
    "checked": true,
    "id": "694b9d26ff4b01861811c19b0c59684f0dd5366d",
    "semantic_title": "craym: neural field optimization via camera ray matching",
    "citation_count": 1,
    "authors": [
      "Liqiang Lin",
      "Wenpeng Wu",
      "Chi-Wing Fu",
      "Hao Zhang",
      "Hui Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/136b9a13861308c8948cd308ccd02658-Abstract-Conference.html": {
    "title": "The Road Less Scheduled",
    "volume": "main",
    "abstract": "Existing learning rate schedules that do not require specification of the optimization stopping step $T$ are greatly out-performed by learning rate schedules that depend on $T$. We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. An open source implementation of our method is available at https://github.com/facebookresearch/schedule_free. Schedule-Free AdamW is the core algorithm behind our winning entry to the MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge Self-Tuning track",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaron Defazio",
      "Xingyu Yang",
      "Ahmed Khaled",
      "Konstantin Mishchenko",
      "Harsh Mehta",
      "Ashok Cutkosky"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/13707aad517ddd6c09ea02e0f55e1e7a-Abstract-Conference.html": {
    "title": "Resource-Aware Federated Self-Supervised Learning with Global Class Representations",
    "volume": "main",
    "abstract": "Due to the heterogeneous architectures and class skew, the global representation models training in resource-adaptive federated self-supervised learning face with tricky challenges: $\\textit{deviated representation abilities}$ and $\\textit{inconsistent representation spaces}$. In this work, we are the first to propose a multi-teacher knowledge distillation framework, namely $\\textit{FedMKD}$, to learn global representations with whole class knowledge from heterogeneous clients even under extreme class skew. Firstly, the adaptive knowledge integration mechanism is designed to learn better representations from all heterogeneous models with deviated representation abilities. Then the weighted combination of the self-supervised loss and the distillation loss can support the global model to encode all classes from clients into a unified space. Besides, the global knowledge anchored alignment module can make the local representation spaces close to the global spaces, which further improves the representation abilities of local ones. Finally, extensive experiments conducted on two datasets demonstrate the effectiveness of $\\textit{FedMKD}$ which outperforms state-of-the-art baselines 4.78\\% under linear evaluation on average",
    "checked": true,
    "id": "4bfeac6a48dbe862bb8e70795099a85cbcfd97c4",
    "semantic_title": "resource-aware federated self-supervised learning with global class representations",
    "citation_count": 1,
    "authors": [
      "Mingyi Li",
      "Xiao Zhang",
      "Qi Wang",
      "Tengfei LIU",
      "Ruofan Wu",
      "Weiqiang Wang",
      "Fuzhen Zhuang",
      "Hui Xiong",
      "Dongxiao Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/137101016144540ed3191dc2b02f09a5-Abstract-Conference.html": {
    "title": "Spiking Transformer with Experts Mixture",
    "volume": "main",
    "abstract": "Spiking Neural Networks (SNNs) provide a sparse spike-driven mechanism which is believed to be critical for energy-efficient deep learning. Mixture-of-Experts (MoE), on the other side, aligns with the brain mechanism of distributed and sparse processing, resulting in an efficient way of enhancing model capacity and conditional computation. In this work, we consider how to incorporate SNNs' spike-driven and MoE's conditional computation into a unified framework. However, MoE uses softmax to get the dense conditional weights for each expert and TopK to hard-sparsify the network, which does not fit the properties of SNNs. To address this issue, we reformulate MoE in SNNs and introduce the Spiking Experts Mixture Mechanism (SEMM) from the perspective of sparse spiking activation. Both the experts and the router output spiking sequences, and their element-wise operation makes SEMM computation spike-driven and dynamic sparse-conditional. By developing SEMM into Spiking Transformer, the Experts Mixture Spiking Attention (EMSA) and the Experts Mixture Spiking Perceptron (EMSP) are proposed, which performs routing allocation for head-wise and channel-wise spiking experts, respectively. Experiments show that SEMM realizes sparse conditional computation and obtains a stable improvement on neuromorphic and static datasets with approximate computational overhead based on the Spiking Transformer baselines",
    "checked": true,
    "id": "28524b8da3393b02c0a368f0293e76499a5cacb6",
    "semantic_title": "spiking transformer with experts mixture",
    "citation_count": 3,
    "authors": [
      "Zhaokun Zhou",
      "Yijie Lu",
      "Yanhao Jia",
      "Kaiwei Che",
      "Jun Niu",
      "Liwei Huang",
      "Xinyu Shi",
      "Yuesheng Zhu",
      "Guoqi Li",
      "Zhaofei Yu",
      "Li Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/137cc5c0b8a3c932805e3c14812070ca-Abstract-Conference.html": {
    "title": "Semantic Routing via Autoregressive Modeling",
    "volume": "main",
    "abstract": "We study learning-based approaches to semantic route planning, which concerns producing routes in response to rich queries that specify various criteria and preferences. Semantic routing is already widely found in industry applications, especially navigational services like Google Maps; however, existing implementations only support limited route criteria and narrow query sets as they rely on repurposing classical route optimization algorithms. We argue for a learning-based approach to semantic routing as a more scalable and general alternative. To foster interest in this important application of graph learning, we are releasing a large-scale publicly-licensed benchmark for semantic routing consisting of real-world multi-objective navigation problems---expressed via natural language queries---on the richly annotated road networks of US cities. In addition to being intractable with existing approaches to semantic routing, our benchmark poses a significant scaling challenge for graph learning methods. As a proof-of-concept, we show that---at scale---even a standard transformer network is a powerful semantic routing system and achieves non-trivial performance on our benchmark. In the process, we demonstrate a simple solution to the challenge of scaling up graph learning: an autoregressive approach that decomposes semantic routing into smaller ``next-edge'' prediction problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Zhao",
      "Pranjal Awasthi",
      "Zhengdao Chen",
      "Sreenivas Gollapudi",
      "Daniel Delling"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/13848b5893119ff772b69812c95914fa-Abstract-Conference.html": {
    "title": "TSDS: Data Selection for Task-Specific Model Finetuning",
    "volume": "main",
    "abstract": "Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (Task-Specific Data Selection), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data.We connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques.We evaluate our method on data selection for both continued pretraining and instruction tuning of language models.We show that instruction tuning using data selected by our method with a 1\\% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zifan Liu",
      "Amin Karbasi",
      "Theodoros Rekatsinas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/13cd22c32c1330decd69c13cf8cadc0a-Abstract-Conference.html": {
    "title": "Fully Unconstrained Online Learning",
    "volume": "main",
    "abstract": "We provide a technique for OLO that obtains regret $G\\|w_\\star\\|\\sqrt{T\\log(\\|w_\\star\\|G\\sqrt{T})} + \\|w_\\star\\|^2 + G^2$ on $G$-Lipschitz losses for any comparison point $w_\\star$ without knowing either $G$ or $\\|w_\\star\\|$. Importantly, this matches the optimal bound $G\\|w_\\star\\|\\sqrt{T}$ available with such knowledge (up to logarithmic factors), unless either $\\|w_\\star\\|$ or $G$ is so large that even $G\\|w_\\star\\|\\sqrt{T}$ is roughly linear in $T$. Thus, at a high level it matches the optimal bound in all cases in which one can achieve sublinear regret",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashok Cutkosky",
      "Zak Mhammedi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/13d7f172259b11b230cc5da8768abc5f-Abstract-Conference.html": {
    "title": "Masked Hard-Attention Transformers Recognize Exactly the Star-Free Languages",
    "volume": "main",
    "abstract": "The expressive power of transformers over inputs of unbounded size can be studied through their ability to recognize classes of formal languages. In this paper, we establish exact characterizations of transformers with hard attention (in which all attention is focused on exactly one position) and attention masking (in which each position only attends to positions on one side). With strict masking (each position cannot attend to itself) and without position embeddings, these transformers are expressively equivalent to linear temporal logic (LTL), which defines exactly the star-free languages. A key technique is the use of Boolean RASP as a convenient intermediate language between transformers and LTL. We then take numerous results known for LTL and apply them to transformers, showing how position embeddings, strict masking, and depth all increase expressive power",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andy Yang",
      "David Chiang",
      "Dana Angluin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/13e8be77982beb73d7ed0bbf122f9f3c-Abstract-Conference.html": {
    "title": "CosAE: Learnable Fourier Series for Image Restoration",
    "volume": "main",
    "abstract": "In this paper, we introduce Cosine Autoencoder (CosAE), a novel, generic Autoencoder that seamlessly leverages the classic Fourier series with a feed-forward neural network. CosAE represents an input image as a series of 2D Cosine time series, each defined by a tuple of learnable frequency and Fourier coefficients. This method stands in contrast to a conventional Autoencoder that often sacrifices detail in their reduced-resolution bottleneck latent spaces. CosAE, however, encodes frequency coefficients, i.e., the amplitudes and phases, in its bottleneck. This encoding enables extreme spatial compression, e.g., $64\\times$ downsampled feature maps in the bottleneck, without losing detail upon decoding. We showcase the advantage of CosAE via extensive experiments on flexible-resolution super-resolution and blind image restoration, two highly challenging tasks that demand the restoration network to effectively generalize to complex and even unknown image degradations. Our method surpasses state-of-the-art approaches, highlighting its capability to learn a generalizable representation for image restoration. The project page is maintained at [https://sifeiliu.net/CosAE-page/](https://sifeiliu.net/CosAE-page/)",
    "checked": true,
    "id": "88334864f85bc1aa9cd042ed4a0e5099b8b50c76",
    "semantic_title": "cosae: learnable fourier series for image restoration",
    "citation_count": 3,
    "authors": [
      "Sifei Liu",
      "Shalini De Mello",
      "Jan Kautz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/13f972adf12bdf886583d48cd528002f-Abstract-Conference.html": {
    "title": "Exactly Minimax-Optimal Locally Differentially Private Sampling",
    "volume": "main",
    "abstract": "The sampling problem under local differential privacy has recently been studied with potential applications to generative models, but a fundamental analysis of its privacy-utility trade-off (PUT) remains incomplete. In this work, we define the fundamental PUT of private sampling in the minimax sense, using the $f$-divergence between original and sampling distributions as the utility measure. We characterize the exact PUT for both finite and continuous data spaces under some mild conditions on the data distributions, and propose sampling mechanisms that are universally optimal for all $f$-divergences. Our numerical experiments demonstrate the superiority of our mechanisms over baselines, in terms of theoretical utilities for finite data space and of empirical utilities for continuous data space",
    "checked": true,
    "id": "f38ae395dd0ee367bd8661ae0dbcdcf26142a31a",
    "semantic_title": "exactly minimax-optimal locally differentially private sampling",
    "citation_count": 2,
    "authors": [
      "Hyun-Young Park",
      "Shahab Asoodeh",
      "Si-Hyeon Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1403ab1a427050538ec59c7f570aec8b-Abstract-Conference.html": {
    "title": "Exploring Context Window of Large Language Models via Decomposed Positional Vectors",
    "volume": "main",
    "abstract": "Transformer-based large language models (LLMs) typically have a limited context window, resulting in significant performance degradation when processing text beyond the length of the context window. Extensive studies have been proposed to extend the context window and achieve length extrapolation of LLMs, but there is still a lack of in-depth interpretation of these approaches. In this study, we explore the positional information within and beyond the context window for deciphering the underlying mechanism of LLMs. By using a mean-based decomposition method, we disentangle positional vectors from hidden states of LLMs and analyze their formation and effect on attention. Furthermore, when texts exceed the context window, we analyze the change of positional vectors in two settings, i.e., direct extrapolation and context window extension. Based on our findings, we design two training-free context window extension methods, positional vector replacement and attention window extension. Experimental results show that our methods can effectively extend the context window length",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zican Dong",
      "Junyi Li",
      "Xin Men",
      "Xin Zhao",
      "Bingning Wang",
      "Zhen Tian",
      "weipeng chen",
      "Ji-Rong Wen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/140aac600566125915df7e74ff538f66-Abstract-Conference.html": {
    "title": "Spectral Graph Pruning Against Over-Squashing and Over-Smoothing",
    "volume": "main",
    "abstract": "Message Passing Graph Neural Networks are known to suffer from two problems that are sometimes believed to be diametrically opposed: over-squashing and over-smoothing. The former results from topological bottlenecks that hamper the information flow from distant nodes and are mitigated by spectral gap maximization, primarily, by means of edge additions. However, such additions often promote over-smoothing that renders nodes of different classes less distinguishable. Inspired by the Braess phenomenon, we argue that deleting edges can address over-squashing and over-smoothing simultaneously. This insight explains how edge deletions can improve generalization, thus connecting spectral gap optimization to a seemingly disconnected objective of reducing computational resources by pruning graphs for lottery tickets. To this end, we propose a computationally effective spectral gap optimization framework to add or delete edges and demonstrate its effectiveness on the long range graph benchmark and on larger heterophilous datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adarsh Jamadandi",
      "Celia Rubio-Madrigal",
      "Rebekka Burkholz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/140edeced4facf41b14ca4f71a2322b7-Abstract-Conference.html": {
    "title": "Fairness in Social Influence Maximization via Optimal Transport",
    "volume": "main",
    "abstract": "We study fairness in social influence maximization, whereby one seeks to selectseeds that spread a given information throughout a network, ensuring balancedoutreach among different communities (e.g. demographic groups). In the literature,fairness is often quantified in terms of the expected outreach within individualcommunities. In this paper, we demonstrate that such fairness metrics can bemisleading since they overlook the stochastic nature of information diffusionprocesses. When information diffusion occurs in a probabilistic manner, multipleoutreach scenarios can occur. As such, outcomes such as \"In 50% of the cases, noone in group 1 gets the information, while everyone in group 2 does, and in theother 50%, it is the opposite\", which always results in largely unfair outcomes,are classified as fair by a variety of fairness metrics in the literature. We tacklethis problem by designing a new fairness metric, mutual fairness, that capturesvariability in outreach through optimal transport theory. We propose a new seed-selection algorithm that optimizes both outreach and mutual fairness, and we showits efficacy on several real datasets. We find that our algorithm increases fairnesswith only a minor decrease (and at times, even an increase) in efficiency",
    "checked": true,
    "id": "188acad44928bc4b0ebed60f996f9c3da717c639",
    "semantic_title": "fairness in social influence maximization via optimal transport",
    "citation_count": 0,
    "authors": [
      "Shubham Chowdhary",
      "Giulia De Pasquale",
      "Nicolas Lanzetti",
      "Ana-Andreea Stoica",
      "Florian Dorfler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1419d8554191a65ea4f2d8e1057973e4-Abstract-Conference.html": {
    "title": "ECLipsE: Efficient Compositional Lipschitz Constant Estimation for Deep Neural Networks",
    "volume": "main",
    "abstract": "The Lipschitz constant plays a crucial role in certifying the robustness of neural networks to input perturbations. Since calculating the exact Lipschitz constant is NP-hard, efforts have been made to obtain tight upper bounds on the Lipschitz constant. Typically, this involves solving a large matrix verification problem, the computational cost of which grows significantly for both deeper and wider networks. In this paper, we provide a compositional approach to estimate Lipschitz constants for deep feed-forward neural networks. We first obtain an exact decomposition of the large matrix verification problem into smaller sub-problems. Then, leveraging the underlying cascade structure of the network, we develop two algorithms. The first algorithm explores the geometric features of the problem and enables us to provide Lipschitz estimates that are comparable to existing methods by solving small semidefinite programs (SDPs) that are only as large as the size of each layer. The second algorithm relaxes these sub-problems and provides a closed-form solution to each sub-problem for extremely fast estimation, altogether eliminating the need to solve SDPs. The two algorithms represent different levels of trade-offs between efficiency and accuracy. Finally, we demonstrate that our approach provides a steep reduction in computation time (as much as several thousand times faster, depending on the algorithm for deeper networks) while yielding Lipschitz bounds that are very close to or even better than those achieved by state-of-the-art approaches in a broad range of experiments. In summary, our approach considerably advances the scalability and efficiency of certifying neural network robustness, making it particularly attractive for online learning tasks",
    "checked": true,
    "id": "38105e26e96cab5e9887de2e463c06cf82046eed",
    "semantic_title": "eclipse: efficient compositional lipschitz constant estimation for deep neural networks",
    "citation_count": 1,
    "authors": [
      "Yuezhu Xu",
      "S Sivaranjani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/143ea4a156ef64f32d4d905206cf32e1-Abstract-Conference.html": {
    "title": "Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training",
    "volume": "main",
    "abstract": "LLMs are computationally expensive to pre-train due to their large scale.Model growth emerges as a promising approach by leveraging smaller models to accelerate the training of larger ones. However, the viability of these model growth methods in efficient LLM pre-training remains underexplored.This work identifies three critical $\\underline{\\textit{O}}$bstacles: ($\\textit{O}$1) lack of comprehensive evaluation, ($\\textit{O}$2) untested viability for scaling, and ($\\textit{O}$3) lack of empirical guidelines.To tackle $\\textit{O}$1, we summarize existing approaches into four atomic growth operators and systematically evaluate them in a standardized LLM pre-training setting.Our findings reveal that a depthwise stacking operator, called $G_{\\text{stack}}$, exhibits remarkable acceleration in training, leading to decreased loss and improved overall performance on eight standard NLP benchmarks compared to strong baselines. Motivated by these promising results, we conduct extensive experiments to delve deeper into $G_{\\text{stack}}$ to address $\\textit{O}$2 and $\\textit{O}$3.For $\\textit{O}$2 (untested scalability), our study shows that $G_{\\text{stack}}$ is scalable and consistently performs well, with experiments up to 7B LLMs after growth and pre-training LLMs with 750B tokens.For example, compared to a conventionally trained 7B model using 300B tokens, our $G_{\\text{stack}}$ model converges to the same loss with 194B tokens, resulting in a 54.6\\% speedup. We further address $\\textit{O}$3 (lack of empirical guidelines) by formalizing guidelines to determine growth timing and growth factor for $G_{\\text{stack}}$, making it practical in general LLM pre-training.We also provide in-depth discussions and comprehensive ablation studies of $G_{\\text{stack}}$. Our code and pre-trained model are available at https://llm-stacking.github.io/",
    "checked": true,
    "id": "bc6a196c92670b1d6750c2fab5a335ee14fcaaea",
    "semantic_title": "stacking your transformers: a closer look at model growth for efficient llm pre-training",
    "citation_count": 16,
    "authors": [
      "Wenyu Du",
      "Tongxu Luo",
      "Zihan Qiu",
      "Zeyu Huang",
      "Yikang Shen",
      "Reynold Cheng",
      "Yike Guo",
      "Jie Fu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1456560769bbc38e4f8c5055048ea712-Abstract-Conference.html": {
    "title": "Annealed Multiple Choice Learning: Overcoming limitations of Winner-takes-all with annealing",
    "volume": "main",
    "abstract": "We introduce Annealed Multiple Choice Learning (aMCL) which combines simulated annealing with MCL. MCL is a learning framework handling ambiguous tasks by predicting a small set of plausible hypotheses. These hypotheses are trained using the Winner-takes-all (WTA) scheme, which promotes the diversity of the predictions. However, this scheme may converge toward an arbitrarily suboptimal local minimum, due to the greedy nature of WTA. We overcome this limitation using annealing, which enhances the exploration of the hypothesis space during training. We leverage insights from statistical physics and information theory to provide a detailed description of the model training trajectory. Additionally, we validate our algorithm by extensive experiments on synthetic datasets, on the standard UCI benchmark, and on speech separation",
    "checked": true,
    "id": "e7bd04111e8e347fd4a9219d7441c9eea0e17359",
    "semantic_title": "annealed multiple choice learning: overcoming limitations of winner-takes-all with annealing",
    "citation_count": 6,
    "authors": [
      "David Perera",
      "Victor Letzelter",
      "Theo Mariotte",
      "Adrien Cortes",
      "Mickael Chen",
      "Slim Essid",
      "Gaël Richard"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1468ecc3d7e9dc2fbf336eed9bb292e0-Abstract-Conference.html": {
    "title": "Are Multiple Instance Learning Algorithms Learnable for Instances?",
    "volume": "main",
    "abstract": "Multiple Instance Learning (MIL) has been increasingly adopted to mitigate the high costs and complexity associated with labeling individual instances, learning instead from bags of instances labeled at the bag level and enabling instance-level labeling. While existing research has primarily focused on the learnability of MIL at the bag level, there is an absence of theoretical exploration to check if a given MIL algorithm is learnable at the instance level. This paper proposes a theoretical framework based on probably approximately correct (PAC) learning theory to assess the instance-level learnability of deep multiple instance learning (Deep MIL) algorithms. Our analysis exposes significant gaps between current Deep MIL algorithms, highlighting the theoretical conditions that must be satisfied by MIL algorithms to ensure instance-level learnability. With these conditions, we interpret the learnability of the representative Deep MIL algorithms and validate them through empirical studies",
    "checked": true,
    "id": "4e953d2c60fa3f77d50eeddb444783aeed924547",
    "semantic_title": "are multiple instance learning algorithms learnable for instances?",
    "citation_count": 2,
    "authors": [
      "Jaeseok Jang",
      "HYUK-YOON KWON"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/14730e0dd6ac1c4a5765310909fd51b1-Abstract-Conference.html": {
    "title": "Layer-Adaptive State Pruning for Deep State Space Models",
    "volume": "main",
    "abstract": "Due to the lack of state dimension optimization methods, deep state space models (SSMs) have sacrificed model capacity, training search space, or stability to alleviate computational costs caused by high state dimensions. In this work, we provide a structured pruning method for SSMs, Layer-Adaptive STate pruning (LAST), which reduces the state dimension of each layer in minimizing model-level output energy loss by extending modal truncation for a single system. LAST scores are evaluated using the $\\mathcal{H}_{\\infty}$ norms of subsystems and layer-wise energy normalization. The scores serve as global pruning criteria, enabling cross-layer comparison of states and layer-adaptive pruning. Across various sequence benchmarks, LAST optimizes previous SSMs, revealing the redundancy and compressibility of their state spaces. Notably, we demonstrate that, on average, pruning 33\\% of states still maintains performance with 0.52\\% accuracy loss in multi-input multi-output SSMs without retraining. Code is available at https://github.com/msgwak/LAST",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minseon Gwak",
      "Seongrok Moon",
      "Joohwan Ko",
      "PooGyeon Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1486bbd863ba396398ff4227c5b3dccd-Abstract-Conference.html": {
    "title": "Score-based 3D molecule generation with neural fields",
    "volume": "main",
    "abstract": "We introduce a new representation for 3D molecules based on their continuous atomic density fields. Using this representation, we propose a new model based on walk-jump sampling for unconditional 3D molecule generation in the continuous space using neural fields. Our model, FuncMol, encodes molecular fields into latent codes using a conditional neural field, samples noisy codes from a Gaussian-smoothed distribution with Langevin MCMC (walk), denoises these samples in a single step (jump), and finally decodes them into molecular fields. FuncMol performs all-atom generation of 3D molecules without assumptions on the molecular structure and scales well with the size of molecules, unlike most approaches. Our method achieves competitive results on drug-like molecules and easily scales to macro-cyclic peptides, with at least one order of magnitude faster sampling. The code is available at https://github.com/prescient-design/funcmol",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthieu Kirchmeyer",
      "Pedro O O. Pinheiro",
      "Saeed Saremi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1496b12ccd6c8d2b4fd98f24a12bd438-Abstract-Conference.html": {
    "title": "On Sparse Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "The classical Canonical Correlation Analysis (CCA) identifies the correlations between two sets of multivariate variables based on their covariance, which has been widely applied in diverse fields such as computer vision, natural language processing, and speech analysis. Despite its popularity, CCA can encounter challenges in explaining correlations between two variable sets within high-dimensional data contexts. Thus, this paper studies Sparse Canonical Correlation Analysis (SCCA) that enhances the interpretability of CCA. We first show that SCCA generalizes three well-known sparse optimization problems, sparse PCA, sparse SVD, and sparse regression, which are all classified as NP-hard problems. This result motivates us to develop strong formulations and efficient algorithms. Our main contributions include (i) the introduction of a combinatorial formulation that captures the essence of SCCA and allows the development of exact and approximation algorithms; (ii) the establishment of the complexity results for two low-rank special cases of SCCA; and (iii) the derivation of an equivalent mixed-integer semidefinite programming model that facilitates a specialized branch-and-cut algorithm with analytical cuts. The effectiveness of our proposed formulations and algorithms is validated through numerical experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongchun Li",
      "Santanu Dey",
      "Weijun Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/149ad6e32c08b73a3ecc3d11977fcc47-Abstract-Conference.html": {
    "title": "A Pairwise Pseudo-likelihood Approach for Matrix Completion with Informative Missingness",
    "volume": "main",
    "abstract": "While several recent matrix completion methods are developed to deal with non-uniform observation probabilities across matrix entries, very few allow the missingness to depend on the mostly unobserved matrix measurements, which is generally ill-posed. We aim to tackle a subclass of these ill-posed settings, characterized by a flexible separable observation probability assumption that can depend on the matrix measurements. We propose a regularized pairwise pseudo-likelihood approach for matrix completion and prove that the proposed estimator can asymptotically recover the low-rank parameter matrix up to an identifiable equivalence class of a constant shift and scaling, at a near-optimal asymptotic convergence rate of the standard well-posed (non-informative missing) setting, while effectively mitigating the impact of informative missingness. The efficacy of our method is validated via numerical experiments, positioning it as a robust tool for matrix completion to mitigate data bias",
    "checked": true,
    "id": "e47f254d3523d3cc1431bc12cabc5b6a6d048fc4",
    "semantic_title": "a pairwise pseudo-likelihood approach for matrix completion with informative missingness",
    "citation_count": 2,
    "authors": [
      "Jiangyuan Li",
      "Jiayi Wang",
      "Raymond K. W. Wong",
      "Kwun Chuen Gary Chan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/14a1f12a530a934dc034f4c1e2d97aa8-Abstract-Conference.html": {
    "title": "Toward Global Convergence of Gradient EM for Over-Paramterized Gaussian Mixture Models",
    "volume": "main",
    "abstract": "We study the gradient Expectation-Maximization (EM) algorithm for Gaussian Mixture Models (GMM) in the over-parameterized setting, where a general GMM with $n>1$ components learns from data that are generated by a single ground truth Gaussian distribution. While results for the special case of 2-Gaussian mixtures are well-known, a general global convergence analysis for arbitrary $n$ remains unresolved and faces several new technical barriers since the convergence becomes sub-linear and non-monotonic. To address these challenges, we construct a novel likelihood-based convergence analysis framework and rigorously prove that gradient EM converges globally with a sublinear rate $O(1/\\sqrt{t})$. This is the first global convergence result for Gaussian mixtures with more than $2$ components. The sublinear convergence rate is due to the algorithmic nature of learning over-parameterized GMM with gradient EM. We also identify a new emerging technical challenge for learning general over-parameterized GMM: the existence of bad local regions that can trap gradient EM for an exponential number of steps",
    "checked": false,
    "id": "f519f27a9b91aefd75b384ba8e36b3535a311424",
    "semantic_title": "toward global convergence of gradient em for over-parameterized gaussian mixture models",
    "citation_count": 3,
    "authors": [
      "Weihang Xu",
      "Maryam Fazel",
      "Simon S Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/14ad9256c430e6c8977e470d8e268320-Abstract-Conference.html": {
    "title": "Conditioning non-linear and infinite-dimensional diffusion processes",
    "volume": "main",
    "abstract": "Generative diffusion models and many stochastic models in science and engineering naturally live in infinite dimensions before discretisation. To incorporate observed data for statistical and learning tasks, one needs to condition on observations. While recent work has treated conditioning linear processes in infinite dimensions, conditioning non-linear processes in infinite dimensions has not been explored. This paper conditions function valued stochastic processes without prior discretisation. To do so, we use an infinite-dimensional version of Girsanov's theorem to condition a function-valued stochastic process, leading to a stochastic differential equation (SDE) for the conditioned process involving the score. We apply this technique to do time series analysis for shapes of organisms in evolutionary biology, where we discretise via the Fourier basis and then learn the coefficients of the score function with score matching methods",
    "checked": true,
    "id": "64ca9e7b4efb75c706a24c4f0227801f9a39466c",
    "semantic_title": "conditioning non-linear and infinite-dimensional diffusion processes",
    "citation_count": 9,
    "authors": [
      "Elizabeth L. Baker",
      "Gefan Yang",
      "Michael Severinsen",
      "Christy Hipsley",
      "Stefan Sommer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/14bb27f680bee45d83bc769738e7f9b5-Abstract-Conference.html": {
    "title": "On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions",
    "volume": "main",
    "abstract": "In this paper, we study Adam in non-convex smooth scenarios with potential unbounded gradients and affine variance noise. We consider a general noise model which governs affine variance noise, bounded noise, and sub-Gaussian noise. We show that Adam with a specific hyper-parameter setup can find a stationary point with a $\\mathcal{O}(\\text{poly}(\\log T)/\\sqrt{T})$ rate in high probability under this general noise model where $T$ denotes total number iterations, matching the lower rate of stochastic first-order algorithms up to logarithm factors. We also provide a probabilistic convergence result for Adam under a generalized smooth condition which allows unbounded smoothness parameters and has been illustrated empirically to capture the smooth property of many practical objective functions more accurately",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusu Hong",
      "Junhong Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/14bc8528848d15d2d096127d0f64c1f9-Abstract-Conference.html": {
    "title": "Quantum algorithm for large-scale market equilibrium computation",
    "volume": "main",
    "abstract": "Classical algorithms for market equilibrium computation such as proportional response dynamics face scalability issues with Internet-based applications such as auctions, recommender systems, and fair division, despite having an almost linear runtime in terms of the product of buyers and goods. In this work, we provide the first quantum algorithm for market equilibrium computation with sub-linear performance. Our algorithm provides a polynomial runtime speedup in terms of the product of the number of buyers and goods while reaching the same optimization objective value as the classical algorithm. Numerical simulations of a system with 16384 buyers and goods support our theoretical results that our quantum algorithm provides a significant speedup",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Po-Wei Huang",
      "Patrick Rebentrost"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/14c00f4bc19a5498982b16647998e894-Abstract-Conference.html": {
    "title": "Adversarially Robust Dense-Sparse Tradeoffs via Heavy-Hitters",
    "volume": "main",
    "abstract": "In the adversarial streaming model, the input is a sequence of adaptive updates that defines an underlying dataset and the goal is to approximate, collect, or compute some statistic while using space sublinear in the size of the dataset. In 2022, Ben-Eliezer, Eden, and Onak showed a dense-sparse trade-off technique that elegantly combined sparse recovery with known techniques using differential privacy and sketch switching to achieve adversarially robust algorithms for $L_p$ estimation and other algorithms on turnstile streams. However, there has been no progress since, either in terms of achievability or impossibility. In this work, we first give improved algorithms for adversarially robust $L_p$-heavy hitters, utilizing deterministic turnstile heavy-hitter algorithms with better tradeoffs. We then utilize our heavy-hitter algorithm to reduce the problem to estimating the frequency moment of the tail vector. We give a new algorithm for this problem in the classical streaming setting, which achieves additive error and uses space independent in the size of the tail. We then leverage these ingredients to give an improved algorithm for adversarially robust $L_p$ estimation on turnstile streams. We believe that our results serve as an important conceptual message, demonstrating that there is no inherent barrier at the previous state-of-the-art",
    "checked": true,
    "id": "7b6bd9186c5e1b60cedc0ed09b9b09f648873836",
    "semantic_title": "adversarially robust dense-sparse tradeoffs via heavy-hitters",
    "citation_count": 4,
    "authors": [
      "David Woodruff",
      "Samson Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/14c018d2e72c521605b0567029ef0efb-Abstract-Conference.html": {
    "title": "I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token",
    "volume": "main",
    "abstract": "Large Language Models are known to capture real-world knowledge, allowing them to excel in many downstream tasks. Despite recent advances, these models are still prone to what are commonly known as hallucinations, causing them to emit unwanted and factually incorrect text. In this work, we propose a novel calibration method that can be used to combat hallucinations. We add a special [IDK] (\"I Don't Know\") token to the model's vocabulary and introduce an objective function that shifts probability mass to the [IDK] token for incorrect predictions. This approach allows the model to express uncertainty in its output explicitly. We evaluate our proposed method across multiple model architectures and factual downstream tasks.We find that models trained with our method are able to express uncertainty in places where they would previously make mistakes while suffering only a small loss of encoded knowledge. We further perform extensive ablation studies of multiple variations of our approach and provide a detailed analysis of the precision-recall tradeoff of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roi Cohen",
      "Konstantin Dobler",
      "Eden Biran",
      "Gerard de Melo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/14cdc9013d80338bf81483a7736ea05c-Abstract-Conference.html": {
    "title": "Scaling Continuous Latent Variable Models as Probabilistic Integral Circuits",
    "volume": "main",
    "abstract": "Probabilistic integral circuits (PICs) have been recently introduced as probabilistic models enjoying the key ingredient behind expressive generative models: continuous latent variables (LVs). PICs are symbolic computational graphs defining continuous LV models as hierarchies of functions that are summed and multiplied together, or integrated over some LVs. They are tractable if LVs can be analytically integrated out, otherwise they can be approximated by tractable probabilistic circuits (PC) encoding a hierarchical numerical quadrature process, called QPCs.So far, only tree-shaped PICs have been explored, and training them via numerical quadrature requires memory-intensive processing at scale. In this paper, we address these issues, and present: (i) a pipeline for building DAG-shaped PICs out of arbitrary variable decompositions, (ii) a procedure for training PICs using tensorized circuit architectures, and (iii) neural functional sharing techniques to allow scalable training. In extensive experiments, we showcase the effectiveness of functional sharing and the superiority of QPCs over traditional PCs",
    "checked": true,
    "id": "daa8d6554383b4951414495db653ba69d6846883",
    "semantic_title": "scaling continuous latent variable models as probabilistic integral circuits",
    "citation_count": 6,
    "authors": [
      "Gennaro Gala",
      "Cassio P de Campos",
      "Antonio Vergari",
      "Erik Quaeghebeur"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/14d2d59c5e2302bfa6ce6cae59156e33-Abstract-Conference.html": {
    "title": "What do Graph Neural Networks learn? Insights from Tropical Geometry",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have been analyzed from multiple perspectives, including the WL-hierarchy, which exposes limits on their expressivity to distinguish graphs. However, characterizing the class of functions that they learn has remained unresolved. We address this fundamental question for message passing GNNs under ReLU activations, i.e., the de-facto choice for most GNNs.We first show that such GNNs learn tropical rational signomial maps or continuous piecewise linear functions, establishing an equivalence with feedforward networks (FNNs). We then elucidate the role of the choice of aggregation and update functions, and derive the first general upper and lower bounds on the geometric complexity (i.e., the number of linear regions), establishing new results for popular architectures such as GraphSAGE and GIN. We also introduce and theoretically analyze several new architectures to illuminate the relative merits of the feedforward and the message passing layers, and the tradeoffs involving depth and number of trainable parameters. Finally, we also characterize the decision boundary for node and graph classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuan Anh Pham",
      "Vikas Garg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/14da7aea05debb963b3d8d46449d51a0-Abstract-Conference.html": {
    "title": "Understanding the Expressivity and Trainability of Fourier Neural Operator: A Mean-Field Perspective",
    "volume": "main",
    "abstract": "In this paper, we explores the expressivity and trainability of the Fourier Neural Operator (FNO). We establish a mean-field theory for the FNO, analyzing the behavior of the random FNO from an \\emph{edge of chaos} perspective. Our investigation into the expressivity of a random FNO involves examining the ordered-chaos phase transition of the network based on the weight distribution. This phase transition demonstrates characteristics unique to the FNO, induced by mode truncation, while also showcasing similarities to those of densely connected networks. Furthermore, we identify a connection between expressivity and trainability: the ordered and chaotic phases correspond to regions of vanishing and exploding gradients, respectively. This finding provides a practical prerequisite for the stable training of the FNO. Our experimental results corroborate our theoretical findings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takeshi Koshizuka",
      "Masahiro Fujisawa",
      "Yusuke Tanaka",
      "Issei Sato"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/14fc4a68da97a3d31eb11c642b0b10fc-Abstract-Conference.html": {
    "title": "Selective Attention: Enhancing Transformer through Principled Context Control",
    "volume": "main",
    "abstract": "The attention mechanism within the transformer architecture enables the model to weigh and combine tokens based on their relevance to the query. While self-attention has enjoyed major success, it notably treats all queries $q$ in the same way by applying the mapping $V^\\top\\text{softmax}(Kq)$, where $V,K$ are the value and key embeddings respectively. In this work, we argue that this uniform treatment hinders the ability to control contextual sparsity and relevance. As a solution, we introduce the Selective Self-Attention (SSA) layer that augments the softmax nonlinearity with a principled temperature scaling strategy. By controlling temperature, SSA adapts the contextual sparsity of the attention map to the query embedding and its position in the context window. Through theory and experiments, we demonstrate that this alleviates attention dilution, aids the optimization process, and enhances the model's ability to control softmax spikiness of individual queries. We also incorporate temperature scaling for value embeddings and show that it boosts the model's ability to suppress irrelevant/noisy tokens. Notably, SSA is a lightweight method which introduces less than 0.5\\% new parameters through a weight-sharing strategy and can be fine-tuned on existing LLMs. Extensive empirical evaluations demonstrate that SSA-equipped models achieve a noticeable and consistent accuracy improvement on language modeling benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuechen Zhang",
      "Xiangyu Chang",
      "Mingchen Li",
      "Amit Roy-Chowdhury",
      "Jiasi Chen",
      "Samet Oymak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/14fef58f09f2ebe69306e0a322e3be2b-Abstract-Conference.html": {
    "title": "Differentially Private Stochastic Gradient Descent with Fixed-Size Minibatches: Tighter RDP Guarantees with or without Replacement",
    "volume": "main",
    "abstract": "Differentially private stochastic gradient descent (DP-SGD) has been instrumental in privately training deep learning models by providing a framework to control and track the privacy loss incurred during training. At the core of this computation lies a subsampling method that uses a privacy amplification lemma to enhance the privacy guarantees provided by the additive noise. Fixed size subsampling is appealing for its constant memory usage, unlike the variable sized minibatches in Poisson subsampling. It is also of interest in addressing class imbalance and federated learning. Current computable guarantees for fixed-size subsampling are not tight and do not consider both add/remove and replace-one adjacency relationships. We present a new and holistic Rényi differential privacy (RDP) accountant for DP-SGD with fixed-size subsampling without replacement (FSwoR) and with replacement (FSwR). For FSwoR we consider both add/remove and replace-one adjacency, where we improve on the best current computable bound by a factor of $4$. We also show for the first time that the widely-used Poisson subsampling and FSwoR with replace-one adjacency have the same privacy to leading order in the sampling probability. Our work suggests that FSwoR is often preferable to Poisson subsampling due to constant memory usage. Our FSwR accountant includes explicit non-asymptotic upper and lower bounds and, to the authors' knowledge, is the first such RDP analysis of fixed-size subsampling with replacement for DP-SGD. We analytically and empirically compare fixed size and Poisson subsampling, and show that DP-SGD gradients in a fixed-size subsampling regime exhibit lower variance in practice in addition to memory usage benefits",
    "checked": true,
    "id": "0ef1dea9736a2f6f73e281b557339eaa5a1a6acb",
    "semantic_title": "differentially private stochastic gradient descent with fixed-size minibatches: tighter rdp guarantees with or without replacement",
    "citation_count": 2,
    "authors": [
      "Jeremiah Birrell",
      "Reza Ebrahimi",
      "Rouzbeh Behnia",
      "Jason Pacheco"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/150f35763dc51bfc269690d36a5a7c88-Abstract-Conference.html": {
    "title": "A Simple and Optimal Approach for Universal Online Learning with Gradient Variations",
    "volume": "main",
    "abstract": "We investigate the problem of universal online learning with gradient-variation regret. Universal online learning aims to achieve regret guarantees without prior knowledge of the curvature of the online functions. Moreover, we study the problem-dependent gradient-variation regret as it plays a crucial role in bridging stochastic and adversarial optimization as well as game theory. In this work, we design a universal approach with the *optimal* gradient-variation regret simultaneously for strongly convex, exp-concave, and convex functions, thus addressing an open problem highlighted by [Yan et al. [2023]](https://openreview.net/forum?id=AA1xrgAP5z). Our approach is *simple* since it is algorithmically efficient-to-implement with a two-layer online ensemble structure and only $1$ gradient query per round, and theoretically easy-to-analyze with a novel and alternative analysis to the gradient-variation regret. Concretely, previous works on gradient variations require controlling the algorithmic stability, which is challenging and leads to sub-optimal regret and less efficient algorithm design. Our analysis overcomes this issue by using a Bregman divergence negative term from linearization and a useful smoothness property",
    "checked": true,
    "id": "9117cf6198f86c8c32b67d5e78619ff680e77f74",
    "semantic_title": "a simple and optimal approach for universal online learning with gradient variations",
    "citation_count": 2,
    "authors": [
      "Yu-Hu Yan",
      "Peng Zhao",
      "Zhi-Hua Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/150fe800c9e6ed36d16f6a157eedb6e1-Abstract-Conference.html": {
    "title": "LESS: Label-Efficient and Single-Stage Referring 3D Segmentation",
    "volume": "main",
    "abstract": "Referring 3D Segmentation is a visual-language task that segments all points of the specified object from a 3D point cloud described by a sentence of query. Previous works perform a two-stage paradigm, first conducting language-agnostic instance segmentation then matching with given text query. However, the semantic concepts from text query and visual cues are separately interacted during the training, and both instance and semantic labels for each object are required, which is time consuming and human-labor intensive. To mitigate these issues, we propose a novel Referring 3D Segmentation pipeline, Label-Efficient and Single-Stage, dubbed LESS, which is only under the supervision of efficient binary mask. Specifically, we design a Point-Word Cross-Modal Alignment module for aligning the fine-grained features of points and textual embedding. Query Mask Predictor module and Query-Sentence Alignment module are introduced for coarse-grained alignment between masks and query. Furthermore, we propose an area regularization loss, which coarsely reduces irrelevant background predictions on a large scale. Besides, a point-to-point contrastive loss is proposed concentrating on distinguishing points with subtly similar features. Through extensive experiments, we achieve state-of-the-art performance on ScanRefer dataset by surpassing the previous methods about 3.7% mIoU using only binary labels. Code is available at https://github.com/mellody11/LESS",
    "checked": true,
    "id": "98bbbf1508e5148886ee971bd0ebd10146fff89b",
    "semantic_title": "less: label-efficient and single-stage referring 3d segmentation",
    "citation_count": 1,
    "authors": [
      "Xuexun Liu",
      "Xiaoxu Xu",
      "Jinlong Li",
      "Qiudan Zhang",
      "Xu Wang",
      "Nicu Sebe",
      "Lin Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/152035ddc7b4f35cf7ede4125c39ea4a-Abstract-Conference.html": {
    "title": "Optimal Algorithms for Augmented Testing of Discrete Distributions",
    "volume": "main",
    "abstract": "We consider the problem of hypothesis testing for discrete distributions. In the standard model, where we have sample access to an underlying distribution $p$, extensive research has established optimal bounds for uniformity testing, identity testing (goodness of fit), and closeness testing (equivalence or two-sample testing). We explore these problems in a setting where a predicted data distribution, possibly derived from historical data or predictive machine learning models, is available. We demonstrate that such a predictor can indeed reduce the number of samples required for all three property testing tasks. The reduction in sample complexity depends directly on the predictor's quality, measured by its total variation distance from $p$. A key advantage of our algorithms is their adaptability to the precision of the prediction. Specifically, our algorithms can self-adjust their sample complexity based on the accuracy of the available prediction, operating without any prior knowledge of the estimation's accuracy (i.e. they are consistent). Additionally, we never use more samples than the standard approaches require, even if the predictions provide no meaningful information (i.e. they are also robust). We provide lower bounds to indicate that the improvements in sample complexity achieved by our algorithms are information-theoretically optimal. Furthermore, experimental results show that the performance of our algorithms on real data significantly exceeds our worst-case guarantees for sample complexity, demonstrating the practicality of our approach",
    "checked": true,
    "id": "96b2e96763bd9b540fde8e867350067847b20016",
    "semantic_title": "optimal algorithms for augmented testing of discrete distributions",
    "citation_count": 0,
    "authors": [
      "Maryam Aliakbarpour",
      "Piotr Indyk",
      "Ronitt Rubinfeld",
      "Sandeep Silwal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1529a05fdff470f4e7c239c80d85e28e-Abstract-Conference.html": {
    "title": "Mind the Gap Between Prototypes and Images in Cross-domain Finetuning",
    "volume": "main",
    "abstract": "In cross-domain few-shot classification (CFC), recent works mainly focus on adapting a simple transformation head on top of a frozen pre-trained backbone with few labeled data to project embeddings into a task-specific metric space where classification can be performed by measuring similarities between image instance and prototype representations. Technically, an assumption implicitly adopted in such a framework is that the prototype and image instance embeddings share the same representation transformation. However, in this paper, we find that there naturally exists a gap, which resembles the modality gap, between the prototype and image instance embeddings extracted from the frozen pre-trained backbone, and simply applying the same transformation during the adaptation phase constrains exploring the optimal representation distributions and shrinks the gap between prototype and image representations. To solve this problem, we propose a simple yet effective method, contrastive prototype-image adaptation (CoPA), to adapt different transformations for prototypes and images similarly to CLIP by treating prototypes as text prompts. Extensive experiments on Meta-Dataset demonstrate that CoPA achieves the state-of-the-art performance more efficiently. Meanwhile, further analyses also indicate that CoPA can learn better representation clusters, enlarge the gap, and achieve the minimum validation loss at the enlarged gap",
    "checked": true,
    "id": "ad55abc374f827ce4610c6318137cd12d419d339",
    "semantic_title": "mind the gap between prototypes and images in cross-domain finetuning",
    "citation_count": 1,
    "authors": [
      "Hongduan Tian",
      "Feng Liu",
      "Zhanke Zhou",
      "Tongliang Liu",
      "Chengqi Zhang",
      "Bo Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1533b2d13d0e0078fd193ec78ac3f8a5-Abstract-Conference.html": {
    "title": "Robust Reinforcement Learning with General Utility",
    "volume": "main",
    "abstract": "Reinforcement Learning (RL) problem with general utility is a powerful decision making framework that covers standard RL with cumulative cost, exploration problems, and demonstration learning. Existing works on RL with general utility do not consider the robustness under environmental perturbation, which is important to adapt RL system in the real-world environment that differs from the training environment. To train a robust policy, we propose a robust RL framework with general utility, which subsumes many existing RL frameworks including RL, robust RL, RL with general utility, constrained RL, robust constrained RL, pure exploration, robust entropy regularized RL, etc. Then we focus on popular convex utility functions, with which our proposed learning framework is a challenging nonconvex-nonconcave minimax optimization problem, and design a two-phase stochastic policy gradient type algorithm and obtain its sample complexity result for gradient convergence. Furthermore, for convex utility on a widely used polyhedral ambiguity set, we design an algorithm and obtain its convergence rate to a global optimal solution",
    "checked": true,
    "id": "7caec093edf65fb7d960ca19e029d79b8f720816",
    "semantic_title": "robust reinforcement learning with general utility",
    "citation_count": 0,
    "authors": [
      "Ziyi Chen",
      "Yan Wen",
      "Zhengmian Hu",
      "Heng Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/154926e0b66e2b2a8c1120852f31a12d-Abstract-Conference.html": {
    "title": "Addressing Bias in Online Selection with Limited Budget of Comparisons",
    "volume": "main",
    "abstract": "Consider a hiring process with candidates coming from different universities. It is easy to order candidates with the same background, yet it can be challenging to compare them otherwise. The latter case requires additional costly assessments, leading to a potentially high total cost for the hiring organization. Given an assigned budget, what would be an optimal strategy to select the most qualified candidate?We model the above problem as a multicolor secretary problem, allowing comparisons between candidates from distinct groups at a fixed cost. Our study explores how the allocated budget enhances the success probability of online selection algorithms",
    "checked": true,
    "id": "bc3cddee3cf9b0d731f013401d08a82aed97f4d4",
    "semantic_title": "addressing bias in online selection with limited budget of comparisons",
    "citation_count": 4,
    "authors": [
      "Ziyad Benomar",
      "Evgenii Chzhen",
      "Nicolas Schreuder",
      "Vianney Perchet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/154bfcc104885ad8e0bfeb829a1839e8-Abstract-Conference.html": {
    "title": "MonoMAE: Enhancing Monocular 3D Detection through Depth-Aware Masked Autoencoders",
    "volume": "main",
    "abstract": "Monocular 3D object detection aims for precise 3D localization and identification of objects from a single-view image. Despite its recent progress, it often struggles while handling pervasive object occlusions that tend to complicate and degrade the prediction of object dimensions, depths, and orientations. We design MonoMAE, a monocular 3D detector inspired by Masked Autoencoders that addresses the object occlusion issue by masking and reconstructing objects in the feature space. MonoMAE consists of two novel designs. The first is depth-aware masking that selectively masks certain parts of non-occluded object queries in the feature space for simulating occluded object queries for network training. It masks non-occluded object queries by balancing the masked and preserved query portions adaptively according to the depth information. The second is lightweight query completion that works with the depth-aware masking to learn to reconstruct and complete the masked object queries. With the proposed feature-space occlusion and completion, MonoMAE learns enriched 3D representations that achieve superior monocular 3D detection performance qualitatively and quantitatively for both occluded and non-occluded objects. Additionally, MonoMAE learns generalizable representations that can work well in new domains",
    "checked": true,
    "id": "ad8322fafce2e1f021dc8ba20af3a0cfbc82da37",
    "semantic_title": "monomae: enhancing monocular 3d detection through depth-aware masked autoencoders",
    "citation_count": 9,
    "authors": [
      "Xueying Jiang",
      "Sheng Jin",
      "Xiaoqin Zhang",
      "Ling Shao",
      "Shijian Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1551c01d7a3d0bf21e2518331e9f7074-Abstract-Conference.html": {
    "title": "Measuring Goal-Directedness",
    "volume": "main",
    "abstract": "We define maximum entropy goal-directedness (MEG), a formal measure of goal-directedness in causal models and Markov decision processes, and give algorithmsfor computing it. Measuring goal-directedness is important, as it is a criticalelement of many concerns about harm from AI. It is also of philosophical interest,as goal-directedness is a key aspect of agency. MEG is based on an adaptation ofthe maximum causal entropy framework used in inverse reinforcement learning. Itcan measure goal-directedness with respect to a known utility function, a hypothesisclass of utility functions, or a set of random variables. We prove that MEG satisfiesseveral desiderata and demonstrate our algorithms with small-scale experiments",
    "checked": true,
    "id": "45cb6b329cef15d788b53e03dfb989a6bdde8ef8",
    "semantic_title": "measuring goal-directedness",
    "citation_count": 2,
    "authors": [
      "Matt MacDermott",
      "James Fox",
      "Francesco Belardinelli",
      "Tom Everitt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/155a94c71f0a2a3cb7eacbf733b5c64b-Abstract-Conference.html": {
    "title": "Long-Range Feedback Spiking Network Captures Dynamic and Static Representations of the Visual Cortex under Movie Stimuli",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) are widely used models for investigating biological visual representations. However, existing DNNs are mostly designed to analyze neural responses to static images, relying on feedforward structures and lacking physiological neuronal mechanisms. There is limited insight into how the visual cortex represents natural movie stimuli that contain context-rich information. To address these problems, this work proposes the long-range feedback spiking network (LoRaFB-SNet), which mimics top-down connections between cortical regions and incorporates spike information processing mechanisms inherent to biological neurons. Taking into account the temporal dependence of representations under movie stimuli, we present Time-Series Representational Similarity Analysis (TSRSA) to measure the similarity between model representations and visual cortical representations of mice. LoRaFB-SNet exhibits the highest level of representational similarity, outperforming other well-known and leading alternatives across various experimental paradigms, especially when representing long movie stimuli. We further conduct experiments to quantify how temporal structures (dynamic information) and static textures (static information) of the movie stimuli influence representational similarity, suggesting that our model benefits from long-range feedback to encode context-dependent representations just like the brain. Altogether, LoRaFB-SNet is highly competent in capturing both dynamic and static representations of the mouse visual cortex and contributes to the understanding of movie processing mechanisms of the visual system. Our codes are available at https://github.com/Grasshlw/SNN-Neural-Similarity-Movie",
    "checked": true,
    "id": "bb6acda877af9ad061b52fb6c804574b3c0d33d2",
    "semantic_title": "long-range feedback spiking network captures dynamic and static representations of the visual cortex under movie stimuli",
    "citation_count": 2,
    "authors": [
      "Liwei Huang",
      "Zhengyu Ma",
      "Liutao Yu",
      "Huihui Zhou",
      "Yonghong Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/155ace40b55215479b6ad4665d1f6c06-Abstract-Conference.html": {
    "title": "Typicalness-Aware Learning for Failure Detection",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) often suffer from the overconfidence issue, where incorrect predictions are made with high confidence scores, hindering the applications in critical systems. In this paper, we propose a novel approach called Typicalness-Aware Learning (TAL) to address this issue and improve failure detection performance. We observe that, with the cross-entropy loss, model predictions are optimized to align with the corresponding labels via increasing logit magnitude or refining logit direction. However, regarding atypical samples, the image content and their labels may exhibit disparities. This discrepancy can lead to overfitting on atypical samples, ultimately resulting in the overconfidence issue that we aim to address.To address this issue, we have devised a metric that quantifies the typicalness of each sample, enabling the dynamic adjustment of the logit magnitude during the training process. By allowing relatively atypical samples to be adequately fitted while preserving reliable logit direction, the problem of overconfidence can be mitigated. TAL has been extensively evaluated on benchmark datasets, and the results demonstrate its superiority over existing failure detection methods. Specifically, TAL achieves a more than 5\\% improvement on CIFAR100 in terms of the Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art. Code is available at https://github.com/liuyijungoon/TAL",
    "checked": true,
    "id": "9bba4dc5fe3ce37c9bec07136b64a8a0e6ed8b1e",
    "semantic_title": "typicalness-aware learning for failure detection",
    "citation_count": 3,
    "authors": [
      "Yijun Liu",
      "Jiequan Cui",
      "Zhuotao Tian",
      "Senqiao Yang",
      "Qingdong He",
      "Xiaoling Wang",
      "Jingyong Su"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1588dc2b2ef339d6e4c47d212e36f991-Abstract-Conference.html": {
    "title": "Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Understanding cognitive processes in multi-agent interactions is a primary goal in cognitive science. It can guide the direction of artificial intelligence (AI) research toward social decision-making in multi-agent systems, which includes uncertainty from character heterogeneity. In this paper, we introduce episodic future thinking (EFT) mechanism for a reinforcement learning (RL) agent, inspired by the cognitive processes observed in animals. To enable future thinking functionality, we first develop a multi-character policy that captures diverse characters with an ensemble of heterogeneous policies. The character of an agent is defined as a different weight combination on reward components, representing distinct behavioral preferences. The future thinking agent collects observation-action trajectories of the target agents and leverages the pre-trained multi-character policy to infer their characters. Once the character is inferred, the agent predicts the upcoming actions of target agents and simulates the potential future scenario. This capability allows the agent to adaptively select the optimal action, considering the predicted future scenario in multi-agent scenarios. To evaluate the proposed mechanism, we consider the multi-agent autonomous driving scenario in which autonomous vehicles with different driving traits are on the road. Simulation results demonstrate that the EFT mechanism with accurate character inference leads to a higher reward than existing multi-agent solutions. We also confirm that the effect of reward improvement remains valid across societies with different levels of character diversity",
    "checked": true,
    "id": "a98ef94d016397735589267bd685a29a3190daab",
    "semantic_title": "episodic future thinking mechanism for multi-agent reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Dongsu Lee",
      "Minhae Kwon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/158ac5698e36a01ee5ca9e6732685b34-Abstract-Conference.html": {
    "title": "Slot State Space Models",
    "volume": "main",
    "abstract": "Recent State Space Models (SSMs) such as S4, S5, and Mamba have shown remarkable computational benefits in long-range temporal dependency modeling. However, in many sequence modeling problems, the underlying process is inherently modular and it is of interest to have inductive biases that mimic this modular structure. In this paper, we introduce SlotSSMs, a novel framework for incorporating independent mechanisms into SSMs to preserve or encourage separation of information. Unlike conventional SSMs that maintain a monolithic state vector, SlotSSMs maintains the state as a collection of multiple vectors called slots. Crucially, the state transitions are performed independently per slot with sparse interactions across slots implemented via the bottleneck of self-attention. In experiments, we evaluate our model in object-centric learning, 3D visual reasoning, and long-context video understanding tasks, which involve modeling multiple objects and their long-range temporal dependencies. We find that our proposed design offers substantial performance gains over existing sequence modeling methods. Project page is available at \\url{https://slotssms.github.io/}",
    "checked": true,
    "id": "4cfbf0d0740e94e6b1aeeddbb5fbaae03a9029cc",
    "semantic_title": "slot state space models",
    "citation_count": 7,
    "authors": [
      "Jindong Jiang",
      "Fei Deng",
      "Gautam Singh",
      "Minseung Lee",
      "Sungjin Ahn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/158f036baa5b80a4fe2af094de8f7539-Abstract-Conference.html": {
    "title": "Incorporating Test-Time Optimization into Training with Dual Networks for Human Mesh Recovery",
    "volume": "main",
    "abstract": "Human Mesh Recovery (HMR) is the task of estimating a parameterized 3D human mesh from an image. There is a kind of methods first training a regression model for this problem, then further optimizing the pretrained regression model for any specific sample individually at test time. However, the pretrained model may not provide an ideal optimization starting point for the test-time optimization. Inspired by meta-learning, we incorporate the test-time optimization into training, performing a step of test-time optimization for each sample in the training batch before really conducting the training optimization over all the training samples. In this way, we obtain a meta-model, the meta-parameter of which is friendly to the test-time optimization. At test time, after several test-time optimization steps starting from the meta-parameter, we obtain much higher HMR accuracy than the test-time optimization starting from the simply pretrained regression model. Furthermore, we find test-time HMR objectives are different from training-time objectives, which reduces the effectiveness of the learning of the meta-model. To solve this problem, we propose a dual-network architecture that unifies the training-time and test-time objectives. Our method, armed with meta-learning and the dual networks, outperforms state-of-the-art regression-based and optimization-based HMR approaches, as validated by the extensive experiments. The codes are available at https://github.com/fmx789/Meta-HMR",
    "checked": true,
    "id": "0d7fe7fd0c2c3dd9e79454354137da2dc1df623e",
    "semantic_title": "incorporating test-time optimization into training with dual networks for human mesh recovery",
    "citation_count": 0,
    "authors": [
      "Yongwei Nie",
      "Mingxian Fan",
      "Chengjiang Long",
      "Qing Zhang",
      "Jian Zhu",
      "Xuemiao Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/15aaa9224a35527d76188b4d40e02308-Abstract-Conference.html": {
    "title": "On the Identifiability of Poisson Branching Structural Causal Model Using Probability Generating Function",
    "volume": "main",
    "abstract": "Causal discovery from observational data, especially for count data, is essential across scientific and industrial contexts, such as biology, economics, and network operation maintenance. For this task, most approaches model count data using Bayesian networks or ordinal relations. However, they overlook the inherent branching structures that are frequently encountered, e.g., a browsing event might trigger an adding cart or purchasing event. This can be modeled by a binomial thinning operator (for branching) and an additive independent Poisson distribution (for noising), known as Poisson Branching Structure Causal Model (PB-SCM). There is a provably sound cumulant-based causal discovery method that allows the identification of the causal structure under a branching structure. However, we show that there still remains a gap in that there exist causal directions that are identifiable while the algorithm fails to identify them. In this work, we address this gap by exploring the identifiability of PB-SCM using the Probability Generating Function (PGF). By developing a compact and exact closed-form solution for the PGF of PB-SCM, we demonstrate that each component in this closed-form solution uniquely encodes a specific local structure, enabling the identification of the local structures by testing their corresponding component appearances in the PGF. Building on this, we propose a practical algorithm for learning causal skeletons and identifying causal directions of PB-SCM using PGF. The effectiveness of our method is demonstrated through experiments on both synthetic and real datasets",
    "checked": true,
    "id": "e9645be3991fd7861f62770a8d309fc0ef89c714",
    "semantic_title": "on the identifiability of poisson branching structural causal model using probability generating function",
    "citation_count": 0,
    "authors": [
      "Yu Xiang",
      "Jie Qiao",
      "Zefeng Liang",
      "Zihuai Zeng",
      "Ruichu Cai",
      "Zhifeng Hao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/15b780350b302a1bf9a3bd273f5c15a4-Abstract-Conference.html": {
    "title": "Variational Flow Matching for Graph Generation",
    "volume": "main",
    "abstract": "We present a formulation of flow matching as variational inference, which we refer to as variational flow matching (VFM). We use this formulation to develop CatFlow, a flow matching method for categorical data that is easy to implement, computationally efficient, and achieves strong results on graph generation tasks. In VFM, the objective is to approximate the posterior probability path, which is a distribution over possible end points of a trajectory. VFM admits both the original flow matching objective and the CatFlow objective as special cases. We also relate VFM to score-based models, in which the dynamics are stochastic rather than deterministic, and derive a bound on the model likelihood based on a reweighted VFM objective. We evaluate CatFlow on one abstract graph generation task and two molecular generation tasks. In all cases, CatFlow exceeds or matches performance of the current state-of-the-art models",
    "checked": true,
    "id": "22006158219969f0b4db98df83bdc339320f5062",
    "semantic_title": "variational flow matching for graph generation",
    "citation_count": 22,
    "authors": [
      "Floor Eijkelboom",
      "Grigory Bartosh",
      "Christian Andersson Naesseth",
      "Max Welling",
      "Jan-Willem van de Meent"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/15ba84c1e19b0eb75f96922f5da0a021-Abstract-Conference.html": {
    "title": "A Gradient Accumulation Method for Dense Retriever under Memory Constraint",
    "volume": "main",
    "abstract": "InfoNCE loss is commonly used to train dense retriever in information retrieval tasks. It is well known that a large batch is essential to stable and effective training with InfoNCE loss, which requires significant hardware resources. Due to the dependency of large batch, dense retriever has bottleneck of application and research. Recently, memory reduction methods have been broadly adopted to resolve the hardware bottleneck by decomposing forward and backward or using a memory bank. However, current methods still suffer from slow and unstable train. To address these issues, we propose Contrastive Accumulation (ContAccum), a stable and efficient memory reduction method for dense retriever trains that uses a dual memory bank structure to leverage previously generated query and passage representations. Experiments on widely used five information retrieval datasets indicate that ContAccum can surpass not only existing memory reduction methods but also high-resource scenarios. Moreover, theoretical analysis and experimental results confirm that ContAccum provides more stable dual-encoder training than current memory bank utilization methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehee Kim",
      "Yukyung Lee",
      "Pilsung Kang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/15cc8e4a46565dab0c1a1220884bd503-Abstract-Conference.html": {
    "title": "Road Network Representation Learning with the Third Law of Geography",
    "volume": "main",
    "abstract": "Road network representation learning aims to learn compressed and effective vectorized representations for road segments that are applicable to numerous tasks. In this paper, we identify the limitations of existing methods, particularly their overemphasis on the distance effect as outlined in the First Law of Geography. In response, we propose to endow road network representation with the principles of the recent Third Law of Geography. To this end, we propose a novel graph contrastive learning framework that employs geographic configuration-aware graph augmentation and spectral negative sampling, ensuring that road segments with similar geographic configurations yield similar representations, and vice versa, aligning with the principles stated in the Third Law. The framework further fuses the Third Law with the First Law through a dual contrastive learning objective to effectively balance the implications of both laws. We evaluate our framework on two real-world datasets across three downstream tasks. The results show that the integration of the Third Law significantly improves the performance of road segment representations in downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haicang Zhou",
      "Weiming Huang",
      "Yile Chen",
      "Tiantian He",
      "Gao Cong",
      "Yew Soon Ong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/15e5fccdfeb10bb54f8e74944de1c8bf-Abstract-Conference.html": {
    "title": "HyperPrism: An Adaptive Non-linear Aggregation Framework for Distributed Machine Learning over Non-IID Data and Time-varying Communication Links",
    "volume": "main",
    "abstract": "While Distributed Machine Learning (DML) has been widely used to achieve decent performance, it is still challenging to take full advantage of data and devices distributed at multiple vantage points to adapt and learn, especially it is non-trivial to address dynamic and divergence challenges based on the linear aggregation framework as follows: (1) heterogeneous learning data at different devices (i.e., non-IID data) resulting in model divergence and (2) in the case of time-varying communication links, the limited ability for devices to reconcile model divergence. In this paper, we contribute a non-linear class aggregation framework HyperPrism that leverages distributed mirror descent with averaging done in the mirror descent dual space and adapts the degree of Weighted Power Mean (WPM) used in each round. Moreover, HyperPrism could adaptively choose different mapping for different layers of the local model with a dedicated hypernetwork per device, achieving automatic optimization of DML in high divergence settings. We perform rigorous analysis and experimental evaluations to demonstrate the effectiveness of adaptive, mirror-mapping DML. In particular, we extend the generalizability of existing related works and position them as special cases within HyperPrism. Our experimental results show that HyperPrism can improve the convergence speed up to 98.63% and scale well to more devices compared with the state-of-the-art, all with little additional computation overhead compared to traditional linear aggregation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haizhou Du",
      "Yijian Chen",
      "Ryan Yang",
      "Yuchen Li",
      "Linghe Kong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/15f4cefb0e143c7ad9d40e879b0a9d0c-Abstract-Conference.html": {
    "title": "A Versatile Diffusion Transformer with Mixture of Noise Levels for Audiovisual Generation",
    "volume": "main",
    "abstract": "Training diffusion models for audiovisual sequences allows for a range of generation tasks by learning conditional distributions of various input-output combinations of the two modalities. Nevertheless, this strategy often requires training a separate model for each task which is expensive. Here, we propose a novel training approach to effectively learn arbitrary conditional distributions in the audiovisual space. Our key contribution lies in how we parameterize the diffusion timestep in the forward diffusion process. Instead of the standard fixed diffusion timestep, we propose applying variable diffusion timesteps across the temporal dimension and across modalities of the inputs. This formulation offers flexibility to introduce variable noise levels for various portions of the input, hence the term mixture of noise levels. We propose a transformer-based audiovisual latent diffusion model and show that it can be trained in a task-agnostic fashion using our approach to enable a variety of audiovisual generation tasks at inference time. Experiments demonstrate the versatility of our method in tackling cross-modal and multimodal interpolation tasks in the audiovisual space. Notably, our proposed approach surpasses baselines in generating temporally and perceptually consistent samples conditioned on the input. Project page: neurips13025.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gwanghyun Kim",
      "Alonso Martinez",
      "Yu-Chuan Su",
      "Brendan Jou",
      "Jose Lezama",
      "Agrim Gupta",
      "Lijun Yu",
      "Lu Jiang",
      "Aren Jansen",
      "Jacob Walker",
      "Krishna Somandepalli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/15f80ec0fed53885d2ca6272edb96ede-Abstract-Conference.html": {
    "title": "GarmentLab: A Unified Simulation and Benchmark for Garment Manipulation",
    "volume": "main",
    "abstract": "Manipulating garments and fabrics has long been a critical endeavor in the development of home-assistant robots. However, due to complex dynamics and topological structures, garment manipulations pose significant challenges. Recent successes in reinforcement learning and vision-based methods offer promising avenues for learning garment manipulation. Nevertheless, these approaches are severely constrained by current benchmarks, which exhibit offer limited diversity of tasks and unrealistic simulation behavior. Therefore, we present GarmentLab, a content-rich benchmark and realistic simulation designed for deformable object and garment manipulation. Our benchmark encompasses a diverse range of garment types, robotic systems and manipulators. The abundant tasks in the benchmark further explores of the interactions between garments, deformable objects, rigid bodies, fluids, and human body. Moreover, by incorporating multiple simulation methods such as FEM and PBD, along with our proposed sim-to-real algorithms and real-world benchmark, we aim to significantly narrow the sim-to-real gap. We evaluate state-of-the-art vision methods, reinforcement learning, and imitation learning approaches on these tasks, highlighting the challenges faced by current algorithms, notably their limited generalization capabilities. Our proposed open-source environments and comprehensive analysis show promising boost to future research in garment manipulation by unlocking the full potential of these methods. We guarantee that we will open-source our code as soon as possible. You can watch the videos in supplementary files to learn more about the details of our work",
    "checked": true,
    "id": "b1c48a0383423c8090fec89e332554eb3a84fc34",
    "semantic_title": "garmentlab: a unified simulation and benchmark for garment manipulation",
    "citation_count": 8,
    "authors": [
      "Haoran Lu",
      "Ruihai Wu",
      "Yitong Li",
      "Sijie Li",
      "Ziyu Zhu",
      "Chuanruo Ning",
      "Yan Zhao",
      "Longzan Luo",
      "Yuanpei Chen",
      "Hao Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/16009ce3d8a6872d79f056c75618911d-Abstract-Conference.html": {
    "title": "Interpretable Generalized Additive Models for Datasets with Missing Values",
    "volume": "main",
    "abstract": "Many important datasets contain samples that are missing one or more feature values. Maintaining the interpretability of machine learning models in the presence of such missing data is challenging. Singly or multiply imputing missing values complicates the model's mapping from features to labels. On the other hand, reasoning on indicator variables that represent missingness introduces a potentially large number of additional terms, sacrificing sparsity. We solve these problems with M-GAM, a sparse, generalized, additive modeling approach that incorporates missingness indicators and their interaction terms while maintaining sparsity through $\\ell_0$ regularization. We show that M-GAM provides similar or superior accuracy to prior methods while significantly improving sparsity relative to either imputation or naïve inclusion of indicator variables",
    "checked": true,
    "id": "e7300d39b8eff54bd857083e6a95f6d6e32b18a0",
    "semantic_title": "interpretable generalized additive models for datasets with missing values",
    "citation_count": 4,
    "authors": [
      "Hayden McTavish",
      "Jon Donnelly",
      "Margo Seltzer",
      "Cynthia Rudin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/16336d94a5ffca8de019087ab7fe403f-Abstract-Conference.html": {
    "title": "Kangaroo: Lossless Self-Speculative Decoding for Accelerating LLMs via Double Early Exiting",
    "volume": "main",
    "abstract": "Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models (LLMs) while maintaining an identical sampling distribution. However, the conventional approach of training separate draft model to achieve a satisfactory token acceptance rate can be costly and impractical. In this paper, we propose a novel self-speculative decoding framework \\emph{Kangaroo} with \\emph{double} early exiting strategy, which leverages the shallow sub-network and the \\texttt{LM Head} of the well-trained target LLM to construct a self-drafting model. Then, the self-verification stage only requires computing the remaining layers over the \\emph{early-exited} hidden states in parallel. To bridge the representation gap between the sub-network and the full model, we train a lightweight and efficient adapter module on top of the sub-network. One significant challenge that comes with the proposed method is that the inference latency of the self-draft model may no longer be negligible compared to the big model. To boost the token acceptance rate while minimizing the latency of the self-drafting model, we introduce an additional \\emph{early exiting} mechanism for both single-sequence and the tree decoding scenarios. Specifically, we dynamically halt the small model's subsequent prediction during the drafting phase once the confidence level for the current step falls below a certain threshold. This approach reduces unnecessary computations and improves overall efficiency. Extensive experiments on multiple benchmarks demonstrate our effectiveness, where Kangaroo achieves walltime speedups up to 2.04$\\times$, outperforming Medusa-1 with 88.7\\% fewer additional parameters. The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangcheng Liu",
      "Yehui Tang",
      "Zhenhua Liu",
      "Yunsheng Ni",
      "Duyu Tang",
      "Kai Han",
      "Yunhe Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/163b048741e1deea2b3d9a46c2c88af3-Abstract-Conference.html": {
    "title": "Identifying Selections for Unsupervised Subtask Discovery",
    "volume": "main",
    "abstract": "When solving long-horizon tasks, it is intriguing to decompose the high-level task into subtasks. Decomposing experiences into reusable subtasks can improve data efficiency, accelerate policy generalization, and in general provide promising solutions to multi-task reinforcement learning and imitation learning problems. However, the concept of subtasks is not sufficiently understood and modeled yet, and existing works often overlook the true structure of the data generation process: subtasks are the results of a selection mechanism on actions, rather than possible underlying confounders or intermediates. Specifically, we provide a theory to identify, and experiments to verify the existence of selection variables in such data. These selections serve as subgoals that indicate subtasks and guide policy. In light of this idea, we develop a sequential non-negative matrix factorization (seq- NMF) method to learn these subgoals and extract meaningful behavior patterns as subtasks. Our empirical results on a challenging Kitchen environment demonstrate that the learned subtasks effectively enhance the generalization to new tasks in multi-task imitation learning scenarios. The codes are provided at this link",
    "checked": true,
    "id": "b296cb3127dc7fc0471a9ba20c8181b97ff51183",
    "semantic_title": "identifying selections for unsupervised subtask discovery",
    "citation_count": 0,
    "authors": [
      "Yiwen Qiu",
      "Yujia Zheng",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1646e34971facbcda3727d1dc28ab635-Abstract-Conference.html": {
    "title": "Bandits with Preference Feedback: A Stackelberg Game Perspective",
    "volume": "main",
    "abstract": "Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries. This model allows for incorporating human feedback into online inference and optimization and has been employed in systems for tuning large language models.The problem is fairly understood in toy settings with linear target functions or over finite small domains that limits practical interest.Taking the next step, we consider infinite domains and kernelized rewards. In this setting, selecting a pair of actions is quite challenging and requires balancing exploration and exploitation at two levels: within the pair, and along the iterations of the algorithm.We propose MaxMinLCB, which emulates this trade-off as a zero-sum Stackelberg game and chooses action pairs that are informative and have favorable reward values. MaxMinLCB consistently outperforms algorithms in the literature and satisfies an anytime-valid rate-optimal regret guarantee. This is owed to our novel preference-based confidence sequences for kernelized logistic estimators, which are of independent interest",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Barna Pásztor",
      "Parnian Kassraie",
      "Andreas Krause"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1651f1ea5ee6213e301a89f222f3fec7-Abstract-Conference.html": {
    "title": "DMesh: A Differentiable Mesh Representation",
    "volume": "main",
    "abstract": "We present a differentiable representation, DMesh, for general 3D triangular meshes. DMesh considers both the geometry and connectivity information of a mesh. In our design, we first get a set of convex tetrahedra that compactly tessellates the domain based on Weighted Delaunay Triangulation (WDT), and select triangular faces on the tetrahedra to define the final mesh. We formulate probability of faces to exist on the actual surface in a differentiable manner based on the WDT. This enables DMesh to represent meshes of various topology in a differentiable way, and allows us to reconstruct the mesh under various observations, such as point clouds and multi-view images using gradient-based optimization. We publicize the source code and supplementary material at our project page (https://sonsang.github.io/dmesh-project)",
    "checked": true,
    "id": "c0deba42b911583aac3ee73d79a4dea2112ef45d",
    "semantic_title": "dmesh: a differentiable mesh representation",
    "citation_count": 1,
    "authors": [
      "Sanghyun Son",
      "Matheus Gadelha",
      "Yang Zhou",
      "Zexiang Xu",
      "Ming Lin",
      "Yi Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/167bcf2af2cd08fcf75b932022db0311-Abstract-Conference.html": {
    "title": "MoE Jetpack: From Dense Checkpoints to Adaptive Mixture of Experts for Vision Tasks",
    "volume": "main",
    "abstract": "The sparsely activated mixture of experts (MoE) model presents an effective alternative to densely activated (dense) models, combining improved accuracy with computational efficiency. However, training MoE models from scratch requires extensive data and computational resources, a challenge that limits their widespread adoption. To address this, we introduce MoE Jetpack, a framework designed to fine-tune the abundant and easily accessible dense checkpoints into MoE models. MoE Jetpack incorporates two key techniques: (1) checkpoint recycling, which initializes MoE models with dense checkpoints to accelerate convergence and enhance accuracy, minimizing the need for extensive pre-training; (2) the hyperspherical adaptive MoE (SpheroMoE) layer, which optimizes the MoE architecture to enhance fine-tuning performance and efficiency.Experimental results indicate that MoE Jetpack doubles the convergence speed and enhances accuracy by 2.8% on ImageNet-1K. On smaller datasets, it achieves up to 8-fold faster convergence and over 30% accuracy gains, highlighting its efficiency.The code is available at https://github.com/Adlith/MoE-Jetpack",
    "checked": true,
    "id": "ff8a40349db17daaed78def5f192229c3c2e2527",
    "semantic_title": "moe jetpack: from dense checkpoints to adaptive mixture of experts for vision tasks",
    "citation_count": 8,
    "authors": [
      "Xingkui Zhu",
      "Yiran Guan",
      "Dingkang Liang",
      "Yuchao Chen",
      "Yuliang Liu",
      "Xiang Bai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/167cb476aab527e23741e314be80a4ea-Abstract-Conference.html": {
    "title": "Differentially Private Graph Diffusion with Applications in Personalized PageRanks",
    "volume": "main",
    "abstract": "Graph diffusion, which iteratively propagates real-valued substances among the graph, is used in numerous graph/network-involved applications. However, releasing diffusion vectors may reveal sensitive linking information in the data such as transaction information in financial network data. However, protecting the privacy of graph data is challenging due to its interconnected nature. This work proposes a novel graph diffusion framework with edge-level different privacy guarantees by using noisy diffusion iterates. The algorithm injects Laplace noise per diffusion iteration and adopts a degree-based thresholding function to mitigate the high sensitivity induced by low-degree nodes. Our privacy loss analysis is based on Privacy Amplification by Iteration (PABI), which to our best knowledge, is the first effort that analyzes PABI with Laplace noise and provides relevant applications. We also introduce a novel $\\infty$-Wasserstein distance tracking method, which tightens the analysis of privacy leakage and makes PABI more applicable in practice. We evaluate this framework by applying it to Personalized Pagerank computation for ranking tasks. Experiments on real-world network data demonstrate the superiority of our method under stringent privacy conditions",
    "checked": true,
    "id": "5c3bf506ec7adc09ea2e8701483b793668fb4668",
    "semantic_title": "differentially private graph diffusion with applications in personalized pageranks",
    "citation_count": 6,
    "authors": [
      "Rongzhe Wei",
      "Eli Chien",
      "Pan Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/16986b69068fbe6acf64eb6566519c74-Abstract-Conference.html": {
    "title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation",
    "volume": "main",
    "abstract": "Capitalizing on the complementary advantages of generative and discriminative models has always been a compelling vision in machine learning, backed by a growing body of research. This work discloses the hidden semantic structure within score-based generative models, unveiling their potential as effective discriminative priors. Inspired by our theoretical findings, we propose DUSA to exploit the structured semantic priors underlying diffusion score to facilitate the test-time adaptation of image classifiers or dense predictors. Notably, DUSA extracts knowledge from a single timestep of denoising diffusion, lifting the curse of Monte Carlo-based likelihood estimation over timesteps. We demonstrate the efficacy of our DUSA in adapting a wide variety of competitive pre-trained discriminative models on diverse test-time scenarios. Additionally, a thorough ablation study is conducted to dissect the pivotal elements in DUSA. Code is publicly available at https://github.com/BIT-DA/DUSA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjia Li",
      "Shuang Li",
      "Tongrui Su",
      "Longhui Yuan",
      "Jian Liang",
      "Wei Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/16bce4070c4e23434451b180348e3814-Abstract-Conference.html": {
    "title": "Action Imitation in Common Action Space for Customized Action Image Synthesis",
    "volume": "main",
    "abstract": "We propose a novel method, \\textbf{TwinAct}, to tackle the challenge of decoupling actions and actors in order to customize the text-guided diffusion models (TGDMs) for few-shot action image generation. TwinAct addresses the limitations of existing methods that struggle to decouple actions from other semantics (e.g., the actor's appearance) due to the lack of an effective inductive bias with few exemplar images. Our approach introduces a common action space, which is a textual embedding space focused solely on actions, enabling precise customization without actor-related details. Specifically, TwinAct involves three key steps: 1) Building common action space based on a set of representative action phrases; 2) Imitating the customized action within the action space; and 3) Generating highly adaptable customized action images in diverse contexts with action similarity loss. To comprehensively evaluate TwinAct, we construct a novel benchmark, which provides sample images with various forms of actions. Extensive experiments demonstrate TwinAct's superiority in generating accurate, context-independent customized actions while maintaining the identity consistency of different subjects, including animals, humans, and even customized actors",
    "checked": true,
    "id": "6a4c3dad2ae4442a9a461b5fcebe9c1e5f89c128",
    "semantic_title": "action imitation in common action space for customized action image synthesis",
    "citation_count": 12,
    "authors": [
      "wang lin",
      "Jingyuan Chen",
      "Jiaxin Shi",
      "Zirun Guo",
      "Yichen Zhu",
      "Zehan Wang",
      "Tao Jin",
      "Zhou Zhao",
      "Fei Wu",
      "Shuicheng Yan",
      "Hanwang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/16c5b4102a6b6eb061e502ce6736ad8a-Abstract-Conference.html": {
    "title": "PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless Imaging",
    "volume": "main",
    "abstract": "Lensless cameras offer significant advantages in size, weight, and cost compared to traditional lens-based systems. Without a focusing lens, lensless cameras rely on computational algorithms to recover the scenes from multiplexed measurements. However, current algorithms struggle with inaccurate forward imaging models and insufficient priors to reconstruct high-quality images. To overcome these limitations, we introduce a novel two-stage approach for consistent and photorealistic lensless image reconstruction. The first stage of our approach ensures data consistency by focusing on accurately reconstructing the low-frequency content with a spatially varying deconvolution method that adjusts to changes in the Point Spread Function (PSF) across the camera's field of view. The second stage enhances photorealism by incorporating a generative prior from pre-trained diffusion models. By conditioning on the low-frequency content retrieved in the first stage, the diffusion model effectively reconstructs the high-frequency details that are typically lost in the lensless imaging process, while also maintaining image fidelity. Our method achieves a superior balance between data fidelity and visual quality compared to existing methods, as demonstrated with two popular lensless systems, PhlatCam and DiffuserCam",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Cai",
      "Zhiyuan You",
      "Hailong Zhang",
      "Jinwei Gu",
      "Wentao Liu",
      "Tianfan Xue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/16c628ab12dc4caca8e7712affa6c767-Abstract-Conference.html": {
    "title": "The Importance of Online Data: Understanding Preference Fine-tuning via Coverage",
    "volume": "main",
    "abstract": "Learning from human preference data has emerged as the dominant paradigm for fine-tuning large language models (LLMs). The two most common families of techniques -- online reinforcement learning (RL) such as Proximal Policy Optimization (PPO) and offline contrastive methods such as Direct Preference Optimization (DPO) -- were positioned as equivalent in prior work due to the fact that both have to start from the same offline preference dataset. To further expand our theoretical understanding of the similarities and differences between online and offline techniques for preference fine-tuning, we conduct a rigorous analysis through the lens of dataset coverage, a concept that captures how the training data covers the test distribution and is widely used in RL. We prove that a global coverage condition is both necessary and sufficient for offline contrastive methods to converge to the optimal policy, but a weaker partial coverage condition suffices for online RL methods. This separation provides one explanation of why online RL methods can perform better than offline methods, especially when the offline preference data is not diverse enough. Finally, motivated by our preceding theoretical observations, we derive a hybrid preference optimization (HyPO) algorithm that uses offline data for contrastive-based preference optimization and online unlabeled data for KL regularization. Theoretically and empirically, we demonstrate that HyPO is more performant than its pure offline counterpart DPO, while still preserving its computation and memory efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuda Song",
      "Gokul Swamy",
      "Aarti Singh",
      "J. A. Bagnell",
      "Wen Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/16ce2c09a2ec6a162bee47f09659e366-Abstract-Conference.html": {
    "title": "LiveScene: Language Embedding Interactive Radiance Fields for Physical Scene Control and Rendering",
    "volume": "main",
    "abstract": "This paper scales object-level reconstruction to complex scenes, advancing interactive scene reconstruction. We introduce two datasets, OmniSim and InterReal, featuring 28 scenes with multiple interactive objects. To tackle the challenge of inaccurate interactive motion recovery in complex scenes, we propose LiveScene, a scene-level language-embedded interactive radiance field that efficiently reconstructs and controls multiple objects. By decomposing the interactive scene into local deformable fields, LiveScene enables separate reconstruction of individual object motions, reducing memory consumption. Additionally, our interaction-aware language embedding localizes individual interactive objects, allowing for arbitrary control using natural language. Our approach demonstrates significant superiority in novel view synthesis, interactive scene control, and language grounding performance through extensive experiments. Project page: https://livescenes.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Delin Qu",
      "Qizhi Chen",
      "Pingrui Zhang",
      "Xianqiang Gao",
      "Bin Zhao",
      "Zhigang Wang",
      "Dong Wang",
      "Xuelong Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/16e18fa3b3add076c30f2a2598f03031-Abstract-Conference.html": {
    "title": "What makes unlearning hard and what to do about it",
    "volume": "main",
    "abstract": "Machine unlearning is the problem of removing the effect of a subset of training data (the ``forget set'') from a trained model without damaging the model's utility e.g. to comply with users' requests to delete their data, or remove mislabeled, poisoned or otherwise problematic data.With unlearning research still being at its infancy, many fundamental open questions exist: Are there interpretable characteristics of forget sets that substantially affect the difficulty of the problem? How do these characteristics affect different state-of-the-art algorithms?With this paper, we present the first investigation aiming to answer these questions. We identify two key factors affecting unlearning difficulty and the performance of unlearning algorithms. Evaluation on forget sets that isolate these identified factors reveals previously-unknown behaviours of state-of-the-art algorithms that don't materialize on random forget sets.Based on our insights, we develop a framework coined Refined-Unlearning Meta-algorithm (RUM) that encompasses: (i) refining the forget set into homogenized subsets, according to different characteristics; and (ii) a meta-algorithm that employs existing algorithms to unlearn each subset and finally delivers a model that has unlearned the overall forget set. We find that RUM substantially improves top-performing unlearning algorithms. Overall, we view our work as an important step in (i) deepening our scientific understanding of unlearning and (ii) revealing new pathways to improving the state-of-the-art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "KAIRAN ZHAO",
      "Meghdad Kurmanji",
      "George-Octavian Bărbulescu",
      "Eleni Triantafillou",
      "Peter Triantafillou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/16f8a0852b31bc9dc791ecf313247a57-Abstract-Conference.html": {
    "title": "The Power of Resets in Online Reinforcement Learning",
    "volume": "main",
    "abstract": "Simulators are a pervasive tool in reinforcement learning, but most existing algorithms cannot efficiently exploit simulator access -- particularly in high-dimensional domains that require general function approximation. We explore the power of simulators through online reinforcement learning with local simulator access (or, local planning), an RL protocol where the agent is allowed to reset to previously observed states and follow their dynamics during training. We use local simulator access to unlock new statistical guarantees that were previously out of reach:- We show that MDPs with low coverability (Xie et al. 2023) -- a general structural condition that subsumes Block MDPs and Low-Rank MDPs -- can be learned in a sample-efficient fashion with only Q⋆-realizability (realizability of the optimal state-value function); existing online RL algorithms require significantly stronger representation conditions.- As a consequence, we show that the notorious Exogenous Block MDP problem (Efroni et al. 2022) is tractable under local simulator access.The results above are achieved through a computationally inefficient algorithm. We complement them with a more computationally efficient algorithm, RVFS (Recursive Value Function Search), which achieves provable sample complexity guarantees under a strengthened statistical assumption known as pushforward coverability. RVFS can be viewed as a principled, provable counterpart to a successful empirical paradigm that combines recursive search (e.g., MCTS) with value function approximation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zak Mhammedi",
      "Dylan J Foster",
      "Alexander Rakhlin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1700ad4e6252e8f2955909f96367b34d-Abstract-Conference.html": {
    "title": "Gaussian Approximation and Multiplier Bootstrap for Polyak-Ruppert Averaged Linear Stochastic Approximation with Applications to TD Learning",
    "volume": "main",
    "abstract": "In this paper, we obtain the Berry–Esseen bound for multivariate normal approximation for the Polyak-Ruppert averaged iterates of the linear stochastic approximation (LSA) algorithm with decreasing step size. Moreover, we prove the non-asymptotic validity of the confidence intervals for parameter estimation with LSA based on multiplier bootstrap. This procedure updates the LSA estimate together with a set of randomly perturbed LSA estimates upon the arrival of subsequent observations. We illustrate our findings in the setting of temporal difference learning with linear function approximation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergey Samsonov",
      "Eric Moulines",
      "Qi-Man Shao",
      "Zhuo-Song Zhang",
      "Alexey Naumov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1704ddd0bb89f159dfe609b32c889995-Abstract-Conference.html": {
    "title": "DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning",
    "volume": "main",
    "abstract": "Pre-trained vision language models (VLMs), though powerful, typically lack training on decision-centric data, rendering them sub-optimal for decision-making tasks such as in-the-wild device control through Graphical User Interfaces (GUIs) when used off-the-shelf. While training with static demonstrations has shown some promise, we show that such methods fall short when controlling real GUIs due to their failure to deal with real world stochasticity and dynamism not captured in static observational data. This paper introduces a novel autonomous RL approach, called DigiRL, for training in-the-wild device control agents through fine-tuning a pre-trained VLM in two stages: offline and offline-to-online RL. We first build a scalable and parallelizable Android learning environment equipped with a VLM-based general-purpose evaluator and then identify the key design choices for simple and effective RL in this domain. We demonstrate the effectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our 1.5B VLM trained with RL achieves a 49.5\\% absolute improvement -- from 17.7 to 67.2\\% success rate -- over supervised fine-tuning with static human demonstration data. It is worth noting that such improvement is achieved without any additional supervision or demonstration data. These results significantly surpass not only the prior best agents, including AppAgent with GPT-4V (8.3\\% success rate) and the 17B CogAgent trained with AitW data (14.4\\%), but also our implementation of prior best autonomous RL approach based on filtered behavior cloning (57.8\\%), thereby establishing a new state-of-the-art for digital agents for in-the-wild device control",
    "checked": true,
    "id": "c6e3e5f794233da2b27ea5dd4af19e62b95c863f",
    "semantic_title": "digirl: training in-the-wild device-control agents with autonomous reinforcement learning",
    "citation_count": 83,
    "authors": [
      "Hao Bai",
      "Yifei Zhou",
      "Jiayi Pan",
      "Mert Cemri",
      "Alane Suhr",
      "Sergey Levine",
      "Aviral Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1704fe7aaff33a54802b83a016050ab8-Abstract-Conference.html": {
    "title": "Invertible Consistency Distillation for Text-Guided Image Editing in Around 7 Steps",
    "volume": "main",
    "abstract": "Diffusion distillation represents a highly promising direction for achieving faithful text-to-image generation in a few sampling steps. However, despite recent successes, existing distilled models still do not provide the full spectrum of diffusion abilities, such as real image inversion, which enables many precise image manipulation methods. This work aims to enrich distilled text-to-image diffusion models with the ability to effectively encode real images into their latent space. To this end, we introduce invertible Consistency Distillation (iCD), a generalized consistency distillation framework that facilitates both high-quality image synthesis and accurate image encoding in only 3-4 inference steps. Though the inversion problem for text-to-image diffusion models gets exacerbated by high classifier-free guidance scales, we notice that dynamic guidance significantly reduces reconstruction errors without noticeable degradation in generation performance. As a result, we demonstrate that iCD equipped with dynamic guidance may serve as a highly effective tool for zero-shot text-guided image editing, competing with more expensive state-of-the-art alternatives",
    "checked": true,
    "id": "2eb1af02069cca7e23a029d8c4d3adaa67e7584b",
    "semantic_title": "invertible consistency distillation for text-guided image editing in around 7 steps",
    "citation_count": 8,
    "authors": [
      "Nikita Starodubcev",
      "Mikhail Khoroshikh",
      "Artem Babenko",
      "Dmitry Baranchuk"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/170dc3e41f2d03e327e04dbab0fccbfb-Abstract-Conference.html": {
    "title": "Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithms",
    "volume": "main",
    "abstract": "The sim-to-real gap, which represents the disparity between training and testing environments, poses a significant challenge in reinforcement learning (RL). A promising approach to addressing this challenge is distributionally robust RL, often framed as a robust Markov decision process (RMDP). In this framework, the objective is to find a robust policy that achieves good performance under the worst-case scenario among all environments within a pre-specified uncertainty set centered around the training environment. Unlike previous work, which relies on a generative model or a pre-collected offline dataset enjoying good coverage of the deployment environment, we tackle robust RL via interactive data collection, where the learner interacts with the training environment only and refines the policy through trial and error. In this robust RL paradigm, two main challenges emerge: managing distributional robustness while striking a balance between exploration and exploitation during data collection. Initially, we establish that sample-efficient learning without additional assumptions is unattainable owing to the curse of support shift; i.e., the potential disjointedness of the distributional supports between the training and testing environments. To circumvent such a hardness result, we introduce the vanishing minimal value assumption to RMDPs with a total-variation (TV) distance robust set, postulating that the minimal value of the optimal robust value function is zero. We prove that such an assumption effectively eliminates the support shift issue for RMDPs with a TV distance robust set, and present an algorithm with a provable sample complexity guarantee. Our work makes the initial step to uncovering the inherent difficulty of robust RL via interactive data collection and sufficient conditions for designing a sample-efficient algorithm accompanied by sharp sample complexity analysis",
    "checked": false,
    "id": "7ff4d0cb301cf9696326aeed3c33657d331b1c83",
    "semantic_title": "distributionally robust reinforcement learning with interactive data collection: fundamental hardness and near-optimal algorithm",
    "citation_count": 12,
    "authors": [
      "Miao Lu",
      "Han Zhong",
      "Tong Zhang",
      "Jose Blanchet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/171291d8fed723c6dfc76330aa827ff8-Abstract-Conference.html": {
    "title": "Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference",
    "volume": "main",
    "abstract": "As Large Language Models (LLMs) demonstrate extensive capability in learning from documents, LLM unlearning becomes an increasingly important research area to address concerns of LLMs in terms of privacy, copyright, etc. A conventional LLM unlearning task typically involves two goals: (1) The target LLM should forget the knowledge in the specified forget documents; and (2) it should retain the other knowledge that the LLM possesses, for which we assume access to a small number of retain documents. To achieve both goals, a mainstream class of LLM unlearning methods introduces an optimization framework with a combination of two objectives – maximizing the prediction loss on the forget documents while minimizing that on the retain documents, which suffers from two challenges, degenerated output and catastrophic forgetting. In this paper, we propose a novel unlearning framework called Unlearning from Logit Difference (ULD), which introduces an assistant LLM that aims to achieve the opposite of the unlearning goals: remembering the forget documents and forgetting the retain knowledge. ULD then derives the unlearned LLM by computing the logit difference between the target and the assistant LLMs. We show that such reversed objectives would naturally resolve both aforementioned challenges while significantly improving the training efficiency. Extensive experiments demonstrate that our method efficiently achieves the intended forgetting while preserving the LLM's overall capabilities, reducing training time by more than threefold. Notably, our method loses 0% of model utility on the ToFU benchmark, whereas baseline methods may sacrifice 17% of utility on average to achieve comparable forget quality",
    "checked": true,
    "id": "593b8663c22300d91bca8d99f9f08f2fd4c527ed",
    "semantic_title": "reversing the forget-retain objectives: an efficient llm unlearning framework from logit difference",
    "citation_count": 41,
    "authors": [
      "Jiabao Ji",
      "Yujian Liu",
      "Yang Zhang",
      "Gaowen Liu",
      "Ramana Kompella",
      "Sijia Liu",
      "Shiyu Chang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1716d022edeac750e57a2986a7135e13-Abstract-Conference.html": {
    "title": "Scaling the Codebook Size of VQ-GAN to 100,000 with a Utilization Rate of 99%",
    "volume": "main",
    "abstract": "In the realm of image quantization exemplified by VQGAN, the process encodes images into discrete tokens drawn from a codebook with a predefined size. Recent advancements, particularly with LLAMA 3, reveal that enlarging the codebook significantly enhances model performance. However, VQGAN and its derivatives, such as VQGAN-FC (Factorized Codes) and VQGAN-EMA, continue to grapple with challenges related to expanding the codebook size and enhancing codebook utilization. For instance, VQGAN-FC is restricted to learning a codebook with a maximum size of 16,384, maintaining a typically low utilization rate of less than 12% on ImageNet. In this work, we propose a novel image quantization model named VQGAN-LC (Large Codebook), which extends the codebook size to 100,000, achieving an utilization rate exceeding 99%. Unlike previous methods that optimize each codebook entry, our approach begins with a codebook initialized with 100,000 features extracted by a pre-trained vision encoder. Optimization then focuses on training a projector that aligns the entire codebook with the feature distributions of the encoder in VQGAN-LC. We demonstrate the superior performance of our model over its counterparts across a variety of tasks, including image reconstruction, image classification, auto-regressive image generation using GPT, and image creation with diffusion- and flow-based generative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Zhu",
      "Fangyun Wei",
      "Yanye Lu",
      "Dong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/172be8b0b88fc2b4aee74237d43f8c04-Abstract-Conference.html": {
    "title": "Representation Noising: A Defence Mechanism Against Harmful Finetuning",
    "volume": "main",
    "abstract": "Releasing open-source large language models (LLMs) presents a dual-use risk since bad actors can easily fine-tune these models for harmful purposes. Even without the open release of weights, weight stealing and fine-tuning APIs make closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety measures like preventing jailbreaks and improving safety guardrails are important, such measures can easily be reversed through fine-tuning. In this work, we propose Representation Noising (\\textsf{\\small RepNoise}), a defence mechanism that operates even when attackers have access to the weights. \\textsf{\\small RepNoise} works by removing information about harmful representations such that it is difficult to recover them during fine-tuning. Importantly, our defence is also able to generalize across different subsets of harm that have not been seen during the defence process as long as they are drawn from the same distribution of the attack set. Our method does not degrade the general capability of LLMs and retains the ability to train the model on harmless tasks. We provide empirical evidence that the efficacy of our defence lies in its ``depth'': the degree to which information about harmful representations is removed across {\\em all layers} of the LLM. We also find areas where \\textsf{\\small RepNoise} still remains ineffective and highlight how those limitations can inform future research",
    "checked": true,
    "id": "f1de0ef4befcafe5736f9a0255a546e43af17889",
    "semantic_title": "representation noising: a defence mechanism against harmful finetuning",
    "citation_count": 39,
    "authors": [
      "Domenic Rosati",
      "Jan Wehner",
      "Kai Williams",
      "Lukasz Bartoszcze",
      "Robie Gonzales",
      "carsten maple",
      "Subhabrata Majumdar",
      "Hassan Sajjad",
      "Frank Rudzicz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/173e4732a89fab9fb225203f35996677-Abstract-Conference.html": {
    "title": "Towards Calibrated Robust Fine-Tuning of Vision-Language Models",
    "volume": "main",
    "abstract": "Improving out-of-distribution (OOD) generalization during in-distribution (ID) adaptation is a primary goal of robust fine-tuning of zero-shot models beyond naive fine-tuning. However, despite decent OOD generalization performance from recent robust fine-tuning methods, confidence calibration for reliable model output has not been fully addressed. This work proposes a robust fine-tuning method that improves both OOD accuracy and confidence calibration simultaneously in vision language models. Firstly, we show that both OOD classification and OOD calibration errors have a shared upper bound consisting of two terms of ID data: 1) ID calibration error and 2) the smallest singular value of the ID input covariance matrix. Based on this insight, we design a novel framework that conducts fine-tuning with a constrained multimodal contrastive loss enforcing a larger smallest singular value, which is further guided by the self-distillation of a moving-averaged model to achieve calibrated prediction as well. Starting from empirical evidence supporting our theoretical statements, we provide extensive experimental results on ImageNet distribution shift benchmarks that demonstrate the effectiveness of our theorem and its practical implementation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changdae Oh",
      "Hyesu Lim",
      "Mijoo Kim",
      "Dongyoon Han",
      "Sangdoo Yun",
      "Jaegul Choo",
      "Alexander Hauptmann",
      "Zhi-Qi Cheng",
      "Kyungwoo Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1741917e3df34daa1a4c564e2980bb59-Abstract-Conference.html": {
    "title": "Towards Safe Concept Transfer of Multi-Modal Diffusion via Causal Representation Editing",
    "volume": "main",
    "abstract": "Recent advancements in vision-language-to-image (VL2I) diffusion generation have made significant progress. While generating images from broad vision-language inputs holds promise, it also raises concerns about potential misuse, such as copying artistic styles without permission, which could have legal and social consequences. Therefore, it's crucial to establish governance frameworks to ensure ethical and copyright integrity, especially with widely used diffusion models. To address these issues, researchers have explored various approaches, such as dataset filtering, adversarial perturbations, machine unlearning, and inference-time refusals. However, these methods often lack either scalability or effectiveness. In response, we propose a new framework called causal representation editing (CRE), which extends representation editing from large language models (LLMs) to diffusion-based models. CRE enhances the efficiency and flexibility of safe content generation by intervening at diffusion timesteps causally linked to unsafe concepts. This allows for precise removal of harmful content while preserving acceptable content quality, demonstrating superior effectiveness, precision and scalability compared to existing methods. CRE can handle complex scenarios, including incomplete or blurred representations of unsafe concepts, offering a promising solution to challenges in managing harmful content generation in diffusion-based models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiran Dong",
      "Bingjie WANG",
      "Song Guo",
      "Junxiao Wang",
      "Jie ZHANG",
      "Zicong Hong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/175fa4fc8f275f877ec85340131c5d7a-Abstract-Conference.html": {
    "title": "Variational Distillation of Diffusion Policies into Mixture of Experts",
    "volume": "main",
    "abstract": "This work introduces Variational Diffusion Distillation (VDD), a novel method that distills denoising diffusion policies into Mixtures of Experts (MoE) through variational inference. Diffusion Models are the current state-of-the-art in generative modeling due to their exceptional ability to accurately learn and represent complex, multi-modal distributions. This ability allows Diffusion Models to replicate the inherent diversity in human behavior, making them the preferred models in behavior learning such as Learning from Human Demonstrations (LfD).However, diffusion models come with some drawbacks, including the intractability of likelihoods and long inference times due to their iterative sampling process. The inference times, in particular, pose a significant challenge to real-time applications such as robot control.In contrast, MoEs effectively address the aforementioned issues while retaining the ability to represent complex distributions but are notoriously difficult to train.VDD is the first method that distills pre-trained diffusion models into MoE models, and hence, combines the expressiveness of Diffusion Models with the benefits of Mixture Models.Specifically, VDD leverages a decompositional upper bound of the variational objective that allows the training of each expert separately, resulting in a robust optimization scheme for MoEs.VDD demonstrates across nine complex behavior learning tasks, that it is able to: i) accurately distill complex distributions learned by the diffusion model, ii) outperform existing state-of-the-art distillation methods, and iii) surpass conventional methods for training MoE. The code and videos are available at https://intuitive-robots.github.io/vdd-website",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyi Zhou",
      "Denis Blessing",
      "Ge Li",
      "Onur Celik",
      "Xiaogang Jia",
      "Gerhard Neumann",
      "Rudolf Lioutikov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/17790fc9b4828206f99a0dea35da3657-Abstract-Conference.html": {
    "title": "Handling Learnwares from Heterogeneous Feature Spaces with Explicit Label Exploitation",
    "volume": "main",
    "abstract": "The learnware paradigm aims to help users leverage numerous existing high-performing models instead of starting from scratch, where a learnware consists of a well-trained model and the specification describing its capability. Numerous learnwares are accommodated by a learnware dock system. When users solve tasks with the system, models that fully match the task feature space are often rare or even unavailable. However, models with heterogeneous feature space can still be helpful. This paper finds that label information, particularly model outputs, is helpful yet previously less exploited in the accommodation of heterogeneous learnwares. We extend the specification to better leverage model pseudo-labels and subsequently enrich the unified embedding space for better specification evolvement. With label information, the learnware identification can also be improved by additionally comparing conditional distributions. Experiments demonstrate that, even without a model explicitly tailored to user tasks, the system can effectively handle tasks by leveraging models from diverse feature spaces",
    "checked": true,
    "id": "a01797bfc0b4ab7de3465ea06ecae11b1b1fb68f",
    "semantic_title": "handling learnwares from heterogeneous feature spaces with explicit label exploitation",
    "citation_count": 2,
    "authors": [
      "Peng Tan",
      "Hai-Tian Liu",
      "Zhi-Hao Tan",
      "Zhi-Hua Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/178022c409938a9d634b88ce924c4b14-Abstract-Conference.html": {
    "title": "ROIDICE: Offline Return on Investment Maximization for Efficient Decision Making",
    "volume": "main",
    "abstract": "In this paper, we propose a novel policy optimization framework that maximizes Return on Investment (ROI) of a policy using a fixed dataset within a Markov Decision Process (MDP) equipped with a cost function. ROI, defined as the ratio between the return and the accumulated cost of a policy, serves as a measure of efficiency of the policy. Despite the importance of maximizing ROI in various applications, it remains a challenging problem due to its nature as a ratio of two long-term values: return and accumulated cost. To address this, we formulate the ROI maximizing reinforcement learning problem as a linear fractional programming. We then incorporate the stationary distribution correction (DICE) framework to develop a practical offline ROI maximization algorithm.Our proposed algorithm, ROIDICE, yields an efficient policy that offers a superior trade-off between return and accumulated cost compared to policies trained using existing frameworks",
    "checked": true,
    "id": "195faff1066afc099c1da5d84d817b093de9f608",
    "semantic_title": "roidice: offline return on investment maximization for efficient decision making",
    "citation_count": 1,
    "authors": [
      "Woosung Kim",
      "Hayeong Lee",
      "Jongmin Lee",
      "Byung-Jun Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1780db9272f81961459f4906036ebc6f-Abstract-Conference.html": {
    "title": "Generalization Bound and Learning Methods for Data-Driven Projections in Linear Programming",
    "volume": "main",
    "abstract": "How to solve high-dimensional linear programs (LPs) efficiently is a fundamental question.Recently, there has been a surge of interest in reducing LP sizes using *random projections*, which can accelerate solving LPs independently of improving LP solvers. This paper explores a new direction of *data-driven projections*, which use projection matrices learned from data instead of random projection matrices.Given training data of $n$-dimensional LPs, we learn an $n\\times k$ projection matrix with $n > k$. When addressing a future LP instance, we reduce its dimensionality from $n$ to $k$ via the learned projection matrix, solve the resulting LP to obtain a $k$-dimensional solution, and apply the learned matrix to it to recover an $n$-dimensional solution.On the theoretical side, a natural question is: how much data is sufficient to ensure the quality of recovered solutions? We address this question based on the framework of *data-driven algorithm design*, which connects the amount of data sufficient for establishing generalization bounds to the *pseudo-dimension* of performance metrics. We obtain an $\\tilde{\\mathrm{O}}(nk^2)$ upper bound on the pseudo-dimension, where $\\tilde{\\mathrm{O}}$ compresses logarithmic factors. We also provide an $\\Omega(nk)$ lower bound, implying our result is tight up to an $\\tilde{\\mathrm{O}}(k)$ factor. On the practical side, we explore two simple methods for learning projection matrices: PCA- and gradient-based methods. While the former is relatively efficient, the latter can sometimes achieve better solution quality. Experiments demonstrate that learning projection matrices from data is indeed beneficial: it leads to significantly higher solution quality than the existing random projection while greatly reducing the time for solving LPs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shinsaku Sakaue",
      "Taihei Oki"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1787533e171dcc8549cc2eb5a4840eec-Abstract-Conference.html": {
    "title": "CV-VAE: A Compatible Video VAE for Latent Generative Video Models",
    "volume": "main",
    "abstract": "Spatio-temporal compression of videos, utilizing networks such as Variational Autoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other video generative models. For instance, many LLM-like video models learn the distribution of discrete tokens derived from 3D VAEs within the VQVAE framework, while most diffusion-based video models capture the distribution of continuous latent extracted by 2D VAEs without quantization. The temporal compression is simply realized by uniform frame sampling which results in unsmooth motion between consecutive frames. Currently, there lacks of a commonly used continuous video (3D) VAE for latent diffusion-based video models in the research community. Moreover, since current diffusion-based approaches are often implemented using pre-trained text-to-image (T2I) models, directly training a video VAE without considering the compatibility with existing T2I models will result in a latent space gap between them, which will take huge computational resources for training to bridge the gap even with the T2I models as initialization. To address this issue, we propose a method for training a video VAE of latent video models, namely CV-VAE, whose latent space is compatible with that of a given image VAE, e.g., image VAE of Stable Diffusion (SD). The compatibility is achieved by the proposed novel latent space regularization, which involves formulating a regularization loss using the image VAE. Benefiting from the latent space compatibility, video models can be trained seamlessly from pre-trained T2I or video models in a truly spatio-temporally compressed latent space, rather than simply sampling video frames at equal intervals. To improve the training efficiency, we also design a novel architecture for the video VAE. With our CV-VAE, existing video models can generate four times more frames with minimal finetuning. Extensive experiments are conducted to demonstrate the effectiveness of the proposed video VAE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijie Zhao",
      "Yong Zhang",
      "Xiaodong Cun",
      "Shaoshu Yang",
      "Muyao Niu",
      "Xiaoyu Li",
      "Wenbo HU",
      "Ying Shan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/178ae4ba29022eb7bf509c2e27bc8ab8-Abstract-Conference.html": {
    "title": "Historical Test-time Prompt Tuning for Vision Foundation Models",
    "volume": "main",
    "abstract": "Test-time prompt tuning, which learns prompts online with unlabelled test samples during the inference stage, has demonstrated great potential by learning effective prompts on-the-fly without requiring any task-specific annotations. However, its performance often degrades clearly along the tuning process when the prompts are continuously updated with the test data flow, and the degradation becomes more severe when the domain of test samples changes continuously. We propose HisTPT, a Historical Test-time Prompt Tuning technique that memorizes the useful knowledge of the learnt test samples and enables robust test-time prompt tuning with the memorized knowledge. HisTPT introduces three types of knowledge banks, namely, local knowledge bank, hard-sample knowledge bank, and global knowledge bank, each of which works with different mechanisms for effective knowledge memorization and test-time prompt optimization. In addition, HisTPT features an adaptive knowledge retrieval mechanism that regularizes the prediction of each test sample by adaptively retrieving the memorized knowledge. Extensive experiments show that HisTPT achieves superior prompt tuning performance consistently while handling different visual recognition tasks (e.g., image classification, semantic segmentation, and object detection) and test samples from continuously changing domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Zhang",
      "Jiaxing Huang",
      "Xiaoqin Zhang",
      "Ling Shao",
      "Shijian Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/178b306c7ee66a66db2171646e17da36-Abstract-Conference.html": {
    "title": "Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation",
    "volume": "main",
    "abstract": "Stochastic Gradient Descent (SGD) with adaptive steps is widely used to train deep neural networks and generative models. Most theoretical results assume that it is possible to obtain unbiased gradient estimators, which is not the case in several recent deep learning and reinforcement learning applications that use Monte Carlo methods.This paper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps for non-convex smooth functions. Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias of the gradient estimator. In particular, we establish that Adagrad, RMSProp, and AMSGRAD, an exponential moving average variant of Adam, with biased gradients, converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case. Finally, we provide experimental results using Variational Autoenconders (VAE) and applications to several learning frameworks that illustrate our convergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning",
    "checked": true,
    "id": "25040ff3d5157170ea12285cc4c3c30a69100ca7",
    "semantic_title": "non-asymptotic analysis of biased adaptive stochastic approximation",
    "citation_count": 2,
    "authors": [
      "Sobihan Surendran",
      "Adeline Fermanian",
      "Antoine Godichon-Baggioni",
      "Sylvain Le Corff"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/178b6cd141e003fa5ff808c7d7d8e2cc-Abstract-Conference.html": {
    "title": "Regret Minimization in Stackelberg Games with Side Information",
    "volume": "main",
    "abstract": "Algorithms for playing in Stackelberg games have been deployed in real-world domains including airport security, anti-poaching efforts, and cyber-crime prevention. However, these algorithms often fail to take into consideration the additional information available to each player (e.g. traffic patterns, weather conditions, network congestion), a salient feature of reality which may significantly affect both players' optimal strategies. We formalize such settings as Stackelberg games with side information, in which both players observe an external context before playing. The leader commits to a (context-dependent) strategy, and the follower best-responds to both the leader's strategy and the context. We focus on the online setting in which a sequence of followers arrive over time, and the context may change from round-to-round. In sharp contrast to the non-contextual version, we show that it is impossible for the leader to achieve good performance (measured by regret) in the full adversarial setting. Motivated by our impossibility result, we show that no-regret learning is possible in two natural relaxations: the setting in which the sequence of followers is chosen stochastically and the sequence of contexts is adversarial, and the setting in which the sequence of contexts is stochastic and the sequence of followers is chosen by an adversary",
    "checked": true,
    "id": "ccc6a681147f047f9afc08f552b12349009fa461",
    "semantic_title": "regret minimization in stackelberg games with side information",
    "citation_count": 4,
    "authors": [
      "Keegan Harris",
      "Steven Z. Wu",
      "Maria-Florina F Balcan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/178de34a090423c359a0ca2d51a25706-Abstract-Conference.html": {
    "title": "Navigating the Effect of Parametrization for Dimensionality Reduction",
    "volume": "main",
    "abstract": "Parametric dimensionality reduction methods have gained prominence for their ability to generalize to unseen datasets, an advantage that traditional non-parametric approaches typically lack. Despite their growing popularity, there remains a prevalent misconception among practitioners about the equivalence in performance between parametric and non-parametric methods. Here, we show that these methods are not equivalent -- parametric methods retain global structure but lose significant local details. To explain this, we provide evidence that parameterized approaches lack the ability to repulse negative samples, and the choice of loss function also has an impact.Addressing these issues, we developed a new parametric method, ParamRepulsor, that incorporates Hard Negative Mining and a loss function that applies a strong repulsive force. This new method achieves state-of-the-art performance on local structure preservation for parametric methods without sacrificing the fidelity of global structural representation. Our code is available at https://github.com/hyhuang00/ParamRepulsor",
    "checked": true,
    "id": "a3aa5c408928d95b0330c387b823b4ee6ce243c4",
    "semantic_title": "navigating the effect of parametrization for dimensionality reduction",
    "citation_count": 1,
    "authors": [
      "Haiyang Huang",
      "Yingfan Wang",
      "Cynthia Rudin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/179a2b276b954a1369ec9119c4badabb-Abstract-Conference.html": {
    "title": "Peri-midFormer: Periodic Pyramid Transformer for Time Series Analysis",
    "volume": "main",
    "abstract": "Time series analysis finds wide applications in fields such as weather forecasting, anomaly detection, and behavior recognition. Previous methods attempted to model temporal variations directly using 1D time series. However, this has been quite challenging due to the discrete nature of data points in time series and the complexity of periodic variation. In terms of periodicity, taking weather and traffic data as an example, there are multi-periodic variations such as yearly, monthly, weekly, and daily, etc. In order to break through the limitations of the previous methods, we decouple the implied complex periodic variations into inclusion and overlap relationships among different level periodic components based on the observation of the multi-periodicity therein and its inclusion relationships. This explicitly represents the naturally occurring pyramid-like properties in time series, where the top level is the original time series and lower levels consist of periodic components with gradually shorter periods, which we call the periodic pyramid. To further extract complex temporal variations, we introduce self-attention mechanism into the periodic pyramid, capturing complex periodic relationships by computing attention between periodic components based on their inclusion, overlap, and adjacency relationships. Our proposed Peri-midFormer demonstrates outstanding performance in five mainstream time series analysis tasks, including short- and long-term forecasting, imputation, classification, and anomaly detection",
    "checked": true,
    "id": "3303ceb65f24b62fe8ea3c73270e89d364a8f020",
    "semantic_title": "peri-midformer: periodic pyramid transformer for time series analysis",
    "citation_count": 2,
    "authors": [
      "Qiang Wu",
      "Gechang Yao",
      "Zhixi Feng",
      "Yang Shuyuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/179f5dcdeedc149443ebd3ba70811dbd-Abstract-Conference.html": {
    "title": "QKFormer: Hierarchical Spiking Transformer using Q-K Attention",
    "volume": "main",
    "abstract": "Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for low energy consumption and high performance. However, there remains a substantial gap in performance between SNNs and Artificial Neural Networks (ANNs). To narrow this gap, we have developed QKFormer, a direct training spiking transformer with the following features: i) Linear complexity and high energy efficiency, the novel spike-form Q-K attention module efficiently models the token or channel attention through binary vectors and enables the construction of larger models. ii) Multi-scale spiking representation, achieved by a hierarchical structure with the different numbers of tokens across blocks. iii) Spiking Patch Embedding with Deformed Shortcut (SPEDS), enhances spiking information transmission and integration, thus improving overall performance. It is shown that QKFormer achieves significantly superior performance over existing state-of-the-art SNN models on various mainstream datasets. Notably, with comparable size to Spikformer (66.34 M, 74.81\\%), QKFormer (64.96 M) achieves a groundbreaking top-1 accuracy of 85.65\\% on ImageNet-1k, substantially outperforming Spikformer by 10.84\\%. To our best knowledge, this is the first time that directly training SNNs have exceeded 85\\% accuracy on ImageNet-1K",
    "checked": true,
    "id": "a116109c6cc53dc204592d8243d25ef25fddb891",
    "semantic_title": "qkformer: hierarchical spiking transformer using q-k attention",
    "citation_count": 22,
    "authors": [
      "chenlin zhou",
      "Han Zhang",
      "Zhaokun Zhou",
      "Liutao Yu",
      "Liwei Huang",
      "Xiaopeng Fan",
      "Li Yuan",
      "Zhengyu Ma",
      "Huihui Zhou",
      "Yonghong Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/17a1a1439421f1837e10cd612bf92861-Abstract-Conference.html": {
    "title": "DapperFL: Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices",
    "volume": "main",
    "abstract": "Federated learning (FL) has emerged as a prominent machine learning paradigm in edge computing environments, enabling edge devices to collaboratively optimize a global model without sharing their private data. However, existing FL frameworks suffer from efficacy deterioration due to the system heterogeneity inherent in edge computing, especially in the presence of domain shifts across local data. In this paper, we propose a heterogeneous FL framework DapperFL, to enhance model performance across multiple domains. In DapperFL, we introduce a dedicated Model Fusion Pruning (MFP) module to produce personalized compact local models for clients to address the system heterogeneity challenges. The MFP module prunes local models with fused knowledge obtained from both local and remaining domains, ensuring robustness to domain shifts. Additionally, we design a Domain Adaptive Regularization (DAR) module to further improve the overall performance of DapperFL. The DAR module employs regularization generated by the pruned model, aiming to learn robust representations across domains. Furthermore, we introduce a specific aggregation algorithm for aggregating heterogeneous local models with tailored architectures and weights. We implement DapperFL on a real-world FL platform with heterogeneous clients. Experimental results on benchmark datasets with multiple domains demonstrate that DapperFL outperforms several state-of-the-art FL frameworks by up to 2.28%, while significantly achieving model volume reductions ranging from 20% to 80%. Our code is available at: https://github.com/jyzgh/DapperFL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongzhe Jia",
      "Xuyun Zhang",
      "Hongsheng Hu",
      "Kim-Kwang Raymond Choo",
      "Lianyong Qi",
      "Xiaolong Xu",
      "Amin Beheshti",
      "Wanchun Dou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/17af43527227c5c96db0f8d4c6aadc4e-Abstract-Conference.html": {
    "title": "Policy-shaped prediction: avoiding distractions in model-based reinforcement learning",
    "volume": "main",
    "abstract": "Model-based reinforcement learning (MBRL) is a promising route to sample-efficient policy optimization. However, a known vulnerability of reconstruction-based MBRL consists of scenarios in which detailed aspects of the world are highly predictable, but irrelevant to learning a good policy. Such scenarios can lead the model to exhaust its capacity on meaningless content, at the cost of neglecting important environment dynamics. While existing approaches attempt to solve this problem, we highlight its continuing impact on leading MBRL methods ---including DreamerV3 and DreamerPro--- with a novel environment where background distractions are intricate, predictable, and useless for planning future actions. To address this challenge we develop a method for focusing the capacity of the world model through a synergy of a pretrained segmentation model, a task-aware reconstruction loss, and adversarial learning. Our method outperforms a variety of other approaches designed to reduce the impact of distractors, and is an advance towards robust model-based reinforcement learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miles Hutson",
      "Isaac Kauvar",
      "Nick Haber"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/17b08a9de93e2accf13429643e7eafdc-Abstract-Conference.html": {
    "title": "Fundamental Convergence Analysis of Sharpness-Aware Minimization",
    "volume": "main",
    "abstract": "The paper investigates the fundamental convergence properties of Sharpness-Aware Minimization (SAM), a recently proposed gradient-based optimization method (Foret et al., 2021) that significantly improves the generalization of deep neural networks. The convergence properties including the stationarity of accumulation points, the convergence of the sequence of gradients to the origin, the sequence of function values to the optimal value, and the sequence of iterates to the optimal solution are established for the method. The universality of the provided convergence analysis based on inexact gradient descent frameworks (Khanh et al., 2023b) allows its extensions to the normalized versions of SAM such as F-SAM (Li et al. 2024), VaSSO (Li & Giannakis, 2023), RSAM (Liu et al., 2022), and to the unnormalized versions of SAM such as USAM (Andriushchenko & Flammarion, 2022). Numerical experiments are conducted on classification tasks using deep learning models to confirm the practical aspects of our analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pham Khanh",
      "Hoang-Chau Luong",
      "Boris Mordukhovich",
      "Dat Tran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/17bae482066287ce54814514cf12f248-Abstract-Conference.html": {
    "title": "Honor Among Bandits: No-Regret Learning for Online Fair Division",
    "volume": "main",
    "abstract": "We consider the problem of online fair division of indivisible goods to players when there are a finite number of types of goods and player values are drawn from distributions with unknown means. Our goal is to maximize social welfare subject to allocating the goods fairly in expectation. When a player's value for an item is unknown at the time of allocation, we show that this problem reduces to a variant of (stochastic) multi-armed bandits, where there exists an arm for each player's value for each type of good. At each time step, we choose a distribution over arms which determines how the next item is allocated. We consider two sets of fairness constraints for this problem: envy-freeness in expectation and proportionality in expectation. Our main result is the design of an explore-then-commit algorithm that achieves $\\tilde{O}(T^{2/3})$ regret while maintaining either fairness constraint. This result relies on unique properties fundamental to fair-division constraints that allow faster rates of learning, despite the restricted action space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ariel D Procaccia",
      "Ben Schiffer",
      "Shirley Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/17d60fef592086d1a5cb136f1946df59-Abstract-Conference.html": {
    "title": "Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks",
    "volume": "main",
    "abstract": "Large language models can solve tasks that were not present in the training set. This capability is believed to be due to in-context learning and skill composition. In this work, we study the emergence of in-context learning and skill composition in a collection of modular arithmetic tasks. Specifically, we consider a finite collection of linear modular functions $z = a x + b y \\text{ mod } p$ labeled by the vector $(a, b) \\in \\mathbb{Z}_p^2$. We use some of these tasks for pre-training and the rest for out-of-distribution testing. We empirically show that a GPT-style transformer exhibits a transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases. We find that the smallest model capable of out-of-distribution generalization requires two transformer blocks, while for deeper models, the out-of-distribution generalization phase is *transient*, necessitating early stopping. Finally, we perform an interpretability study of the pre-trained models, revealing highly structured representations in both attention heads and MLPs; and discuss the learned algorithms. Notably, we find an algorithmic shift in deeper models, as we go from few to many in-context examples",
    "checked": true,
    "id": "7c6d29ed91d5e1427d3e7e1fdff9d5724e4b75f6",
    "semantic_title": "learning to grok: emergence of in-context learning and skill composition in modular arithmetic tasks",
    "citation_count": 21,
    "authors": [
      "Tianyu He",
      "Darshil Doshi",
      "Aritra Das",
      "Andrey Gromov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/17f21a0a111c9e3a4cd96708d30064f2-Abstract-Conference.html": {
    "title": "Zero-Shot Event-Intensity Asymmetric Stereo via Visual Prompting from Image Domain",
    "volume": "main",
    "abstract": "Event-intensity asymmetric stereo systems have emerged as a promising approach for robust 3D perception in dynamic and challenging environments by integrating event cameras with frame-based sensors in different views. However, existing methods often suffer from overfitting and poor generalization due to limited dataset sizes and lack of scene diversity in the event domain. To address these issues, we propose a zero-shot framework that utilizes monocular depth estimation and stereo matching models pretrained on diverse image datasets. Our approach introduces a visual prompting technique to align the representations of frames and events, allowing the use of off-the-shelf stereo models without additional training. Furthermore, we introduce a monocular cue-guided disparity refinement module to improve robustness across static and dynamic regions by incorporating monocular depth information from foundation models. Extensive experiments on real-world datasets demonstrate the superior zero-shot evaluation performance and enhanced generalization ability of our method compared to existing approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyue Lou",
      "Jinxiu (Sherry) Liang",
      "Minggui Teng",
      "Bin Fan",
      "Yong Xu",
      "Boxin Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/18023809c155d6bbed27e443043cdebf-Abstract-Conference.html": {
    "title": "On improved Conditioning Mechanisms and Pre-training Strategies for Diffusion Models",
    "volume": "main",
    "abstract": "Large-scale training of latent diffusion models (LDMs) has enabled unprecedented quality in image generation. However, large-scale end-to-end training of these models is computationally costly, and hence most research focuses either on finetuning pretrained models or experiments at smaller scales.In this work we aim to improve the training efficiency and performance of LDMs with the goal of scaling to larger datasets and higher resolutions.We focus our study on two points that are critical for good performance and efficient training: (i) the mechanisms used for semantic level (\\eg a text prompt, or class name) and low-level (crop size, random flip, \\etc) conditioning of the model, and (ii) pre-training strategies to transfer representations learned on smaller and lower-resolution datasets to larger ones.The main contributions of our work are the following: we present systematic experimental study of these points, we propose a novel conditioning mechanism that disentangles semantic and low-level conditioning, we obtain state-of-the-art performance on CC12M for text-to-image at 512 resolution",
    "checked": true,
    "id": "bfffb24e4c97580114bc14e38e305e7ad8fed76b",
    "semantic_title": "on improved conditioning mechanisms and pre-training strategies for diffusion models",
    "citation_count": 4,
    "authors": [
      "Tariq Berrada Ifriqi",
      "Pietro Astolfi",
      "Melissa Hall",
      "Reyhane Askari Hemmat",
      "Yohann Benchetrit",
      "Marton Havasi",
      "Matthew Muckley",
      "Karteek Alahari",
      "Adriana Romero-Soriano",
      "Jakob J. Verbeek",
      "Michal Drozdzal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/180d4373aca26bd86bf45fc50d1a709f-Abstract-Conference.html": {
    "title": "A Full-duplex Speech Dialogue Scheme Based On Large Language Model",
    "volume": "main",
    "abstract": "We present a generative dialogue system capable of operating in a full-duplex manner, allowing for seamless interaction. It is based on a large language model (LLM) carefully aligned to be aware of a perception module, a motor function module, and the concept of a simple finite state machine (called neural FSM) with two states. The perception and motor function modules operate in tandem, allowing the system to speak and listen to the user simultaneously. The LLM generates textual tokens for inquiry responses and makes autonomous decisions to start responding to, wait for, or interrupt the user by emitting control tokens to the neural FSM. All these tasks of the LLM are carried out as next token prediction on a serialized view of the dialogue in real-time. In automatic quality evaluations simulating real-life interaction, the proposed system reduces the average conversation response latency by more than threefold compared with LLM-based half-duplex dialogue systems while responding within less than 500 milliseconds in more than 50% of evaluated interactions. Running an LLM with only 8 billion parameters, our system exhibits an 8% higher interruption precision rate than the best available commercial LLM for voice-based dialogue",
    "checked": false,
    "id": "45b11ebadc9eef2c0868853368b03e08eb14e5fc",
    "semantic_title": "a full-duplex speech dialogue scheme based on large language models",
    "citation_count": 18,
    "authors": [
      "Peng Wang",
      "Songshuo Lu",
      "Yaohua Tang",
      "Sijie Yan",
      "Wei Xia",
      "Yuanjun Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/180f6184a3458fa19c28c5483bc61877-Abstract-Conference.html": {
    "title": "MultiPull: Detailing Signed Distance Functions by Pulling Multi-Level Queries at Multi-Step",
    "volume": "main",
    "abstract": "Reconstructing a continuous surface from a raw 3D point cloud is a challenging task. Latest methods employ supervised learning or pretrained priors to learn a signed distance function (SDF). However, neural networks tend to smooth local details due to the lack of ground truth signed distnaces or normals, which limits the performance of learning-based methods in reconstruction tasks. To resolve this issue, we propose a novel method, named MultiPull, to learn multi-scale implicit fields from raw point clouds to optimize accurate SDFs from coarse to fine. We achieve this by mapping 3D query points into a set of frequency features, which makes it possible to leverage multi-level features during optimization. Meanwhile, we introduce optimization constraints from the perspective of spatial distance and normal consistency, which play a key role in point cloud reconstruction based on multi-scale optimization strategies. Our experiments on widely used object and scene benchmarks demonstrate that our method outperforms the state-of-the-art methods in surface reconstruction",
    "checked": true,
    "id": "5c121bf47d4b3b4196a950adc73ee6aef7c4270d",
    "semantic_title": "multipull: detailing signed distance functions by pulling multi-level queries at multi-step",
    "citation_count": 9,
    "authors": [
      "Takeshi Noda",
      "Chao Chen",
      "Weiqi Zhang",
      "Xinhai Liu",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1812042b83f20707a898ff6f8af7db84-Abstract-Conference.html": {
    "title": "Learning Cooperative Trajectory Representations for Motion Forecasting",
    "volume": "main",
    "abstract": "Motion forecasting is an essential task for autonomous driving, and utilizing information from infrastructure and other vehicles can enhance forecasting capabilities.Existing research mainly focuses on leveraging single-frame cooperative information to enhance the limited perception capability of the ego vehicle, while underutilizing the motion and interaction context of traffic participants observed from cooperative devices. In this paper, we propose a forecasting-oriented representation paradigm to utilize motion and interaction features from cooperative information. Specifically, we present V2X-Graph, a representative framework to achieve interpretable and end-to-end trajectory feature fusion for cooperative motion forecasting. V2X-Graph is evaluated on V2X-Seq in vehicle-to-infrastructure (V2I) scenarios.To further evaluate on vehicle-to-everything (V2X) scenario, we construct the first real-world V2X motion forecasting dataset V2X-Traj, which contains multiple autonomous vehicles and infrastructure in every scenario.Experimental results on both V2X-Seq and V2X-Traj show the advantage of our method. We hope both V2X-Graph and V2X-Traj will benefit the further development of cooperative motion forecasting.Find the project at https://github.com/AIR-THU/V2X-Graph",
    "checked": true,
    "id": "05b848a352797d3d26edda59c58627a18120881f",
    "semantic_title": "learning cooperative trajectory representations for motion forecasting",
    "citation_count": 19,
    "authors": [
      "Hongzhi Ruan",
      "Haibao Yu",
      "Wenxian Yang",
      "Siqi Fan",
      "Zaiqing Nie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/18595bc3e802a3b11035927fd928eb9c-Abstract-Conference.html": {
    "title": "eXponential FAmily Dynamical Systems (XFADS): Large-scale nonlinear Gaussian state-space modeling",
    "volume": "main",
    "abstract": "State-space graphical models and the variational autoencoder framework provide a principled apparatus for learning dynamical systems from data. State-of-the-art probabilistic approaches are often able to scale to large problems at the cost of flexibility of the variational posterior or expressivity of the dynamics model. However, those consolidations can be detrimental if the ultimate goal is to learn a generative model capable of explaining the spatiotemporal structure of the data and making accurate forecasts. We introduce a low-rank structured variational autoencoding framework for nonlinear Gaussian state-space graphical models capable of capturing dense covariance structures that are important for learning dynamical systems with predictive capabilities. Our inference algorithm exploits the covariance structures that arise naturally from sample based approximate Gaussian message passing and low-rank amortized posterior updates -- effectively performing approximate variational smoothing with time complexity scaling linearly in the state dimensionality. In comparisons with other deep state-space model architectures our approach consistently demonstrates the ability to learn a more predictive generative model. Furthermore, when applied to neural physiological recordings, our approach is able to learn a dynamical system capable of forecasting population spiking and behavioral correlates from a small portion of single trials",
    "checked": true,
    "id": "e9d94fffa99484923804a48295d946ac12892a5a",
    "semantic_title": "exponential family dynamical systems (xfads): large-scale nonlinear gaussian state-space modeling",
    "citation_count": 7,
    "authors": [
      "Matthew Dowling",
      "Yuan Zhao",
      "Memming Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/185a120a3f709187e68bd092e6098851-Abstract-Conference.html": {
    "title": "AROMA: Preserving Spatial Structure for Latent PDE Modeling with Local Neural Fields",
    "volume": "main",
    "abstract": "We present AROMA (Attentive Reduced Order Model with Attention), a framework designed to enhance the modeling of partial differential equations (PDEs) using local neural fields. Our flexible encoder-decoder architecture can obtain smooth latent representations of spatial physical fields from a variety of data types, including irregular-grid inputs and point clouds. This versatility eliminates the need for patching and allows efficient processing of diverse geometries. The sequential nature of our latent representation can be interpreted spatially and permits the use of a conditional transformer for modeling the temporal dynamics of PDEs. By employing a diffusion-based formulation, we achieve greater stability and enable longer rollouts compared to conventional MSE training. AROMA's superior performance in simulating 1D and 2D equations underscores the efficacy of our approach in capturing complex dynamical behaviors",
    "checked": true,
    "id": "ed3fa74ebc0adcad4aee915c4625b384eaddbc91",
    "semantic_title": "aroma: preserving spatial structure for latent pde modeling with local neural fields",
    "citation_count": 7,
    "authors": [
      "Louis Serrano",
      "Thomas X Wang",
      "Etienne Le Naour",
      "Jean-Noël Vittaut",
      "Patrick Gallinari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1862ea85f21ca819eed2b75af39bba60-Abstract-Conference.html": {
    "title": "Parameter Efficient Adaptation for Image Restoration with Heterogeneous Mixture-of-Experts",
    "volume": "main",
    "abstract": "Designing single-task image restoration models for specific degradation has seen great success in recent years. To achieve generalized image restoration, all-in-one methods have recently been proposed and shown potential for multiple restoration tasks using one single model. Despite the promising results, the existing all-in-one paradigm still suffers from high computational costs as well as limited generalization on unseen degradations. In this work, we introduce an alternative solution to improve the generalization of image restoration models. Drawing inspiration from recent advancements in Parameter Efficient Transfer Learning (PETL), we aim to tune only a small number of parameters to adapt pre-trained restoration models to various tasks. However, current PETL methods fail to generalize across varied restoration tasks due to their homogeneous representation nature. To this end, we propose AdaptIR, a Mixture-of-Experts (MoE) with orthogonal multi-branch design to capture local spatial, global spatial, and channel representation bases, followed by adaptive base combination to obtain heterogeneous representation for different degradations. Extensive experiments demonstrate that our AdaptIR achieves stable performance on single-degradation tasks, and excels in hybrid-degradation tasks, with training only 0.6% parameters for 8 hours",
    "checked": true,
    "id": "495bd4d650aee25fe762ee5c47f296989168dcc6",
    "semantic_title": "parameter efficient adaptation for image restoration with heterogeneous mixture-of-experts",
    "citation_count": 3,
    "authors": [
      "Hang Guo",
      "Tao Dai",
      "Yuanchao Bai",
      "Bin Chen",
      "Xudong Ren",
      "Zexuan Zhu",
      "Shu-Tao Xia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1867748a011e1425b924ec72a4066b62-Abstract-Conference.html": {
    "title": "DiffCut: Catalyzing Zero-Shot Semantic Segmentation with Diffusion Features and Recursive Normalized Cut",
    "volume": "main",
    "abstract": "Foundation models have emerged as powerful tools across various domains including language, vision, and multimodal tasks. While prior works have addressed unsupervised semantic segmentation, they significantly lag behind supervised models. In this paper, we use a diffusion UNet encoder as a foundation vision encoder and introduce DiffCut, an unsupervised zero-shot segmentation method that solely harnesses the output features from the final self-attention block. Through extensive experimentation, we demonstrate that using these diffusion features in a graph based segmentation algorithm, significantly outperforms previous state-of-the-art methods on zero-shot segmentation. Specifically, we leverage a recursive Normalized Cut algorithm that regulates the granularity of detected objects and produces well-defined segmentation maps that precisely capture intricate image details. Our work highlights the remarkably accurate semantic knowledge embedded within diffusion UNet encoders that could then serve as foundation vision encoders for downstream tasks",
    "checked": true,
    "id": "966e5b7466de892fcf92c31ea9fe23eb633d6c59",
    "semantic_title": "diffcut: catalyzing zero-shot semantic segmentation with diffusion features and recursive normalized cut",
    "citation_count": 10,
    "authors": [
      "Paul Couairon",
      "Mustafa Shukor",
      "Jean-Emmanuel HAUGEARD",
      "Matthieu Cord",
      "Nicolas THOME"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/187d94b3c93343f0e925b5cf729eadd5-Abstract-Conference.html": {
    "title": "BitDelta: Your Fine-Tune May Only Be Worth One Bit",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it is intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, thus reducing per-user generation latency by more than 10x in multi-tenant settings. We validate BitDelta through experiments across Llama-2, Mistral and MPT model families, and on models up to 70B parameters, showcasing minimal performance degradation in all tested settings",
    "checked": true,
    "id": "a624041a028404e7d1fdb40987b1780ce4f1c842",
    "semantic_title": "bitdelta: your fine-tune may only be worth one bit",
    "citation_count": 27,
    "authors": [
      "James Liu",
      "Guangxuan Xiao",
      "Kai Li",
      "Jason Lee",
      "Song Han",
      "Tri Dao",
      "Tianle Cai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/189699197ead7a012c7fa4cdc4cc413d-Abstract-Conference.html": {
    "title": "LION: Linear Group RNN for 3D Object Detection in Point Clouds",
    "volume": "main",
    "abstract": "The benefit of transformers in large-scale 3D point cloud perception tasks, such as 3D object detection, is limited by their quadratic computation cost when modeling long-range relationships. In contrast, linear RNNs have low computational complexity and are suitable for long-range modeling. Toward this goal, we propose a simple and effective window-based framework built on Linear group RNN (i.e., perform linear RNN for grouped features) for accurate 3D object detection, called LION. The key property is to allow sufficient feature interaction in a much larger group than transformer-based methods. However, effectively applying linear group RNN to 3D object detection in highly sparse point clouds is not trivial due to its limitation in handling spatial modeling. To tackle this problem, we simply introduce a 3D spatial feature descriptor and integrate it into the linear group RNN operators to enhance their spatial features rather than blindly increasing the number of scanning orders for voxel features. To further address the challenge in highly sparse point clouds, we propose a 3D voxel generation strategy to densify foreground features thanks to linear group RNN as a natural property of auto-regressive models. Extensive experiments verify the effectiveness of the proposed components and the generalization of our LION on different linear group RNN operators including Mamba, RWKV, and RetNet. Furthermore, it is worth mentioning that our LION-Mamba achieves state-of-the-art on Waymo, nuScenes, Argoverse V2, and ONCE datasets. Last but not least, our method supports kinds of advanced linear RNN operators (e.g., RetNet, RWKV, Mamba, xLSTM and TTT) on small but popular KITTI dataset for a quick experience with our linear RNN-based framework",
    "checked": true,
    "id": "abdd5f0f6be0f8510864f5bcdcb4cda71f0fa83d",
    "semantic_title": "lion: linear group rnn for 3d object detection in point clouds",
    "citation_count": 19,
    "authors": [
      "Zhe Liu",
      "Jinghua Hou",
      "Xinyu Wang",
      "Xiaoqing Ye",
      "Jingdong Wang",
      "Hengshuang Zhao",
      "Xiang Bai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/18aee41e1bb41bbb8fee53cfff8138b7-Abstract-Conference.html": {
    "title": "$SE(3)$ Equivariant Ray Embeddings for Implicit Multi-View Depth Estimation",
    "volume": "main",
    "abstract": "Incorporating inductive bias by embedding geometric entities (such as rays) as input has proven successful in multi-view learning. However, the methods adopting this technique typically lack equivariance, which is crucial for effective 3D learning. Equivariance serves as a valuable inductive prior, aiding in the generation of robust multi-view features for 3D scene understanding. In this paper, we explore the application of equivariant multi-view learning to depth estimation, not only recognizing its significance for computer vision and robotics but also addressing the limitations of previous research. Most prior studies have either overlooked equivariance in this setting or achieved only approximate equivariance through data augmentation, which often leads to inconsistencies across different reference frames. To address this issue, we propose to embed $SE(3)$ equivariance into the Perceiver IO architecture. We employ Spherical Harmonics for positional encoding to ensure 3D rotation equivariance, and develop a specialized equivariant encoder and decoder within the Perceiver IO architecture. To validate our model, we applied it to the task of stereo depth estimation, achieving state of the art results on real-world datasets without explicit geometric constraints or extensive data augmentation",
    "checked": false,
    "id": "5828a4ea723c53e843945af2690c8b41d42a5567",
    "semantic_title": "se(3) equivariant ray embeddings for implicit multi-view depth estimation",
    "citation_count": 3,
    "authors": [
      "Yinshuang Xu",
      "Dian Chen",
      "Katherine Liu",
      "Sergey Zakharov",
      "Rareș Ambruș",
      "Kostas Daniilidis",
      "Vitor Guizilini"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/18b0b4c788c8f2cf6c2943b989ad18c8-Abstract-Conference.html": {
    "title": "Fast and Memory-Efficient Video Diffusion Using Streamlined Inference",
    "volume": "main",
    "abstract": "The rapid progress in artificial intelligence-generated content (AIGC), especially with diffusion models, has significantly advanced development of high-quality video generation. However, current video diffusion models exhibit demanding computational requirements and high peak memory usage, especially for generating longer and higher-resolution videos. These limitations greatly hinder the practical application of video diffusion models on standard hardware platforms. To tackle this issue, we present a novel, training-free framework named Streamlined Inference, which leverages the temporal and spatial properties of video diffusion models. Our approach integrates three core components: Feature Slicer, Operator Grouping, and Step Rehash. Specifically, Feature Slicer effectively partitions input features into sub-features and Operator Grouping processes each sub-feature with a group of consecutive operators, resulting in significant memory reduction without sacrificing the quality or speed. Step Rehash further exploits the similarity between adjacent steps in diffusion, and accelerates inference through skipping unnecessary steps. Extensive experiments demonstrate that our approach significantly reduces peak memory and computational overhead, making it feasible to generate high-quality videos on a single consumer GPU (e.g., reducing peak memory of Animatediff from 42GB to 11GB, featuring faster inference on 2080Ti)",
    "checked": true,
    "id": "d690dc77e24558c71e00bc4ea25498992a059006",
    "semantic_title": "fast and memory-efficient video diffusion using streamlined inference",
    "citation_count": 7,
    "authors": [
      "Zheng Zhan",
      "Yushu Wu",
      "Yifan Gong",
      "Zichong Meng",
      "Zhenglun Kong",
      "Changdi Yang",
      "Geng Yuan",
      "Pu Zhao",
      "Wei Niu",
      "Yanzhi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/18c0102cb7f1a02c14f0929089b2e576-Abstract-Conference.html": {
    "title": "Sparse High Rank Adapters",
    "volume": "main",
    "abstract": "Low Rank Adaptation (LoRA) has gained massive attention in the recent generative AI research. One of the main advantages of LoRA is its ability to be fused with pretrained models, adding no overhead during inference. However, from a mobile deployment standpoint, we can either avoid inference overhead in the fused mode but lose the ability to switch adapters rapidly, or suffer significant (up to 30% higher) inference latency while enabling rapid switching in the unfused mode. LoRA also exhibits concept-loss when multiple adapters are used concurrently. In this paper, we propose Sparse High Rank Adapters (SHiRA), a new paradigm which incurs no inference overhead, enables rapid switching, and significantly reduces concept-loss. Specifically, SHiRA can be trained by directly tuning only 1-2% of the base model weights while leaving others unchanged. This results in a highly sparse adapter which can be switched directly in the fused mode. We further provide theoretical and empirical insights on how high sparsity in SHiRA can aid multi-adapter fusion by reducing concept loss. Our extensive experiments on LVMs and LLMs demonstrate that finetuning only a small fraction of the parameters in the base model significantly outperforms LoRA while enabling both rapid switching and multi-adapter fusion. Finally, we provide a latency- and memory-efficient SHiRA implementation based on Parameter-Efficient Finetuning (PEFT) Library which trains at nearly the same speed as LoRA while consuming up to 16% lower peak GPU memory, thus making SHiRA easy to adopt for practical use cases. To demonstrate rapid switching benefits during inference, we show that loading SHiRA on a base model can be 5x-16x faster than LoRA fusion on a CPU",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartikeya Bhardwaj",
      "Nilesh Pandey",
      "Sweta Priyadarshi",
      "Viswanath Ganapathy",
      "Shreya Kadambi",
      "Rafael Esteves",
      "Shubhankar Borse",
      "Paul Whatmough",
      "Risheek Garrepalli",
      "Mart van Baalen",
      "Harris Teague",
      "Markus Nagel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/18d3a2f3068d6c669dcae19ceca1bc24-Abstract-Conference.html": {
    "title": "Pretraining with Random Noise for Fast and Robust Learning without Weight Transport",
    "volume": "main",
    "abstract": "The brain prepares for learning even before interacting with the environment, by refining and optimizing its structures through spontaneous neural activity that resembles random noise. However, the mechanism of such a process has yet to be understood, and it is unclear whether this process can benefit the algorithm of machine learning. Here, we study this issue using a neural network with a feedback alignment algorithm, demonstrating that pretraining neural networks with random noise increases the learning efficiency as well as generalization abilities without weight transport. First, we found that random noise training modifies forward weights to match backward synaptic feedback, which is necessary for teaching errors by feedback alignment. As a result, a network with pre-aligned weights learns notably faster and reaches higher accuracy than a network without random noise training, even comparable to the backpropagation algorithm. We also found that the effective dimensionality of weights decreases in a network pretrained with random noise. This pre-regularization allows the network to learn simple solutions of a low rank, reducing the generalization error during subsequent training. This also enables the network to robustly generalize a novel, out-of-distribution dataset. Lastly, we confirmed that random noise pretraining reduces the amount of meta-loss, enhancing the network ability to adapt to various tasks. Overall, our results suggest that random noise training with feedback alignment offers a straightforward yet effective method of pretraining that facilitates quick and reliable learning without weight transport",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghwan Cheon",
      "Sang Wan Lee",
      "Se-Bum Paik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/192956d4857000578f626c5193b34419-Abstract-Conference.html": {
    "title": "High-Resolution Image Harmonization with Adaptive-Interval Color Transformation",
    "volume": "main",
    "abstract": "Existing high-resolution image harmonization methods typically rely on global color adjustments or the upsampling of parameter maps. However, these methods ignore local variations, leading to inharmonious appearances. To address this problem, we propose an Adaptive-Interval Color Transformation method (AICT), which predicts pixel-wise color transformations and adaptively adjusts the sampling interval to model local non-linearities of the color transformation at high resolution. Specifically, a parameter network is first designed to generate multiple position-dependent 3-dimensional lookup tables (3D LUTs), which use the color and position of each pixel to perform pixel-wise color transformations. Then, to enhance local variations adaptively, we separate a color transform into a cascade of sub-transformations using two 3D LUTs to achieve the non-uniform sampling intervals of the color transform. Finally, a global consistent weight learning method is proposed to predict an image-level weight for each color transform, utilizing global information to enhance the overall harmony. Extensive experiments demonstrate that our AICT achieves state-of-the-art performance with a lightweight architecture. The code is available at https://github.com/aipixel/AICT",
    "checked": true,
    "id": "86055c96eec8cf6cde1c884aa0b3324fc9d8f8ae",
    "semantic_title": "high-resolution image harmonization with adaptive-interval color transformation",
    "citation_count": 7,
    "authors": [
      "Quanling Meng",
      "Liu Qinglin",
      "Zonglin Li",
      "Xiangyuan Lan",
      "Shengping Zhang",
      "Liqiang Nie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/19305d2dbcc81c44d4a0120e7569856e-Abstract-Conference.html": {
    "title": "Pseudo-Siamese Blind-spot Transformers for Self-Supervised Real-World Denoising",
    "volume": "main",
    "abstract": "Real-world image denoising remains a challenge task. This paper studies self-supervised image denoising, requiring only noisy images captured in a single shot. We revamping the blind-spot technique by leveraging the transformer's capability for long-range pixel interactions, which is crucial for effectively removing noise dependence in relating pixel–a requirement for achieving great performance for the blind-spot technique. The proposed method integrates these elements with two key innovations: a directional self-attention (DSA) module using a half-plane grid for self-attention, creating a sophisticated blind-spot structure, and a Siamese architecture with mutual learning to mitigate the performance impactsfrom the restricted attention grid in DSA. Experiments on benchmark datasets demonstrate that our method outperforms existing self-supervised and clean-image-free methods. This combination of blind-spot and transformer techniques provides a natural synergy for tackling real-world image denoising challenges",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Quan",
      "Tianxiang Zheng",
      "Hui Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1943190d92efee1fccfc8d6579b0f8d9-Abstract-Conference.html": {
    "title": "Expected Probabilistic Hierarchies",
    "volume": "main",
    "abstract": "Hierarchical clustering has usually been addressed by discrete optimization using heuristics or continuous optimization of relaxed scores for hierarchies. In this work, we propose to optimize expected scores under a probabilistic model over hierarchies. (1) We show theoretically that the global optimal values of the expected Dasgupta cost and Tree-Sampling divergence (TSD), two unsupervised metrics for hierarchical clustering, are equal to the optimal values of their discrete counterparts contrary to some relaxed scores. (2) We propose Expected Probabilistic Hierarchies (EPH), a probabilistic model to learn hierarchies in data by optimizing expected scores. EPH uses differentiable hierarchy sampling enabling end-to-end gradient descent based optimization, and an unbiased subgraph sampling approach to scale to large datasets. (3) We evaluate EPH on synthetic and real-world datasets including vector and graph datasets. EPH outperforms all other approaches quantitatively and provides meaningful hierarchies in qualitative evaluations",
    "checked": true,
    "id": "2dbc052df1287416c326e800301a543f498b0991",
    "semantic_title": "expected probabilistic hierarchies",
    "citation_count": 0,
    "authors": [
      "Marcel Kollovieh",
      "Bertrand Charpentier",
      "Daniel Zügner",
      "Stephan Günnemann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/194fa4536bf36f35a4505d20cd5dd6fc-Abstract-Conference.html": {
    "title": "Video Token Merging for Long Video Understanding",
    "volume": "main",
    "abstract": "As the scale of data and models for video understanding rapidly expand, handling long-form video input in transformer-based models presents a practical challenge. Rather than resorting to input sampling or token dropping, which may result in information loss, token merging shows promising results when used in collaboration with transformers. However, the application of token merging for long-form video processing is not trivial. We begin with the premise that token merging should not rely solely on the similarity of video tokens; the saliency of tokens should also be considered. To address this, we explore various video token merging strategies for long-form video classification, starting with a simple extension of image token merging, moving to region-concentrated merging, and finally proposing a learnable video token merging (VTM) algorithm that dynamically merges tokens based on their saliency. Extensive experimental results show that we achieve better or comparable performances on the LVU, COIN, and Breakfast datasets. Moreover, our approach significantly reduces memory costs by 84% and boosts throughput by approximately 6.89 times compared to baseline algorithms",
    "checked": true,
    "id": "cd7c76dccf476fda819b7df4cc53044cf8ee1eb2",
    "semantic_title": "video token merging for long video understanding",
    "citation_count": 4,
    "authors": [
      "Seon-Ho Lee",
      "Jue Wang",
      "Zhikang Zhang",
      "David Fan",
      "Xinyu Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/195c4e2910bedabd813f9fe4e70c642c-Abstract-Conference.html": {
    "title": "Alignment at Pre-training! Towards Native Alignment for Arabic LLMs",
    "volume": "main",
    "abstract": "The alignment of large language models (LLMs) is critical for developing effective and safe language models. Traditional approaches focus on aligning models during the instruction tuning or reinforcement learning stages, referred to in this paper as `\\textit{post alignment}'. We argue that alignment during the pre-training phase, which we term 'native alignment', warrants investigation. Native alignment aims to prevent unaligned content from the beginning, rather than relying on post-hoc processing. This approach leverages extensively aligned pre-training data to enhance the effectiveness and usability of pre-trained models. Our study specifically explores the application of native alignment in the context of Arabic LLMs. We conduct comprehensive experiments and ablation studies to evaluate the impact of native alignment on model performance and alignment stability. Additionally, we release open-source Arabic LLMs that demonstrate state-of-the-art performance on various benchmarks, providing significant benefits to the Arabic LLM community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juhao Liang",
      "Zhenyang Cai",
      "Jianqing Zhu",
      "Huang Huang",
      "Kewei Zong",
      "Bang An",
      "Mosen Alharthi",
      "Juncai He",
      "Lian Zhang",
      "Haizhou Li",
      "Benyou Wang",
      "Jinchao Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/19774ce2d4b0d17a3a8aea26ad99fe8a-Abstract-Conference.html": {
    "title": "FSP-Laplace: Function-Space Priors for the Laplace Approximation in Bayesian Deep Learning",
    "volume": "main",
    "abstract": "Laplace approximations are popular techniques for endowing deep networks with epistemic uncertainty estimates as they can be applied without altering the predictions of the trained network, and they scale to large models and datasets. While the choice of prior strongly affects the resulting posterior distribution, computational tractability and lack of interpretability of the weight space typically limit the Laplace approximation to isotropic Gaussian priors, which are known to cause pathological behavior as depth increases. As a remedy, we directly place a prior on function space. More precisely, since Lebesgue densities do not exist on infinite-dimensional function spaces, we recast training as finding the so-called weak mode of the posterior measure under a Gaussian process (GP) prior restricted to the space of functions representable by the neural network. Through the GP prior, one can express structured and interpretable inductive biases, such as regularity or periodicity, directly in function space, while still exploiting the implicit inductive biases that allow deep networks to generalize. After model linearization, the training objective induces a negative log-posterior density to which we apply a Laplace approximation, leveraging highly scalable methods from matrix-free linear algebra. Our method provides improved results where prior knowledge is abundant (as is the case in many scientific inference tasks). At the same time, it stays competitive for black-box supervised learning problems, where neural networks typically excel",
    "checked": true,
    "id": "c1a0555187db175d6e1fb7be3e3ea84e032a4d7c",
    "semantic_title": "fsp-laplace: function-space priors for the laplace approximation in bayesian deep learning",
    "citation_count": 8,
    "authors": [
      "Tristan Cinquin",
      "Marvin Pförtner",
      "Vincent Fortuin",
      "Philipp Hennig",
      "Robert Bamler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/19a42d5885e25e51852aca8144e5af0d-Abstract-Conference.html": {
    "title": "SCAFFLSA: Taming Heterogeneity in Federated Linear Stochastic Approximation and TD Learning",
    "volume": "main",
    "abstract": "In this paper, we analyze the sample and communication complexity of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the effects of local training with agent heterogeneity. We show that the communication complexity of FedLSA scales polynomially with the inverse of the desired accuracy ϵ. To overcome this, we propose SCAFFLSA a new variant of FedLSA that uses control variates to correct for client drift, and establish its sample and communication complexities. We show that for statistically heterogeneous agents, its communication complexity scales logarithmically with the desired accuracy, similar to Scaffnew. An important finding is that, compared to the existing results for Scaffnew, the sample complexity scales with the inverse of the number of agents, a property referred to as linear speed-up. Achieving this linear speed-up requires completely new theoretical arguments. We apply the proposed method to federated temporal difference learning with linear function approximation and analyze the corresponding complexity improvements",
    "checked": true,
    "id": "01240457d69f602a40c774c1d06754a21684d0fc",
    "semantic_title": "scafflsa: taming heterogeneity in federated linear stochastic approximation and td learning",
    "citation_count": 1,
    "authors": [
      "Paul Mangold",
      "Sergey Samsonov",
      "Safwan Labbi",
      "Ilya Levin",
      "REDA ALAMI",
      "Alexey Naumov",
      "Eric Moulines"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/19a94fdf9e1c5b387830e4c4fef6972a-Abstract-Conference.html": {
    "title": "Sequential Probability Assignment with Contexts: Minimax Regret, Contextual Shtarkov Sums, and Contextual Normalized Maximum Likelihood",
    "volume": "main",
    "abstract": "We study the fundamental problem of sequential probability assignment, also known as online learning with logarithmic loss, with respect to an arbitrary, possibly nonparametric hypothesis class. Our goal is to obtain a complexity measure for the hypothesis class that characterizes the minimax regret and to determine a general, minimax optimal algorithm. Notably, the sequential $\\ell_{\\infty}$ entropy, extensively studied in the literature (Rakhlin and Sridharan, 2015, Bilodeau et al., 2020, Wu et al., 2023), was shown to not characterize minimax regret in general. Inspired by the seminal work of Shtarkov (1987) and Rakhlin, Sridharan, and Tewari (2010), we introduce a novel complexity measure, the \\emph{contextual Shtarkov sum}, corresponding to the Shtarkov sum after projection onto a multiary context tree, and show that the worst case log contextual Shtarkov sum equals the minimax regret. Using the contextual Shtarkov sum, we derive the minimax optimal strategy, dubbed \\emph{contextual Normalized Maximum Likelihood} (cNML). Our results hold for sequential experts, beyond binary labels, which are settings rarely considered in prior work. To illustrate the utility of this characterization, we provide a short proof of a new regret upper bound in terms of sequential $\\ell_{\\infty}$ entropy, unifying and sharpening state-of-the-art bounds by Bilodeau et al. (2020) and Wu et al. (2023)",
    "checked": true,
    "id": "fbf6dba6c4cedcff119070f4bf77901a0f11f805",
    "semantic_title": "sequential probability assignment with contexts: minimax regret, contextual shtarkov sums, and contextual normalized maximum likelihood",
    "citation_count": 1,
    "authors": [
      "Ziyi Liu",
      "Idan Attias",
      "Dan Roy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/19ae2b95d3831c14373271112f189a22-Abstract-Conference.html": {
    "title": "Functional Bilevel Optimization for Machine Learning",
    "volume": "main",
    "abstract": "In this paper, we introduce a new functional point of view on bilevel optimization problems for machine learning, where the inner objective is minimized over a function space. These types of problems are most often solved by using methods developed in the parametric setting, where the inner objective is strongly convex with respect to the parameters of the prediction function. The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function. We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the benefits of our approach on instrumental regression and reinforcement learning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ieva Petrulionytė",
      "Julien Mairal",
      "Michael Arbel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/19ba2b9448d5de25826f6eb408dab194-Abstract-Conference.html": {
    "title": "Emergence of heavy tails in homogenized stochastic gradient descent",
    "volume": "main",
    "abstract": "It has repeatedly been observed that loss minimization by stochastic gradient descent (SGD) leads to heavy-tailed distributions of neural network parameters. Here, we analyze a continuous diffusion approximation of SGD, called homogenized stochastic gradient descent (hSGD), and show in a regularized linear regression framework that it leads to an asymptotically heavy-tailed parameter distribution, even though local gradient noise is Gaussian. We give explicit upper and lower bounds on the tail-index of the resulting parameter distribution and validate these bounds in numerical experiments. Moreover, the explicit form of these bounds enables us to quantify the interplay between optimization hyperparameters and the tail-index. Doing so, we contribute to the ongoing discussion on links between heavy tails and the generalization performance of neural networks as well as the ability of SGD to avoid suboptimal local minima",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhezhe Jiao",
      "Martin Keller-Ressel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/19c145aaad40927c51f4d10eaa339c20-Abstract-Conference.html": {
    "title": "Initialization is Critical to Whether Transformers Fit Composite Functions by Reasoning or Memorizing",
    "volume": "main",
    "abstract": "Transformers have shown impressive capabilities across various tasks, but their performance on compositional problems remains a topic of debate. In this work, we investigate the mechanisms of how transformers behave on unseen compositional tasks. We discover that the parameter initialization scale plays a critical role in determining whether the model learns inferential (reasoning-based) solutions, which capture the underlying compositional primitives, or symmetric (memory-based) solutions, which simply memorize mappings without understanding the compositional structure. By analyzing the information flow and vector representations within the model, we reveal the distinct mechanisms underlying these solution types. We further find that inferential (reasoning-based) solutions exhibit low complexity bias, which we hypothesize is a key factor enabling them to learn individual mappings for single anchors. We validate our conclusions on various real-world datasets. Our findings provide valuable insights into the role of initialization scale in tuning the reasoning and memorizing ability and we propose the initialization rate $\\gamma$ to be a convenient tunable hyper-parameter in common deep learning frameworks, where $1/d_{\\mathrm{in}}^\\gamma$ is the standard deviation of parameters of the layer with $d_{\\mathrm{in}}$ input neurons",
    "checked": true,
    "id": "a4274517536d66df3320b31e0ee0e2b8a46f7cb7",
    "semantic_title": "initialization is critical to whether transformers fit composite functions by reasoning or memorizing",
    "citation_count": 5,
    "authors": [
      "Zhongwang Zhang",
      "Pengxiao Lin",
      "Zhiwei Wang",
      "Yaoyu Zhang",
      "Zhi-Qin Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/19c9708f31ec44b5b1cbd67f91d05d95-Abstract-Conference.html": {
    "title": "Learning Interaction-aware 3D Gaussian Splatting for One-shot Hand Avatars",
    "volume": "main",
    "abstract": "In this paper, we propose to create animatable avatars for interacting hands with 3D Gaussian Splatting (GS) and single-image inputs. Existing GS-based methods designed for single subjects often yield unsatisfactory results due to limited input views, various hand poses, and occlusions. To address these challenges, we introduce a novel two-stage interaction-aware GS framework that exploits cross-subject hand priors and refines 3D Gaussians in interacting areas. Particularly, to handle hand variations, we disentangle the 3D presentation of hands into optimization-based identity maps and learning-based latent geometric features and neural texture maps. Learning-based features are captured by trained networks to provide reliable priors for poses, shapes, and textures, while optimization-based identity maps enable efficient one-shot fitting of out-of-distribution hands. Furthermore, we devise an interaction-aware attention module and a self-adaptive Gaussian refinement module. These modules enhance image rendering quality in areas with intra- and inter-hand interactions, overcoming the limitations of existing GS-based methods. Our proposed method is validated via extensive experiments on the large-scale InterHand2.6M dataset, and it significantly improves the state-of-the-art performance in image quality. Code and models will be released upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Huang",
      "Hanhui Li",
      "Wanquan Liu",
      "Xiaodan Liang",
      "Yiqiang Yan",
      "Yuhao Cheng",
      "CHENQIANG GAO"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/19cdab1dee61d55158cf106244ceab08-Abstract-Conference.html": {
    "title": "Learning-Augmented Dynamic Submodular Maximization",
    "volume": "main",
    "abstract": "In dynamic submodular maximization, the goal is to maintain a high-value solution over a sequence of element insertions and deletions with a fast update time. Motivated by large-scale applications and the fact that dynamic data often exhibits patterns, we ask the following question: can predictions be used to accelerate the update time of dynamic submodular maximization algorithms? We consider the model for dynamic algorithms with predictions where predictions regarding the insertion and deletion times of elements can be used for preprocessing. Our main result is an algorithm with an $O(\\text{poly}(\\log \\eta, \\log w, \\log k))$ amortized update time over the sequence of updates that achieves a $1/2 - \\epsilon$ approximation for dynamic monotone submodular maximization under a cardinality constraint $k$, where the prediction error $\\eta$ is the number of elements that are not inserted and deleted within $w$ time steps of their predicted insertion and deletion times. This amortized update time is independent of the length of the stream and instead depends on the prediction error",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arpit Agarwal",
      "Eric Balkanski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/19ded4cfc36a7feb7fce975393d378fd-Abstract-Conference.html": {
    "title": "IF-Font: Ideographic Description Sequence-Following Font Generation",
    "volume": "main",
    "abstract": "Few-shot font generation (FFG) aims to learn the target style from a limited number of reference glyphs and generate the remaining glyphs in the target font. Previous works focus on disentangling the content and style features of glyphs, combining the content features of the source glyph with the style features of the reference glyph to generate new glyphs. However, the disentanglement is challenging due to the complexity of glyphs, often resulting in glyphs that are influenced by the style of the source glyph and prone to artifacts. We propose IF-Font, a novel paradigm which incorporates Ideographic Description Sequence (IDS) instead of the source glyph to control the semantics of generated glyphs. To achieve this, we quantize the reference glyphs into tokens, and model the token distribution of target glyphs using corresponding IDS and reference tokens. The proposed method excels in synthesizing glyphs with neat and correct strokes, and enables the creation of new glyphs based on provided IDS. Extensive experiments demonstrate that our method greatly outperforms state-of-the-art methods in both one-shot and few-shot settings, particularly when the target styles differ significantly from the training font styles. The code is available at https://github.com/Stareven233/IF-Font",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinping Chen",
      "Xiao Ke",
      "Wenzhong Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/19e9a88d91917775b34fdad447ed8908-Abstract-Conference.html": {
    "title": "Pretrained Optimization Model for Zero-Shot Black Box Optimization",
    "volume": "main",
    "abstract": "Zero-shot optimization involves optimizing a target task that was not seen during training, aiming to provide the optimal solution without or with minimal adjustments to the optimizer. It is crucial to ensure reliable and robust performance in various applications. Current optimizers often struggle with zero-shot optimization and require intricate hyperparameter tuning to adapt to new tasks. To address this, we propose a Pretrained Optimization Model (POM) that leverages knowledge gained from optimizing diverse tasks, offering efficient solutions to zero-shot optimization through direct application or fine-tuning with few-shot samples. Evaluation on the BBOB benchmark and two robot control tasks demonstrates that POM outperforms state-of-the-art black-box optimization methods, especially for high-dimensional tasks. Fine-tuning POM with a small number of samples and budget yields significant performance improvements. Moreover, POM demonstrates robust generalization across diverse task distributions, dimensions, population sizes, and optimization horizons. For code implementation, see https://github.com/ninja-wm/POM/",
    "checked": true,
    "id": "8e05a8d64d6a457018eb62710ca3cf1996001299",
    "semantic_title": "pretrained optimization model for zero-shot black box optimization",
    "citation_count": 4,
    "authors": [
      "Xiaobin Li",
      "Kai Wu",
      "yujian li",
      "Xiaoyu Zhang",
      "Handing Wang",
      "Jing Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/19f10adb6749b0c9f1ff7610bd01d44d-Abstract-Conference.html": {
    "title": "Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning",
    "volume": "main",
    "abstract": "Training models with longer in-context lengths is a significant challenge for multimodal machine learning due to substantial GPU memory and computational costs. This exploratory study does not present state-of-the-art models; rather, it introduces an innovative method designed to increase in-context text length in multi-modality large language models (MLLMs) efficiently. We present \\ModelFullName (\\ModelName), which processes long in-context text using visual tokens. This technique significantly reduces GPU memory usage and floating point operations (FLOPs). For instance, our method expands the pre-training in-context length from 256 to 2048 tokens with fewer FLOPs for a 56 billion parameter MOE model. Experimental results demonstrate that \\ModelName enhances OCR capabilities and delivers superior performance on common downstream benchmarks for in-context few-shot evaluation. Additionally, \\ModelName proves effective for long context inference, achieving results comparable to full text input while maintaining computational efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Jinpeng Wang",
      "Linjie Li",
      "Yiqi Lin",
      "Min Li",
      "Lijuan Wang",
      "Mike Zheng Shou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/19f461132d8ed04764f58f88b7a8e842-Abstract-Conference.html": {
    "title": "Boundary Decomposition for Nadir Objective Vector Estimation",
    "volume": "main",
    "abstract": "The nadir objective vector plays a key role in solving multi-objective optimization problems (MOPs), where it is often used to normalize the objective space and guide the search. The current methods for estimating the nadir objective vector perform effectively only on specific MOPs. This paper reveals the limitations of these methods: exact methods can only work on discrete MOPs, while heuristic methods cannot deal with the MOP with a complicated feasible objective region. To fill this gap, we propose a general and rigorous method, namely boundary decomposition for nadir objective vector estimation (BDNE). BDNE scalarizes the MOP into a set of boundary subproblems. By utilizing bilevel optimization, boundary subproblems are optimized and adjusted alternately, thereby refining their optimal solutions to align with the nadir objective vector. We prove that the bilevel optimization identifies the nadir objective vector under mild conditions. We compare BDNE with existing methods on various black-box MOPs. The results conform to the theoretical analysis and show the significant potential of BDNE for real-world application",
    "checked": true,
    "id": "6435ff4fd47761d8c5e48ff2834a43aafd8659b8",
    "semantic_title": "boundary decomposition for nadir objective vector estimation",
    "citation_count": 1,
    "authors": [
      "Ruihao Zheng",
      "Zhenkun Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/19f7f755908372efb25826d61959cdf9-Abstract-Conference.html": {
    "title": "Recurrent Reinforcement Learning with Memoroids",
    "volume": "main",
    "abstract": "Memory models such as Recurrent Neural Networks (RNNs) and Transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models called Linear Recurrent Models. We discover that the recurrent update of these models resembles a monoid, leading us to reformulate existing models using a novel monoid-based framework that we call memoroids. We revisit the traditional approach to batching in recurrent reinforcement learning, highlighting theoretical and empirical deficiencies. We leverage memoroids to propose a batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in reinforcement learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steven Morad",
      "Chris Lu",
      "Ryan Kortvelesy",
      "Stephan Liwicki",
      "Jakob Foerster",
      "Amanda Prorok"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1a000ee0f122d0bbd3edb9bf55170ea3-Abstract-Conference.html": {
    "title": "Image Copy Detection for Diffusion Models",
    "volume": "main",
    "abstract": "Images produced by diffusion models are increasingly popular in digital artwork and visual marketing. However, such generated images might replicate content from existing ones and pose the challenge of content originality. Existing Image Copy Detection (ICD) models, though accurate in detecting hand-crafted replicas, overlook the challenge from diffusion models. This motivates us to introduce ICDiff, the first ICD specialized for diffusion models. To this end, we construct a Diffusion-Replication (D-Rep) dataset and correspondingly propose a novel deep embedding method. D-Rep uses a state-of-the-art diffusion model (Stable Diffusion V1.5) to generate 40, 000 image-replica pairs, which are manually annotated into 6 replication levels ranging from 0 (no replication) to 5 (total replication). Our method, PDF-Embedding, transforms the replication level of each image-replica pair into a probability density function (PDF) as the supervision signal. The intuition is that the probability of neighboring replication levels should be continuous and smooth. Experimental results show that PDF-Embedding surpasses protocol-driven methods and non-PDF choices on the D-Rep test set. Moreover, by utilizing PDF-Embedding, we find that the replication ratios of well-known diffusion models against an open-source gallery range from 10% to 20%. The project is publicly available at https://icdiff.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Wang",
      "Yifan Sun",
      "Zhentao Tan",
      "Yi Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1a134b50202088aa8c595cc99b310e5a-Abstract-Conference.html": {
    "title": "Federated Fine-tuning of Large Language Models under Heterogeneous Tasks and Client Resources",
    "volume": "main",
    "abstract": "Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the \"buckets effect\" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving thousands of clients performing heterogeneous NLP tasks and client resources, our experiments validate the efficacy of FlexLoRA, with the federated global model achieving consistently better improvement over SOTA FL methods in downstream NLP task performance across various heterogeneous distributions. FlexLoRA's practicality is further underscored by our theoretical analysis and its seamless integration with existing LoRA-based FL methods, offering a path toward cross-device, privacy-preserving federated tuning for LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiamu Bai",
      "Daoyuan Chen",
      "Bingchen Qian",
      "Liuyi Yao",
      "Yaliang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1a22b912945fb7c0bdd079e792b31b6f-Abstract-Conference.html": {
    "title": "Interaction-Force Transport Gradient Flows",
    "volume": "main",
    "abstract": "This paper presents a new gradient flow dissipation geometry over non-negative and probability measures.This is motivated by a principled construction that combines the unbalanced optimal transport and interaction forces modeled by reproducing kernels. Using a precise connection between the Hellinger geometry and the maximum mean discrepancy (MMD), we propose the interaction-force transport (IFT) gradient flows and its spherical variant via an infimal convolution of the Wasserstein and spherical MMD tensors. We then develop a particle-based optimization algorithm based on the JKO-splitting scheme of the mass-preserving spherical IFT gradient flows. Finally, we provide both theoretical global exponential convergence guarantees and improved empirical simulation results for applying the IFT gradient flows to the sampling task of MMD-minimization. Furthermore, we prove that the spherical IFT gradient flow enjoys the best of both worlds by providing the global exponential convergence guarantee for both the MMD and KL energy",
    "checked": true,
    "id": "d409f0c785dc36be007e1d3f5b2f16183db3e4ce",
    "semantic_title": "interaction-force transport gradient flows",
    "citation_count": 6,
    "authors": [
      "Egor Gladin",
      "Pavel Dvurechenskii",
      "Alexander Mielke",
      "Jia-Jie Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1a63a6a092a95bd45f0237766ac878ba-Abstract-Conference.html": {
    "title": "TreeVI: Reparameterizable Tree-structured Variational Inference for Instance-level Correlation Capturing",
    "volume": "main",
    "abstract": "Mean-field variational inference (VI) is computationally scalable, but its highly-demanding independence requirement hinders it from being applied to wider scenarios. Although many VI methods that take correlation into account have been proposed, these methods generally are not scalable enough to capture the correlation among data instances, which often arises in applications with graph-structured data or explicit constraints. In this paper, we developed the Tree-structured Variational Inference (TreeVI), which uses a tree structure to capture the correlation of latent variables in the posterior distribution. We show that samples from the tree-structured posterior can be reparameterized efficiently and parallelly, making its training cost just 2 or 3 times that of VI under the mean-field assumption. To capture correlation with more complicated structure, the TreeVI is further extended to the multiple-tree case. Furthermore, we show that the underlying tree structure can be automatically learned from training data. With experiments on synthetic datasets, constrained clustering, user matching and link prediction, we demonstrate that the TreeVI is superior in capturing instance-level correlation in posteriors and enhancing the performance of downstream applications",
    "checked": true,
    "id": "06e4102ba149d124475b63d5afbe1c193668239d",
    "semantic_title": "treevi: reparameterizable tree-structured variational inference for instance-level correlation capturing",
    "citation_count": 1,
    "authors": [
      "Junxi Xiao",
      "Qinliang Su"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1a675d804f50509b8e21d0d3ca709d03-Abstract-Conference.html": {
    "title": "Optimal Parallelization of Boosting",
    "volume": "main",
    "abstract": "Recent works on the parallel complexity of Boosting have established strong lower bounds on the tradeoff between the number of training rounds $p$ and the total parallel work per round $t$.These works have also presented highly non-trivial parallel algorithms that shed light on different regions of this tradeoff.Despite these advancements, a significant gap persists between the theoretical lower bounds and the performance of these algorithms across much of the tradeoff space.In this work, we essentially close this gap by providing both improved lower bounds on the parallel complexity of weak-to-strong learners, and a parallel Boosting algorithm whose performance matches these bounds across the entire $p$ vs. $t$ compromise spectrum, up to logarithmic factors.Ultimately, this work settles the parallel complexity of Boosting algorithms that are nearly sample-optimal",
    "checked": true,
    "id": "9958e4d3a2539267a66e9dac8eebd870ac5a806e",
    "semantic_title": "optimal parallelization of boosting",
    "citation_count": 0,
    "authors": [
      "Arthur da Cunha",
      "Mikael Møller Høgsgaard",
      "Kasper Green Larsen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1a8189929f3d7bd6183718f42c3f4309-Abstract-Conference.html": {
    "title": "Catastrophic Goodhart: regularizing RLHF with KL divergence does not mitigate heavy-tailed reward misspecification",
    "volume": "main",
    "abstract": "When applying reinforcement learning from human feedback (RLHF), the reward is learned from data and, therefore, always has some error. It is common to mitigate this by regularizing the policy with KL divergence from a base model, with the hope that balancing reward with regularization will achieve desirable outcomes despite this reward misspecification. We show that when the reward function has light-tailed error, optimal policies under less restrictive KL penalties achieve arbitrarily high utility. However, if error is heavy-tailed, some policies obtain arbitrarily high reward despite achieving no more utility than the base model--a phenomenon we call catastrophic Goodhart. We adapt a discrete optimization method to measure the tails of reward models, finding that they are consistent with light-tailed error. However, the pervasiveness of heavy-tailed distributions in many real-world applications indicates that future sources of RL reward could have heavy-tailed error, increasing the likelihood of reward hacking even with KL regularization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Kwa",
      "Drake Thomas",
      "Adrià Garriga-Alonso"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1a8d295871250443f9747d239925b89d-Abstract-Conference.html": {
    "title": "Segmenting Watermarked Texts From Language Models",
    "volume": "main",
    "abstract": "Watermarking is a technique that involves embedding nearly unnoticeable statistical signals within generated content to help trace its source. This work focuses on a scenario where an untrusted third-party user sends prompts to a trusted language model (LLM) provider, who then generates a text from their LLM with a watermark. This setup makes it possible for a detector to later identify the source of the text if the user publishes it. The user can modify the generated text by substitutions, insertions, or deletions. Our objective is to develop a statistical method to detect if a published text is LLM-generated from the perspective of a detector. We further propose a methodology to segment the published text into watermarked and non-watermarked sub-strings. The proposed approach is built upon randomization tests and change point detection techniques. We demonstrate that our method ensures Type I and Type II error control and can accurately identify watermarked sub-strings by finding the corresponding change point locations. To validate our technique, we apply it to texts generated by several language models with prompts extracted from Google's C4 dataset and obtain encouraging numerical results. We release all code publicly at https://github.com/doccstat/llm-watermark-cpd",
    "checked": true,
    "id": "82afcfad1c3bbefbbe40b29777e9a94e4070c27b",
    "semantic_title": "segmenting watermarked texts from language models",
    "citation_count": 0,
    "authors": [
      "Xingchi Li",
      "Guanxun Li",
      "Xianyang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1a958c6e813989a0d4b677e6a6f3339a-Abstract-Conference.html": {
    "title": "Data-Efficient Learning with Neural Programs",
    "volume": "main",
    "abstract": "Many computational tasks can be naturally expressed as a composition of a DNN followed by a program written in a traditional programming language or an API call to an LLM. We call such composites \"neural programs\" and focus on the problem of learning the DNN parameters when the training data consist of end-to-end input-output labels for the composite. When the program is written in a differentiable logic programming language, techniques from neurosymbolic learning are applicable, but in general, the learning for neural programs requires estimating the gradients of black-box components. We present an algorithm for learning neural programs, called ISED, that only relies on input-output samples of black-box components. For evaluation, we introduce new benchmarks that involve calls to modern LLMs such as GPT-4 and also consider benchmarks from the neurosymbolic learning literature. Our evaluation shows that for the latter benchmarks, ISED has comparable performance to state-of-the-art neurosymbolic frameworks. For the former, we use adaptations of prior work on gradient approximations of black-box components as a baseline, and show that ISED achieves comparable accuracy but in a more data- and sample-efficient manner",
    "checked": true,
    "id": "078736ea36dbba288860ec2441d34d30411735c1",
    "semantic_title": "data-efficient learning with neural programs",
    "citation_count": 6,
    "authors": [
      "Alaia Solko-Breslin",
      "Seewon Choi",
      "Ziyang Li",
      "Neelay Velingker",
      "Rajeev Alur",
      "Mayur Naik",
      "Eric Wong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1a970a3e62ac31c76ec3cea3a9f68fdf-Abstract-Conference.html": {
    "title": "Language Models as Hierarchy Encoders",
    "volume": "main",
    "abstract": "Interpreting hierarchical structures latent in language is a key limitation of current language models (LMs). While previous research has implicitly leveraged these hierarchies to enhance LMs, approaches for their explicit encoding are yet to be explored. To address this, we introduce a novel approach to re-train transformer encoder-based LMs as Hierarchy Transformer encoders (HiTs), harnessing the expansive nature of hyperbolic space. Our method situates the output embedding space of pre-trained LMs within a Poincaré ball with a curvature that adapts to the embedding dimension, followed by re-training on hyperbolic clustering and centripetal losses. These losses are designed to effectively cluster related entities (input as texts) and organise them hierarchically. We evaluate HiTs against pre-trained LMs, standard fine-tuned LMs, and several hyperbolic embedding baselines, focusing on their capabilities in simulating transitive inference, predicting subsumptions, and transferring knowledge across hierarchies. The results demonstrate that HiTs consistently outperform all baselines in these tasks, underscoring the effectiveness and transferability of our re-trained hierarchy encoders",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan He",
      "Moy Yuan",
      "Jiaoyan Chen",
      "Ian Horrocks"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1abed6ee581b9ceb4e2ddf37822c7fcb-Abstract-Conference.html": {
    "title": "Unconditional stability of a recurrent neural circuit implementing divisive normalization",
    "volume": "main",
    "abstract": "Stability in recurrent neural models poses a significant challenge, particularly in developing biologically plausible neurodynamical models that can be seamlessly trained. Traditional cortical circuit models are notoriously difficult to train due to expansive nonlinearities in the dynamical system, leading to an optimization problem with nonlinear stability constraints that are difficult to impose. Conversely, recurrent neural networks (RNNs) excel in tasks involving sequential data but lack biological plausibility and interpretability. In this work, we address these challenges by linking dynamic divisive normalization (DN) to the stability of \"oscillatory recurrent gated neural integrator circuits'' (ORGaNICs), a biologically plausible recurrent cortical circuit model that dynamically achieves DN and that has been shown to simulate a wide range of neurophysiological phenomena. By using the indirect method of Lyapunov, we prove the remarkable property of unconditional local stability for an arbitrary-dimensional ORGaNICs circuit when the recurrent weight matrix is the identity. We thus connect ORGaNICs to a system of coupled damped harmonic oscillators, which enables us to derive the circuit's energy function, providing a normative principle of what the circuit, and individual neurons, aim to accomplish. Further, for a generic recurrent weight matrix, we prove the stability of the 2D model and demonstrate empirically that stability holds in higher dimensions. Finally, we show that ORGaNICs can be trained by backpropagation through time without gradient clipping/scaling, thanks to its intrinsic stability property and adaptive time constants, which address the problems of exploding, vanishing, and oscillating gradients. By evaluating the model's performance on RNN benchmarks, we find that ORGaNICs outperform alternative neurodynamical models on static image classification tasks and perform comparably to LSTMs on sequential tasks",
    "checked": true,
    "id": "ccdcb80c6e6b3b49d3ff34e7430795b7ceddb8be",
    "semantic_title": "unconditional stability of a recurrent neural circuit implementing divisive normalization",
    "citation_count": 2,
    "authors": [
      "Shivang Rawat",
      "David Heeger",
      "Stefano Martiniani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1ac14e44228aeadabb3c934822c1212a-Abstract-Conference.html": {
    "title": "G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training",
    "volume": "main",
    "abstract": "Medical imaging tasks require an understanding of subtle and localized visual features due to the inherently detailed and area-specific nature of pathological patterns, which are crucial for clinical diagnosis. Although recent advances in medical vision-language pre-training (VLP) enable models to learn clinically relevant visual features by leveraging both medical images and their associated radiology reports, current medical VLP methods primarily focus on aligning images with entire reports. This focus hinders the learning of dense (pixel-level) visual features and is suboptimal for dense prediction tasks (e.g., medical image segmentation).To address this challenge, we propose a novel medical VLP framework, named Global to Dense level representation learning (G2D), which aims to learn global and dense visual features simultaneously using only image-text pairs without extra annotations. In particular, G2D designs a Pseudo Segmentation (PS) task, which enables the model to learn dense visual features during VLP. Notably, generating PS masks can be performed on the fly during VLP, which does not incur extra trainable parameters. With this simple yet effective idea, G2D achieves superior performance across 5 medical imaging tasks and 25 diseases. Particularly, in the segmentation task which requires dense visual features, G2D surpasses existing models even with just 1% of the training data for finetuning, compared to 100% used by other models. The code can be found in https://github.com/cheliu-computation/G2D-NeurIPS24/tree/main",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Che Liu",
      "Cheng Ouyang",
      "Sibo Cheng",
      "Anand Shah",
      "Wenjia Bai",
      "Rossella Arcucci"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1ac3030fc57850b0fb11dfe9d4880ad7-Abstract-Conference.html": {
    "title": "On the Role of Attention Masks and LayerNorm in Transformers",
    "volume": "main",
    "abstract": "Self-attention is the key mechanism of transformers, which are the essential building blocks of modern foundation models. Recent studies have shown that pure self-attention suffers from an increasing degree of rank collapse as depth increases, limiting model expressivity and further utilization of model depth. The existing literature on rank collapse, however, has mostly overlooked other critical components in transformers that may alleviate the rank collapse issue. In this paper, we provide a general analysis of rank collapse under self-attention, taking into account the effects of attention masks and layer normalization (LayerNorm). In particular, we find that although pure masked attention still suffers from exponential collapse to a rank one subspace, sparse or local masked attention can provably slow down the collapse rate. In the case of self-attention with LayerNorm, we first show that for certain classes of value matrices, collapse to a rank one subspace still happens exponentially. However, through construction of nontrivial counterexamples, we then establish that with proper choice of value matrices, a general class of sequences may not converge to a rank one subspace, and the self-attention dynamics with LayerNorm can simultaneously possess a rich set of equilibria with any possible rank between one and full. Our result refutes the previous hypothesis that LayerNorm plays no role in the rank collapse of self-attention and suggests that self-attention with LayerNorm constitutes a much more expressive, versatile nonlinear dynamical system than what was originally thought",
    "checked": true,
    "id": "35c991b167708b7a76bc8bb2ed1eea2cb1a4f11b",
    "semantic_title": "on the role of attention masks and layernorm in transformers",
    "citation_count": 14,
    "authors": [
      "Xinyi Wu",
      "Amir Ajorlou",
      "Yifei Wang",
      "Stefanie Jegelka",
      "Ali Jadbabaie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1ac83203e88eb6cf6b30642f0239b932-Abstract-Conference.html": {
    "title": "Directional Smoothness and Gradient Methods: Convergence and Adaptivity",
    "volume": "main",
    "abstract": "We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization, rather than on global, worst-case constants. Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective. Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes. For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness. Experiments on logistic regression show our convergence guarantees are tighter than the classical theory based on $L$-smoothness",
    "checked": true,
    "id": "4c8ca28c83cce512e22b78221721d7242d2c1f19",
    "semantic_title": "directional smoothness and gradient methods: convergence and adaptivity",
    "citation_count": 9,
    "authors": [
      "Aaron Mishkin",
      "Ahmed Khaled",
      "Yuanhao Wang",
      "Aaron Defazio",
      "Robert Gower"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1ae3fa230adc0c9ac6a81f4b88dcc7ff-Abstract-Conference.html": {
    "title": "The Implicit Bias of Heterogeneity towards Invariance: A Study of Multi-Environment Matrix Sensing",
    "volume": "main",
    "abstract": "Models are expected to engage in invariance learning, which involves distinguishing the core relations that remain consistent across varying environments to ensure the predictions are safe, robust and fair. While existing works consider specific algorithms to realize invariance learning, we show that model has the potential to learn invariance through standard training procedures. In other words, this paper studies the implicit bias of Stochastic Gradient Descent (SGD) over heterogeneous data and shows that the implicit bias drives the model learning towards an invariant solution. We call the phenomenon the implicit invariance learning. Specifically, we theoretically investigate the multi-environment low-rank matrix sensing problem where in each environment, the signal comprises (i) a lower-rank invariant part shared across all environments; and (ii) a significantly varying environment-dependent spurious component. The key insight is, through simply employing the large step size large-batch SGD sequentially in each environment without any explicit regularization, the oscillation caused by heterogeneity can provably prevent model learning spurious signals. The model reaches the invariant solution after certain iterations. In contrast, model learned using pooled SGD over all data would simultaneously learn both the invariant and spurious signals. Overall, we unveil another implicit bias that is a result of the symbiosis between the heterogeneity of data and modern algorithms, which is, to the best of our knowledge, first in the literature",
    "checked": true,
    "id": "894d61ba6cd59ca00fd8b8bc82f19ff4e16be556",
    "semantic_title": "the implicit bias of heterogeneity towards invariance: a study of multi-environment matrix sensing",
    "citation_count": 0,
    "authors": [
      "Yang Xu",
      "Yihong Gu",
      "Cong Fang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1af3e0bf5905e33789979f666c31192d-Abstract-Conference.html": {
    "title": "MemVLT: Vision-Language Tracking with Adaptive Memory-based Prompts",
    "volume": "main",
    "abstract": "Vision-language tracking (VLT) enhances traditional visual object tracking by integrating language descriptions, requiring the tracker to flexibly understand complex and diverse text in addition to visual information. However, most existing vision-language trackers still overly rely on initial fixed multimodal prompts, which struggle to provide effective guidance for dynamically changing targets. Fortunately, the Complementary Learning Systems (CLS) theory suggests that the human memory system can dynamically store and utilize multimodal perceptual information, thereby adapting to new scenarios. Inspired by this, (i) we propose a Memory-based Vision-Language Tracker (MemVLT). By incorporating memory modeling to adjust static prompts, our approach can provide adaptive prompts for tracking guidance. (ii) Specifically, the memory storage and memory interaction modules are designed in accordance with CLS theory. These modules facilitate the storage and flexible interaction between short-term and long-term memories, generating prompts that adapt to target variations. (iii) Finally, we conduct extensive experiments on mainstream VLT datasets (e.g., MGIT, TNL2K, LaSOT and LaSOT$_{ext}$). Experimental results show that MemVLT achieves new state-of-the-art performance. Impressively, it achieves 69.4% AUC on the MGIT and 63.3% AUC on the TNL2K, improving the existing best result by 8.4% and 4.7%, respectively",
    "checked": true,
    "id": "ade025ebf760382205acfd9cd6a4646e9f785bf0",
    "semantic_title": "memvlt: vision-language tracking with adaptive memory-based prompts",
    "citation_count": 11,
    "authors": [
      "Xiaokun Feng",
      "Xuchen Li",
      "Shiyu Hu",
      "Dailing Zhang",
      "wu meiqi",
      "Jing Zhang",
      "Xiaotang Chen",
      "Kaiqi Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1b0da24d136f46bfaee78e8da907127e-Abstract-Conference.html": {
    "title": "Towards Universal Mesh Movement Networks",
    "volume": "main",
    "abstract": "Solving complex Partial Differential Equations (PDEs) accurately and efficiently is an essential and challenging problem in all scientific and engineering disciplines. Mesh movement methods provide the capability to improve the accuracy of the numerical solution without increasing the overall mesh degree of freedom count. Conventional sophisticated mesh movement methods are extremely expensive and struggle to handle scenarios with complex boundary geometries. However, existing learning-based methods require re-training from scratch given a different PDE type or boundary geometry, which limits their applicability, and also often suffer from robustness issues in the form of inverted elements. In this paper, we introduce the Universal Mesh Movement Network (UM2N), which -- once trained -- can be applied in a non-intrusive, zero-shot manner to move meshes with different size distributions and structures, for solvers applicable to different PDE types and boundary geometries. UM2N consists of a Graph Transformer (GT) encoder for extracting features and a Graph Attention Network (GAT) based decoder for moving the mesh. We evaluate our method on advection and Navier-Stokes based examples, as well as a real-world tsunami simulation case. Our method out-performs existing learning-based mesh movement methods in terms of the benchmarks described above. In comparison to the conventional sophisticated Monge-Ampère PDE-solver based method, our approach not only significantly accelerates mesh movement, but also proves effective in scenarios where the conventional method fails. Our project page can be found at https://erizmr.github.io/UM2N/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingrui Zhang",
      "Chunyang Wang",
      "Stephan C. Kramer",
      "Joseph G Wallwork",
      "Siyi Li",
      "Jiancheng Liu",
      "Xiang Chen",
      "Matthew Piggott"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1b10264c77a2a1e0ef8abfbd68d36583-Abstract-Conference.html": {
    "title": "Natural Counterfactuals With Necessary Backtracking",
    "volume": "main",
    "abstract": "Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires too much deviation from the observed scenarios to be feasible, as we show using simple examples. To mitigate this difficulty, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are more feasible with respect to the actual data distribution. Our methodology incorporates a certain amount of backtracking when needed, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. Specifically, we introduce a novel optimization framework that permits but also controls the extent of backtracking with a \"naturalness'' criterion. Empirical experiments demonstrate the effectiveness of our method. The code is available at https://github.com/GuangyuanHao/natural_counterfactuals",
    "checked": true,
    "id": "f66f7bfb3ca519dfa2deebd088c02b4c78011d80",
    "semantic_title": "natural counterfactuals with necessary backtracking",
    "citation_count": 0,
    "authors": [
      "GUANG-YUAN HAO",
      "Jiji Zhang",
      "Biwei Huang",
      "Hao Wang",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1b2df10d5bc3ca563339c801fa2e14db-Abstract-Conference.html": {
    "title": "BLAST: Block-Level Adaptive Structured Matrices for Efficient Deep Neural Network Inference",
    "volume": "main",
    "abstract": "Large-scale foundation models have demonstrated exceptional performance in language and vision tasks. However, the numerous dense matrix-vector operations involved in these large networks pose significant computational challenges during inference. To address these challenges, we introduce the Block-Level Adaptive STructured (BLAST) matrix, designed to learn and leverage efficient structures prevalent in the weight matrices of linear layers within deep learning models. Compared to existing structured matrices, the BLAST matrix offers substantial flexibility, as it can represent various types of structures that are either learned from data or computed from pre-existing weight matrices. We demonstrate the efficiency of using the BLAST matrix for compressing both language and vision tasks, showing that (i) for medium-sized models such as ViT and GPT-2, training with BLAST weights boosts performance while reducing complexity by 70\\% and 40\\%, respectively; and (ii) for large foundation models such as Llama-7B and DiT-XL, the BLAST matrix achieves a 2x compression while exhibiting the lowest performance degradation among all tested structured matrices. Our code is available at https://github.com/changwoolee/BLAST",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changwoo Lee",
      "Soo Min Kwon",
      "Qing Qu",
      "Hun-Seok Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1b33deb9e1e7c31f05300b1c2d4a4f7d-Abstract-Conference.html": {
    "title": "CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning",
    "volume": "main",
    "abstract": "Data selection has emerged as a core issue for large-scale visual-language model pretaining (e.g., CLIP), particularly with noisy web-curated datasets. Three main data selection approaches are: (1) leveraging external non-CLIP models to aid data selection, (2) training new CLIP-style embedding models that are more effective at selecting high-quality data than the original OpenAI CLIP model, and (3) designing better metrics or strategies universally applicable to any CLIP embedding without requiring specific model properties (e.g., CLIPScore is one popular metric). While the first two approaches have been extensively studied, the third remains under-explored. In this paper, we advance the third approach by proposing two new methods. Firstly, instead of classical CLIP scores that only consider the alignment between two modalities from a single sample, we introduce $\\textbf{negCLIPLoss}$, a method inspired by CLIP training loss that adds the alignment between one sample and its contrastive pairs as an extra normalization term to CLIPScore for better quality measurement. Secondly, when downstream tasks are known, we propose a new norm-based metric, $\\textbf{NormSim}$, to measure the similarity between pretraining data and target data. We test our methods on the data selection benchmark, DataComp [Gadre et al., 2023]. Compared to the best baseline using only OpenAI's CLIP-L/14, our methods achieve a 5.3\\% improvement on ImageNet-1k and a 2.8\\% improvement on 38 downstream evaluation tasks. Moreover, both $\\textbf{negCLIPLoss}$ and $\\textbf{NormSim}$ are compatible with existing techniques. By combining our methods with the current best methods DFN [Fang et al., 2023] and HYPE [Kim et al., 2024], we can boost average performance on downstream tasks by 0.9\\%, achieving a new state-of-the-art on the DataComp-medium benchmark",
    "checked": true,
    "id": "f0f4801449505330ffdffb7b5f3813a1a11add31",
    "semantic_title": "cliploss and norm-based data selection methods for multimodal contrastive learning",
    "citation_count": 10,
    "authors": [
      "Yiping Wang",
      "Yifang Chen",
      "Wendan Yan",
      "Alex Fang",
      "Wenjing Zhou",
      "Kevin G. Jamieson",
      "Simon S Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1b3750390ca8b931fb9ca988647940cb-Abstract-Conference.html": {
    "title": "ReLIZO: Sample Reusable Linear Interpolation-based Zeroth-order Optimization",
    "volume": "main",
    "abstract": "Gradient estimation is critical in zeroth-order optimization methods, which aims to obtain the descent direction by sampling update directions and querying function evaluations. Extensive research has been conducted including smoothing and linear interpolation. The former methods smooth the objective function, causing a biased gradient estimation, while the latter often enjoys more accurate estimates, at the cost of large amounts of samples and queries at each iteration to update variables. This paper resorts to the linear interpolation strategy and proposes to reduce the complexity of gradient estimation by reusing queries in the prior iterations while maintaining the sample size unchanged. Specifically, we model the gradient estimation as a quadratically constrained linear program problem and manage to derive the analytical solution. It innovatively decouples the required sample size from the variable dimension without extra conditions required, making it able to leverage the queries in the prior iterations. Moreover, part of the intermediate variables that contribute to the gradient estimation can be directly indexed, significantly reducing the computation complexity. Experiments on both simulation functions and real scenarios (black-box adversarial attacks neural architecture search, and parameter-efficient fine-tuning for large language models), show its efficacy and efficiency. Our code is available at https://github.com/Thinklab-SJTU/ReLIZO.git",
    "checked": true,
    "id": "795d0abbb612d68c30c34958afbacb53a4de4e7e",
    "semantic_title": "relizo: sample reusable linear interpolation-based zeroth-order optimization",
    "citation_count": 5,
    "authors": [
      "Xiaoxing Wang",
      "Xiaohan Qin",
      "Xiaokang Yang",
      "Junchi Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1b57aaddf85ab01a2445a79c9edc1f4b-Abstract-Conference.html": {
    "title": "Entity Alignment with Noisy Annotations from Large Language Models",
    "volume": "main",
    "abstract": "Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying equivalent entity pairs. While existing methods heavily rely on human-generated labels, it is prohibitively expensive to incorporate cross-domain experts for annotation in real-world scenarios. The advent of Large Language Models (LLMs) presents new avenues for automating EA with annotations, inspired by their comprehensive capability to process semantic information. However, it is nontrivial to directly apply LLMs for EA since the annotation space in real-world KGs is large. LLMs could also generate noisy labels that may mislead the alignment. To this end, we propose a unified framework, LLM4EA, to effectively leverage LLMs for EA. Specifically, we design a novel active learning policy to significantly reduce the annotation space by prioritizing the most valuable entities based on the entire inter-KG and intra-KG structure. Moreover, we introduce an unsupervised label refiner to continuously enhance label accuracy through in-depth probabilistic reasoning. We iteratively optimize the policy based on the feedback from a base EA model. Extensive experiments demonstrate the advantages of LLM4EA on four benchmark datasets in terms of effectiveness, robustness, and efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyuan Chen",
      "Qinggang Zhang",
      "Junnan Dong",
      "Wen Hua",
      "Qing Li",
      "Xiao Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1b60893887328a6a50dd00ae0d5ed51a-Abstract-Conference.html": {
    "title": "How Diffusion Models Learn to Factorize and Compose",
    "volume": "main",
    "abstract": "Diffusion models are capable of generating photo-realistic images that combine elements which do not appear together in natural images, demonstrating their ability to compositionally generalize. Nonetheless, the precise mechanism of compositionality and how it is acquired through training remains elusive. Here, we consider a highly reduced setting to examine whether diffusion models learn semantically meaningful and fully factorized representations of composable features. We performed extensive controlled experiments on conditional DDPMs trained to generate various forms of 2D Gaussian data. We demonstrate that the models learn factorized, semi-continuous manifold representations that are orthogonal in underlying continuous latent features of independent variations but are not aligned for different values of the same feature. With such representations, models demonstrate superior compositionality but have limited ability to interpolate over unseen values of a given feature. Our experimental results further demonstrate that diffusion models can attain compositionality with a small amount of compositional examples, suggesting a novel way to train DDPMs. Finally, we connect manifold formation in diffusion models to percolation theory in physics, thereby offering insights into the sudden onset of factorized representation learning. Our thorough toy experiments thus contribute a deeper understanding of how diffusion models capture compositional structure in data, paving the way for future research aimed at enhancing factorization and compositional generalization in generative models for real-world applications",
    "checked": true,
    "id": "0c8f2de8283370c15cf9e96feba10ac83a306be4",
    "semantic_title": "how diffusion models learn to factorize and compose",
    "citation_count": 10,
    "authors": [
      "Qiyao Liang",
      "Ziming Liu",
      "Mitchell Ostrow",
      "Ila Fiete"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1b6811b37b0d9fd49a8fefd288810a94-Abstract-Conference.html": {
    "title": "Learning to Mitigate Externalities: the Coase Theorem with Hindsight Rationality",
    "volume": "main",
    "abstract": "In Economics, the concept of externality refers to any indirect effect resulting from an interaction between players and affecting a third party without compensation. Most of the models within which externality has been studied assume that agents have perfect knowledge of their environment and preferences. This is a major hindrance to the practical implementation of many proposed solutions. To adress this issue, we consider a two-players bandit game setting where the actions of one of the player affect the other one. Building upon this setup, we extend the Coase theorem [Coase, 2013], which suggests that the optimal approach for maximizing the social welfare in the presence of externality is to establish property rights, i.e., enabling transfers and bargaining between the players. Nonetheless, this fundamental result relies on the assumption that bargainers possess perfect knowledge of the underlying game. We first demonstrate that in the absence of property rights in the considered online scenario, the social welfare breaks down. We then provide a policy for the players, which allows them to learn a bargaining strategy which maximizes the total welfare, recovering the Coase theorem under uncertainty",
    "checked": true,
    "id": "5b82f83dd7abb13a1381f83b7597c9a09b18a497",
    "semantic_title": "learning to mitigate externalities: the coase theorem with hindsight rationality",
    "citation_count": 5,
    "authors": [
      "Antoine Scheid",
      "Aymeric Capitaine",
      "Etienne Boursier",
      "Eric Moulines",
      "Michael I. Jordan",
      "Alain Durmus"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1b96f01343ff10150e6719eb163e1536-Abstract-Conference.html": {
    "title": "Lower Bounds of Uniform Stability in Gradient-Based Bilevel Algorithms for Hyperparameter Optimization",
    "volume": "main",
    "abstract": "Gradient-based bilevel programming leverages unrolling differentiation (UD) or implicit function theorem (IFT) to solve hyperparameter optimization (HO) problems, and is proven effective and scalable in practice. To understand their generalization behavior, existing works establish upper bounds on the uniform stability of these algorithms, while their tightness is still unclear. To this end, this paper attempts to establish stability lower bounds for UD-based and IFT-based algorithms. A central technical challenge arises from the dependency of each outer-level update on the concurrent stage of inner optimization in bilevel programming. To address this problem, we introduce lower-bounded expansion properties to characterize the instability in update rules which can serve as general tools for lower-bound analysis. These properties guarantee the hyperparameter divergence at the outer level and the Lipschitz constant of inner output at the inner level in the context of HO.Guided by these insights, we construct a quadratic example that yields tight lower bounds for the UD-based algorithm and meaningful bounds for a representative IFT-based algorithm.Our tight result indicates that uniform stability has reached its limit in stability analysis for the UD-based algorithm",
    "checked": true,
    "id": "369b8be167538e24c3ed22d7f10a20af18126e30",
    "semantic_title": "lower bounds of uniform stability in gradient-based bilevel algorithms for hyperparameter optimization",
    "citation_count": 0,
    "authors": [
      "Rongzhen Wang",
      "Chenyu Zheng",
      "Guoqiang Wu",
      "Xu Min",
      "Xiaolu Zhang",
      "Jun Zhou",
      "Chongxuan LI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1b99db17b54735d22dbed15c24f2dbdc-Abstract-Conference.html": {
    "title": "Improving self-training under distribution shifts via anchored confidence with theoretical guarantees",
    "volume": "main",
    "abstract": "Self-training often falls short under distribution shifts due to an increased discrepancy between prediction confidence and actual accuracy. This typically necessitates computationally demanding methods such as neighborhood or ensemble-based label corrections. Drawing inspiration from insights on early learning regularization, we develop a principled method to improve self-training under distribution shifts based on temporal consistency. Specifically, we build an uncertainty-aware temporal ensemble with a simple relative thresholding. Then, this ensemble smooths noisy pseudo labels to promote selective temporal consistency. We show that our temporal ensemble is asymptotically correct and our label smoothing technique can reduce the optimality gap of self-training. Our extensive experiments validate that our approach consistently improves self-training performances by 8% to 16% across diverse distribution shift scenarios without a computational overhead. Besides, our method exhibits attractive properties, such as improved calibration performance and robustness to different hyperparameter choices",
    "checked": true,
    "id": "dcb5a455c781c87a379c5dad15bb6129a0846688",
    "semantic_title": "improving self-training under distribution shifts via anchored confidence with theoretical guarantees",
    "citation_count": 0,
    "authors": [
      "Taejong Joo",
      "Diego Klabjan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1bbfea488a8968e2d3c6565639b08e5e-Abstract-Conference.html": {
    "title": "4Diffusion: Multi-view Video Diffusion Model for 4D Generation",
    "volume": "main",
    "abstract": "Current 4D generation methods have achieved noteworthy efficacy with the aid of advanced diffusion generative models. However, these methods lack multi-view spatial-temporal modeling and encounter challenges in integrating diverse prior knowledge from multiple diffusion models, resulting in inconsistent temporal appearance and flickers. In this paper, we propose a novel 4D generation pipeline, namely $\\textbf{4Diffusion}$, aimed at generating spatial-temporally consistent 4D content from a monocular video. We first design a unified diffusion model tailored for multi-view video generation by incorporating a learnable motion module into a frozen 3D-aware diffusion model to capture multi-view spatial-temporal correlations. After training on a curated dataset, our diffusion model acquires reasonable temporal consistency and inherently preserves the generalizability and spatial consistency of the 3D-aware diffusion model. Subsequently, we propose 4D-aware Score Distillation Sampling loss, which is based on our multi-view video diffusion model, to optimize 4D representation parameterized by dynamic NeRF. This aims to eliminate discrepancies arising from multiple diffusion models, allowing for generating spatial-temporally consistent 4D content. Moreover, we devise an anchor loss to enhance the appearance details and facilitate the learning of dynamic NeRF. Extensive qualitative and quantitative experiments demonstrate that our method achieves superior performance compared to previous methods",
    "checked": true,
    "id": "1b412ae5392172c5e6e017481b29f6afd9d3ef8a",
    "semantic_title": "4diffusion: multi-view video diffusion model for 4d generation",
    "citation_count": 47,
    "authors": [
      "Haiyu Zhang",
      "Xinyuan Chen",
      "Yaohui WANG",
      "Xihui Liu",
      "Yunhong Wang",
      "Yu Qiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1bd359b32ab8b2a6bbafa1ed2856cf40-Abstract-Conference.html": {
    "title": "How do Large Language Models Handle Multilingualism?",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across diverse languages. This study explores how LLMs handle multilingualism. Based on observed language ratio shifts among layers and the relationships between network structures and certain capabilities, we hypothesize the LLM's multilingual workflow ($\\texttt{MWork}$): LLMs initially understand the query, converting multilingual inputs into English for task-solving. In the intermediate layers, they employ English for thinking and incorporate multilingual knowledge with self-attention and feed-forward structures, respectively. In the final layers, LLMs generate responses aligned with the original language of the query. To verify $\\texttt{MWork}$, we introduce Parallel Language-specific Neuron Detection ($\\texttt{PLND}$) to identify activated neurons for inputs in different languages without any labeled data. Using $\\texttt{PLND}$, we validate $\\texttt{MWork}$ through extensive experiments involving the deactivation of language-specific neurons across various layers and structures. Moreover, $\\texttt{MWork}$ allows fine-tuning of language-specific neurons with a small dataset, enhancing multilingual abilities in a specific language without compromising others. This approach results in an average improvement of $3.6\\%$ for high-resource languages and $2.3\\%$ for low-resource languages across all tasks with just $400$ documents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiran Zhao",
      "Wenxuan Zhang",
      "Guizhen Chen",
      "Kenji Kawaguchi",
      "Lidong Bing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1bd8a2967508c5cf068ac204151b6ecd-Abstract-Conference.html": {
    "title": "Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn",
    "volume": "main",
    "abstract": "Deep neural networks provide Reinforcement Learning (RL) powerful function approximators to address large-scale decision-making problems. However, these approximators introduce challenges due to the non-stationary nature of RL training. One source of the challenges in RL is that output predictions can churn, leading to uncontrolled changes after each batch update for states not included in the batch. Although such a churn phenomenon exists in each step of network training, it remains under-explored on how churn occurs and impacts RL. In this work, we start by characterizing churn in a view of Generalized Policy Iteration with function approximation, and we discover a chain effect of churn that leads to a cycle where the churns in value estimation and policy improvement compound and bias the learning dynamics throughout the iteration. Further, we concretize the study and focus on the learning issues caused by the chain effect in different settings, including greedy action deviation in value-based methods, trust region violation in proximal policy optimization, and dual bias of policy value in actor-critic methods. We then propose a method to reduce the chain effect across different settings, called Churn Approximated ReductIoN (CHAIN), which can be easily plugged into most existing DRL algorithms. Our experiments demonstrate the effectiveness of our method in both reducing churn and improving learning performance across online and offline, value-based and policy-based RL settings",
    "checked": true,
    "id": "59184c4e52b3f5478bdad1be320dc24cd51f8c17",
    "semantic_title": "improving deep reinforcement learning by reducing the chain effect of value and policy churn",
    "citation_count": 5,
    "authors": [
      "Hongyao Tang",
      "Glen Berseth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1bf4cad47f5a54c98fbe7d10516ebf77-Abstract-Conference.html": {
    "title": "No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations",
    "volume": "main",
    "abstract": "This paper introduces FUNGI, Features from UNsupervised GradIents, a method to enhance the features of transformer encoders by leveraging self-supervised gradients. Our method is simple: given any pretrained model, we first compute gradients from various self-supervised objectives for each input. These gradients are projected to a lower dimension and then concatenated with the model's output embedding. The resulting features are evaluated on k-nearest neighbor classification over 11 datasets from vision, 5 from natural language processing, and 2 from audio. Across backbones spanning various sizes and pretraining strategies, FUNGI features provide consistent performance improvements over the embeddings. We also show that using FUNGI features can benefit linear classification, clustering and image retrieval, and that they significantly improve the retrieval-based in-context scene understanding abilities of pretrained models, for example improving upon DINO by +17% for semantic segmentation - without any training. Code is available at https://github.com/WalterSimoncini/fungivision",
    "checked": true,
    "id": "599e200c663956688c8488ba1746295f7b140638",
    "semantic_title": "no train, all gain: self-supervised gradients improve deep frozen representations",
    "citation_count": 2,
    "authors": [
      "Walter Simoncini",
      "Andrei Bursuc",
      "Spyridon Gidaris",
      "Yuki Asano"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1c0d54ebd0a6e58c4eca7d591e374b9d-Abstract-Conference.html": {
    "title": "ChatQA: Surpassing GPT-4 on Conversational QA and RAG",
    "volume": "main",
    "abstract": "In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on retrieval-augmented generation (RAG) and conversational question answering (QA). To enhance generation, we propose a two-stage instruction tuning method that significantly boosts the performance of RAG. For effective retrieval, we introduce a dense retriever optimized for conversational QA, which yields results comparable to the alternative state-of-the-art query rewriting models, while substantially reducing deployment costs. We also present the ChatRAG Bench, which encompasses ten datasets covering comprehensive evaluations on RAG, table-related QA, arithmetic calculations, and scenarios involving unanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score: 53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without relying on any synthetic data from OpenAI GPT models. Notably, Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09 by a margin. These results demonstrate the exceptional quality of the proposed ChatQA recipe. To advance research in this field, we open-sourced the model weights, instruction tuning data, ChatRAG Bench, and retriever for the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Liu",
      "Wei Ping",
      "Rajarshi Roy",
      "Peng Xu",
      "Chankyu Lee",
      "Mohammad Shoeybi",
      "Bryan Catanzaro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1c1b099a19621e4bd2753ac572e1dbd5-Abstract-Conference.html": {
    "title": "Improving Linear System Solvers for Hyperparameter Optimisation in Iterative Gaussian Processes",
    "volume": "main",
    "abstract": "Scaling hyperparameter optimisation to very large datasets remains an open problem in the Gaussian process community. This paper focuses on iterative methods, which use linear system solvers, like conjugate gradients, alternating projections or stochastic gradient descent, to construct an estimate of the marginal likelihood gradient. We discuss three key improvements which are applicable across solvers: (i) a pathwise gradient estimator, which reduces the required number of solver iterations and amortises the computational cost of making predictions, (ii) warm starting linear system solvers with the solution from the previous step, which leads to faster solver convergence at the cost of negligible bias, (iii) early stopping linear system solvers after a limited computational budget, which synergises with warm starting, allowing solver progress to accumulate over multiple marginal likelihood steps. These techniques provide speed-ups of up to $72\\times$ when solving to tolerance, and decrease the average residual norm by up to $7\\times$ when stopping early",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihao Andreas Lin",
      "Shreyas Padhy",
      "Bruno Mlodozeniec",
      "Javier Antorán",
      "José Miguel Hernández-Lobato"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1c2b1c8f7d317719a9ce32dd7386ba35-Abstract-Conference.html": {
    "title": "Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) has emerged as a pivotal technique for fine-tuning large language models (LLMs) on specific tasks. However, prevailing RL fine-tuning methods predominantly rely on PPO and its variants. Though these algorithms are effective in general RL settings, they often exhibit suboptimal performance and vulnerability to distribution collapse when applied to the fine-tuning of LLMs. In this paper, we propose CORY, extending the RL fine-tuning of LLMs to a sequential cooperative multi-agent reinforcement learning framework, to leverage the inherent coevolution and emergent capabilities of multi-agent systems. In CORY, the LLM to be fine-tuned is initially duplicated into two autonomous agents: a pioneer and an observer. The pioneer generates responses based on queries, while the observer generates responses using both the queries and the pioneer's responses. The two agents are trained together. During training, the agents exchange roles periodically, fostering cooperation and coevolution between them. Experiments evaluate CORY's performance by fine-tuning GPT-2 and Llama-2 under subjective and objective reward functions on the IMDB Review and GSM8K datasets, respectively. Results show that CORY outperforms PPO in terms of policy optimality, resistance to distribution collapse, and training robustness, thereby underscoring its potential as a superior methodology for refining LLMs in real-world applications",
    "checked": true,
    "id": "7d44d096ea5ce822a40ee7b5519fb12b65eeba4e",
    "semantic_title": "coevolving with the other you: fine-tuning llm with sequential cooperative multi-agent reinforcement learning",
    "citation_count": 9,
    "authors": [
      "Hao Ma",
      "Tianyi Hu",
      "Zhiqiang Pu",
      "Liu Boyin",
      "Xiaolin Ai",
      "Yanyan Liang",
      "Min Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1c32452f112719f7c1db6d983d060f78-Abstract-Conference.html": {
    "title": "CoBo: Collaborative Learning via Bilevel Optimization",
    "volume": "main",
    "abstract": "Collaborative learning is an important tool to train multiple clients more effectively by enabling communication among clients. Identifying helpful clients, however, presents challenging and often introduces significant overhead. In this paper, we model client-selection and model-training as two interconnected optimization problems, proposing a novel bilevel optimization problem for collaborative learning.We introduce CoBo, a scalable and elastic, SGD-type alternating optimization algorithm that efficiently addresses these problem with theoretical convergence guarantees. Empirically, CoBo achieves superior performance, surpassing popular personalization algorithms by 9.3% in accuracy on a task with high heterogeneity, involving datasets distributed among 80 clients",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diba Hashemi",
      "Lie He",
      "Martin Jaggi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1c562586c1cb1da0547e92a9612879bc-Abstract-Conference.html": {
    "title": "FasMe: Fast and Sample-efficient Meta Estimator for Precision Matrix Learning in Small Sample Settings",
    "volume": "main",
    "abstract": "Precision matrix estimation is a ubiquitous task featuring numerous applications such as rare disease diagnosis and neural connectivity exploration. However, this task becomes challenging in small sample settings, where the number of samples is significantly less than the number of dimensions, leading to unreliable estimates. Previous approaches either fail to perform well in small sample settings or suffer from inefficient estimation processes, even when incorporating meta-learning techniques.To this end, we propose a novel approach FasMe for Fast and Sample-efficient Meta Precision Matrix Learning, which first extracts meta-knowledge through a multi-task learning diagram. Then, meta-knowledge constraints are applied using a maximum determinant matrix completion algorithm for the novel task. As a result, we reduce the sample size requirements to $O(\\log p/K)$ per meta-training task and $O(\\log\\vert \\mathcal{G}\\vert)$ for the meta-testing task. Moreover, the hereby proposed model only needs $O(p \\log\\epsilon^{-1})$ time and $O(p)$ memory for converging to an $\\epsilon$-accurate solution. On multiple synthetic and biomedical datasets, FasMe is at least ten times faster than the four baselines while promoting prediction accuracy in small sample settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Tan",
      "Yiqin Wang",
      "Yangyang Shen",
      "Dian Shen",
      "Meng Wang",
      "Peibo Duan",
      "Beilun Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1c9c85bae6161d52182d0fe2f3640512-Abstract-Conference.html": {
    "title": "HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis",
    "volume": "main",
    "abstract": "Many structured prediction and reasoning tasks can be framed as program synthesis problems, where the goal is to generate a program in a \\emph{domain-specific language} (DSL) that transforms input data into the desired output. Unfortunately, purely neural approaches, such as large language models (LLMs), often fail to produce fully correct programs in unfamiliar DSLs, while purely symbolic methods based on combinatorial search scale poorly to complex problems. Motivated by these limitations, we introduce a hybrid approach, where LLM completions for a given task are used to learn a task-specific, context-free surrogate model, which is then used to guide program synthesis. We evaluate this hybrid approach on three domains, and show that it outperforms both unguided search and direct sampling from LLMs, as well as existing program synthesizers",
    "checked": true,
    "id": "5f8b4e2e8c337447bfbcf47044af4a1d5f75f41e",
    "semantic_title": "hysynth: context-free llm approximation for guiding program synthesis",
    "citation_count": 8,
    "authors": [
      "Shraddha Barke",
      "Emmanuel Anaya Gonzalez",
      "Saketh Ram Kasibatla",
      "Taylor Berg-Kirkpatrick",
      "Nadia Polikarpova"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1caf09c9f4e6b0150b06a07e77f2710c-Abstract-Conference.html": {
    "title": "MTGS: A Novel Framework for Multi-Person Temporal Gaze Following and Social Gaze Prediction",
    "volume": "main",
    "abstract": "Gaze following and social gaze prediction are fundamental tasks providing insights into human communication behaviors, intent, and social interactions. Most previous approaches addressed these tasks separately, either by designing highly specialized social gaze models that do not generalize to other social gaze tasks or by considering social gaze inference as an ad-hoc post-processing of the gaze following task. Furthermore, the vast majority of gaze following approaches have proposed models that can handle only one person at a time and are static, therefore failing to take advantage of social interactions and temporal dynamics. In this paper, we address these limitations and introduce a novel framework to jointly predict the gaze target and social gaze label for all people in the scene. It comprises (i) a temporal, transformer-based architecture that, in addition to frame tokens, handles person-specific tokens capturing the gaze information related to each individual; (ii) a new dataset, VSGaze, built from multiple gaze following and social gaze datasets by extending and validating head detections and tracks, and unifying annotation types. We demonstrate that our model can address and benefit from training on all tasks jointly, achieving state-of-the-art results for multi-person gaze following and social gaze prediction. Our annotations and code will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anshul Gupta",
      "Samy Tafasca",
      "Arya Farkhondeh",
      "Pierre Vuillecard",
      "Jean-marc Odobez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1cb57fcf7ff3f6d37eebae5becc9ea6d-Abstract-Conference.html": {
    "title": "Can Large Language Model Agents Simulate Human Trust Behavior?",
    "volume": "main",
    "abstract": "Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: can LLM agents really simulate human behavior? In this paper, we focus on one critical and elemental behavior in human interactions, trust, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior, indicating the feasibility of simulating human trust behavior with LLM agents. In addition, we probe the biases of agent trust and differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond value alignment. We further illustrate broader implications of our discoveries for applications where trust is paramount",
    "checked": false,
    "id": "775e9844a13b518aeb0cde401ba6891ba3538611",
    "semantic_title": "can large language model agents simulate human trust behaviors?",
    "citation_count": 61,
    "authors": [
      "Chengxing Xie",
      "Canyu Chen",
      "Feiran Jia",
      "Ziyu Ye",
      "Shiyang Lai",
      "Kai Shu",
      "Jindong Gu",
      "Adel Bibi",
      "Ziniu Hu",
      "David Jurgens",
      "James Evans",
      "Philip Torr",
      "Bernard Ghanem",
      "Guohao Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1cb5b3d64bdf3c6642c8d9a8fbecd019-Abstract-Conference.html": {
    "title": "Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control",
    "volume": "main",
    "abstract": "As the development and application of Large Language Models (LLMs) continue to advance rapidly, enhancing their trustworthiness and aligning them with human preferences has become a critical area of research. Traditional methods rely heavily on extensive data for Reinforcement Learning from Human Feedback (RLHF), but representation engineering offers a new, training-free approach. This technique leverages semantic features to control the representation of LLM's intermediate hidden states, enabling the model to meet specific requirements such as increased honesty or heightened safety awareness. However, a significant challenge arises when attempting to fulfill multiple requirements simultaneously. It proves difficult to encode various semantic contents, like honesty and safety, into a singular semantic feature, restricting its practicality.In this work, we address this challenge through Sparse Activation Control. By delving into the intrinsic mechanisms of LLMs, we manage to identify and pinpoint modules that are closely related to specific tasks within the model, i.e. attention heads. These heads display sparse characteristics that allow for near-independent control over different tasks. Our experiments, conducted on the open-source Llama series models, have yielded encouraging results. The models were able to align with human preferences on issues of safety, factualness, and bias concurrently",
    "checked": true,
    "id": "09992d02d809c9598c192277ec24c918849beca0",
    "semantic_title": "enhancing multiple dimensions of trustworthiness in llms via sparse activation control",
    "citation_count": 1,
    "authors": [
      "Yuxin Xiao",
      "Wan Chaoqun",
      "Yonggang Zhang",
      "Wenxiao Wang",
      "Binbin Lin",
      "Xiaofei He",
      "Xu Shen",
      "Jieping Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1cded4f97cf5f01a284c574110b7e3b9-Abstract-Conference.html": {
    "title": "Observational Scaling Laws and the Predictability of Langauge Model Performance",
    "volume": "main",
    "abstract": "Understanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different scales has limited their use. We propose an alternative, observational approach that bypasses model training and instead builds scaling laws from ~100 publically available models. Building a single scaling law from multiple model families is challenging due to large variations in their training compute efficiencies and capabilities. However, we show that these variations are consistent with a simple, generalized scaling law where language model performance is a function of a low-dimensional capability space, and model families only vary in their efficiency in converting training compute to capabilities. Using this approach, we show the surprising predictability of complex scaling phenomena: we show that several emergent phenomena follow a smooth, sigmoidal behavior and are predictable from small models; we show that the agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks; and we show how to predict the impact of post-training interventions like Chain-of-Thought and Self-Consistency as language model capabilities continue to improve",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangjun Ruan",
      "Chris J Maddison",
      "Tatsunori B Hashimoto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1ce12df021f8fb43cf531c556efcebdc-Abstract-Conference.html": {
    "title": "Embedding-Aligned Language Models",
    "volume": "main",
    "abstract": "We propose a novel approach for training large language models (LLMs) to adhere to objectives defined within a latent embedding space. Our method leverages reinforcement learning (RL), treating a pre-trained LLM as an environment. Our embedding-aligned guided language (EAGLE) agent is trained to iteratively steer the LLM's generation towards optimal regions of the latent embedding space, w.r.t. some predefined criterion. We demonstrate the effectiveness of the EAGLE agent using the MovieLens 25M and Amazon Review datasets to surface content gaps that satisfy latent user demand. We also demonstrate the benefit of using an optimal design of a state-dependent action set to improve EAGLE's efficiency. Our work paves the way for controlled and grounded text generation using LLMs, ensuring consistency with domain-specific knowledge and data representations",
    "checked": true,
    "id": "af5f53facaf8abbbeb3968798f96d5f496c74cc6",
    "semantic_title": "embedding-aligned language models",
    "citation_count": 3,
    "authors": [
      "Guy Tennenholtz",
      "Yinlam Chow",
      "Chih-wei Hsu",
      "Lior Shani",
      "Yi Liang",
      "Craig Boutilier"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1cf760a547822e2b8276881ad45f0fe9-Abstract-Conference.html": {
    "title": "How to Boost Any Loss Function",
    "volume": "main",
    "abstract": "Boosting is a highly successful ML-born optimization setting in which one is required to computationally efficiently learn arbitrarily good models based on the access to a weak learner oracle, providing classifiers performing at least slightly differently from random guessing. A key difference with gradient-based optimization is that boosting's original model does not requires access to first order information about a loss, yet the decades long history of boosting has quickly evolved it into a first order optimization setting -- sometimes even wrongfully *defining* it as such. Owing to recent progress extending gradient-based optimization to use only a loss' zeroth ($0^{th}$) order information to learn, this begs the question: what loss functions be efficiently optimized with boosting and what is the information really needed for boosting to meet the *original* boosting blueprint's requirements ?We provide a constructive formal answer essentially showing that *any* loss function can be optimized with boosting and thus boosting can achieve a feat not yet known to be possible in the classical $0^{th}$ order setting, since loss functions are not required to be be convex, nor differentiable or Lipschitz -- and in fact not required to be continuous either. Some tools we use are rooted in quantum calculus, the mathematical field -- not to be confounded with quantum computation -- that studies calculus without passing to the limit, and thus without using first order information",
    "checked": true,
    "id": "b502bbb7864b6ac7c9d983d3d145ad42e88f593a",
    "semantic_title": "how to boost any loss function",
    "citation_count": 0,
    "authors": [
      "Richard Nock",
      "Yishay Mansour"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1d051fb631f104cb2a621451f37676b9-Abstract-Conference.html": {
    "title": "Pearls from Pebbles: Improved Confidence Functions for Auto-labeling",
    "volume": "main",
    "abstract": "Auto-labeling is an important family of techniques that produce labeled training sets with minimum manual annotation. A prominent variant, threshold-based auto-labeling (TBAL), works by finding thresholds on a model's confidence scores above which it can accurately automatically label unlabeled data. However, many models are known to produce overconfident scores, leading to poor TBAL performance. While a natural idea is to apply off-the-shelf calibration methods to alleviate the overconfidence issue, we show that such methods fall short. Rather than experimenting with ad-hoc choices of confidence functions, we propose a framework for studying the optimal TBAL confidence function. We develop a tractable version of the framework to obtain Colander (Confidence functions for Efficient and Reliable Auto-labeling), a new post-hoc method specifically designed to maximize performance in TBAL systems. We perform an extensive empirical evaluation of Colander and compare it against methods designed for calibration. Colander achieves up to 60% improvement on coverage over the baselines while maintaining error level below 5% and using the same amount of labeled data",
    "checked": true,
    "id": "64766f0af337db522a590df0658f5fd4452b2582",
    "semantic_title": "pearls from pebbles: improved confidence functions for auto-labeling",
    "citation_count": 3,
    "authors": [
      "Harit Vishwakarma",
      "Yi Chen",
      "Sui Jiet Tay",
      "Satya Sai Srinath Namburi",
      "Frederic Sala",
      "Ramya Korlakai Vinayak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1d0815b056ee8bb243f936adb989e2aa-Abstract-Conference.html": {
    "title": "SHMT: Self-supervised Hierarchical Makeup Transfer via Latent Diffusion Models",
    "volume": "main",
    "abstract": "This paper studies the challenging task of makeup transfer, which aims to apply diverse makeup styles precisely and naturally to a given facial image. Due to the absence of paired data, current methods typically synthesize sub-optimal pseudo ground truths to guide the model training, resulting in low makeup fidelity. Additionally, different makeup styles generally have varying effects on the person face, but existing methods struggle to deal with this diversity. To address these issues, we propose a novel Self-supervised Hierarchical Makeup Transfer (SHMT) method via latent diffusion models. Following a \"decoupling-and-reconstruction\" paradigm, SHMT works in a self-supervised manner, freeing itself from the misguidance of imprecise pseudo-paired data. Furthermore, to accommodate a variety of makeup styles, hierarchical texture details are decomposed via a Laplacian pyramid and selectively introduced to the content representation. Finally, we design a novel Iterative Dual Alignment (IDA) module that dynamically adjusts the injection condition of the diffusion model, allowing the alignment errors caused by the domain gap between content and makeup representations to be corrected. Extensive quantitative and qualitative analyses demonstrate the effectiveness of our method. Our code is available at https://github.com/Snowfallingplum/SHMT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyang Sun",
      "Shengwu Xiong",
      "Yaxiong Chen",
      "Fei Du",
      "Weihua Chen",
      "Fan Wang",
      "Yi Rong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1d0bcb52067128f826c86db234280dce-Abstract-Conference.html": {
    "title": "Mixture of Link Predictors on Graphs",
    "volume": "main",
    "abstract": "Link prediction, which aims to forecast unseen connections in graphs, is a fundamental task in graph machine learning. Heuristic methods, leveraging a range of different pairwise measures such as common neighbors and shortest paths, often rival the performance of vanilla Graph Neural Networks (GNNs). Therefore, recent advancements in GNNs for link prediction (GNN4LP) have primarily focused on integrating one or a few types of pairwise information. In this work, we reveal that different node pairs within the same dataset necessitate varied pairwise information for accurate prediction and models that only apply the same pairwise information uniformly could achieve suboptimal performance.As a result, we propose a simple mixture of experts model Link-MoE for link prediction. Link-MoE utilizes various GNNs as experts and strategically selects the appropriate expert for each node pair based on various types of pairwise information. Experimental results across diverse real-world datasets demonstrate substantial performance improvement from Link-MoE. Notably, Link-Mo achieves a relative improvement of 18.71% on the MRR metric for the Pubmed dataset and 9.59% on the Hits@100 metric for the ogbl-ppa dataset, compared to the best baselines. The code is available at https://github.com/ml-ml/Link-MoE/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Ma",
      "Haoyu Han",
      "Juanhui Li",
      "Harry Shomer",
      "Hui Liu",
      "Xiaofeng Gao",
      "Jiliang Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1d0ed12c3fda52f2c241a0cebcf739a6-Abstract-Conference.html": {
    "title": "No Regrets: Investigating and Improving Regret Approximations for Curriculum Discovery",
    "volume": "main",
    "abstract": "What data or environments to use for training to improve downstream performance is a longstanding and very topical question in reinforcement learning. In particular, Unsupervised Environment Design (UED) methods have gained recent attention as their adaptive curricula promise to enable agents to be robust to in- and out-of-distribution tasks.This work investigates how existing UED methods select training environments, focusing on task prioritisation metrics.Surprisingly, despite methods aiming to maximise regret in theory, the practical approximations do not correlate with regret but with success rate.As a result, a significant portion of an agent's experience comes from environments it has already mastered, offering little to no contribution toward enhancing its abilities. Put differently, current methods fail to predict intuitive measures of learnability. Specifically, they are unable to consistently identify those scenarios that the agent can sometimes solve, but not always.Based on our analysis, we develop a method that directly trains on scenarios with high learnability. This simple and intuitive approach outperforms existing UED methods in several binary-outcome environments, including the standard domain of Minigrid and a novel setting closely inspired by a real-world robotics problem. We further introduce a new adversarial evaluation procedure for directly measuring robustness, closely mirroring the conditional value at risk (CVaR).We open-source all our code and present visualisations of final policies here: https://github.com/amacrutherford/sampling-for-learnability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Rutherford",
      "Michael Beukman",
      "Timon Willi",
      "Bruno Lacerda",
      "Nick Hawes",
      "Jakob Foerster"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1d10fe211f5139de49f94c6f0c7cecbe-Abstract-Conference.html": {
    "title": "Automated Efficient Estimation using Monte Carlo Efficient Influence Functions",
    "volume": "main",
    "abstract": "Many practical problems involve estimating low dimensional statistical quantities with high-dimensional models and datasets. Several approaches address these estimation tasks based on the theory of influence functions, such as debiased/double ML or targeted minimum loss estimation. We introduce \\textit{Monte Carlo Efficient Influence Functions} (MC-EIF), a fully automated technique for approximating efficient influence functions that integrates seamlessly with existing differentiable probabilistic programming systems. MC-EIF automates efficient statistical estimation for a broad class of models and functionals that previously required rigorous custom analysis. We prove that MC-EIF is consistent, and that estimators using MC-EIF achieve optimal $\\sqrt{N}$ convergence rates. We show empirically that estimators using MC-EIF are at parity with estimators using analytic EIFs. Finally, we present a novel capstone example using MC-EIF for optimal portfolio selection",
    "checked": true,
    "id": "a02bfbb39a4ad3258cf556724ab0fb59f1d6e65e",
    "semantic_title": "automated efficient estimation using monte carlo efficient influence functions",
    "citation_count": 3,
    "authors": [
      "Raj Agrawal",
      "Sam Witty",
      "Andy Zane",
      "Elias Bingham"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1d3591b6746204b332acb464b775d38d-Abstract-Conference.html": {
    "title": "AR-Pro: Counterfactual Explanations for Anomaly Repair with Formal Properties",
    "volume": "main",
    "abstract": "Anomaly detection is widely used for identifying critical errors and suspicious behaviors, but current methods lack interpretability.We leverage common properties of existing methods and recent advances in generative models to introduce counterfactual explanations for anomaly detection.Given an input, we generate its counterfactual as a diffusion-based repair that shows what a non-anomalous version $\\textit{should have looked like}$.A key advantage of this approach is that it enables a domain-independent formal specification of explainability desiderata, offering a unified framework for generating and evaluating explanations.We demonstrate the effectiveness of our anomaly explainability framework, AR-Pro, on vision (MVTec, VisA) and time-series (SWaT, WADI, HAI) anomaly datasets. The code used for the experiments is accessible at: https://github.com/xjiae/arpro",
    "checked": true,
    "id": "f9cc9fc89d5c4be856813cd13eacb5e81267b6cc",
    "semantic_title": "ar-pro: counterfactual explanations for anomaly repair with formal properties",
    "citation_count": 1,
    "authors": [
      "Xiayan Ji",
      "Anton Xue",
      "Eric Wong",
      "Oleg Sokolsky",
      "Insup Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1d35a777e932235b115645d5141e0342-Abstract-Conference.html": {
    "title": "Stability and Generalization of Adversarial Training for Shallow Neural Networks with Smooth Activation",
    "volume": "main",
    "abstract": "Adversarial training has emerged as a popular approach for training models that are robust to inference-time adversarial attacks. However, our theoretical understanding of why and when it works remains limited. Prior work has offered generalization analysis of adversarial training, but they are either restricted to the Neural Tangent Kernel (NTK) regime or they make restrictive assumptions about data such as (noisy) linear separability or robust realizability. In this work, we study the stability and generalization of adversarial training for two-layer networks without any data distribution assumptions and beyond the NTK regime. Our findings suggest that for networks with any given initialization and sufficiently large width, the generalization bound can be effectively controlled via early stopping. We further improve the generalization bound by leveraging smoothing using Moreau's envelope",
    "checked": true,
    "id": "4654869e73b9b9b4b6b70edf441cea7f8fb02a09",
    "semantic_title": "stability and generalization of adversarial training for shallow neural networks with smooth activation",
    "citation_count": 2,
    "authors": [
      "Kaibo Zhang",
      "Yunjuan Wang",
      "Raman Arora"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1d35af80e775e342f4cd3792e4405837-Abstract-Conference.html": {
    "title": "DPIC: Decoupling Prompt and Intrinsic Characteristics for LLM Generated Text Detection",
    "volume": "main",
    "abstract": "Large language models (LLMs) have the potential to generate texts that pose risks of misuse, such as plagiarism, planting fake reviews on e-commerce platforms, or creating inflammatory false tweets. Consequently, detecting whether a text is generated by LLMs has become increasingly important. Existing high-quality detection methods usually require access to the interior of the model to extract the intrinsic characteristics. However, since we do not have access to the interior of the black-box model, we must resort to surrogate models, which impacts detection quality. In order to achieve high-quality detection of black-box models, we would like to extract deep intrinsic characteristics of the black-box model generated texts. We view the generation process as a coupled process of prompt and intrinsic characteristics of the generative model. Based on this insight, we propose to decouple prompt and intrinsic characteristics (DPIC) for LLM-generated text detection method. Specifically, given a candidate text, DPIC employs an auxiliary LLM to reconstruct the prompt corresponding to the candidate text, then uses the prompt to regenerate text by the auxiliary LLM, which makes the candidate text and the regenerated text align with their prompts, respectively. Then, the similarity between the candidate text and the regenerated text is used as a detection feature, thus eliminating the prompt in the detection process, which allows the detector to focus on the intrinsic characteristics of the generative model. Compared to the baselines, DPIC has achieved an average improvement of 6.76\\% and 2.91\\% in detecting texts from different domains generated by GPT4 and Claude3, respectively",
    "checked": true,
    "id": "fad38bc34a1fb77175477757ae5bc7a7ca6aeed8",
    "semantic_title": "dpic: decoupling prompt and intrinsic characteristics for llm generated text detection",
    "citation_count": 13,
    "authors": [
      "XIAO YU",
      "Yuang Qi",
      "Kejiang Chen",
      "Guoqiang Chen",
      "Xi Yang",
      "Pengyuan Zhu",
      "Xiuwei Shang",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1d49235669869ab737c1da9d64b7c769-Abstract-Conference.html": {
    "title": "Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control",
    "volume": "main",
    "abstract": "Research on video generation has recently made tremendous progress, enabling high-quality videos to be generated from text prompts or images. Adding control to the video generation process is an important goal moving forward and recent approaches that condition video generation models on camera trajectories take an important step towards this goal. Yet, it remains challenging to generate a video of the same scene from multiple different camera trajectories. Solutions to this multi-video generation problem could enable large-scale 3D scene generation with editable camera trajectories, among other applications. We introduce collaborative video diffusion (CVD) as an important step towards this vision. The CVD framework includes a novel cross-video synchronization module that promotes consistency between corresponding frames of the same video rendered from different camera poses using an epipolar attention mechanism. Trained on top of a state-of-the-art camera-control module for video generation, CVD generates multiple videos rendered from different camera trajectories with significantly better consistency than baselines, as shown in extensive experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengfei Kuang",
      "Shengqu Cai",
      "Hao He",
      "Yinghao Xu",
      "Hongsheng Li",
      "Leonidas Guibas",
      "Gordon Wetzstein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1d774c112926348c3e25ea47d87c835b-Abstract-Conference.html": {
    "title": "Cryptographic Hardness of Score Estimation",
    "volume": "main",
    "abstract": "We show that L2-accurate score estimation, in the absence of strong assumptions on the data distribution, is computationally hard even when sample complexity is polynomial in the relevant problem parameters. Our reduction builds on the result of Chen et al. (ICLR 2023), who showed that the problem of generating samples from an unknown data distribution reduces to L2-accurate score estimation. Our hard-to-estimate distributions are the \"Gaussian pancakes\" distributions, originally due to Diakonikolas et al. (FOCS 2017), which have been shown to be computationally indistinguishable from the standard Gaussian under widely believed hardness assumptions from lattice-based cryptography (Bruna et al., STOC 2021; Gupte et al., FOCS 2022)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Jae Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1d8dc55c1f6cf124af840ce1d92d1896-Abstract-Conference.html": {
    "title": "The Secretary Problem with Predicted Additive Gap",
    "volume": "main",
    "abstract": "The secretary problem is one of the fundamental problems in online decision making; a tight competitive ratio for this problem of $1/e \\approx 0.368$ has been known since the 1960s. Much more recently, the study of algorithms with predictions was introduced: The algorithm is equipped with a (possibly erroneous) additional piece of information upfront which can be used to improve the algorithm's performance. Complementing previous work on secretary problems with prior knowledge, we tackle the following question: _What is the weakest piece of information that allows us to break the $1/e$ barrier?_To this end, we introduce the secretary problem with predicted additive gap. As in the classical problem, weights are fixed by an adversary and elements appear in random order. In contrast to previous variants of predictions, our algorithm only has access to a much weaker piece of information: an _additive gap_ $c$. This gap is the difference between the highest and $k$-th highest weight in the sequence.Unlike previous pieces of advice, knowing an exact additive gap does not make the problem trivial. Our contribution is twofold. First, we show that for any index $k$ and any gap $c$, we can obtain a competitive ratio of $0.4$ when knowing the exact gap (even if we do not know $k$), hence beating the prevalent bound for the classical problem by a constant. Second, a slightly modified version of our algorithm allows to prove standard robustness-consistency properties as well as improved guarantees when knowing a range for the error of the prediction",
    "checked": true,
    "id": "9b19f3625d763c6ba1e9a17beeefe34dc0f4bfdc",
    "semantic_title": "the secretary problem with predicted additive gap",
    "citation_count": 0,
    "authors": [
      "Alexander Braun",
      "Sherry Sarkar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1d91d5689e251d27993a3c2182dddcf7-Abstract-Conference.html": {
    "title": "SpecExec: Massively Parallel Speculative Decoding For Interactive LLM Inference on Consumer Devices",
    "volume": "main",
    "abstract": "As large language models gain widespread adoption, running them efficiently becomes a crucial task. Recent works on LLM inference use speculative decoding to achieve extreme speedups. However, most of these works implicitly design their algorithms for high-end datacenter hardware. In this work, we ask the opposite question: how fast can we run LLMs on consumer machines? Consumer GPUs can no longer fit the largest available models and must offload them to RAM or SSD. With parameter offloading, hundreds or thousands of tokens can be processed in batches within the same time as just one token, making it a natural fit for speculative decoding. We propose SpecExec (Speculative Execution), a simple parallel decoding method that can generate up to 20 tokens per target model iteration for popular LLM families. SpecExec takes the most probable continuations from the draft model to build a \"cache\" tree for the target model, which then gets validated in a single pass. Using SpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with RAM offloading at 4--6 tokens per second with 4-bit quantization or 2--3 tokens per second with 16-bit weights. Our code is available at https://github.com/yandex-research/specexec",
    "checked": true,
    "id": "05bde17d7cfe69fff3ab574d2521b9d806fc901e",
    "semantic_title": "specexec: massively parallel speculative decoding for interactive llm inference on consumer devices",
    "citation_count": 23,
    "authors": [
      "Ruslan Svirschevski",
      "Avner May",
      "Zhuoming Chen",
      "Beidi Chen",
      "Zhihao Jia",
      "Max Ryabinin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1da38b872e19f1f4a3c2846720e8f64a-Abstract-Conference.html": {
    "title": "From Unstructured Data to In-Context Learning: Exploring What Tasks Can Be Learned and When",
    "volume": "main",
    "abstract": "Large language models (LLMs) like transformers demonstrate impressive in-context learning (ICL) capabilities, allowing them to makepredictions for new tasks based on prompt exemplars without parameter updates. While existing ICL theories often assume structured training data resembling ICL tasks (e.g., x-y pairs for linear regression), LLMs are typically trained unsupervised on unstructured text, such as web content, which lacks clear parallels to tasks like word analogy. To address this gap, we examine what enables ICL in models trained on unstructured data, focusing on critical sequence model requirements and training data structure. We find that many ICL capabilities canemerge simply from co-occurrence of semantically related word pairs in unstructured data; word analogy completion, for example, can provably arise purely through co-occurrence modeling, using classical language models like continuous bag of words (CBOW), without needing positional information or attention mechanisms. However, positional information becomes crucial for logic reasoning tasks requiring generalization to unseen tokens. Finally, we identify two cases where ICL fails: one in logic reasoning tasks that require generalizing to new, unseen patterns, and another in analogy completion where relevant word pairs appear only in fixed training positions. These findings suggest that LLMs' ICL abilities depend heavily on the structural elements within their training data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Christian Wibisono",
      "Yixin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1da9ca7e9cef4b1af63913f05d1630a4-Abstract-Conference.html": {
    "title": "Are Large-scale Soft Labels Necessary for Large-scale Dataset Distillation?",
    "volume": "main",
    "abstract": "In ImageNet-condensation, the storage for auxiliary soft labels exceeds that of the condensed dataset by over 30 times.However, are large-scale soft labels necessary for large-scale dataset distillation?In this paper, we first discover that the high within-class similarity in condensed datasets necessitates the use of large-scale soft labels.This high within-class similarity can be attributed to the fact that previous methods use samples from different classes to construct a single batch for batch normalization (BN) matching.To reduce the within-class similarity, we introduce class-wise supervision during the image synthesizing process by batching the samples within classes, instead of across classes.As a result, we can increase within-class diversity and reduce the size of required soft labels.A key benefit of improved image diversity is that soft label compression can be achieved through simple random pruning, eliminating the need for complex rule-based strategies. Experiments validate our discoveries.For example, when condensing ImageNet-1K to 200 images per class, our approach compresses the required soft labels from 113 GB to 2.8 GB (40$\\times$ compression) with a 2.6\\% performance gain.Code is available at: https://github.com/he-y/soft-label-pruning-for-dataset-distillation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingao Xiao",
      "Yang He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1dc9fbdb6b4d9955ad377cb983232c9f-Abstract-Conference.html": {
    "title": "Linear Uncertainty Quantification of Graphical Model Inference",
    "volume": "main",
    "abstract": "Uncertainty Quantification (UQ) is vital for decision makers as it offers insights into the potential reliability of data and model, enabling more informed and risk-aware decision-making. Graphical models, capable of representing data with complex dependencies, are widely used across domains.Existing sampling-based UQ methods are unbiased but cannot guarantee convergence and are time-consuming on large-scale graphs. There are fast UQ methods for graphical models with closed-form solutions and convergence guarantee but with uncertainty underestimation.We propose LinUProp, a UQ method that utilizes a novel linear propagation of uncertainty to model uncertainty among related nodes additively instead of multiplicatively, to offer linear scalability, guaranteed convergence, and closed-form solutions without underestimating uncertainty.Theoretically, we decompose the expected prediction error of the graphical model and prove that the uncertainty computed by LinUProp is the generalized variance component of the decomposition.Experimentally, we demonstrate that LinUProp is consistent with the sampling-based method but with linear scalability and fast convergence.Moreover, LinUProp outperforms competitors in uncertainty-based active learning on four real-world graph datasets, achieving higher accuracy with a lower labeling budget",
    "checked": true,
    "id": "02d93a624fb4cf1f4529e0806cff5bcc1c5d390e",
    "semantic_title": "linear uncertainty quantification of graphical model inference",
    "citation_count": 0,
    "authors": [
      "Chenghua Guo",
      "Han Yu",
      "Jiaxin Liu",
      "Chao Chen",
      "Qi Li",
      "Sihong Xie",
      "Xi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1dccfc3ee01871d05e33457c61037d59-Abstract-Conference.html": {
    "title": "4+3 Phases of Compute-Optimal Neural Scaling Laws",
    "volume": "main",
    "abstract": "We consider the solvable neural scaling model with three parameters: data complexity, target complexity, and model-parameter-count. We use this neural scaling model to derive new predictions about the compute-limited, infinite-data scaling law regime. To train the neural scaling model, we run one-pass stochastic gradient descent on a mean-squared loss. We derive a representation of the loss curves which holds over all iteration counts and improves in accuracy as the model parameter count grows. We then analyze the compute-optimal model-parameter-count, and identify 4 phases (+3 subphases) in the data-complexity/target-complexity phase-plane. The phase boundaries are determined by the relative importance of model capacity, optimizer noise, and embedding of the features. We furthermore derive, with mathematical proof and extensive numerical evidence, the scaling-law exponents in all of these phases, in particular computing the optimal model-parameter-count as a function of floating point operation budget. We include a colab notebook https://tinyurl.com/2saj6bkj, nanoChinchilla, that reproduces some key results of the paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elliot Paquette",
      "Courtney Paquette",
      "Lechao Xiao",
      "Jeffrey Pennington"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1dcee1cd6890ab7fcdf173ec10526da9-Abstract-Conference.html": {
    "title": "Learning Mixtures of Unknown Causal Interventions",
    "volume": "main",
    "abstract": "The ability to conduct interventions plays a pivotal role in learning causal relationships among variables, thus facilitating applications across diverse scientific disciplines such as genomics, economics, and machine learning. However, in many instances within these applications, the process of generating interventional data is subject to noise: rather than data being sampled directly from the intended interventional distribution, interventions often yield data sampled from a blend of both intended and unintended interventional distributions.We consider the fundamental challenge of disentangling mixed interventional and observational data within linear Structural Equation Models (SEMs) with Gaussian additive noise without the knowledge of the true causal graph. We demonstrate that conducting interventions, whether do or soft, yields distributions with sufficient diversity and properties conducive to efficiently recovering each component within the mixture. Furthermore, we establish that the sample complexity required to disentangle mixed data inversely correlates with the extent of change induced by an intervention in the equations governing the affected variable values. As a result, the causal graph can be identified up to its interventional Markov Equivalence Class, similar to scenarios where no noise influences the generation of interventional data. We further support our theoretical findings by conducting simulations wherein we perform causal discovery from such mixed data",
    "checked": true,
    "id": "8baf3b03f91698fa6bf380d9a72c1796183e6ce1",
    "semantic_title": "learning mixtures of unknown causal interventions",
    "citation_count": 0,
    "authors": [
      "Abhinav Kumar",
      "Kirankumar Shiragur",
      "Caroline Uhler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1dd8ea647d80e38b3702cf01a0855bed-Abstract-Conference.html": {
    "title": "On the Noise Robustness of In-Context Learning for Text Generation",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown impressive performance on downstream tasks by in-context learning (ICL), which heavily relies on the quality of demonstrations selected from a large set of annotated examples. Recent works claim that in-context learning is robust to noisy demonstrations in text classification. In this work, we show that, on text generation tasks, noisy annotations significantly hurt the performance of in-context learning. To circumvent the issue, we propose a simple and effective approach called Local Perplexity Ranking (LPR), which replaces the \"noisy\" candidates with their nearest neighbors that are more likely to be clean. Our method is motivated by analyzing the perplexity deviation caused by noisy labels and decomposing perplexity into inherent perplexity and matching perplexity. Our key idea behind LPR is thus to decouple the matching perplexity by performing the ranking among the neighbors in semantic space. Our approach can prevent the selected demonstrations from including mismatched input-label pairs while preserving the effectiveness of the original selection methods. Extensive experiments demonstrate the effectiveness of LPR, improving the EM score by up to 18.75 on common benchmarks with noisy annotations",
    "checked": true,
    "id": "bb928287d96ba550c7cc2aef8fde1a93db322add",
    "semantic_title": "on the noise robustness of in-context learning for text generation",
    "citation_count": 5,
    "authors": [
      "hongfu gao",
      "Feipeng Zhang",
      "Wenyu Jiang",
      "Jun Shu",
      "Feng Zheng",
      "Hongxin Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1df5c96e327ea0cbd32e0d8bae835994-Abstract-Conference.html": {
    "title": "Implicit Regularization of Decentralized Gradient Descent for Sparse Regression",
    "volume": "main",
    "abstract": "We consider learning a sparse model from linear measurements taken by a network of agents. Different from existing decentralized methods designed based on the LASSO regression with explicit $\\ell_1$ norm regularization, we exploit the implicit regularization of decentralized optimization method applied to an over-parameterized nonconvex least squares formulation without penalization. Our first result shows that despite nonconvexity, if the network connectivity is good, the well-known decentralized gradient descent algorithm (DGD) with small initialization and early stopping can compute the statistically optimal solution. Sufficient conditions on the initialization scale, choice of step size, network connectivity, and stopping time are further provided to achieve convergence. Our result recovers the convergence rate of gradient descent in the centralized setting, showing its tightness. Based on the analysis of DGD, we further propose a communication-efficient version, termed T-DGD, by truncating the iterates before transmission. In the high signal-to-noise ratio (SNR) regime, we show that T-DGD achieves comparable statistical accuracy to DGD, while the communication cost is logarithmic in the number of parameters. Numerical results are provided to validate the effectiveness of DGD and T-DGD for sparse learning through implicit regularization",
    "checked": true,
    "id": "8bfdc3e8f33506fea4bdd138fc5d3163b1fc6c70",
    "semantic_title": "implicit regularization of decentralized gradient descent for sparse regression",
    "citation_count": 2,
    "authors": [
      "Tongle Wu",
      "Ying Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1e027da6bec9ceb2ec37951ceeccae93-Abstract-Conference.html": {
    "title": "Loki: Low-rank Keys for Efficient Sparse Attention",
    "volume": "main",
    "abstract": "Inference on large language models (LLMs) can be expensive in terms of thecompute and memory costs involved, especially when long sequence lengths areused. In particular, the self-attention mechanism used in LLM inference contributessignificantly to these costs, which has sparked an interest in approximating the self-attention computation to reduce such costs. In this work, we propose to approximateself-attention by focusing on the dimensionality of key vectors computed in theattention block. Our analysis reveals that key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting thisobservation, we propose Loki, a novel sparse attention method that ranks and selectstokens in the KV-cache based on attention scores computed in low-dimensionalspace. Our evaluations show that Loki is able to speed up the attention computationdue to reduced data movement (load/store) and compute costs while maintainingthe efficacy of the models better than other popular approximation methods",
    "checked": true,
    "id": "f4c07dc79976a4e3a558bb6fcd0d615673e8ecef",
    "semantic_title": "loki: low-rank keys for efficient sparse attention",
    "citation_count": 26,
    "authors": [
      "Prajwal Singhania",
      "Siddharth Singh",
      "Shwai He",
      "Soheil Feizi",
      "Abhinav Bhatele"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1e0d38c676d5855bcfab7f6d29d20ad9-Abstract-Conference.html": {
    "title": "VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks",
    "volume": "main",
    "abstract": "As the adoption of large language models increases and the need for per-user or per-task model customization grows, the parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) and its variants, incur substantial storage and transmission costs. To further reduce stored parameters, we introduce a \"divide-and-share\" paradigm that breaks the barriers of low-rank decomposition across matrix dimensions, modules, and layers by sharing parameters globally via a vector bank. As an instantiation of the paradigm to LoRA, our proposed VB-LoRA composites all the low-rank matrices of LoRA from a shared vector bank with a differentiable top-$k$ admixture module. VB-LoRA achieves extreme parameter efficiency while maintaining comparable or better performance compared to state-of-the-art PEFT methods. Extensive experiments demonstrate the effectiveness of VB-LoRA on natural language understanding, natural language generation, instruction tuning, and mathematical reasoning tasks. When fine-tuning the Llama2-13B model, VB-LoRA only uses 0.4% of LoRA's stored parameters, yet achieves superior results. Our source code is available at https://github.com/leo-yangli/VB-LoRA. This method has been merged into the Hugging Face PEFT package",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Li",
      "Shaobo Han",
      "Jonathan Shihao Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1e0ebde5c2152b03420d9258f7139a58-Abstract-Conference.html": {
    "title": "ST$_k$: A Scalable Module for Solving Top-k Problems",
    "volume": "main",
    "abstract": "The cost of ranking becomes significant in the new stage of deep learning. We propose ST$_k$, a fully differentiable module with a single trainable parameter, designed to solve the Top-k problem without requiring additional time or GPU memory. Due to its fully differentiable nature, ST$_k$ can be embedded end-to-end into neural networks and optimize the Top-k problems within a unified computational graph. We apply ST$_k$ to the Average Top-k Loss (AT$_k$), which inherently faces a Top-k problem. The proposed ST$_k$ Loss outperforms AT$_k$ Loss and achieves the best average performance on multiple benchmarks, with the lowest standard deviation. With the assistance of ST$_k$ Loss, we surpass the state-of-the-art (SOTA) on both CIFAR-100-LT and Places-LT leaderboards",
    "checked": true,
    "id": "a3e10b7b653b8302377c2e764fdab8397480c1f7",
    "semantic_title": "st$_k$: a scalable module for solving top-k problems",
    "citation_count": 0,
    "authors": [
      "Hanchen Xia",
      "Weidong Liu",
      "Xiaojun Mao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1e1cf05517b959c1ce5934734efc421b-Abstract-Conference.html": {
    "title": "Non-geodesically-convex optimization in the Wasserstein space",
    "volume": "main",
    "abstract": "We study a class of optimization problems in the Wasserstein space (the space of probability measures) where the objective function is nonconvex along generalized geodesics. Specifically, the objective exhibits some difference-of-convex structure along these geodesics. The setting also encompasses sampling problems where the logarithm of the target distribution is difference-of-convex. We derive multiple convergence insights for a novel semi Forward-Backward Euler scheme under several nonconvex (and possibly nonsmooth) regimes. Notably, the semi Forward-Backward Euler is just a slight modification of the Forward-Backward Euler whose convergence is---to our knowledge---still unknown in our very general non-geodesically-convex setting",
    "checked": true,
    "id": "9347ab1956f37f82b12f902871fafc872cc5bb02",
    "semantic_title": "non-geodesically-convex optimization in the wasserstein space",
    "citation_count": 2,
    "authors": [
      "Hoang Phuc Hau Luu",
      "Hanlin Yu",
      "Bernardo Williams",
      "Petrus Mikkola",
      "Marcelo Hartmann",
      "Kai Puolamäki",
      "Arto Klami"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1e269abc604816c35f600ae14b354efd-Abstract-Conference.html": {
    "title": "Non-asymptotic Global Convergence Analysis of BFGS with the Armijo-Wolfe Line Search",
    "volume": "main",
    "abstract": "In this paper, we present the first explicit and non-asymptotic global convergence rates of the BFGS method when implemented with an inexact line search scheme satisfying the Armijo-Wolfe conditions. We show that BFGS achieves a global linear convergence rate of $(1 - \\frac{1}{\\kappa})^t$ for $\\mu$-strongly convex functions with $L$-Lipschitz gradients, where $\\kappa = \\frac{L}{\\mu}$ represents the condition number. Additionally, if the objective function's Hessian is Lipschitz, BFGS with the Armijo-Wolfe line search achieves a linear convergence rate that depends solely on the line search parameters, independent of the condition number. We also establish a global superlinear convergence rate of $\\mathcal{O}((\\frac{1}{t})^t)$. These global bounds are all valid for any starting point $x_0$ and any symmetric positive definite initial Hessian approximation matrix $B_0$, though the choice of $B_0$ impacts the number of iterations needed to achieve these rates. By synthesizing these results, we outline the first global complexity characterization of BFGS with the Armijo-Wolfe line search. Additionally, we clearly define a mechanism for selecting the step size to satisfy the Armijo-Wolfe conditions and characterize its overall complexity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiujiang Jin",
      "Ruichen Jiang",
      "Aryan Mokhtari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1e2dd2f1efbc6e65b68f17ce6e158b34-Abstract-Conference.html": {
    "title": "Enhancing Preference-based Linear Bandits via Human Response Time",
    "volume": "main",
    "abstract": "Interactive preference learning systems infer human preferences by presenting queries as pairs of options and collecting binary choices. Although binary choices are simple and widely used, they provide limited information about preference strength. To address this, we leverage human response times, which are inversely related to preference strength, as an additional signal. We propose a computationally efficient method that combines choices and response times to estimate human utility functions, grounded in the EZ diffusion model from psychology. Theoretical and empirical analyses show that for queries with strong preferences, response times complement choices by providing extra information about preference strength, leading to significantly improved utility estimation. We incorporate this estimator into preference-based linear bandits for fixed-budget best-arm identification. Simulations on three real-world datasets demonstrate that using response times significantly accelerates preference learning compared to choice-only approaches. Additional materials, such as code, slides, and talk video, are available at https://shenlirobot.github.io/pages/NeurIPS24.html",
    "checked": true,
    "id": "c24e41c6fda052971c940eb157958027f7266e2e",
    "semantic_title": "enhancing preference-based linear bandits via human response time",
    "citation_count": 1,
    "authors": [
      "Shen Li",
      "Yuyang Zhang",
      "Zhaolin Ren",
      "Claire Liang",
      "Na Li",
      "Julie A Shah"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1e38b2a0b77541b14a3315c99697b835-Abstract-Conference.html": {
    "title": "Zero-Shot Reinforcement Learning from Low Quality Data",
    "volume": "main",
    "abstract": "Zero-shot reinforcement learning (RL) promises to provide agents that can perform any task in an environment after an offline, reward-free pre-training phase. Methods leveraging successor measures and successor features have shown strong performance in this setting, but require access to large heterogenous datasets for pre-training which cannot be expected for most real problems. Here, we explore how the performance of zero-shot RL methods degrades when trained on small homogeneous datasets, and propose fixes inspired by conservatism, a well-established feature of performant single-task offline RL algorithms. We evaluate our proposals across various datasets, domains and tasks, and show that conservative zero-shot RL algorithms outperform their non-conservative counterparts on low quality datasets, and perform no worse on high quality datasets. Somewhat surprisingly, our proposals also outperform baselines that get to see the task during training. Our code is available via the project page https://enjeeneer.io/projects/zero-shot-rl/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Scott Jeen",
      "Tom Bewley",
      "Jonathan Cullen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1e5cff01121223de917a84a242de30a5-Abstract-Conference.html": {
    "title": "Ordered Momentum for Asynchronous SGD",
    "volume": "main",
    "abstract": "Distributed learning is essential for training large-scale deep models.Asynchronous SGD (ASGD) and its variants are commonly used distributed learning methods, particularly in scenarios where the computing capabilities of workers in the cluster are heterogeneous.Momentum has been acknowledged for its benefits in both optimization and generalization in deep model training. However, existing works have found that naively incorporating momentum into ASGD can impede the convergence.In this paper, we propose a novel method called ordered momentum (OrMo) for ASGD. In OrMo, momentum is incorporated into ASGD by organizing the gradients in order based on their iteration indexes. We theoretically prove the convergence of OrMo with both constant and delay-adaptive learning rates for non-convex problems. To the best of our knowledge, this is the first work to establish the convergence analysis of ASGD with momentum without dependence on the maximum delay. Empirical results demonstrate that OrMo can achieve better convergence performance compared with ASGD and other asynchronous methods with momentum",
    "checked": true,
    "id": "c68970e2976b67d032003266a11501d7e3ccea90",
    "semantic_title": "ordered momentum for asynchronous sgd",
    "citation_count": 0,
    "authors": [
      "Chang-Wei Shi",
      "Yi-Rui Yang",
      "Wu-Jun Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1e616bde0438cb10cb6adf076ae7d336-Abstract-Conference.html": {
    "title": "Real-Time Recurrent Learning using Trace Units in Reinforcement Learning",
    "volume": "main",
    "abstract": "Recurrent Neural Networks (RNNs) are used to learn representations in partially observable environments. For agents that learn online and continually interact with the environment, it is desirable to train RNNs with real-time recurrent learning (RTRL); unfortunately, RTRL is prohibitively expensive for standard RNNs. A promising direction is to use linear recurrent architectures (LRUs), where dense recurrent weights are replaced with a complex-valued diagonal, making RTRL efficient. In this work, we build on these insights to provide a lightweight but effective approach for training RNNs in online RL. We introduce Recurrent Trace Units (RTUs), a small modification on LRUs that we nonetheless find to have significant performance benefits over LRUs when trained with RTRL. We find RTUs significantly outperform GRUs and Transformers across several partially observable environments while using significantly less computation",
    "checked": true,
    "id": "ed1ad08aee5a613238e31c051eb9abdbad19c4ce",
    "semantic_title": "real-time recurrent learning using trace units in reinforcement learning",
    "citation_count": 4,
    "authors": [
      "Esraa Elelimy",
      "Adam White",
      "Michael Bowling",
      "Martha White"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1e6dcc16ffa7ced2228d1f2fdc8b5adf-Abstract-Conference.html": {
    "title": "Abstract Reward Processes: Leveraging State Abstraction for Consistent Off-Policy Evaluation",
    "volume": "main",
    "abstract": "Evaluating policies using off-policy data is crucial for applying reinforcement learning to real-world problems such as healthcare and autonomous driving. Previous methods for off-policy evaluation (OPE) generally suffer from high variance or irreducible bias, leading to unacceptably high prediction errors. In this work, we introduce STAR, a framework for OPE that encompasses a broad range of estimators -- which include existing OPE methods as special cases -- that achieve lower mean squared prediction errors. STAR leverages state abstraction to distill complex, potentially continuous problems into compact, discrete models which we call abstract reward processes (ARPs). Predictions from ARPs estimated from off-policy data are provably consistent (asymptotically correct). Rather than proposing a specific estimator, we present a new framework for OPE and empirically demonstrate that estimators within STAR outperform existing methods. The best STAR estimator outperforms baselines in all twelve cases studied, and even the median STAR estimator surpasses the baselines in seven out of the twelve cases",
    "checked": true,
    "id": "4e1e5d5aef138cc2e20beaa8e7c099203a2897d7",
    "semantic_title": "abstract reward processes: leveraging state abstraction for consistent off-policy evaluation",
    "citation_count": 1,
    "authors": [
      "Shreyas Chaudhari",
      "Ameet Deshpande",
      "Bruno C. da Silva",
      "Philip S. Thomas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1eaa5146756be028ad6fff1efcc8e6bd-Abstract-Conference.html": {
    "title": "A Globally Optimal Portfolio for m-Sparse Sharpe Ratio Maximization",
    "volume": "main",
    "abstract": "The Sharpe ratio is an important and widely-used risk-adjusted return in financial engineering. In modern portfolio management, one may require an m-sparse (no more than m active assets) portfolio to save managerial and financial costs. However, few existing methods can optimize the Sharpe ratio with the m-sparse constraint, due to the nonconvexity and the complexity of this constraint. We propose to convert the m-sparse fractional optimization problem into an equivalent m-sparse quadratic programming problem. The semi-algebraic property of the resulting objective function allows us to exploit the Kurdyka-Lojasiewicz property to develop an efficient Proximal Gradient Algorithm (PGA) that leads to a portfolio which achieves the globally optimal m-sparse Sharpe ratio under certain conditions. The convergence rates of PGA are also provided. To the best of our knowledge, this is the first proposal that achieves a globally optimal m-sparse Sharpe ratio with a theoretically-sound guarantee",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizun Lin",
      "Zhao-Rong Lai",
      "Cheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1ed4723f12853cbd02aecb8160f5e0c9-Abstract-Conference.html": {
    "title": "Supra-Laplacian Encoding for Transformer on Dynamic Graphs",
    "volume": "main",
    "abstract": "Fully connected Graph Transformers (GT) have rapidly become prominent in the static graph community as an alternative to Message-Passing models, which suffer from a lack of expressivity, oversquashing, and under-reaching.However, in a dynamic context, by interconnecting all nodes at multiple snapshots with self-attention,GT loose both structural and temporal information. In this work, we introduce Supra-LAplacian encoding for spatio-temporal TransformErs (SLATE), a new spatio-temporal encoding to leverage the GT architecture while keeping spatio-temporal information.Specifically, we transform Discrete Time Dynamic Graphs into multi-layer graphs and take advantage of the spectral properties of their associated supra-Laplacian matrix.Our second contribution explicitly model nodes' pairwise relationships with a cross-attention mechanism, providing an accurate edge representation for dynamic link prediction.SLATE outperforms numerous state-of-the-art methods based on Message-Passing Graph Neural Networks combined with recurrent models (e.g, LSTM), and Dynamic Graph Transformers,on~9 datasets. Code is open-source and available at this link https://github.com/ykrmm/SLATE",
    "checked": true,
    "id": "fd0b2bee1ab06b1a3b3d5881025cb1a14de02d58",
    "semantic_title": "supra-laplacian encoding for transformer on dynamic graphs",
    "citation_count": 3,
    "authors": [
      "Yannis Karmim",
      "Marc Lafon",
      "Raphael Fournier-S&#x27;niehotta",
      "Nicolas THOME"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1ef130b8249625e47ef96a7b27464845-Abstract-Conference.html": {
    "title": "Enhancing Efficiency of Safe Reinforcement Learning via Sample Manipulation",
    "volume": "main",
    "abstract": "Safe reinforcement learning (RL) is crucial for deploying RL agents in real-world applications, as it aims to maximize long-term rewards while satisfying safety constraints. However, safe RL often suffers from sample inefficiency, requiring extensive interactions with the environment to learn a safe policy. We propose Efficient Safe Policy Optimization (ESPO), a novel approach that enhances the efficiency of safe RL through sample manipulation. ESPO employs an optimization framework with three modes: maximizing rewards, minimizing costs, and balancing the trade-off between the two. By dynamically adjusting the sampling process based on the observed conflict between reward and safety gradients, ESPO theoretically guarantees convergence, optimization stability, and improved sample complexity bounds. Experiments on the Safety-MuJoCo and Omnisafe benchmarks demonstrate that ESPO significantly outperforms existing primal-based and primal-dual-based baselines in terms of reward maximization and constraint satisfaction. Moreover, ESPO achieves substantial gains in sample efficiency, requiring 25--29\\% fewer samples than baselines, and reduces training time by 21--38\\%",
    "checked": true,
    "id": "1eb8cf490e8bc3170d9577653b0df3a93d723364",
    "semantic_title": "enhancing efficiency of safe reinforcement learning via sample manipulation",
    "citation_count": 3,
    "authors": [
      "Shangding Gu",
      "Laixi Shi",
      "Yuhao Ding",
      "Alois Knoll",
      "Costas J Spanos",
      "Adam Wierman",
      "Ming Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1f0832859514e53a0e4f229fc9b3a4a2-Abstract-Conference.html": {
    "title": "Incremental Learning of Retrievable Skills For Efficient Continual Task Adaptation",
    "volume": "main",
    "abstract": "Continual Imitation Learning (CiL) involves extracting and accumulating task knowledge from demonstrations across multiple stages and tasks to achieve a multi-task policy. With recent advancements in foundation models, there has been a growing interest in adapter-based CiL approaches, where adapters are established parameter-efficiently for tasks newly demonstrated. While these approaches isolate parameters for specific tasks and tend to mitigate catastrophic forgetting, they limit knowledge sharing among different demonstrations. We introduce IsCiL, an adapter-based CiL framework that addresses this limitation of knowledge sharing by incrementally learning shareable skills from different demonstrations, thus enabling sample-efficient task adaptation using the skills particularly in non-stationary CiL environments. In IsCiL, demonstrations are mapped into the state embedding space, where proper skills can be retrieved upon input states through prototype-based memory. These retrievable skills are incrementally learned on their corresponding adapters. Our CiL experiments with complex tasks in the Franka-Kitchen and Meta-World demonstrate the robust performance of IsCiL in both task adaptation and sample-efficiency. We also show a simple extension of IsCiL for task unlearning scenarios",
    "checked": true,
    "id": "41f4c0462d23dd945bde4a8563eb73c8e35b6df0",
    "semantic_title": "incremental learning of retrievable skills for efficient continual task adaptation",
    "citation_count": 10,
    "authors": [
      "Daehee Lee",
      "Minjong Yoo",
      "Woo Kyung Kim",
      "Wonje Choi",
      "Honguk Woo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1f1628d502c62ef3725fb3b0b8eb4219-Abstract-Conference.html": {
    "title": "Preference-based Pure Exploration",
    "volume": "main",
    "abstract": "We study the preference-based pure exploration problem for bandits with vector-valued rewards and a set of preferences imposed over them. Specifically, we aim to identify the most preferred policy over a set of arms according to the preferences induced on the reward vectors by an ordering cone $C$. First, to quantify the impact of preferences, we derive a novel lower bound on the sample complexity for identifying the most preferred arm with confidence level $1-\\delta$. Our lower bound shows that how the geometry of the preferences and reward vectors changes the hardness of this problem. We further explicate this geometry for Gaussian distributions of rewards, and provide a convex reformulation of the lower bound solvable with linear programming. Then, we leverage this convex reformulation of the lower bound to design the Track and Stop with Preferences (TSwP) algorithm that identifies the most preferred policy. Finally, we derive a new concentration result for vector-valued rewards, and show that TSwP achieves a matching sample complexity upper bound",
    "checked": true,
    "id": "1c85ee706cce5546db52d497ae08b14eafdd0788",
    "semantic_title": "preference-based pure exploration",
    "citation_count": 1,
    "authors": [
      "Apurv Shukla",
      "Debabrota Basu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1f28e9341ab99d8e5a5734f0a76601c7-Abstract-Conference.html": {
    "title": "Policy Optimization for Robust Average Reward MDPs",
    "volume": "main",
    "abstract": "This paper studies first-order policy optimization for robust average cost Markov decision processes (MDPs). Specifically, we focus on ergodic Markov chains. For robust average cost MDPs, the goal is to optimize the worst-case average cost over an uncertainty set of transition kernels. We first develop a sub-gradient of the robust average cost. Based on the sub-gradient, a robust policy mirror descent approach is further proposed. To characterize its iteration complexity, we develop a lower bound on the difference of robust average cost between two policies and further show that the robust average cost satisfies the PL-condition. We then show that with increasing step size, our robust policy mirror descent achieves a linear convergence rate in the optimality gap, and with constant step size, our algorithm converges to an $\\epsilon$-optimal policy with an iteration complexity of $\\mathcal{O}(1/\\epsilon)$. The convergence rate of our algorithm matches with the best convergence rate of policy-based algorithms for robust MDPs. Moreover, our algorithm is the first algorithm that converges to the global optimum with general uncertainty sets for robust average cost MDPs. We provide simulation results to demonstrate the performance of our algorithm",
    "checked": true,
    "id": "b142df8df7ad3a50123a75bd364bd76ecb1b86e8",
    "semantic_title": "policy optimization for robust average reward mdps",
    "citation_count": 2,
    "authors": [
      "Zhongchang Sun",
      "Sihong He",
      "Fei Miao",
      "Shaofeng Zou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1f471322127d6347e5ae09a14b1e5cf7-Abstract-Conference.html": {
    "title": "Beyond Euclidean: Dual-Space Representation Learning for Weakly Supervised Video Violence Detection",
    "volume": "main",
    "abstract": "While numerous Video Violence Detection (VVD) methods have focused on representation learning in Euclidean space, they struggle to learn sufficiently discriminative features, leading to weaknesses in recognizing normal events that are visually similar to violent events (i.e., ambiguous violence). In contrast, hyperbolic representation learning, renowned for its ability to model hierarchical and complex relationships between events, has the potential to amplify the discrimination between visually similar events. Inspired by these, we develop a novel Dual-Space Representation Learning (DSRL) method for weakly supervised VVD to utilize the strength of both Euclidean and hyperbolic geometries, capturing the visual features of events while also exploring the intrinsic relations between events, thereby enhancing the discriminative capacity of the features. DSRL employs a novel information aggregation strategy to progressively learn event context in hyperbolic spaces, which selects aggregation nodes through layer-sensitive hyperbolic association degrees constrained by hyperbolic Dirichlet energy. Furthermore, DSRL attempts to break the cyber-balkanization of different spaces, utilizing cross-space attention to facilitate information interactions between Euclidean and hyperbolic space to capture better discriminative features for final violence detection. Comprehensive experiments demonstrate the effectiveness of our proposed DSRL",
    "checked": true,
    "id": "6b1953cf4b171a40ab486f50a48fb82fd8b5ab0e",
    "semantic_title": "beyond euclidean: dual-space representation learning for weakly supervised video violence detection",
    "citation_count": 6,
    "authors": [
      "Jiaxu Leng",
      "Zhanjie Wu",
      "Mingpi Tan",
      "Yiran Liu",
      "Ji Gan",
      "Haosheng Chen",
      "Xinbo Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1f530eef1ae1d4d4f4e0f51437976395-Abstract-Conference.html": {
    "title": "Learning Discrete Latent Variable Structures with Tensor Rank Conditions",
    "volume": "main",
    "abstract": "Unobserved discrete data are ubiquitous in many scientific disciplines, and how to learn the causal structure of these latent variables is crucial for uncovering data patterns. Most studies focus on the linear latent variable model or impose strict constraints on latent structures, which fail to address cases in discrete data involving non-linear relationships or complex latent structures. To achieve this, we explore a tensor rank condition on contingency tables for an observed variable set $\\mathbf{X}_p$, showing that the rank is determined by the minimum support of a specific conditional set (not necessary in $\\mathbf{X}_p$) that d-separates all variables in $\\mathbf{X}_p$. By this, one can locate the latent variable through probing the rank on different observed variables set, and further identify the latent causal structure under some structure assumptions. We present the corresponding identification algorithm and conduct simulated experiments to verify the effectiveness of our method. In general, our results elegantly extend the identification boundary for causal discovery with discrete latent variables and expand the application scope of causal discovery with latent variables",
    "checked": true,
    "id": "6e944755ffc6020b478aa006e57e492c5bbe841b",
    "semantic_title": "learning discrete latent variable structures with tensor rank conditions",
    "citation_count": 1,
    "authors": [
      "Zhengming Chen",
      "Ruichu Cai",
      "Feng Xie",
      "Jie Qiao",
      "Anpeng Wu",
      "Zijian Li",
      "Zhifeng Hao",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1f59562caae05e6aae0ffd1145bea5da-Abstract-Conference.html": {
    "title": "The Map Equation Goes Neural: Mapping Network Flows with Graph Neural Networks",
    "volume": "main",
    "abstract": "Community detection is an essential tool for unsupervised data exploration and revealing the organisational structure of networked systems. With a long history in network science, community detection typically relies on objective functions, optimised with custom-tailored search algorithms, but often without leveraging recent advances in deep learning. Recently, first works have started incorporating such objectives into loss functions for deep graph clustering and pooling. We consider the map equation, a popular information-theoretic objective function for unsupervised community detection, and express it in differentiable tensor form for optimisation through gradient descent. Our formulation turns the map equation compatible with any neural network architecture, enables end-to-end learning, incorporates node features, and chooses the optimal number of clusters automatically, all without requiring explicit regularisation. Applied to unsupervised graph clustering tasks, we achieve competitive performance against state-of-the-art deep graph clustering baselines in synthetic and real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Blöcker",
      "Chester Tan",
      "Ingo Scholtes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1f5c76187c28e8d9d3b938d0c504436c-Abstract-Conference.html": {
    "title": "Locating What You Need: Towards Adapting Diffusion Models to OOD Concepts In-the-Wild",
    "volume": "main",
    "abstract": "The recent large-scale text-to-image generative models have attained unprecedented performance, while people established adaptor modules like LoRA and DreamBooth to extend this performance to even more unseen concept tokens. However, we empirically find that this workflow often fails to accurately depict the out-of-distribution concepts. This failure is highly related to the low quality of training data. To resolve this, we present a framework called Controllable Adaptor Towards Out-of-Distribution Concepts (CATOD). Our framework follows the active learning paradigm which includes high-quality data accumulation and adaptor training, enabling a finer-grained enhancement of generative results. The aesthetics score and concept-matching score are two major factors that impact the quality of synthetic results. One key component of CATOD is the weighted scoring system that automatically balances between these two scores and we also offer comprehensive theoretical analysis for this point. Then, it determines how to select data and schedule the adaptor training based on this scoring system. The extensive results show that CATOD significantly outperforms the prior approaches with an 11.10 boost on the CLIP score and a 33.08% decrease on the CMMD metric",
    "checked": true,
    "id": "0dedcc8a48c6ecf25b182378db2eed3f3b4b6f40",
    "semantic_title": "locating what you need: towards adapting diffusion models to ood concepts in-the-wild",
    "citation_count": 1,
    "authors": [
      "Jianan Yang",
      "Chenchao Gao",
      "Zhiqing Xiao",
      "Junbo Zhao",
      "Sai Wu",
      "Gang Chen",
      "Haobo Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1f6591cc41be737e9ba4cc487ac8082d-Abstract-Conference.html": {
    "title": "ESPACE: Dimensionality Reduction of Activations for Model Compression",
    "volume": "main",
    "abstract": "We propose ESPACE, an LLM compression technique based on dimensionality reduction of activations. Unlike prior works on weight-centric tensor decomposition, ESPACE projects activations onto a pre-calibrated set of principal components. The activation-centrality of the approach enables retraining LLMs with no loss of expressivity; while at inference, weight decomposition is obtained as a byproduct of matrix multiplication associativity. Theoretical results on the construction of projection matrices with optimal computational accuracy are provided. Experimentally, we find ESPACE enables 50% compression of GPT3, Llama2, and Nemotron4 models with small accuracy degradation, as low as a 0.18 perplexity increase on GPT3-22B. At lower compression rates of 20% to 40%, ESPACE drives GPT3 models to outperforming their baseline, by up to a 0.38 decrease in perplexity for GPT3-8B. ESPACE also reduces GEMM execution time and prefill inference latency on existing hardware. Comparison with related works on compressing Llama2-7B via matrix factorization shows that ESPACE is a first step in advancing the state-of-the-art in tensor decomposition compression of LLMs",
    "checked": true,
    "id": "611075d9f612687a285fafeca10816f3e5a38bb6",
    "semantic_title": "espace: dimensionality reduction of activations for model compression",
    "citation_count": 5,
    "authors": [
      "Charbel Sakr",
      "Brucek Khailany"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1f69928210578f4cf5b538a8c8806798-Abstract-Conference.html": {
    "title": "Toxicity Detection for Free",
    "volume": "main",
    "abstract": "Current LLMs are generally aligned to follow safety requirements and tend to refuse toxic prompts. However, LLMs can fail to refuse toxic prompts or be overcautious and refuse benign examples. In addition, state-of-the-art toxicity detectors have low TPRs at low FPR, incurring high costs in real-world applications where toxic examples are rare. In this paper, we introduce Moderation Using LLM Introspection (MULI), which detects toxic prompts using the information extracted directly from LLMs themselves. We found we can distinguish between benign and toxic prompts from the distribution of the first response token's logits. Using this idea, we build a robust detector of toxic prompts using a sparse logistic regression model on the first response token logits. Our scheme outperforms SOTA detectors under multiple metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanhao Hu",
      "Julien Piet",
      "Geng Zhao",
      "Jiantao Jiao",
      "David Wagner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1f6af963e891e7efa229c24a1607fa7f-Abstract-Conference.html": {
    "title": "PERIA: Perceive, Reason, Imagine, Act via Holistic Language and Vision Planning for Manipulation",
    "volume": "main",
    "abstract": "Long-horizon manipulation tasks with general instructions often implicitly encapsulate multiple sub-tasks, posing significant challenges in instruction following.While language planning is a common approach to decompose general instructions into stepwise sub-instructions, text-only guidance may lack expressiveness and lead to potential ambiguity. Considering that humans often imagine and visualize sub-instructions reasoning out before acting, the imagined subgoal images can provide more intuitive guidance and enhance the reliability of decomposition. Inspired by this, we propose PERIA(PErceive, Reason, Imagine, Act), a novel framework that integrates holistic language planning and vision planning for long-horizon manipulation tasks with complex instructions, leveraging both logical and intuitive aspects of task decomposition.Specifically, we first perform a lightweight multimodal alignment on the encoding side to empower the MLLM to perceive visual details and language instructions. The MLLM is then jointly instruction-tuned with a pretrained image-editing model to unlock capabilities of simultaneous reasoning of language instructions and generation of imagined subgoals. Furthermore, we introduce a consistency alignment loss to encourage coherent subgoal images and align with their corresponding instructions, mitigating potential hallucinations and semantic conflicts between the two planning manners.Comprehensive evaluations across three task domains demonstrate that PERIA, benefiting from holistic language and vision planning, significantly outperforms competitive baselines in both instruction following accuracy and task success rate on complex manipulation tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Ni",
      "Jianye Hao",
      "Shiguang Wu",
      "Longxin Kou",
      "Yifu Yuan",
      "Zibin Dong",
      "Jinyi Liu",
      "MingZhi Li",
      "Yuzheng Zhuang",
      "YAN ZHENG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1f6f0b6eec8a4ff0f6baa707ff91a442-Abstract-Conference.html": {
    "title": "CLIPAway: Harmonizing focused embeddings for removing objects via diffusion models",
    "volume": "main",
    "abstract": "Advanced image editing techniques, particularly inpainting, are essential for seamlessly removing unwanted elements while preserving visual integrity. Traditional GAN-based methods have achieved notable success, but recent advancements in diffusion models have produced superior results due to their training on large-scale datasets, enabling the generation of remarkably realistic inpainted images.Despite their strengths, diffusion models often struggle with object removal tasks without explicit guidance, leading to unintended hallucinations of the removed object. To address this issue, we introduce CLIPAway, a novel approach leveraging CLIP embeddings to focus on background regions while excluding foreground elements. CLIPAway enhances inpainting accuracy and quality by identifying embeddings that prioritize the background, thus achieving seamless object removal. Unlike other methods that rely on specialized training datasets or costly manual annotations, CLIPAway provides a flexible, plug-and-play solution compatible with various diffusion-based inpainting techniques",
    "checked": true,
    "id": "13bd2d950ec327b3fb7786bdd592c2ac872b9ce0",
    "semantic_title": "clipaway: harmonizing focused embeddings for removing objects via diffusion models",
    "citation_count": 11,
    "authors": [
      "Yiğit Ekin",
      "Ahmet Burak Yildirim",
      "Erdem Eren Çağlar",
      "Aykut Erdem",
      "Erkut Erdem",
      "Aysegul Dundar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1f7b3b0dd7710af02aac0db5be4cfc8d-Abstract-Conference.html": {
    "title": "Banded Square Root Matrix Factorization for Differentially Private Model Training",
    "volume": "main",
    "abstract": "Current state-of-the-art methods for differentially private model training are based on matrix factorization techniques. However, these methods suffer from high computational overhead because they require numerically solving a demanding optimization problem to determine an approximately optimal factorization prior to the actual model training. In this work, we present a new matrix factorization approach, BSR, which overcomes this computational bottleneck. By exploiting properties of the standard matrix square root, BSR allows to efficiently handle also large-scale problems. For the key scenario of stochastic gradient descent with momentum and weight decay, we even derive analytical expressions for BSR that render the computational overhead negligible. We prove bounds on the approximation quality that hold both in the centralized and in the federated learning setting. Our numerical experiments demonstrate that models trained using BSR perform on par with the best existing methods, while completely avoiding their computational overhead",
    "checked": true,
    "id": "02c8ecca774e32a723290fee3ce34230314e15cc",
    "semantic_title": "banded square root matrix factorization for differentially private model training",
    "citation_count": 7,
    "authors": [
      "Kalinin Nikita",
      "Christoph H. Lampert"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1f7e17e9d60e7bc692b72f41d2178b95-Abstract-Conference.html": {
    "title": "Bayesian-guided Label Mapping for Visual Reprogramming",
    "volume": "main",
    "abstract": "Visual reprogramming (VR) leverages the intrinsic capabilities of pretrained vision models by adapting their input or output interfaces to solve downstream tasks whose labels (i.e., downstream labels) might be totally different from the labels associated with the pretrained models (i.e., pretrained labels). When adapting the output interface, label mapping methods transform the pretrained labels to downstream labels by establishing a gradient-free one-to-one correspondence between the two sets of labels.However, in this paper, we reveal that one-to-one mappings may overlook the complex relationship between pretrained and downstream labels. Motivated by this observation, we propose a Bayesian-guided Label Mapping (BLM) method. BLM constructs an iteratively-updated probabilistic label mapping matrix, with each element quantifying a pairwise relationship between pretrained and downstream labels.The assignment of values to the constructed matrix is guided by Bayesian conditional probability, considering the joint distribution of the downstream labels and the labels predicted by the pretrained model on downstream samples. Experiments conducted on both pretrained vision models (e.g., ResNeXt) and vision-language models (e.g., CLIP) demonstrate the superior performance of BLM over existing label mapping methods. The success of BLM also offers a probabilistic lens through which to understand and analyze the effectiveness of VR.Our code is available at https://github.com/tmlr-group/BayesianLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyi Cai",
      "Zesheng Ye",
      "Lei Feng",
      "Jianzhong Qi",
      "Feng Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1f84412e84da6440ca355d87184cb1b3-Abstract-Conference.html": {
    "title": "Visual Anchors Are Strong Information Aggregators For Multimodal Large Language Model",
    "volume": "main",
    "abstract": "In the realm of Multimodal Large Language Models (MLLMs), vision-language connector plays a crucial role to link the pre-trained vision encoders with Large Language Models (LLMs). Despite its importance, the vision-language connector has been relatively less explored. In this study, we aim to propose a strong vision-language connector that enables MLLM to simultaneously achieve high accuracy and low computation cost. We first reveal the existence of the visual anchors in Vision Transformer and propose a cost-effective search algorithm to progressively extract them. Building on these findings, we introduce the Anchor Former (AcFormer), a novel vision-language connector designed to leverage the rich prior knowledge obtained from these visual anchors during pretraining, guiding the aggregation of information. Through extensive experimentation, we demonstrate that the proposed method significantly reduces computational costs by nearly two-thirds, while simultaneously outperforming baseline methods. This highlights the effectiveness and efficiency of AcFormer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haogeng Liu",
      "Quanzeng You",
      "Xiaotian Han",
      "Yongfei Liu",
      "Huaibo Huang",
      "Ran He",
      "Hongxia Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1f96b24df4b06f5d68389845a9a13ed9-Abstract-Conference.html": {
    "title": "Combining Observational Data and Language for Species Range Estimation",
    "volume": "main",
    "abstract": "Species range maps (SRMs) are essential tools for research and policy-making in ecology, conservation, and environmental management. However, traditional SRMs rely on the availability of environmental covariates and high-quality observational data, both of which can be challenging to obtain due to geographic inaccessibility and resource constraints. We propose a novel approach combining millions of citizen science species observations with textual descriptions from Wikipedia, covering habitat preferences and range descriptions for tens of thousands of species. Our framework maps location, species, and text descriptions into a common space, facilitating the learning of rich spatial covariates at a global scale and enabling zero-shot range estimation from textual descriptions. Evaluated on held-out species, our zero-shot SRMs significantly outperform baselines and match the performance of SRMs obtained using tens of observations. Our approach also acts as a strong prior when combined with observational data, resulting in more accurate range estimation with less data. We present extensive quantitative and qualitative analyses of the learned representations in the context of range estimation and other spatial tasks, demonstrating the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Hamilton",
      "Christian Lange",
      "Elijah Cole",
      "Alexander Shepard",
      "Samuel Heinrich",
      "Oisin Mac Aodha",
      "Grant Van Horn",
      "Subhransu Maji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1fa3d6ccbcd15f7285fee666b2bd57be-Abstract-Conference.html": {
    "title": "From an Image to a Scene: Learning to Imagine the World from a Million 360° Videos",
    "volume": "main",
    "abstract": "Three-dimensional (3D) understanding of objects and scenes play a key role in humans' ability to interact with the world and has been an active area of research in computer vision, graphics, and robotics. Large scale synthetic and object-centric 3D datasets have shown to be effective in training models that have 3D understanding of objects. However, applying a similar approach to real-world objects and scenes is difficult due to a lack of large-scale data. Videos are a potential source for real-world 3D data, but finding diverse yet corresponding views of the same content have shown to be difficult at scale. Furthermore, standard videos come with fixed viewpoints, determined at the time of capture. This restricts the ability to access scenes from a variety of more diverse and potentially useful perspectives. We argue that large scale ODIN videos can address these limitations to provide scalable corresponding frames from diverse views. In this paper we introduce 360-1M, a 360° video dataset consisting of 1 million videos, and a process for efficiently finding corresponding frames from diverse viewpoints at scale. We train our diffusion-based model, ODIN, on 360-1M. Empowered by the largest real-world, multi-view dataset to date, ODIN is able to freely generate novel views of real-world scenes. Unlike previous methods, ODIN can move the camera through the environment, enabling the model to infer the geometry and layout of the scene. Additionally, we show improved performance on standard novel view synthesis and 3D reconstruction benchmarks",
    "checked": false,
    "id": "a74dc9c0c9f206482943610d1f4c49cbd344571c",
    "semantic_title": "from an image to a scene: learning to imagine the world from a million 360 videos",
    "citation_count": 9,
    "authors": [
      "Matthew Wallingford",
      "Anand Bhattad",
      "Aditya Kusupati",
      "Vivek Ramanujan",
      "Matt Deitke",
      "Aniruddha Kembhavi",
      "Roozbeh Mottaghi",
      "Wei-Chiu Ma",
      "Ali Farhadi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1fac855f8225e0b9cdb904a1e0118fdc-Abstract-Conference.html": {
    "title": "Multiview Scene Graph",
    "volume": "main",
    "abstract": "A proper scene representation is central to the pursuit of spatial intelligence where agents can robustly reconstruct and efficiently understand 3D scenes. A scene representation is either metric, such as landmark maps in 3D reconstruction, 3D bounding boxes in object detection, or voxel grids in occupancy prediction, or topological, such as pose graphs with loop closures in SLAM or visibility graphs in SfM. In this work, we propose to build Multiview Scene Graphs (MSG) from unposed images, representing a scene topologically with interconnected place and object nodes. The task of building MSG is challenging for existing representation learning methods since it needs to jointly address both visual place recognition, object detection, and object association from images with limited fields of view and potentially large viewpoint changes. To evaluate any method tackling this task, we developed an MSG dataset and annotation based on a public 3D dataset. We also propose an evaluation metric based on the intersection-over-union score of MSG edges. Moreover, we develop a novel baseline method built on mainstream pretrained vision models, combining visual place recognition and object association into one Transformer decoder architecture. Experiments demonstrate that our method has superior performance compared to existing relevant baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juexiao Zhang",
      "Gao Zhu",
      "Sihang Li",
      "Xinhao Liu",
      "Haorui Song",
      "Xinran Tang",
      "Chen Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1fb0a4de9c14f5557eeea886e22569cd-Abstract-Conference.html": {
    "title": "Fixed Confidence Best Arm Identification in the Bayesian Setting",
    "volume": "main",
    "abstract": "We consider the fixed-confidence best arm identification (FC-BAI) problem in the Bayesian setting. This problem aims to find the arm of the largest mean with a fixed confidence level when the bandit model has been sampled from the known prior. Most studies on the FC-BAI problem have been conducted in the frequentist setting, where the bandit model is predetermined before the game starts. We show that the traditional FC-BAI algorithms studied in the frequentist setting, such as track-and-stop and top-two algorithms, result in arbitrarily suboptimal performances in the Bayesian setting. We also obtain a lower bound of the expected number of samples in the Bayesian setting and introduce a variant of successive elimination that has a matching performance with the lower bound up to a logarithmic factor. Simulations verify the theoretical results",
    "checked": true,
    "id": "313305cfcce051b3f411ee0f6e2f8d69912da677",
    "semantic_title": "fixed confidence best arm identification in the bayesian setting",
    "citation_count": 0,
    "authors": [
      "Kyoungseok Jang",
      "Junpei Komiyama",
      "Kazutoshi Yamazaki"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1fd2b71226c67013756d318d70c40eee-Abstract-Conference.html": {
    "title": "Fairness-Aware Estimation of Graphical Models",
    "volume": "main",
    "abstract": "This paper examines the issue of fairness in the estimation of graphical models (GMs), particularly Gaussian, Covariance, and Ising models. These models play a vital role in understanding complex relationships in high-dimensional data. However, standard GMs can result in biased outcomes, especially when the underlying data involves sensitive characteristics or protected groups. To address this, we introduce a comprehensive framework designed to reduce bias in the estimation of GMs related to protected attributes. Our approach involves the integration of the pairwise graph disparity error and a tailored loss function into a nonsmooth multi-objective optimization problem, striving to achieve fairness across different sensitive groups while maintaining the effectiveness of the GMs. Experimental evaluations on synthetic and real-world datasets demonstrate that our framework effectively mitigates bias without undermining GMs' performance",
    "checked": true,
    "id": "a2a65955e1c8da1519d11b483e87f9cfd8cc8f36",
    "semantic_title": "fairness-aware estimation of graphical models",
    "citation_count": 0,
    "authors": [
      "Zhuoping Zhou",
      "Davoud Ataee Tarzanagh",
      "Bojian Hou",
      "Qi Long",
      "Li Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html": {
    "title": "Addressing Spatial-Temporal Heterogeneity: General Mixed Time Series Analysis via Latent Continuity Recovery and Alignment",
    "volume": "main",
    "abstract": "Mixed time series (MiTS) comprising both continuous variables (CVs) and discrete variables (DVs) are frequently encountered yet under-explored in time series analysis. Essentially, CVs and DVs exhibit different temporal patterns and distribution types. Overlooking these heterogeneities would lead to insufficient and imbalanced representation learning, bringing biased results. This paper addresses the problem with two insights: 1) DVs may originate from intrinsic latent continuous variables (LCVs), which lose fine-grained information due to extrinsic discretization; 2) LCVs and CVs share similar temporal patterns and interact spatially. Considering these similarities and interactions, we propose a general MiTS analysis framework MiTSformer, which recovers LCVs behind DVs for sufficient and balanced spatial-temporal modeling by designing two essential inductive biases: 1) hierarchically aggregating multi-scale temporal context information to enrich the information granularity of DVs; 2) adaptively learning the aggregation processes via the adversarial guidance from CVs. Subsequently, MiTSformer captures complete spatial-temporal dependencies within and across LCVs and CVs via cascaded self- and cross-attention blocks. Empirically, MiTSformer achieves consistent SOTA on five mixed time series analysis tasks, including classification, extrinsic regression, anomaly detection, imputation, and long-term forecasting. The code is available at https://github.com/chunhuiz/MiTSformer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Chen",
      "春晖 赵"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/1feddcfb42229ca84d3070c7d540daaa-Abstract-Conference.html": {
    "title": "Recursive PAC-Bayes: A Frequentist Approach to Sequential Prior Updates with No Information Loss",
    "volume": "main",
    "abstract": "PAC-Bayesian analysis is a frequentist framework for incorporating prior knowledge into learning. It was inspired by Bayesian learning, which allows sequential data processing and naturally turns posteriors from one processing step into priors for the next. However, despite two and a half decades of research, the ability to update priors sequentially without losing confidence information along the way remained elusive for PAC-Bayes. While PAC-Bayes allows construction of data-informed priors, the final confidence intervals depend only on the number of points that were not used for the construction of the prior, whereas confidence information in the prior, which is related to the number of points used to construct the prior, is lost. This limits the possibility and benefit of sequential prior updates, because the final bounds depend only on the size of the final batch.We present a novel and, in retrospect, surprisingly simple and powerful PAC-Bayesian procedure that allows sequential prior updates with no information loss. The procedure is based on a novel decomposition of the expected loss of randomized classifiers. The decomposition rewrites the loss of the posterior as an excess loss relative to a downscaled loss of the prior plus the downscaled loss of the prior, which is bounded recursively. As a side result, we also present a generalization of the split-kl and PAC-Bayes-split-kl inequalities to discrete random variables, which we use for bounding the excess losses, and which can be of independent interest. In empirical evaluation the new procedure significantly outperforms state-of-the-art",
    "checked": true,
    "id": "4734857010ac03c0b62d8864302f2dcce25aaade",
    "semantic_title": "recursive pac-bayes: a frequentist approach to sequential prior updates with no information loss",
    "citation_count": 2,
    "authors": [
      "Yi-Shan Wu",
      "Yijie Zhang",
      "Badr-Eddine Cherief-Abdellatif",
      "Yevgeny Seldin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/201408406e0c5cf7626c4baeae6eaadd-Abstract-Conference.html": {
    "title": "Towards Learning Group-Equivariant Features for Domain Adaptive 3D Detection",
    "volume": "main",
    "abstract": "The performance of 3D object detection in large outdoor point clouds deteriorates significantly in an unseen environment due to the inter-domain gap. To address these challenges, most existing methods for domain adaptation harness self-training schemes and attempt to bridge the gap by focusing on a single factor that causes the inter-domain gap, such as objects' sizes, shapes, and foreground density variation. However, the resulting adaptations suggest that there is still a substantial inter-domain gap left to be minimized. We argue that this is due to two limitations: 1) Biased pseudo-label collection from self-training. 2) Multiple factors jointly contributing to how the object is perceived in the unseen target domain. In this work, we propose a grouping-exploration strategy framework, Group Explorer Domain Adaptation ($\\textbf{GroupEXP-DA}$), to addresses those two issues. Specifically, our grouping divides the available label sets into multiple clusters and ensures all of them have equal learning attention with the group-equivariant spatial feature, avoiding dominant types of objects causing imbalance problems. Moreover, grouping learns to divide objects by considering inherent factors in a data-driven manner, without considering each factor separately as existing works. On top of the group-equivariant spatial feature that selectively detects objects similar to the input group, we additionally introduce an explorative group update strategy that reduces the false negative detection in the target domain, further reducing the inter-domain gap. During inference, only the learned group features are necessary for making the group-equivariant spatial feature, placing our method as a simple add-on that can be applicable to most existing detectors. We show how each module contributes to substantially bridging the inter-domain gaps compared to existing works across large urban outdoor datasets such as NuScenes, Waymo, and KITTI",
    "checked": true,
    "id": "5927a597ef56973a32343bf43d84ba4a7b6e680e",
    "semantic_title": "towards learning group-equivariant features for domain adaptive 3d detection",
    "citation_count": 0,
    "authors": [
      "Sangyun Shin",
      "Yuhang He",
      "Madhu Vankadari",
      "Ta-Ying Cheng",
      "Qian Xie",
      "Andrew Markham",
      "Niki Trigoni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/20468143d610020710689e3368338ffc-Abstract-Conference.html": {
    "title": "Beyond Efficiency: Molecular Data Pruning for Enhanced Generalization",
    "volume": "main",
    "abstract": "With the emergence of various molecular tasks and massive datasets, how to perform efficient training has become an urgent yet under-explored issue in the area. Data pruning (DP), as an oft-stated approach to saving training burdens, filters out less influential samples to form a coreset for training. However, the increasing reliance on pretrained models for molecular tasks renders traditional in-domain DP methods incompatible. Therefore, we propose a Molecular data Pruning framework for enhanced Generalization (MolPeg), which focuses on the source-free data pruning scenario, where data pruning is applied with pretrained models. By maintaining two models with different updating paces during training, we introduce a novel scoring function to measure the informativeness of samples based on the loss discrepancy. As a plug-and-play framework, MolPeg realizes the perception of both source and target domain and consistently outperforms existing DP methods across four downstream tasks. Remarkably, it can surpass the performance obtained from full-dataset training, even when pruning up to 60-70% of the data on HIV and PCBA dataset. Our work suggests that the discovery of effective data-pruning metrics could provide a viable path to both enhanced efficiency and superior generalization in transfer learning",
    "checked": true,
    "id": "ebf5ea24118f80ad547c0cda4edd41cd7c2a4e73",
    "semantic_title": "beyond efficiency: molecular data pruning for enhanced generalization",
    "citation_count": 5,
    "authors": [
      "Dingshuo Chen",
      "Zhixun Li",
      "Yuyan Ni",
      "Guibin Zhang",
      "Ding Wang",
      "Qiang Liu",
      "Shu Wu",
      "Jeffrey Yu",
      "Liang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/20563b8508ba42e1b688d922e926ee26-Abstract-Conference.html": {
    "title": "FedAvP: Augment Local Data via Shared Policy in Federated Learning",
    "volume": "main",
    "abstract": "Federated Learning (FL) allows multiple clients to collaboratively train models without directly sharing their private data. While various data augmentation techniques have been actively studied in the FL environment, most of these methods share input-level or feature-level data information over communication, posing potential privacy leakage. In response to this challenge, we introduce a federated data augmentation algorithm named FedAvP that shares only the augmentation policies, not the data-related information. For data security and efficient policy search, we interpret the policy loss as a meta update loss in standard FL algorithms and utilize the first-order gradient information to further enhance privacy and reduce communication costs. Moreover, we propose a meta-learning method to search for adaptive personalized policies tailored to heterogeneous clients. Our approach outperforms existing best performing augmentation policy search methods and federated data augmentation methods, in the benchmarks for heterogeneous FL",
    "checked": true,
    "id": "de33e194fd6c121266fda21599f2a05bf41cc544",
    "semantic_title": "fedavp: augment local data via shared policy in federated learning",
    "citation_count": 1,
    "authors": [
      "Minui Hong",
      "Junhyeog Yun",
      "Insu Jeon",
      "Gunhee Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/206018a258033def63607fbdf364bd2d-Abstract-Conference.html": {
    "title": "The Representation Landscape of Few-Shot Learning and Fine-Tuning in Large Language Models",
    "volume": "main",
    "abstract": "In-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks. Despite their different natures, these strategies often lead to comparable performance gains. However, little is known about whether they induce similar representations inside LLMs. We approach this problem by analyzing the probability landscape of their hidden representations in the two cases. More specifically, we compare how LLMs solve the same question-answering task, finding that ICL and SFT create very different internal structures, in both cases undergoing a sharp transition in the middle of the network. In the first half of the network, ICL shapes interpretable representations hierarchically organized according to their semantic content. In contrast, the probability landscape obtained with SFT is fuzzier and semantically mixed. In the second half of the model, the fine-tuned representations develop probability modes that better encode the identity of answers, while less-defined peaks characterize the landscape of ICL representations. Our approach reveals the diverse computational strategies developed inside LLMs to solve the same task across different conditions, allowing us to make a step towards designing optimal methods to extract information from language models",
    "checked": true,
    "id": "011bce5ed79da4351efbc5ca7648b595f20ef4cb",
    "semantic_title": "the representation landscape of few-shot learning and fine-tuning in large language models",
    "citation_count": 5,
    "authors": [
      "Diego Doimo",
      "Alessandro Serra",
      "Alessio Ansuini",
      "Alberto Cazzaniga"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2063a00c435aafbcc58c16ce1e522139-Abstract-Conference.html": {
    "title": "Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification",
    "volume": "main",
    "abstract": "Vision models excel in image classification but struggle to generalize to unseen data, such as classifying images from unseen domains or discovering novel categories. In this paper, we explore the relationship between logical reasoning and deep learning generalization in visual classification. A logical regularization termed L-Reg is derived which bridges a logical analysis framework to image classification. Our work reveals that L-Reg reduces the complexity of the model in terms of the feature distribution and classifier weights. Specifically, we unveil the interpretability brought by L-Reg, as it enables the model to extract the salient features, such as faces to persons, for classification. Theoretical analysis and experiments demonstrate that L-Reg enhances generalization across various scenarios, including multi-domain generalization and generalized category discovery. In complex real-world scenarios where images span unknown classes and unseen domains, L-Reg consistently improves generalization, highlighting its practical efficacy",
    "checked": true,
    "id": "59da88c46764ddd7128cf3c919db34ced0861485",
    "semantic_title": "interpret your decision: logical reasoning regularization for generalization in visual classification",
    "citation_count": 2,
    "authors": [
      "Zhaorui Tan",
      "Xi Yang",
      "Qiufeng Wang",
      "Anh Nguyen",
      "Kaizhu Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2093ed77c549eda95bd6f7212b735b43-Abstract-Conference.html": {
    "title": "Curriculum Fine-tuning of Vision Foundation Model for Medical Image Classification Under Label Noise",
    "volume": "main",
    "abstract": "Deep neural networks have demonstrated remarkable performance in various vision tasks, but their success heavily depends on the quality of the training data. Noisy labels are a critical issue in medical datasets and can significantly degrade model performance. Previous clean sample selection methods have not utilized the well pre-trained features of vision foundation models (VFMs) and assumed that training begins from scratch. In this paper, we propose CUFIT, a curriculum fine-tuning paradigm of VFMs for medical image classification under label noise. Our method is motivated by the fact that linear probing of VFMs is relatively unaffected by noisy samples, as it does not update the feature extractor of the VFM, thus robustly classifying the training samples. Subsequently, curriculum fine-tuning of two adapters is conducted, starting with clean sample selection from the linear probing phase. Our experimental results demonstrate that CUFIT outperforms previous methods across various medical image benchmarks. Specifically, our method surpasses previous baselines by 5.0\\%, 2.1\\%, 4.6\\%, and 5.8\\% at a 40\\% noise rate on the HAM10000, APTOS-2019, BloodMnist, and OrgancMnist datasets, respectively. Furthermore, we provide extensive analyses to demonstrate the impact of our method on noisy label detection. For instance, our method shows higher label precision and recall compared to previous approaches. Our work highlights the potential of leveraging VFMs in medical image classification under challenging conditions of noisy labels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeonguk Yu",
      "Minhwan Ko",
      "Sungho Shin",
      "Kangmin Kim",
      "Kyoobin Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/209423f076b6479ab3a4f45886e30306-Abstract-Conference.html": {
    "title": "Inferring stochastic low-rank recurrent neural networks from neural data",
    "volume": "main",
    "abstract": "A central aim in computational neuroscience is to relate the activity of large populations of neurons to an underlying dynamical system. Models of these neural dynamics should ideally be both interpretable and fit the observed data well. Low-rank recurrent neural networks (RNNs) exhibit such interpretability by having tractable dynamics. However, it is unclear how to best fit low-rank RNNs to data consisting of noisy observations of an underlying stochastic system. Here, we propose to fit stochastic low-rank RNNs with variational sequential Monte Carlo methods. We validate our method on several datasets consisting of both continuous and spiking neural data, where we obtain lower dimensional latent dynamics than current state of the art methods. Additionally, for low-rank models with piecewise linear nonlinearities, we show how to efficiently identify all fixed points in polynomial rather than exponential cost in the number of units, making analysis of the inferred dynamics tractable for large RNNs. Our method both elucidates the dynamical systems underlying experimental recordings and provides a generative model whose trajectories match observed variability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthijs Pals",
      "A Erdem Sağtekin",
      "Felix Pei",
      "Manuel Gloeckler",
      "Jakob H Macke"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2096bafd3073a2224f6f0adb594068df-Abstract-Conference.html": {
    "title": "Cardinality-Aware Set Prediction and Top-$k$ Classification",
    "volume": "main",
    "abstract": "We present a detailed study of cardinality-aware top-$k$ classification, a novel approach that aims to learn an accurate top-$k$ set predictor while maintaining a low cardinality. We introduce a new target loss function tailored to this setting that accounts for both the classification error and the cardinality of the set predicted. To optimize this loss function, we propose two families of surrogate losses: cost-sensitive comp-sum losses and cost-sensitive constrained losses. Minimizing these loss functions leads to new cardinality-aware algorithms that we describe in detail in the case of both top-$k$ and threshold-based classifiers. We establish $H$-consistency bounds for our cardinality-aware surrogate loss functions, thereby providing a strong theoretical foundation for our algorithms. We report the results of extensive experiments on CIFAR-10, CIFAR-100, ImageNet, and SVHN datasets demonstrating the effectiveness and benefits of our cardinality-aware algorithms",
    "checked": false,
    "id": "b4051f5482cb6198340eb8def97cac3322543e7b",
    "semantic_title": "cardinality-aware set prediction and top-k classification",
    "citation_count": 13,
    "authors": [
      "Corinna Cortes",
      "Anqi Mao",
      "Christopher Mohri",
      "Mehryar Mohri",
      "Yutao Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/20b6b87ca17792337f414d948af7b0e8-Abstract-Conference.html": {
    "title": "In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization",
    "volume": "main",
    "abstract": "We study the \\emph{in-context learning} (ICL) ability of a \\emph{Linear Transformer Block} (LTB) that combines a linear attention component and a linear multi-layer perceptron (MLP) component. For ICL of linear regression with a Gaussian prior and a \\emph{non-zero mean}, we show that LTB can achieve nearly Bayes optimal ICL risk. In contrast, using only linear attention must incur an irreducible additive approximation error. Furthermore, we establish a correspondence between LTB and one-step gradient descent estimators with learnable initialization ($\\mathsf{GD}-\\beta$), in the sense that every $\\mathsf{GD}-\\beta$ estimator can be implemented by an LTB estimator and every optimal LTB estimator that minimizes the in-class ICL risk is effectively a $\\mathsf{GD}-\\beta$ estimator.Finally, we show that $\\mathsf{GD}-\\beta$ estimators can be efficiently optimized with gradient flow, despite a non-convex training objective.Our results reveal that LTB achieves ICL by implementing $\\mathsf{GD}-\\beta$, and they highlight the role of MLP layers in reducing approximation error",
    "checked": true,
    "id": "ea3152821ce9ddd005429378073197feb6684398",
    "semantic_title": "in-context learning of a linear transformer block: benefits of the mlp component and one-step gd initialization",
    "citation_count": 18,
    "authors": [
      "Ruiqi Zhang",
      "Jingfeng Wu",
      "Peter L. Bartlett"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/20c3775b73f9190b861f55ec0be9f53e-Abstract-Conference.html": {
    "title": "Learning 3D Equivariant Implicit Function with Patch-Level Pose-Invariant Representation",
    "volume": "main",
    "abstract": "Implicit neural representation gains popularity in modeling the continuous 3D surface for 3D representation and reconstruction. In this work, we are motivated by the fact that the local 3D patches repeatedly appear on 3D shapes/surfaces if the factor of poses is removed. Based on this observation, we propose the 3D patch-level equivariant implicit function (PEIF) based on the 3D patch-level pose-invariant representation, allowing us to reconstruct 3D surfaces by estimating equivariant displacement vector fields for query points. Specifically, our model is based on the pose-normalized query/patch pairs and enhanced by the proposed intrinsic patch geometry representation, modeling the intrinsic 3D patch geometry feature by learnable multi-head memory banks. Extensive experiments show that our model achieves state-of-the-art performance on multiple surface reconstruction datasets, and also exhibits better generalization to crossdataset shapes and robustness to arbitrary rotations. Our code will be available at https://github.com/mathXin112/PEIF.git",
    "checked": true,
    "id": "85dc4d51c257d101644f25ad1f7cb6617809ae48",
    "semantic_title": "learning 3d equivariant implicit function with patch-level pose-invariant representation",
    "citation_count": 1,
    "authors": [
      "Xin Hu",
      "Xiaole Tang",
      "Ruixuan Yu",
      "Jian Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/20cea6c1b36ae5f69c48427a68b67fbc-Abstract-Conference.html": {
    "title": "Communication Efficient Distributed Training with Distributed Lion",
    "volume": "main",
    "abstract": "The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages in memory, computation, and sample efficiency. In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments. Leveraging the sign operator in Lion, our Distributed Lion only requires to communicate binary or lower-precision vectorsbetween workers to the center server, significantly reducing the communication cost. Our theoretical analysis confirms Distributed Lion's convergence properties. Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems. Notably, Distributed Lion attains comparable performance to standard Lion or AdamW optimizers applied on aggregated gradients, but with significantly reduced communication bandwidth. This feature is particularly advantageous for training large models. In addition, we also demonstrate that \\mavolion{} presents a more favorable performance-bandwidth balance compared to existing efficient distributed methods such as deep gradient compression and ternary gradients",
    "checked": true,
    "id": "012fdaeb8d0a681a21777cb06b7cf9315de63a59",
    "semantic_title": "communication efficient distributed training with distributed lion",
    "citation_count": 7,
    "authors": [
      "Bo Liu",
      "Lemeng Wu",
      "Lizhang Chen",
      "Kaizhao Liang",
      "Jiaxu Zhu",
      "Chen Liang",
      "Raghuraman Krishnamoorthi",
      "Qiang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/20ceffa1fcec0f882868e5b891e3e7fa-Abstract-Conference.html": {
    "title": "Fairness and Efficiency in Online Class Matching",
    "volume": "main",
    "abstract": "The online bipartite matching problem, extensively studied in the literature, deals with the allocation of online arriving vertices (items) to a predetermined set of offline vertices (agents). However, little attention has been given to the concept of class fairness, where agents are categorized into different classes, and the matching algorithm must ensure equitable distribution across these classes.We here focus on randomized algorithms for the fair matching of indivisible items, subject to various definitions of fairness. Our main contribution is the first (randomized) non-wasteful algorithm that simultaneously achieves a $1/2$ approximation to class envy-freeness (CEF) while simultaneously ensuring an equivalent approximation to the class proportionality (CPROP) and utilitarian social welfare (USW) objectives. We supplement this result by demonstrating that no non-wasteful algorithm can achieve an $\\alpha$-CEF guarantee for $\\alpha > 0.761$. In a similar vein, we provide a novel input instance for deterministic divisible matching that demonstrates a nearly tight CEF approximation.Lastly, we define the ``price of fairness,\" which represents the trade-off between optimal and fair matching. We demonstrate that increasing the level of fairness in the approximation of the solution leads to a decrease in the objective of maximizing USW, following an inverse proportionality relationship",
    "checked": true,
    "id": "a24416fb359e2bee1cd4fd5995e74e7a0228bc33",
    "semantic_title": "fairness and efficiency in online class matching",
    "citation_count": 0,
    "authors": [
      "MohammadTaghi Hajiaghayi",
      "Shayan Jahan",
      "Mohammad Sharifi",
      "Suho Shin",
      "Max Springer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/20dcab0f14046a5c6b02b61da9f13229-Abstract-Conference.html": {
    "title": "Ad Auctions for LLMs via Retrieval Augmented Generation",
    "volume": "main",
    "abstract": "In the field of computational advertising, the integration of ads into the outputs of large language models (LLMs) presents an opportunity to support these services without compromising content integrity. This paper introduces novel auction mechanisms for ad allocation and pricing within the textual outputs of LLMs, leveraging retrieval-augmented generation (RAG). We propose a \\emph{segment auction} where an ad is probabilistically retrieved for each discourse segment (paragraph, section, or entire output) according to its bid and relevance, following the RAG framework, and priced according to competing bids. We show that our auction maximizes logarithmic social welfare, a new notion of welfare that balances allocation efficiency and fairness, and we characterize the associated incentive-compatible pricing rule. These results are extended to multi-ad allocation per segment. An empirical evaluation validates the feasibility and effectiveness of our approach over several ad auction scenarios, and exhibits inherent tradeoffs in metrics as we allow the LLM more flexibility to allocate ads",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MohammadTaghi Hajiaghayi",
      "Sébastien Lahaie",
      "Keivan Rezaei",
      "Suho Shin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/20e6b4dd2b1f82bc599c593882f67f75-Abstract-Conference.html": {
    "title": "ReVideo: Remake a Video with Motion and Content Control",
    "volume": "main",
    "abstract": "Despite significant advancements in video generation and editing using diffusion models, achieving accurate and localized video editing remains a substantial challenge. Additionally, most existing video editing methods primarily focus on altering visual content, with limited research dedicated to motion editing. In this paper, we present a novel attempt to Remake a Video (ReVideo) which stands out from existing methods by allowing precise video editing in specific areas through the specification of both content and motion. Content editing is facilitated by modifying the first frame, while the trajectory-based motion control offers an intuitive user interaction experience. ReVideo addresses a new task involving the coupling and training imbalance between content and motion control. To tackle this, we develop a three-stage training strategy that progressively decouples these two aspects from coarse to fine. Furthermore, we propose a spatiotemporal adaptive fusion module to integrate content and motion control across various sampling steps and spatial locations. Extensive experiments demonstrate that our ReVideo has promising performance on several accurate video editing applications, i.e., (1) locally changing video content while keeping the motion constant, (2) keeping content unchanged and customizing new motion trajectories, (3) modifying both content and motion trajectories. Our method can also seamlessly extend these applications to multi-area editing without specific training, demonstrating its flexibility and robustness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Mou",
      "Mingdeng Cao",
      "Xintao Wang",
      "Zhaoyang Zhang",
      "Ying Shan",
      "Jian Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/20fdaf67581e6d7157376d1ed584040a-Abstract-Conference.html": {
    "title": "Finding Transformer Circuits With Edge Pruning",
    "volume": "main",
    "abstract": "The path to interpreting a language model often proceeds via analysis of circuits---sparse computational subgraphs of the model that capture specific aspects of its behavior. Recent work has automated the task of discovering circuits. Yet, these methods have practical limitations, as they either rely on inefficient search algorithms or inaccurate approximations. In this paper, we frame circuit discovery as an optimization problem and propose Edge Pruning as an effective and scalable solution. Edge Pruning leverages gradient-based pruning techniques, but instead of removing neurons or components, prunes the edges between components. Our method finds circuits in GPT-2 that use less than half the number of edges than circuits found by previous methods while being equally faithful to the full model predictions on standard circuit-finding tasks. Edge Pruning is efficient on tasks involving up to 100,000 examples, outperforming previous methods in speed and producing substantially better circuits. It also perfectly recovers the ground-truth circuits in two models compiled with Tracr. Thanks to its efficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the size of GPT-2.We use this setting for a case study, where we compare the mechanisms behind instruction prompting and in-context learning.We find two circuits with more than 99.96% sparsity that match the performance of the full model. Further analysis reveals that the mechanisms in the two settings overlap substantially. This shows that Edge Pruning is a practical and scalable tool for interpretability, which can shed light on behaviors that only emerge in large models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adithya Bhaskar",
      "Alexander Wettig",
      "Dan Friedman",
      "Danqi Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2122ee4d9f8933f71bc21f748a37e245-Abstract-Conference.html": {
    "title": "Globally Convergent Variational Inference",
    "volume": "main",
    "abstract": "In variational inference (VI), an approximation of the posterior distribution is selected from a family of distributions through numerical optimization. With the most common variational objective function, known as the evidence lower bound (ELBO), only convergence to a local optimum can be guaranteed. In this work, we instead establish the global convergence of a particular VI method. This VI method, which may be considered an instance of neural posterior estimation (NPE), minimizes an expectation of the inclusive (forward) KL divergence to fit a variational distribution that is parameterized by a neural network. Our convergence result relies on the neural tangent kernel (NTK) to characterize the gradient dynamics that arise from considering the variational objective in function space. In the asymptotic regime of a fixed, positive-definite neural tangent kernel, we establish conditions under which the variational objective admits a unique solution in a reproducing kernel Hilbert space (RKHS). Then, we show that the gradient descent dynamics in function space converge to this unique function. In ablation studies and practical problems, we demonstrate that our results explain the behavior of NPE in non-asymptotic finite-neuron settings, and show that NPE outperforms ELBO-based optimization, which often converges to shallow local optima",
    "checked": false,
    "id": "616dae5f6c995e64de90f6b55a9c9818e921d904",
    "semantic_title": "on the convergence of coordinate ascent variational inference",
    "citation_count": 13,
    "authors": [
      "Declan McNamara",
      "Jackson Loper",
      "Jeffrey Regier"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/21315abf210cef71317e887e9cda78b3-Abstract-Conference.html": {
    "title": "Auditing Local Explanations is Hard",
    "volume": "main",
    "abstract": "In sensitive contexts, providers of machine learning algorithms are increasingly required to give explanations for their algorithms' decisions. However, explanation receivers might not trust the provider, who potentially could output misleading or manipulated explanations. In this work, we investigate an auditing framework in which a third-party auditor or a collective of users attempts to sanity-check explanations: they can query model decisions and the corresponding local explanations, pool all the information received, and then check for basic consistency properties. We prove upper and lower bounds on the amount of queries that are needed for an auditor to succeed within this framework. Our results show that successful auditing requires a potentially exorbitant number of queries -- particularly in high dimensional cases. Our analysis also reveals that a key property is the ``locality'' of the provided explanations --- a quantity that so far has not been paid much attention to in the explainability literature. Looking forward, our results suggest that for complex high-dimensional settings, merely providing a pointwise prediction and explanation could be insufficient, as there is no way for the users to verify that the provided explanations are not completely made-up",
    "checked": true,
    "id": "881098faf3287ca65675a15a8818d993cdd8730e",
    "semantic_title": "auditing local explanations is hard",
    "citation_count": 4,
    "authors": [
      "Robi Bhattacharjee",
      "Ulrike V. Luxburg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/215aeb07b5996c969c0123c3c6ee8f54-Abstract-Conference.html": {
    "title": "HumanVLA: Towards Vision-Language Directed Object Rearrangement by Physical Humanoid",
    "volume": "main",
    "abstract": "Physical Human-Scene Interaction (HSI) plays a crucial role in numerous applications. However, existing HSI techniques are limited to specific object dynamics and privileged information, which prevents the development of more comprehensive applications. To address this limitation, we introduce HumanVLA for general object rearrangement directed by practical vision and language. A teacher-student framework is utilized to develop HumanVLA. A state-based teacher policy is trained first using goal-conditioned reinforcement learning and adversarial motion prior. Then, it is distilled into a vision-language-action model via behavior cloning. We propose several key insights to facilitate the large-scale learning process. To support general object rearrangement by physical humanoid, we introduce a novel Human-in-the-Room dataset encompassing various rearrangement tasks. Through extensive experiments and analysis, we demonstrate the effectiveness of our approach",
    "checked": true,
    "id": "9f5ba6feaa9555868a7d7bde052a55d9081e1e87",
    "semantic_title": "humanvla: towards vision-language directed object rearrangement by physical humanoid",
    "citation_count": 11,
    "authors": [
      "Xinyu Xu",
      "Yizheng Zhang",
      "Yong-Lu Li",
      "Lei Han",
      "Cewu Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/216f4cd12cfd69d46770bb2b491ae24b-Abstract-Conference.html": {
    "title": "Efficient Reinforcement Learning by Discovering Neural Pathways",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) algorithms have been very successful at tackling complex control problems, such as AlphaGo or fusion control. However, current research mainly emphasizes solution quality, often achieved by using large models trained on large amounts of data, and does not account for the financial, environmental, and societal costs associated with developing and deploying such models. Modern neural networks are often overparameterized and a significant number of parameters can be pruned without meaningful loss in performance, resulting in more efficient use of the model's capacity lottery ticket. We present a methodology for identifying sub-networks within a larger network in reinforcement learning (RL). We call such sub-networks, neural pathways. We show empirically that even very small learned sub-networks, using less than 5% of the large network's parameters, can provide very good quality solutions. We also demonstrate the training of multiple pathways within the same networks in a multitask setup, where each pathway is encouraged to tackle a separate task. We evaluate empirically our approach on several continuous control tasks, in both online and offline training",
    "checked": true,
    "id": "9d55795a524399c2c0541ca357f84e1fe5204986",
    "semantic_title": "efficient reinforcement learning by discovering neural pathways",
    "citation_count": 4,
    "authors": [
      "Samin Yeasar Arnob",
      "Riyasat Ohib",
      "Sergey Plis",
      "Amy Zhang",
      "Alessandro Sordoni",
      "Doina Precup"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/21912f7057935149fa58408ee8cb460e-Abstract-Conference.html": {
    "title": "MeMo: Meaningful, Modular Controllers via Noise Injection",
    "volume": "main",
    "abstract": "Robots are often built from standardized assemblies, (e.g. arms, legs, or fingers), but each robot must be trained from scratch to control all the actuators of all the parts together. In this paper we demonstrate a new approach that takes a single robot and its controller as input and produces a set of modular controllers for each of these assemblies such that when a new robot is built from the same parts, its control can be quickly learned by reusing the modular controllers. We achieve this with a framework called MeMo which learns (Me)aningful, (Mo)dular controllers. Specifically, we propose a novel modularity objective to learn an appropriate division of labor among the modules. We demonstrate that this objective can be optimized simultaneously with standard behavior cloning loss via noise injection. We benchmark our framework in locomotion and grasping environments on simple to complex robot morphology transfer. We also show that the modules help in task transfer. On both structure and task transfer, MeMo achieves improved training efficiency to graph neural network and Transformer baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Megan Tjandrasuwita",
      "Jie Xu",
      "Armando Solar-Lezama",
      "Wojciech Matusik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/21a33eba893ca3890e395651b38810df-Abstract-Conference.html": {
    "title": "FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning",
    "volume": "main",
    "abstract": "Finding specific preference-guided Pareto solutions that represent different trade-offs among multiple objectives is critical yet challenging in multi-objective problems. Existing methods are restrictive in preference definitions and/or their theoretical guarantees.In this work, we introduce a Flexible framEwork for pREfeRence-guided multi-Objective learning (FERERO) by casting it as a constrained vector optimization problem.Specifically, two types of preferences are incorporated into this formulation -- the relative preference defined by the partial ordering induced by a polyhedral cone, and the absolute preference defined by constraints that are linear functions of the objectives. To solve this problem, convergent algorithms are developed with both single-loop and stochastic variants. Notably, this is the first single-loop primal algorithm for constrained optimization to our knowledge. The proposed algorithms adaptively adjust to both constraint and objective values, eliminating the need to solve different subproblems at different stages of constraint satisfaction. Experiments on multiple benchmarks demonstrate the proposed method is very competitive in finding preference-guided optimal solutions.Code is available at https://github.com/lisha-chen/FERERO/",
    "checked": true,
    "id": "cbe901f436440d0e196bb02e273b57044af228e6",
    "semantic_title": "ferero: a flexible framework for preference-guided multi-objective learning",
    "citation_count": 3,
    "authors": [
      "Lisha Chen",
      "A Saif",
      "Yanning Shen",
      "Tianyi Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/21a7b312c42af86b3cd17a26a8ec499e-Abstract-Conference.html": {
    "title": "RGMDT: Return-Gap-Minimizing Decision Tree Extraction in Non-Euclidean Metric Space",
    "volume": "main",
    "abstract": "Deep Reinforcement Learning (DRL) algorithms have achieved great success in solving many challenging tasks while their black-box nature hinders interpretability and real-world applicability, making it difficult for human experts to interpret and understand DRL policies. Existing works on interpretable reinforcement learning have shown promise in extracting decision tree (DT) based policies from DRL policies with most focus on the single-agent settings while prior attempts to introduce DT policies in multi-agent scenarios mainly focus on heuristic designs which do not provide any quantitative guarantees on the expected return.In this paper, we establish an upper bound on the return gap between the oracle expert policy and an optimal decision tree policy. This enables us to recast the DT extraction problem into a novel non-euclidean clustering problem over the local observation and action values space of each agent, with action values as cluster labels and the upper bound on the return gap as clustering loss.Both the algorithm and the upper bound are extended to multi-agent decentralized DT extractions by an iteratively-grow-DT procedure guided by an action-value function conditioned on the current DTs of other agents. Further, we propose the Return-Gap-Minimization Decision Tree (RGMDT) algorithm, which is a surprisingly simple design and is integrated with reinforcement learning through the utilization of a novel Regularized Information Maximization loss. Evaluations on tasks like D4RL show that RGMDT significantly outperforms heuristic DT-based baselines and can achieve nearly optimal returns under given DT complexity constraints (e.g., maximum number of DT nodes)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingdi Chen",
      "Hanhan Zhou",
      "Yongsheng Mei",
      "Carlee Joe-Wong",
      "Gina C. Adam",
      "Nathaniel Bastian",
      "Tian Lan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/21b5883bc8fec922fdbbb06675388164-Abstract-Conference.html": {
    "title": "WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off",
    "volume": "main",
    "abstract": "Watermarking is a technical means to dissuade malfeasant usage of Large Language Models.This paper proposes a novel watermarking scheme, so-called WaterMax, that enjoys high detectability while sustaining the quality of the generated text of the original LLM.Its new design leaves the LLM untouched (no modification of the weights, logits or temperature).WaterMax balances robustness and computational complexity contrary to the watermarking techniques of the literature inherently provoking a trade-off between quality and robustness.Its performance is both theoretically proven and experimentally validated.It outperforms all the SotA techniques under the most complete benchmark suite",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eva Giboulot",
      "Teddy Furon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/21b7e46991301ad2126c3e39384df6ed-Abstract-Conference.html": {
    "title": "Gene-Gene Relationship Modeling Based on Genetic Evidence for Single-Cell RNA-Seq Data Imputation",
    "volume": "main",
    "abstract": "Single-cell RNA sequencing (scRNA-seq) technologies enable the exploration of cellular heterogeneity and facilitate the construction of cell atlases. However, scRNA-seq data often contain a large portion of missing values (false zeros) or noisy values, hindering downstream analyses. To recover these false zeros, propagation-based imputation methods have been proposed using $k$-NN graphs. However they model only associating relationships among genes within a cell, while, according to well-known genetic evidence, there are both associating and dissociating relationships among genes. To apply this genetic evidence to gene-gene relationship modeling, this paper proposes a novel imputation method that newly employs dissociating relationships in addition to associating relationships. Our method constructs a $k$-NN graph to additionally model dissociating relationships via the negation of a given cell-gene matrix. Moreover, our method standardizes the value distribution (mean and variance) of each gene to have standard distributions regardless of the gene. Through extensive experiments, we demonstrate that the proposed method achieves exceptional performance gains over state-of-the-art methods in both cell clustering and gene expression recovery across six scRNA-seq datasets, validating the significance of using complete gene-gene relationships in accordance with genetic evidence. The source code is available at https://github.com/daehoum1/scCR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daeho Um",
      "Ji Won Yoon",
      "Seong Jin Ahn",
      "Yunha Yeo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/21c86d5b10cdc28664ccdadf0a29065a-Abstract-Conference.html": {
    "title": "Suppress Content Shift: Better Diffusion Features via Off-the-Shelf Generation Techniques",
    "volume": "main",
    "abstract": "Diffusion models are powerful generative models, and this capability can also be applied to discrimination. The inner activations of a pre-trained diffusion model can serve as features for discriminative tasks, namely, diffusion feature. We discover that diffusion feature has been hindered by a hidden yet universal phenomenon that we call content shift. To be specific, there are content differences between features and the input image, such as the exact shape of a certain object. We locate the cause of content shift as one inherent characteristic of diffusion models, which suggests the broad existence of this phenomenon in diffusion feature. Further empirical study also indicates that its negative impact is not negligible even when content shift is not visually perceivable. Hence, we propose to suppress content shift to enhance the overall quality of diffusion features. Specifically, content shift is related to the information drift during the process of recovering an image from the noisy input, pointing out the possibility of turning off-the-shelf generation techniques into tools for content shift suppression. We further propose a practical guideline named GATE to efficiently evaluate the potential benefit of a technique and provide an implementation of our methodology. Despite the simplicity, the proposed approach has achieved superior results on various tasks and datasets, validating its potential as a generic booster for diffusion features. Our code is available at https://github.com/Darkbblue/diffusion-content-shift",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benyuan Meng",
      "Qianqian Xu",
      "Zitai Wang",
      "Zhiyong Yang",
      "Xiaochun Cao",
      "Qingming Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/21cf8411ed825614e00006a1d9aab7e4-Abstract-Conference.html": {
    "title": "Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting",
    "volume": "main",
    "abstract": "When humans need to learn a new skill, we can acquire knowledge through written books, including textbooks, tutorials, etc. However, current research for decision-making, like reinforcement learning (RL), has primarily required numerous real interactions with the target environment to learn a skill, while failing to utilize the existing knowledge already summarized in the text. The success of Large Language Models (LLMs) sheds light on utilizing such knowledge behind the books. In this paper, we discuss a new policy learning problem called Policy Learning from tutorial Books (PLfB) upon the shoulders of LLMs' systems, which aims to leverage rich resources such as tutorial books to derive a policy network. Inspired by how humans learn from books, we solve the problem via a three-stage framework: Understanding, Rehearsing, and Introspecting (URI). In particular, it first rehearses decision-making trajectories based on the derived knowledge after understanding the books, then introspects in the imaginary dataset to distill a policy network. We build two benchmarks for PLfB~based on Tic-Tac-Toe and Football games. In experiment, URI's policy achieves at least 44% net win rate against GPT-based agents without any real data; In Football game, which is a complex scenario, URI's policy beat the built-in AIs with a 37% while using GPT-based agent can only achieve a 6\\% winning rate. The project page: https://plfb-football.github.io",
    "checked": true,
    "id": "82cbf4ce6db5c5d7ec6ffdbcfd3147e6c8f33df5",
    "semantic_title": "policy learning from tutorial books via understanding, rehearsing and introspecting",
    "citation_count": 4,
    "authors": [
      "Xiong-Hui Chen",
      "Ziyan Wang",
      "Yali Du",
      "Shengyi Jiang",
      "Meng Fang",
      "Yang Yu",
      "Jun Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/21eba560be81c3a1e1f3404493a92a6a-Abstract-Conference.html": {
    "title": "Image Reconstruction Via Autoencoding Sequential Deep Image Prior",
    "volume": "main",
    "abstract": "Recently, Deep Image Prior (DIP) has emerged as an effective unsupervised one-shot learner, delivering competitive results across various image recovery problems. This method only requires the noisy measurements and a forward operator, relying solely on deep networks initialized with random noise to learn and restore the structure of the data. However, DIP is notorious for its vulnerability to overfitting due to the overparameterization of the network. Building upon insights into the impact of the DIP input and drawing inspiration from the gradual denoising process in cutting-edge diffusion models, we introduce Autoencoding Sequential DIP (aSeqDIP) for image reconstruction. This method progressively denoises and reconstructs the image through a sequential optimization of network weights. This is achieved using an input-adaptive DIP objective, combined with an autoencoding regularization term. Compared to diffusion models, our method does not require training data and outperforms other DIP-based methods in mitigating noise overfitting while maintaining a similar number of parameter updates as Vanilla DIP. Through extensive experiments, we validate the effectiveness of our method in various image reconstruction tasks, such as MRI and CT reconstruction, as well as in image restoration tasks like image denoising, inpainting, and non-linear deblurring",
    "checked": true,
    "id": "0211e5841d3a05e236b423a1191814a25fc07cf5",
    "semantic_title": "image reconstruction via autoencoding sequential deep image prior",
    "citation_count": 3,
    "authors": [
      "Ismail Alkhouri",
      "Shijun Liang",
      "Evan Bell",
      "Qing Qu",
      "Rongrong Wang",
      "Saiprasad Ravishankar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/21f6bdb10b7e019543daf35012e79210-Abstract-Conference.html": {
    "title": "Invariant subspaces and PCA in nearly matrix multiplication time",
    "volume": "main",
    "abstract": "Approximating invariant subspaces of generalized eigenvalue problems (GEPs) is a fundamental computational problem at the core of machine learning and scientific computing. It is, for example, the root of Principal Component Analysis (PCA) for dimensionality reduction, data visualization, and noise filtering, and of Density Functional Theory (DFT), arguably the most popular method to calculate the electronic structure of materials. Given Hermitian $H,S\\in\\mathbb{C}^{n\\times n}$, where $S$ is positive-definite, let $\\Pi_k$ be the true spectral projector on the invariant subspace that is associated with the $k$ smallest (or largest) eigenvalues of the GEP $HC=SC\\Lambda$, for some $k\\in[n]$. We show that we can compute a matrix $\\widetilde\\Pi_k$ such that $\\lVert\\Pi_k-\\widetilde\\Pi_k\\rVert_2\\leq \\epsilon$, in $O\\left( n^{\\omega+\\eta}\\mathrm{polylog}(n,\\epsilon^{-1},\\kappa(S),\\mathrm{gap}_k^{-1}) \\right)$ bit operations in the floating point model, for some $\\epsilon\\in(0,1)$, with probability $1-1/n$. Here, $\\eta>0$ is arbitrarily small, $\\omega\\lesssim 2.372$ is the matrix multiplication exponent, $\\kappa(S)=\\lVert S\\rVert_2\\lVert S^{-1}\\rVert_2$, and $\\mathrm{gap}_k$ is the gap between eigenvalues $k$ and $k+1$. To achieve such provable \"forward-error\" guarantees, our methods rely on a new $O(n^{\\omega+\\eta})$ stability analysis for the Cholesky factorization, and a smoothed analysis for computing spectral gaps, which can be of independent interest.Ultimately, we obtain new matrix multiplication-type bit complexity upper bounds for PCA problems, including classical PCA and (randomized) low-rank approximation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandros Sobczyk",
      "Marko Mladenovic",
      "Mathieu Luisier"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/21f76686538a5f06dc431efea5f475f5-Abstract-Conference.html": {
    "title": "Boosting Graph Pooling with Persistent Homology",
    "volume": "main",
    "abstract": "Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power. However, naively plugging PH features into GNN layers always results in marginal improvement with low interpretability. In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner. In this fashion, message passing in the coarsened graph acts along persistent pooled topology, leading to improved performance. Experimentally, we apply our mechanism to a collection of graph pooling methods and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility",
    "checked": true,
    "id": "da6adce623a27a22194a1d9890a76c99880d54d0",
    "semantic_title": "boosting graph pooling with persistent homology",
    "citation_count": 6,
    "authors": [
      "Chaolong Ying",
      "Xinjian Zhao",
      "Tianshu Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/21f7b745f73ce0d1f9bcea7f40b1388e-Abstract-Conference.html": {
    "title": "OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary Understanding",
    "volume": "main",
    "abstract": "This paper introduces OpenGaussian, a method based on 3D Gaussian Splatting (3DGS) that possesses the capability for 3D point-level open vocabulary understanding. Our primary motivation stems from observing that existing 3DGS-based open vocabulary methods mainly focus on 2D pixel-level parsing. These methods struggle with 3D point-level tasks due to weak feature expressiveness and inaccurate 2D-3D feature associations. To ensure robust feature presentation and 3D point-level understanding, we first employ SAM masks without cross-frame associations to train instance features with 3D consistency. These features exhibit both intra-object consistency and inter-object distinction. Then, we propose a two-stage codebook to discretize these features from coarse to fine levels. At the coarse level, we consider the positional information of 3D points to achieve location-based clustering, which is then refined at the fine level.Finally, we introduce an instance-level 3D-2D feature association method that links 3D points to 2D masks, which are further associated with 2D CLIP features. Extensive experiments, including open vocabulary-based 3D object selection, 3D point cloud understanding, click-based 3D object selection, and ablation studies, demonstrate the effectiveness of our proposed method. The source code is available at our project page https://3d-aigc.github.io/OpenGaussian",
    "checked": true,
    "id": "ed1643e6ac201a38767ae24ee07aeda6837068ea",
    "semantic_title": "opengaussian: towards point-level 3d gaussian-based open vocabulary understanding",
    "citation_count": 53,
    "authors": [
      "Yanmin Wu",
      "Jiarui Meng",
      "Haijie LI",
      "Chenming Wu",
      "Yahao Shi",
      "Xinhua Cheng",
      "Chen Zhao",
      "Haocheng Feng",
      "Errui Ding",
      "Jingdong Wang",
      "Jian Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/220cbc7435d6a56205c87d73d15d9eda-Abstract-Conference.html": {
    "title": "Proportional Fairness in Non-Centroid Clustering",
    "volume": "main",
    "abstract": "We revisit the recently developed framework of proportionally fair clustering, where the goal is to provide group fairness guarantees that become stronger for groups of data points that are large and cohesive. Prior work applies this framework to centroid-based clustering, where points are partitioned into clusters, and the cost to each data point is measured by its distance to a centroid assigned to its cluster. However, real-life applications often do not require such centroids. We extend the theory of proportionally fair clustering to non-centroid clustering by considering a variety of cost functions, both metric and non-metric, for a data point to be placed in a cluster with other data points. Our results indicate that Greedy Capture, a clustering algorithm developed for centroid clustering, continues to provide strong proportional fairness guarantees for non-centroid clustering, although the guarantees are significantly different and establishing them requires novel proof ideas. We also design algorithms for auditing proportional fairness of a given clustering solution. We conduct experiments on real data which suggest that traditional clustering algorithms are highly unfair, while our algorithms achieve strong fairness guarantees with a moderate loss in common clustering objectives",
    "checked": true,
    "id": "6d58616105fc3a9357c2d7d82efb1900114a7035",
    "semantic_title": "proportional fairness in non-centroid clustering",
    "citation_count": 6,
    "authors": [
      "Ioannis Caragiannis",
      "Evi Micha",
      "Nisarg Shah"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/221ccaeaef4b9cc8f89b63d6fc98a271-Abstract-Conference.html": {
    "title": "PaGoDA: Progressive Growing of a One-Step Generator from a Low-Resolution Diffusion Teacher",
    "volume": "main",
    "abstract": "The diffusion model performs remarkable in generating high-dimensional content but is computationally intensive, especially during training. We propose Progressive Growing of Diffusion Autoencoder (PaGoDA), a novel pipeline that reduces the training costs through three stages: training diffusion on downsampled data, distilling the pretrained diffusion, and progressive super-resolution. With the proposed pipeline, PaGoDA achieves a $64\\times$ reduced cost in training its diffusion model on $8\\times$ downsampled data; while at the inference, with the single-step, it performs state-of-the-art on ImageNet across all resolutions from $64\\times64$ to $512\\times512$, and text-to-image. PaGoDA's pipeline can be applied directly in the latent space, adding compression alongside the pre-trained autoencoder in Latent Diffusion Models (e.g., Stable Diffusion). The code is available at https://github.com/sony/pagoda",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongjun Kim",
      "Chieh-Hsin Lai",
      "Wei-Hsiang Liao",
      "Yuhta Takida",
      "Naoki Murata",
      "Toshimitsu Uesaka",
      "Yuki Mitsufuji",
      "Stefano Ermon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/223df9b793a8b384bf38b963e35e2cac-Abstract-Conference.html": {
    "title": "Interpretable Concept-Based Memory Reasoning",
    "volume": "main",
    "abstract": "The lack of transparency in the decision-making processes of deep learning systems presents a significant challenge in modern artificial intelligence (AI), as it impairs users' ability to rely on and verify these systems. To address this challenge, Concept Bottleneck Models (CBMs) have made significant progress by incorporating human-interpretable concepts into deep learning architectures. This approach allows predictions to be traced back to specific concept patterns that users can understand and potentially intervene on. However, existing CBMs' task predictors are not fully interpretable, preventing a thorough analysis and any form of formal verification of their decision-making process prior to deployment, thereby raising significant reliability concerns. To bridge this gap, we introduce Concept-based Memory Reasoner (CMR), a novel CBM designed to provide a human-understandable and provably-verifiable task prediction process. Our approach is to model each task prediction as a neural selection mechanism over a memory of learnable logic rules, followed by a symbolic evaluation of the selected rule. The presence of an explicit memory and the symbolic evaluation allow domain experts to inspect and formally verify the validity of certain global properties of interest for the task prediction process. Experimental results demonstrate that CMR achieves better accuracy-interpretability trade-offs to state-of-the-art CBMs, discovers logic rules consistent with ground truths, allows for rule interventions, and allows pre-deployment verification",
    "checked": true,
    "id": "f99e060ab2d16603091ba21737934a24975782b7",
    "semantic_title": "interpretable concept-based memory reasoning",
    "citation_count": 8,
    "authors": [
      "David Debot",
      "Pietro Barbiero",
      "Francesco Giannini",
      "Gabriele Ciravegna",
      "Michelangelo Diligenti",
      "Giuseppe Marra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/223eb69a2f2fb97fde58eaa958babb7a-Abstract-Conference.html": {
    "title": "Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion",
    "volume": "main",
    "abstract": "Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description?In this paper, we present Make-An-Agent, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation. Guided by behavior embeddings that encode trajectory information, our policy generator synthesizes latent parameter representations, which can then be decoded into policy networks. Trained on policy network checkpoints and their corresponding trajectories, our generation model demonstrates remarkable versatility and scalability on multiple tasks and has a strong generalization ability on unseen tasks to output well-performed policies with only few-shot demonstrations as inputs. We showcase its efficacy and efficiency on various domains and tasks, including varying objectives, behaviors, and even across different robot manipulators. Beyond simulation, we directly deploy policies generated by Make-An-Agent onto real-world robots on locomotion tasks. Project page: https://cheryyunl.github.io/make-an-agent/",
    "checked": true,
    "id": "9bb60b570d9dcb9adb820e3c7cd7974a2c756882",
    "semantic_title": "make-an-agent: a generalizable policy network generator with behavior-prompted diffusion",
    "citation_count": 4,
    "authors": [
      "Yongyuan Liang",
      "Tingqiang Xu",
      "Kaizhe Hu",
      "Guangqi Jiang",
      "Furong Huang",
      "Huazhe Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/227404a13d20898dec2018ebe368b202-Abstract-Conference.html": {
    "title": "Evaluating the design space of diffusion-based generative models",
    "volume": "main",
    "abstract": "Most existing theoretical investigations of the accuracy of diffusion models, albeit significant, assume the score function has been approximated to a certain accuracy, and then use this a priori bound to control the error of generation. This article instead provides a first quantitative understanding of the whole generation process, i.e., both training and sampling. More precisely, it conducts a non-asymptotic convergence analysis of denoising score matching under gradient descent. In addition, a refined sampling error analysis for variance exploding models is also provided. The combination of these two results yields a full error analysis, which elucidates (again, but this time theoretically) how to design the training and sampling processes for effective generation. For instance, our theory implies a preference toward noise distribution and loss weighting in training that qualitatively agree with the ones used in [Karras et al., 2022]. It also provides perspectives on the choices of time and variance schedules in sampling: when the score is well trained, the design in [Song et al., 2021] is more preferable, but when it is less trained, the design in [Karras et al., 2022] becomes more preferable",
    "checked": true,
    "id": "5925fcf34d6f17c695cf8270e75005f33c3f38a8",
    "semantic_title": "evaluating the design space of diffusion-based generative models",
    "citation_count": 7,
    "authors": [
      "Yuqing Wang",
      "Ye He",
      "Molei Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2277e2c5d7d20f847c2b7d5f9075ce30-Abstract-Conference.html": {
    "title": "SOI: Scaling Down Computational Complexity by Estimating Partial States of the Model",
    "volume": "main",
    "abstract": "Consumer electronics used to follow the miniaturization trend described by Moore's Law. Despite increased processing power in Microcontroller Units (MCUs), MCUs used in the smallest appliances are still not capable of running even moderately big, state-of-the-art artificial neural networks (ANNs) especially in time-sensitive scenarios. In this work, we present a novel method called Scattered Online Inference (SOI) that aims to reduce the computational complexity of ANNs. SOI leverages the continuity and seasonality of time-series data and model predictions, enabling extrapolation for processing speed improvements, particularly in deeper layers. By applying compression, SOI generates more general inner partial states of ANN, allowing skipping full model recalculation at each inference",
    "checked": true,
    "id": "e3edf94a4bfe209e7c263482a3216ec9d20ac834",
    "semantic_title": "soi: scaling down computational complexity by estimating partial states of the model",
    "citation_count": 0,
    "authors": [
      "Grzegorz Stefański",
      "Paweł Daniluk",
      "Artur Szumaczuk",
      "Jakub Tkaczuk"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/22862040c1781356c8c3df4d00e5811b-Abstract-Conference.html": {
    "title": "Robust Neural Contextual Bandit against Adversarial Corruptions",
    "volume": "main",
    "abstract": "Contextual bandit algorithms aim to identify the optimal arm with the highest reward among a set of candidates, based on the accessible contextual information. Among these algorithms, neural contextual bandit methods have shown generally superior performances against linear and kernel ones, due to the representation power of neural networks. However, similar to other neural network applications, neural bandit algorithms can be vulnerable to adversarial attacks or corruptions on the received labels (i.e., arm rewards), which can lead to unexpected performance degradation without proper treatments. As a result, it is necessary to improve the robustness of neural bandit models against potential reward corruptions. In this work, we propose a novel neural contextual bandit algorithm named R-NeuralUCB, which utilizes a novel context-aware Gradient Descent (GD) training strategy to improve the robustness against adversarial reward corruptions. Under over-parameterized neural network settings, we provide regret analysis for R-NeuralUCB to quantify reward corruption impacts, without the commonly adopted arm separateness assumption in existing neural bandit works. We also conduct experiments against baselines on real data sets under different scenarios, in order to demonstrate the effectiveness of our proposed R-NeuralUCB",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhe Qi",
      "Yikun Ban",
      "Arindam Banerjee",
      "Jingrui He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/22912080c32a244c20bfc101033a6ac7-Abstract-Conference.html": {
    "title": "An Expectation-Maximization Algorithm for Training Clean Diffusion Models from Corrupted Observations",
    "volume": "main",
    "abstract": "Diffusion models excel in solving imaging inverse problems due to their ability to model complex image priors. However, their reliance on large, clean datasets for training limits their practical use where clean data is scarce. In this paper, we propose EMDiffusion, an expectation-maximization (EM) approach to train diffusion models from corrupted observations. Our method alternates between reconstructing clean images from corrupted data using a known diffusion model (E-step) and refining diffusion model weights based on these reconstructions (M-step). This iterative process leads the learned diffusion model to gradually converge to a local optimum, that is, to approximate the true clean data distribution. We validate our method through extensive experiments on diverse computational imaging tasks, including random inpainting, denoising, and deblurring, achieving new state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weimin Bai",
      "Yifei Wang",
      "Wenzheng Chen",
      "He Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/22ae669a35bb9e70eb93ab77c1eff5b4-Abstract-Conference.html": {
    "title": "Beyond Slow Signs in High-fidelity Model Extraction",
    "volume": "main",
    "abstract": "Deep neural networks, costly to train and rich in intellectual property value, areincreasingly threatened by model extraction attacks that compromise their confiden-tiality. Previous attacks have succeeded in reverse-engineering model parametersup to a precision of float64 for models trained on random data with at most threehidden layers using cryptanalytical techniques. However, the process was identifiedto be very time consuming and not feasible for larger and deeper models trained onstandard benchmarks. Our study evaluates the feasibility of parameter extractionmethods of Carlini et al. [1] further enhanced by Canales-Martínez et al. [2] formodels trained on standard benchmarks. We introduce a unified codebase thatintegrates previous methods and reveal that computational tools can significantlyinfluence performance. We develop further optimisations to the end-to-end attackand improve the efficiency of extracting weight signs by up to 14.8 times com-pared to former methods through the identification of easier and harder to extractneurons. Contrary to prior assumptions, we identify extraction of weights, notextraction of weight signs, as the critical bottleneck. With our improvements, a16,721 parameter model with 2 hidden layers trained on MNIST is extracted withinonly 98 minutes compared to at least 150 minutes previously. Finally, addressingmethodological deficiencies observed in previous studies, we propose new ways ofrobust benchmarking for future model extraction attacks",
    "checked": true,
    "id": "b892ac05369526432384a4cdf1d4d087f8bc45de",
    "semantic_title": "beyond slow signs in high-fidelity model extraction",
    "citation_count": 2,
    "authors": [
      "Hanna Foerster",
      "Robert Mullins",
      "I Shumailov",
      "Jamie Hayes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/22b111819c74453837899689166c4cf9-Abstract-Conference.html": {
    "title": "FUGAL: Feature-fortified Unrestricted Graph Alignment",
    "volume": "main",
    "abstract": "The necessity to align two graphs, minimizing a structural distance metric, is prevalent in biology, chemistry, recommender systems, and social network analysis. Due to the problem's NP-hardness, prevailing graph alignment methods follow a modular and mediated approach, solving the problem by restricting to the domain of intermediary graph representations or products like embeddings, spectra, and graph signals. Restricting the problem to this intermediate space may distort the original problem and are hence predisposed to miss high-quality solutions. In this paper, we propose an unrestricted method, FUGAL, which finds a permutation matrix that maps one graph to another by directly operating on their adjacency matrices with judicious constraint relaxation. Extensive experimentation demonstrates that FUGAL consistently surpasses state-of-the-art graph alignment methods in accuracy across all benchmark datasets without encumbering efficiency",
    "checked": true,
    "id": "64b6d1eeb661967a811dc2b15ee3671eec56c152",
    "semantic_title": "fugal: feature-fortified unrestricted graph alignment",
    "citation_count": 2,
    "authors": [
      "Aditya Bommakanti",
      "Harshith Vonteri",
      "Konstantinos Skitsas",
      "Sayan Ranu",
      "Davide Mottin",
      "Panagiotis Karras"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/22b6bc18be9c2bfaa48adc1122f0a971-Abstract-Conference.html": {
    "title": "Progressive Entropic Optimal Transport Solvers",
    "volume": "main",
    "abstract": "Optimal transport (OT) has profoundly impacted machine learning by providing theoretical and computational tools to realign datasets.In this context, given two large point clouds of sizes $n$ and $m$ in $\\mathbb{R}^d$, entropic OT (EOT) solvers have emerged as the most reliable tool to either solve the Kantorovich problem and output a $n\\times m$ coupling matrix, or to solve the Monge problem and learn a vector-valued push-forward map. While the robustness of EOT couplings/maps makes them a go-to choice in practical applications, EOT solvers remain difficult to tune because of a small but influential set of hyperparameters, notably the omnipresent entropic regularization strength $\\varepsilon$. Setting $\\varepsilon$ can be difficult, as it simultaneously impacts various performance metrics, such as compute speed, statistical performance, generalization, and bias. In this work, we propose a new class of EOT solvers (ProgOT), that can estimate both plans and transport maps.We take advantage of several opportunities to optimize the computation of EOT solutions by *dividing* mass displacement using a time discretization, borrowing inspiration from dynamic OT formulations, and *conquering* each of these steps using EOT with properly scheduled parameters. We provide experimental evidence demonstrating that ProgOT is a faster and more robust alternative to *standard solvers* when computing couplings at large scales, even outperforming neural network-based approaches. We also prove statistical consistency of our approach for estimating OT maps",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parnian Kassraie",
      "Aram-Alexandre Pooladian",
      "Michal Klein",
      "James Thornton",
      "Jonathan Niles-Weed",
      "Marco Cuturi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/22c799f287fd05e7174fd65a3ce134af-Abstract-Conference.html": {
    "title": "Optimizing the coalition gain in Online Auctions with Greedy Structured Bandits",
    "volume": "main",
    "abstract": "Motivated by online display advertising, this work considers repeated second-price auctions, where agents sample their value from an unknown distribution with cumulative distribution function $F$. In each auction $t$, a decision-maker bound by limited observations selects $n_t$ agents from a coalition of $N$ to compete for a prize with $p$ other agents, aiming to maximize the cumulative reward of the coalition across all auctions.The problem is framed as an $N$-armed structured bandit, each number of player sent being an arm $n$, with expected reward $r(n)$ fully characterized by $F$ and $p+n$. We present two algorithms, Local-Greedy (LG) and Greedy-Grid (GG), both achieving *constant* problem-dependent regret. This relies on three key ingredients: **1.** an estimator of $r(n)$ from feedback collected from any arm $k$, **2.** concentration bounds of these estimates for $k$ within an estimation neighborhood of $n$ and **3.** the unimodality property of $r$ under standard assumptions on $F$. Additionally, GG exhibits problem-independent guarantees on top of best problem-dependent guarantees. However, by avoiding to rely on confidence intervals, LG practically outperforms GG, as well as standard unimodal bandit algorithms such as OSUB or multi-armed bandit algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dorian Baudry",
      "Hugo Richard",
      "Maria Cherifa",
      "Vianney Perchet",
      "Clément Calauzènes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/22d258dfbdf840ccbf266bbc545dd95f-Abstract-Conference.html": {
    "title": "DEFT: Efficient Fine-tuning of Diffusion Models by Learning the Generalised $h$-transform",
    "volume": "main",
    "abstract": "Generative modelling paradigms based on denoising diffusion processes have emerged as a leading candidate for conditional sampling in inverse problems. In many real-world applications, we often have access to large, expensively trained unconditional diffusion models, which we aim to exploit for improving conditional sampling.Most recent approaches are motivated heuristically and lack a unifying framework, obscuring connections between them. Further, they often suffer from issues such as being very sensitive to hyperparameters, being expensive to train or needing access to weights hidden behind a closed API. In this work, we unify conditional training and sampling using the mathematically well-understood Doob's h-transform. This new perspective allows us to unify many existing methods under a common umbrella. Under this framework, we propose DEFT (Doob's h-transform Efficient FineTuning), a new approach for conditional generation that simply fine-tunes a very small network to quickly learn the conditional $h$-transform, while keeping the larger unconditional network unchanged. DEFT is much faster than existing baselines while achieving state-of-the-art performance across a variety of linear and non-linear benchmarks. On image reconstruction tasks, we achieve speedups of up to 1.6$\\times$, while having the best perceptual quality on natural images and reconstruction performance on medical images. Further, we also provide initial experiments on protein motif scaffolding and outperform reconstruction guidance methods",
    "checked": true,
    "id": "8899a55b0db3d1305ad380db5cbe8408972cf294",
    "semantic_title": "deft: efficient fine-tuning of diffusion models by learning the generalised $h$-transform",
    "citation_count": 12,
    "authors": [
      "Alexander Denker",
      "Francisco Vargas",
      "Shreyas Padhy",
      "Kieran Didi",
      "Simon Mathis",
      "Riccardo Barbano",
      "Vincent Dutordoir",
      "Emile Mathieu",
      "Urszula Julia Komorowska",
      "Pietro Lió"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/22d72e9f55bc29cafcca6814a7feac8c-Abstract-Conference.html": {
    "title": "LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate",
    "volume": "main",
    "abstract": "High-resolution images offer more information about scenes that can improve model accuracy. However, the dominant model architecture in computer vision, the vision transformer (ViT), cannot effectively leverage larger images without finetuning — ViTs poorly extrapolate to more patches at test time, although transformers offer sequence length flexibility. We attribute this shortcoming to the current patch position encoding methods, which create a distribution shift when extrapolating.We propose a drop-in replacement for the position encoding of plain ViTs that restricts attention heads to fixed fields of view, pointed in different directions, using 2D attention masks. Our novel method, called LookHere, provides translation-equivariance, ensures attention head diversity, and limits the distribution shift that attention heads face when extrapolating. We demonstrate that LookHere improves performance on classification (avg. 1.6%), against adversarial attack (avg. 5.4%), and decreases calibration error (avg. 1.5%) — on ImageNet without extrapolation. With extrapolation, LookHere outperforms the current SoTA position encoding method, 2D-RoPE, by 21.7% on ImageNet when trained at $224^2$ px and tested at $1024^2$ px. Additionally, we release a high-resolution test set to improve the evaluation of high-resolution image classifiers, called ImageNet-HR",
    "checked": true,
    "id": "265b113b0fa8b6fc5192148a6f6730dddda2d2b6",
    "semantic_title": "lookhere: vision transformers with directed attention generalize and extrapolate",
    "citation_count": 4,
    "authors": [
      "Anthony Fuller",
      "Daniel Kyrollos",
      "Yousef Yassin",
      "James Green"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2302f4e66752149be7f63015a548a84c-Abstract-Conference.html": {
    "title": "Achieving Near-Optimal Convergence for Distributed Minimax Optimization with Adaptive Stepsizes",
    "volume": "main",
    "abstract": "In this paper, we show that applying adaptive methods directly to distributed minimax problems can result in non-convergence due to inconsistency in locally computed adaptive stepsizes. To address this challenge, we propose D-AdaST, a Distributed Adaptive minimax method with Stepsize Tracking. The key strategy is to employ an adaptive stepsize tracking protocol involving the transmission of two extra (scalar) variables. This protocol ensures the consistency among stepsizes of nodes, eliminating the steady-state error due to the lack of coordination of stepsizes among nodes that commonly exists in vanilla distributed adaptive methods, and thus guarantees exact convergence. For nonconvex-strongly-concave distributed minimax problems, we characterize the specific transient times that ensure time-scale separation of stepsizes and quasi-independence of networks, leading to a near-optimal convergence rate of $\\tilde{\\mathcal{O}} \\left( \\epsilon ^{-\\left( 4+\\delta \\right)} \\right)$ for any small $\\delta > 0$, matching that of the centralized counterpart. To our best knowledge, D-AdaST is the *first* distributed adaptive method achieving near-optimal convergence without knowing any problem-dependent parameters for nonconvex minimax problems. Extensive experiments are conducted to validate our theoretical results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Huang",
      "Xiang Li",
      "Yipeng Shen",
      "Niao He",
      "Jinming Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2318d75a06437eaa257737a5cf3ab83c-Abstract-Conference.html": {
    "title": "Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving",
    "volume": "main",
    "abstract": "\\emph{Metacognitive knowledge} refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans.To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems",
    "checked": true,
    "id": "79846e748a9e6324742c300453ec293661e90884",
    "semantic_title": "metacognitive capabilities of llms: an exploration in mathematical problem solving",
    "citation_count": 36,
    "authors": [
      "Aniket Didolkar",
      "Anirudh Goyal",
      "Nan Rosemary Ke",
      "Siyuan Guo",
      "Michal Valko",
      "Timothy Lillicrap",
      "Danilo Jimenez Rezende",
      "Yoshua Bengio",
      "Michael Mozer",
      "Sanjeev Arora"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/233ff375b9795cd890f78a1f64459b8d-Abstract-Conference.html": {
    "title": "Complete Graphical Criterion for Sequential Covariate Adjustment in Causal Inference",
    "volume": "main",
    "abstract": "Covariate adjustment, also known as back-door adjustment, is a fundamental tool in causal inference. Although a sound and complete graphical identification criterion, known as the adjustment criterion (Shpitser, 2010), exists for static contexts, sequential contexts present challenges. Current practices, such as the sequential back-door adjustment (Pearl, 1995) or multi-outcome sequential back-door adjustment (Jung, 2020), are sound but incomplete; i.e., there are graphical scenarios where the causal effect is expressible via covariate adjustment, yet these criteria do not cover. In this paper, we exemplify this incompleteness and then present the sequential adjustment criterion, a sound and complete criterion for sequential covariate adjustment. We provide a constructive sequential adjustment criterion that identifies a set that satisfies the sequential adjustment criterion if and only if the causal effect can be expressed as a sequential covariate adjustment. Finally, we present an algorithm for identifying a minimal sequential covariate adjustment set, which optimizes efficiency by ensuring that no unnecessary vertices are included",
    "checked": true,
    "id": "7a9cbdde556dd0b7aa076bea4da72e0206c99058",
    "semantic_title": "complete graphical criterion for sequential covariate adjustment in causal inference",
    "citation_count": 0,
    "authors": [
      "Yonghan Jung",
      "Min Woo Park",
      "Sanghack Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2341b14df6b4f684a30eb4e99807bea6-Abstract-Conference.html": {
    "title": "RETR: Multi-View Radar Detection Transformer for Indoor Perception",
    "volume": "main",
    "abstract": "Indoor radar perception has seen rising interest due to affordable costs driven by emerging automotive imaging radar developments and the benefits of reduced privacy concerns and reliability under hazardous conditions (e.g., fire and smoke). However, existing radar perception pipelines fail to account for distinctive characteristics of the multi-view radar setting. In this paper, we propose Radar dEtection TRansformer (RETR), an extension of the popular DETR architecture, tailored for multi-view radar perception. RETR inherits the advantages of DETR, eliminating the need for hand-crafted components for object detection and segmentation in the image plane. More importantly, RETR incorporates carefully designed modifications such as 1) depth-prioritized feature similarity via a tunable positional encoding (TPE); 2) a tri-plane loss from both radar and camera coordinates; and 3) a learnable radar-to-camera transformation via reparameterization, to account for the unique multi-view radar setting. Evaluated on two indoor radar perception datasets, our approach outperforms existing state-of-the-art methods by a margin of 15.38+ AP for object detection and 11.91+ IoU for instance segmentation, respectively. Our implementation is available at https://github.com/merlresearch/radar-detection-transformer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryoma Yataka",
      "Adriano Cardace",
      "Perry Wang",
      "Petros Boufounos",
      "Ryuhei Takahashi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2345275663a15ee92a06bc957be54a2c-Abstract-Conference.html": {
    "title": "On the Scalability of GNNs for Molecular Graphs",
    "volume": "main",
    "abstract": "Scaling deep learning models has been at the heart of recent revolutions in language modelling and image generation. Practitioners have observed a strong relationship between model size, dataset size, and performance. However, structure-based architectures such as Graph Neural Networks (GNNs) are yet to show the benefits of scale mainly due to lower efficiency of sparse operations, large data requirements, and lack of clarity about the effectiveness of various architectures. We address this drawback of GNNs by studying their scaling behavior. Specifically, we analyze message-passing networks, graph Transformers, and hybrid architectures on the largest public collection of 2D molecular graphs for supervised pretraining. For the first time, we observe that GNNs benefit tremendously from the increasing scale of depth, width, number of molecules and associated labels. A major factor is the diversity of the pretraining data that comprises thousands of labels per molecule derived from bio-assays, quantum simulations, transcriptomics and phenomic imaging. We further demonstrate strong finetuning scaling behavior on 38 highly competitive downstream tasks, outclassing previous large models. This gives rise to MolGPS, a new graph foundation model that allows to navigate the chemical space, outperforming the previous state-of-the-arts on 26 out the 38 downstream tasks. We hope that our work paves the way for an era where foundational GNNs drive pharmaceutical drug discovery",
    "checked": true,
    "id": "2af95645fdbec60ca7b46d264a29950d8ccedaaf",
    "semantic_title": "on the scalability of gnns for molecular graphs",
    "citation_count": 21,
    "authors": [
      "Maciej Sypetkowski",
      "Frederik Wenkel",
      "Farimah Poursafaei",
      "Nia Dickson",
      "Karush Suri",
      "Philip Fradkin",
      "Dominique Beaini"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2360da01c2ed6592bb691326424de184-Abstract-Conference.html": {
    "title": "Improving Viewpoint-Independent Object-Centric Representations through Active Viewpoint Selection",
    "volume": "main",
    "abstract": "Given the complexities inherent in visual scenes, such as object occlusion, a comprehensive understanding often requires observation from multiple viewpoints. Existing multi-viewpoint object-centric learning methods typically employ random or sequential viewpoint selection strategies. While applicable across various scenes, these strategies may not always be ideal, as certain scenes could benefit more from specific viewpoints. To address this limitation, we propose a novel active viewpoint selection strategy. This strategy predicts images from unknown viewpoints based on information from observation images for each scene. It then compares the object-centric representations extracted from both viewpoints and selects the unknown viewpoint with the largest disparity, indicating the greatest gain in information, as the next observation viewpoint. Through experiments on various datasets, we demonstrate the effectiveness of our active viewpoint selection strategy, significantly enhancing segmentation and reconstruction performance compared to random viewpoint selection. Moreover, our method can accurately predict images from unknown viewpoints",
    "checked": true,
    "id": "f552367595bde989cd016ced86932c30a78e7e11",
    "semantic_title": "improving viewpoint-independent object-centric representations through active viewpoint selection",
    "citation_count": 0,
    "authors": [
      "Yinxuan Huang",
      "Chengmin Gao",
      "Bin Li",
      "Xiangyang Xue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2375085c9e5afdfcab8c564e42ec0019-Abstract-Conference.html": {
    "title": "Aggregating Quantitative Relative Judgments: From Social Choice to Ranking Prediction",
    "volume": "main",
    "abstract": "Quantitative Relative Judgment Aggregation (QRJA) is a new research topic in (computational) social choice. In the QRJA model, agents provide judgments on the relative quality of different candidates, and the goal is to aggregate these judgments across all agents. In this work, our main conceptual contribution is to explore the interplay between QRJA in a social choice context and its application to ranking prediction. We observe that in QRJA, judges do not have to be people with subjective opinions; for example, a race can be viewed as a ``judgment'' on the contestants' relative abilities. This allows us to aggregate results from multiple races to evaluate the contestants' true qualities. At a technical level, we introduce new aggregation rules for QRJA and study their structural and computational properties. We evaluate the proposed methods on data from various real races and show that QRJA-based methods offer effective and interpretable ranking predictions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Xu",
      "Hanrui Zhang",
      "Yu Cheng",
      "Vincent Conitzer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/23866f14c85e3dc534ad7202f94378ef-Abstract-Conference.html": {
    "title": "Vision Foundation Model Enables Generalizable Object Pose Estimation",
    "volume": "main",
    "abstract": "Object pose estimation plays a crucial role in robotic manipulation, however, its practical applicability still suffers from limited generalizability. This paper addresses the challenge of generalizable object pose estimation, particularly focusing on category-level object pose estimation for unseen object categories. Current methods either require impractical instance-level training or are confined to predefined categories, limiting their applicability. We propose VFM-6D, a novel framework that explores harnessing existing vision and language models, to elaborate object pose estimation into two stages: category-level object viewpoint estimation and object coordinate map estimation. Based on the two-stage framework, we introduce a 2D-to-3D feature lifting module and a shape-matching module, both of which leverage pre-trained vision foundation models to improve object representation and matching accuracy. VFM-6D is trained on cost-effective synthetic data and exhibits superior generalization capabilities. It can be applied to both instance-level unseen object pose estimation and category-level object pose estimation for novel categories. Evaluations on benchmark datasets demonstrate the effectiveness and versatility of VFM-6D in various real-world scenarios",
    "checked": true,
    "id": "5238d9c47cfa6424f7be24e2a51619bca4fc7c3d",
    "semantic_title": "vision foundation model enables generalizable object pose estimation",
    "citation_count": 0,
    "authors": [
      "Kai Chen",
      "Yiyao Ma",
      "Xingyu Lin",
      "Stephen James",
      "Jianshu Zhou",
      "Yun-Hui Liu",
      "Pieter Abbeel",
      "DOU QI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/238e6167319ad5da33c7b38594a1edb1-Abstract-Conference.html": {
    "title": "Curvature Clues: Decoding Deep Learning Privacy with Input Loss Curvature",
    "volume": "main",
    "abstract": "In this paper, we explore the properties of loss curvature with respect to input data in deep neural networks. Curvature of loss with respect to input (termed input loss curvature) is the trace of the Hessian of the loss with respect to the input. We investigate how input loss curvature varies between train and test sets, and its implications for train-test distinguishability. We develop a theoretical framework that derives an upper bound on the train-test distinguishability based on privacy and the size of the training set. This novel insight fuels the development of a new black box membership inference attack utilizing input loss curvature. We validate our theoretical findings through experiments in computer vision classification tasks, demonstrating that input loss curvature surpasses existing methods in membership inference effectiveness. Our analysis highlights how the performance of membership inference attack (MIA) methods varies with the size of the training set, showing that curvature-based MIA outperforms other methods on sufficiently large datasets. This condition is often met by real datasets, as demonstrated by our results on CIFAR10, CIFAR100, and ImageNet. These findings not only advance our understanding of deep neural network behavior but also improve the ability to test privacy-preserving techniques in machine learning",
    "checked": true,
    "id": "8b4f72836c295c3d8603290701b5d712f8736127",
    "semantic_title": "curvature clues: decoding deep learning privacy with input loss curvature",
    "citation_count": 0,
    "authors": [
      "Deepak Ravikumar",
      "Efstathia Soufleri",
      "Kaushik Roy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/23c32cb7ac397f612b7c16aaa2bf0340-Abstract-Conference.html": {
    "title": "Exploring Fixed Point in Image Editing: Theoretical Support and Convergence Optimization",
    "volume": "main",
    "abstract": "In image editing, Denoising Diffusion Implicit Models (DDIM) inversion has become a widely adopted method and is extensively used in various image editing approaches. The core concept of DDIM inversion stems from the deterministic sampling technique of DDIM, which allows the DDIM process to be viewed as an Ordinary Differential Equation (ODE) process that is reversible. This enables the prediction of corresponding noise from a reference image, ensuring that the restored image from this noise remains consistent with the reference image. Image editing exploits this property by modifying the cross-attention between text and images to edit specific objects while preserving the remaining regions. However, in the DDIM inversion, using the $t-1$ time step to approximate the noise prediction at time step $t$ introduces errors between the restored image and the reference image. Recent approaches have modeled each step of the DDIM inversion process as finding a fixed-point problem of an implicit function. This approach significantly mitigates the error in the restored image but lacks theoretical support regarding the existence of such fixed points. Therefore, this paper focuses on the study of fixed points in DDIM inversion and provides theoretical support. Based on the obtained theoretical insights, we further optimize the loss function for the convergence of fixed points in the original DDIM inversion, improving the visual quality of the edited image. Finally, we extend the fixed-point based image editing to the application of unsupervised image dehazing, introducing a novel text-based approach for unsupervised dehazing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Hang",
      "Zhe Ma",
      "Haoming Chen",
      "Xuwei Fang",
      "Vincent Xie",
      "Faming Fang",
      "Guixu Zhang",
      "Hongbin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/23c9c94227f937cfb50592a15e7fbb63-Abstract-Conference.html": {
    "title": "MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models",
    "volume": "main",
    "abstract": "Gesture synthesis is a vital realm of human-computer interaction, with wide-ranging applications across various fields like film, robotics, and virtual reality. Recent advancements have utilized the diffusion model to improve gesture synthesis. However, the high computational complexity of these techniques limits the application in reality. In this study, we explore the potential of state space models (SSMs).Direct application of SSMs in gesture synthesis encounters difficulties, which stem primarily from the diverse movement dynamics of various body parts. The generated gestures may also exhibit unnatural jittering issues.To address these, we implement a two-stage modeling strategy with discrete motion priors to enhance the quality of gestures.Built upon the selective scan mechanism, we introduce MambaTalk, which integrates hybrid fusion modules, local and global scans to refine latent space representations.Subjective and objective experiments demonstrate that our method surpasses the performance of state-of-the-art models. Our project is publicly available at~\\url{https://kkakkkka.github.io/MambaTalk/}",
    "checked": true,
    "id": "97388c71be60282e149c2c3d00db7c0eb2c946e4",
    "semantic_title": "mambatalk: efficient holistic gesture synthesis with selective state space models",
    "citation_count": 29,
    "authors": [
      "Zunnan Xu",
      "Yukang Lin",
      "Haonan Han",
      "Sicheng Yang",
      "Ronghui Li",
      "Yachao Zhang",
      "Xiu Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/23d64d26abb5a0e9f2014cfcc700f82a-Abstract-Conference.html": {
    "title": "Efficient Streaming Algorithms for Graphlet Sampling",
    "volume": "main",
    "abstract": "Given a graph $G$ and a positive integer $k$, the Graphlet Sampling problem asks to sample a connected induced $k$-vertex subgraph of $G$ uniformly at random.Graphlet sampling enhances machine learning applications by transforming graph structures into feature vectors for tasks such as graph classification and subgraph identification, boosting neural network performance, and supporting clustered federated learning by capturing local structures and relationships.A recent work has shown that the problem admits an algorithm that preprocesses $G$ in time $O(nk^2 \\log k + m)$, and draws one sample in expected time $k^{O(k)} \\log n$, where $n=|V(G)|$ and $m=|E(G)|$. Such an algorithm relies on the assumption that the input graph fits into main memory and it does not seem to be straightforward to adapt it to very large graphs. We consider Graphlet Sampling in the semi-streaming setting, where we have a memory of $M = \\Omega(n \\log n)$ words, and $G$ can be only read through sequential passes over the edge list. We develop a semi-streaming algorithm that preprocesses $G$ in $p={O}(\\log n)$ passes and samples $\\Theta(M k^{-O(k)})$ independent uniform $k$-graphlets in $O(k)$ passes. For constant $k$, both phases run in time $O((n+m)\\log n)$. We also show that the tradeoff between memory and number of passes of our algorithms is near-optimal. Our extensive evaluation on very large graphs shows the effectiveness of our algorithms",
    "checked": true,
    "id": "90229cad6659d90604eb99cf48994c2a96c75ccf",
    "semantic_title": "efficient streaming algorithms for graphlet sampling",
    "citation_count": 0,
    "authors": [
      "Yann Bourreau",
      "Marco Bressan",
      "T-H. Hubert Chan",
      "Qipeng Kuang",
      "Mauro Sozio"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/23fb7cd2350c3125db48a551ae28f4bf-Abstract-Conference.html": {
    "title": "Semi-Supervised Sparse Gaussian Classification: Provable Benefits of Unlabeled Data",
    "volume": "main",
    "abstract": "The premise of semi-supervised learning (SSL) is that combining labeled and unlabeled data yields significantly more accurate models.Despite empirical successes, the theoretical understanding of SSL is still far from complete. In this work, we study SSL for high dimensional sparse Gaussian classification. To construct an accurate classifier a key task is feature selection, detecting the few variables that separate the two classes.For this SSL setting, we analyze information theoretic lower bounds for accurate feature selection as well as computational lower bounds, assuming the low-degree likelihood hardness conjecture. Our key contribution is the identification of a regime in the problem parameters (dimension, sparsity, number of labeled and unlabeled samples) where SSL is guaranteed to be advantageous for classification.Specifically, there is a regime where it is possible to construct in polynomial time an accurate SSL classifier.However, any computationally efficient supervised or unsupervised learning schemes, that separately use only the labeled or unlabeled data would fail. Our work highlights the provable benefits of combining labeled and unlabeled data for classification and feature selection in high dimensions. We present simulations that complement our theoretical analysis",
    "checked": true,
    "id": "f7b1d57c4448b6d2bcd3f530b25fee437dc80d03",
    "semantic_title": "semi-supervised sparse gaussian classification: provable benefits of unlabeled data",
    "citation_count": 1,
    "authors": [
      "Eyar Azar",
      "Boaz Nadler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/240225294cdd2c9b692c2519d3278a08-Abstract-Conference.html": {
    "title": "DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation",
    "volume": "main",
    "abstract": "Large language models (LLMs) have achieved significant success across various domains. However, training these LLMs typically involves substantial memory and computational costs during both forward and backward propagation. While parameter-efficient fine-tuning (PEFT) considerably reduces the training memory associated with parameters, it does not address the significant computational costs and activation memory. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs and activation memory while maintaining accuracy. DropBP randomly drops layers during backward propagation, which is essentially equivalent to training shallow submodules generated by undropped layers and residual connections. Additionally, DropBP calculates the sensitivity of each layer to assign an appropriate drop rate, thereby stabilizing the training process. DropBP is not only applicable to full fine-tuning but can also be orthogonally integrated with all types of PEFT by dropping layers during backward propagation. Specifically, DropBP can reduce training time by 44% with comparable accuracy to the baseline, accelerate convergence to the same perplexity by 1.5$\\times$, and enable training with a sequence length 6.2$\\times$ larger on a single NVIDIA-A100 GPU. Furthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100 GPU and 117% on an Intel Gaudi2 HPU. The code is available at [https://github.com/WooSunghyeon/dropbp](https://github.com/WooSunghyeon/dropbp)",
    "checked": true,
    "id": "ffe34206504bfc55524312504b9ab7c538b83a5b",
    "semantic_title": "dropbp: accelerating fine-tuning of large language models by dropping backward propagation",
    "citation_count": 3,
    "authors": [
      "Sunghyeon Woo",
      "Baeseong Park",
      "Byeongwook Kim",
      "Minjung Jo",
      "Se Jung Kwon",
      "Dongsuk Jeon",
      "Dongsoo Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2406694fd7bc7e7bf257446a14f9ea63-Abstract-Conference.html": {
    "title": "Grounding Multimodal Large Language Models in Actions",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated a wide range of capabilities across many domains including Embodied AI. In this work, we study how to best ground a MLLM into different embodiments and their associated action spaces, including both continuous and discrete actions. For continuous actions, a set of learned tokenizations that capture an action at various resolutions allows for sufficient modeling precision, yielding the best performance on downstream tasks. For discrete actions, semantically aligning these actions with the native output token space of the MLLM leads to the strongest performance. We arrive at these lessons via a thorough study of seven action grounding approaches on five different environments, encompassing over 114 embodied tasks",
    "checked": true,
    "id": "6480a454486119fdec3f8eec9e7c1cf13dde71db",
    "semantic_title": "grounding multimodal large language models in actions",
    "citation_count": 16,
    "authors": [
      "Andrew Szot",
      "Bogdan Mazoure",
      "Harsh Agrawal",
      "R Devon Hjelm",
      "Zsolt Kira",
      "Alexander Toshev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/240d297094fc76d1e7aa27b01f221b00-Abstract-Conference.html": {
    "title": "Optimal Algorithms for Learning Partitions with Faulty Oracles",
    "volume": "main",
    "abstract": "We consider a clustering problem where a learner seeks to partition a finite set by querying a faulty oracle. This models applications where learners crowdsource information from non-expert human workers or conduct noisy experiments to determine group structure. The learner aims to exactly recover a partition by submitting queries of the form ``are $u$ and $v$ in the same group?'' for any pair of elements $u$ and $v$ in the set. Moreover, because the learner only has access to faulty sources of information, they require an error-tolerant algorithm for this task: i.e. they must fully recover the correct partition, even if up to $\\ell$ answers are incorrect, for some error-tolerance parameter $\\ell$. We study the question: for any given error-tolerance $\\ell$, what is the minimum number of queries needed to learn a finite set partition of $n$ elements into $k$ groups? We design algorithms for this task and prove that they achieve optimal query complexity. To analyze our algorithms, we first highlight a connection between this task and correlation clustering. We then use this connection to build a Rényi-Ulam style analytical framework for this problem, which yields matching lower bounds. Our analysis also reveals an inherent asymmetry between the query complexity necessary to be robust against false negative errors as opposed to false positive errors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adela DePavia",
      "Olga Medrano Martin del Campo",
      "Erasmo Tani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/24143e25a82f856aeed58b2f497d623b-Abstract-Conference.html": {
    "title": "MemoryFormer : Minimize Transformer Computation by Removing Fully-Connected Layers",
    "volume": "main",
    "abstract": "In order to reduce the computational complexity of large language models, great efforts have been made to to improve the efficiency of transformer models such as linear attention and flash-attention. However, the model size and corresponding computational complexity are constantly scaled up in pursuit of higher performance. In this work, we present MemoryFormer, a novel transformer architecture which significantly reduces the computational complexity (FLOPs) from a new perspective. We eliminate nearly all the computations of the transformer model except for the necessary computation required by the multi-head attention operation. This is made possible by utilizing an alternative method for feature transformation to replace the linear projection of fully-connected layers. Specifically, we first construct a group of in-memory lookup tables that store a large amount of discrete vectors to replace the weight matrix used in linear projection. We then use a hash algorithm to retrieve a correlated subset of vectors dynamically based on the input embedding. The retrieved vectors combined together will form the output embedding, which provides an estimation of the result of matrix multiplication operation in a fully-connected layer. Compared to conducting matrix multiplication, retrieving data blocks from memory is a much cheaper operation which requires little computations. We train MemoryFormer from scratch and conduct extensive experiments on various benchmarks to demonstrate the effectiveness of the proposed model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Ding",
      "Yehui Tang",
      "Haochen Qin",
      "Zhenli Zhou",
      "Chao Xu",
      "Lin Li",
      "Kai Han",
      "Liao Heng",
      "Yunhe Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/24258872c8566deedf9b56845e5ae074-Abstract-Conference.html": {
    "title": "LCGen: Mining in Low-Certainty Generation for View-consistent Text-to-3D",
    "volume": "main",
    "abstract": "The Janus Problem is a common issue in SDS-based text-to-3D methods. Due to view encoding approach and 2D diffusion prior guidance, the 3D representation model tends to learn content with higher certainty from each perspective, leading to view inconsistency. In this work, we first model and analyze the problem, visualizing the specific causes of the Janus Problem, which are associated with discrete view encoding and shared priors in 2D lifting. Based on this, we further propose the LCGen method, which guides text-to-3D to obtain different priors with different certainty from various viewpoints, aiding in view-consistent generation. Experiments have proven that our LCGen method can be directly applied to different SDS-based text-to-3D methods, alleviating the Janus Problem without introducing additional information, increasing excessive training burden, or compromising the generation effect",
    "checked": true,
    "id": "ab5922cd9c8a1e472edc8d90027e4474686c42ec",
    "semantic_title": "lcgen: mining in low-certainty generation for view-consistent text-to-3d",
    "citation_count": 1,
    "authors": [
      "Zeng Tao",
      "Tong Yang",
      "Junxiong Lin",
      "Xinji Mai",
      "Haoran Wang",
      "Beining Wang",
      "Enyu Zhou",
      "Yan Wang",
      "Wenqiang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2428ff361a08bc6864fb240bc83fba42-Abstract-Conference.html": {
    "title": "Learning and Transferring Sparse Contextual Bigrams with Linear Transformers",
    "volume": "main",
    "abstract": "Transformers have achieved significant success in natural language modeling because of their exceptional capabilities to combine contextual information and global knowledge, yet their theoretical basis remains unclear. In this paper, we first propose Sparse Contextual Bigram (SCB), a natural extension to the classical bigram model, where the generation of the next token depends on a sparse set of earlier positions determined by the last token. We investigate the training dynamics and sample complexity of learning SCB using a one-layer linear transformer with a gradient-based algorithm. We show that when trained from scratch, the training process can be split into an initial sample-intensive stage where the correlation is boosted from zero to a nontrivial value, followed by a more sample-efficient stage of further improvement. Additionally, we prove that, provided a nontrivial correlation between the downstream and pretraining tasks, finetuning from a pretrained model allows us to bypass the initial sample-intensive stage. We also empirically demonstrate that our algorithm can outperform SGD in our setting",
    "checked": true,
    "id": "a5c5b56ca3ffc237473406116a5ed70f31a7c7c9",
    "semantic_title": "learning and transferring sparse contextual bigrams with linear transformers",
    "citation_count": 2,
    "authors": [
      "Yunwei Ren",
      "Zixuan Wang",
      "Jason Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2432eb0ddcf3bb630b5bcf96ca7e592d-Abstract-Conference.html": {
    "title": "Predictor-Corrector Enhanced Transformers with Exponential Moving Average Coefficient Learning",
    "volume": "main",
    "abstract": "Residual networks, as discrete approximations of Ordinary Differential Equations (ODEs), have inspired significant advancements in neural network design, including multistep methods, high-order methods, and multi-particle dynamical systems. The precision of the solution to ODEs significantly affects parameter optimization, thereby impacting model performance. In this work, we present a series of advanced explorations of Transformer architecture design to minimize the error compared to the true ``solution.'' First, we introduce a predictor-corrector learning framework to minimize truncation errors, which consists of a high-order predictor and a multistep corrector. Second, we propose an exponential moving average-based coefficient learning method to strengthen our higher-order predictor. Extensive experiments on large-scale machine translation, abstractive summarization, language modeling, and natural language understanding benchmarks demonstrate the superiority of our approach. On the WMT'14 English-German and English-French tasks, our model achieved BLEU scores of 30.95 and 44.27, respectively. Furthermore, on the OPUS multilingual machine translation task, our model surpasses a robust 3.8B DeepNet by an average of 2.9 SacreBLEU, using only 1/3 parameters. Notably, it also beats LLama models by 5.7 accuracy points on the LM Harness Evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bei Li",
      "Tong Zheng",
      "Rui Wang",
      "Jiahao Liu",
      "清妍 郭",
      "Junliang Guo",
      "Xu Tan",
      "Tong Xiao",
      "JingBo Zhu",
      "Jingang Wang",
      "Xunliang Cai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/243697ace81f57daef8737ff2c5cffd3-Abstract-Conference.html": {
    "title": "Public-data Assisted Private Stochastic Optimization: Power and Limitations",
    "volume": "main",
    "abstract": "We study the limits and capability of public-data assisted differentially private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data. For complete/labeled public data, we show that any $(\\epsilon,\\delta)$-PA-DP has excess risk $\\tilde{\\Omega}\\big(\\min(\\frac{1}{\\sqrt{n_{\\text{pub}}}},\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon} ) \\big)$, where $d$ is the dimension, ${n_{\\text{pub}}}$ is the number of public samples, ${n_{\\text{priv}}}$ is the number of private samples, and $n={n_{\\text{pub}}}+{n_{\\text{priv}}}$. These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form. Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal. We also study PA-DP supervised learning with \\textit{unlabeled} public samples. In contrast to our previous result, we here show novel methods for leveraging public data in private supervised learning. For generalized linear models (GLM) with unlabeled public data, we show an efficient algorithm which, given $\\tilde{O}({n_{\\text{priv}}}\\epsilon)$ unlabeled public samples, achieves the dimension independent rate $\\tilde{O}\\big(\\frac{1}{\\sqrt{{n_{\\text{priv}}}}} + \\frac{1}{\\sqrt{{n_{\\text{priv}}}\\epsilon}}\\big)$. We develop new lower bounds for this setting which shows that this rate cannot be improved with more public samples, and any fewer public samples leads to a worse rate. Finally, we provide extensions of this result to general hypothesis classes with finite \\textit{fat-shattering dimension} with applications to neural networks and non-Euclidean geometries",
    "checked": true,
    "id": "cc30188e592712a85aab352502e90d2c69e71ed4",
    "semantic_title": "public-data assisted private stochastic optimization: power and limitations",
    "citation_count": 3,
    "authors": [
      "Enayat Ullah",
      "Michael Menart",
      "Raef Bassily",
      "Cristóbal Guzmán",
      "Raman Arora"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/244da015b91e64f2d9362703fa2a902b-Abstract-Conference.html": {
    "title": "Globally Q-linear Gauss-Newton Method for Overparameterized Non-convex Matrix Sensing",
    "volume": "main",
    "abstract": "This paper focuses on the optimization of overparameterized, non-convex low-rank matrix sensing (LRMS)—an essential component in contemporary statistics and machine learning. Recent years have witnessed significant breakthroughs in first-order methods, such as gradient descent, for tackling this non-convex optimization problem. However, the presence of numerous saddle points often prolongs the time required for gradient descent to overcome these obstacles. Moreover, overparameterization can markedly decelerate gradient descent methods, transitioning its convergence rate from linear to sub-linear. In this paper, we introduce an approximated Gauss-Newton (AGN) method for tackling the non-convex LRMS problem. Notably, AGN incurs a computational cost comparable to gradient descent per iteration but converges much faster without being slowed down by saddle points. We prove that, despite the non-convexity of the objective function, AGN achieves Q-linear convergence from random initialization to the global optimal solution. The global Q-linear convergence of AGN represents a substantial enhancement over the convergence of the existing methods for the overparameterized non-convex LRMS. The code for this paper is available at \\url{https://github.com/hsijiaxidian/AGN}",
    "checked": true,
    "id": "7c4aee326afbbdc03e6344eb1e8fe3c0ec6c884b",
    "semantic_title": "globally q-linear gauss-newton method for overparameterized non-convex matrix sensing",
    "citation_count": 1,
    "authors": [
      "Xixi Jia",
      "Fangchen FENG",
      "Deyu Meng",
      "Defeng Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/24662461d2194d1bc70a47b6b6771026-Abstract-Conference.html": {
    "title": "Implicit Curriculum in Procgen Made Explicit",
    "volume": "main",
    "abstract": "Procedurally generated environments such as Procgen Benchmark provide a testbed for evaluating the agent's ability to robustly learn a relevant skill, by situating the agent in ever-changing levels. The diverse levels associated with varying contexts are naturally connected to curriculum learning. Existing works mainly focus on arranging the levels to explicitly form a curriculum. In this work, we take a close look at the learning process itself under the multi-level training in Procgen. Interestingly, the learning process exhibits a gradual shift from easy contexts to hard contexts, suggesting an implicit curriculum in multi-level training. Our analysis is made possible through C-Procgen, a benchmark we build upon Procgen that enables explicit control of the contexts. We believe our findings will foster a deeper understanding of learning in diverse contexts, and our benchmark will benefit future research in curriculum reinforcement learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxiong Tan",
      "Kaixin Wang",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2492288f6878e6f99124b362604e58f5-Abstract-Conference.html": {
    "title": "Easy Regional Contrastive Learning of Expressive Fashion Representations",
    "volume": "main",
    "abstract": "When learning vision-language models (VLM) for the fashion domain, most existing works design new architectures from vanilla BERT with additional objectives, or perform dense multi-task learning with fashion-specific tasks. Though progress has been made, their architecture or objectives are often intricate and the extendibility is limited.By contrast, with simple architecture (comprising only two unimodal encoders) and just the contrastive objective, popular pre-trained VL models (e.g., CLIP) achieve superior performance in general domains, which are further easily extended to downstream tasks.However, inheriting such benefits of CLIP in the fashion domain is non-trivial in the presence of the notable domain gap. Empirically, we find that directly finetuning on fashion data leads CLIP to frequently ignore minor yet important details such as logos and composition, which are critical in fashion tasks such as retrieval and captioning.In this work, to maintain CLIP's simple architecture and objective while explicitly attending to fashion details, we propose $E^2$: Easy Regional Contrastive Learning of Expressive Fashion Representations.$E^2$ introduces only a few selection tokens and fusion blocks (just 1.9\\% additional parameters in total) with only contrastive losses. Despite lightweight, in our primary focus, cross-modal retrieval, $E^2$ notably outperforms existing fashion VLMs with various fashion-specific objectives.Moreover, thanks to CLIP's widespread use in downstream tasks in general domains (e.g., zero-shot composed image retrieval and image captioning), our model can easily extend these models from general domain to the fashion domain with notable improvement.To conduct a comprehensive evaluation, we further collect data from Amazon Reviews to build a new dataset for cross-modal retrieval in the fashion domain",
    "checked": true,
    "id": "9f9b87d8bfab2bf7bff22de4561294bbe19e6c78",
    "semantic_title": "easy regional contrastive learning of expressive fashion representations",
    "citation_count": 1,
    "authors": [
      "Daiqing Qi",
      "Handong Zhao",
      "Sheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/249b8d8f41970822651435629e68a6e1-Abstract-Conference.html": {
    "title": "Gradient Methods for Online DR-Submodular Maximization with Stochastic Long-Term Constraints",
    "volume": "main",
    "abstract": "In this paper, we consider the problem of online monotone DR-submodular maximization subject to long-term stochastic constraints. Specifically, at each round $t\\in [T]$, after committing an action $\\mathbf{x}_t$, a random reward $f_t(\\mathbf{x}_t)$ and an unbiased gradient estimate of the point $\\widetilde{\\nabla}f_t(\\mathbf{x}_t)$ (semi-bandit feedback) are revealed. Meanwhile, a budget of $g_t(\\mathbf{x}_t)$, which is linear and stochastic, is consumed of its total allotted budget $B_T$. We propose a gradient ascent based algorithm that achieves $\\frac{1}{2}$-regret of $\\mathcal{O}(\\sqrt{T})$ with $\\mathcal{O}(T^{3/4})$ constraint violation with high probability. Moreover, when first-order full-information feedback is available, we propose an algorithm that achieves $(1-1/e)$-regret of $\\mathcal{O}(\\sqrt{T})$ with $\\mathcal{O}(T^{3/4})$ constraint violation. These algorithms significantly improve over the state-of-the-art in terms of query complexity",
    "checked": true,
    "id": "b581934c951736dbaa1275a7fd951abf6c8bd388",
    "semantic_title": "gradient methods for online dr-submodular maximization with stochastic long-term constraints",
    "citation_count": 0,
    "authors": [
      "Guanyu Nie",
      "Vaneet Aggarwal",
      "Christopher Quinn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/24c523085d10743633f9964e0623dbe0-Abstract-Conference.html": {
    "title": "General Articulated Objects Manipulation in Real Images via Part-Aware Diffusion Process",
    "volume": "main",
    "abstract": "Articulated object manipulation in real images is a fundamental step in computer and robotic vision tasks. Recently, several image editing methods based on diffusion models have been proposed to manipulate articulated objects according to text prompts. However, these methods often generate weird artifacts or even fail in real images. To this end, we introduce the Part-Aware Diffusion Model to approach the manipulation of articulated objects in real images. First, we develop Abstract 3D Models to represent and manipulate articulated objects efficiently. Then we propose dynamic feature maps to transfer the appearance of objects from input images to edited ones, meanwhile generating the novel-appearing parts reasonably. Extensive experiments are provided to illustrate the advanced manipulation capabilities of our method concerning state-of-the-art editing works. Additionally, we verify our method on 3D articulated object understanding forembodied robot scenarios and the promising results prove that our method supports this task strongly. The project page is https://mvig-rhos.com/pa_diffusion",
    "checked": true,
    "id": "7812ddaf02f1cb5af858f6ba99dea7dcdb177c70",
    "semantic_title": "general articulated objects manipulation in real images via part-aware diffusion process",
    "citation_count": 0,
    "authors": [
      "ZHOU FANG",
      "Yong-Lu Li",
      "Lixin Yang",
      "Cewu Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/24c53bfa5b53fc2cf05644f5a7a26bb0-Abstract-Conference.html": {
    "title": "Edit Distance Robust Watermarks via Indexing Pseudorandom Codes",
    "volume": "main",
    "abstract": "Motivated by the problem of detecting AI-generated text, we consider the problem of watermarking the output of language models with provable guarantees. We aim for watermarks which satisfy: (a) undetectability, a cryptographic notion introduced by Christ, Gunn, & Zamir (2023) which stipulates that it is computationally hard to distinguish watermarked language model outputs from the model's actual output distribution; and (b) robustness to channels which introduce a constant fraction of adversarial insertions, substitutions, and deletions to the watermarked text. Earlier schemes could only handle stochastic substitutions and deletions, and thus we are aiming for a more natural and appealing robustness guarantee that holds with respect to edit distance. Our main result is a watermarking scheme which achieves both (a) and (b) when the alphabet size for the language model is allowed to grow as a polynomial in the security parameter. To derive such a scheme, we follow an approach introduced by Christ & Gunn (2024), which proceeds via first constructing pseudorandom codes satisfying undetectability and robustness properties analogous to those above; our codes have the additional benefit of relying on weaker computational assumptions than used in previous work. Then we show that there is a generic transformation from such codes over large alphabets to watermarking schemes for arbitrary language models",
    "checked": true,
    "id": "a2e5b2043a64ba8944d61d6f8c89a65986e5cde9",
    "semantic_title": "edit distance robust watermarks via indexing pseudorandom codes",
    "citation_count": 7,
    "authors": [
      "Noah Golowich",
      "Ankur Moitra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/24d2dd6dc9b79116f8ebc852ddb9dc94-Abstract-Conference.html": {
    "title": "Nesterov acceleration despite very noisy gradients",
    "volume": "main",
    "abstract": "We present a generalization of Nesterov's accelerated gradient descent algorithm. Our algorithm (AGNES) provably achieves acceleration for smooth convex and strongly convex minimization tasks with noisy gradient estimates if the noise intensity is proportional to the magnitude of the gradient at every point. Nesterov's method converges at an accelerated rate if the constant of proportionality is below 1, while AGNES accommodates any signal-to-noise ratio. The noise model is motivated by applications in overparametrized machine learning. AGNES requires only two parameters in convex and three in strongly convex minimization tasks, improving on existing methods. We further provide clear geometric interpretations and heuristics for the choice of parameters",
    "checked": true,
    "id": "129350d3bcedad71aa9b88b6fc06ad12a09e8e97",
    "semantic_title": "nesterov acceleration despite very noisy gradients",
    "citation_count": 0,
    "authors": [
      "Kanan Gupta",
      "Jonathan W. Siegel",
      "Stephan Wojtowytsch"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/24e8b46430df965674221665816a4964-Abstract-Conference.html": {
    "title": "DiffAug: A Diffuse-and-Denoise Augmentation for Training Robust Classifiers",
    "volume": "main",
    "abstract": "We introduce DiffAug, a simple and efficient diffusion-based augmentation technique to train image classifiers for the crucial yet challenging goal of improved classifier robustness. Applying DiffAug to a given example consists of one forward-diffusion step followed by one reverse-diffusion step. Using both ResNet-50 and Vision Transformer architectures, we comprehensively evaluate classifiers trained with DiffAug and demonstrate the surprising effectiveness of single-step reverse diffusion in improving robustness to covariate shifts, certified adversarial accuracy and out of distribution detection. When we combine DiffAug with other augmentations such as AugMix and DeepAugment we demonstrate further improved robustness. Finally, building on this approach, we also improve classifier-guided diffusion wherein we observe improvements in: (i) classifier-generalization, (ii) gradient quality (i.e., improved perceptual alignment) and (iii) image generation performance. We thus introduce a computationally efficient technique for training with improved robustness that does not require any additional data, and effectively complements existing augmentation approaches",
    "checked": true,
    "id": "fcaf2fa977c142496d0f8a74ed30fe4fd231c30e",
    "semantic_title": "diffaug: a diffuse-and-denoise augmentation for training robust classifiers",
    "citation_count": 2,
    "authors": [
      "Chandramouli Shama Sastry",
      "Sri Harsha Dumpala",
      "Sageev Oore"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/24ef004f733548db6b3197d9f68dcb85-Abstract-Conference.html": {
    "title": "Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective",
    "volume": "main",
    "abstract": "In long-term time series forecasting (LTSF) tasks, an increasing number of works have acknowledged that discrete time series originate from continuous dynamic systems and have attempted to model their underlying dynamics. Recognizing the chaotic nature of real-world data, our model, Attraos, incorporates chaos theory into LTSF, perceiving real-world time series as low-dimensional observations from unknown high-dimensional chaotic dynamical systems. Under the concept of attractor invariance, Attraos utilizes non-parametric Phase Space Reconstruction embedding along with a novel multi-resolution dynamic memory unit to memorize historical dynamical structures, and evolves by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets with only one-twelfth of the parameters compared to PatchTST",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxi Hu",
      "Yuehong HU",
      "Wei Chen",
      "Ming Jin",
      "Shirui Pan",
      "Qingsong Wen",
      "Yuxuan Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/24f3041067ab24157912330344dc3bbd-Abstract-Conference.html": {
    "title": "Active Learning with LLMs for Partially Observed and Cost-Aware Scenarios",
    "volume": "main",
    "abstract": "Conducting experiments and gathering data for machine learning models is a complex and expensive endeavor, particularly when confronted with limited information. Typically, extensive _experiments_ to obtain features and labels come with a significant acquisition cost, making it impractical to carry out all of them. Therefore, it becomes crucial to strategically determine what to acquire to maximize the predictive performance while minimizing costs. To perform this task, existing data acquisition methods assume the availability of an initial dataset that is both fully-observed and labeled, crucially overlooking the **partial observability** of features characteristic of many real-world scenarios. In response to this challenge, we present Partially Observable Cost-Aware Active-Learning (POCA), a new learning approach aimed at improving model generalization in data-scarce and data-costly scenarios through label and/or feature acquisition. Introducing $\\mu$POCA as an instantiation, we maximise the uncertainty reduction in the predictive model when obtaining labels and features, considering associated costs. $\\mu$POCA enhance traditional Active Learning metrics based solely on the observed features by generating the unobserved features through Generative Surrogate Models, particularly Large Language Models (LLMs). We empirically validate $\\mu$POCA across diverse tabular datasets, varying data availability, acquisition costs, and LLMs",
    "checked": true,
    "id": "796f4f8476161f691dcbef86356ef944c966a7a5",
    "semantic_title": "active learning with llms for partially observed and cost-aware scenarios",
    "citation_count": 5,
    "authors": [
      "Nicolás Astorga",
      "Tennison Liu",
      "Nabeel Seedat",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/24f7b98aef14fcd68acf3c941af1b59e-Abstract-Conference.html": {
    "title": "Local Superior Soups: A Catalyst for Model Merging in Cross-Silo Federated Learning",
    "volume": "main",
    "abstract": "Federated learning (FL) is a learning paradigm that enables collaborative training of models using decentralized data. Recently, the utilization of pre-trained weight initialization in FL has been demonstrated to effectively improve model performance. However, the evolving complexity of current pre-trained models, characterized by a substantial increase in parameters, markedly intensifies the challenges associated with communication rounds required for their adaptation to FL. To address these communication cost issues and increase the performance of pre-trained model adaptation in FL, we propose an innovative model interpolation-based local training technique called ``Local Superior Soups.''Our method enhances local training across different clients, encouraging the exploration of a connected low-loss basin within a few communication rounds through regularized model interpolation. This approach acts as a catalyst for the seamless adaptation of pre-trained models in in FL.We demonstrated its effectiveness and efficiency across diverse widely-used FL datasets",
    "checked": true,
    "id": "ae12d1d21c813a15e2e21338f12f34fd0523b479",
    "semantic_title": "local superior soups: a catalyst for model merging in cross-silo federated learning",
    "citation_count": 4,
    "authors": [
      "Minghui Chen",
      "Meirui Jiang",
      "Xin Zhang",
      "DOU QI",
      "Zehua Wang",
      "Xiaoxiao Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/24f8dd1b8f154f1ee0d7a59e368eccf3-Abstract-Conference.html": {
    "title": "Boosting the Transferability of Adversarial Attack on Vision Transformer with Adaptive Token Tuning",
    "volume": "main",
    "abstract": "Vision transformers (ViTs) perform exceptionally well in various computer vision tasks but remain vulnerable to adversarial attacks. Recent studies have shown that the transferability of adversarial examples exists for CNNs, and the same holds true for ViTs. However, existing ViT attacks aggressively regularize the largest token gradients to exact zero within each layer of the surrogate model, overlooking the interactions between layers, which limits their transferability in attacking black-box models. Therefore, in this paper, we focus on boosting the transferability of adversarial attacks on ViTs through adaptive token tuning (ATT). Specifically, we propose three optimization strategies: an adaptive gradient re-scaling strategy to reduce the overall variance of token gradients, a self-paced patch out strategy to enhance the diversity of input tokens, and a hybrid token gradient truncation strategy to weaken the effectiveness of attention mechanism. We demonstrate that scaling correction of gradient changes using gradient variance across different layers can produce highly transferable adversarial examples. In addition, introducing attentional truncation can mitigate the overfitting over complex interactions between tokens in deep ViT layers to further improve the transferability. On the other hand, using feature importance as a guidance to discard a subset of perturbation patches in each iteration, along with combining self-paced learning and progressively more sampled attacks, significantly enhances the transferability over attacks that use all perturbation patches. Extensive experiments conducted on ViTs, undefended CNNs, and defended CNNs validate the superiority of our proposed ATT attack method. On average, our approach improves the attack performance by 10.1% compared to state-of-the-art transfer-based attacks. Notably, we achieve the best attack performance with an average of 58.3% on three defended CNNs. Code is available at https://github.com/MisterRpeng/ATT",
    "checked": true,
    "id": "5c71ce2159b2e3a449dbaade1a15fda21958add7",
    "semantic_title": "boosting the transferability of adversarial attack on vision transformer with adaptive token tuning",
    "citation_count": 3,
    "authors": [
      "Di Ming",
      "Peng Ren",
      "Yunlong Wang",
      "Xin Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/250190819ff1dda47cd23cecc0c5a69b-Abstract-Conference.html": {
    "title": "Maia-2: A Unified Model for Human-AI Alignment in Chess",
    "volume": "main",
    "abstract": "There are an increasing number of domains in which artificial intelligence (AI) systems both surpass human ability and accurately model human behavior. This introduces the possibility of algorithmically-informed teaching in these domains through more relatable AI partners and deeper insights into human decision-making. Critical to achieving this goal, however, is coherently modeling human behavior at various skill levels. Chess is an ideal model system for conducting research into this kind of human-AI alignment, with its rich history as a pivotal testbed for AI research, mature superhuman AI systems like AlphaZero, and precise measurements of skill via chess rating systems. Previous work in modeling human decision-making in chess uses completely independent models to capture human style at different skill levels, meaning they lack coherence in their ability to adapt to the full spectrum of human improvement and are ultimately limited in their effectiveness as AI partners and teaching tools. In this work, we propose a unified modeling approach for human-AI alignment in chess that coherently captures human style across different skill levels and directly captures how people improve. Recognizing the complex, non-linear nature of human learning, we introduce a skill-aware attention mechanism to dynamically integrate players' strengths with encoded chess positions, enabling our model to be sensitive to evolving player skill. Our experimental results demonstrate that this unified framework significantly enhances the alignment between AI and human players across a diverse range of expertise levels, paving the way for deeper insights into human decision-making and AI-guided teaching tools",
    "checked": true,
    "id": "f3561b58d95f87aa46ee662367d7ab93911c772e",
    "semantic_title": "maia-2: a unified model for human-ai alignment in chess",
    "citation_count": 7,
    "authors": [
      "Zhenwei Tang",
      "Difan Jiao",
      "Reid McIlroy-Young",
      "Jon M. Kleinberg",
      "Siddhartha Sen",
      "Ashton Anderson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/25297252dcd3f4b11eec7ec7ab06bc80-Abstract-Conference.html": {
    "title": "Derivative-enhanced Deep Operator Network",
    "volume": "main",
    "abstract": "The deep operator networks (DeepONet), a class of neural operators that learn mappings between function spaces, have recently been developed as surrogate models for parametric partial differential equations (PDEs). In this work we propose a derivative-enhanced deep operator network (DE-DeepONet), which leverages derivative information to enhance the solution prediction accuracy and provides a more accurate approximation of solution-to-parameter derivatives, especially when training data are limited. DE-DeepONet explicitly incorporates linear dimension reduction of high dimensional parameter input into DeepONet to reduce training cost and adds derivative loss in the loss function to reduce the number of required parameter-solution pairs. We further demonstrate that the use of derivative loss can be extended to enhance other neural operators, such as the Fourier neural operator (FNO). Numerical experiments validate the effectiveness of our approach",
    "checked": true,
    "id": "7c08219ba9b104e07a4bc9bcf8d41b5ac2845705",
    "semantic_title": "derivative-enhanced deep operator network",
    "citation_count": 5,
    "authors": [
      "Yuan Qiu",
      "Nolan Bridges",
      "Peng Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/252cd9568ab41b3c10439ddb7cddf25e-Abstract-Conference.html": {
    "title": "No-Regret Bandit Exploration based on Soft Tree Ensemble Model",
    "volume": "main",
    "abstract": "We propose a novel stochastic bandit algorithm that employs reward estimates using a tree ensemble model. Specifically, our focus is on a soft tree model, a variant of the conventional decision tree that has undergone both practical and theoretical scrutiny in recent years. By deriving several non-trivial properties of soft trees, we extend the existing analytical techniques used for neural bandit algorithms to our soft tree-based algorithm. We demonstrate that our algorithm achieves a smaller cumulative regret compared to the existing ReLU-based neural bandit algorithms. We also show that this advantage comes with a trade-off: the hypothesis space of the soft tree ensemble model is more constrained than that of a ReLU-based neural network",
    "checked": true,
    "id": "f487d97a7c4641dad2601d5eb69dd7ae8c275a68",
    "semantic_title": "no-regret bandit exploration based on soft tree ensemble model",
    "citation_count": 1,
    "authors": [
      "Shogo Iwazaki",
      "Shinya Suzumura"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/254404d551f6ce17bb7407b4d6b3c87b-Abstract-Conference.html": {
    "title": "A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks",
    "volume": "main",
    "abstract": "Recent advancements in Vision-Language Models (VLMs) have enabled complex multimodal tasks by processing text and image data simultaneously, significantly enhancing the field of artificial intelligence. However, these models often exhibit biases that can skew outputs towards societal stereotypes, thus necessitating debiasing strategies. Existing debiasing methods focus narrowly on specific modalities or tasks, and require extensive retraining. To address these limitations, this paper introduces Selective Feature Imputation for Debiasing (SFID), a novel methodology that integrates feature pruning and low confidence imputation (LCI) to effectively reduce biases in VLMs. SFID is versatile, maintaining the semantic integrity of outputs and costly effective by eliminating the need for retraining. Our experimental results demonstrate SFID's effectiveness across various VLMs tasks including zero-shot classification, text-to-image retrieval, image captioning, and text-to-image generation, by significantly reducing gender biases without compromising performance. This approach not only enhances the fairness of VLMs applications but also preserves their efficiency and utility across diverse scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoin Jung",
      "Taeuk Jang",
      "Xiaoqian Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2548fbe155ed2405488b7d5373013a64-Abstract-Conference.html": {
    "title": "Bridging semantics and pragmatics in information-theoretic emergent communication",
    "volume": "main",
    "abstract": "Human languages support both semantic categorization and local pragmatic interactions that require context-sensitive reasoning about meaning. While semantics and pragmatics are two fundamental aspects of language, they are typically studied independently and their co-evolution is largely under-explored. Here, we aim to bridge this gap by studying how a shared lexicon may emerge from local pragmatic interactions. To this end, we extend a recent information-theoretic framework for emergent communication in artificial agents, which integrates utility maximization, associated with pragmatics, with general communicative constraints that are believed to shape human semantic systems. Specifically, we show how to adapt this framework to train agents via unsupervised pragmatic interactions, and then evaluate their emergent lexical semantics. We test this approach in a rich visual domain of naturalistic images, and find that key human-like properties of the lexicon emerge when agents are guided by both context-specific utility and general communicative pressures, suggesting that both aspects are crucial for understanding how language may evolve in humans and in artificial agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eleonora Gualdoni",
      "Mycal Tucker",
      "Roger P. Levy",
      "Noga Zaslavsky"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2567c95fd41459a98a73ba893775d22a-Abstract-Conference.html": {
    "title": "Watermarking Makes Language Models Radioactive",
    "volume": "main",
    "abstract": "We investigate the radioactivity of text generated by large language models (LLM), \\ie whether it is possible to detect that such synthetic input was used to train a subsequent LLM.Current methods like membership inference or active IP protection either work only in settings where the suspected text is known or do not provide reliable statistical guarantees.We discover that, on the contrary, it is possible to reliably determine if a language model was trained on synthetic data if that data is output by a watermarked LLM.Our new methods, specialized for radioactivity, detects with a provable confidence weak residuals of the watermark signal in the fine-tuned LLM.We link the radioactivity contamination level to the following properties: the watermark robustness, its proportion in the training set, and the fine-tuning process.For instance, if the suspect model is open-weight, we demonstrate that training on watermarked instructions can be detected with high confidence ($p$-value $< 10^{-5}$) even when as little as $5\\%$ of training text is watermarked",
    "checked": true,
    "id": "02c4ca0c184cbb81931f8e43ef943a181d5dad84",
    "semantic_title": "watermarking makes language models radioactive",
    "citation_count": 24,
    "authors": [
      "Tom Sander",
      "Pierre Fernandez",
      "Alain Durmus",
      "Matthijs Douze",
      "Teddy Furon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/257b9a6a0e3856735d0e624e38fb6803-Abstract-Conference.html": {
    "title": "Approximation-Aware Bayesian Optimization",
    "volume": "main",
    "abstract": "High-dimensional Bayesian optimization (BO) tasks such as molecular design often require $>10,$$000$ function evaluations before obtaining meaningful results. While methods like sparse variational Gaussian processes (SVGPs) reduce computational requirements in these settings, the underlying approximations result in suboptimal data acquisitions that slow the progress of optimization. In this paper we modify SVGPs to better align with the goals of BO: targeting informed data acquisition over global posterior fidelity. Using the framework of utility-calibrated variational inference (Lacoste–Julien et al., 2011), we unify GP approximation and data acquisition into a joint optimization problem, thereby ensuring optimal decisions under a limited computational budget. Our approach can be used with any decision-theoretic acquisition function and is readily compatible with trust region methods like TuRBO (Eriksson et al., 2019). We derive efficient joint objectives for the expected improvement (EI) and knowledge gradient (KG) acquisition functions in both the standard and batch BO settings. On a variety of recent high dimensional benchmark tasks in control and molecular design, our approach significantly outperforms standard SVGPs and is capable of achieving comparable rewards with up to $10\\times$ fewer function evaluations",
    "checked": true,
    "id": "8652bf063d46d896f9b6c04702378192f37d4e1a",
    "semantic_title": "approximation-aware bayesian optimization",
    "citation_count": 4,
    "authors": [
      "Natalie Maus",
      "Kyurae Kim",
      "David Eriksson",
      "Geoff Pleiss",
      "John P. Cunningham",
      "Jacob Gardner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/257c140d25d1f7fc10f8ae5e17296299-Abstract-Conference.html": {
    "title": "Differential Privacy in Scalable General Kernel Learning via $K$-means Nystr{\\\"o}m Random Features",
    "volume": "main",
    "abstract": "As the volume of data invested in statistical learning increases and concerns regarding privacy grow, the privacy leakage issue has drawn significant attention. Differential privacy has emerged as a widely accepted concept capable of mitigating privacy concerns, and numerous differentially private (DP) versions of machine learning algorithms have been developed. However, existing works on DP kernel learning algorithms have exhibited practical limitations, including scalability, restricted choice of kernels, or dependence on test data availability. We propose DP scalable kernel empirical risk minimization (ERM) algorithms and a DP kernel mean embedding (KME) release algorithm suitable for general kernels. Our approaches address the shortcomings of previous algorithms by employing Nyström methods, classical techniques in non-private scalable kernel learning. These methods provide data-dependent low-rank approximations of the kernel matrix for general kernels in a DP manner. We present excess empirical risk bounds and computational complexities for the scalable kernel DP ERM, KME algorithms, contrasting them with established methodologies. Furthermore, we develop a private data-generating algorithm capable of learning diverse kernel models. We conduct experiments to demonstrate the performance of our algorithms, comparing them with existing methods to highlight their superiority",
    "checked": true,
    "id": "db94c6a910cf27078b38b21dd69da2f39674b3fb",
    "semantic_title": "differential privacy in scalable general kernel learning via $k$-means nystr{\\\"o}m random features",
    "citation_count": 0,
    "authors": [
      "Bonwoo Lee",
      "Jeongyoun Ahn",
      "Cheolwoo Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/25869dbf7682272357bc2cbbf860e1c8-Abstract-Conference.html": {
    "title": "Taming Generative Diffusion Prior for Universal Blind Image Restoration",
    "volume": "main",
    "abstract": "Diffusion models have been widely utilized for image restoration. However, previous blind image restoration methods still need to assume the type of degradation model while leaving the parameters to be optimized, limiting their real-world applications. Therefore, we aim to tame generative diffusion prior for universal blind image restoration dubbed BIR-D, which utilizes an optimizable convolutional kernel to simulate the degradation model and dynamically update the parameters of the kernel in the diffusion steps, enabling it to achieve blind image restoration results even in various complex situations. Besides, based on mathematical reasoning, we have provided an empirical formula for the chosen of adaptive guidance scale, eliminating the need for a grid search for the optimal parameter. Experimentally, Our BIR-D has demonstrated superior practicality and versatility than off-the-shelf unsupervised methods across various tasks both on real-world and synthetic datasets, qualitatively and quantitatively. BIR-D is able to fulfill multi-guidance blind image restoration. Moreover, BIR-D can also restore images that undergo multiple and complicated degradations, demonstrating the practical applications. The code is available at https://github.com/Tusiwei/BIR-D",
    "checked": true,
    "id": "90f58704078d07fa1ce3a647c4d77dda81867fa8",
    "semantic_title": "taming generative diffusion prior for universal blind image restoration",
    "citation_count": 3,
    "authors": [
      "Siwei Tu",
      "Weidong Yang",
      "Ben Fei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/259762417183b58aa5bb842c1e502076-Abstract-Conference.html": {
    "title": "Discrete Dictionary-based Decomposition Layer for Structured Representation Learning",
    "volume": "main",
    "abstract": "Neuro-symbolic neural networks have been extensively studied to integrate symbolic operations with neural networks, thereby improving systematic generalization. Specifically, Tensor Product Representation (TPR) framework enables neural networks to perform differentiable symbolic operations by encoding the symbolic structure of data within vector spaces. However, TPR-based neural networks often struggle to decompose unseen data into structured TPR representations, undermining their symbolic operations. To address this decomposition problem, we propose a Discrete Dictionary-based Decomposition (D3) layer designed to enhance the decomposition capabilities of TPR-based models. D3 employs discrete, learnable key-value dictionaries trained to capture symbolic features essential for decomposition operations. It leverages the prior knowledge acquired during training to generate structured TPR representations by mapping input data to pre-learned symbolic features within these dictionaries. D3 is a straightforward drop-in layer that can be seamlessly integrated into any TPR-based model without modifications. Our experimental results demonstrate that D3 significantly improves the systematic generalization of various TPR-based models while requiring fewer additional parameters. Notably, D3 outperforms baseline models on the synthetic task that demands the systematic decomposition of unseen combinatorial data",
    "checked": true,
    "id": "b250c272d4ed9df8c19a6cc8f060eb00ab0db567",
    "semantic_title": "discrete dictionary-based decomposition layer for structured representation learning",
    "citation_count": 0,
    "authors": [
      "Taewon Park",
      "Hyun-Chul Kim",
      "Minho Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/25c0fe7b157821dd3140727dc07461da-Abstract-Conference.html": {
    "title": "WildGaussians: 3D Gaussian Splatting In the Wild",
    "volume": "main",
    "abstract": "While the field of 3D scene reconstruction is dominated by NeRFs due to their photorealistic quality, 3D Gaussian Splatting (3DGS) has recently emerged, offering similar quality with real-time rendering speeds. However, both methods primarily excel with well-controlled 3D scenes, while in-the-wild data - characterized by occlusions, dynamic objects, and varying illumination - remains challenging. NeRFs can adapt to such conditions easily through per-image embedding vectors, but 3DGS struggles due to its explicit representation and lack of shared parameters. To address this, we introduce WildGaussians, a novel approach to handle occlusions and appearance changes with 3DGS. By leveraging robust DINO features and integrating an appearance modeling module within 3DGS, our method achieves state-of-the-art results. We demonstrate that WildGaussians matches the real-time rendering speed of 3DGS while surpassing both 3DGS and NeRF baselines in handling in-the-wild data, all within a simple architectural framework",
    "checked": true,
    "id": "ddfe3283005ce2776fa3321238d6dc1a2246c81e",
    "semantic_title": "wildgaussians: 3d gaussian splatting in the wild",
    "citation_count": 71,
    "authors": [
      "Jonas Kulhanek",
      "Songyou Peng",
      "Zuzana Kukelova",
      "Marc Pollefeys",
      "Torsten Sattler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/25cc3adf8c85f7c70989cb8a97a691a7-Abstract-Conference.html": {
    "title": "What If the Input is Expanded in OOD Detection?",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) detection aims to identify OOD inputs from unknown classes, which is important for the reliable deployment of machine learning models in the open world. Various scoring functions are proposed to distinguish it from in-distribution (ID) data. However, existing methods generally focus on excavating the discriminative information from a single input, which implicitly limits its representation dimension. In this work, we introduce a novel perspective, i.e., employing different common corruptions on the input space, to expand that. We reveal an interesting phenomenon termed confidence mutation, where the confidence of OOD data can decrease significantly under the corruptions, while the ID data shows a higher confidence expectation considering the resistance of semantic features. Based on that, we formalize a new scoring method, namely, Confidence aVerage (CoVer), which can capture the dynamic differences by simply averaging the scores obtained from different corrupted inputs and the original ones, making the OOD and ID distributions more separable in detection tasks. Extensive experiments and analyses have been conducted to understand and verify the effectiveness of CoVer",
    "checked": true,
    "id": "a9cd82ca926b09e30e3487dd87129f45e7310f3f",
    "semantic_title": "what if the input is expanded in ood detection?",
    "citation_count": 4,
    "authors": [
      "Boxuan Zhang",
      "Jianing Zhu",
      "Zengmao Wang",
      "Tongliang Liu",
      "Bo Du",
      "Bo Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/25ead0efeed514aec00109301d93bbbb-Abstract-Conference.html": {
    "title": "PageRank Bandits for Link Prediction",
    "volume": "main",
    "abstract": "Link prediction is a critical problem in graph learning with broad applications such as recommender systems and knowledge graph completion. Numerous research efforts have been directed at solving this problem, including approaches based on similarity metrics and Graph Neural Networks (GNN). However, most existing solutions are still rooted in conventional supervised learning, which makes it challenging to adapt over time to changing customer interests and to address the inherent dilemma of exploitation versus exploration in link prediction.To tackle these challenges, this paper reformulates link prediction as a sequential decision-making process, where each link prediction interaction occurs sequentially. We propose a novel fusion algorithm, PRB (PageRank Bandits), which is the first to combine contextual bandits with PageRank for collaborative exploitation and exploration. We also introduce a new reward formulation and provide a theoretical performance guarantee for PRB. Finally, we extensively evaluate PRB in both online and offline settings, comparing it with bandit-based and graph-based methods. The empirical success of PRB demonstrates the value of the proposed fusion approach. Our code is released at https://github.com/jiaruzouu/PRB",
    "checked": true,
    "id": "271106faf16a47b6ab8161142cbe25aaa41cbeaa",
    "semantic_title": "pagerank bandits for link prediction",
    "citation_count": 10,
    "authors": [
      "Yikun Ban",
      "Jiaru Zou",
      "Zihao Li",
      "Yunzhe Qi",
      "Dongqi Fu",
      "Jian Kang",
      "Hanghang Tong",
      "Jingrui He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/25f7be9694d7b32d5cc670927b8091e1-Abstract-Conference.html": {
    "title": "DreamMesh4D: Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh Hybrid Representation",
    "volume": "main",
    "abstract": "Recent advancements in 2D/3D generative techniques have facilitated the generation of dynamic 3D objects from monocular videos. Previous methods mainly rely on the implicit neural radiance fields (NeRF) or explicit Gaussian Splatting as the underlying representation, and struggle to achieve satisfactory spatial-temporal consistency and surface appearance. Drawing inspiration from modern 3D animation pipelines, we introduce DreamMesh4D, a novel framework combining mesh representation with geometric skinning technique to generate high-quality 4D object from a monocular video. Instead of utilizing classical texture map for appearance, we bind Gaussian splats to triangle face of mesh for differentiable optimization of both the texture and mesh vertices. In particular, DreamMesh4D begins with a coarse mesh obtained through an image-to-3D generation procedure. Sparse points are then uniformly sampled across the mesh surface, and are used to build a deformation graph to drive the motion of the 3D object for the sake of computational efficiency and providing additional constraint. For each step, transformations of sparse control points are predicted using a deformation network, and the mesh vertices as well as the surface Gaussians are deformed via a novel geometric skinning algorithm. The skinning algorithm is a hybrid approach combining LBS (linear blending skinning) and DQS (dual-quaternion skinning), mitigating drawbacks associated with both approaches. The static surface Gaussians and mesh vertices as well as the dynamic deformation network are learned via reference view photometric loss, score distillation loss as well as other regularization losses in a two-stage manner. Extensive experiments demonstrate superior performance of our method in terms of both rendering quality and spatial-temporal consistency. Furthermore, our method is compatible with modern graphic pipelines, showcasing its potential in the 3D gaming and film industry",
    "checked": true,
    "id": "d4e8b741da00041116fb166ef628b54c79896fd4",
    "semantic_title": "dreammesh4d: video-to-4d generation with sparse-controlled gaussian-mesh hybrid representation",
    "citation_count": 18,
    "authors": [
      "Zhiqi Li",
      "Yiming Chen",
      "Peidong Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/25fb771c98a4adf83da8a050ea21cef6-Abstract-Conference.html": {
    "title": "Spatio-Temporal Interactive Learning for Efficient Image Reconstruction of Spiking Cameras",
    "volume": "main",
    "abstract": "The spiking camera is an emerging neuromorphic vision sensor that records high-speed motion scenes by asynchronously firing continuous binary spike streams. Prevailing image reconstruction methods, generating intermediate frames from these spike streams, often rely on complex step-by-step network architectures that overlook the intrinsic collaboration of spatio-temporal complementary information. In this paper, we propose an efficient spatio-temporal interactive reconstruction network to jointly perform inter-frame feature alignment and intra-frame feature filtering in a coarse-to-fine manner. Specifically, it starts by extracting hierarchical features from a concise hybrid spike representation, then refines the motion fields and target frames scale-by-scale, ultimately obtaining a full-resolution output. Meanwhile, we introduce a symmetric interactive attention block and a multi-motion field estimation block to further enhance the interaction capability of the overall network. Experiments on synthetic and real-captured data show that our approach exhibits excellent performance while maintaining low model complexity",
    "checked": true,
    "id": "783d5c6b1753768b8db924ce98b668e20d019630",
    "semantic_title": "spatio-temporal interactive learning for efficient image reconstruction of spiking cameras",
    "citation_count": 3,
    "authors": [
      "Bin Fan",
      "Jiaoyang Yin",
      "Yuchao Dai",
      "Chao Xu",
      "Tiejun Huang",
      "Boxin Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/26289c647c6828e862e271ca3c490486-Abstract-Conference.html": {
    "title": "Acceleration Exists! Optimization Problems When Oracle Can Only Compare Objective Function Values",
    "volume": "main",
    "abstract": "Frequently, the burgeoning field of black-box optimization encounters challenges due to a limited understanding of the mechanisms of the objective function. To address such problems, in this work we focus on the deterministic concept of Order Oracle, which only utilizes order access between function values (possibly with some bounded noise), but without assuming access to their values. As theoretical results, we propose a new approach to create non-accelerated optimization algorithms (obtained by integrating Order Oracle into existing optimization \"tools\") in non-convex, convex, and strongly convex settings that are as good as both SOTA coordinate algorithms with first-order oracle and SOTA algorithms with Order Oracle up to logarithm factor. Moreover, using the proposed approach, we provide the first accelerated optimization algorithm using the Order Oracle. And also, using an already different approach we provide the asymptotic convergence of the first algorithm with the stochastic Order Oracle concept. Finally, our theoretical results demonstrate effectiveness of proposed algorithms through numerical experiments",
    "checked": true,
    "id": "cefd98cea25123e217184fbbd6081b5640d3ca18",
    "semantic_title": "acceleration exists! optimization problems when oracle can only compare objective function values",
    "citation_count": 3,
    "authors": [
      "Aleksandr Lobanov",
      "Alexander Gasnikov",
      "Andrey Krasnov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2628d4d3b054c2d7ad33ab03435204f4-Abstract-Conference.html": {
    "title": "Towards the Transferability of Rewards Recovered via Regularized Inverse Reinforcement Learning",
    "volume": "main",
    "abstract": "Inverse reinforcement learning (IRL) aims to infer a reward from expert demonstrations, motivated by the idea that the reward, rather than the policy, is the most succinct and transferable description of a task [Ng et al., 2000]. However, the reward corresponding to an optimal policy is not unique, making it unclear if an IRL-learned reward is transferable to new transition laws in the sense that its optimal policy aligns with the optimal policy corresponding to the expert's true reward. Past work has addressed this problem only under the assumption of full access to the expert's policy, guaranteeing transferability when learning from two experts with the same reward but different transition laws that satisfy a specific rank condition [Rolland et al., 2022]. In this work, we show that the conditions developed under full access to the expert's policy cannot guarantee transferability in the more practical scenario where we have access only to demonstrations of the expert. Instead of a binary rank condition, we propose principal angles as a more refined measure of similarity and dissimilarity between transition laws. Based on this, we then establish two key results: 1) a sufficient condition for transferability to any transition laws when learning from at least two experts with sufficiently different transition laws, and 2) a sufficient condition for transferability to local changes in the transition law when learning from a single expert. Furthermore, we also provide a probably approximately correct (PAC) algorithm and an end-to-end analysis for learning transferable rewards from demonstrations of multiple experts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Schlaginhaufen",
      "Maryam Kamgarpour"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/26300457961c3e056ea61c9d3ebec2a4-Abstract-Conference.html": {
    "title": "OneActor: Consistent Subject Generation via Cluster-Conditioned Guidance",
    "volume": "main",
    "abstract": "Text-to-image diffusion models benefit artists with high-quality image generation. Yet their stochastic nature hinders artists from creating consistent images of the same subject. Existing methods try to tackle this challenge and generate consistent content in various ways. However, they either depend on external restricted data or require expensive tuning of the diffusion model. For this issue, we propose a novel one-shot tuning paradigm, termed OneActor. It efficiently performs consistent subject generation solely driven by prompts via a learned semantic guidance to bypass the laborious backbone tuning. We lead the way to formalize the objective of consistent subject generation from a clustering perspective, and thus design a cluster-conditioned model. To mitigate the overfitting challenge shared by one-shot tuning pipelines, we augment the tuning with auxiliary samples and devise two inference strategies: semantic interpolation and cluster guidance. These techniques are later verified to significantly improve the generation quality. Comprehensive experiments show that our method outperforms a variety of baselines with satisfactory subject consistency, superior prompt conformity as well as high image quality. Our method is capable of multi-subject generation and compatible with popular diffusion extensions. Besides, we achieve a $4\\times$ faster tuning speed than tuning-based baselines and, if desired, avoid increasing the inference time. Furthermore, our method can be naturally utilized to pre-train a consistent subject generation network from scratch, which will implement this research task into more practical applications. (Project page: https://johnneywang.github.io/OneActor-webpage/)",
    "checked": true,
    "id": "79b13a0b3a0384e3deca42cc3670b7d7dae91756",
    "semantic_title": "oneactor: consistent subject generation via cluster-conditioned guidance",
    "citation_count": 9,
    "authors": [
      "Jiahao Wang",
      "Caixia Yan",
      "Haonan Lin",
      "Weizhan Zhang",
      "Mengmeng Wang",
      "Tieliang Gong",
      "Guang Dai",
      "Hao Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/263c763d00c6126d37ba670a1fa10847-Abstract-Conference.html": {
    "title": "Injecting Undetectable Backdoors in Obfuscated Neural Networks and Language Models",
    "volume": "main",
    "abstract": "As ML models become increasingly complex and integral to high-stakes domains such as finance and healthcare, they also become more susceptible to sophisticated adversarial attacks. We investigate the threat posed by undetectable backdoors, as defined in Goldwasser et al. [2022], in models developed by insidious external expert firms. When such backdoors exist, they allow the designer of the model to sell information on how to slightly perturb their input to change the outcome of the model. We develop a general strategy to plant backdoors to obfuscated neural networks, that satisfy the security properties of the celebrated notion of indistinguishability obfuscation. Applying obfuscation before releasing neural networks is a strategy that is well motivated to protect sensitive information of the external expert firm. Our method to plant backdoors ensures that even if the weights and architecture of the obfuscated model are accessible, the existence ofthe backdoor is still undetectable. Finally, we introduce the notion of undetectable backdoors to language models and extend our neural network backdoor attacks to such models based on the existence of steganographic functions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alkis Kalavasis",
      "Amin Karbasi",
      "Argyris Oikonomou",
      "Katerina Sotiraki",
      "Grigoris Velegkas",
      "Manolis Zampetakis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/264a9b3ce46abdf572dcfe0401141989-Abstract-Conference.html": {
    "title": "Nimbus: Secure and Efficient Two-Party Inference for Transformers",
    "volume": "main",
    "abstract": "Transformer models have gained significant attention due to their power in machine learning tasks. Their extensive deployment has raised concerns about the potential leakage of sensitive information during inference. However, when being applied to Transformers, existing approaches based on secure two-party computation (2PC) bring about efficiency limitations in two folds: (1) resource-intensive matrix multiplications in linear layers, and (2) complex non-linear activation functions like $\\mathsf{GELU}$ and $\\mathsf{Softmax}$. This work presents a new two-party inference framework $\\mathsf{Nimbus}$ for Transformer models. Specifically, we propose a new 2PC paradigm to securely compute matrix multiplications based on an outer-product insight, which achieves $2.9\\times \\sim 12.5\\times$ performance improvements compared to the state-of-the-art (SOTA) protocol. Furthermore, through a new observation of utilizing the input distribution, we propose an approach of low-degree polynomial approximation for $\\mathsf{GELU}$ and $\\mathsf{Softmax}$, which improves the performance of the SOTA polynomial approximation by $2.9\\times \\sim 4.0\\times$, where the average accuracy loss of our approach is 0.08\\% compared to the non-2PC inference without privacy. Compared with the SOTA two-party inference, $\\mathsf{Nimbus}$ improves the end-to-end performance of $BERT_{base}$ inference by $2.7\\times \\sim 4.7\\times$ across different network settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyi Li",
      "Kang Yang",
      "Jin Tan",
      "Wen-jie Lu",
      "Haoqi Wu",
      "Xiao Wang",
      "Yu Yu",
      "Derun Zhao",
      "Yancheng Zheng",
      "Minyi Guo",
      "Jingwen Leng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/265d0413fa2efa41084f58b52a4a2a7f-Abstract-Conference.html": {
    "title": "SureMap: Simultaneous mean estimation for single-task and multi-task disaggregated evaluation",
    "volume": "main",
    "abstract": "Disaggregated evaluation—estimation of performance of a machine learning model on different subpopulations—is a core task when assessing performance and group-fairness of AI systems.A key challenge is that evaluation data is scarce, and subpopulations arising from intersections of attributes (e.g., race, sex, age) are often tiny.Today, it is common for multiple clients to procure the same AI model from a model developer, and the task of disaggregated evaluation is faced by each customer individually. This gives rise to what we call the multi-task disaggregated evaluation problem, wherein multiple clients seek to conduct a disaggregated evaluation of a given model in their own data setting (task). In this work we develop a disaggregated evaluation method called SureMap that has high estimation accuracy for both multi-task and single-task disaggregated evaluations of blackbox models. SureMap's efficiency gains come from(1) transforming the problem into structured simultaneous Gaussian mean estimation and (2) incorporating external data, e.g., from the AI system creator or from their other clients. Our method combines maximum a posteriori (MAP) estimation using a well-chosen prior together with cross-validation-free tuning via Stein's unbiased risk estimate (SURE).We evaluate SureMap on disaggregated evaluation tasks in multiple domains, observing significant accuracy improvements over several strong competitors",
    "checked": true,
    "id": "67d427fb9d62fb25fc441f975c1888c14b60a02a",
    "semantic_title": "suremap: simultaneous mean estimation for single-task and multi-task disaggregated evaluation",
    "citation_count": 0,
    "authors": [
      "Misha Khodak",
      "Lester W. Mackey",
      "Alexandra Chouldechova",
      "Miro Dudik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/266c0f191b04cbbbe529016d0edc847e-Abstract-Conference.html": {
    "title": "FUSE: Fast Unified Simulation and Estimation for PDEs",
    "volume": "main",
    "abstract": "The joint prediction of continuous fields and statistical estimation of the underlying discrete parameters is a common problem for many physical systems, governed by PDEs. Hitherto, it has been separately addressed by employing operator learning surrogates for field prediction while using simulation-based inference (and its variants) for statistical parameter determination. Here, we argue that solving both problems within the same framework can lead to consistent gains in accuracy and robustness. To this end, we propose a novel and flexible formulation of the operator learning problem that jointly predicts continuous quantities and infers distributions of discrete parameters, thereby amortizing the cost of both the inverse and the surrogate models to a joint pre-training step. We present the capabilities of the proposed methodology for predicting continuous and discrete biomarkers in full-body haemodynamics simulations under different levels of missing information. We also consider a test case for atmospheric large-eddy simulation of a two-dimensional dry cold bubble, where we infer both continuous time-series and information about the system's conditions. We present comparisons against different baselines to showcase significantly increased accuracy in both the inverse and the surrogate tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Levi Lingsch",
      "Dana Grund",
      "Siddhartha Mishra",
      "Georgios Kissas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2676109d49d1eb26d6bc584a8f556305-Abstract-Conference.html": {
    "title": "The Collusion of Memory and Nonlinearity in Stochastic Approximation With Constant Stepsize",
    "volume": "main",
    "abstract": "In this work, we investigate stochastic approximation (SA) with Markovian data and nonlinear updates under constant stepsize $\\alpha>0$. Existing work has primarily focused on either i.i.d. data or linear update rules. We take a new perspective and carefully examine the simultaneous presence of Markovian dependency of data and nonlinear update rules, delineating how the interplay between these two structures leads to complications that are not captured by prior techniques. By leveraging the smoothness and recurrence properties of the SA updates, we develop a fine-grained analysis of the correlation between the SA iterates $\\theta_k$ and Markovian data $x_k$. This enables us to overcome the obstacles in existing analysis and establish for the first time the weak convergence of the joint process $(x_k, \\theta_k)$. Furthermore, we present a precise characterization of the asymptotic bias of the SA iterates, given by $\\mathbb{E}[\\theta_\\infty]-\\theta^\\ast=\\alpha(b_\\textup{m}+b_\\textup{n}+b_\\textup{c})+\\mathcal{O}(\\alpha^{3/2})$. Here, $b_\\textup{m}$ is associated with the Markovian noise, $b_\\textup{n}$ is tied to the nonlinearity of the SA operator, and notably, $b_\\textup{c}$ represents a multiplicative interaction between the Markovian noise and the nonlinearity of the operator, which is absent in previous works. As a by-product of our analysis, we derive finite-time bounds on higher moment $\\mathbb{E}[||\\theta_k-\\theta^\\ast||^{2p}]$ and present non-asymptotic geometric convergence rates for the iterates, along with a Central Limit Theorem",
    "checked": true,
    "id": "c1aeaab87dadfd0591a8e8a6c5bee31f08ad1b99",
    "semantic_title": "the collusion of memory and nonlinearity in stochastic approximation with constant stepsize",
    "citation_count": 4,
    "authors": [
      "Dongyan Lucy Huo",
      "Yixuan Zhang",
      "Yudong Chen",
      "Qiaomin Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/26b8e3dc3a21fcd660d80c63b767f324-Abstract-Conference.html": {
    "title": "Interventionally Consistent Surrogates for Complex Simulation Models",
    "volume": "main",
    "abstract": "Large-scale simulation models of complex socio-technical systems provide decision-makers with high-fidelity testbeds in which policy interventions can be evaluated and what-if scenarios explored. Unfortunately, the high computational cost of such models inhibits their widespread use in policy-making settings. Surrogate models can address these computational limitations, but to do so they must behave consistently with the simulator under interventions of interest. In this paper, we build upon recent developments in causal abstractions to develop a framework for learning interventionally consistent surrogate models for large-scale, complex simulation models. We provide theoretical results showing that our proposed approach induces surrogates to behave consistently with high probability with respect to the simulator across interventions of interest, facilitating rapid experimentation with policy interventions in complex systems. We further demonstrate with empirical studies that conventionally trained surrogates can misjudge the effect of interventions and misguide decision-makers towards suboptimal interventions, while surrogates trained for interventional consistency with our method closely mimic the behaviour of the original simulator under interventions of interest",
    "checked": true,
    "id": "d0a30c2e1cbc3cc73408509e367f6b8ac74067a2",
    "semantic_title": "interventionally consistent surrogates for complex simulation models",
    "citation_count": 4,
    "authors": [
      "Joel Dyer",
      "Nicholas Bishop",
      "Yorgos Felekis",
      "Fabio Massimo Zennaro",
      "Anisoara Calinescu",
      "Theodoros Damoulas",
      "Michael Wooldridge"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/26cfdcd8fe6fd75cc53e92963a656c58-Abstract-Conference.html": {
    "title": "Depth Anything V2",
    "volume": "main",
    "abstract": "This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with sparse depth annotations to facilitate future research. Models are available at https://github.com/DepthAnything/Depth-Anything-V2",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lihe Yang",
      "Bingyi Kang",
      "Zilong Huang",
      "Zhen Zhao",
      "Xiaogang Xu",
      "Jiashi Feng",
      "Hengshuang Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/26d01e5ed42d8dcedd6aa0e3e99cffc4-Abstract-Conference.html": {
    "title": "Transferring disentangled representations: bridging the gap between synthetic and real images",
    "volume": "main",
    "abstract": "Developing meaningful and efficient representations that separate the fundamental structure of the data generation mechanism is crucial in representation learning. However, Disentangled Representation Learning has not fully shown its potential on real images, because of correlated generative factors, their resolution and limited access to ground truth labels. Specifically on the latter, we investigate the possibility of leveraging synthetic data to learn general-purpose disentangled representations applicable to real data, discussing the effect of fine-tuning and what properties of disentanglement are preserved after the transfer. We provide an extensive empirical study to address these issues. In addition, we propose a new interpretable intervention-based metric, to measure the quality of factors encoding in the representation. Our results indicate that some level of disentanglement, transferring a representation from synthetic to real data, is possible and effective",
    "checked": true,
    "id": "936867d0598ffc3714bff20fa9e488059cbb95af",
    "semantic_title": "transferring disentangled representations: bridging the gap between synthetic and real images",
    "citation_count": 1,
    "authors": [
      "Jacopo Dapueto",
      "Nicoletta Noceti",
      "Francesca Odone"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/26f88f4fc9ee99578a066be7a0ede6dd-Abstract-Conference.html": {
    "title": "Absorb & Escape: Overcoming Single Model Limitations in Generating Heterogeneous Genomic Sequences",
    "volume": "main",
    "abstract": "Recent advances in immunology and synthetic biology have accelerated the development of deep generative methods for DNA sequence design. Two dominant approaches in this field are AutoRegressive (AR) models and Diffusion Models (DMs). However, genomic sequences are functionally heterogeneous, consisting of multiple connected regions (e.g., Promoter Regions, Exons, and Introns) where elements within each region come from the same probability distribution, but the overall sequence is non-homogeneous. This heterogeneous nature presents challenges for a single model to accurately generate genomic sequences. In this paper, we analyze the properties of AR models and DMs in heterogeneous genomic sequence generation, pointing out crucial limitations in both methods: (i) AR models capture the underlying distribution of data by factorizing and learning the transition probability but fail to capture the global property of DNA sequences. (ii) DMs learn to recover the global distribution but tend to produce errors at the base pair level. To overcome the limitations of both approaches, we propose a post-training sampling method, termed Absorb & Escape (A&E) to perform compositional generation from AR models and DMs. This approach starts with samples generated by DMs and refines the sample quality using an AR model through the alternation of the Absorb and Escape steps. To assess the quality of generated sequences, we conduct extensive experiments on 15 species for conditional and unconditional DNA generation. The experiment results from motif distribution, diversity checks, and genome integration tests unequivocally show that A&E outperforms state-of-the-art AR models and DMs in genomic sequence generation. A&E does not suffer from the slowness of traditional MCMC to sample from composed distributions with Energy-Based Models whilst it obtains higher quality samples than single models. Our research sheds light on the limitations of current single-model approaches in DNA generation and provides a simple but effective solution for heterogeneous sequence generation. Code is available at the Github Repo",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehui Li",
      "Yuhao Ni",
      "Guoxuan Xia",
      "William Beardall",
      "Akashaditya Das",
      "Guy-Bart Stan",
      "Yiren Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2718a032d15e0b80cd164b240220df89-Abstract-Conference.html": {
    "title": "Can We Leave Deepfake Data Behind in Training Deepfake Detector?",
    "volume": "main",
    "abstract": "The generalization ability of deepfake detectors is vital for their applications in real-world scenarios. One effective solution to enhance this ability is to train the models with manually-blended data, which we termed ''blendfake'', encouraging models to learn generic forgery artifacts like blending boundary. Interestingly, current SoTA methods utilize blendfake $\\textit{without}$ incorporating any deepfake data in their training process. This is likely because previous empirical observations suggest that vanilla hybrid training (VHT), which combines deepfake and blendfake data, results in inferior performance to methods using only blendfake data (so-called \"1+1<2\"). Therefore, a critical question arises: Can we leave deepfake behind and rely solely on blendfake data to train an effective deepfake detector? Intuitively, as deepfakes also contain additional informative forgery clues ($\\textit{e.g.,}$ deep generative artifacts), excluding all deepfake data in training deepfake detectors seems counter-intuitive. In this paper, we rethink the role of blendfake in detecting deepfakes and formulate the process from \"real to blendfake to deepfake\" to be a $\\textit{progressive transition}$. Specifically, blendfake and deepfake can be explicitly delineated as the oriented pivot anchors between \"real-to-fake\" transitions. The accumulation of forgery information should be oriented and progressively increasing during this transition process. To this end, we propose an $\\underline{O}$riented $\\underline{P}$rogressive $\\underline{R}$egularizor (OPR) to establish the constraints that compel the distribution of anchors to be discretely arranged. Furthermore, we introduce feature bridging to facilitate the smooth transition between adjacent anchors. Extensive experiments confirm that our design allows leveraging forgery information from both blendfake and deepfake effectively and comprehensively. Code is available at https://github.com/beautyremain/ProDet",
    "checked": true,
    "id": "6b186896a5b2c15ea07a1e516c41ce01f2c15772",
    "semantic_title": "can we leave deepfake data behind in training deepfake detector?",
    "citation_count": 23,
    "authors": [
      "Jikang Cheng",
      "Zhiyuan Yan",
      "Ying Zhang",
      "Yuhao Luo",
      "Zhongyuan Wang",
      "Chen Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2725d878e29131c755db73fdd1d387e1-Abstract-Conference.html": {
    "title": "UNION: Unsupervised 3D Object Detection using Object Appearance-based Pseudo-Classes",
    "volume": "main",
    "abstract": "Unsupervised 3D object detection methods have emerged to leverage vast amounts of data without requiring manual labels for training. Recent approaches rely on dynamic objects for learning to detect mobile objects but penalize the detections of static instances during training. Multiple rounds of (self) training are used to add detected static instances to the set of training targets; this procedure to improve performance is computationally expensive. To address this, we propose the method UNION. We use spatial clustering and self-supervised scene flow to obtain a set of static and dynamic object proposals from LiDAR. Subsequently, object proposals' visual appearances are encoded to distinguish static objects in the foreground and background by selecting static instances that are visually similar to dynamic objects. As a result, static and dynamic mobile objects are obtained together, and existing detectors can be trained with a single training. In addition, we extend 3D object discovery to detection by using object appearance-based cluster labels as pseudo-class labels for training object classification. We conduct extensive experiments on the nuScenes dataset and increase the state-of-the-art performance for unsupervised 3D object discovery, i.e. UNION more than doubles the average precision to 38.4. The code is available at github.com/TedLentsch/UNION",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ted Lentsch",
      "Holger Caesar",
      "Dariu Gavrila"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/272efd3a6091ceefcbc79f1f3a6fdba4-Abstract-Conference.html": {
    "title": "Adaptive Variance Reduction for Stochastic Optimization under Weaker Assumptions",
    "volume": "main",
    "abstract": "This paper explores adaptive variance reduction methods for stochastic optimization based on the STORM technique. Existing adaptive extensions of STORM rely on strong assumptions like bounded gradients and bounded function values, or suffer an additional $\\mathcal{O}(\\log T)$ term in the convergence rate. To address these limitations, we introduce a novel adaptive STORM method that achieves an optimal convergence rate of $\\mathcal{O}(T^{-1/3})$ for non-convex functions with our newly designed learning rate strategy. Compared with existing approaches, our method requires weaker assumptions and attains the optimal convergence rate without the additional $\\mathcal{O}(\\log T)$ term. We also extend the proposed technique to stochastic compositional optimization, obtaining the same optimal rate of $\\mathcal{O}(T^{-1/3})$. Furthermore, we investigate the non-convex finite-sum problem and develop another innovative adaptive variance reduction method that achieves an optimal convergence rate of $\\mathcal{O}(n^{1/4} T^{-1/2} )$, where $n$ represents the number of component functions. Numerical experiments across various tasks validate the effectiveness of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Jiang",
      "Sifan Yang",
      "Yibo Wang",
      "Lijun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/274d0146144643ee2459a602123c60ff-Abstract-Conference.html": {
    "title": "Kaleidoscope: Learnable Masks for Heterogeneous Multi-agent Reinforcement Learning",
    "volume": "main",
    "abstract": "In multi-agent reinforcement learning (MARL), parameter sharing is commonly employed to enhance sample efficiency. However, the popular approach of full parameter sharing often leads to homogeneous policies among agents, potentially limiting the performance benefits that could be derived from policy diversity. To address this critical limitation, we introduce \\emph{Kaleidoscope}, a novel adaptive partial parameter sharing scheme that fosters policy heterogeneity while still maintaining high sample efficiency. Specifically, Kaleidoscope maintains one set of common parameters alongside multiple sets of distinct, learnable masks for different agents, dictating the sharing of parameters. It promotes diversity among policy networks by encouraging discrepancy among these masks, without sacrificing the efficiencies of parameter sharing. This design allows Kaleidoscope to dynamically balance high sample efficiency with a broad policy representational capacity, effectively bridging the gap between full parameter sharing and non-parameter sharing across various environments. We further extend Kaleidoscope to critic ensembles in the context of actor-critic algorithms, which could help improve value estimations. Our empirical evaluations across extensive environments, including multi-agent particle environment, multi-agent MuJoCo and StarCraft multi-agent challenge v2, demonstrate the superior performance of Kaleidoscope compared with existing parameter sharing approaches, showcasing its potential for performance enhancement in MARL. The code is publicly available at \\url{https://github.com/LXXXXR/Kaleidoscope}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinran Li",
      "Ling Pan",
      "Jun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/27571b74d6cd650b8eb6cf1837953ae8-Abstract-Conference.html": {
    "title": "Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning",
    "volume": "main",
    "abstract": "The recent success of interleaved Large Multimodal Models (LMMs) in few-shot learning suggests that in-context learning (ICL) with many examples can be promising for learning new tasks. However, this many-shot multimodal ICL setting has one crucial problem: it is fundamentally limited by the model's context length set at pretraining. The problem is especially prominent in the multimodal domain, which processes both text and images, requiring additional tokens. This motivates the need for a multimodal method to compress many shots into fewer tokens without finetuning. In this work, we enable LMMs to perform multimodal, many-shot in-context learning by leveraging Multimodal Task Vectors (MTV)---compact implicit representations of in-context examples compressed in the model's attention heads. Specifically, we first demonstrate the existence of such MTV in LMMs and then leverage these extracted MTV to enable many-shot in-context learning for various vision-and-language tasks. Our experiments suggest that MTV can scale in performance with the number of compressed shots and generalize to similar out-of-domain tasks without additional context length for inference. Code: https://github.com/Brandon3964/MultiModal-Task-Vector",
    "checked": true,
    "id": "bea90a9bdf75248643f3b9e84bc632348f3491c4",
    "semantic_title": "multimodal task vectors enable many-shot multimodal in-context learning",
    "citation_count": 25,
    "authors": [
      "Brandon Huang",
      "Chancharik Mitra",
      "Leonid Karlinsky",
      "Assaf Arbelle",
      "Trevor Darrell",
      "Roei Herzig"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/27666e699d9a94540fac44eae955d8ed-Abstract-Conference.html": {
    "title": "Quadratic Quantum Variational Monte Carlo",
    "volume": "main",
    "abstract": "This paper introduces the Quadratic Quantum Variational Monte Carlo (Q$^2$VMC) algorithm, an innovative algorithm in quantum chemistry that significantly enhances the efficiency and accuracy of solving the Schrödinger equation. Inspired by the discretization of imaginary-time Schrödinger evolution, Q$^2$VMC employs a novel quadratic update mechanism that integrates seamlessly with neural network-based ansatzes. Our extensive experiments showcase Q$^2$VMC's superior performance, achieving faster convergence and lower ground state energies in wavefunction optimization across various molecular systems, without additional computational cost. This study not only advances the field of computational quantum chemistry but also highlights the important role of discretized evolution in variational quantum algorithms, offering a scalable and robust framework for future quantum research",
    "checked": true,
    "id": "4644f2856ff5febeb688afa664a35caf41519873",
    "semantic_title": "quadratic quantum variational monte carlo",
    "citation_count": 0,
    "authors": [
      "Baiyu Su",
      "Qiang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/277628cff838927d869cd1f671328ce0-Abstract-Conference.html": {
    "title": "Lorentz-Equivariant Geometric Algebra Transformers for High-Energy Physics",
    "volume": "main",
    "abstract": "Extracting scientific understanding from particle-physics experiments requires solving diverse learning problems with high precision and good data efficiency. We propose the Lorentz Geometric Algebra Transformer (L-GATr), a new multi-purpose architecture for high-energy physics. L-GATr represents high-energy data in a geometric algebra over four-dimensional space-time and is equivariant under Lorentz transformations, the symmetry group of relativistic kinematics. At the same time, the architecture is a Transformer, which makes it versatile and scalable to large systems. L-GATr is first demonstrated on regression and classification tasks from particle physics. We then construct the first Lorentz-equivariant generative model: a continuous normalizing flow based on an L-GATr network, trained with Riemannian flow matching. Across our experiments, L-GATr is on par with or outperforms strong domain-specific baselines",
    "checked": true,
    "id": "81e1ab8f0bc160286d156c7cd8efc87ad15f6f57",
    "semantic_title": "lorentz-equivariant geometric algebra transformers for high-energy physics",
    "citation_count": 20,
    "authors": [
      "Jonas Spinner",
      "Victor Breso",
      "Pim de Haan",
      "Tilman Plehn",
      "Jesse Thaler",
      "Johann Brehmer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2783192ea2696ee2ceb8746f5eea6681-Abstract-Conference.html": {
    "title": "Rethinking the Power of Timestamps for Robust Time Series Forecasting: A Global-Local Fusion Perspective",
    "volume": "main",
    "abstract": "Time series forecasting has played a pivotal role across various industries, including finance, transportation, energy, healthcare, and climate. Due to the abundant seasonal information they contain, timestamps possess the potential to offer robust global guidance for forecasting techniques. However, existing works primarily focus on local observations, with timestamps being treated merely as an optional supplement that remains underutilized. When data gathered from the real world is polluted, the absence of global information will damage the robust prediction capability of these algorithms. To address these problems, we propose a novel framework named GLAFF. Within this framework, the timestamps are modeled individually to capture the global dependencies. Working as a plugin, GLAFF adaptively adjusts the combined weights for global and local information, enabling seamless collaboration with any time series forecasting backbone. Extensive experiments conducted on nine real-world datasets demonstrate that GLAFF significantly enhances the average performance of widely used mainstream forecasting models by 12.5\\%, surpassing the previous state-of-the-art method by 5.5\\%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengsen Wang",
      "Qi Qi",
      "Jingyu Wang",
      "Haifeng Sun",
      "Zirui Zhuang",
      "Jinming Wu",
      "Jianxin Liao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/27aa3a0e6d63db269977bb2df5607cb8-Abstract-Conference.html": {
    "title": "Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure",
    "volume": "main",
    "abstract": "Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer. Taking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more \"relevant\" tokens; for integer addition tasks, we regard digits of the same significance as in the same position. On the empirical side, we show that with the proposed position coupling, our models trained on 1 to 30-digit additions can generalize up to 200-digit additions (6.67x of the trained length). On the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it. We also demonstrate that position coupling can be applied to other algorithmic tasks such as Nx2 multiplication and a two-dimensional task. Our codebase is available at github.com/HanseulJo/position-coupling",
    "checked": true,
    "id": "2403122c4ad323e4ef34b2c4370693ecf1011e8b",
    "semantic_title": "position coupling: improving length generalization of arithmetic transformers using task structure",
    "citation_count": 10,
    "authors": [
      "Hanseul Cho",
      "Jaeyoung Cha",
      "Pranjal Awasthi",
      "Srinadh Bhojanapalli",
      "Anupam Gupta",
      "Chulhee Yun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/27c8b849acba6793f0b73f7ee7ea7397-Abstract-Conference.html": {
    "title": "Advancing Fine-Grained Classification by Structure and Subject Preserving Augmentation",
    "volume": "main",
    "abstract": "Fine-grained visual classification (FGVC) involves classifying closely related subcategories. This task is inherently difficult due to the subtle differences between classes and the high intra-class variance. Moreover, FGVC datasets are typically small and challenging to gather, thus highlighting a significant need for effective data augmentation.Recent advancements in text-to-image diffusion models have introduced new possibilities for data augmentation in image classification. While these models have been used to generate training data for classification tasks, their effectiveness in full-dataset training of FGVC models remains under-explored. Recent techniques that rely on text-to-image generation or Img2Img methods, such as SDEdit, often struggle to generate images that accurately represent the class while modifying them to a degree that significantly increases the dataset's diversity. To address these challenges, we present SaSPA: Structure and Subject Preserving Augmentation. Contrary to recent methods, our method does not use real images as guidance, thereby increasing generation flexibility and promoting greater diversity. To ensure accurate class representation, we employ conditioning mechanisms, specifically by conditioning on image edges and subject representation.We conduct extensive experiments and benchmark SaSPA against both traditional and generative data augmentation techniques. SaSPA consistently outperforms all established baselines across multiple settings, including full dataset training and contextual bias. Additionally, our results reveal interesting patterns in using synthetic data for FGVC models; for instance, we find a relationship between the amount of real data used and the optimal proportion of synthetic data",
    "checked": true,
    "id": "6e4b3bed67d7d20178926c3f41f0a655f27c3030",
    "semantic_title": "advancing fine-grained classification by structure and subject preserving augmentation",
    "citation_count": 2,
    "authors": [
      "Eyal Michaeli",
      "Ohad Fried"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/27e5626cabdbb6cd5c56ce4114ff93e4-Abstract-Conference.html": {
    "title": "RCDN: Towards Robust Camera-Insensitivity Collaborative Perception via Dynamic Feature-based 3D Neural Modeling",
    "volume": "main",
    "abstract": "Collaborative perception is dedicated to tackling the constraints of single-agent perception, such as occlusions, based on the multiple agents' multi-view sensor inputs. However, most existing works assume an ideal condition that all agents' multi-view cameras are continuously available. In reality, cameras may be highly noisy, obscured or even failed during the collaboration. In this work, we introduce a new robust camera-insensitivity problem: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? To address above problems, we propose RCDN, a Robust Camera-insensitivity collaborative perception with a novel Dynamic feature-based 3D Neural modeling mechanism. The key intuition of RCDN is to construct collaborative neural rendering field representations to recover failed perceptual messages sent by multiple agents. To better model collaborative neural rendering field, RCDN first establishes a geometry BEV feature based time-invariant static field with other agents via fast hash grid modeling. Based on the static background field, the proposed time-varying dynamic field can model corresponding motion vector for foregrounds with appropriate positions. To validate RCDN, we create OPV2V-N, a new large-scale dataset with manual labelling under different camera failed scenarios. Extensive experiments conducted on OPV2V-N show that RCDN can be ported to other baselines and improve their robustness in extreme camera-insensitivity setting. Our code and datasets will be available soon",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhang Wang",
      "Fan Lu",
      "Zehan Zheng",
      "Zhijun Li",
      "Guang Chen",
      "changjun jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2818054fc6de6dacdda0f142a3475933-Abstract-Conference.html": {
    "title": "TFG: Unified Training-Free Guidance for Diffusion Models",
    "volume": "main",
    "abstract": "Given an unconditional diffusion model and a predictor for a target property of interest (e.g., a classifier), the goal of training-free guidance is to generate samples with desirable target properties without additional training. Existing methods, though effective in various individual applications, often lack theoretical grounding and rigorous testing on extensive benchmarks. As a result, they could even fail on simple tasks, and applying them to a new problem becomes unavoidably difficult. This paper introduces a novel algorithmic framework encompassing existing methods as special cases, unifying the study of training-free guidance into the analysis of an algorithm-agnostic design space. Via theoretical and empirical investigation, we propose an efficient and effective hyper-parameter searching strategy that can be readily applied to any downstream task. We systematically benchmark across 7 diffusion models on 16 tasks with 40 targets, and improve performance by 8.5% on average. Our framework and benchmark offer a solid foundation for conditional generation in a training-free manner",
    "checked": true,
    "id": "af1562eb85e7929d46881b8b725df24e32f8fade",
    "semantic_title": "tfg: unified training-free guidance for diffusion models",
    "citation_count": 35,
    "authors": [
      "Haotian Ye",
      "Haowei Lin",
      "Jiaqi Han",
      "Minkai Xu",
      "Sheng Liu",
      "Yitao Liang",
      "Jianzhu Ma",
      "James Y Zou",
      "Stefano Ermon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/281a6b49c66b94f81f3b1ae8f1a0dea4-Abstract-Conference.html": {
    "title": "Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models",
    "volume": "main",
    "abstract": "Modern optimizers such as AdamW, equipped with momentum and adaptive learning rate, are designed to escape local minima and explore the vast parameter space. This exploration is beneficial for finding good loss basins when training from scratch. It is not necessarily ideal when resuming from a powerful foundation model because it can lead to large deviations from the pre-trained initialization and, consequently, worse robustness and generalization. At the same time, strong regularization on all parameters can lead to under-fitting. We hypothesize that selectively regularizing the parameter space is the key to fitting and retraining the pre-trained knowledge. This paper proposes a new weight decay technique, Selective Projection Decay (SPD), that selectively imposes a strong penalty on certain layers while allowing others to change freely. Intuitively, SPD expands and contracts the parameter search space for layers with consistent and inconsistent loss reduction, respectively. Experimentally, when equipped with SPD, Adam consistently provides better in-distribution generalization and out-of-distribution robustness performance on multiple popular vision and language benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjiao Tian",
      "Chengyue Huang",
      "Zsolt Kira"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/28236482f64a72eec43706b6f3a6c511-Abstract-Conference.html": {
    "title": "Efficient multi-prompt evaluation of LLMs",
    "volume": "main",
    "abstract": "Most popular benchmarks for comparing LLMs rely on a limited set of prompt templates, which may not fully capture the LLMs' abilities and can affect the reproducibility of results on leaderboards. Many recent works empirically verify prompt sensitivity and advocate for changes in LLM evaluation. In this paper, we consider the problem of estimating the performance distribution across many prompt variants instead of finding a single prompt to evaluate with. We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets. The resulting distribution can be used to obtain performance quantiles to construct various robust performance metrics (e.g., top 95% quantile or median). We prove that PromptEval consistently estimates the performance distribution and demonstrate its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry; for example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations. Moreover, we show how PromptEval can be useful in LLM-as-a-judge and best prompt identification applications",
    "checked": true,
    "id": "a1e2557fa6d5373c8f89b8c4d426168cdf31d7d5",
    "semantic_title": "efficient multi-prompt evaluation of llms",
    "citation_count": 31,
    "authors": [
      "Felipe Maia Polo",
      "Ronald Xu",
      "Lucas Weber",
      "Mírian Silva",
      "Onkar Bhardwaj",
      "Leshem Choshen",
      "Allysson de Oliveira",
      "Yuekai Sun",
      "Mikhail Yurochkin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/28312c9491d60ed0c77f7fff4ad86dd1-Abstract-Conference.html": {
    "title": "FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations",
    "volume": "main",
    "abstract": "The rapid development of Large Language Models (LLMs) has been pivotal in advancing AI, with pre-trained LLMs being adaptable to diverse downstream tasks through fine-tuning. Federated learning (FL) further enhances fine-tuning in a privacy-aware manner by utilizing clients' local data through in-situ computation, eliminating the need for data movement. However, fine-tuning LLMs, given their massive scale of parameters, poses challenges for clients with constrained and heterogeneous resources in FL. Previous methods employed low-rank adaptation (LoRA) for efficient federated fine-tuning but utilized traditional FL aggregation strategies on LoRA adapters. This approach led to mathematically inaccurate aggregation noise, reducing fine-tuning effectiveness and failing to address heterogeneous LoRAs. In this work, we first highlight the mathematical incorrectness of LoRA aggregation in existing federated fine-tuning methods. We introduce a new approach called FLoRA that enables federated fine-tuning on heterogeneous LoRA adapters across clients through a novel stacking-based aggregation method. Our approach is noise-free and seamlessly supports heterogeneous LoRAs. Extensive experiments demonstrate FLoRA's superior performance in both homogeneous and heterogeneous settings, surpassing state-of-the-art methods. We envision this work as a milestone for efficient, privacy-preserving, and accurate federated fine-tuning of LLMs",
    "checked": true,
    "id": "bd597fa58a845b94aa7b2445c14ba9c65548ace5",
    "semantic_title": "flora: federated fine-tuning large language models with heterogeneous low-rank adaptations",
    "citation_count": 59,
    "authors": [
      "Ziyao Wang",
      "Zheyu Shen",
      "Yexiao He",
      "Guoheng Sun",
      "Hongyi Wang",
      "Lingjuan Lyu",
      "Ang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2844b1bec82dd4be201f68715c03ed1b-Abstract-Conference.html": {
    "title": "Learning the Expected Core of Strictly Convex Stochastic Cooperative Games",
    "volume": "main",
    "abstract": "Reward allocation, also known as the credit assignment problem, has been an important topic in economics, engineering, and machine learning. An important concept in reward allocation is the core, which is the set of stable allocations where no agent has the motivation to deviate from the grand coalition. In previous works, computing the core requires either knowledge of the reward function in deterministic games or the reward distribution in stochastic games. However, this is unrealistic, as the reward function or distribution is often only partially known and may be subject to uncertainty. In this paper, we consider the core learning problem in stochastic cooperative games, where the reward distribution is unknown. Our goal is to learn the expected core, that is, the set of allocations that are stable in expectation, given an oracle that returns a stochastic reward for an enquired coalition each round. Within the class of strictly convex games, we present an algorithm named \\texttt{Common-Points-Picking} that returns a point in the expected core given a polynomial number of samples, with high probability. To analyse the algorithm, we develop a new extension of the separation hyperplane theorem for multiple convex sets.t",
    "checked": true,
    "id": "90d3afd605aa6b5e983ae28aecc67e14d33e8427",
    "semantic_title": "learning the expected core of strictly convex stochastic cooperative games",
    "citation_count": 0,
    "authors": [
      "Phuong Nam Tran",
      "The Anh Ta",
      "shuqing shi",
      "Debmalya Mandal",
      "Yali Du",
      "Long Tran-Thanh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2847043899e1171183ceadf86bdbb280-Abstract-Conference.html": {
    "title": "ParallelEdits: Efficient Multi-Aspect Text-Driven Image Editing with Attention Grouping",
    "volume": "main",
    "abstract": "Text-driven image synthesis has made significant advancements with the development of diffusion models, transforming how visual content is generated from text prompts. Despite these advances, text-driven image editing, a key area in computer graphics, faces unique challenges. A major challenge is making simultaneous edits across multiple objects or attributes. Applying these methods sequentially for multi-attribute edits increases computational demands and efficiency losses. In this paper, we address these challenges with significant contributions. Our main contribution is the development of ParallelEdits, a method that seamlessly manages simultaneous edits across multiple attributes. In contrast to previous approaches, ParallelEdits not only preserves the quality of single attribute edits but also significantly improves the performance of multitasking edits. This is achieved through innovative attention distribution mechanism and multi-branch design that operates across several processing heads. Additionally, we introduce the PIE-Bench++ dataset, an expansion of the original PIE-Bench dataset, to better support evaluating image-editing tasks involving multiple objects and attributes simultaneously. This dataset is a benchmark for evaluating text-driven image editing methods in multifaceted scenarios",
    "checked": true,
    "id": "85cead83196d1b04fa769a3dde473f56a7fa108c",
    "semantic_title": "paralleledits: efficient multi-aspect text-driven image editing with attention grouping",
    "citation_count": 7,
    "authors": [
      "Mingzhen Huang",
      "Jialing Cai",
      "Shan Jia",
      "Vishnu Lokhande",
      "Siwei Lyu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2847d43f17410c5beb25b2736c3ae778-Abstract-Conference.html": {
    "title": "Looks Too Good To Be True: An Information-Theoretic Analysis of Hallucinations in Generative Restoration Models",
    "volume": "main",
    "abstract": "The pursuit of high perceptual quality in image restoration has driven the development of revolutionary generative models, capable of producing results often visually indistinguishable from real data.However, as their perceptual quality continues to improve, these models also exhibit a growing tendency to generate hallucinations – realistic-looking details that do not exist in the ground truth images.Hallucinations in these models create uncertainty about their reliability, raising major concerns about their practical application.This paper investigates this phenomenon through the lens of information theory, revealing a fundamental tradeoff between uncertainty and perception. We rigorously analyze the relationship between these two factors, proving that the global minimal uncertainty in generative models grows in tandem with perception. In particular, we define the inherent uncertainty of the restoration problem and show that attaining perfect perceptual quality entails at least twice this uncertainty. Additionally, we establish a relation between distortion, uncertainty and perception, through which we prove the aforementioned uncertainly-perception tradeoff induces the well-known perception-distortion tradeoff.We demonstrate our theoretical findings through experiments with super-resolution and inpainting algorithms.This work uncovers fundamental limitations of generative models in achieving both high perceptual quality and reliable predictions for image restoration. Thus, we aim to raise awareness among practitioners about this inherent tradeoff, empowering them to make informed decisions and potentially prioritize safety over perceptual performance",
    "checked": true,
    "id": "e665772b46abb07c5a786e603b553e14234176dd",
    "semantic_title": "looks too good to be true: an information-theoretic analysis of hallucinations in generative restoration models",
    "citation_count": 4,
    "authors": [
      "Regev Cohen",
      "Idan Kligvasser",
      "Ehud Rivlin",
      "Daniel Freedman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2858e880333b3cd64f8192f13ddcca2f-Abstract-Conference.html": {
    "title": "Implicit Optimization Bias of Next-token Prediction in Linear Models",
    "volume": "main",
    "abstract": "We initiate an investigation into the optimization properties of next-token prediction (NTP), the dominant training paradigm for modern language models. Specifically, we study the structural properties of the solutions selected by gradient-based optimizers among the many possible minimizers of the NTP objective. By framing NTP as cross-entropy minimization across \\emph{distinct} contexts, each tied with a \\emph{sparse} conditional probability distribution across a finite vocabulary of tokens, we introduce ``NTP-separability conditions'' that enable reaching the data-entropy lower bound. With this setup, and focusing on linear models with fixed context embeddings, we characterize the optimization bias of gradient descent (GD): Within the data subspace defined by the sparsity patterns of distinct contexts, GD selects parameters that equate the logits' differences of in-support tokens to their log-odds. In the orthogonal subspace, the GD parameters diverge in norm and select the direction that maximizes a margin specific to NTP. These findings extend previous research on implicit bias in one-hot classification to the NTP setting, highlighting key differences and prompting further research into the optimization and generalization properties of NTP, irrespective of the specific architecture used to generate the context embeddings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christos Thrampoulidis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/285b06e0dd856f20591b0a5beb954151-Abstract-Conference.html": {
    "title": "Not so griddy: Internal representations of RNNs path integrating more than one agent",
    "volume": "main",
    "abstract": "Success in collaborative and competitive environments, where agents must work with or against each other, requires individuals to encode the position and trajectory of themselves and others. Decades of neurophysiological experiments have shed light on how brain regions [e.g., medial entorhinal cortex (MEC), hippocampus] encode the self's position and trajectory. However, it has only recently been discovered that MEC and hippocampus are modulated by the positions and trajectories of others. To understand how encoding spatial information of multiple agents shapes neural representations, we train a recurrent neural network (RNN) model that captures properties of MEC to path integrate trajectories of two agents simultaneously navigating the same environment. We find significant differences between these RNNs and those trained to path integrate only a single agent. At the individual unit level, RNNs trained to path integrate more than one agent develop weaker grid responses, stronger border responses, and tuning for the relative position of the two agents. At the population level, they develop more distributed and robust representations, with changes in network dynamics and manifold topology. Our results provide testable predictions and open new directions with which to study the neural computations supporting spatial navigation",
    "checked": true,
    "id": "33eaf224d51e8bd6d1d2d2ad1fd499bd635eecac",
    "semantic_title": "not so griddy: internal representations of rnns path integrating more than one agent",
    "citation_count": 2,
    "authors": [
      "William Redman",
      "Francisco Acosta",
      "Santiago Acosta-Mendoza",
      "Nina Miolane"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/285cf10c8c6153d66b8cd6a3ab0d69ce-Abstract-Conference.html": {
    "title": "Out-Of-Distribution Detection with Diversification (Provably)",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) detection is crucial for ensuring reliable deployment of machine learning models. Recent advancements focus on utilizing easily accessible auxiliary outliers (e.g., data from the web or other datasets) in training. However, we experimentally reveal that these methods still struggle to generalize their detection capabilities to unknown OOD data, due to the limited diversity of the auxiliary outliers collected. Therefore, we thoroughly examine this problem from the generalization perspective and demonstrate that a more diverse set of auxiliary outliers is essential for enhancing the detection capabilities. However, in practice, it is difficult and costly to collect sufficiently diverse auxiliary outlier data. Therefore, we propose a simple yet practical approach with a theoretical guarantee, termed Diversity-induced Mixup for OOD detection (diverseMix), which enhances the diversity of auxiliary outlier set for training in an efficient way. Extensive experiments show that diverseMix achieves superior performance on commonly used and recent challenging large-scale benchmarks, which further confirm the importance of the diversity of auxiliary outliers",
    "checked": true,
    "id": "565d5a9038154fbbcba3d4a6f17671af9515fbcc",
    "semantic_title": "out-of-distribution detection with diversification (provably)",
    "citation_count": 5,
    "authors": [
      "Haiyun Yao",
      "Zongbo Han",
      "Huazhu Fu",
      "Xi Peng",
      "Qinghua Hu",
      "Changqing Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/286d67ff96f99c614f75dbcfb72a3e5f-Abstract-Conference.html": {
    "title": "Breaking Determinism: Fuzzy Modeling of Sequential Recommendation Using Discrete State Space Diffusion Model",
    "volume": "main",
    "abstract": "Sequential recommendation (SR) aims to predict items that users may be interested in based on their historical behavior sequences. We revisit SR from a novel information-theoretic perspective and find that conventional sequential modeling methods fail to adequately capture the randomness and unpredictability of user behavior. Inspired by fuzzy information processing theory, this paper introduces the DDSR model, which uses fuzzy sets of interaction sequences to overcome the limitations and better capture the evolution of users' real interests. Formally based on diffusion transition processes in discrete state spaces, which is unlike common diffusion models such as DDPM that operate in continuous domains. It is better suited for discrete data, using structured transitions instead of arbitrary noise introduction to avoid information loss. Additionally, to address the inefficiency of matrix transformations due to the vast discrete space, we use semantic labels derived from quantization or RQ-VAE to replace item IDs, enhancing efficiency and improving cold start issues. Testing on three public benchmark datasets shows that DDSR outperforms existing state-of-the-art methods in various settings, demonstrating its potential and effectiveness in handling SR tasks",
    "checked": true,
    "id": "30b4b98e169170ec45ca21be48ac707e3f9228c7",
    "semantic_title": "breaking determinism: fuzzy modeling of sequential recommendation using discrete state space diffusion model",
    "citation_count": 7,
    "authors": [
      "Wenjia Xie",
      "Hao Wang",
      "Luankang Zhang",
      "Rui Zhou",
      "Defu Lian",
      "Enhong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/286e7ab0ce6a68282394c92361c27b57-Abstract-Conference.html": {
    "title": "QVAE-Mole: The Quantum VAE with Spherical Latent Variable Learning for 3-D Molecule Generation",
    "volume": "main",
    "abstract": "Molecule generation ideally in its 3-D form has enjoyed wide applications in material, chemistry, life science, etc. We propose the first quantum parametric circuit for 3-D molecule generation for its potential quantum advantage especially considering the arrival of Noisy Intermediate-Scale Quantum (NISQ) era. We choose the Variational AutoEncoder (VAE) scheme for its simplicity and one-shot generation ability, which we believe is more quantum-friendly compared with the auto-regressive generative models or diffusion models as used in classic approaches. Specifically, we present a quantum encoding scheme designed for 3-D molecules with qubits complexity $\\mathcal{O}(C\\log n)$ ($n$ is the number of atoms) and adopt a von Mises-Fisher (vMF) distributed latent space to meet the inherent coherence of the quantum system. We further design to encode conditions into quantum circuits for property-specified generation. Experimentally, our model could generate plausible 3-D molecules and achieve competitive quantitative performance with significantly reduced circuit parameters compared with their classic counterparts. The source code will be released upon publication",
    "checked": true,
    "id": "29b65d0638b0953c9e3e9bc648e8e4fd93b5abcc",
    "semantic_title": "qvae-mole: the quantum vae with spherical latent variable learning for 3-d molecule generation",
    "citation_count": 5,
    "authors": [
      "Huaijin Wu",
      "Xinyu Ye",
      "Junchi Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/28795419a644f41ede3fa058b13fc622-Abstract-Conference.html": {
    "title": "Sample Complexity Reduction via Policy Difference Estimation in Tabular Reinforcement Learning",
    "volume": "main",
    "abstract": "In this paper, we study the non-asymptotic sample complexity for the pure exploration problem in contextual bandits and tabular reinforcement learning (RL): identifying an $\\epsilon$-optimal policy from a set of policies $\\Pi$ with high probability. Existing work in bandits has shown that it is possible to identify the best policy by estimating only the *difference* between the behaviors of individual policies–which can have substantially lower variance than estimating the behavior of each policy directly—yet the best-known complexities in RL fail to take advantage of this, and instead estimate the behavior of each policy directly. Does it suffice to estimate only the differences in the behaviors of policies in RL? We answer this question positively for contextual bandits, but in the negative for tabular RL, showing a separation between contextual bandits and RL. However, inspired by this, we show that it *almost* suffices to estimate only the differences in RL: if we can estimate the behavior of a *single* reference policy, it suffices to only estimate how any other policy deviates from this reference policy. We develop an algorithm which instantiates this principle and obtains, to the best of our knowledge, the tightest known bound on the sample complexity of tabular RL",
    "checked": true,
    "id": "54cf7928e1e3a14e4a404101254767f477c2bf84",
    "semantic_title": "sample complexity reduction via policy difference estimation in tabular reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Adhyyan Narang",
      "Andrew Wagenmaker",
      "Lillian Ratliff",
      "Kevin G. Jamieson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/288b63aa98084366c4536ba0574a0f22-Abstract-Conference.html": {
    "title": "Decision Mamba: A Multi-Grained State Space Model with Self-Evolution Regularization for Offline RL",
    "volume": "main",
    "abstract": "While the conditional sequence modeling with the transformer architecture has demonstrated its effectiveness in dealing with offline reinforcement learning (RL) tasks, it is struggle to handle out-of-distribution states and actions.Existing work attempts to address this issue by data augmentation with the learned policy or adding extra constraints with the value-based RL algorithm. However, these studies still fail to overcome the following challenges: (1) insufficiently utilizing the historical temporal information among inter-steps, (2) overlooking the local intra-step relationships among return-to-gos (RTGs), states, and actions, (3) overfitting suboptimal trajectories with noisy labels. To address these challenges, we propose $\\textbf{D}$ecision $\\textbf{M}$amba ($\\textbf{DM}$), a novel multi-grained state space model (SSM) with a self-evolving policy learning strategy.DM explicitly models the historical hidden state to extract the temporal information by using the mamba architecture. To capture the relationship among RTG-state-action triplets, a fine-grained SSM module is designed and integrated into the original coarse-grained SSM in mamba, resulting in a novel mamba architecture tailored for offline RL. Finally, to mitigate the overfitting issue on noisy trajectories, a self-evolving policy is proposed by using progressive regularization. The policy evolves by using its own past knowledge to refine the suboptimal actions, thus enhancing its robustness on noisy demonstrations. Extensive experiments on various tasks show that DM outperforms other baselines substantially",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Lv",
      "Xiang Deng",
      "Gongwei Chen",
      "MICHAEL YU WANG",
      "Liqiang Nie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2897c21877ff242e9831ec112c95f6bb-Abstract-Conference.html": {
    "title": "Distribution Learning with Valid Outputs Beyond the Worst-Case",
    "volume": "main",
    "abstract": "Generative models at times produce \"invalid\" outputs, such as images with generation artifacts and unnatural sounds. Validity-constrained distribution learning attempts to address this problem by requiring that the learned distribution have a provably small fraction of its mass in invalid parts of space -- something which standard loss minimization does not always ensure. To this end, a learner in this model can guide the learning via \"validity queries\", which allow it to ascertain the validity of individual examples. Prior work on this problem takes a worst-case stance, showing that proper learning requires an exponential number of validity queries, and demonstrating an improper algorithm which -- while generating guarantees in a wide-range of settings -- makes a relatively large polynomial number of validity queries. In this work, we take a first step towards characterizing regimes where guaranteeing validity is easier than in the worst-case. We show that when the data distribution lies in the model class and the log-loss is minimized, the number samples required to ensure validity has a weak dependence on the validity requirement. Additionally, we show that when the validity region belongs to a VC-class, a limited number of validity queries are often sufficient",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Rittler",
      "Kamalika Chaudhuri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/28997f1826c70bbe4024b62aa28e838f-Abstract-Conference.html": {
    "title": "FairQueue: Rethinking Prompt Learning for Fair Text-to-Image Generation",
    "volume": "main",
    "abstract": "Recently, prompt learning has emerged as the state-of-the-art (SOTA) for fair text-to-image (T2I) generation. Specifically, this approach leverages readily available reference images to learn inclusive prompts for each target Sensitive Attribute (tSA), allowing for fair image generation. In this work, we first reveal that this prompt learning-based approach results in degraded sample quality. Our analysis shows that the approach's training objective--which aims to align the embedding differences of learned prompts and reference images-- could be sub-optimal, resulting in distortion of the learned prompts and degraded generated images. To further substantiate this claim, as our major contribution, we deep dive into the denoising subnetwork of the T2I model to track down the effect of these learned prompts by analyzing the cross-attention maps. In our analysis, we propose a novel prompt switching analysis: I2H and H2I. Furthermore, we propose new quantitative characterization of cross-attention maps. Our analysis reveals abnormalities in the early denoising steps, perpetuating improper global structure that results in degradation in the generated samples. Building on insights from our analysis, we propose two ideas: (i) Prompt Queuing and (ii) Attention Amplification to address the quality issue. Extensive experimental results on a wide range of tSAs show that our proposed method outperforms SOTA approach's image generation quality, while achieving competitive fairness. More resources at FairQueue Project site: https://sutd-visual-computing-group.github.io/FairQueue",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Teo",
      "Milad Abdollahzadeh",
      "Xinda Ma",
      "Ngai-Man (Man) Cheung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/28ab418242603e0f7323e54185d19bde-Abstract-Conference.html": {
    "title": "SnapKV: LLM Knows What You are Looking for Before Generation",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications.We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an `observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications",
    "checked": true,
    "id": "1784c987e681d60c634765fe64c8d9c26f73d5ff",
    "semantic_title": "snapkv: llm knows what you are looking for before generation",
    "citation_count": 244,
    "authors": [
      "Yuhong Li",
      "Yingbing Huang",
      "Bowen Yang",
      "Bharat Venkitesh",
      "Acyr Locatelli",
      "Hanchen Ye",
      "Tianle Cai",
      "Patrick Lewis",
      "Deming Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/28d38c036365420f61ce03300418e44a-Abstract-Conference.html": {
    "title": "Boosting the Potential of Large Language Models with an Intelligent Information Assistant",
    "volume": "main",
    "abstract": "The emergence of Large Language Models (LLMs) has significantly advanced natural language processing, but these models often generate factually incorrect information, known as \"hallucination.\" Initial retrieval-augmented generation (RAG) methods like the \"Retrieve-Read\" framework was inadequate for complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised Fine-Tuning (SFT) methods improved performance but required frequent retraining and risked altering foundational LLM capabilities. To cope with these challenges, we propose Assistant-based Retrieval-Augmented Generation (AssistRAG), integrating an intelligent information assistant within LLMs. This assistant manages memory and knowledge through tool usage, action execution, memory building, and plan specification. Using a two-phase training approach—Curriculum Assistant Learning and Reinforced Preference Optimization—AssistRAG enhances information retrieval and decision-making. Experiments show AssistRAG significantly outperforms benchmarks, especially benefiting less advanced LLMs, by providing superior reasoning capabilities and accurate responses",
    "checked": false,
    "id": "4e112479c28ed33fd291a672d539b098d6684f00",
    "semantic_title": "assistrag: boosting the potential of large language models with an intelligent information assistant",
    "citation_count": 2,
    "authors": [
      "Yujia Zhou",
      "Zheng Liu",
      "Zhicheng Dou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/28ef7ee7cd3e03093acc39e1272411b7-Abstract-Conference.html": {
    "title": "Parallel Backpropagation for Shared-Feature Visualization",
    "volume": "main",
    "abstract": "High-level visual brain regions contain subareas in which neurons appear to respond more strongly to examples of a particular semantic category, like faces or bodies, rather than objects. However, recent work has shown that while this finding holds on average, some out-of-category stimuli also activate neurons in these regions. This may be due to visual features common among the preferred class also being present in other images. Here, we propose a deep-learning-based approach for visualizing these features. For each neuron, we identify relevant visual features driving its selectivity by modelling responses to images based on latent activations of a deep neural network. Given an out-of-category image which strongly activates the neuron, our method first identifies a reference image from the preferred category yielding a similar feature activation pattern. We then backpropagate latent activations of both images to the pixel level, while enhancing the identified shared dimensions and attenuating non-shared features. The procedure highlights image regions containing shared features driving responses of the model neuron. We apply the algorithm to novel recordings from body-selective regions in macaque IT cortex in order to understand why some images of objects excite these neurons. Visualizations reveal object parts which resemble parts of a macaque body, shedding light on neural preference of these objects",
    "checked": true,
    "id": "6db14583d8d0ca7f66daf7a89452e5f7967b3673",
    "semantic_title": "parallel backpropagation for shared-feature visualization",
    "citation_count": 1,
    "authors": [
      "Alexander Lappe",
      "Anna Bognár",
      "Ghazaleh Ghamkahri Nejad",
      "Albert Mukovskiy",
      "Lucas Martini",
      "Martin A. Giese",
      "Rufin Vogels"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/290141d6bfd7ea4d3f4483d126609bf6-Abstract-Conference.html": {
    "title": "A two-scale Complexity Measure for Deep Learning Models",
    "volume": "main",
    "abstract": "We introduce a novel capacity measure 2sED for statistical models based on the effective dimension. The new quantity provably bounds the generalization error under mild assumptions on the model. Furthermore, simulations on standard data sets and popular model architectures show that 2sED correlates well with the training error. For Markovian models, we show how to efficiently approximate 2sED from below through a layerwise iterative approach, which allows us to tackle deep learning models with a large number of parameters. Simulation results suggest that the approximation is good for different prominent models and data sets",
    "checked": true,
    "id": "5fa433e4840ec00ec560966a4643e9f478960a1b",
    "semantic_title": "a two-scale complexity measure for deep learning models",
    "citation_count": 2,
    "authors": [
      "Massimiliano Datres",
      "Gian Leonardi",
      "Alessio Figalli",
      "David Sutter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/29021b06afa4c648ee438584f7ef3e7e-Abstract-Conference.html": {
    "title": "Communication-Efficient Federated Group Distributionally Robust Optimization",
    "volume": "main",
    "abstract": "Federated learning faces challenges due to the heterogeneity in data volumes and distributions at different clients, which can compromise model generalization ability to various distributions. Existing approaches to address this issue based on group distributionally robust optimization (GDRO) often lead to high communication and sample complexity.To this end, this work introduces algorithms tailored for communication-efficient Federated Group Distributionally Robust Optimization (FGDRO). Our contributions are threefold: Firstly, we introduce the FGDRO-CVaR algorithm, which optimizes the average top-K losses while reducing communication complexity to $O(1/\\epsilon^4)$, where $\\epsilon$ denotes the desired precision level. Secondly, our FGDRO-KL algorithm is crafted to optimize KL regularized FGDRO, cutting communication complexity to $O(1/\\epsilon^3)$. Lastly, we propose FGDRO-KL-Adam to utilize Adam-type local updates in FGDRO-KL, which not only maintains a communication cost of $O(1/\\epsilon^3)$ but also shows potential to surpass SGD-type local steps in practical applications.The effectiveness of our algorithms has been demonstrated on a variety of real-world tasks, including natural language processing and computer vision",
    "checked": true,
    "id": "aded40384d341b5b7d153596a0ed3404757239b8",
    "semantic_title": "communication-efficient federated group distributionally robust optimization",
    "citation_count": 1,
    "authors": [
      "Zhishuai Guo",
      "Tianbao Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/290848c892a85a6936b473d4daa6d0ad-Abstract-Conference.html": {
    "title": "On Differentially Private U Statistics",
    "volume": "main",
    "abstract": "We consider the problem of privately estimating a parameter $\\mathbb{E}[h(X_1,\\dots,X_k)]$, where $X_1$, $X_2$, $\\dots$, $X_k$ are i.i.d. data from some distribution and $h$ is a permutation-invariant function. Without privacy constraints, the standard estimators for this task are U-statistics, which commonly arise in a wide range of problems, including nonparametric signed rank tests, symmetry testing, uniformity testing, and subgraph counts in random networks, and are the unique minimum variance unbiased estimators under mild conditions. Despite the recent outpouring of interest in private mean estimation, privatizing U-statistics has received little attention. While existing private mean estimation algorithms can be applied in a black-box manner to obtain confidence intervals, we show that they can lead to suboptimal private error, e.g., constant-factor inflation in the leading term, or even $\\Theta(1/n)$ rather than $O(1/n^2)$ in degenerate settings. To remedy this, we propose a new thresholding-based approach that reweights different subsets of the data using _local Hájek projections_. This leads to nearly optimal private error for non-degenerate U-statistics and a strong indication of near-optimality for degenerate U-statistics",
    "checked": true,
    "id": "993e2fc2d5e64eca295d3ea829d314fb1e937924",
    "semantic_title": "on differentially private u statistics",
    "citation_count": 1,
    "authors": [
      "Kamalika Chaudhuri",
      "Po-Ling Loh",
      "Shourya Pandey",
      "Purnamrita Sarkar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/29219efdb96e8164c589b4a0124451b7-Abstract-Conference.html": {
    "title": "Sketched Lanczos uncertainty score: a low-memory summary of the Fisher information",
    "volume": "main",
    "abstract": "Current uncertainty quantification is memory and compute expensive, which hinders practical uptake. To counter, we develop Sketched Lanczos Uncertainty (SLU): an architecture-agnostic uncertainty score that can be applied to pre-trained neural networks with minimal overhead. Importantly, the memory use of SLU only grows logarithmically with the number of model parameters. We combine Lanczos' algorithm with dimensionality reduction techniques to compute a sketch of the leading eigenvectors of a matrix. Applying this novel algorithm to the Fisher information matrix yields a cheap and reliable uncertainty score. Empirically, SLU yields well-calibrated uncertainties, reliably detects out-of-distribution examples, and consistently outperforms existing methods in the low-memory regime",
    "checked": true,
    "id": "90104dbb5a0c8b5ed5b396d3b01440538298f311",
    "semantic_title": "sketched lanczos uncertainty score: a low-memory summary of the fisher information",
    "citation_count": 2,
    "authors": [
      "Marco Miani",
      "Lorenzo Beretta",
      "Søren Hauberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/29416b66c2149872b9d1415a3fd2c5e0-Abstract-Conference.html": {
    "title": "Unified Generative and Discriminative Training for Multi-modal Large Language Models",
    "volume": "main",
    "abstract": "In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms. Generative training has enabled Multimodal Large Language Models (MLLMs) to tackle various complex tasks, yet issues such as hallucinations and weak object discrimination persist. Discriminative training, exemplified by models like CLIP, excels in zero-shot image-text classification and retrieval, yet struggles with complex scenarios requiring fine-grained semantic differentiation. This paper addresses these challenges by proposing a unified approach that integrates the strengths of both paradigms. Considering interleaved image-text sequences as the general format of input samples, we introduce a structure-induced training strategy that imposes semantic relationships between input samples and the MLLM's hidden state. This approach enhances the MLLM's ability to capture global semantics and distinguish fine-grained semantics. By leveraging dynamic sequence alignment within the Dynamic Time Warping framework and integrating a novel kernel for fine-grained semantic differentiation, our method effectively balances generative and discriminative tasks. Extensive experiments demonstrate the effectiveness of our approach, achieving state-of-the-art results in multiple generative tasks, especially those requiring cognitive and discrimination abilities. Additionally, our method surpasses discriminative benchmarks in interleaved and fine-grained retrieval tasks. By employing a retrieval-augmented generation strategy, our approach further enhances performance in some generative tasks within one model, offering a promising direction for future research in vision-language modeling",
    "checked": true,
    "id": "5cdb81f9742ae64370681356c1fa5fa0d8a974f6",
    "semantic_title": "unified generative and discriminative training for multi-modal large language models",
    "citation_count": 7,
    "authors": [
      "Wei Chow",
      "Juncheng Li",
      "Qifan Yu",
      "Kaihang Pan",
      "Hao Fei",
      "Zhiqi Ge",
      "Shuaiyang",
      "Siliang Tang",
      "Hanwang Zhang",
      "QIANRU SUN"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/29496c942ed6e08ecc469f4521ebfff0-Abstract-Conference.html": {
    "title": "Why Do We Need Weight Decay in Modern Deep Learning?",
    "volume": "main",
    "abstract": "Weight decay is a broadly used technique for training state-of-the-art deep networks from image classification to large language models. Despite its widespread usage and being extensively studied in the classical literature, its role remains poorly understood for deep learning. In this work, we highlight that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For deep networks on vision tasks trained with multipass SGD, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the loss stabilization mechanism. In contrast, for large language models trained with nearly one-epoch training, we describe how weight decay balances the bias-variance tradeoff in stochastic optimization leading to lower training loss and improved training stability. Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight decay is never useful as an explicit regularizer but instead changes the training dynamics in a desirable way",
    "checked": true,
    "id": "2f1df8c0806d5756929a09aba60ac7f43778a938",
    "semantic_title": "why do we need weight decay in modern deep learning?",
    "citation_count": 40,
    "authors": [
      "Francesco D&#x27;Angelo",
      "Maksym Andriushchenko",
      "Aditya Vardhan Varre",
      "Nicolas Flammarion"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/29571f8fda54fe93631c41aad4215abc-Abstract-Conference.html": {
    "title": "Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization",
    "volume": "main",
    "abstract": "Recent research indicates that large language models (LLMs) are susceptible to jailbreaking attacks that can generate harmful content. This paper introduces a novel token-level attack method, Adaptive Dense-to-Sparse Constrained Optimization (ADC), which has been shown to successfully jailbreak multiple open-source LLMs. Drawing inspiration from the difficulties of discrete token optimization, our method relaxes the discrete jailbreak optimization into a continuous optimization process while gradually increasing the sparsity of the optimizing vectors. This technique effectively bridges the gap between discrete and continuous space optimization. Experimental results demonstrate that our method is more effective and efficient than state-of-the-art token-level methods. On Harmbench, our approach achieves the highest attack success rate on seven out of eight LLMs compared to the latest jailbreak methods. \\textcolor{red}{Trigger Warning: This paper contains model behavior that can be offensive in nature.}",
    "checked": true,
    "id": "93fc14b69d7aa20ab7af0dda89b58810dbd32f54",
    "semantic_title": "efficient llm jailbreak via adaptive dense-to-sparse constrained optimization",
    "citation_count": 12,
    "authors": [
      "Kai Hu",
      "Weichen Yu",
      "Yining Li",
      "Tianjun Yao",
      "Xiang Li",
      "Wenhe Liu",
      "Lijun Yu",
      "Zhiqiang Shen",
      "Kai Chen",
      "Matt Fredrikson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2974844555dc383ea16c5f35833c7a57-Abstract-Conference.html": {
    "title": "On conditional diffusion models for PDE simulations",
    "volume": "main",
    "abstract": "Modelling partial differential equations (PDEs) is of crucial importance in science and engineering, and it includes tasks ranging from forecasting to inverse problems, such as data assimilation. However, most previous numerical and machine learning approaches that target forecasting cannot be applied out-of-the-box for data assimilation. Recently, diffusion models have emerged as a powerful tool for conditional generation, being able to flexibly incorporate observations without retraining. In this work, we perform a comparative study of score-based diffusion models for forecasting and assimilation of sparse observations. In particular, we focus on diffusion models that are either trained in a conditional manner, or conditioned after unconditional training. We address the shortcomings of existing models by proposing 1) an autoregressive sampling approach, that significantly improves performance in forecasting, 2) a new training strategy for conditional score-based models that achieves stable performance over a range of history lengths, and 3) a hybrid model which employs flexible pre-training conditioning on initial conditions and flexible post-training conditioning to handle data assimilation. We empirically show that these modifications are crucial for successfully tackling the combination of forecasting and data assimilation, a task commonly encountered in real-world scenarios",
    "checked": true,
    "id": "38cb9e1786280dc517557855641323e746534611",
    "semantic_title": "on conditional diffusion models for pde simulations",
    "citation_count": 18,
    "authors": [
      "Aliaksandra Shysheya",
      "Cristiana Diaconu",
      "Federico Bergamin",
      "Paris Perdikaris",
      "José Miguel Hernández-Lobato",
      "Richard Turner",
      "Emile Mathieu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/29753d93c5fc11167567e5df800308ae-Abstract-Conference.html": {
    "title": "Detecting Brittle Decisions for Free: Leveraging Margin Consistency in Deep Robust Classifiers",
    "volume": "main",
    "abstract": "Despite extensive research on adversarial training strategies to improve robustness, the decisions of even the most robust deep learning models can still be quite sensitive to imperceptible perturbations, creating serious risks when deploying them for high-stakes real-world applications. While detecting such cases may be critical, evaluating a model's vulnerability at a per-instance level using adversarial attacks is computationally too intensive and unsuitable for real-time deployment scenarios. The input space margin is the exact score to detect non-robust samples and is intractable for deep neural networks. This paper introduces the concept of margin consistency -- a property that links the input space margins and the logit margins in robust models -- for efficient detection of vulnerable samples. First, we establish that margin consistency is a necessary and sufficient condition to use a model's logit margin as a score for identifying non-robust samples. Next, through comprehensive empirical analysis of various robustly trained models on CIFAR10 and CIFAR100 datasets, we show that they indicate high margin consistency with a strong correlation between their input space margins and the logit margins. Then, we show that we can effectively use the logit margin to confidently detect brittle decisions with such models. Finally, we address cases where the model is not sufficiently margin-consistent by learning a pseudo-margin from the feature representation. Our findings highlight the potential of leveraging deep representations to efficiently assess adversarial vulnerability in deployment scenarios",
    "checked": true,
    "id": "cdf9a412743efe4f2be748d7dcb100144e9e4dcf",
    "semantic_title": "detecting brittle decisions for free: leveraging margin consistency in deep robust classifiers",
    "citation_count": 0,
    "authors": [
      "JONAS NGNAWE",
      "Sabyasachi Sahoo",
      "Yann Pequignot",
      "Frederic Precioso",
      "Christian Gagné"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/298c3e32d7d402189444be2ff5d19979-Abstract-Conference.html": {
    "title": "Generalizing Weather Forecast to Fine-grained Temporal Scales via Physics-AI Hybrid Modeling",
    "volume": "main",
    "abstract": "Data-driven artificial intelligence (AI) models have made significant advancements in weather forecasting, particularly in medium-range and nowcasting. However, most data-driven weather forecasting models are black-box systems that focus on learning data mapping rather than fine-grained physical evolution in the time dimension. Consequently, the limitations in the temporal scale of datasets prevent these models from forecasting at finer time scales. This paper proposes a physics-AI hybrid model (i.e., WeatherGFT) which generalizes weather forecasts to finer-grained temporal scales beyond training dataset. Specifically, we employ a carefully designed PDE kernel to simulate physical evolution on a small time scale (e.g., 300 seconds) and use a parallel neural networks with a learnable router for bias correction. Furthermore, we introduce a lead time-aware training framework to promote the generalization of the model at different lead times. The weight analysis of physics-AI modules indicates that physics conducts major evolution while AI performs corrections adaptively. Extensive experiments show that WeatherGFT trained on an hourly dataset, effectively generalizes forecasts across multiple time scales, including 30-minute, which is even smaller than the dataset's temporal resolution",
    "checked": true,
    "id": "67afa64612fc21f160f8ca75653ac8f44436463b",
    "semantic_title": "generalizing weather forecast to fine-grained temporal scales via physics-ai hybrid modeling",
    "citation_count": 9,
    "authors": [
      "Wanghan Xu",
      "Fenghua Ling",
      "zhangwenlong",
      "Tao Han",
      "Hao Chen",
      "Wanli Ouyang",
      "LEI BAI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/29a0ea49a103a233b17c0705cdeccb66-Abstract-Conference.html": {
    "title": "Cross-Modality Perturbation Synergy Attack for Person Re-identification",
    "volume": "main",
    "abstract": "In recent years, there has been significant research focusing on addressing security concerns in single-modal person re-identification (ReID) systems that are based on RGB images. However, the safety of cross-modality scenarios, which are more commonly encountered in practical applications involving images captured by infrared cameras, has not received adequate attention. The main challenge in cross-modality ReID lies in effectively dealing with visual differences between different modalities. For instance, infrared images are typically grayscale, unlike visible images that contain color information. Existing attack methods have primarily focused on the characteristics of the visible image modality, overlooking the features of other modalities and the variations in data distribution among different modalities. This oversight can potentially undermine the effectiveness of these methods in image retrieval across diverse modalities. This study represents the first exploration into the security of cross-modality ReID models and proposes a universal perturbation attack specifically designed for cross-modality ReID. This attack optimizes perturbations by leveraging gradients from diverse modality data, thereby disrupting the discriminator and reinforcing the differences between modalities. We conducted experiments on three widely used cross-modality datasets, namely RegDB, SYSU, and LLCM. The results not only demonstrate the effectiveness of our method but also provide insights for future improvements in the robustness of cross-modality ReID systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunpeng Gong",
      "Zhun Zhong",
      "Yansong Qu",
      "Zhiming Luo",
      "Rongrong Ji",
      "Min JIANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/29c294ddc628c94cd2c636383ef106c1-Abstract-Conference.html": {
    "title": "NeuroBOLT: Resting-state EEG-to-fMRI Synthesis with Multi-dimensional Feature Mapping",
    "volume": "main",
    "abstract": "Functional magnetic resonance imaging (fMRI) is an indispensable tool in modern neuroscience, providing a non-invasive window into whole-brain dynamics at millimeter-scale spatial resolution. However, fMRI is constrained by issues such as high operation costs and immobility. With the rapid advancements in cross-modality synthesis and brain decoding, the use of deep neural networks has emerged as a promising solution for inferring whole-brain, high-resolution fMRI features directly from electroencephalography (EEG), a more widely accessible and portable neuroimaging modality. Nonetheless, the complex projection from neural activity to fMRI hemodynamic responses and the spatial ambiguity of EEG pose substantial challenges both in modeling and interpretability. Relatively few studies to date have developed approaches for EEG-fMRI translation, and although they have made significant strides, the inference of fMRI signals in a given study has been limited to a small set of brain areas and to a single condition (i.e., either resting-state or a specific task). The capability to predict fMRI signals in other brain areas, as well as to generalize across conditions, remain critical gaps in the field. To tackle these challenges, we introduce a novel and generalizable framework: NeuroBOLT, i.e., Neuro-to-BOLD Transformer, which leverages multi-dimensional representation learning from temporal, spatial, and spectral domains to translate raw EEG data to the corresponding fMRI activity signals across the brain. Our experiments demonstrate that NeuroBOLT effectively reconstructs unseen resting-state fMRI signals from primary sensory, high-level cognitive areas, and deep subcortical brain regions, achieving state-of-the-art accuracy with the potential to generalize across varying conditions and sites, which significantly advances the integration of these two modalities",
    "checked": true,
    "id": "0b40003971175fa78415f49a16320d7c2f3af9dd",
    "semantic_title": "neurobolt: resting-state eeg-to-fmri synthesis with multi-dimensional feature mapping",
    "citation_count": 7,
    "authors": [
      "Yamin Li",
      "Ange Lou",
      "Ziyuan Xu",
      "Shengchao Zhang",
      "Shiyu Wang",
      "Dario Englot",
      "Soheil Kolouri",
      "Daniel Moyer",
      "Roza Bayrak",
      "Catie Chang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/29c861b02308a57aec18990f0dfe3777-Abstract-Conference.html": {
    "title": "Fast Last-Iterate Convergence of Learning in Games Requires Forgetful Algorithms",
    "volume": "main",
    "abstract": "Self play via online learning is one of the premier ways to solve large-scale zero-sum games, both in theory and practice. Particularly popular algorithms include optimistic multiplicative weights update (OMWU) and optimistic gradient-descent-ascent (OGDA). While both algorithms enjoy $O(1/T)$ ergodic convergence to Nash equilibrium in two-player zero-sum games, OMWU offers several advantages, including logarithmic dependence on the size of the payoff matrix and $\\tilde{O}(1/T)$ convergence to coarse correlated equilibria even in general-sum games. However, in terms of last-iterate convergence in two-player zero-sum games, an increasingly popular topic in this area, OGDA guarantees that the duality gap shrinks at a rate of $(1/\\sqrt{T})$, while the best existing last-iterate convergence for OMWU depends on some game-dependent constant that could be arbitrarily large. This begs the question: is this potentially slow last-iterate convergence an inherent disadvantage of OMWU, or is the current analysis too loose? Somewhat surprisingly, we show that the former is true. More generally, we prove that a broad class of algorithms that do not forget the past quickly all suffer the same issue: for any arbitrarily small $\\delta>0$, there exists a $2\\times 2$ matrix game such that the algorithm admits a constant duality gap even after $1/\\delta$ rounds. This class of algorithms includes OMWU and other standard optimistic follow-the-regularized-leader algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Cai",
      "Gabriele Farina",
      "Julien Grand-Clément",
      "Christian Kroer",
      "Chung-Wei Lee",
      "Haipeng Luo",
      "Weiqiang Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/29c8c615b3187ee995029284702d3f43-Abstract-Conference.html": {
    "title": "Coherent 3D Scene Diffusion From a Single RGB Image",
    "volume": "main",
    "abstract": "We present a novel diffusion-based approach for coherent 3D scene reconstruction from a single RGB image. Our method utilizes an image-conditioned 3D scene diffusion model to simultaneously denoise the 3D poses and geometries of all objects within the scene.Motivated by the ill-posed nature of the task and to obtain consistent scene reconstruction results, we learn a generative scene prior by conditioning on all scene objects simultaneously to capture scene context and by allowing the model to learn inter-object relationships throughout the diffusion process.We further propose an efficient surface alignment loss to facilitate training even in the absence of full ground-truth annotation, which is common in publicly available datasets. This loss leverages an expressive shape representation, which enables direct point sampling from intermediate shape predictions.By framing the task of single RGB image 3D scene reconstruction as a conditional diffusion process, our approach surpasses current state-of-the-art methods, achieving a 12.04\\% improvement in AP3D on SUN RGB-D and a 13.43\\% increase in F-Score on Pix3D",
    "checked": true,
    "id": "b4f67a14d44fb66f9ed77be3faffe7265f9f1e91",
    "semantic_title": "coherent 3d scene diffusion from a single rgb image",
    "citation_count": 0,
    "authors": [
      "Manuel Dahnert",
      "Angela Dai",
      "Norman Müller",
      "Matthias Niessner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/29cd7f8331d13ede6dc6d6ef3dfacb70-Abstract-Conference.html": {
    "title": "DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs",
    "volume": "main",
    "abstract": "Most large multimodal models (LMMs) are implemented by feeding visual tokens as a sequence into the first layer of a large language model (LLM). The resulting architecture is simple but significantly increases computation and memory costs, as it has to handle a large number of additional tokens in its input layer. This paper presents a new architecture *DeepStack* for LMMs. Considering $N$ layers in the language and vision transformer of LMMs, we stack the visual tokens into $N$ groups and feed each group to its aligned transformer layer from bottom to top. Surprisingly, this simple method greatly enhances the power of LMMs to model interactions among visual tokens across layers but with minimal additional cost. We apply *DeepStack* to both language and vision transformer in LMMs, and validate the effectiveness of *DeepStack* LMMs with extensive empirical results. Using the same context length, our DeepStack 7B and 13B parameters surpass their counterparts by 2.7 and 2.9 on average across 9 benchmarks, respectively. Using only one-fifth of the context length, DeepStack rivals closely to the counterparts that use the full context length. These gains are particularly pronounced on high-resolution tasks, *e.g.*, 4.2, 11.0, and 4.0 improvements on TextVQA, DocVQA, and InfoVQA compared to LLaVA-1.5-7B, respectively. We further apply *DeepStack* to vision transformer layers, which brings us a similar amount of improvements, 3.8 on average compared with LLaVA-1.5-7B",
    "checked": true,
    "id": "8c304d35f16295056a20bc8bf20a3c72f828827f",
    "semantic_title": "deepstack: deeply stacking visual tokens is surprisingly simple and effective for lmms",
    "citation_count": 13,
    "authors": [
      "Lingchen Meng",
      "Jianwei Yang",
      "Rui Tian",
      "Xiyang Dai",
      "Zuxuan Wu",
      "Jianfeng Gao",
      "Yu-Gang Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/29d319f7c1513c9ecd81d3a6e9632a6e-Abstract-Conference.html": {
    "title": "Neuro-Symbolic Data Generation for Math Reasoning",
    "volume": "main",
    "abstract": "A critical question about Large Language Models (LLMs) is whether their apparent deficiency in mathematical reasoning is inherent, or merely a result of insufficient exposure to high-quality mathematical data. To explore this, we developed an automated method for generating high-quality, supervised mathematical datasets. The method carefully mutates existing math problems, ensuring both diversity and validity of the newly generated problems. This is achieved by a neuro-symbolic data generation framework combining the intuitive informalization strengths of LLMs, and the precise symbolic reasoning of math solvers along with projected Markov chain Monte Carlo sampling in the highly-irregular symbolic space.Empirical experiments demonstrate the high quality of data generated by the proposed method, and that the LLMs, specifically LLaMA-2 and Mistral, when realigned with the generated data, surpass their state-of-the-art counterparts",
    "checked": true,
    "id": "31184f3bc8b7d7e1b29120ea57091847ada84996",
    "semantic_title": "neuro-symbolic data generation for math reasoning",
    "citation_count": 11,
    "authors": [
      "Zenan Li",
      "Zhi Zhou",
      "Yuan Yao",
      "Xian Zhang",
      "Yu-Feng Li",
      "Chun Cao",
      "Fan Yang",
      "Xiaoxing Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/29d4e09f060a95118762296d240b5e63-Abstract-Conference.html": {
    "title": "Consistency Diffusion Bridge Models",
    "volume": "main",
    "abstract": "Diffusion models (DMs) have become the dominant paradigm of generative modeling in a variety of domains by learning stochastic processes from noise to data. Recently, diffusion denoising bridge models (DDBMs), a new formulation of generative modeling that builds stochastic processes between fixed data endpoints based on a reference diffusion process, have achieved empirical success across tasks with coupled data distribution, such as image-to-image translation. However, DDBM's sampling process typically requires hundreds of network evaluations to achieve decent performance, which may impede their practical deployment due to high computational demands. In this work, inspired by the recent advance of consistency models in DMs, we tackle this problem by learning the consistency function of the probability-flow ordinary differential equation (PF-ODE) of DDBMs, which directly predicts the solution at a starting step given any point on the ODE trajectory. Based on a dedicated general-form ODE solver, we propose two paradigms: consistency bridge distillation and consistency bridge training, which is flexible to apply on DDBMs with broad design choices. Experimental results show that our proposed method could sample $4\\times$ to $50\\times$ faster than the base DDBM and produce better visual quality given the same step in various tasks with pixel resolution ranging from $64 \\times 64$ to $256 \\times 256$, as well as supporting downstream tasks such as semantic interpolation in the data space",
    "checked": true,
    "id": "09035f0d603b19feed64fb5a02e63745e1ba1781",
    "semantic_title": "consistency diffusion bridge models",
    "citation_count": 8,
    "authors": [
      "Guande He",
      "Kaiwen Zheng",
      "Jianfei Chen",
      "Fan Bao",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/29ff36c8fbed10819b2e50267862a52a-Abstract-Conference.html": {
    "title": "Dense Associative Memory Through the Lens of Random Features",
    "volume": "main",
    "abstract": "Dense Associative Memories are high storage capacity variants of the Hopfield networks that are capable of storing a large number of memory patterns in the weights of the network of a given size. Their common formulations typically require storing each pattern in a separate set of synaptic weights, which leads to the increase of the number of synaptic weights when new patterns are introduced. In this work we propose an alternative formulation of this class of models using random features, commonly used in kernel methods. In this formulation the number of network's parameters remains fixed. At the same time, new memories can be added to the network by modifying existing weights. We show that this novel network closely approximates the energy function and dynamics of conventional Dense Associative Memories and shares their desirable computational properties",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Hoover",
      "Duen Horng Chau",
      "Hendrik Strobelt",
      "Parikshit Ram",
      "Dmitry Krotov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2a02b560822d564119fe3ac3be024ac6-Abstract-Conference.html": {
    "title": "A Simple yet Universal Framework for Depth Completion",
    "volume": "main",
    "abstract": "Consistent depth estimation across diverse scenes and sensors is a crucial challenge in computer vision, especially when deploying machine learning models in the real world. Traditional methods depend heavily on extensive pixel-wise labeled data, which is costly and labor-intensive to acquire, and frequently have difficulty in scale issues on various depth sensors. In response, we define Universal Depth Completion (UniDC) problem. We also present a baseline architecture, a simple yet effective approach tailored to estimate scene depth across a wide range of sensors and environments using minimal labeled data. Our approach addresses two primary challenges: generalizable knowledge of unseen scene configurations and strong adaptation to arbitrary depth sensors with various specifications. To enhance versatility in the wild, we utilize a foundation model for monocular depth estimation that provides a comprehensive understanding of 3D structures in scenes. Additionally, for fast adaptation to off-the-shelf sensors, we generate a pixel-wise affinity map based on the knowledge from the foundation model. We then adjust depth information from arbitrary sensors to the monocular depth along with the constructed affinity. Furthermore, to boost up both the adaptability and generality, we embed the learned features into hyperbolic space, which builds implicit hierarchical structures of 3D data from fewer examples. Extensive experiments demonstrate the proposed method's superior generalization capabilities for UniDC problem over state-of-the-art depth completion. Source code is publicly available at https://github.com/JinhwiPark/UniDC",
    "checked": true,
    "id": "e8a53392fb22781a67d3752ab345e5221ad55508",
    "semantic_title": "a simple yet universal framework for depth completion",
    "citation_count": 7,
    "authors": [
      "Jin-Hwi Park",
      "Hae-Gon Jeon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2a07348a6a7b2c208ab5cb1ee0e78ab5-Abstract-Conference.html": {
    "title": "Identifying Equivalent Training Dynamics",
    "volume": "main",
    "abstract": "Study of the nonlinear evolution deep neural network (DNN) parameters undergo during training has uncovered regimes of distinct dynamical behavior. While a detailed understanding of these phenomena has the potential to advance improvements in training efficiency and robustness, the lack of methods for identifying when DNN models have equivalent dynamics limits the insight that can be gained from prior work. Topological conjugacy, a notion from dynamical systems theory, provides a precise definition of dynamical equivalence, offering a possible route to address this need. However, topological conjugacies have historically been challenging to compute. By leveraging advances in Koopman operator theory, we develop a framework for identifying conjugate and non-conjugate training dynamics. To validate our approach, we demonstrate that comparing Koopman eigenvalues can correctly identify a known equivalence between online mirror descent and online gradient descent. We then utilize our approach to: (a) identify non-conjugate training dynamics between shallow and wide fully connected neural networks; (b) characterize the early phase of training dynamics in convolutional neural networks; (c) uncover non-conjugate training dynamics in Transformers that do and do not undergo grokking. Our results, across a range of DNN architectures, illustrate the flexibility of our framework and highlight its potential for shedding new light on training dynamics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Redman",
      "Juan Bello-Rivas",
      "Maria Fonoberova",
      "Ryan Mohr",
      "Yannis Kevrekidis",
      "Igor Mezic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2a1e2162d17c4986934d7740255c0157-Abstract-Conference.html": {
    "title": "DRACO: A Denoising-Reconstruction Autoencoder for Cryo-EM",
    "volume": "main",
    "abstract": "Foundation models in computer vision have demonstrated exceptional performance in zero-shot and few-shot tasks by extracting multi-purpose features from large-scale datasets through self-supervised pre-training methods. However, these models often overlook the severe corruption in cryogenic electron microscopy (cryo-EM) images by high-level noises. We introduce DRACO, a Denoising-Reconstruction Autoencoder for CryO-EM, inspired by the Noise2Noise (N2N) approach. By processing cryo-EM movies into odd and even images and treating them as independent noisy observations, we apply a denoising-reconstruction hybrid training scheme. We mask both images to create denoising and reconstruction tasks. For DRACO's pre-training, the quality of the dataset is essential, we hence build a high-quality, diverse dataset from an uncurated public database, including over 270,000 movies or micrographs. After pre-training, DRACO naturally serves as a generalizable cryo-EM image denoiser and a foundation model for various cryo-EM downstream tasks. DRACO demonstrates the best performance in denoising, micrograph curation, and particle picking tasks compared to state-of-the-art baselines",
    "checked": true,
    "id": "9f729dfae10e7c54a37322b69f5140915b927fd3",
    "semantic_title": "draco: a denoising-reconstruction autoencoder for cryo-em",
    "citation_count": 3,
    "authors": [
      "YingJun Shen",
      "Haizhao Dai",
      "Qihe Chen",
      "Yan Zeng",
      "Jiakai Zhang",
      "Yuan Pei",
      "Jingyi Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2a54def490213ee10631b991c5acc6b5-Abstract-Conference.html": {
    "title": "Human-Object Interaction Detection Collaborated with Large Relation-driven Diffusion Models",
    "volume": "main",
    "abstract": "Prevalent human-object interaction (HOI) detection approaches typically leverage large-scale visual-linguistic models to help recognize events involving humans and objects. Though promising, models trained via contrastive learning on text-image pairs often neglect mid/low-level visual cues and struggle at compositional reasoning. In response, we introduce DIFFUSIONHOI, a new HOI detector shedding light on text-to-image diffusion models. Unlike the aforementioned models, diffusion models excel in discerning mid/low-level visual concepts as generative models, and possess strong compositionality to handle novel concepts expressed in text inputs. Considering diffusion models usually emphasize instance objects, we first devise an inversion-based strategy to learn the expression of relation patterns between humans and objects in embedding space. These learned relation embeddings then serve as textual prompts, to steer diffusion models generate images that depict specific interactions, and extract HOI-relevant cues from images without heavy finetuning. Benefited from above, DIFFUSIONHOI achieves SOTA performance on three datasets under both regular and zero-shot setups",
    "checked": true,
    "id": "b7d4bd93b2c542b96033e80817ec1e5b76a52111",
    "semantic_title": "human-object interaction detection collaborated with large relation-driven diffusion models",
    "citation_count": 12,
    "authors": [
      "Liulei Li",
      "Wenguan Wang",
      "Yi Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2a568a9a84577769d838793433c817d9-Abstract-Conference.html": {
    "title": "Ensemble sampling for linear bandits: small ensembles suffice",
    "volume": "main",
    "abstract": "We provide the first useful and rigorous analysis of ensemble sampling for the stochastic linear bandit setting. In particular, we show that, under standard assumptions, for a $d$-dimensional stochastic linear bandit with an interaction horizon $T$, ensemble sampling with an ensemble of size of order $\\smash{d \\log T}$ incurs regret at most of the order $\\smash{(d \\log T)^{5/2} \\sqrt{T}}$. Ours is the first result in any structured setting not to require the size of the ensemble to scale linearly with $T$---which defeats the purpose of ensemble sampling---while obtaining near $\\smash{\\sqrt{T}}$ order regret. Our result is also the first to allow for infinite action sets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Janz",
      "Alexander Litvak",
      "Csaba Szepesvari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2a5a41a536d3ada8fbf61a9d6fbf18d2-Abstract-Conference.html": {
    "title": "A Unifying Post-Processing Framework for Multi-Objective Learn-to-Defer Problems",
    "volume": "main",
    "abstract": "Learn-to-Defer is a paradigm that enables learning algorithms to work not in isolation but as a team with human experts. In this paradigm, we permit the system to defer a subset of its tasks to the expert. Although there are currently systems that follow this paradigm and are designed to optimize the accuracy of the final human-AI team, the general methodology for developing such systems under a set of constraints (e.g., algorithmic fairness, expert intervention budget, defer of anomaly, etc.) remains largely unexplored. In this paper, using a d-dimensional generalization to the fundamental lemma of Neyman and Pearson (d-GNP), we obtain the Bayes optimal solution for learn-to-defer systems under various constraints. Furthermore, we design a generalizable algorithm to estimate that solution and apply this algorithm to the COMPAS, Hatespeech, and ACSIncome datasets. Our algorithm shows improvements in terms of constraint violation over a set of learn-to-defer baselines and can control multiple constraint violations at once. The use of d-GNP is beyond learn-to-defer applications and can potentially obtain a solution to decision-making problems with a set of controlled expected performance measures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad-Amin Charusaie",
      "Samira Samadi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2a6fc6774932e7835349089b0287ed2c-Abstract-Conference.html": {
    "title": "If You Want to Be Robust, Be Wary of Initialization",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable performance across a spectrum of graph-related tasks, however concerns persist regarding their vulnerability to adversarial perturbations. While prevailing defense strategies focus primarily on pre-processing techniques and adaptive message-passing schemes, this study delves into an under-explored dimension: the impact of weight initialization and associated hyper-parameters, such as training epochs, on a model's robustness.We introduce a theoretical framework bridging the connection between initialization strategies and a network's resilience to adversarial perturbations. Our analysis reveals a direct relationship between initial weights, number of training epochs and the model's vulnerability, offering new insights into adversarial robustness beyond conventional defense mechanisms. While our primary focus is on GNNs, we extend our theoretical framework, providing a general upper-bound applicable to Deep Neural Networks.Extensive experiments, spanning diverse models and real-world datasets subjected to various adversarial attacks, validate our findings. We illustrate that selecting appropriate initialization not only ensures performance on clean datasets but also enhances model robustness against adversarial perturbations, with observed gaps of up to 50\\% compared to alternative initialization approaches",
    "checked": true,
    "id": "8668a8931bc768b2773cc7bf30a1f9f62aeb49d5",
    "semantic_title": "if you want to be robust, be wary of initialization",
    "citation_count": 0,
    "authors": [
      "Sofiane ENNADIR",
      "Johannes Lutzeyer",
      "Michalis Vazirgiannis",
      "El Houcine Bergou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2a7157c84dcf263f77b37d6c11d7d149-Abstract-Conference.html": {
    "title": "Is Score Matching Suitable for Estimating Point Processes?",
    "volume": "main",
    "abstract": "Score matching estimators for point processes have gained widespread attention in recent years because they do not require the calculation of intensity integrals, thereby effectively addressing the computational challenges in maximum likelihood estimation (MLE). Some existing works have proposed score matching estimators for point processes. However, this work demonstrates that the incompleteness of the estimators proposed in those works renders them applicable only to specific problems, and they fail for more general point processes. To address this issue, this work introduces the weighted score matching estimator to point processes. Theoretically, we prove the consistency of the estimator we propose. Experimental results indicate that our estimator accurately estimates model parameters on synthetic data and yields results consistent with MLE on real data. In contrast, existing score matching estimators fail to perform effectively. Codes are publicly available at \\url{https://github.com/KenCao2007/WSM_TPP}",
    "checked": true,
    "id": "817fb56ccb056d57303359006c935734a8be9c08",
    "semantic_title": "is score matching suitable for estimating point processes?",
    "citation_count": 2,
    "authors": [
      "Haoqun Cao",
      "Zizhuo Meng",
      "Tianjun Ke",
      "Feng Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2a79b96b0fc217afb317fbdb1c082639-Abstract-Conference.html": {
    "title": "Is Knowledge Power? On the (Im)possibility of Learning from Strategic Interactions",
    "volume": "main",
    "abstract": "When learning in strategic environments, a key question is whether agents can overcome uncertainty about their preferences to achieve outcomes they could have achieved absent any uncertainty. Can they do this solely through interactions with each other? We focus this question on the ability of agents to attain the value of their Stackelberg optimal strategy and study the impact of information asymmetry. We study repeated interactions in fully strategic environments where players' actions are decided based on learning algorithms that take into account their observed histories and knowledge of the game. We study the pure Nash equilibria (PNE) of a meta-game where players choose these algorithms as their actions. We demonstrate that if one player has perfect knowledge about the game, then any initial informational gap persists. That is, while there is always a PNE in which the informed agent achieves her Stackelberg value, there is a game where no PNE of the meta-game allows the partially informed player to achieve her Stackelberg value. On the other hand, if both players start with some uncertainty about the game, the quality of information alone does not determine which agent can achieve her Stackelberg value. In this case, the concept of information asymmetry becomes nuanced and depends on the game's structure. Overall, our findings suggest that repeated strategic interactions alone cannot facilitate learning effectively enough to earn an uninformed player her Stackelberg value",
    "checked": false,
    "id": "5946cab9d74f361aa1ad3cbf23f98aa33aa9e1d5",
    "semantic_title": "is knowledge power? on the (im)possibility of learning from strategic interaction",
    "citation_count": 5,
    "authors": [
      "Nivasini Ananthakrishnan",
      "Nika Haghtalab",
      "Chara Podimata",
      "Kunhe Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2a7e91c6e4b68325d9884a7469804837-Abstract-Conference.html": {
    "title": "Data Free Backdoor Attacks",
    "volume": "main",
    "abstract": "Backdoor attacks aim to inject a backdoor into a classifier such that it predicts any input with an attacker-chosen backdoor trigger as an attacker-chosen target class. Existing backdoor attacks require either retraining the classifier with some clean data or modifying the model's architecture.As a result, they are 1) not applicable when clean data is unavailable, 2) less efficient when the model is large, and 3) less stealthy due to architecture changes. In this work, we propose DFBA, a novel retraining-free and data-free backdoor attack without changing the model architecture. Technically, our proposed method modifies a few parameters of a classifier to inject a backdoor. Through theoretical analysis, we verify that our injected backdoor is provably undetectable and unremovable by various state-of-the-art defenses under mild assumptions. Our evaluation on multiple datasets further demonstrates that our injected backdoor: 1) incurs negligible classification loss, 2) achieves 100\\% attack success rates, and 3) bypasses six existing state-of-the-art defenses. Moreover, our comparison with a state-of-the-art non-data-free backdoor attack shows our attack is more stealthy and effective against various defenses while achieving less classification accuracy loss.We will release our code upon paper acceptance",
    "checked": true,
    "id": "c6cee7cb34d10056c6d62921781495e8f121cb72",
    "semantic_title": "data free backdoor attacks",
    "citation_count": 0,
    "authors": [
      "Bochuan Cao",
      "Jinyuan Jia",
      "Chuxuan Hu",
      "Wenbo Guo",
      "Zhen J. Xiang",
      "Jinghui Chen",
      "Bo Li",
      "Dawn Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2a952768bb85041f95ed06a5b60cf4d5-Abstract-Conference.html": {
    "title": "Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning",
    "volume": "main",
    "abstract": "Recently, vision model pre-training has evolved from relying on manually annotated datasets to leveraging large-scale, web-crawled image-text data. Despite these advances, there is no pre-training method that effectively exploits the interleaved image-text data, which is very prevalent on the Internet. Inspired by the recent success of compression learning in natural language processing, we propose a novel vision model pre-training method called Latent Compression Learning (LCL) for interleaved image-text data. This method performs latent compression learning by maximizing the mutual information between the inputs and outputs of a causal attention model. The training objective can be decomposed into two basic tasks: 1) contrastive learning between visual representation and preceding context, and 2) generating subsequent text based on visual representation. Our experiments demonstrate that our method not only matches the performance of CLIP on paired pre-training datasets (e.g., LAION), but can also leverage interleaved pre-training data (e.g., MMC4) to learn robust visual representations from scratch, showcasing the potential of vision model pre-training with interleaved image-text data",
    "checked": true,
    "id": "b7286301e8511dce2f5555c73719ba40fc62699a",
    "semantic_title": "vision model pre-training on interleaved image-text data via latent compression learning",
    "citation_count": 6,
    "authors": [
      "CHENYU YANG",
      "Xizhou Zhu",
      "Jinguo Zhu",
      "Weijie Su",
      "Junjie Wang",
      "Xuan Dong",
      "Wenhai Wang",
      "Bin Li",
      "Jie Zhou",
      "Yu Qiao",
      "Jifeng Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2aba6ec20299931d46ddeefd5ddcb442-Abstract-Conference.html": {
    "title": "Linear Causal Bandits: Unknown Graph and Soft Interventions",
    "volume": "main",
    "abstract": "Designing causal bandit algorithms depends on two central categories of assumptions: (i) the extent of information about the underlying causal graphs and (ii) the extent of information about interventional statistical models. There have been extensive recent advances in dispensing with assumptions on either category. These include assuming known graphs but unknown interventional distributions, and the converse setting of assuming unknown graphs but access to restrictive hard/$\\operatorname{do}$ interventions, which removes the stochasticity and ancestral dependencies. Nevertheless, the problem in its general form, i.e., _unknown_ graph and _unknown_ stochastic intervention models, remains open. This paper addresses this problem and establishes that in a graph with $N$ nodes, maximum in-degree $d$ and maximum causal path length $L$, after $T$ interaction rounds the regret upper bound scales as $\\tilde{\\mathcal{O}}((cd)^{L-\\frac{1}{2}}\\sqrt{T} + d + RN)$ where $c>1$ is a constant and $R$ is a measure of intervention power. A universal minimax lower bound is also established, which scales as $\\Omega(d^{L-\\frac{3}{2}}\\sqrt{T})$. Importantly, the graph size $N$ has a diminishing effect on the regret as $T$ grows. These bounds have matching behavior in $T$, exponential dependence on $L$, and polynomial dependence on $d$ (with the gap $d\\ $). On the algorithmic aspect, the paper presents a novel way of designing a computationally efficient CB algorithm, addressing a challenge that the existing CB algorithms using soft interventions face",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Yan",
      "Ali Tajer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2ac79356a03fe5e9250e5e77ebc76e6e-Abstract-Conference.html": {
    "title": "The Implicit Bias of Adam on Separable Data",
    "volume": "main",
    "abstract": "Adam has become one of the most favored optimizers in deep learning problems. Despite its success in practice, numerous mysteries persist regarding its theoretical understanding. In this paper, we study the implicit bias of Adam in linear logistic regression. Specifically, we show that when the training data are linearly separable, the iterates of Adam converge towards a linear classifier that achieves the maximum $\\ell_\\infty$-margin in direction. Notably, for a general class of diminishing learning rates, this convergence occurs within polynomial time. Our result shed light on the difference between Adam and (stochastic) gradient descent from a theoretical perspective",
    "checked": true,
    "id": "4e869a9fd9a78f7f3aebe667b2707208e3cdd099",
    "semantic_title": "the implicit bias of adam on separable data",
    "citation_count": 10,
    "authors": [
      "Chenyang Zhang",
      "Difan Zou",
      "Yuan Cao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2ad2dffba5079687651226ac8752df97-Abstract-Conference.html": {
    "title": "Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs",
    "volume": "main",
    "abstract": "Large language models can memorize and repeat their training data, causing privacy and copyright risks. To mitigate memorization, we introduce a subtle modification to the next-token training objective that we call the goldfish loss. During training, a randomly sampled subsets of tokens are excluded from the loss computation. These dropped tokens are not memorized by the model, which prevents verbatim reproduction of a complete chain of tokens from the training set. We run extensive experiments training billion-scale LLaMA-2 models, both pre-trained and trained from scratch, and demonstrate significant reductions in extractable memorization with little to no impact on downstream benchmarks.Code and checkpoints: https://github.com/ahans30/goldfish-loss",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhimanyu Hans",
      "John Kirchenbauer",
      "Yuxin Wen",
      "Neel Jain",
      "Hamid Kazemi",
      "Prajwal Singhania",
      "Siddharth Singh",
      "Gowthami Somepalli",
      "Jonas Geiping",
      "Abhinav Bhatele",
      "Tom Goldstein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2ae6b2bdf3a179e3e24129e2c54bd871-Abstract-Conference.html": {
    "title": "SIRIUS : Contexual Sparisty with Correction for Efficient LLMs",
    "volume": "main",
    "abstract": "With the blossom of large language models (LLM), inference efficiency becomes increasingly important. Various approximate methods are proposed to reduce the cost at inference time. Contextual Sparsity (CS) is appealing for its training-free nature and its ability to reach a higher compression ratio seemingly without significant performance degradation. However, after a comprehensive evaluation of contextual sparsity methods on various complex generation tasks, we find that although CS succeeds in prompt-understanding tasks, it significantly degrades the model performance for reasoning, deduction, and knowledge-based tasks. Despite the gap in end-to-end accuracy, we observed that sparse models and original models often share the general problem-solving logic and require only a few token corrections to recover the original model performance. This paper introduces SIRIUS, an efficient correction mechanism, which significantly boosts CS models on reasoning tasks while maintaining its efficiency gain. SIRIUS is evaluated on 6 models with 8 difficult generation tasks in reasoning, deduction, and coding and shows consistent effectiveness and efficiency. Also, we carefully develop a system implementation for SIRIUS and show that SIRIUS delivers theoretical latency reduction with roughly a 20% reduction in latency for 8B model on-chip and a 35% reduction in latency for 70B model offloading. We open-source our implementation of Sirius at https://github.com/Infini-AI-Lab/Sirius.git",
    "checked": true,
    "id": "b1ad33569597e61522b1199d333ed75fa8a85496",
    "semantic_title": "sirius : contexual sparisty with correction for efficient llms",
    "citation_count": 3,
    "authors": [
      "Yang Zhou",
      "Zhuoming Chen",
      "Zhaozhuo Xu",
      "Victoria Lin",
      "Beidi Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2aee1c4159e48407d68fe16ae8e6e49e-Abstract-Conference.html": {
    "title": "Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion",
    "volume": "main",
    "abstract": "This paper presents Diffusion Forcing, a new training paradigm where a diffusion model is trained to denoise a set of tokens with independent per-token noise levels. We apply Diffusion Forcing to sequence generative modeling by training a causal next-token prediction model to generate one or several future tokens without fully diffusing past ones. Our approach is shown to combine the strengths of next-token prediction models, such as variable-length generation, with the strengths of full-sequence diffusion models, such as the ability to guide sampling to desirable trajectories. Our method offers a range of additional capabilities, such as (1) rolling-out sequences of continuous tokens, such as video, with lengths past the training horizon, where baselines diverge and (2) new sampling and guiding schemes that uniquely profit from Diffusion Forcing's variable-horizon and causal architecture, and which lead to marked performance gains in decision-making and planning tasks. In addition to its empirical success, our method is proven to optimize a variational lower bound on the likelihoods of all subsequences of tokens drawn from the true joint distribution. Project website: https://boyuan.space/diffusion-forcing/",
    "checked": true,
    "id": "40d63dc2b465c9081e4efc5a19514da151e97fe7",
    "semantic_title": "diffusion forcing: next-token prediction meets full-sequence diffusion",
    "citation_count": 136,
    "authors": [
      "Boyuan Chen",
      "Diego Martí Monsó",
      "Yilun Du",
      "Max Simchowitz",
      "Russ Tedrake",
      "Vincent Sitzmann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2af641762dc02035c31a9314b2d090b6-Abstract-Conference.html": {
    "title": "MiSO: Optimizing brain stimulation to create neural population activity states",
    "volume": "main",
    "abstract": "Brain stimulation has the potential to create desired neural population activity states. However, it is challenging to search the large space of stimulation parameters, for example, selecting which subset of electrodes to be used for stimulation. In this scenario, creating a model that maps the configuration of stimulation parameters to the brain's response can be beneficial. Training such an expansive model usually requires more stimulation-response samples than can be collected in a given experimental session. Furthermore, changes in the properties of the recorded activity over time can make it challenging to merge stimulation-response samples across sessions. To address these challenges, we propose MiSO (MicroStimulation Optimization), a closed-loop stimulation framework to drive neural population activity toward specified states by optimizing over a large stimulation parameter space. MiSO consists of three key components: 1) a neural activity alignment method to merge stimulation-response samples across sessions, 2) a statistical model trained on the merged samples to predict the brain's response to untested stimulation parameter configurations, and 3) an online optimization algorithm to adaptively update the stimulation parameter configuration based on the model's predictions. In this study, we implemented MiSO with a factor analysis (FA) based alignment method, a convolutional neural network (CNN), and an epsilon greedy optimization algorithm. We tested MiSO in closed-loop experiments using electrical microstimulation in the prefrontal cortex of a non-human primate. Guided by the CNN predictions, MiSO successfully searched amongst thousands of stimulation parameter configurations to drive the neural population activity toward specified states. More broadly, MiSO increases the clinical viability of neuromodulation technologies by enabling the use of many-fold larger stimulation parameter spaces",
    "checked": true,
    "id": "cc1d986133e3fb70e14275b8884e4acf2beecfb8",
    "semantic_title": "miso: optimizing brain stimulation to create neural population activity states",
    "citation_count": 1,
    "authors": [
      "Yuki Minai",
      "Joana Soldado-Magraner",
      "Matthew A. Smith",
      "Byron M. Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2b09bb02b90584e2be94ff3ae09289bc-Abstract-Conference.html": {
    "title": "Ferrari: Federated Feature Unlearning via Optimizing Feature Sensitivity",
    "volume": "main",
    "abstract": "The advent of Federated Learning (FL) highlights the practical necessity for the 'right to be forgotten' for all clients, allowing them to request data deletion from the machine learning model's service provider. This necessity has spurred a growing demand for Federated Unlearning (FU). Feature unlearning has gained considerable attention due to its applications in unlearning sensitive, backdoor, and biased features. Existing methods employ the influence function to achieve feature unlearning, which is impractical for FL as it necessitates the participation of other clients, if not all, in the unlearning process. Furthermore, current research lacks an evaluation of the effectiveness of feature unlearning. To address these limitations, we define feature sensitivity in evaluating feature unlearning according to Lipschitz continuity. This metric characterizes the model output's rate of change or sensitivity to perturbations in the input feature. We then propose an effective federated feature unlearning framework called Ferrari, which minimizes feature sensitivity. Extensive experimental results and theoretical analysis demonstrate the effectiveness of Ferrari across various feature unlearning scenarios, including sensitive, backdoor, and biased features. The code is publicly available at https://github.com/OngWinKent/Federated-Feature-Unlearning",
    "checked": true,
    "id": "9d3bc4ec1056e28c8f9afe8e6b359b0feff92aa1",
    "semantic_title": "ferrari: federated feature unlearning via optimizing feature sensitivity",
    "citation_count": 8,
    "authors": [
      "Hanlin Gu",
      "WinKent Ong",
      "Chee Seng Chan",
      "Lixin Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2b0e14abd8128e6bf98b6b0bec1cfcbf-Abstract-Conference.html": {
    "title": "Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in LLMs",
    "volume": "main",
    "abstract": "In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines:1. An uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur,2. Uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and3. A reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward.In experiments on medical diagnosis, troubleshooting and the `20 Questions' game, UoT achieves an average performance improvement of 38.1% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task)",
    "checked": true,
    "id": "8c46669ae82b8110239ff34111348216476dd8b4",
    "semantic_title": "uncertainty of thoughts: uncertainty-aware planning enhances information seeking in llms",
    "citation_count": 8,
    "authors": [
      "Zhiyuan Hu",
      "Chumin Liu",
      "Xidong Feng",
      "Yilun Zhao",
      "See-Kiong Ng",
      "Anh Tuan Luu",
      "Junxian He",
      "Pang Wei W Koh",
      "Bryan Hooi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2b22bacd7ad8677f4837b28a11fe496f-Abstract-Conference.html": {
    "title": "Causal Discovery from Event Sequences by Local Cause-Effect Attribution",
    "volume": "main",
    "abstract": "Sequences of events, such as crashes in the stock market or outages in a network, contain strong temporal dependencies, whose understanding is crucial to react to and influence future events. In this paper, we study the problem of discovering the underlying causal structure from event sequences. To this end, we introduce a new causal model, where individual events of the cause trigger events of the effect with dynamic delays. We show that in contrast to existing methods based on Granger causality, our model is identifiable for both instant and delayed effects.We base our approach on the Algorithmic Markov Condition, by which we identify the true causal network as the one that minimizes the Kolmogorov complexity. As the Kolmogorov complexity is not computable, we instantiate our model using Minimum Description Length and show that the resulting score identifies the causal direction. To discover causal graphs, we introduce the Cascade algorithm, which adds edges in topological order. Extensive evaluation shows that Cascade outperforms existing methods in settings with instantaneous effects, noise, and multiple colliders, and discovers insightful causal graphs on real-world data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joscha Cüppers",
      "Sascha Xu",
      "Ahmed Musa",
      "Jilles Vreeken"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2b2e142192092fd72a9ad2f4e10aa482-Abstract-Conference.html": {
    "title": "Corruption-Robust Linear Bandits: Minimax Optimality and Gap-Dependent Misspecification",
    "volume": "main",
    "abstract": "In linear bandits, how can a learner effectively learn when facing corrupted rewards? While significant work has explored this question, a holistic understanding across different adversarial models and corruption measures is lacking, as is a full characterization of the minimax regret bounds. In this work, we compare two types of corruptions commonly considered: strong corruption, where the corruption level depends on the learner's chosen action, and weak corruption, where the corruption level does not depend on the learner's chosen action. We provide a unified framework to analyze these corruptions. For stochastic linear bandits, we fully characterize the gap between the minimax regret under strong and weak corruptions. We also initiate the study of corrupted adversarial linear bandits, obtaining upper and lower bounds with matching dependencies on the corruption level. Next, we reveal a connection between corruption-robust learning and learning with gap-dependent misspecification—a setting first studied by Liu et al. (2023a), where the misspecification level of an action or policy is proportional to its suboptimality. We present a general reduction that enables any corruption-robust algorithm to handle gap-dependent misspecification. This allows us to recover the results of Liu et al. (2023a) in a black-box manner and significantly generalize them to settings like linear MDPs, yielding the first results for gap-dependent misspecification in reinforcement learning. However, this general reduction does not attain the optimal rate for gap-dependent misspecification. Motivated by this, we develop a specialized algorithm that achieves optimal bounds for gap-dependent misspecification in linear bandits, thus answering an open question posed by Liu et al. (2023a)",
    "checked": true,
    "id": "4d6de6b2e85e2f6bef658b1556c8caa514065a4a",
    "semantic_title": "corruption-robust linear bandits: minimax optimality and gap-dependent misspecification",
    "citation_count": 1,
    "authors": [
      "Haolin Liu",
      "Artin Tajdini",
      "Andrew Wagenmaker",
      "Chen-Yu Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2b47305e1c81890b1089a405686ad183-Abstract-Conference.html": {
    "title": "Structured Matrix Basis for Multivariate Time Series Forecasting with Interpretable Dynamics",
    "volume": "main",
    "abstract": "Multivariate time series forecasting is of central importance in modern intelligent decision systems. The dynamics of multivariate time series are jointly characterized by temporal dependencies and spatial correlations. Hence, it is equally important to build the forecasting models from both perspectives. The real-world multivariate time series data often presents spatial correlations that show structures and evolve dynamically. To capture such dynamic spatial structures, the existing forecasting approaches often rely on a two-stage learning process (learning dynamic series representations and then generating spatial structures), which is sensitive to the small time-window input data and has high variance. To address this, we propose a novel forecasting model with a structured matrix basis. At its core is a dynamic spatial structure generation function whose output space is well-constrained and the generated structures have lower variance, meanwhile, it is more expressive and can offer interpretable dynamics. This is achieved via a novel structured parameterization and imposing structure regularization on the matrix basis. The resulting forecasting model can achieve up to $8.5\\%$ improvements over the existing methods on six benchmark datasets, and meanwhile, it enables us to gain insights into the dynamics of underlying systems",
    "checked": true,
    "id": "162ef1856890ce6a5174433a1f58977034c1ade6",
    "semantic_title": "structured matrix basis for multivariate time series forecasting with interpretable dynamics",
    "citation_count": 1,
    "authors": [
      "Xiaodan Chen",
      "Xiucheng Li",
      "Xinyang Chen",
      "Zhijun Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2b567c9aa100a8edcd13607a91ed3022-Abstract-Conference.html": {
    "title": "PAC-Bayes-Chernoff bounds for unbounded losses",
    "volume": "main",
    "abstract": "We introduce a new PAC-Bayes oracle bound for unbounded losses that extends Cramér-Chernoff bounds to the PAC-Bayesian setting. The proof technique relies on controlling the tails of certain random variables involving the Cramér transform of the loss. Our approach naturally leverages properties of Cramér-Chernoff bounds, such as exact optimization of the free parameter in many PAC-Bayes bounds. We highlight several applications of the main theorem. Firstly, we show that our bound recovers and generalizes previous results. Additionally, our approach allows working with richer assumptions that result in more informative and potentially tighter bounds. In this direction, we provide a general bound under a new model-dependent assumption from which we obtain bounds based on parameter norms and log-Sobolev inequalities. Notably, many of these bounds can be minimized to obtain distributions beyond the Gibbs posterior and provide novel theoretical coverage to existing regularization techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ioar Casado Telletxea",
      "Luis Antonio Ortega Andrés",
      "Aritz Pérez",
      "Andres Masegosa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2b8f4db0464cc5b6e9d5e6bea4b9f308-Abstract-Conference.html": {
    "title": "Transcoders find interpretable LLM feature circuits",
    "volume": "main",
    "abstract": "A key goal in mechanistic interpretability is circuit analysis: finding sparse subgraphs of models corresponding to specific behaviors or capabilities. However, MLP sublayers make fine-grained circuit analysis on transformer-based language models difficult. In particular, interpretable features—such as those found by sparse autoencoders (SAEs)—are typically linear combinations of extremely many neurons, each with its own nonlinearity to account for. Circuit analysis in this setting thus either yields intractably large circuits or fails to disentangle local and global behavior. To address this we explore transcoders, which seek to faithfully approximate a densely activating MLP layer with a wider, sparsely-activating MLP layer. We introduce a novel method for using transcoders to perform weights-based circuit analysis through MLP sublayers. The resulting circuits neatly factorize into input-dependent and input-invariant terms. We then successfully train transcoders on language models with 120M, 410M, and 1.4B parameters, and find them to perform at least on par with SAEs in terms of sparsity, faithfulness, and human-interpretability. Finally, we apply transcoders to reverse-engineer unknown circuits in the model, and we obtain novel insights regarding the \"greater-than circuit\" in GPT2-small. Our results suggest that transcoders can prove effective in decomposing model computations involving MLPs into interpretable circuits. Code is available at https://github.com/jacobdunefsky/transcoder_circuits/",
    "checked": true,
    "id": "3c6da6f1601aee99b8e5b8dcf2d21c42d9252b04",
    "semantic_title": "transcoders find interpretable llm feature circuits",
    "citation_count": 41,
    "authors": [
      "Jacob Dunefsky",
      "Philippe Chlenski",
      "Neel Nanda"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2b94c01ad2b79432f9499501f8754aee-Abstract-Conference.html": {
    "title": "SymILO: A Symmetry-Aware Learning Framework for Integer Linear Optimization",
    "volume": "main",
    "abstract": "Integer linear programs (ILPs) are commonly employed to model diverse practical problems such as scheduling and planning. Recently, machine learning techniques have been utilized to solve ILPs. A straightforward idea is to train a model via supervised learning, with an ILP as the input and an optimal solution as the label. An ILP is symmetric if its variables can be permuted without changing the problem structure, resulting in numerous equivalent and optimal solutions. Randomly selecting an optimal solution as the label can introduce variability in the training data, which may hinder the model from learning stable patterns. In this work, we incorporate the intrinsic symmetry of ILPs and propose a novel training framework called SymILO. Specifically, we modify the learning task by introducing solution permutation along with neural network weights as learnable parameters and then design an alternating algorithm to jointly optimize the loss function.We conduct extensive experiments on ILPs involving different symmetries and the computational results demonstrate that our symmetry-aware approach significantly outperforms three existing methods----achieving $50.3\\\\%$, $66.5\\\\%$, and $45.4\\\\%$ average improvements, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Chen",
      "Tianjian Zhang",
      "Linxin Yang",
      "Qingyu Han",
      "Akang Wang",
      "Ruoyu Sun",
      "Xiaodong Luo",
      "Tsung-Hui Chang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2ba538ff6a168e937d07e360045c0d6b-Abstract-Conference.html": {
    "title": "Bias in Motion: Theoretical Insights into the Dynamics of Bias in SGD Training",
    "volume": "main",
    "abstract": "Machine learning systems often acquire biases by leveraging undesired features in the data, impacting accuracy variably across different sub-populations of the data. However, our current understanding of bias formation mostly focuses on the initial and final stages of learning, leaving a gap in knowledge regarding the transient dynamics. To address this gap, this paper explores the evolution of bias in a teacher-student setup that models different data sub-populations with a Gaussian-mixture model. We provide an analytical description of the stochastic gradient descent dynamics of a linear classifier in this setup, which we prove to be exact in high dimension.Notably, our analysis identifies different properties of the sub-populations that drive bias at different timescales and hence shows a shifting preference of our classifier during training. By applying our general solution to fairness and robustness, we delineate how and when heterogeneous data and spurious features can generate and amplify bias. We empirically validate our results in more complex scenarios by training deeper networks on synthetic and real data, i.e. using CIFAR10, MNIST, and CelebA datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anchit Jain",
      "Rozhin Nobahari",
      "Aristide Baratin",
      "Stefano Sarao Mannelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2bd3ffba268a2699c212a233ed2907f1-Abstract-Conference.html": {
    "title": "Long-range Brain Graph Transformer",
    "volume": "main",
    "abstract": "Understanding communication and information processing among brain regions of interest (ROIs) is highly dependent on long-range connectivity, which plays a crucial role in facilitating diverse functional neural integration across the entire brain. However, previous studies generally focused on the short-range dependencies within brain networks while neglecting the long-range dependencies, limiting an integrated understanding of brain-wide communication. To address this limitation, we propose Adaptive Long-range aware TransformER (ALTER), a brain graph transformer to capture long-range dependencies between brain ROIs utilizing biased random walk. Specifically, we present a novel long-range aware strategy to explicitly capture long-range dependencies between brain ROIs. By guiding the walker towards the next hop with higher correlation value, our strategy simulates the real-world brain-wide communication. Furthermore, by employing the transformer framework, ALERT adaptively integrates both short- and long-range dependencies between brain ROIs, enabling an integrated understanding of multi-level communication across the entire brain. Extensive experiments on ABIDE and ADNI datasets demonstrate that ALTER consistently outperforms generalized state-of-the-art graph learning methods (including SAN, Graphormer, GraphTrans, and LRGNN) and other graph learning based brain network analysis methods (including FBNETGEN, BrainNetGNN, BrainGNN, and BrainNETTF) in neurological disease diagnosis",
    "checked": false,
    "id": "f7a3d9bcf052f2b4ef7d59dcca4013ea11081d0f",
    "semantic_title": "long range graph benchmark",
    "citation_count": 229,
    "authors": [
      "Shuo Yu",
      "Shan Jin",
      "Ming Li",
      "Tabinda Sarwar",
      "Feng Xia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2bd6c9e37df10754a8f5286fca465a80-Abstract-Conference.html": {
    "title": "One-to-Multiple: A Progressive Style Transfer Unsupervised Domain-Adaptive Framework for Kidney Tumor Segmentation",
    "volume": "main",
    "abstract": "In multi-sequence Magnetic Resonance Imaging (MRI), the accurate segmentation of the kidney and tumor based on traditional supervised methods typically necessitates detailed annotation for each sequence, which is both time-consuming and labor-intensive. Unsupervised Domain Adaptation (UDA) methods can effectively mitigate inter-domain differences by aligning cross-modal features, thereby reducing the annotation burden. However, most existing UDA methods are limited to one-to-one domain adaptation, which tends to be inefficient and resource-intensive when faced with multi-target domain transfer tasks. To address this challenge, we propose a novel and efficient One-to-Multiple Progressive Style Transfer Unsupervised Domain-Adaptive (PSTUDA) framework for kidney and tumor segmentation in multi-sequence MRI. Specifically, we develop a multi-level style dictionary to explicitly store the style information of each target domain at various stages, which alleviates the burden of a single generator in a multi-target transfer task and enables effective decoupling of content and style. Concurrently, we employ multiple cascading style fusion modules that utilize point-wise instance normalization to progressively recombine content and style features, which enhances cross-modal alignment and structural consistency. Experiments conducted on the private MSKT and public KiTS19 datasets demonstrate the superiority of the proposed PSTUDA over comparative methods in multi-sequence kidney and tumor segmentation. The average Dice Similarity Coefficients are increased by at least 1.8% and 3.9%, respectively. Impressively, our PSTUDA not only significantly reduces the floating-point computation by approximately 72% but also reduces the number of model parameters by about 50%, bringing higher efficiency and feasibility to practical clinical applications",
    "checked": true,
    "id": "ee6a3dc0e09c8d60c1fdef545d88face7397b577",
    "semantic_title": "one-to-multiple: a progressive style transfer unsupervised domain-adaptive framework for kidney tumor segmentation",
    "citation_count": 1,
    "authors": [
      "Kai Hu",
      "JinHao Li",
      "Yuan Zhang",
      "Xiongjun Ye",
      "Xieping Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2bda52aca6d214904eceffbce50f2e8c-Abstract-Conference.html": {
    "title": "CLIP in Mirror: Disentangling text from visual images through reflection",
    "volume": "main",
    "abstract": "The CLIP network excels in various tasks, but struggles with text-visual images i.e., images that contain both text and visual objects; it risks confusing textual and visual representations. To address this issue, we propose MirrorCLIP, a zero-shot framework, which disentangles the image features of CLIP by exploiting the difference in the mirror effect between visual objects and text in the images. Specifically, MirrorCLIP takes both original and flipped images as inputs, comparing their features dimension-wise in the latent space to generate disentangling masks. With disentangling masks, we further design filters to separate textual and visual factors more precisely, and then get disentangled representations. Qualitative experiments using stable diffusion models and class activation mapping (CAM) validate the effectiveness of our disentanglement. Moreover, our proposed MirrorCLIP reduces confusion when encountering text-visual images and achieves a substantial improvement on typographic defense, further demonstrating its superior ability of disentanglement. Our code is available at https://github.com/tcwangbuaa/MirrorCLIP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiancheng Wang",
      "Yuguang Yang",
      "Linlin Yang",
      "Shaohui Lin",
      "Juan Zhang",
      "Guodong Guo",
      "Baochang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2bdc2267c3d7d01523e2e17ac0a754f3-Abstract-Conference.html": {
    "title": "Grammar-Aligned Decoding",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar. In this paper we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM's distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM's distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM's distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kanghee Park",
      "Jiayu Wang",
      "Taylor Berg-Kirkpatrick",
      "Nadia Polikarpova",
      "Loris D&#x27;Antoni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2bde8fef08f7ebe42b584266cbcfc909-Abstract-Conference.html": {
    "title": "Controlled maximal variability along with reliable performance in recurrent neural networks",
    "volume": "main",
    "abstract": "Natural behaviors, even stereotyped ones, exhibit variability. Despite its role in exploring and learning, the function and neural basis of this variability is still not well understood. Given the coupling between neural activity and behavior, we ask what type of neural variability does not compromise behavioral performance. While previous studies typically curtail variability to allow for high task performance in neural networks, our approach takes the reversed perspective. We investigate how to generate maximal neural variability while at the same time having high network performance. To do so, we extend to neural activity the maximum occupancy principle (MOP) developed for behavior, and refer to this new neural principle as NeuroMOP. NeuroMOP posits that the goal of the nervous system is to maximize future action-state entropy, a reward-free, intrinsic motivation that entails creating all possible activity patterns while avoiding terminal or dangerous ones.We show that this goal can be achieved through a neural network controller that injects currents (actions) into a recurrent neural network of fixed random weights to maximize future cumulative action-state entropy. High activity variability can be induced while adhering to an energy constraint or while avoiding terminal states defined by specific neurons' activities, also in a context-dependent manner. The network solves these tasks by flexibly switching between stochastic and deterministic modes as needed and projecting noise onto a null space. Based on future maximum entropy production, NeuroMOP contributes to a novel theory of neural variability that reconciles stochastic and deterministic behaviors within a single framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiara Mastrogiuseppe",
      "Ruben Moreno Bote"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2bed6c14cd5ea97a9bc1e6094941bde7-Abstract-Conference.html": {
    "title": "Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models",
    "volume": "main",
    "abstract": "We present a maximum entropy inverse reinforcement learning (IRL) approach for improving the sample quality of diffusion generative models, especially when the number of generation time steps is small. Similar to how IRL trains a policy based on the reward function learned from expert demonstrations, we train (or fine-tune) a diffusion model using the log probability density estimated from training data. Since we employ an energy-based model (EBM) to represent the log density, our approach boils down to the joint training of a diffusion model and an EBM. Our IRL formulation, named Diffusion by Maximum Entropy IRL (DxMI), is a minimax problem that reaches equilibrium when both models converge to the data distribution. The entropy maximization plays a key role in DxMI, facilitating the exploration of the diffusion model and ensuring the convergence of the EBM. We also propose Diffusion by Dynamic Programming (DxDP), a novel reinforcement learning algorithm for diffusion models, as a subroutine in DxMI. DxDP makes the diffusion model update in DxMI efficient by transforming the original problem into an optimal control formulation where value functions replace back-propagation in time. Our empirical studies show that diffusion models fine-tuned using DxMI can generate high-quality samples in as few as 4 and 10 steps. Additionally, DxMI enables the training of an EBM without MCMC, stabilizing EBM training dynamics and enhancing anomaly detection performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangwoong Yoon",
      "Himchan Hwang",
      "Dohyun Kwon",
      "Yung-Kyun Noh",
      "Frank Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2bf9868e940198406713451d7656c981-Abstract-Conference.html": {
    "title": "Global Rewards in Restless Multi-Armed Bandits",
    "volume": "main",
    "abstract": "Restless multi-armed bandits (RMAB) extend multi-armed bandits so arm pulls impact future arm states. Despite the success of RMABs, a key limiting assumption is the separability of rewards into a sum across arms. We address this deficiency by proposing restless-multi-armed bandit with global rewards (RMAB-G), a generalization of RMABs to global non-separable rewards. To solve RMAB-G, we develop the Linear-Whittle and Shapley-Whittle indices, which extend Whittle indices from RMABs to RMAB-Gs. We prove approximation bounds which demonstrate how Linear and Shapley-Whittle indices fail for non-linear rewards. To overcome this limitation, we propose two sets of adaptive policies: the first computes indices iteratively and the second combines indices with Monte-Carlo Tree Search (MCTS). Empirically, we demonstrate that adaptive policies outperform both pre-computed index policies and baselines in synthetic and real-world food rescue datasets",
    "checked": true,
    "id": "e3fd1e02dd871b36a7f7d97bf913fd6d96e03224",
    "semantic_title": "global rewards in restless multi-armed bandits",
    "citation_count": 3,
    "authors": [
      "Naveen Raman",
      "Zheyuan Shi",
      "Fei Fang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2bfb9435865f86ecaf7a43c49a00ba04-Abstract-Conference.html": {
    "title": "A Comprehensive Analysis on the Learning Curve in Kernel Ridge Regression",
    "volume": "main",
    "abstract": "This paper conducts a comprehensive study of the learning curves of kernel ridge regression (KRR) under minimal assumptions.Our contributions are three-fold: 1) we analyze the role of key properties of the kernel, such as its spectral eigen-decay, the characteristics of the eigenfunctions, and the smoothness of the kernel; 2) we demonstrate the validity of the Gaussian Equivalent Property (GEP), which states that the generalization performance of KRR remains the same when the whitened features are replaced by standard Gaussian vectors, thereby shedding light on the success of previous analyzes under the Gaussian Design Assumption; 3) we derive novel bounds that improve over existing bounds across a broad range of setting such as (in)dependent feature vectors and various combinations of eigen-decay rates in the over/underparameterized regimes",
    "checked": true,
    "id": "129dcb03b89e691bb41b9c5fc3ac791158d46370",
    "semantic_title": "a comprehensive analysis on the learning curve in kernel ridge regression",
    "citation_count": 1,
    "authors": [
      "Tin Sum Cheng",
      "Aurelien Lucchi",
      "Anastasis Kratsios",
      "David Belius"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2c15b0221da28bc6f4373a7e78b896dd-Abstract-Conference.html": {
    "title": "Statistical Efficiency of Distributional Temporal Difference Learning",
    "volume": "main",
    "abstract": "Distributional reinforcement learning (DRL) has achieved empirical success in various domains.One of the core tasks in the field of DRL is distributional policy evaluation, which involves estimating the return distribution $\\eta^\\pi$ for a given policy $\\pi$.The distributional temporal difference learning has been accordingly proposed, whichis an extension of the temporal difference learning (TD) in the classic RL area.In the tabular case, Rowland et al. [2018] and Rowland et al. [2023] proved the asymptotic convergence of two instances of distributional TD, namely categorical temporal difference learning (CTD) and quantile temporal difference learning (QTD), respectively.In this paper, we go a step further and analyze the finite-sample performance of distributional TD.To facilitate theoretical analysis, we propose a non-parametric distributional TD learning (NTD).For a $\\gamma$-discounted infinite-horizon tabular Markov decision process,we show that for NTD we need $\\widetilde O\\left(\\frac{1}{\\varepsilon^{2p}(1-\\gamma)^{2p+1}}\\right)$ iterations to achieve an $\\varepsilon$-optimal estimator with high probability, when the estimation error is measured by the $p$-Wasserstein distance.This sample complexity bound is minimax optimal (up to logarithmic factors) in the case of the $1$-Wasserstein distance.To achieve this, we establish a novel Freedman's inequality in Hilbert spaces, which would be of independent interest.In addition, we revisit CTD, showing that the same non-asymptotic convergence bounds hold for CTD in the case of the $p$-Wasserstein distance",
    "checked": true,
    "id": "ad15d083fe9fce7cc37e2d96ad03d6bd14143c32",
    "semantic_title": "statistical efficiency of distributional temporal difference learning",
    "citation_count": 0,
    "authors": [
      "Yang Peng",
      "Liangyu Zhang",
      "Zhihua Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2c23b3c72127e15fedc276722faee927-Abstract-Conference.html": {
    "title": "Faster Repeated Evasion Attacks in Tree Ensembles",
    "volume": "main",
    "abstract": "Tree ensembles are one of the most widely used model classes. However, these models are susceptible to adversarial examples, i.e., slightly perturbed examples that elicit a misprediction. There has been significant research on designing approaches to construct such examples for tree ensembles. But this is a computationally challenging problem that often must be solved a large number of times (e.g., for all examples in a training set). This is compounded by the fact that current approaches attempt to find such examples from scratch. In contrast, we exploit the fact that multiple similar problems are being solved. Specifically, our approach exploits the insight that adversarial examples for tree ensembles tend to perturb a consistent but relatively small set of features. We show that we can quickly identify this set of features and use this knowledge to speedup constructing adversarial examples",
    "checked": true,
    "id": "6511c1b13b9cc478ef621ee181b778b8e45cf134",
    "semantic_title": "faster repeated evasion attacks in tree ensembles",
    "citation_count": 0,
    "authors": [
      "Lorenzo Cascioli",
      "Laurens Devos",
      "Ondrej Kuzelka",
      "Jesse Davis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2c2e95b75a10adbd2359f8ed5c0a38cd-Abstract-Conference.html": {
    "title": "DiPEx: Dispersing Prompt Expansion for Class-Agnostic Object Detection",
    "volume": "main",
    "abstract": "Class-agnostic object detection (OD) can be a cornerstone or a bottleneck for many downstream vision tasks. Despite considerable advancements in bottom-up and multi-object discovery methods that leverage basic visual cues to identify salient objects, consistently achieving a high recall rate remains difficult due to the diversity of object types and their contextual complexity. In this work, we investigate using vision-language models (VLMs) to enhance object detection via a self-supervised prompt learning strategy. Our initial findings indicate that manually crafted text queries often result in undetected objects, primarily because detection confidence diminishes when the query words exhibit semantic overlap. To address this, we propose a Dispersing Prompt Expansion (DiPEx) approach. DiPEx progressively learns to expand a set of distinct, non-overlapping hyperspherical prompts to enhance recall rates, thereby improving performance in downstream tasks such as out-of-distribution OD. Specifically, DiPEx initiates the process by self-training generic parent prompts and selecting the one with the highest semantic uncertainty for further expansion. The resulting child prompts are expected to inherit semantics from their parent prompts while capturing more fine-grained semantics. We apply dispersion losses to ensure high inter-class discrepancy among child prompts while preserving semantic consistency between parent-child prompt pairs. To prevent excessive growth of the prompt sets, we utilize the maximum angular coverage (MAC) of the semantic space as a criterion for early termination. We demonstrate the effectiveness of DiPEx through extensive class-agnostic OD and OOD-OD experiments on MS-COCO and LVIS, surpassing other prompting methods by up to 20.1% in AR and achieving a 21.3% AP improvement over SAM",
    "checked": true,
    "id": "a6bb9c134959c03c6db941cc12211e7f57c885dd",
    "semantic_title": "dipex: dispersing prompt expansion for class-agnostic object detection",
    "citation_count": 2,
    "authors": [
      "Jia S Lim",
      "Zhuoxiao Chen",
      "Zhi Chen",
      "Mahsa Baktashmotlagh",
      "Xin Yu",
      "Zi Huang",
      "Yadan Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2c30a37c75f062e0bf79297c73db8c6c-Abstract-Conference.html": {
    "title": "ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown impressive performance on language tasks but face challenges when deployed on resource-constrained devices due to their extensive parameters and reliance on dense multiplications, resulting in high memory demands and latency bottlenecks. Shift-and-add reparameterization offers a promising solution by replacing costly multiplications with hardware-friendly primitives in both the attention and multi-layer perceptron (MLP) layers of an LLM. However, current reparameterization techniques require training from scratch or full parameter fine-tuning to restore accuracy, which is resource-intensive for LLMs. To address this, we propose accelerating pretrained LLMs through post-training shift-and-add reparameterization, creating efficient multiplication-free models, dubbed ShiftAddLLM. Specifically, we quantize each weight matrix into binary matrices paired with group-wise scaling factors. The associated multiplications are reparameterized into (1) shifts between activations and scaling factors and (2) queries and adds according to the binary matrices. To reduce accuracy loss, we present a multi-objective optimization method to minimize both weight and output activation reparameterization errors. Additionally, based on varying sensitivity across layers to reparameterization, we develop an automated bit allocation strategy to further reduce memory usage and latency. Experiments on five LLM families and eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving average perplexity reductions of 5.6 and 22.7 points at comparable or lower latency compared to the most competitive quantized LLMs at 3- and 2-bit precision, respectively, and more than 80% memory and energy reductions over the original LLMs. Codes and models are available at https://github.com/GATECH-EIC/ShiftAddLLM",
    "checked": true,
    "id": "63cf5220c962f8550cf7f08060fdd498c9925c73",
    "semantic_title": "shiftaddllm: accelerating pretrained llms via post-training multiplication-less reparameterization",
    "citation_count": 14,
    "authors": [
      "Haoran You",
      "Yipin Guo",
      "Yichao Fu",
      "Wei Zhou",
      "Huihong Shi",
      "Xiaofan Zhang",
      "Souvik Kundu",
      "Amir Yazdanbakhsh",
      "Yingyan (Celine) Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2c37c5bcef24b9541550261dcd63261b-Abstract-Conference.html": {
    "title": "Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks",
    "volume": "main",
    "abstract": "The Spiking Neural Network (SNN) is a biologically inspired neural network infrastructure that has recently garnered significant attention. It utilizes binary spike activations to transmit information, thereby replacing multiplications with additions and resulting in high energy efficiency. However, training an SNN directly poses a challenge due to the undefined gradient of the firing spike process. Although prior works have employed various surrogate gradient training methods that use an alternative function to replace the firing process during back-propagation, these approaches ignore an intrinsic problem: gradient vanishing. To address this issue, we propose a shortcut back-propagation method in the paper, which advocates for transmitting the gradient directly from the loss to the shallow layers. This enables us to present the gradient to the shallow layers directly, thereby significantly mitigating the gradient vanishing problem. Additionally, this method does not introduce any burden during the inference phase.To strike a balance between final accuracy and ease of training, we also propose an evolutionary training framework and implement it by inducing a balance coefficient that dynamically changes with the training epoch, which further improves the network's performance. Extensive experiments conducted over static and dynamic datasets using several popular network structures reveal that our method consistently outperforms state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Guo",
      "Yuanpei Chen",
      "Zecheng Hao",
      "Weihang Peng",
      "Zhou Jie",
      "Yuhan Zhang",
      "Xiaode Liu",
      "Zhe Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2c428bb07062012236519b589db63f34-Abstract-Conference.html": {
    "title": "Pedestrian-Centric 3D Pre-collision Pose and Shape Estimation from Dashcam Perspective",
    "volume": "main",
    "abstract": "Pedestrian pre-collision pose is one of the key factors to determine the degree of pedestrian-vehicle injury in collision. Human pose estimation algorithm is an effective method to estimate pedestrian emergency pose from accident video. However, the pose estimation model trained by the existing daily human pose datasets has poor robustness under specific poses such as pedestrian pre-collision pose, and it is difficult to obtain human pose datasets in the wild scenes, especially lacking scarce data such as pedestrian pre-collision pose in traffic scenes. In this paper, we collect pedestrian-vehicle collision pose from the dashcam perspective of dashcam and construct the first Pedestrian-Vehicle Collision Pose dataset (PVCP) in a semi-automatic way, including 40k+ accident frames and 20K+ pedestrian pre-collision pose annotation (2D, 3D, Mesh). Further, we construct a Pedestrian Pre-collision Pose Estimation Network (PPSENet) to estimate the collision pose and shape sequence of pedestrians from pedestrian-vehicle accident videos. The PPSENet first estimates the 2D pose from the image (Image to Pose, ITP) and then lifts the 2D pose to 3D mesh (Pose to Mesh, PTM). Due to the small size of the dataset, we introduce a pre-training model that learns the human pose prior on a large number of pose datasets, and use iterative regression to estimate the pre-collision pose and shape of pedestrians. Further, we classify the pre-collision pose sequence and introduce pose class loss, which achieves the best accuracy compared with the existing relevant \\textit{state-of-the-art} methods. Code and data are available for research at https://github.com/wmj142326/PVCP",
    "checked": true,
    "id": "9a552c38b9773ba731a0cd9d3ce89cf3f33a27ec",
    "semantic_title": "pedestrian-centric 3d pre-collision pose and shape estimation from dashcam perspective",
    "citation_count": 1,
    "authors": [
      "MeiJun Wang",
      "Yu Meng",
      "Zhongwei Qiu",
      "Chao Zheng",
      "Yan Xu",
      "Pengxiaorui",
      "Jian Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2c487f8a54cf24c0684c32abc77fed56-Abstract-Conference.html": {
    "title": "Aligning Diffusion Models by Optimizing Human Utility",
    "volume": "main",
    "abstract": "We present Diffusion-KTO, a novel approach for aligning text-to-image diffusion models by formulating the alignment objective as the maximization of expected human utility. Unlike previous methods, Diffusion-KTO does not require collecting pairwise preference data nor training a complex reward model. Instead, our objective uses per-image binary feedback signals, e.g. likes or dislikes, to align the model with human preferences. After fine-tuning using Diffusion-KTO, text-to-image diffusion models exhibit improved performance compared to existing techniques, including supervised fine-tuning and Diffusion-DPO, both in terms of human judgment and automatic evaluation metrics such as PickScore and ImageReward. Overall, Diffusion-KTO unlocks the potential of leveraging readily available per-image binary preference signals and broadens the applicability of aligning text-to-image diffusion models with human preferences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shufan Li",
      "Konstantinos Kallidromitis",
      "Akash Gokul",
      "Yusuke Kato",
      "Kazuki Kozuka"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2c570b0f9938c7a58a612e5b00af9cc0-Abstract-Conference.html": {
    "title": "BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models",
    "volume": "main",
    "abstract": "This work presents BAdam, an optimization method that leverages the block coordinate descent (BCD) framework with Adam's update rule. BAdam offers a memory efficient approach to the full parameter finetuning of large language models. We conduct a theoretical convergence analysis for BAdam in the deterministic case. Experimentally, we apply BAdam to finetune the Llama 3-8B and Llama 3-70B models using a single RTX3090-24GB GPU and 4 A100-80GB GPUs, respectively. The results confirm BAdam's efficiency in terms of memory usage, running time, and optimization capability. Furthermore, the downstream performance evaluation based on MT-bench and math benchmarks shows that BAdam outperforms existing memory efficient baselines such as LoRA. It also demonstrates that BAdam can achieve comparable or even superior performance compared to Adam. Finally, the ablation study using SGD's update rule illustrates the suitability of BCD for finetuning LLMs. Our code can be easily integrated into any PyTorch-based codebase and is available at https://github.com/Ledzy/BAdam",
    "checked": true,
    "id": "177cea5cf71b81685d8227d054104dc1d2334a54",
    "semantic_title": "badam: a memory efficient full parameter optimization method for large language models",
    "citation_count": 8,
    "authors": [
      "Qijun Luo",
      "Hengxu Yu",
      "Xiao Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2c572cad9ae98c5cb6f3fca040b2bc54-Abstract-Conference.html": {
    "title": "Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural Networks using the Marginal Likelihood",
    "volume": "main",
    "abstract": "Neural network sparsification is a promising avenue to save computational time and memory costs, especially in an age where many successful AI models are becoming too large to naively deploy on consumer hardware. While much work has focused on different weight pruning criteria, the overall sparsifiability of the network, i.e., its capacity to be pruned without quality loss, has often been overlooked. We present Sparsifiability via the Marginal likelihood (SpaM), a sparsification framework that highlights the effectiveness of using the Bayesian marginal likelihood in conjunction with sparsity-inducing priors for making neural networks more sparsifiable. Our approach implements an automatic Occam's razor that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification. In addition, we demonstrate that the pre-computed posterior precision from the Laplace approximation can be re-used to define a cheap pruning criterion, which outperforms many existing (more expensive) approaches. We demonstrate the effectiveness of our framework, especially at high sparsity levels, across a range of different neural network architectures and datasets",
    "checked": true,
    "id": "2f3f0350bca2b66292b2f96e72faa6e3da8e9583",
    "semantic_title": "shaving weights with occam's razor: bayesian sparsification for neural networks using the marginal likelihood",
    "citation_count": 6,
    "authors": [
      "Rayen Dhahri",
      "Alexander Immer",
      "Bertrand Charpentier",
      "Stephan Günnemann",
      "Vincent Fortuin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2c8047bf3ed8ef6905351608d641f02f-Abstract-Conference.html": {
    "title": "First-Order Minimax Bilevel Optimization",
    "volume": "main",
    "abstract": "Multi-block minimax bilevel optimization has been studied recently due to its great potential in multi-task learning, robust machine learning, and few-shot learning. However, due to the complex three-level optimization structure, existing algorithms often suffer from issues such as high computing costs due to the second-order model derivatives or high memory consumption in storing all blocks' parameters. In this paper, we tackle these challenges by proposing two novel fully first-order algorithms named FOSL and MemCS. FOSL features a fully single-loop structure by updating all three variables simultaneously, and MemCS is a memory-efficient double-loop algorithm with cold-start initialization. We provide a comprehensive convergence analysis for both algorithms under full and partial block participation, and show that their sample complexities match or outperform those of the same type of methods in standard bilevel optimization. We evaluate our methods in two applications: the recently proposed multi-task deep AUC maximization and a novel rank-based robust meta-learning. Our methods consistently improve over existing methods with better performance over various datasets",
    "checked": true,
    "id": "68f588fe412ba5f8d308d9cf3eb9d743d1cc35a8",
    "semantic_title": "first-order minimax bilevel optimization",
    "citation_count": 0,
    "authors": [
      "Yifan Yang",
      "Zhaofeng Si",
      "Siwei Lyu",
      "Kaiyi Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2c9d78ed62ff5bf2377c3840188114c0-Abstract-Conference.html": {
    "title": "Sample Complexity of Algorithm Selection Using Neural Networks and Its Applications to Branch-and-Cut",
    "volume": "main",
    "abstract": "Data-driven algorithm design is a paradigm that uses statistical and machine learning techniques to select from a class of algorithms for a computational problem an algorithm that has the best expected performance with respect to some (unknown) distribution on the instances of the problem. We build upon recent work in this line of research by considering the setup where, instead of selecting a single algorithm that has the best performance, we allow the possibility of selecting an algorithm based on the instance to be solved, using neural networks. In particular, given a representative sample of instances, we learn a neural network that maps an instance of the problem to the most appropriate algorithm for that instance. We formalize this idea and derive rigorous sample complexity bounds for this learning problem, in the spirit of recent work in data-driven algorithm design. We then apply this approach to the problem of making good decisions in the branch-and-cut framework for mixed-integer optimization (e.g., which cut to add?). In other words, the neural network will take as input a mixed-integer optimization instance and output a decision that will result in a small branch-and-cut tree for that instance. Our computational results provide evidence that our particular way of using neural networks for cut selection can make a significant impact in reducing branch-and-cut tree sizes, compared to previous data-driven approaches",
    "checked": true,
    "id": "4e774be980d328b230fbbd17328ded7039c66457",
    "semantic_title": "sample complexity of algorithm selection using neural networks and its applications to branch-and-cut",
    "citation_count": 2,
    "authors": [
      "Hongyu Cheng",
      "Sammy Khalife",
      "Barbara Fiedorowicz",
      "Amitabh Basu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2cc0b08447bf9668db268e6c86364a6e-Abstract-Conference.html": {
    "title": "A Simple Image Segmentation Framework via In-Context Examples",
    "volume": "main",
    "abstract": "Recently, there have been explorations of generalist segmentation models that can effectively tackle a variety of image segmentation tasks within a unified in-context learning framework. However, these methods still struggle with task ambiguity in in-context segmentation, as not all in-context examples can accurately convey the task information. In order to address this issue, we present SINE, a simple image $\\textbf{S}$egmentation framework utilizing $\\textbf{in}$-context $\\textbf{e}$xamples. Our approach leverages a Transformer encoder-decoder structure, where the encoder provides high-quality image representations, and the decoder is designed to yield multiple task-specific output masks to eliminate task ambiguity effectively. Specifically, we introduce an In-context Interaction module to complement in-context information and produce correlations between the target image and the in-context example and a Matching Transformer that uses fixed matching and a Hungarian algorithm to eliminate differences between different tasks. In addition, we have further perfected the current evaluation system for in-context image segmentation, aiming to facilitate a holistic appraisal of these models. Experiments on various segmentation tasks show the effectiveness of the proposed method",
    "checked": true,
    "id": "567dfe57a85d4b11e3916b74d8f9ab382f083bf9",
    "semantic_title": "a simple image segmentation framework via in-context examples",
    "citation_count": 10,
    "authors": [
      "Yang Liu",
      "Chenchen Jing",
      "Hengtao Li",
      "Muzhi Zhu",
      "Hao Chen",
      "Xinlong Wang",
      "Chunhua Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2cc8dc30e52798b27d37b795cc153310-Abstract-Conference.html": {
    "title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
    "volume": "main",
    "abstract": "Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained LLMs add numbers using Fourier features---dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features.Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy.Introducing pre-trained token embeddings to a randomly initialized model rescues its performance.Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks",
    "checked": true,
    "id": "f86fb205cd4d85478e65304fc38bdf1e4bed2440",
    "semantic_title": "pre-trained large language models use fourier features to compute addition",
    "citation_count": 19,
    "authors": [
      "Tianyi Zhou",
      "Deqing Fu",
      "Vatsal Sharan",
      "Robin Jia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2cd36d327f33d47b372d4711edd08de0-Abstract-Conference.html": {
    "title": "Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators",
    "volume": "main",
    "abstract": "Neural operators, serving as physics surrogate models, have recently gained increased interest. With ever increasing problem complexity, the natural question arises: what is an efficient way to scale neural operators to larger and more complex simulations - most importantly by taking into account different types of simulation datasets. This is of special interest since, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. Whereas the flexibility of transformers has enabled unified architectures across domains, neural operators mostly follow a problem specific design, where GNNs are commonly used for Lagrangian simulations and grid-based models predominate Eulerian simulations. We introduce Universal Physics Transformers (UPTs), an efficient and unified learning paradigm for a wide range of spatio-temporal problems. UPTs operate without grid- or particle-based latent structures, enabling flexibility and scalability across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space representation at any point in space-time. We demonstrate diverse applicability and efficacy of UPTs in mesh-based fluid simulations, and steady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based dynamics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benedikt Alkin",
      "Andreas Fürst",
      "Simon Schmid",
      "Lukas Gruber",
      "Markus Holzleitner",
      "Johannes Brandstetter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2cdf71a65e95bf1874212f6e604c64db-Abstract-Conference.html": {
    "title": "ReMAP: Neural Model Reprogramming with Network Inversion and Retrieval-Augmented Mapping for Adaptive Motion Forecasting",
    "volume": "main",
    "abstract": "Mobility impairment caused by limb loss, aging, stroke, and other movement deficiencies is a significant challenge faced by millions of individuals worldwide. Advanced assistive technologies, such as prostheses and orthoses, have the potential to greatly improve the quality of life for such individuals. A critical component in the design of these technologies is the accurate forecasting of reference joint motion for impaired limbs, which is hindered by the scarcity of joint locomotion data available for these patients. To address this, we propose ReMAP, a novel model repurposing strategy that leverages deep learning's reprogramming property, incorporating network inversion principles and retrieval-augmented mapping. Our approach adapts models originally designed for able-bodied individuals to forecast joint motion in limb-impaired patients without altering model parameters. We demonstrate the efficacy of ReMAP through extensive empirical studies on data from below-knee amputated patients, showcasing significant improvements over traditional transfer learning and fine-tuning methods. These findings have significant implications for advancing assistive technology and mobility for patients with amputations, stroke, or aging",
    "checked": true,
    "id": "0e4134e03cace91abc743e8b35332785733f668a",
    "semantic_title": "remap: neural model reprogramming with network inversion and retrieval-augmented mapping for adaptive motion forecasting",
    "citation_count": 0,
    "authors": [
      "Sharmita Dey",
      "Sarath Ravindran Nair"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2ce10f144bb93449767f355c01f24cc1-Abstract-Conference.html": {
    "title": "Cross-video Identity Correlating for Person Re-identification Pre-training",
    "volume": "main",
    "abstract": "Recent researches have proven that pre-training on large-scale person images extracted from internet videos is an effective way in learning better representations for person re-identification. However, these researches are mostly confined to pre-training at the instance-level or single-video tracklet-level. They ignore the identity-invariance in images of the same person across different videos, which is a key focus in person re-identification. To address this issue, we propose a Cross-video Identity-cOrrelating pre-traiNing (CION) framework. Defining a noise concept that comprehensively considers both intra-identity consistency and inter-identity discrimination, CION seeks the identity correlation from cross-video images by modeling it as a progressive multi-level denoising problem. Furthermore, an identity-guided self-distillation loss is proposed to implement better large-scale pre-training by mining the identity-invariance within person images. We conduct extensive experiments to verify the superiority of our CION in terms of efficiency and performance. CION achieves significantly leading performance with even fewer training samples. For example, compared with the previous state-of-the-art ISR, CION with the same ResNet50-IBN achieves higher mAP of 93.3% and 74.3% on Market1501 and MSMT17, while only utilizing 8% training samples. Finally, with CION demonstrating superior model-agnostic ability, we contribute a model zoo named ReIDZoo to meet diverse research and application needs in this field. It contains a series of CION pre-trained models with spanning structures and parameters, totaling 32 models with 10 different structures, including GhostNet, ConvNext, RepViT, FastViT and so on. The code and models will be open-sourced",
    "checked": true,
    "id": "c64c46ae3265b2420b93a6d542f2ec08c44dc1e8",
    "semantic_title": "cross-video identity correlating for person re-identification pre-training",
    "citation_count": 5,
    "authors": [
      "Jialong Zuo",
      "Ying Nie",
      "Hanyu Zhou",
      "Huaxin Zhang",
      "Haoyu Wang",
      "Tianyu Guo",
      "Nong Sang",
      "Changxin Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2ce4f0b8e24c45318352068603153590-Abstract-Conference.html": {
    "title": "Cloud Object Detector Adaptation by Integrating Different Source Knowledge",
    "volume": "main",
    "abstract": "We propose to explore an interesting and promising problem, Cloud Object Detector Adaptation (CODA), where the target domain leverages detections provided by a large cloud model to build a target detector. Despite with powerful generalization capability, the cloud model still cannot achieve error-free detection in a specific target domain. In this work, we present a novel Cloud Object detector adaptation method by Integrating different source kNowledge (COIN). The key idea is to incorporate a public vision-language model (CLIP) to distill positive knowledge while refining negative knowledge for adaptation by self-promotion gradient direction alignment. To that end, knowledge dissemination, separation, and distillation are carried out successively. Knowledge dissemination combines knowledge from cloud detector and CLIP model to initialize a target detector and a CLIP detector in target domain. By matching CLIP detector with the cloud detector, knowledge separation categorizes detections into three parts: consistent, inconsistent and private detections such that divide-and-conquer strategy can be used for knowledge distillation. Consistent and private detections are directly used to train target detector; while inconsistent detections are fused based on a consistent knowledge generation network, which is trained by aligning the gradient direction of inconsistent detections to that of consistent detections, because it provides a direction toward an optimal target detector. Experiment results demonstrate that the proposed COIN method achieves the state-of-the-art performance",
    "checked": true,
    "id": "5407b01f6ab0be68e8e0571c26f3d4ab50e2e5a8",
    "semantic_title": "cloud object detector adaptation by integrating different source knowledge",
    "citation_count": 2,
    "authors": [
      "Shuaifeng Li",
      "Mao Ye",
      "Lihua Zhou",
      "Nianxin Li",
      "Siying Xiao",
      "Song Tang",
      "Xiatian Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2ceda49041816da6d5a34eb3b612607f-Abstract-Conference.html": {
    "title": "DETAIL: Task DEmonsTration Attribution for Interpretable In-context Learning",
    "volume": "main",
    "abstract": "In-context learning (ICL) allows transformer-based language models that are pre-trained on general text to quickly learn a specific task with a few \"task demonstrations\" without updating their parameters, significantly boosting their flexibility and generality. ICL possesses many distinct characteristics from conventional machine learning, thereby requiring new approaches to interpret this learning paradigm. Taking the viewpoint of recent works showing that transformers learn in context by formulating an internal optimizer, we propose an influence function-based attribution technique, DETAIL, that addresses the specific characteristics of ICL. We empirically verify the effectiveness of our approach for demonstration attribution while being computationally efficient. Leveraging the results, we then show how DETAIL can help improve model performance in real-world scenarios through demonstration reordering and curation. Finally, we experimentally prove the wide applicability of DETAIL by showing our attribution scores obtained on white-box models are transferable to black-box models in improving model performance",
    "checked": true,
    "id": "f5d64b7c8b4cfead0c04d3ee5c5bc88575378623",
    "semantic_title": "detail: task demonstration attribution for interpretable in-context learning",
    "citation_count": 4,
    "authors": [
      "Zijian Zhou",
      "Xiaoqiang Lin",
      "Xinyi Xu",
      "Alok Prakash",
      "Daniela Rus",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2cf153951b5e9b39564fc4a0ef6adc1a-Abstract-Conference.html": {
    "title": "Mirror and Preconditioned Gradient Descent in Wasserstein Space",
    "volume": "main",
    "abstract": "As the problem of minimizing functionals on the Wasserstein space encompasses many applications in machine learning, different optimization algorithms on $\\mathbb{R}^d$ have received their counterpart analog on the Wasserstein space. We focus here on lifting two explicit algorithms: mirror descent and preconditioned gradient descent. These algorithms have been introduced to better capture the geometry of the function to minimize and are provably convergent under appropriate (namely relative) smoothness and convexity conditions. Adapting these notions to the Wasserstein space, we prove guarantees of convergence of some Wasserstein-gradient-based discrete-time schemes for new pairings of objective functionals and regularizers. The difficulty here is to carefully select along which curves the functionals should be smooth and convex. We illustrate the advantages of adapting the geometry induced by the regularizer on ill conditioned optimization tasks, and showcase the improvement of choosing different discrepancies and geometries in a computational biology task of aligning single-cells",
    "checked": true,
    "id": "45044b7ab3d477b547c0cfdd9de06bcd0c41264a",
    "semantic_title": "mirror and preconditioned gradient descent in wasserstein space",
    "citation_count": 6,
    "authors": [
      "Clément Bonet",
      "Théo Uscidda",
      "Adam David",
      "Pierre-Cyril Aubin-Frankowski",
      "Anna Korba"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2cfa9b0d9be8a5c01cf3eb7f21b4f2b8-Abstract-Conference.html": {
    "title": "Retrieval-Retro: Retrieval-based Inorganic Retrosynthesis with Expert Knowledge",
    "volume": "main",
    "abstract": "While inorganic retrosynthesis planning is essential in the field of chemical science, the application of machine learning in this area has been notably less explored compared to organic retrosynthesis planning. In this paper, we propose Retrieval-Retro for inorganic retrosynthesis planning, which implicitly extracts the precursor information of reference materials that are retrieved from the knowledge base regarding domain expertise in the field. Specifically, instead of directly employing the precursor information of reference materials, we propose implicitly extracting it with various attention layers, which enables the model to learn novel synthesis recipes more effectively.Moreover, during retrieval, we consider the thermodynamic relationship between target material and precursors, which is essential domain expertise in identifying the most probable precursor set among various options. Extensive experiments demonstrate the superiority of Retrieval-Retro in retrosynthesis planning, especially in discovering novel synthesis recipes, which is crucial for materials discovery.The source code for Retrieval-Retro is available at https://github.com/HeewoongNoh/Retrieval-Retro",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heewoong Noh",
      "Namkyeong Lee",
      "Gyoung S. Na",
      "Chanyoung Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2d0842550e6d92b0e27e7e810b1a4792-Abstract-Conference.html": {
    "title": "Kernel-Based Function Approximation for Average Reward Reinforcement Learning: An Optimist No-Regret Algorithm",
    "volume": "main",
    "abstract": "Reinforcement Learning (RL) utilizing kernel ridge regression to predict the expected value function represents a powerful method with great representational capacity. This setting is a highly versatile framework amenable to analytical results. We consider kernel-based function approximation for RL in the infinite horizon average reward setting, also referred to as the undiscounted setting. We propose an optimistic algorithm, similar to acquisition function based algorithms in the special case of bandits. We establish novel no-regret performance guarantees for our algorithm, under kernel-based modelling assumptions. Additionally, we derive a novel confidence interval for the kernel-based prediction of the expected value function, applicable across various RL problems",
    "checked": true,
    "id": "9d512bec75bd2a3b5e00b56e76d92e994340b258",
    "semantic_title": "kernel-based function approximation for average reward reinforcement learning: an optimist no-regret algorithm",
    "citation_count": 0,
    "authors": [
      "Sattar Vakili",
      "Julia Olkhovskaya"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2d2cf241331d7e71a6f20e9ed798a06b-Abstract-Conference.html": {
    "title": "Generative Fractional Diffusion Models",
    "volume": "main",
    "abstract": "We introduce the first continuous-time score-based generative model that leverages fractional diffusion processes for its underlying dynamics. Although diffusion models have excelled at capturing data distributions, they still suffer from various limitations such as slow convergence, mode-collapse on imbalanced data, and lack of diversity. These issues are partially linked to the use of light-tailed Brownian motion (BM) with independent increments. In this paper, we replace BM with an approximation of its non-Markovian counterpart, fractional Brownian motion (fBM), characterized by correlated increments and Hurst index $H \\in (0,1)$, where $H=0.5$ recovers the classical BM. To ensure tractable inference and learning, we employ a recently popularized Markov approximation of fBM (MA-fBM) and derive its reverse-time model, resulting in *generative fractional diffusion models* (GFDM). We characterize the forward dynamics using a continuous reparameterization trick and propose *augmented score matching* to efficiently learn the score function, which is partly known in closed form, at minimal added cost. The ability to drive our diffusion model via MA-fBM offers flexibility and control. $H \\leq 0.5$ enters the regime of *rough paths* whereas $H>0.5$ regularizes diffusion paths and invokes long-term memory. The Markov approximation allows added control by varying the number of Markov processes linearly combined to approximate fBM. Our evaluations on real image datasets demonstrate that GFDM achieves greater pixel-wise diversity and enhanced image quality, as indicated by a lower FID, offering a promising alternative to traditional diffusion models",
    "checked": true,
    "id": "58ae04c984ac7549908d03183f886f883ec0ae83",
    "semantic_title": "generative fractional diffusion models",
    "citation_count": 6,
    "authors": [
      "Gabriel Nobis",
      "Maximilian Springenberg",
      "Marco Aversa",
      "Michael Detzel",
      "Rembert Daems",
      "Roderick Murray-Smith",
      "Shinichi Nakajima",
      "Sebastian Lapuschkin",
      "Stefano Ermon",
      "Tolga Birdal",
      "Manfred Opper",
      "Christoph Knochenhauer",
      "Luis Oala",
      "Wojciech Samek"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2d43f7a61b57f83619f82c971e4bddc0-Abstract-Conference.html": {
    "title": "Least Squares Regression Can Exhibit Under-Parameterized Double Descent",
    "volume": "main",
    "abstract": "The relationship between the number of training data points, the number of parameters, and the generalization capabilities of models has been widely studied. Previous work has shown that double descent can occur in the over-parameterized regime and that the standard bias-variance trade-off holds in the under-parameterized regime. These works provide multiple reasons for the existence of the peak. We postulate that the location of the peak depends on the technical properties of both the spectrum as well as the eigenvectors of the sample covariance. We present two simple examples that provably exhibit double descent in the under-parameterized regime and do not seem to occur for reasons provided in prior work",
    "checked": true,
    "id": "314180d0c91c64ae2e178148fc283fa5db474af2",
    "semantic_title": "least squares regression can exhibit under-parameterized double descent",
    "citation_count": 3,
    "authors": [
      "Xinyue Li",
      "Rishi Sonthalia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2d4eaf042567f1c03c086103cc154c1f-Abstract-Conference.html": {
    "title": "AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation",
    "volume": "main",
    "abstract": "Pre-trained vision-language models (VLMs) have shown impressive results in various visual classification tasks.However, we often fail to fully unleash their potential when adapting them for new concept understanding due to limited information on new classes.To address this limitation, we introduce a novel adaptation framework, AWT (Augment, Weight, then Transport). AWT comprises three key components: augmenting inputs with diverse visual perspectives and enriched class descriptions through image transformations and language models; dynamically weighting inputs based on the prediction entropy; and employing optimal transport to mine semantic correlations in the vision-language space.AWT can be seamlessly integrated into various VLMs, enhancing their zero-shot capabilities without additional training and facilitating few-shot learning through an integrated multimodal adapter module.We verify AWT in multiple challenging scenarios, including zero-shot and few-shot image classification, zero-shot video action recognition, and out-of-distribution generalization. AWT consistently outperforms the state-of-the-art methods in each setting. In addition, our extensive studies further demonstrate AWT's effectiveness and adaptability across different VLMs, architectures, and scales",
    "checked": true,
    "id": "a25a68a2ae0f66e036d35fec872d51976ff87c8d",
    "semantic_title": "awt: transferring vision-language models via augmentation, weighting, and transportation",
    "citation_count": 14,
    "authors": [
      "Yuhan Zhu",
      "Yuyang Ji",
      "Zhiyu Zhao",
      "Gangshan Wu",
      "Limin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2d52879ef2ba487445ca2e143b104c3b-Abstract-Conference.html": {
    "title": "Decentralized Noncooperative Games with Coupled Decision-Dependent Distributions",
    "volume": "main",
    "abstract": "Distribution variations in machine learning, driven by the dynamic nature of deployment environments, significantly impact the performance of learning models. This paper explores endogenous distribution shifts in learning systems, where deployed models influence environments and subsequently alter data distributions. This phenomenon is formulated by a decision-dependent distribution mapping within the recently proposed framework of performative prediction (PP) Perdomo et al. (2020). We investigate the performative effect in a decentralized noncooperative game, where players aim to minimize private cost functions while simultaneously managing coupled inequality constraints. Under performativity, we examine two equilibrium concepts for the studied game: performative stable equilibrium (PSE) and Nash equilibrium (NE), and establish sufficient conditions for their existence and uniqueness. Notably, we provide the first upper bound on the distance between the PSE and NE in the literature, which is challenging to evaluate due to the absence of strong convexity on the joint cost function. Furthermore, we develop a decentralized stochastic primal-dual algorithm for efficiently computing the PSE point. By carefully bounding the performative effect in theoretical analysis, we prove that the proposed algorithm achieves sublinear convergence rates for both performative regrets and constraint violation and maintains the same order of convergence rate as the case without performativity. Numerical experiments validate the effectiveness of our algorithm and theoretical results",
    "checked": true,
    "id": "7c9933c88bfe62b5e567de8df9b02b0a594897a1",
    "semantic_title": "decentralized noncooperative games with coupled decision-dependent distributions",
    "citation_count": 0,
    "authors": [
      "Wenjing YAN",
      "Xuanyu Cao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2d5e30cc59c46aebe9d35a73ff41d32b-Abstract-Conference.html": {
    "title": "Geometric Trajectory Diffusion Models",
    "volume": "main",
    "abstract": "Generative models have shown great promise in generating 3D geometric systems, which is a fundamental problem in many natural science domains such as molecule and protein design. However, existing approaches only operate on static structures, neglecting the fact that physical systems are always dynamic in nature. In this work, we propose geometric trajectory diffusion models (GeoTDM), the first diffusion model for modeling the temporal distribution of 3D geometric trajectories. Modeling such distribution is challenging as it requires capturing both the complex spatial interactions with physical symmetries and temporal correspondence encapsulated in the dynamics. We theoretically justify that diffusion models with equivariant temporal kernels can lead to density with desired symmetry, and develop a novel transition kernel leveraging SE(3)-equivariant spatial convolution and temporal attention. Furthermore, to induce an expressive trajectory distribution for conditional generation, we introduce a generalized learnable geometric prior into the forward diffusion process to enhance temporal conditioning. We conduct extensive experiments on both unconditional and conditional generation in various scenarios, including physical simulation, molecular dynamics, and pedestrian motion. Empirical results on a wide suite of metrics demonstrate that GeoTDM can generate realistic geometric trajectories with significantly higher quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Han",
      "Minkai Xu",
      "Aaron Lou",
      "Haotian Ye",
      "Stefano Ermon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2d66a70c770de7835678f1c1e65fe5e1-Abstract-Conference.html": {
    "title": "GoMatching: A Simple Baseline for Video Text Spotting via Long and Short Term Matching",
    "volume": "main",
    "abstract": "Beyond the text detection and recognition tasks in image text spotting, video text spotting presents an augmented challenge with the inclusion of tracking. While advanced end-to-end trainable methods have shown commendable performance, the pursuit of multi-task optimization may pose the risk of producing sub-optimal outcomes for individual tasks. In this paper, we identify a main bottleneck in the state-of-the-art video text spotter: the limited recognition capability. In response to this issue, we propose to efficiently turn an off-the-shelf query-based image text spotter into a specialist on video and present a simple baseline termed GoMatching, which focuses the training efforts on tracking while maintaining strong recognition performance. To adapt the image text spotter to video datasets, we add a rescoring head to rescore each detected instance's confidence via efficient tuning, leading to a better tracking candidate pool. Additionally, we design a long-short term matching module, termed LST-Matcher, to enhance the spotter's tracking capability by integrating both long- and short-term matching results via Transformer. Based on the above simple designs, GoMatching delivers new records on ICDAR15-video, DSText, BOVText, and our proposed novel test set with arbitrary-shaped text termed ArTVideo, which demonstates GoMatching's capability to accommodate general, dense, small, arbitrary-shaped, Chinese and English text scenarios while saving considerable training budgets. The code will be released",
    "checked": true,
    "id": "f63205b517e8ad4e8daa747869dbf1db95115c21",
    "semantic_title": "gomatching: a simple baseline for video text spotting via long and short term matching",
    "citation_count": 5,
    "authors": [
      "Haibin He",
      "Maoyuan Ye",
      "Jing Zhang",
      "Juhua Liu",
      "Bo Du",
      "Dacheng Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2d69e771d9f274f7c624198ea74f5b98-Abstract-Conference.html": {
    "title": "Multi-Scale VMamba: Hierarchy in Hierarchy Visual State Space Model",
    "volume": "main",
    "abstract": "Despite the significant achievements of Vision Transformers (ViTs) in various vision tasks, they are constrained by the quadratic complexity. Recently, State Space Models (SSMs) have garnered widespread attention due to their global receptive field and linear complexity with respect to the input length, demonstrating substantial potential across fields including natural language processing and computer vision. To improve the performance of SSMs in vision tasks, a multi-scan strategy is widely adopted, which leads to significant redundancy of SSMs. For a better trade-off between efficiency and performance, we analyze the underlying reasons behind the success of the multi-scan strategy, where long-range dependency plays an important role. Based on the analysis, we introduce Multi-Scale Vision Mamba (MSVMamba) to preserve the superiority of SSMs in vision tasks with limited parameters. It employs a multi-scale 2D scanning technique on both original and downsampled feature maps, which not only benefits long-range dependency learning but also reduces computational costs. Additionally, we integrate a Convolutional Feed-Forward Network (ConvFFN) to address the lack of channel mixing. Our experiments demonstrate that MSVMamba is highly competitive, with the MSVMamba-Tiny model achieving 83.0% top-1 accuracy on ImageNet, 46.9% box mAP, and 42.5% instance mAP with the Mask R-CNN framework, 1x training schedule on COCO, and 47.9% mIoU with single-scale testing on ADE20K. Code is available at https://github.com/YuHengsss/MSVMamba",
    "checked": true,
    "id": "1b62b9b36cd5adda75d0b26e8d18d94aa2f5729e",
    "semantic_title": "multi-scale vmamba: hierarchy in hierarchy visual state space model",
    "citation_count": 39,
    "authors": [
      "Yuheng Shi",
      "Minjing Dong",
      "Chang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2d779258dd899505b56f237de66ae470-Abstract-Conference.html": {
    "title": "AdanCA: Neural Cellular Automata As Adaptors For More Robust Vision Transformer",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) demonstrate remarkable performance in image classification through visual-token interaction learning, particularly when equipped with local information via region attention or convolutions. Although such architectures improve the feature aggregation from different granularities, they often fail to contribute to the robustness of the networks. Neural Cellular Automata (NCA) enables the modeling of global visual-token representations through local interactions, with its training strategies and architecture design conferring strong generalization ability and robustness against noisy input. In this paper, we propose Adaptor Neural Cellular Automata (AdaNCA) for Vision Transformers that uses NCA as plug-and-play adaptors between ViT layers, thus enhancing ViT's performance and robustness against adversarial samples as well as out-of-distribution inputs. To overcome the large computational overhead of standard NCAs, we propose Dynamic Interaction for more efficient interaction learning. Using our analysis of AdaNCA placement and robustness improvement, we also develop an algorithm for identifying the most effective insertion points for AdaNCA. With less than a 3% increase in parameters, AdaNCA contributes to more than 10% absolute improvement in accuracy under adversarial attacks on the ImageNet1K benchmark. Moreover, we demonstrate with extensive evaluations across eight robustness benchmarks and four ViT architectures that AdaNCA, as a plug-and-play module, consistently improves the robustness of ViTs",
    "checked": true,
    "id": "d52b041f93129a8170407bb85938f370327d2cd0",
    "semantic_title": "adanca: neural cellular automata as adaptors for more robust vision transformer",
    "citation_count": 2,
    "authors": [
      "Yitao Xu",
      "Tong Zhang",
      "Sabine Süsstrunk"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2d880acd7b31e25d45097455c8e8257f-Abstract-Conference.html": {
    "title": "PhyRecon: Physically Plausible Neural Scene Reconstruction",
    "volume": "main",
    "abstract": "We address the issue of physical implausibility in multi-view neural reconstruction. While implicit representations have gained popularity in multi-view 3D reconstruction, previous work struggles to yield physically plausible results, limiting their utility in domains requiring rigorous physical accuracy. This lack of plausibility stems from the absence of physics modeling in existing methods and their inability to recover intricate geometrical structures. In this paper, we introduce PHYRECON, the first approach to leverage both differentiable rendering and differentiable physics simulation to learn implicit surface representations. PHYRECON features a novel differentiable particle-based physical simulator built on neural implicit representations. Central to this design is an efficient transformation between SDF-based implicit representations and explicit surface points via our proposed Surface Points Marching Cubes (SP-MC), enabling differentiable learning with both rendering and physical losses. Additionally, PHYRECON models both rendering and physical uncertainty to identify and compensate for inconsistent and inaccurate monocular geometric priors. The physical uncertainty further facilitates physics-guided pixel sampling to enhance the learning of slender structures. By integrating these techniques, our model supports differentiable joint modeling of appearance, geometry, and physics. Extensive experiments demonstrate that PHYRECON significantly improves the reconstruction quality. Our results also exhibit superior physical stability in physical simulators, with at least a 40% improvement across all datasets, paving the way for future physics-based applications",
    "checked": true,
    "id": "ce1e604fb2ec79f25adb7f6a96cc52c5fc56d8cc",
    "semantic_title": "phyrecon: physically plausible neural scene reconstruction",
    "citation_count": 19,
    "authors": [
      "Junfeng Ni",
      "Yixin Chen",
      "Bohan Jing",
      "Nan Jiang",
      "Bin Wang",
      "Bo Dai",
      "Puhao Li",
      "Yixin Zhu",
      "Song-Chun Zhu",
      "Siyuan Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2d8f2351de4e9248d91ffa52dae2e6a2-Abstract-Conference.html": {
    "title": "Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling",
    "volume": "main",
    "abstract": "We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates.Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads.These theoretical insights are validated experimentally and offer natural suggestions for alternative architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingze Wang",
      "Weinan E"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2d950a2cfd8a75124c178a89545b97fd-Abstract-Conference.html": {
    "title": "A Combinatorial Algorithm for the Semi-Discrete Optimal Transport Problem",
    "volume": "main",
    "abstract": "Optimal Transport (OT, also known as the Wasserstein distance) is a popular metric for comparing probability distributions and has been successfully used in many machine-learning applications.In the semi-discrete $2$-Wasserstein problem, we wish to compute the cheapest way to transport all the mass from a continuous distribution $\\mu$ to a discrete distribution $\\nu$ in $\\mathbb{R}^d$ for $d\\ge 1$, where the cost of transporting unit mass between points $a$ and $b$ is $d(a,b)=||a-b||^2$. When both distributions are discrete, a simple combinatorial framework has been used to find the exact solution (see e.g. [Orlin, STOC 1988]). In this paper, we propose a combinatorial framework for the semi-discrete OT, which can be viewed as an extension of the combinatorial framework for the discrete OT but requires several new ideas. We present a new algorithm that given $\\mu$ and $\\nu$ in $\\mathbb{R}^2$ and a parameter $\\varepsilon>0$, computes an $\\varepsilon$-additive approximate semi-discrete transport plan in $O(n^{4}\\log n\\log \\frac{1}{\\varepsilon})$ time (in the worst case), where $n$ is the support-size of the discrete distribution $\\nu$ and we assume that the mass of $\\mu$ inside a triangle can be computed in $O(1)$ time. Our algorithm is significantly faster than the known algorithms, and unlike many numerical algorithms, it does not make any assumptions on the smoothness of $\\mu$.As an application of our algorithm, we describe a data structure to store a large discrete distribution $\\mu$ (with support size $N$) using $O(N)$ space so that, given a query discrete distribution $\\nu$ (with support size $k$), an $\\varepsilon$-additive approximate transport plan can be computed in $O(k^{3}\\sqrt{N}\\log \\frac{1}{\\varepsilon})$ time in $2$ dimensions. Our algorithm and data structure extend to higher dimensions as well as to $p$-Wasserstein problem for any $p \\ge 1$",
    "checked": true,
    "id": "29394afb16e252c028af765fb748011465326353",
    "semantic_title": "a combinatorial algorithm for the semi-discrete optimal transport problem",
    "citation_count": 0,
    "authors": [
      "Pankaj Agarwal",
      "Sharath Raghvendra",
      "Pouyan Shirzadian",
      "Keegan Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2d9c6cdb4cfe93869c090fea7375044b-Abstract-Conference.html": {
    "title": "Learning 1D Causal Visual Representation with De-focus Attention Networks",
    "volume": "main",
    "abstract": "Modality differences have led to the development of heterogeneous architectures for vision and language models. While images typically require 2D non-causal modeling, texts utilize 1D causal modeling. This distinction poses significant challenges in constructing unified multi-modal models. This paper explores the feasibility of representing images using 1D causal modeling. We identify an \"over-focus\" issue in existing 1D causal vision models, where attention overly concentrates on a small proportion of visual tokens. The issue of \"over-focus\" hinders the model's ability to extract diverse visual features and to receive effective gradients for optimization. To address this, we propose De-focus Attention Networks, which employ learnable bandpass filters to create varied attention patterns. During training, large and scheduled drop path rates, and an auxiliary loss on globally pooled features for global understanding tasks are introduced. These two strategies encourage the model to attend to a broader range of tokens and enhance network optimization. Extensive experiments validate the efficacy of our approach, demonstrating that 1D causal visual representation can perform comparably to 2D non-causal representation in tasks such as global perception, dense prediction, and multi-modal understanding. Code shall be released",
    "checked": true,
    "id": "e89458501637509b496345dd94ed9c47339565bd",
    "semantic_title": "learning 1d causal visual representation with de-focus attention networks",
    "citation_count": 2,
    "authors": [
      "Tao Chenxin",
      "Xizhou Zhu",
      "Shiqian Su",
      "Lewei Lu",
      "Changyao Tian",
      "Xuan Luo",
      "Gao Huang",
      "Hongsheng Li",
      "Yu Qiao",
      "Jie Zhou",
      "Jifeng Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2dab2f94544f9297d01a46a5453b93cd-Abstract-Conference.html": {
    "title": "Nonlinear dynamics of localization in neural receptive fields",
    "volume": "main",
    "abstract": "Localized receptive fields—neurons that are selective for certain contiguous spatiotemporal features of their input—populate early sensory regions of the mammalian brain. Unsupervised learning algorithms that optimize explicit sparsity or independence criteria replicate features of these localized receptive fields, but fail to explain directly how localization arises through learning without efficient coding, as occurs in early layers of deep neural networks and might occur in early sensory regions of biological systems. We consider an alternative model in which localized receptive fields emerge without explicit top-down efficiency constraints—a feed-forward neural network trained on a data model inspired by the structure of natural images. Previous work identified the importance of non-Gaussian statistics to localization in this setting but left open questions about the mechanisms driving dynamical emergence. We address these questions by deriving the effective learning dynamics for a single nonlinear neuron, making precise how higher-order statistical properties of the input data drive emergent localization, and we demonstrate that the predictions of these effective dynamics extend to the many-neuron setting. Our analysis provides an alternative explanation for the ubiquity of localization as resulting from the nonlinear dynamics of learning in neural circuits",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leon Lufkin",
      "Andrew Saxe",
      "Erin Grant"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2db08b94565c0d582cc53de6cee5fd47-Abstract-Conference.html": {
    "title": "Learning-Augmented Approximation Algorithms for Maximum Cut and Related Problems",
    "volume": "main",
    "abstract": "In recent years, there has been a surge of interest in the use of machine-learned predictions to bypass worst-case lower bounds for classical problems in combinatorial optimization. So far, the focus has mostly been on online algorithms, where information-theoretic barriers are overcome using predictions about the unknown future. In this paper, we consider the complementary question of using learned information to overcome computational barriers in the form of approximation hardness of polynomial-time algorithms for NP-hard (offline) problems. We show that noisy predictions about the optimal solution can be used to break classical hardness results for maximization problems such as the max-cut problem and more generally, maximization versions of constraint satisfaction problems (CSPs)",
    "checked": true,
    "id": "4b6e04aaa5a02b4f53c98570cf77642db14822c9",
    "semantic_title": "learning-augmented approximation algorithms for maximum cut and related problems",
    "citation_count": 3,
    "authors": [
      "Vincent Cohen-Addad",
      "Tommaso d’Orsi",
      "Anupam Gupta",
      "Euiwoong Lee",
      "Debmalya Panigrahi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2db8ce969b000fe0b3fb172490c33ce8-Abstract-Conference.html": {
    "title": "AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning",
    "volume": "main",
    "abstract": "Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task. Here, we introduce AvaTaR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task. During optimization, we design a comparator module to iteratively deliver insightful and comprehensive prompts to the LLM agent by contrastively reasoning between positive and negative examples sampled from training data. We demon- strate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information, and three general question-answering (QA) datasets. We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets. Code and dataset are available at https://github.com/zou-group/avatar",
    "checked": true,
    "id": "3dc1d321296f0f831863e5eb6f9a2ef93f70bf09",
    "semantic_title": "avatar: optimizing llm agents for tool usage via contrastive reasoning",
    "citation_count": 27,
    "authors": [
      "Shirley Wu",
      "Shiyu Zhao",
      "Qian Huang",
      "Kexin Huang",
      "Michihiro Yasunaga",
      "Kaidi Cao",
      "Vassilis Ioannidis",
      "Karthik Subbian",
      "Jure Leskovec",
      "James Y Zou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2ded44d59f5094eed0d02132fe75b60d-Abstract-Conference.html": {
    "title": "Score Distillation via Reparametrized DDIM",
    "volume": "main",
    "abstract": "While 2D diffusion models generate realistic, high-detail images, 3D shape generation methods like Score Distillation Sampling (SDS) built on these 2D diffusion models produce cartoon-like, over-smoothed shapes. To help explain this discrepancy, we show that the image guidance used in Score Distillation can be understood as the velocity field of a 2D denoising generative process, up to the choice of a noise term. In particular, after a change of variables, SDS resembles a high-variance version of Denoising Diffusion Implicit Models (DDIM) with a differently-sampled noise term: SDS introduces noise i.i.d. randomly at each step, while DDIM infers it from the previous noise predictions. This excessive variance can lead to over-smoothing and unrealistic outputs. We show that a better noise approximation can be recovered by inverting DDIM in each SDS update step. This modification makes SDS's generative process for 2D images almost identical to DDIM. In 3D, it removes over-smoothing, preserves higher-frequency detail, and brings the generation quality closer to that of 2D samplers. Experimentally, our method achieves better or similar 3D generation quality compared to other state-of-the-art Score Distillation methods, all without training additional neural networks or multi-view supervision, and providing useful insights into relationship between 2D and 3D asset generation with diffusion models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artem Lukoianov",
      "Haitz Sáez de Ocáriz Borde",
      "Kristjan Greenewald",
      "Vitor Guizilini",
      "Timur Bagautdinov",
      "Vincent Sitzmann",
      "Justin M Solomon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2e098001625a8b8f22bf6c1aef09c4e5-Abstract-Conference.html": {
    "title": "Smoke and Mirrors in Causal Downstream Tasks",
    "volume": "main",
    "abstract": "Machine Learning and AI have the potential to transform data-driven scientific discovery, enabling accurate predictions for several scientific phenomena. As many scientific questions are inherently causal, this paper looks at the causal inference task of treatment effect estimation, where the outcome of interest is recorded in high-dimensional observations in a Randomized Controlled Trial (RCT). Despite being the simplest possible causal setting and a perfect fit for deep learning, we theoretically find that many common choices in the literature may lead to biased estimates. To test the practical impact of these considerations, we recorded ISTAnt, the first real-world benchmark for causal inference downstream tasks on high-dimensional observations as an RCT studying how garden ants (Lasius neglectus) respond to microparticles applied onto their colony members by hygienic grooming. Comparing 6 480 models fine-tuned from state-of-the-art visual backbones, we find that the sampling and modeling choices significantly affect the accuracy of the causal estimate, and that classification accuracy is not a proxy thereof. We further validated the analysis, repeating it on a synthetically generated visual data set controlling the causal model. Our results suggest that future benchmarks should carefully consider real downstream scientific questions, especially causal ones. Further, we highlight guidelines for representation learning methods to help answer causal questions in the sciences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Cadei",
      "Lukas Lindorfer",
      "Sylvia Cremer",
      "Cordelia Schmid",
      "Francesco Locatello"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2e102d937d094b7211c4d32ce1f1126c-Abstract-Conference.html": {
    "title": "Newton Losses: Using Curvature Information for Learning with Differentiable Algorithms",
    "volume": "main",
    "abstract": "When training neural networks with custom objectives, such as ranking losses and shortest-path losses, a common problem is that they are, per se, non-differentiable. A popular approach is to continuously relax the objectives to provide gradients, enabling learning. However, such differentiable relaxations are often non-convex and can exhibit vanishing and exploding gradients, making them (already in isolation) hard to optimize. Here, the loss function poses the bottleneck when training a deep neural network. We present Newton Losses, a method for improving the performance of existing hard to optimize losses by exploiting their second-order information via their empirical Fisher and Hessian matrices. Instead of training the neural network with second-order techniques, we only utilize the loss function's second-order information to replace it by a Newton Loss, while training the network with gradient descent. This makes our method computationally efficient. We apply Newton Losses to eight differentiable algorithms for sorting and shortest-paths, achieving significant improvements for less-optimized differentiable algorithms, and consistent improvements, even for well-optimized differentiable algorithms",
    "checked": true,
    "id": "35e5299d7526c6c03347ad133c4379d6e50d5b85",
    "semantic_title": "newton losses: using curvature information for learning with differentiable algorithms",
    "citation_count": 1,
    "authors": [
      "Felix Petersen",
      "Christian Borgelt",
      "Tobias Sutter",
      "Hilde Kuehne",
      "Oliver Deussen",
      "Stefano Ermon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2e163450c1ae3167832971e6da29f38d-Abstract-Conference.html": {
    "title": "Capturing the denoising effect of PCA via compression ratio",
    "volume": "main",
    "abstract": "Principal component analysis (PCA) is one of the most fundamental tools in machine learning with broad use as a dimensionality reduction and denoising tool. In the later setting, while PCA is known to be effective at subspace recovery and is proven to aid clustering algorithms in some specific settings, its improvement of noisy data is still not well quantified in general. In this paper, we propose a novel metric called compression ratio to capture the effect of PCA on high-dimensional noisy data.We show that, for data with underlying community structure, PCA significantly reduces the distance of data points belonging to the same community while reducing inter-community distance relatively mildly. We explain this phenomenon through both theoretical proofs and experiments on real-world data. Building on this new metric, we design a straightforward algorithm that could be used to detect outliers. Roughly speaking, we argue that points that have a lower variance of compression ratio do not share a common signal with others (hence could be considered outliers).We provide theoretical justification for this simple outlier detection algorithm and use simulations to demonstrate that our method is competitive with popular outlier detection tools. Finally, we run experiments on real-world high-dimension noisy data (single-cell RNA-seq) to show that removing points from these datasets via our outlier detection method improves the accuracy of clustering algorithms. Our method is very competitive with popular outlier detection tools in this task",
    "checked": true,
    "id": "eb0b4de71ceb02f7b0588789870229b680470661",
    "semantic_title": "capturing the denoising effect of pca via compression ratio",
    "citation_count": 4,
    "authors": [
      "Chandra Sekhar Mukherjee",
      "Nikhil Deorkar",
      "Jiapeng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2e3073cb65608aa887bb945c382e687f-Abstract-Conference.html": {
    "title": "Automated Multi-level Preference for MLLMs",
    "volume": "main",
    "abstract": "Current multimodal Large Language Models (MLLMs) suffer from ''hallucination'', occasionally generating responses that are not grounded in the input images. To tackle this challenge, one promising path is to utilize reinforcement learning from human feedback (RLHF), which steers MLLMs towards learning superior responses while avoiding inferior ones. We rethink the common practice of using binary preferences (i.e., superior, inferior), and find that adopting multi-level preferences (e.g., superior, medium, inferior) is better for two benefits: 1) It narrows the gap between adjacent levels, thereby encouraging MLLMs to discern subtle differences. 2) It further integrates cross-level comparisons (beyond adjacent-level comparisons), thus providing a broader range of comparisons with hallucination examples. To verify our viewpoint, we present the Automated Multi-level Preference (AMP) framework for MLLMs. To facilitate this framework, we first develop an automated dataset generation pipeline that provides high-quality multi-level preference datasets without any human annotators. Furthermore, we design the Multi-level Direct Preference Optimization (MDPO) algorithm to robustly conduct complex multi-level preference learning. Additionally, we propose a new hallucination benchmark, MRHal-Bench. Extensive experiments across public hallucination and general benchmarks, as well as our MRHal-Bench, demonstrate the effectiveness of our proposed method. Code is available at https://github.com/takomc/amp",
    "checked": true,
    "id": "de103054dfeec44c2643b5e82707a81aa3737375",
    "semantic_title": "automated multi-level preference for mllms",
    "citation_count": 17,
    "authors": [
      "Mengxi Zhang",
      "Wenhao Wu",
      "Yu Lu",
      "YuXin Song",
      "KANG RONG",
      "Huanjin Yao",
      "Jianbo Zhao",
      "Fanglong Liu",
      "Haocheng Feng",
      "Jingdong Wang",
      "Yifan Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2e32d3a10985fc94c7e11ee6ea165cca-Abstract-Conference.html": {
    "title": "What Matters in Graph Class Incremental Learning? An Information Preservation Perspective",
    "volume": "main",
    "abstract": "Graph class incremental learning (GCIL) requires the model to classify emerging nodes of new classes while remembering old classes. Existing methods are designed to preserve effective information of old models or graph data to alleviate forgetting, but there is no clear theoretical understanding of what matters in information preservation. In this paper, we consider that present practice suffers from high semantic and structural shifts assessed by two devised shift metrics. We provide insights into information preservation in GCIL and find that maintaining graph information can preserve information of old models in theory to calibrate node semantic and graph structure shifts. We correspond graph information into low-frequency local-global information and high-frequency information in spatial domain. Based on the analysis, we propose a framework, Graph Spatial Information Preservation (GSIP). Specifically, for low-frequency information preservation, the old node representations obtained by inputting replayed nodes into the old model are aligned with the outputs of the node and its neighbors in the new model, and then old and new outputs are globally matched after pooling. For high-frequency information preservation, the new node representations are encouraged to imitate the near-neighbor pair similarity of old node representations. GSIP achieves a 10\\% increase in terms of the forgetting metric compared to prior methods on large-scale datasets. Our framework can also seamlessly integrate existing replay designs. The code is available through https://github.com/Jillian555/GSIP",
    "checked": true,
    "id": "9d0f2293a102c8e83c9ea139f6adbeeccf6047dd",
    "semantic_title": "what matters in graph class incremental learning? an information preservation perspective",
    "citation_count": 2,
    "authors": [
      "Jialu Li",
      "Yu Wang",
      "Pengfei Zhu",
      "Wanyu Lin",
      "Qinghua Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2e4531a45b99f61947b23ccdd608303b-Abstract-Conference.html": {
    "title": "Are High-Degree Representations Really Unnecessary in Equivariant Graph Neural Networks?",
    "volume": "main",
    "abstract": "Equivariant Graph Neural Networks (GNNs) that incorporate E(3) symmetry have achieved significant success in various scientific applications. As one of the most successful models, EGNN leverages a simple scalarization technique to perform equivariant message passing over only Cartesian vectors (i.e., 1st-degree steerable vectors), enjoying greater efficiency and efficacy compared to equivariant GNNs using higher-degree steerable vectors. This success suggests that higher-degree representations might be unnecessary. In this paper, we disprove this hypothesis by exploring the expressivity of equivariant GNNs on symmetric structures, including $k$-fold rotations and regular polyhedra. We theoretically demonstrate that equivariant GNNs will always degenerate to a zero function if the degree of the output representations is fixed to 1 or other specific values. Based on this theoretical insight, we propose HEGNN, a high-degree version of EGNN to increase the expressivity by incorporating high-degree steerable vectors while maintaining EGNN's efficiency through the scalarization trick. Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on toy datasets consisting of symmetric structures, but also shows substantial improvements on more complicated datasets such as $N$-body and MD17. Our theoretical findings and empirical results potentially open up new possibilities for the research of equivariant GNNs",
    "checked": true,
    "id": "d0cd5ede6535f617e40b58517fe593b648b737b0",
    "semantic_title": "are high-degree representations really unnecessary in equivariant graph neural networks?",
    "citation_count": 8,
    "authors": [
      "Jiacheng Cen",
      "Anyi Li",
      "Ning Lin",
      "Yuxiang Ren",
      "Zihe Wang",
      "Wenbing Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2e5060adc71166792bc6e5251240eba4-Abstract-Conference.html": {
    "title": "Improving Decision Sparsity",
    "volume": "main",
    "abstract": "Sparsity is a central aspect of interpretability in machine learning. Typically, sparsity is measured in terms of the size of a model globally, such as the number of variables it uses. However, this notion of sparsity is not particularly relevant for decision making; someone subjected to a decision does not care about variables that do not contribute to the decision. In this work, we dramatically expand a notion of decision sparsity called the Sparse Explanation Value (SEV) so that its explanations are more meaningful. SEV considers movement along a hypercube towards a reference point. By allowing flexibility in that reference and by considering how distances along the hypercube translate to distances in feature space, we can derive sparser and more meaningful explanations for various types of function classes. We present cluster-based SEV and its variant tree-based SEV, introduce a method that improves credibility of explanations, and propose algorithms that optimize decision sparsity in machine learning models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Sun",
      "Tong Wang",
      "Cynthia Rudin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2e53c02ea028cbf603f4b6b47fef3d97-Abstract-Conference.html": {
    "title": "Cluster-Learngene: Inheriting Adaptive Clusters for Vision Transformers",
    "volume": "main",
    "abstract": "In recent years, the merging of vast datasets with powerful computational resources has led to the emergence of large pre-trained models in the field of deep learning. However, the common practices often overgeneralize the applicability of these models, overlooking the task-specific resource constraints. To mitigate this issue, we propose \\textbf{Cluster-Learngene}, which effectively clusters critical internal modules from a large ancestry model and then inherits them to initialize descendant models of elastic scales. Specifically, based on the density characteristics of attention heads, our method adaptively clusters attention heads of each layer and position-wise feed-forward networks (FFNs) in the ancestry model as the learngene. Moreover, we introduce priority weight-sharing and learnable parameter transformations that expand the learngene to initialize descendant models of elastic scales. Through extensive experimentation, we demonstrate that Cluster-Learngene not only is more efficient compared to other initialization methods but also customizes models of elastic scales according to downstream task resources",
    "checked": true,
    "id": "9bc80b17c81780c1e1b432067905d66451ba4ca2",
    "semantic_title": "cluster-learngene: inheriting adaptive clusters for vision transformers",
    "citation_count": 3,
    "authors": [
      "Qiufeng Wang",
      "Xu Yang",
      "Fu Feng",
      "Jingq Wang",
      "Xin Geng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2e5a74735e5ada01d85afbb49caad36f-Abstract-Conference.html": {
    "title": "Multi-Instance Partial-Label Learning with Margin Adjustment",
    "volume": "main",
    "abstract": "Multi-instance partial-label learning (MIPL) is an emerging learning framework where each training sample is represented as a multi-instance bag associated with a candidate label set. Existing MIPL algorithms often overlook the margins for attention scores and predicted probabilities, leading to suboptimal generalization performance. A critical issue with these algorithms is that the highest prediction probability of the classifier may appear on a non-candidate label. In this paper, we propose an algorithm named MIPLMA, i.e., Multi-Instance Partial-Label learning with Margin Adjustment, which adjusts the margins for attention scores and predicted probabilities. We introduce a margin-aware attention mechanism to dynamically adjust the margins for attention scores and propose a margin distributionloss to constrain the margins between the predicted probabilities on candidate and non-candidate label sets. Experimental results demonstrate the superior performance of MIPLMA over existing MIPL algorithms, as well as other well-established multi-instance learning algorithms and partial-label learning algorithms",
    "checked": false,
    "id": "49f3ef070a4d2bbcfc7ed770dac3f18e9916f521",
    "semantic_title": "a multi-class partial hinge loss for partial label learning",
    "citation_count": 4,
    "authors": [
      "Wei Tang",
      "Yin-Fang Yang",
      "Zhaofei Wang",
      "Weijia Zhang",
      "Min-Ling Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2e604995c727e3cd37c52abd6e48cb34-Abstract-Conference.html": {
    "title": "Geometric Exploitation for Indoor Panoramic Semantic Segmentation",
    "volume": "main",
    "abstract": "PAnoramic Semantic Segmentation (PASS) is an important task in computer vision,as it enables semantic understanding of a 360° environment. Currently,most of existing works have focused on addressing the distortion issues in 2Dpanoramic images without considering spatial properties of indoor scene. Thisrestricts PASS methods in perceiving contextual attributes to deal with the ambiguitywhen working with monocular images. In this paper, we propose a novelapproach for indoor panoramic semantic segmentation. Unlike previous works,we consider the panoramic image as a composition of segment groups: oversampledsegments, representing planar structures such as floors and ceilings, andunder-sampled segments, representing other scene elements. To optimize eachgroup, we first enhance over-sampled segments by jointly optimizing with a densedepth estimation task. Then, we introduce a transformer-based context modulethat aggregates different geometric representations of the scene, combinedwith a simple high-resolution branch, it serves as a robust hybrid decoder forestimating under-sampled segments, effectively preserving the resolution of predictedmasks while leveraging various indoor geometric properties. Experimentalresults on both real-world (Stanford2D3DS, Matterport3D) and synthetic (Structured3D)datasets demonstrate the robustness of our framework, by setting newstate-of-the-arts in almost evaluations, The code and updated results are availableat: https://github.com/caodinhduc/verticalrelativedistance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duc Cao Dinh",
      "Seok Joon Kim",
      "Kyusung Cho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2e622ac74f66df03b686a12e2e0e4424-Abstract-Conference.html": {
    "title": "Unified Gradient-Based Machine Unlearning with Remain Geometry Enhancement",
    "volume": "main",
    "abstract": "Machine unlearning (MU) has emerged to enhance the privacy and trustworthiness of deep neural networks. Approximate MU is a practical method for large-scale models. Our investigation into approximate MU starts with identifying the steepest descent direction, minimizing the output Kullback-Leibler divergence to exact MU inside a parameters' neighborhood. This probed direction decomposes into three components: weighted forgetting gradient ascent, fine-tuning retaining gradient descent, and a weight saliency matrix. Such decomposition derived from Euclidean metric encompasses most existing gradient-based MU methods. Nevertheless, adhering to Euclidean space may result in sub-optimal iterative trajectories due to the overlooked geometric structure of the output probability space. We suggest embedding the unlearning update into a manifold rendered by the remaining geometry, incorporating second-order Hessian from the remaining data. It helps prevent effective unlearning from interfering with the retained performance. However, computing the second-order Hessian for large-scale models is intractable. To efficiently leverage the benefits of Hessian modulation, we propose a fast-slow parameter update strategy to implicitly approximate the up-to-date salient unlearning direction.Free from specific modal constraints, our approach is adaptable across computer vision unlearning tasks, including classification and generation. Extensive experiments validate our efficacy and efficiency. Notably, our method successfully performs class-forgetting on ImageNet using DiT and forgets a class on CIFAR-10 using DDPM in just 50 steps, compared to thousands of steps required by previous methods. Code is available at Unified-Unlearning-w-Remain-Geometry",
    "checked": true,
    "id": "2d94f223676db1cdcb3b6bc8cd3a972f95ae461a",
    "semantic_title": "unified gradient-based machine unlearning with remain geometry enhancement",
    "citation_count": 11,
    "authors": [
      "Zhehao Huang",
      "Xinwen Cheng",
      "JingHao Zheng",
      "Haoran Wang",
      "Zhengbao He",
      "Tao Li",
      "Xiaolin Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2e8eaf43f20948ad878e6b8902797d1e-Abstract-Conference.html": {
    "title": "Policy Mirror Descent with Lookahead",
    "volume": "main",
    "abstract": "Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art reinforcement learning (RL) algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement. However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart. In this work, we propose a new class of PMD algorithms called $h$-PMD which incorporates multi-step greedy policy improvement with lookahead depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov Decision Processes with discount factor $\\gamma$, we show that $h$-PMD which generalizes the standard PMD enjoys a faster dimension-free $\\gamma^h$-linear convergence rate, contingent on the computation of multi-step greedy policies. We propose an inexact version of $h$-PMD where lookahead action values are estimated. Under a generative model, we establish a sample complexity for $h$-PMD which improves over prior work. Finally, we extend our result to linear function approximation to scale to large state spaces. Under suitable assumptions, our sample complexity only involves dependence on the dimension of the feature map space instead of the state space size",
    "checked": true,
    "id": "e729aa0b7a38fb68ed44ba595b415ef307589923",
    "semantic_title": "policy mirror descent with lookahead",
    "citation_count": 2,
    "authors": [
      "Kimon Protopapas",
      "Anas Barakat"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2e9d5bcfa9c32d360bae3d4e3d9cc032-Abstract-Conference.html": {
    "title": "Generalization of Hamiltonian algorithms",
    "volume": "main",
    "abstract": "A method to prove generalization results for a class of stochastic learning algorithms is presented. It applies whenever the algorithm generates a distribution, which is absolutely continuous distribution relative to some a-priori measure, and the logarithm of its density is exponentially concentrated about its mean. Applications include bounds for the Gibbs algorithm and randomizations of stable deterministic algorithms, combinations thereof and PAC-Bayesian bounds with data-dependent priors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Maurer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2ea07a4acbf7e38913062fd69a70805f-Abstract-Conference.html": {
    "title": "Stochastic Optimization Algorithms for Instrumental Variable Regression with Streaming Data",
    "volume": "main",
    "abstract": "We develop and analyze algorithms for instrumental variable regression by viewing the problem as a conditional stochastic optimization problem. In the context of least-squares instrumental variable regression, our algorithms neither require matrix inversions nor mini-batches thereby providing a fully online approach for performing instrumental variable regression with streaming data. When the true model is linear, we derive rates of convergence in expectation, that are of order $\\mathcal{O}(\\log T/T)$ and $\\mathcal{O}(1/T^{1-\\epsilon})$ for any $\\epsilon>0$, respectively under the availability of two-sample and one-sample oracles respectively. Importantly, under the availability of the two-sample oracle, the aforementioned rate is actually agnostic to the relationship between confounder and the instrumental variable demonstrating the flexibility of the proposed approach in alleviating the need for explicit model assumptions required in recent works based on reformulating the problem as min-max optimization problems. Experimental validation is provided to demonstrate the advantages of the proposed algorithms over classical approaches like the 2SLS method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuxing Chen",
      "Abhishek Roy",
      "Yifan Hu",
      "Krishnakumar Balasubramanian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2eb31e84089286505c27dd5d5ea7f683-Abstract-Conference.html": {
    "title": "Constrained Diffusion Models via Dual Training",
    "volume": "main",
    "abstract": "Diffusion models have attained prominence for their ability to synthesize a probability distribution for a given dataset via a diffusion process, enabling the generation of new data points with high fidelity. However, diffusion processes are prone to generating samples that reflect biases in a training dataset. To address this issue, we develop constrained diffusion models by imposing diffusion constraints based on desired distributions that are informed by requirements. Specifically, we cast the training of diffusion models under requirements as a constrained distribution optimization problem that aims to reduce the distribution difference between original and generated data while obeying constraints on the distribution of generated data. We show that our constrained diffusion models generate new data from a mixture data distribution that achieves the optimal trade-off among objective and constraints. To train constrained diffusion models, we develop a dual training algorithm and characterize the optimality of the trained constrained diffusion model. We empirically demonstrate the effectiveness of our constrained models in two constrained generation tasks: (i) we consider a dataset with one or more underrepresented classes where we train the model with constraints to ensure fairly sampling from all classes during inference; (ii) we fine-tune a pre-trained diffusion model to sample from a new dataset while avoiding overfitting",
    "checked": true,
    "id": "18a029f81cef994349c02d4454962a4214b9cab3",
    "semantic_title": "constrained diffusion models via dual training",
    "citation_count": 4,
    "authors": [
      "Shervin Khalafi",
      "Dongsheng Ding",
      "Alejandro Ribeiro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2ee1c87245956e3eaa71aaba5f5753eb-Abstract-Conference.html": {
    "title": "Better by default: Strong pre-tuned MLPs and boosted trees on tabular data",
    "volume": "main",
    "abstract": "For classification and regression on tabular data, the dominance of gradient-boosted decision trees (GBDTs) has recently been challenged by often much slower deep learning methods with extensive hyperparameter tuning. We address this discrepancy by introducing (a) RealMLP, an improved multilayer perceptron (MLP), and (b) strong meta-tuned default parameters for GBDTs and RealMLP. We tune RealMLP and the default parameters on a meta-train benchmark with 118 datasets and compare them to hyperparameter-optimized versions on a disjoint meta-test benchmark with 90 datasets, as well as the GBDT-friendly benchmark by Grinsztajn et al. (2022). Our benchmark results on medium-to-large tabular datasets (1K--500K samples) show that RealMLP offers a favorable time-accuracy tradeoff compared to other neural baselines and is competitive with GBDTs in terms of benchmark scores. Moreover, a combination of RealMLP and GBDTs with improved default parameters can achieve excellent results without hyperparameter tuning. Finally, we demonstrate that some of RealMLP's improvements can also considerably improve the performance of TabR with default parameters",
    "checked": true,
    "id": "1fa945c69d2b67d49af88734d3b5fa527d24f910",
    "semantic_title": "better by default: strong pre-tuned mlps and boosted trees on tabular data",
    "citation_count": 35,
    "authors": [
      "David Holzmüller",
      "Leo Grinsztajn",
      "Ingo Steinwart"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2f050fa9f0d898e3f265d515f50ae8f9-Abstract-Conference.html": {
    "title": "DAPE: Data-Adaptive Positional Encoding for Length Extrapolation",
    "volume": "main",
    "abstract": "Positional encoding plays a crucial role in transformers, significantly impact- ing model performance and length generalization. Prior research has introduced absolute positional encoding (APE) and relative positional encoding (RPE) to distinguish token positions in given sequences. However, both APE and RPE remain fixed after model training regardless of input data, limiting their adaptability and flexibility. Hence, we expect that the desired positional encoding should be data-adaptive and can be dynamically adjusted with the given attention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE) method, which dynamically and semantically adjusts based on input context and learned fixed priors. Experimental validation on real-world datasets (Arxiv, Books3, and CHE) demonstrates that DAPE enhances model performances in terms of trained length and length generalization, where the improvements are statistically significant. The model visualization suggests that our model can keep both local and anti-local information. Finally, we successfully train the model on sequence length 128 and achieve better performance at evaluation sequence length 8192, compared with other static positional encoding methods, revealing the benefit of the adaptive positional encoding method",
    "checked": true,
    "id": "77f6f70da3aaa9da0e6da8288a206539e30a07be",
    "semantic_title": "dape: data-adaptive positional encoding for length extrapolation",
    "citation_count": 13,
    "authors": [
      "Chuanyang Zheng",
      "Yihang Gao",
      "Han Shi",
      "Minbin Huang",
      "Jingyao Li",
      "Jing Xiong",
      "Xiaozhe Ren",
      "Michael Ng",
      "Xin Jiang",
      "Zhenguo Li",
      "Yu Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2f0728449cb3150189d765fc87afc913-Abstract-Conference.html": {
    "title": "LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential Recommendation",
    "volume": "main",
    "abstract": "Sequential recommender systems (SRS) aim to predict users' subsequent choices based on their historical interactions and have found applications in diverse fields such as e-commerce and social media. However, in real-world systems, most users interact with only a handful of items, while the majority of items are seldom consumed. These two issues, known as the long-tail user and long-tail item challenges, often pose difficulties for existing SRS. These challenges can adversely affect user experience and seller benefits, making them crucial to address. Though a few works have addressed the challenges, they still struggle with the seesaw or noisy issues due to the intrinsic scarcity of interactions. The advancements in large language models (LLMs) present a promising solution to these problems from a semantic perspective. As one of the pioneers in this field, we propose the Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR). This framework utilizes semantic embeddings derived from LLMs to enhance SRS without adding extra inference load. To address the long-tail item challenge, we design a dual-view modeling framework that combines semantics from LLMs and collaborative signals from conventional SRS. For the long-tail user challenge, we propose a retrieval augmented self-distillation method to enhance user preference representation using more informative interactions from similar users. To verify the effectiveness and versatility of our proposed enhancement framework, we conduct extensive experiments on three real-world datasets using three popular SRS models. The results consistently show that our method surpasses existing baselines. The implementation code is available in Supplementary Material",
    "checked": true,
    "id": "46d382ef09f6da85e6d56b96bf48502990cf4fd2",
    "semantic_title": "llm-esr: large language models enhancement for long-tailed sequential recommendation",
    "citation_count": 28,
    "authors": [
      "Qidong Liu",
      "Xian Wu",
      "Yejing Wang",
      "Zijian Zhang",
      "Feng Tian",
      "Yefeng Zheng",
      "Xiangyu Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2f0bb736ccc8551ef5bcc9165c2a4d9e-Abstract-Conference.html": {
    "title": "Achieving Tractable Minimax Optimal Regret in Average Reward MDPs",
    "volume": "main",
    "abstract": "In recent years, significant attention has been directed towards learning average-reward Markov Decision Processes (MDPs).However, existing algorithms either suffer from sub-optimal regret guarantees or computational inefficiencies.In this paper, we present the first *tractable* algorithm with minimax optimal regret of $\\mathrm{O}\\left(\\sqrt{\\mathrm{sp}(h^*) S A T \\log(SAT)}\\right)$ where $\\mathrm{sp}(h^*)$ is the span of the optimal bias function $h^*$, $S\\times A$ is the size of the state-action space and $T$ the number of learning steps. Remarkably, our algorithm does not require prior information on $\\mathrm{sp}(h^*)$. Our algorithm relies on a novel subroutine, **P**rojected **M**itigated **E**xtended **V**alue **I**teration (`PMEVI`), to compute bias-constrained optimal policies efficiently. This subroutine can be applied to various previous algorithms to obtain improved regret bounds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Boone",
      "Zihan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2f1232aa8c790447419d3aadbc9927b4-Abstract-Conference.html": {
    "title": "Universality of AdaGrad Stepsizes for Stochastic Optimization: Inexact Oracle, Acceleration and Variance Reduction",
    "volume": "main",
    "abstract": "We present adaptive gradient methods (both basic and accelerated) for solvingconvex composite optimization problems in which the main part is approximatelysmooth (a.k.a. $(\\delta, L)$-smooth) and can be accessed only via a(potentially biased) stochastic gradient oracle.This setting covers many interesting examples including Hölder smooth problemsand various inexact computations of the stochastic gradient.Our methods use AdaGrad stepsizes and are adaptive in the sense that they donot require knowing any problem-dependent constants except an estimate of thediameter of the feasible set but nevertheless achieve the best possibleconvergence rates as if they knew the corresponding constants.We demonstrate that AdaGrad stepsizes work in a variety of situationsby proving, in a unified manner, three types of new results.First, we establish efficiency guarantees for our methods in the classicalsetting where the oracle's variance is uniformly bounded.We then show that, under more refined assumptions on the variance,the same methods without any modifications enjoy implicit variancereduction properties allowing us to express their complexity estimates interms of the variance only at the minimizer.Finally, we show how to incorporate explicit SVRG-type variance reduction intoour methods and obtain even faster algorithms.In all three cases, we present both basic and accelerated algorithmsachieving state-of-the-art complexity bounds.As a direct corollary of our results, we obtain universal stochastic gradientmethods for Hölder smooth problems which can be used in all situations",
    "checked": true,
    "id": "ca771dfe3449d406232beace04b0a935aeddaca1",
    "semantic_title": "universality of adagrad stepsizes for stochastic optimization: inexact oracle, acceleration and variance reduction",
    "citation_count": 6,
    "authors": [
      "Anton Rodomanov",
      "Xiaowen Jiang",
      "Sebastian U Stich"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2f1486343c2c942a617e4f5bb0cc64c8-Abstract-Conference.html": {
    "title": "When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search",
    "volume": "main",
    "abstract": "Recent studies developed jailbreaking attacks, which construct jailbreaking prompts to \"fool\" LLMs into responding to harmful questions.Early-stage jailbreaking attacks require access to model internals or significant human efforts. More advanced attacks utilize genetic algorithms for automatic and black-box attacks.However, the random nature of genetic algorithms significantly limits the effectiveness of these attacks.In this paper, we propose RLbreaker, a black-box jailbreaking attack driven by deep reinforcement learning (DRL).We model jailbreaking as a search problem and design an RL agent to guide the search, which is more effective and has less randomness than stochastic search, such as genetic algorithms.Specifically, we design a customized DRL system for the jailbreaking problem, including a novel reward function and a customized proximal policy optimization (PPO) algorithm.Through extensive experiments, we demonstrate that RLbreaker is much more effective than existing jailbreaking attacks against six state-of-the-art (SOTA) LLMs. We also show that RLbreaker is robust against three SOTA defenses and its trained agents can transfer across different LLMs.We further validate the key design choices of RLbreaker via a comprehensive ablation study",
    "checked": true,
    "id": "ff9854d4323e75252fec985d411f3b058eb718bf",
    "semantic_title": "when llm meets drl: advancing jailbreaking efficiency via drl-guided search",
    "citation_count": 25,
    "authors": [
      "Xuan Chen",
      "Yuzhou Nie",
      "Wenbo Guo",
      "Xiangyu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2f32be3e112e151707cb12528bdfa7d5-Abstract-Conference.html": {
    "title": "Diffusion-based Layer-wise Semantic Reconstruction for Unsupervised Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "Unsupervised out-of-distribution (OOD) detection aims to identify out-of-domain data by learning only from unlabeled In-Distribution (ID) training samples, which is crucial for developing a safe real-world machine learning system. Current reconstruction-based method provides a good alternative approach, by measuring the reconstruction error between the input and its corresponding generative counterpart in the pixel/feature space. However, such generative methods face the key dilemma, $i.e.$, improving the reconstruction power of the generative model, while keeping compact representation of the ID data. To address this issue, we propose the diffusion-based layer-wise semantic reconstruction approach for unsupervised OOD detection. The innovation of our approach is that we leverage the diffusion model's intrinsic data reconstruction ability to distinguish ID samples from OOD samples in the latent feature space. Moreover, to set up a comprehensive and discriminative feature representation, we devise a multi-layer semantic feature extraction strategy. Through distorting the extracted features with Gaussian noises and applying the diffusion model for feature reconstruction, the separation of ID and OOD samples is implemented according to the reconstruction errors. Extensive experimental results on multiple benchmarks built upon various datasets demonstrate that our method achieves state-of-the-art performance in terms of detection accuracy and speed",
    "checked": true,
    "id": "22cecfdaa17c3dea448365e1d051de7b00c66ed4",
    "semantic_title": "diffusion-based layer-wise semantic reconstruction for unsupervised out-of-distribution detection",
    "citation_count": 1,
    "authors": [
      "Ying Yang",
      "De Cheng",
      "Chaowei Fang",
      "Yubiao Wang",
      "Changzhe Jiao",
      "Lechao Cheng",
      "Nannan Wang",
      "Xinbo Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2f46ef5725a8eca24f7f24a17955ad1a-Abstract-Conference.html": {
    "title": "Fast samplers for Inverse Problems in Iterative Refinement models",
    "volume": "main",
    "abstract": "Constructing fast samplers for unconditional diffusion and flow-matching models has received much attention recently; however, existing methods for solving inverse problems, such as super-resolution, inpainting, or deblurring, still require hundreds to thousands of iterative steps to obtain high-quality results. We propose a plug-and-play framework for constructing efficient samplers for inverse problems, requiring only pre-trained diffusion or flow-matching models. We present Conditional Conjugate Integrators, which leverage the specific form of the inverse problem to project the respective conditional diffusion/flow dynamics into a more amenable space for sampling. Our method complements popular posterior approximation methods for solving inverse problems using diffusion/flow models. We evaluate the proposed method's performance on various linear image restoration tasks across multiple datasets, employing diffusion and flow-matching models. Notably, on challenging inverse problems like 4x super-resolution on the ImageNet dataset, our method can generate high-quality samples in as few as 5 conditional sampling steps and outperforms competing baselines requiring 20-1000 steps. Our code will be publicly available at https://github.com/mandt-lab/c-pigdm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kushagra Pandey",
      "Ruihan Yang",
      "Stephan Mandt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2f55a8b7b1c2c6312eb86557bb9a2bd5-Abstract-Conference.html": {
    "title": "Advancing Spiking Neural Networks for Sequential Modeling with Central Pattern Generators",
    "volume": "main",
    "abstract": "Spiking neural networks (SNNs) represent a promising approach to developing artificial neural networks that are both energy-efficient and biologically plausible.However, applying SNNs to sequential tasks, such as text classification and time-series forecasting, has been hindered by the challenge of creating an effective and hardware-friendly spike-form positional encoding (PE) strategy.Drawing inspiration from the central pattern generators (CPGs) in the human brain, which produce rhythmic patterned outputs without requiring rhythmic inputs, we propose a novel PE technique for SNNs, termed CPG-PE.We demonstrate that the commonly used sinusoidal PE is mathematically a specific solution to the membrane potential dynamics of a particular CPG.Moreover, extensive experiments across various domains, including time-series forecasting, natural language processing, and image classification, show that SNNs with CPG-PE outperform their conventional counterparts.Additionally, we perform analysis experiments to elucidate the mechanism through which SNNs encode positional information and to explore the function of CPGs in the human brain.This investigation may offer valuable insights into the fundamental principles of neural computation",
    "checked": true,
    "id": "d7127ef38b127692840d9ae9266a9011cb29376e",
    "semantic_title": "advancing spiking neural networks for sequential modeling with central pattern generators",
    "citation_count": 2,
    "authors": [
      "Changze Lv",
      "Dongqi Han",
      "Yansen Wang",
      "Xiaoqing Zheng",
      "Xuanjing Huang",
      "Dongsheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2f6a6317bada76b26a4f61bb70a7db59-Abstract-Conference.html": {
    "title": "Evaluating the World Model Implicit in a Generative Model",
    "volume": "main",
    "abstract": "Recent work suggests that large language models may implicitly learn world models. How should we assess this possibility? We formalize this question for the case where the underlying reality is governed by a deterministic finite automaton. This includes problems as diverse as simple logical reasoning, geographic navigation, game-playing, and chemistry. We propose new evaluation metrics for world model recovery inspired by the classic Myhill-Nerode theorem from language theory. We illustrate their utility in three domains: game playing, logic puzzles, and navigation. In all domains, the generative models we consider do well on existing diagnostics for assessing world models, but our evaluation metrics reveal their world models to be far less coherent than they appear. Such incoherence creates fragility: using a generative model to solve related but subtly different tasks can lead to failures. Building generative models that meaningfully capture the underlying logic of the domains they model would be immensely valuable; our results suggest new ways to assess how close a given model is to that goal",
    "checked": true,
    "id": "61e5bafb567b36960a6e1584b3753d9de09b45aa",
    "semantic_title": "evaluating the world model implicit in a generative model",
    "citation_count": 50,
    "authors": [
      "Keyon Vafa",
      "Justin Chen",
      "Ashesh Rambachan",
      "Jon M. Kleinberg",
      "Sendhil Mullainathan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2f7001684feb23ca6569352eda963f39-Abstract-Conference.html": {
    "title": "Contextual Linear Optimization with Bandit Feedback",
    "volume": "main",
    "abstract": "Contextual linear optimization (CLO) uses predictive contextual features to reduce uncertainty in random cost coefficients and thereby improve average-cost performance. An example is the stochastic shortest path problem with random edge costs (e.g., traffic) and contextual features (e.g., lagged traffic, weather). Existing work on CLO assumes the data has fully observed cost coefficient vectors, but in many applications, we can only see the realized cost of a historical decision, that is, just one projection of the random cost coefficient vector, to which we refer as bandit feedback. We study a class of offline learning algorithms for CLO with bandit feedback, which we term induced empirical risk minimization (IERM), where we fit a predictive model to directly optimize the downstream performance of the policy it induces. We show a fast-rate regret bound for IERM that allows for misspecified model classes and flexible choices of the optimization estimate, and we develop computationally tractable surrogate losses. A byproduct of our theory of independent interest is fast-rate regret bound for IERM with full feedback and misspecified policy class. We compare the performance of different modeling choices numerically using a stochastic shortest path example and provide practical insights from the empirical results",
    "checked": true,
    "id": "a63660a7390cd5c0a03846b65d884a0643682e5d",
    "semantic_title": "contextual linear optimization with bandit feedback",
    "citation_count": 0,
    "authors": [
      "Yichun Hu",
      "Nathan Kallus",
      "Xiaojie Mao",
      "Yanchen Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2f75a57e9c71e8369da0150ea769d5a2-Abstract-Conference.html": {
    "title": "A Surprisingly Simple Approach to Generalized Few-Shot Semantic Segmentation",
    "volume": "main",
    "abstract": "The goal of *generalized* few-shot semantic segmentation (GFSS) is to recognize *novel-class* objects through training with a few annotated examples and the *base-class* model that learned the knowledge about the base classes.Unlike the classic few-shot semantic segmentation, GFSS aims to classify pixels into both base and novel classes, meaning it is a more practical setting.Current GFSS methods rely on several techniques such as using combinations of customized modules, carefully designed loss functions, meta-learning, and transductive learning.However, we found that a simple rule and standard supervised learning substantially improve the GFSS performance.In this paper, we propose a simple yet effective method for GFSS that does not use the techniques mentioned above.Also, we theoretically show that our method perfectly maintains the segmentation performance of the base-class model over most of the base classes.Through numerical experiments, we demonstrated the effectiveness of our method.It improved in novel-class segmentation performance in the $1$-shot scenario by $6.1$% on the PASCAL-$5^i$ dataset, $4.7$% on the PASCAL-$10^i$ dataset, and $1.0$% on the COCO-$20^i$ dataset.Our code is publicly available at https://github.com/IBM/BCM",
    "checked": true,
    "id": "142b9646e3a831cf773fbdf94255c2a7634772e3",
    "semantic_title": "a surprisingly simple approach to generalized few-shot semantic segmentation",
    "citation_count": 2,
    "authors": [
      "Tomoya Sakai",
      "Haoxiang Qiu",
      "Takayuki Katsuki",
      "Daiki Kimura",
      "Takayuki Osogami",
      "Tadanobu Inoue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2f8ee6a3d766b426d2618e555b5aeb39-Abstract-Conference.html": {
    "title": "Are We on the Right Way for Evaluating Large Vision-Language Models?",
    "volume": "main",
    "abstract": "Large vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomenon is prevalent across current benchmarks. For instance, GeminiPro achieves 42.7% on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks near 24% on average. 2) Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM backbone with 17.9%. Both problems lead to misjudgments of actual multi-modal gains and potentially misguide the study of LVLM. To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples. These samples are first roughly selected from current benchmarks with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities. Moreover, two metrics are developed to measure data leakage and actual performance gain in multi-modal training. We evaluate 16 leading LVLMs on MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the proposed metrics to investigate their data leakage and actual multi-modal gain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Chen",
      "Jinsong Li",
      "Xiaoyi Dong",
      "Pan Zhang",
      "Yuhang Zang",
      "Zehui Chen",
      "Haodong Duan",
      "Jiaqi Wang",
      "Yu Qiao",
      "Dahua Lin",
      "Feng Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2f8f01acb3df19fea82fa68b4a310058-Abstract-Conference.html": {
    "title": "On the Benefits of Public Representations for Private Transfer Learning under Distribution Shift",
    "volume": "main",
    "abstract": "Public pretraining is a promising approach to improve differentially private model training. However, recent work has noted that many positive research results studying this paradigm only consider in-distribution tasks, and may not apply to settings where there is distribution shift between the pretraining and finetuning data---a scenario that is likely when finetuning private tasks due to the sensitive nature of the data. In this work, we show empirically across three tasks that even in settings with large distribution shift, where both zero-shot performance from public data and training from scratch with private data give unusably weak results, public features can in fact improve private training accuracy by up to 67\\% over private training from scratch. We provide a theoretical explanation for this phenomenon, showing that if the public and private data share a low-dimensional representation, public representations can improve the sample complexity of private training even if it is \\emph{impossible} to learn the private task from the public data alone. Altogether, our results provide evidence that public data can indeed make private training practical in realistic settings of extreme distribution shift",
    "checked": true,
    "id": "2bee7b10aefa919b898d2b536d881493f602ee25",
    "semantic_title": "on the benefits of public representations for private transfer learning under distribution shift",
    "citation_count": 2,
    "authors": [
      "Pratiksha Thaker",
      "Amrith Setlur",
      "Steven Z. Wu",
      "Virginia Smith"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2f9ee101e35b890d9eae79ee27bcd69a-Abstract-Conference.html": {
    "title": "Reparameterized Multi-Resolution Convolutions for Long Sequence Modelling",
    "volume": "main",
    "abstract": "Global convolutions have shown increasing promise as powerful general-purpose sequence models. However, training long convolutions is challenging, and kernel parameterizations must be able to learn long-range dependencies without overfitting. This work introduces reparameterized multi-resolution convolutions ($\\texttt{MRConv}$), a novel approach to parameterizing global convolutional kernels for long-sequence modeling. By leveraging multi-resolution convolutions, incorporating structural reparameterization and introducing learnable kernel decay, $\\texttt{MRConv}$ learns expressive long-range kernels that perform well across various data modalities. Our experiments demonstrate state-of-the-art performance on the Long Range Arena, Sequential CIFAR, and Speech Commands tasks among convolution models and linear-time transformers. Moreover, we report improved performance on ImageNet classification by replacing 2D convolutions with 1D $\\texttt{MRConv}$ layers",
    "checked": true,
    "id": "45c695a86d7d9dc0cdd9ba8f3c240d1f6ed70882",
    "semantic_title": "reparameterized multi-resolution convolutions for long sequence modelling",
    "citation_count": 1,
    "authors": [
      "Jake Cunningham",
      "Giorgio Giannone",
      "Mingtian Zhang",
      "Marc Deisenroth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2fb57276bfbaf1b832d7bfcba36bb41c-Abstract-Conference.html": {
    "title": "Streaming Bayes GFlowNets",
    "volume": "main",
    "abstract": "Bayes' rule naturally allows for inference refinement in a streaming fashion, without the need to recompute posteriors from scratch whenever new data arrives. In principle, Bayesian streaming is straightforward: we update our prior with the available data and use the resulting posterior as a prior when processing the next data chunk. In practice, however, this recipe entails i) approximating an intractable posterior at each time step; and ii) encapsulating results appropriately to allow for posterior propagation. For continuous state spaces, variational inference (VI) is particularly convenient due to its scalability and the tractability of variational posteriors, For discrete state spaces, however, state-of-the-art VI results in analytically intractable approximations that are ill-suited for streaming settings. To enable streaming Bayesian inference over discrete parameter spaces, we propose streaming Bayes GFlowNets (abbreviated as SB-GFlowNets) by leveraging the recently proposed GFlowNets --- a powerful class of amortized samplers for discrete compositional objects. Notably, SB-GFlowNet approximates the initial posterior using a standard GFlowNet and subsequently updates it using a tailored procedure that requires only the newly observed data. Our case studies in linear preference learning and phylogenetic inference showcase the effectiveness of SB-GFlowNets in sampling from an unnormalized posterior in a streaming setting. As expected, we also observe that SB-GFlowNets is significantly faster than repeatedly training a GFlowNet from scratch to sample from the full posterior",
    "checked": true,
    "id": "27735d2c2187ab41212b1be4b3b5cac9b5381111",
    "semantic_title": "streaming bayes gflownets",
    "citation_count": 2,
    "authors": [
      "Tiago Silva",
      "Daniel Augusto de Souza",
      "Diego Mesquita"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2fbeed1dd7162f91804e7b9246e0c1a8-Abstract-Conference.html": {
    "title": "Inverse Factorized Soft Q-Learning for Cooperative Multi-agent Imitation Learning",
    "volume": "main",
    "abstract": "This paper concerns imitation learning (IL) in cooperative multi-agent systems.The learning problem under consideration poses several challenges, characterized by high-dimensional state and action spaces and intricate inter-agent dependencies. In a single-agent setting, IL was shown to be done efficiently via an inverse soft-Q learning process. However, extending this framework to a multi-agent context introduces the need to simultaneously learn both local value functions to capture local observations and individual actions, and a joint value function for exploiting centralized learning.In this work, we introduce a new multi-agent IL algorithm designed to address these challenges. Our approach enables thecentralized learning by leveraging mixing networks to aggregate decentralized Q functions.We further establish conditions for the mixing networks under which the multi-agent IL objective function exhibits convexity within the Q function space.We present extensive experiments conducted on some challenging multi-agent game environments, including an advanced version of the Star-Craft multi-agent challenge (SMACv2), which demonstrates the effectiveness of our algorithm",
    "checked": true,
    "id": "e71e7a43a269f5d3ce41b97c6c99f1acc43f8b2c",
    "semantic_title": "inverse factorized soft q-learning for cooperative multi-agent imitation learning",
    "citation_count": 2,
    "authors": [
      "The Viet Bui",
      "Tien Mai",
      "Thanh Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2fcd27d2806a6d4f616cb4e6084d74f3-Abstract-Conference.html": {
    "title": "Learning from Pattern Completion: Self-supervised Controllable Generation",
    "volume": "main",
    "abstract": "The human brain exhibits a strong ability to spontaneously associate different visual attributes of the same or similar visual scene, such as associating sketches and graffiti with real-world visual objects, usually without supervising information. In contrast, in the field of artificial intelligence, controllable generation methods like ControlNet heavily rely on annotated training datasets such as depth maps, semantic segmentation maps, and poses, which limits the method's scalability. Inspired by the neural mechanisms that may contribute to the brain's associative power, specifically the cortical modularization and hippocampal pattern completion, here we propose a self-supervised controllable generation (SCG) framework. Firstly, we introduce an equivariance constraint to promote inter-module independence and intra-module correlation in a modular autoencoder network, thereby achieving functional specialization. Subsequently, based on these specialized modules, we employ a self-supervised pattern completion approach for controllable generation training. Experimental results demonstrate that the proposed modular autoencoder effectively achieves functional specialization, including the modular processing of color, brightness, and edge detection, and exhibits brain-like features including orientation selectivity, color antagonism, and center-surround receptive fields. Through self-supervised training, associative generation capabilities spontaneously emerge in SCG, demonstrating excellent zero-shot generalization ability to various tasks such as superresolution, dehaze and associative or conditional generation on painting, sketches, and ancient graffiti. Compared to the previous representative method ControlNet, our proposed approach not only demonstrates superior robustness in more challenging high-noise scenarios but also possesses more promising scalability potential due to its self-supervised manner. Codes are released on Github and Gitee",
    "checked": true,
    "id": "662213fcf58c22fe9ffcf6b5e4cdea7f390a9896",
    "semantic_title": "learning from pattern completion: self-supervised controllable generation",
    "citation_count": 0,
    "authors": [
      "Zhiqiang Chen",
      "Guofan Fan",
      "Jinying Gao",
      "Lei Ma",
      "Bo Lei",
      "Tiejun Huang",
      "Shan Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2feff80094b297bcfb42dbb01f34b875-Abstract-Conference.html": {
    "title": "Diffeomorphic interpolation for efficient persistence-based topological optimization",
    "volume": "main",
    "abstract": "Topological Data Analysis (TDA) provides a pipeline to extract quantitative and powerful topological descriptors from structured objects. This enables the definition of topological loss functions, which assert to which extent a given object exhibits some topological properties. One can then use these losses to perform topological optimization via gradient descent routines. While theoretically sounded, topological optimization faces an important challenge: gradients tend to be extremely sparse, in the sense that the loss function typically depends (locally) on only very few coordinates of the input object, yielding dramatically slow optimization schemes in practice. In this work, focusing on the central case of topological optimization for point clouds, we propose to overcome this limitation using diffeomorphic interpolation, turning sparse gradients into smooth vector fields defined on the whole space. In particular, this approach combines efficiently with subsampling techniques routinely used in TDA, as the diffeomorphism derived from the gradient computed on the subsample can be used to update the coordinates of the full and possibly large input object. We then illustrate the usefulness of our approach on black-box autoencoder (AE) regularization, where we aim at applying some topological priors on the latent spaces associated to fixed, black-box AE models without modifying their (unknown) architectures and parameters. We empirically show that, while vanilla topological optimization has to be re-run every time that new data comes out of the black-box models, learning a diffeomorphic flow can be done once and then re-applied to new data in linear time. Moreover, reverting the flow allows us to generate data by sampling the topologically-optimized latent space directly, allowing for better interpretability of the model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathieu Carrière",
      "Marc Theveneau",
      "Théo Lacombe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/2ff26b12ade4282de80c2461e447c373-Abstract-Conference.html": {
    "title": "A theoretical case-study of Scalable Oversight in Hierarchical Reinforcement Learning",
    "volume": "main",
    "abstract": "A key source of complexity in next-generation AI models is the size of model outputs, making it time-consuming to parse and provide reliable feedback on. To ensure such models are aligned, we will need to bolster our understanding of scalable oversight and how to scale up human feedback. To this end, we study the challenges of scalable oversight in the context of goal-conditioned hierarchical reinforcement learning. Hierarchical structure is a promising entrypoint into studying how to scale up human feedback, which in this work we assume can only be provided for model outputs below a threshold size. In the cardinal feedback setting, we develop an apt sub-MDP reward and algorithm that allows us to acquire and scale up low-level feedback for learning with sublinear regret. In the ordinal feedback setting, we show the necessity of both high- and low-level feedback, and develop a hierarchical experimental design algorithm that efficiently acquires both types of feedback for learning. Altogether, our work aims to consolidate the foundations of scalable oversight, formalizing and studying the various challenges thereof",
    "checked": true,
    "id": "35df81ea3e3799191b29ac661953cfeea4f353c0",
    "semantic_title": "a theoretical case-study of scalable oversight in hierarchical reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Tom Yan",
      "Zachary Lipton"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3018804d037cc101b73624f381bed0cb-Abstract-Conference.html": {
    "title": "Exploring Low-Dimensional Subspace in Diffusion Models for Controllable Image Editing",
    "volume": "main",
    "abstract": "Recently, diffusion models have emerged as a powerful class of generative models. Despite their success, there is still limited understanding of their semantic spaces. This makes it challenging to achieve precise and disentangled image generation without additional training, especially in an unsupervised way. In this work, we improve the understanding of their semantic spaces from intriguing observations: among a certain range of noise levels, (1) the learned posterior mean predictor (PMP) in the diffusion model is locally linear, and (2) the singular vectors of its Jacobian lie in low-dimensional semantic subspaces. We provide a solid theoretical basis to justify the linearity and low-rankness in the PMP. These insights allow us to propose an unsupervised, single-step, training-free LOw-rank COntrollable image editing (LOCO Edit) method for precise local editing in diffusion models. LOCO Edit identified editing directions with nice properties: homogeneity, transferability, composability, and linearity. These properties of LOCO Edit benefit greatly from the low-dimensional semantic subspace.Our method can further be extended to unsupervised or text-supervised editing in various text-to-image diffusion models (T-LOCO Edit). Finally, extensive empirical experiments demonstrate the effectiveness and efficiency of LOCO Edit. The code and the arXiv version can be found on the project website",
    "checked": true,
    "id": "69aafc0065438ff842479dd8d4c07711dd180e24",
    "semantic_title": "exploring low-dimensional subspace in diffusion models for controllable image editing",
    "citation_count": 8,
    "authors": [
      "Siyi Chen",
      "Huijie Zhang",
      "Minzhe Guo",
      "Yifu Lu",
      "Peng Wang",
      "Qing Qu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3040bf93152e90d5719379818a8664b4-Abstract-Conference.html": {
    "title": "Efficiency for Free: Ideal Data Are Transportable Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "PENG SUN",
      "Yi Jiang",
      "Tao Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/305b2288122d46bf0641bdd86c9a7921-Abstract-Conference.html": {
    "title": "Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Luo",
      "Haihong E",
      "Yuhao Yang",
      "Tianyu Yao",
      "Yikai Guo",
      "Zichen Tang",
      "Wentai Zhang",
      "Shiyao Peng",
      "Kaiyang Wan",
      "Meina Song",
      "Wei Lin",
      "Yifan Zhu",
      "Anh Tuan Luu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/30699996ff411d48903c9752b782a5c1-Abstract-Conference.html": {
    "title": "MambaLLIE: Implicit Retinex-Aware Low Light Enhancement with Global-then-Local State Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangwei Weng",
      "Zhiqiang Yan",
      "Ying Tai",
      "Jianjun Qian",
      "Jian Yang",
      "Jun Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/30732ddb12d9faf7180f5d0e8b5b5da7-Abstract-Conference.html": {
    "title": "On Softmax Direct Preference Optimization for Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Chen",
      "Junfei Tan",
      "An Zhang",
      "Zhengyi Yang",
      "Leheng Sheng",
      "Enzhi Zhang",
      "Xiang Wang",
      "Tat-Seng Chua"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/30754e5f4cd69d64b5527cdd87d3cf62-Abstract-Conference.html": {
    "title": "First-Explore, then Exploit: Meta-Learning to Solve Hard Exploration-Exploitation Trade-Offs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Norman",
      "Jeff Clune"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3076133f08b40607d00a8f48f6acd71c-Abstract-Conference.html": {
    "title": "GenRL: Multimodal-foundation world models for generalization in embodied agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pietro Mazzaglia",
      "Tim Verbelen",
      "Bart Dhoedt",
      "Aaron C. Courville",
      "Sai Rajeswar Mudumba"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/309cadc33589efca4018a490c07db263-Abstract-Conference.html": {
    "title": "Quantifying and Optimizing Global Faithfulness in Persona-driven Role-playing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Letian Peng",
      "Jingbo Shang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/309fd617a4168d592e543690fbd094db-Abstract-Conference.html": {
    "title": "Learning Truncated Causal History Model for Video Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirhosein Ghasemabadi",
      "Muhammad Janjua",
      "Mohammad Salameh",
      "Di Niu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/30bdd694743e381f9ba679ec49dab866-Abstract-Conference.html": {
    "title": "UDPM: Upsampling Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shady Abu-Hussein",
      "Raja Giryes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/30df5ecdd245bd2f4b0c5ba48de9674a-Abstract-Conference.html": {
    "title": "Rethinking Inverse Reinforcement Learning: from Data Alignment to Task Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weichao Zhou",
      "Wenchao Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/30dfe47a3ccbee68cffa0c19ccb1bc00-Abstract-Conference.html": {
    "title": "AlphaMath Almost Zero: Process Supervision without Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoxin Chen",
      "Minpeng Liao",
      "Chengxi Li",
      "Kai Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3100d29d662360bb1a40a5ded8e100ae-Abstract-Conference.html": {
    "title": "Optimal deep learning of holomorphic operators between Banach spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Adcock",
      "Nick Dexter",
      "Sebastian Moraga Scheuermann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3103b25853719847502559bf67eb4037-Abstract-Conference.html": {
    "title": "Multi-Agent Imitation Learning: Value is Easy, Regret is Hard",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwu Tang",
      "Gokul Swamy",
      "Fei Fang",
      "Steven Z. Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3106a1387b24c33bf66682e7ff393d56-Abstract-Conference.html": {
    "title": "Constrained Adaptive Attack: Effective Adversarial Attack Against Deep Neural Networks for Tabular Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibault Simonetto",
      "Salah GHAMIZI",
      "Maxime Cordy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3107e4bdb658c79053d7ef59cbc804dd-Abstract-Conference.html": {
    "title": "How Far Can Transformers Reason? The Globality Barrier and Inductive Scratchpad",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emmanuel Abbe",
      "Samy Bengio",
      "Aryo Lotfi",
      "Colin Sandon",
      "Omid Saremi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3122aaa22b2fe83f9cead1a696f65ceb-Abstract-Conference.html": {
    "title": "FineCLIP: Self-distilled Region-based CLIP for Better Fine-grained Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Jing",
      "Xiaolong He",
      "Yutian Luo",
      "Nanyi Fei",
      "guoxing Yang",
      "Wei Wei",
      "Huiwen Zhao",
      "Zhiwu Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/313044ccf6f6b91c4903e6894969f1ba-Abstract-Conference.html": {
    "title": "Generative Forests",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richard Nock",
      "Mathieu Guillame-Bert"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/31421b112e5f7faf4fc577b74e45dab2-Abstract-Conference.html": {
    "title": "Scalable DBSCAN with Random Projections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HaoChuan Xu",
      "Ninh Pham"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/314f72f80227e21cd95f402c73f0d360-Abstract-Conference.html": {
    "title": "Near-Optimal Distributed Minimax Optimization under the Second-Order Similarity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihao Zhou",
      "Haishan Ye",
      "Luo Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/316648eb8b4ffb6010f531b07848c300-Abstract-Conference.html": {
    "title": "M$^3$GPT: An Advanced Multimodal, Multitask Framework for Motion Comprehension and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingshuang Luo",
      "RuiBing Hou",
      "Zhuo Li",
      "Hong Chang",
      "Zimo Liu",
      "Yaowei Wang",
      "Shiguang Shan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3173c427cb4ed2d5eaab029c17f221ae-Abstract-Conference.html": {
    "title": "From Transparent to Opaque: Rethinking Neural Implicit Surfaces with $\\alpha$-NeuS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Zhang",
      "Junkai Deng",
      "Xuhui Chen",
      "Fei Hou",
      "Wencheng Wang",
      "Hong Qin",
      "Chen Qian",
      "Ying He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/317ccced29ed464df181c781cb436180-Abstract-Conference.html": {
    "title": "Test Where Decisions Matter: Importance-driven Testing for Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Pranger",
      "Hana Chockler",
      "Martin Tappler",
      "Bettina Könighofer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3181db351fd3ced43cd589b0b572675d-Abstract-Conference.html": {
    "title": "Don't Look Twice: Faster Video Transformers with Run-Length Tokenization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohan Choudhury",
      "Guanglei Zhu",
      "Sihan Liu",
      "Koichiro Niinuma",
      "Kris Kitani",
      "László Jeni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/31888563b194f9bb33ce1aebc7e1551c-Abstract-Conference.html": {
    "title": "Value-Based Deep Multi-Agent Reinforcement Learning with Dynamic Sparse Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pihe Hu",
      "Shaolong Li",
      "Zhuoran Li",
      "Ling Pan",
      "Longbo Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/318f3ae8be3c97cb7555e1c932f472a1-Abstract-Conference.html": {
    "title": "Rethinking Memory and Communication Costs for Efficient Data Parallel Training of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanxiao Zhang",
      "Lin JU",
      "Chan Wu",
      "Jinjing Huang",
      "Youshao Xiao",
      "Zhenglei Zhou",
      "Zhiming fan",
      "Zhaoxin Huan",
      "Siyuan Li",
      "Fanzhuang Meng",
      "Lei Liang",
      "Xiaolu Zhang",
      "Jun Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3191170938b6102e5c203b036b7c16dd-Abstract-Conference.html": {
    "title": "GTBench: Uncovering the Strategic Reasoning Capabilities of LLMs via Game-Theoretic Evaluations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhao Duan",
      "Renming Zhang",
      "James Diffenderfer",
      "Bhavya Kailkhura",
      "Lichao Sun",
      "Elias Stengel-Eskin",
      "Mohit Bansal",
      "Tianlong Chen",
      "Kaidi Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3192aa37a2227627d7f8cbf582c075f3-Abstract-Conference.html": {
    "title": "DMNet: Self-comparison Driven Model for Subject-independent Seizure Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihao Tu",
      "Linfeng Cao",
      "Daoze Zhang",
      "Junru Chen",
      "Lvbin Ma",
      "Yin Zhang",
      "YANG YANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/31994923f58ae5b2d661b300bd439107-Abstract-Conference.html": {
    "title": "OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junke Wang",
      "Yi Jiang",
      "Zehuan Yuan",
      "BINGYUE PENG",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/31a57804448363bcab777f818f75f5b4-Abstract-Conference.html": {
    "title": "Perplexity-aware Correction for Robust Alignment with Noisy Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keyi Kong",
      "Xilie Xu",
      "Di Wang",
      "Jingfeng ZHANG",
      "Mohan S Kankanhalli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/31d1946b6bccf54bdd4a811bedd9626b-Abstract-Conference.html": {
    "title": "The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Derek Lim",
      "Theo Putterman",
      "Robin Walters",
      "Haggai Maron",
      "Stefanie Jegelka"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/31df6a082046111e605abfec26ef5ccc-Abstract-Conference.html": {
    "title": "Probabilistic Graph Rewiring via Virtual Nodes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chendi Qian",
      "Andrei Manolache",
      "Christopher Morris",
      "Mathias Niepert"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/31e6e0c09325a3be16d93f84e40e0c7e-Abstract-Conference.html": {
    "title": "FuseFL: One-Shot Federated Learning through the Lens of Causality with Progressive Model Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenheng Tang",
      "Yonggang Zhang",
      "Peijie Dong",
      "Yiu-ming Cheung",
      "Amelie Zhou",
      "Bo Han",
      "Xiaowen Chu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/31f119089f702e48ecfd138c1bc82c4a-Abstract-Conference.html": {
    "title": "RL-GPT: Integrating Reinforcement Learning and Code-as-policy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoteng Liu",
      "Haoqi Yuan",
      "Minda Hu",
      "Yanwei Li",
      "Yukang Chen",
      "Shu Liu",
      "Zongqing Lu",
      "Jiaya Jia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/31fb284a0aaaad837d2930a610cd5e50-Abstract-Conference.html": {
    "title": "A Layer-Wise Natural Gradient Optimizer for Training Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaolei Liu",
      "Shaoshuai Li",
      "Kaixin Gao",
      "Binfeng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3209cf3312b2cbb68e33644362ddc2bd-Abstract-Conference.html": {
    "title": "Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingcheng Li",
      "Dingkang Yang",
      "Yang Liu",
      "Shunli Wang",
      "Jiawei Chen",
      "Shuaibing Wang",
      "Jinjie Wei",
      "Yue Jiang",
      "Qingyao Xu",
      "Xiaolu Hou",
      "Mingyang Sun",
      "Ziyun Qian",
      "Dongliang Kou",
      "Lihua Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/32133a6a24d6554263d3584e3ac10faa-Abstract-Conference.html": {
    "title": "Statistical Estimation in the Spiked Tensor Model via the Quantum Approximate Optimization Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leo Zhou",
      "Joao Basso",
      "Song Mei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/321387ba926b8e58d3591c0aeb52ffc2-Abstract-Conference.html": {
    "title": "MoEUT: Mixture-of-Experts Universal Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert Csordas",
      "Kazuki Irie",
      "Jürgen Schmidhuber",
      "Christopher Potts",
      "Christopher D Manning"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/322e4a595afd9442a89f0bfaa441871e-Abstract-Conference.html": {
    "title": "Visual Perception by Large Language Model's Weights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feipeng Ma",
      "Hongwei Xue",
      "Yizhou Zhou",
      "Guangting Wang",
      "Fengyun Rao",
      "Shilin Yan",
      "Yueyi Zhang",
      "Siying Wu",
      "Mike Zheng Shou",
      "Xiaoyan Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/325ce3291a509ddacc1e08f457b4d86c-Abstract-Conference.html": {
    "title": "Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongxin Zhu",
      "Bocheng Li",
      "Hang Zhang",
      "Xin Li",
      "Linli Xu",
      "Lidong Bing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3268f1e2474ef9d1af7f034401197a7f-Abstract-Conference.html": {
    "title": "$C^2M^3$: Cycle-Consistent Multi-Model Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donato Crisostomi",
      "Marco Fumero",
      "Daniele Baieri",
      "Florian Bernard",
      "Emanuele Rodolà"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/327b9b8d4e45c3f81568e11ffc505f77-Abstract-Conference.html": {
    "title": "LaKD: Length-agnostic Knowledge Distillation for Trajectory Prediction with Any Length Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Li",
      "Changsheng Li",
      "Ruilin Lv",
      "Rongqing Li",
      "Ye Yuan",
      "Guoren Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/328b81881da145412f2bc56c998dfb6a-Abstract-Conference.html": {
    "title": "Stochastic Optimal Control for Diffusion Bridges in Function Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byoungwoo Park",
      "Jungwon Choi",
      "Sungbin Lim",
      "Juho Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/328c922d068dd4ccb23cec5c64e6c7fc-Abstract-Conference.html": {
    "title": "Enhancing LLM Reasoning via Vision-Augmented Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Xiao",
      "Dongxiang Zhang",
      "Xiongwei Han",
      "Xiaojin Fu",
      "Wing Yin YU",
      "Tao Zhong",
      "Sai Wu",
      "Yuan Wang",
      "Jianwei Yin",
      "Gang Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/32b80425554e081204e5988ab1c97e9a-Abstract-Conference.html": {
    "title": "MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stella Li",
      "Vidhisha Balachandran",
      "Shangbin Feng",
      "Jonathan Ilgen",
      "Emma Pierson",
      "Pang Wei W Koh",
      "Yulia Tsvetkov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/32cc61322f1e2f56f989d29ccc7cfbb7-Abstract-Conference.html": {
    "title": "QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Song",
      "Tianxiang Gong",
      "Shiqi Gao",
      "Haoyi Zhou",
      "Jianxin Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/32cf311edd3cad32dc6672b4f973366e-Abstract-Conference.html": {
    "title": "AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Swapnil Bhosale",
      "Haosen Yang",
      "Diptesh Kanojia",
      "Jiankang Deng",
      "Xiatian Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/32e07a110c6c6acf1afbf2bf82b614ad-Abstract-Conference.html": {
    "title": "Multi-LLM Debate: Framework, Principals, and Interventions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Estornell",
      "Yang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/32eb183794ef5ef9a3ab1d40a3d2b303-Abstract-Conference.html": {
    "title": "MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rachel S.Y. Teo",
      "Tan Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3310034c97fab48fdbcba18f90fd5364-Abstract-Conference.html": {
    "title": "Addressing Asynchronicity in Clinical Multimodal Fusion via Individualized Chest X-ray Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenfang Yao",
      "Chen Liu",
      "Kejing Yin",
      "William Cheung",
      "Jing Qin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3322a9a72a1707de14badd5e552ff466-Abstract-Conference.html": {
    "title": "Not All Tokens Are What You Need for Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao Lin",
      "Zhibin Gou",
      "Yeyun Gong",
      "Xiao Liu",
      "yelong shen",
      "Ruochen Xu",
      "Chen Lin",
      "Yujiu Yang",
      "Jian Jiao",
      "Nan Duan",
      "Weizhu Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/333a7697dbb67f09249337f81c27d749-Abstract-Conference.html": {
    "title": "A2PO: Towards Effective Offline Reinforcement Learning from an Advantage-aware Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunpeng Qing",
      "Shunyu Liu",
      "Jingyuan Cong",
      "Kaixuan Chen",
      "Yihe Zhou",
      "Mingli Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3365d974ce309623bd8151082d78206c-Abstract-Conference.html": {
    "title": "Chain of Thoughtlessness? An Analysis of CoT in Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaya Stechly",
      "Karthik Valmeekam",
      "Subbarao Kambhampati"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3379db43afd10e49be5cee62ad71cb31-Abstract-Conference.html": {
    "title": "Face2QR: A Unified Framework for Aesthetic, Face-Preserving, and Scannable QR Code Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuehao Cui",
      "Guangyang Wu",
      "Zhenghao Gan",
      "Guangtao Zhai",
      "Xiaohong Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/33870b3e099880cd8e705cd07173ac27-Abstract-Conference.html": {
    "title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Archit Sharma",
      "Sedrick Scott Keh",
      "Eric Mitchell",
      "Chelsea Finn",
      "Kushal Arora",
      "Thomas Kollar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3396657fe1a3c9a43ac7cd809c51a41e-Abstract-Conference.html": {
    "title": "MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhuo Liu",
      "Fei Zhu",
      "Shijie Ma",
      "Cheng-lin Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/33b47b3d2441a17b95344cd635f3dd01-Abstract-Conference.html": {
    "title": "Global Convergence in Training Large-Scale Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Gao",
      "Yuan Cao",
      "Zihao Li",
      "Yihan He",
      "Mengdi Wang",
      "Han Liu",
      "Jason Klusowski",
      "Jianqing Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/33c674cb3ce9dae35021930d8d63308f-Abstract-Conference.html": {
    "title": "Constrained Sampling with Primal-Dual Langevin Monte Carlo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luiz Chamon",
      "Mohammad Reza Karimi Jaghargh",
      "Anna Korba"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/33d93e4dc57453e7667b20f62e7c0681-Abstract-Conference.html": {
    "title": "Scalable Neural Network Verification with Branch-and-bound Inferred Cutting Planes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duo Zhou",
      "Christopher Brix",
      "Grani A. Hanasusanto",
      "Huan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3415a8f8127d5b0ceb7fd321180b1954-Abstract-Conference.html": {
    "title": "MoLE: Enhancing Human-centric Text-to-image Diffusion via Mixture of Low-rank Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Zhu",
      "Yixiong Chen",
      "Mingyu Ding",
      "Ping Luo",
      "Leye Wang",
      "Jingdong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/34268a70fbb49ff67f2de39331fd27b9-Abstract-Conference.html": {
    "title": "Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinlin Lai",
      "Justin Domke",
      "Daniel R. Sheldon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/34293d684b1012ed45c3274b4a7edc00-Abstract-Conference.html": {
    "title": "C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianjiao Luo",
      "Tim Pearce",
      "Huayu Chen",
      "Jianfei Chen",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/345208bdbbb6104616311dfc1d093fe7-Abstract-Conference.html": {
    "title": "VideoTetris: Towards Compositional Text-to-Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Tian",
      "Ling Yang",
      "Haotian Yang",
      "Yuan Gao",
      "Yufan Deng",
      "Xintao Wang",
      "Zhaochen Yu",
      "Xin Tao",
      "Pengfei Wan",
      "Di ZHANG",
      "Bin CUI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/34547650b2ca69d91f3b3c3ae8b21962-Abstract-Conference.html": {
    "title": "Kermut: Composite kernel regression for protein variant effects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Mørch Groth",
      "Mads Kerrn",
      "Lars Olsen",
      "Jesper Salomon",
      "Wouter Boomsma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/347110fb894281e5e937f6ccd998a6eb-Abstract-Conference.html": {
    "title": "Self-Taught Recognizer: Toward Unsupervised Adaptation for Speech Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Hu",
      "CHEN CHEN",
      "Chao-Han Yang",
      "Chengwei Qin",
      "Pin-Yu Chen",
      "Eng-Siong Chng",
      "Chao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3472d9d6bd588315879efca259c35da6-Abstract-Conference.html": {
    "title": "HOPE: Shape Matching Via Aligning Different K-hop Neighbourhoods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Barakeel Fanseu Kamhoua",
      "Huamin Qu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3477ca0ce484aa2fa42c1361ab601c25-Abstract-Conference.html": {
    "title": "SubgDiff: A Subgraph Diffusion Model to Improve Molecular Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JIYING ZHANG",
      "Zijing Liu",
      "Yu Wang",
      "Bin Feng",
      "Yu Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/349956dee974cfdcbbb2d06afad5dd4a-Abstract-Conference.html": {
    "title": "Non-parametric classification via expand-and-sparsify representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaushik Sinha"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/349a45f211fb1b3850da1ccd829e869e-Abstract-Conference.html": {
    "title": "Adversarially Robust Decision Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohang Tang",
      "Afonso Marques",
      "Parameswaran Kamalaruban",
      "Ilija Bogunovic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/34a1fc7890141f1ada3d8bc6199cce07-Abstract-Conference.html": {
    "title": "Protecting Your LLMs with Information Bottleneck",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichuan Liu",
      "Zefan Wang",
      "Linjie Xu",
      "Jinyu Wang",
      "Lei Song",
      "Tianchun Wang",
      "Chunlin Chen",
      "Wei Cheng",
      "Jiang Bian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/34a9582cd36c0b6eb94e5cf11bd6a008-Abstract-Conference.html": {
    "title": "ActAnywhere: Subject-Aware Video Background Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boxiao Pan",
      "Zhan Xu",
      "Chun-Hao Huang",
      "Krishna Kumar Singh",
      "Yang Zhou",
      "Leonidas Guibas",
      "Jimei Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/34aec5ab2f99a8f592e0cca4974013f2-Abstract-Conference.html": {
    "title": "Time-Reversal Provides Unsupervised Feedback to LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yerram Varun",
      "Rahul Madhavan",
      "Sravanti Addepalli",
      "Arun Suggala",
      "Karthikeyan Shanmugam",
      "Prateek Jain"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/34b3a40ec9752c1ae48fe85fef8fe8dc-Abstract-Conference.html": {
    "title": "UQE: A Query Engine for Unstructured Databases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanjun Dai",
      "Bethany Wang",
      "Xingchen Wan",
      "Bo Dai",
      "Sherry Yang",
      "Azade Nova",
      "Pengcheng Yin",
      "Mangpo Phothilimthana",
      "Charles A. Sutton",
      "Dale Schuurmans"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/34b70ece5f8d273fd670a17e2248d034-Abstract-Conference.html": {
    "title": "Tactile DreamFusion: Exploiting Tactile Sensing for 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihan Gao",
      "Kangle Deng",
      "Gengshan Yang",
      "Wenzhen Yuan",
      "Jun-Yan Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/34d3cf97696022b179171e5abda42c0b-Abstract-Conference.html": {
    "title": "Model Fusion through Bayesian Optimization in Language Model Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaeyun Jang",
      "Hyungi Lee",
      "Jungtaek Kim",
      "Juho Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/34d5143080c89a7ce10932c8c5e1907f-Abstract-Conference.html": {
    "title": "Mind the Graph When Balancing Data for Fairness or Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jessica Schrouff",
      "Alexis Bellot",
      "Amal Rannen-Triki",
      "Alan Malek",
      "Isabela Albuquerque",
      "Arthur Gretton",
      "Alexander D'Amour",
      "Silvia Chiappa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/34d6c7090bc5af0b96aeaf92fa074899-Abstract-Conference.html": {
    "title": "RAGraph: A General Retrieval-Augmented Graph Learning Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinke Jiang",
      "Rihong Qiu",
      "Yongxin Xu",
      "WentaoZhang",
      "Yichen Zhu",
      "Ruizhe Zhang",
      "Yuchen Fang",
      "Chu Xu",
      "Junfeng Zhao",
      "Yasha Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/34ec1286b2ccd4794c5ca4ad078b7150-Abstract-Conference.html": {
    "title": "On Giant's Shoulders: Effortless Weak to Strong by Dynamic Logits Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenghao Fan",
      "Zhenyi Lu",
      "Wei Wei",
      "Jie Tian",
      "Xiaoye Qu",
      "Dangyang Chen",
      "Yu Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3501bea1ac61fedbaaff2f88e5fa9447-Abstract-Conference.html": {
    "title": "Measuring Dejavu Memorization Efficiently",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Narine Kokhlikyan",
      "Bargav Jayaraman",
      "Florian Bordes",
      "Chuan Guo",
      "Kamalika Chaudhuri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3504a4fa45685d668ce92797fbbf1895-Abstract-Conference.html": {
    "title": "Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tehila Dahan",
      "Kfir Y. Levy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/350e718ff74062b4bac2c6ffd9e1ac66-Abstract-Conference.html": {
    "title": "Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederik Kunstner",
      "Alan Milligan",
      "Robin Yadav",
      "Mark Schmidt",
      "Alberto Bietti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3514dbacaebf0f38b25adfe59ed81a8a-Abstract-Conference.html": {
    "title": "Using Time-Aware Graph Neural Networks to Predict Temporal Centralities in Dynamic Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Franziska Heeg",
      "Ingo Scholtes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/352b13f01566ae34affacc60e98c16af-Abstract-Conference.html": {
    "title": "Fast T2T: Optimization Consistency Speeds Up Diffusion-Based Training-to-Testing Solving for Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Li",
      "Jinpei Guo",
      "Runzhong Wang",
      "Hongyuan Zha",
      "Junchi Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/356e52580f8d14514eb5e7d2fa1696a0-Abstract-Conference.html": {
    "title": "Combining Statistical Depth and Fermat Distance for Uncertainty Quantification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hai Vy Nguyen",
      "Fabrice Gamboa",
      "Reda CHHAIBI",
      "Sixin Zhang",
      "Serge Gratton",
      "Thierry Giaccone"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3578fd44b2381db12bf16e28a667c934-Abstract-Conference.html": {
    "title": "Learning Human-like Representations to Enable Learning Human Values",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Wynn",
      "Ilia Sucholutsky",
      "Tom Griffiths"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/359ddb9caccb4c54cc915dceeacf4892-Abstract-Conference.html": {
    "title": "Implicit Regularization Paths of Weighted Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin-Hong Du",
      "Pratik Patil"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/35cb54b887e7aafe74829677cce6c5c6-Abstract-Conference.html": {
    "title": "Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Zhao",
      "Hongzhou Zhu",
      "Chendong Xiang",
      "Kaiwen Zheng",
      "Chongxuan LI",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/35d127a008e3ea420dd1775d1e3ed5b4-Abstract-Conference.html": {
    "title": "MV2Cyl: Reconstructing 3D Extrusion Cylinders from Multi-View Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eunji Hong",
      "Minh Hieu Nguyen",
      "Mikaela Angelina Uy",
      "Minhyuk Sung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/35f4adf1bfca0a5c99d6c87967282e26-Abstract-Conference.html": {
    "title": "Hierarchical and Density-based Causal Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwangho Kim",
      "Jisu Kim",
      "Larry Wasserman",
      "Edward Kennedy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/362d683723e7d6c12a093961ec2e5051-Abstract-Conference.html": {
    "title": "How to Use Diffusion Priors under Sparse Views?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qisen Wang",
      "Yifan Zhao",
      "Jiawei Ma",
      "Jia Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3639efedc51a522595372f76b91cbb25-Abstract-Conference.html": {
    "title": "Exponential Quantum Communication Advantage in Distributed Inference and Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dar Gilboa",
      "Hagay Michaeli",
      "Daniel Soudry",
      "Jarrod McClean"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3640a1997a4c9571cea9db2c82e1fc35-Abstract-Conference.html": {
    "title": "Towards Efficient and Optimal Covariance-Adaptive Algorithms for Combinatorial Semi-Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julien Zhou",
      "Pierre Gaillard",
      "Thibaud Rahier",
      "Houssam Zenati",
      "Julyan Arbel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/364d565b4b726c607aa40e1632045873-Abstract-Conference.html": {
    "title": "Zero-to-Hero: Enhancing Zero-Shot Novel View Synthesis via Attention Map Filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ido Sobol",
      "Chenfeng Xu",
      "Or Litany"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3658e78b56268b7fd089e3165843086b-Abstract-Conference.html": {
    "title": "PrefPaint: Aligning Image Inpainting Diffusion Model with Human Preference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kendong Liu",
      "Zhiyu Zhu",
      "Chuanhao Li",
      "Hui LIU",
      "Huanqiang Zeng",
      "Junhui Hou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/36721d1209a059dcb7a090dd543f34c4-Abstract-Conference.html": {
    "title": "Federated Learning from Vision-Language Foundation Models: Theoretical Analysis and Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bikang Pan",
      "Wei Huang",
      "Ye Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3685de48976169ca9fd68cb4c8e48b76-Abstract-Conference.html": {
    "title": "Binarized Diffusion Model for Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Chen",
      "Haotong Qin",
      "Yong Guo",
      "Xiongfei Su",
      "Xin Yuan",
      "Linghe Kong",
      "Yulun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/36b6180f3dab4025ba763e853b044814-Abstract-Conference.html": {
    "title": "Delving into the Reversal Curse: How Far Can Large Language Models Generalize?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengkai Lin",
      "Zhihang Fu",
      "Kai Liu",
      "Liang Xie",
      "Binbin Lin",
      "Wenxiao Wang",
      "Deng Cai",
      "Yue Wu",
      "Jieping Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/36ecc1d1b883afc0e882876cbdd123ab-Abstract-Conference.html": {
    "title": "On the Parameter Identifiability of Partially Observed Linear Causal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinshuai Dong",
      "Ignavier Ng",
      "Biwei Huang",
      "Yuewen Sun",
      "Songyao Jin",
      "Roberto Legaspi",
      "Peter Spirtes",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/37094fdc81632915a5738293cf9b7ad4-Abstract-Conference.html": {
    "title": "Accelerating Transformers with Spectrum-Preserving Token Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chau Tran",
      "Duy M. H. Nguyen",
      "Manh-Duy Nguyen",
      "TrungTin Nguyen",
      "Ngan Le",
      "Pengtao Xie",
      "Daniel Sonntag",
      "James Y Zou",
      "Binh Nguyen",
      "Mathias Niepert"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/370fa2e691f57eb319bc263a07dad4a5-Abstract-Conference.html": {
    "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Li",
      "Xiang Liu",
      "Zhenheng Tang",
      "Peijie Dong",
      "Zeyu Li",
      "Xinglin Pan",
      "Xiaowen Chu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/371713c3e5314dff9483c62c5abb98a8-Abstract-Conference.html": {
    "title": "Model-free Low-Rank Reinforcement Learning via Leveraged Entry-wise Matrix Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Stojanovic",
      "Yassir Jedra",
      "Alexandre Proutiere"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/37294f033582ac0064bf90fa557c2573-Abstract-Conference.html": {
    "title": "Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Xiao",
      "Bohong Wu",
      "Jiacong Wang",
      "Chunyuan Li",
      "zhou Xun",
      "Haoyuan Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3750e99b522bd36a099d2e8b9f0550c7-Abstract-Conference.html": {
    "title": "Solving Minimum-Cost Reach Avoid using Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oswin So",
      "Cheng Ge",
      "Chuchu Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/375fca131243755f9e268d1a37ffcd85-Abstract-Conference.html": {
    "title": "Toward a Well-Calibrated Discrimination via Survival Outcome-Aware Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongjoon Lee",
      "Hyeryn Park",
      "Changhee Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/37611b0fc2b65cdbb60865af5f6cf453-Abstract-Conference.html": {
    "title": "Image Understanding Makes for A Good Tokenizer for Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luting Wang",
      "Yang Zhao",
      "Zijian Zhang",
      "Jiashi Feng",
      "Si Liu",
      "Bingyi Kang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3763f6861da87de0d2b04ef26fd02443-Abstract-Conference.html": {
    "title": "Inverse M-Kernels for Linear Universal Approximators of Non-Negative Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hideaki Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/37664246a1e07e212ddacea6e5a523f2-Abstract-Conference.html": {
    "title": "Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Ping",
      "Shuo Wang",
      "Hanqing Wang",
      "Xu Han",
      "Yuzhuang Xu",
      "Yukun Yan",
      "Yun Chen",
      "Baobao Chang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/377235d5cee7b104501407c0e5066c92-Abstract-Conference.html": {
    "title": "Towards Combating Frequency Simplicity-biased Learning for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xilin He",
      "Jingyu Hu",
      "Qinliang Lin",
      "Cheng Luo",
      "Weicheng Xie",
      "Siyang Song",
      "Muhammad Haris Khan",
      "Linlin Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/377d0752059d3d4686aa021b664a25dd-Abstract-Conference.html": {
    "title": "Generated and Pseudo Content guided Prototype Refinement for Few-shot Point Cloud Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lili Wei",
      "Congyan Lang",
      "Ziyi Chen",
      "Tao Wang",
      "Yidong Li",
      "Jun Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/378226e5df7eded3e401de5c9493143c-Abstract-Conference.html": {
    "title": "Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran He",
      "Chenjia Bai",
      "Ling Pan",
      "Weinan Zhang",
      "Bin Zhao",
      "Xuelong Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3791f5fc0e8e43730466afd2bcdb7493-Abstract-Conference.html": {
    "title": "Estimating the Hallucination Rate of Generative AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Jesson",
      "Nicolas Beltran Velez",
      "Quentin Chu",
      "Sweta Karlekar",
      "Jannik Kossen",
      "Yarin Gal",
      "John P. Cunningham",
      "David M. Blei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3792ddbf94b68ff4369f510f7a3e1777-Abstract-Conference.html": {
    "title": "Do causal predictors generalize better to new domains?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivian Nastl",
      "Moritz Hardt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/379ea6eb0faad176b570c2e26d58ff2b-Abstract-Conference.html": {
    "title": "Computation-Aware Gaussian Processes: Model Selection And Linear-Time Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Wenger",
      "Kaiwen Wu",
      "Philipp Hennig",
      "Jacob Gardner",
      "Geoff Pleiss",
      "John P. Cunningham"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/37c6d0bc4d2917dcbea693b18504bd87-Abstract-Conference.html": {
    "title": "Frequency Adaptive Normalization For Non-stationary Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiwei Ye",
      "Songgaojun Deng",
      "Qiaosha Zou",
      "Ning Gui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/37d4d4413b7c7558cc27a6d3d42ea998-Abstract-Conference.html": {
    "title": "Towards Human-AI Complementarity with Prediction Sets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giovanni De Toni",
      "Nastaran Okati",
      "Suhas Thejaswi",
      "Eleni Straitouri",
      "Manuel Rodriguez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/37d9f19150fce07bced2a81fc87d47a6-Abstract-Conference.html": {
    "title": "Evidence of Learned Look-Ahead in a Chess-Playing Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik Jenner",
      "Shreyas Kapur",
      "Vasil Georgiev",
      "Cameron Allen",
      "Scott Emmons",
      "Stuart J Russell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/37e90dcf2909b5068858b34b5239f187-Abstract-Conference.html": {
    "title": "Universal Online Convex Optimization with $1$ Projection per Round",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Yang",
      "Yibo Wang",
      "Peng Zhao",
      "Lijun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/37eb3a53e5931a811c1a9498edce298a-Abstract-Conference.html": {
    "title": "Towards Croppable Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maor Ashkenazi",
      "Eran Treister"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/37f2f382b1e1f1e887d610e7ea047086-Abstract-Conference.html": {
    "title": "EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular Data Classification via Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhee Kim",
      "Taesung Kim",
      "Jaegul Choo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/37f6bed18b9b404f53dcaec4607c4fb7-Abstract-Conference.html": {
    "title": "On Statistical Rates and Provably Efficient Criteria of Latent Diffusion Transformers (DiTs)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Weimin Wu",
      "Zhuoru Li",
      "Sophia Pi",
      "Zhao Song",
      "Han Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/380a0b16a7e6f8c5010f798c9f2d3c61-Abstract-Conference.html": {
    "title": "Beyond Redundancy: Information-aware Unsupervised Multiplex Graph Structure Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixiang Shen",
      "Shuo Wang",
      "Zhao Kang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/381efefc0d765e680451978c0392f637-Abstract-Conference.html": {
    "title": "Active learning of neural population dynamics using two-photon holographic optogenetics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Wagenmaker",
      "Lu Mi",
      "Marton Rozsa",
      "Matthew Bull",
      "Karel Svoboda",
      "Kayvon Daie",
      "Matthew Golub",
      "Kevin G. Jamieson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/382066d1460144ddcb041f32d05de053-Abstract-Conference.html": {
    "title": "DARNet: Dual Attention Refinement Network with Spatiotemporal Construction for Auditory Attention Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Yan",
      "Cunhang Fan",
      "Hongyu Zhang",
      "Xiaoke Yang",
      "Jianhua Tao",
      "Zhao Lv"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/382a8606a85ca6ec7c06185a1a95ce8b-Abstract-Conference.html": {
    "title": "High-dimensional (Group) Adversarial Training in Linear Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiling Xie",
      "Xiaoming Huo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3838bf9070b80e888d571ec126d844c2-Abstract-Conference.html": {
    "title": "Enhancing Robustness of Graph Neural Networks on Social Media with Explainable Inverse Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuefei Lyu",
      "Chaozhuo Li",
      "Sihong Xie",
      "Xi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3848856978da28639d2057094a1287a5-Abstract-Conference.html": {
    "title": "Interpretable Mesomorphic Networks for Tabular Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arlind Kadra",
      "Sebastian Pineda Arango",
      "Josif Grabocka"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3848fef259495bfd04d60cdc5c1b4db7-Abstract-Conference.html": {
    "title": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aviv Bick",
      "Kevin Li",
      "Eric P. Xing",
      "J. Zico Kolter",
      "Albert Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3852f6d247ba7deb46e4e4be9e702601-Abstract-Conference.html": {
    "title": "Wings: Learning Multimodal LLMs without Text-only Forgetting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Kai Zhang",
      "Shiyin Lu",
      "Yang Li",
      "YanQing Ma",
      "Qingguo Chen",
      "Zhao Xu",
      "Weihua Luo",
      "Kaifu Zhang",
      "De-Chuan Zhan",
      "Han-Jia Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3853f7b92d82318f03567a4f7f128227-Abstract-Conference.html": {
    "title": "Predicting the Performance of Foundation Models via Agreement-on-the-Line",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Saxena",
      "Taeyoun Kim",
      "Aman Mehra",
      "Christina Baek",
      "J. Zico Kolter",
      "Aditi Raghunathan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3855b10f44494f5ba4c283a00079f1e2-Abstract-Conference.html": {
    "title": "Exact Gradients for Stochastic Spiking Neural Networks Driven by Rough Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Holberg",
      "Cristopher Salvi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/386432c7534eec9a1cd7cbeea90d7e9f-Abstract-Conference.html": {
    "title": "Quantum Deep Equilibrium Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Schleich",
      "Marta Skreta",
      "Lasse Kristensen",
      "Rodrigo Vargas-Hernandez",
      "Alan Aspuru-Guzik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/386b84aec8f0c8d9f452ac9986df9939-Abstract-Conference.html": {
    "title": "EfficientCAPER: An End-to-End Framework for Fast and Robust Category-Level Articulated Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Yu",
      "Haonan Jiang",
      "Li Zhang",
      "Lin Yuanbo Wu",
      "Linlin Ou",
      "Liu Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/387936a8cd02bf2ad745b751047b9d49-Abstract-Conference.html": {
    "title": "$\\epsilon$-Softmax: Approximating One-Hot Vectors for Mitigating Label Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialiang Wang",
      "Xiong Zhou",
      "Deming Zhai",
      "Junjun Jiang",
      "Xiangyang Ji",
      "Xianming Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/389a55c90f839d58188060a42bb9138a-Abstract-Conference.html": {
    "title": "Replicable Uniformity Testing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihan Liu",
      "Christopher Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/38b787fc530d0b31825827e2cc306656-Abstract-Conference.html": {
    "title": "Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ce Zhang",
      "Simon Stepputtis",
      "Katia Sycara",
      "Yaqi Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/38c1b63d2369f64dbc01968aa1bd24fc-Abstract-Conference.html": {
    "title": "Low Degree Hardness for Broadcasting on Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Huang",
      "Elchanan Mossel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/38c5feed4b72c96f6cf925ccc9832ecf-Abstract-Conference.html": {
    "title": "Latent Learning Progress Drives Autonomous Goal Selection in Human Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaia Molinaro",
      "Cédric Colas",
      "Pierre-Yves Oudeyer",
      "Anne Collins"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/38c8751fbf792866c8295fab798b47a1-Abstract-Conference.html": {
    "title": "The tree autoencoder model, with application to hierarchical data visualization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miguel A. Carreira-Perpinan",
      "Kuat Gazizov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/38cbd40f8f9f9badf5a667288c2bc305-Abstract-Conference.html": {
    "title": "Neural Network Reparametrization for Accelerated Optimization in Molecular Simulations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nima Dehmamy",
      "Csaba Both",
      "Jeet Mohapatra",
      "Subhro Das",
      "Tommi Jaakkola"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/38d67d1df644cf2efe9ebd5521741dc5-Abstract-Conference.html": {
    "title": "Your Diffusion Model is Secretly a Noise Classifier and Benefits from Contrastive Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunshu Wu",
      "Yingtao Luo",
      "Xianghao Kong",
      "Vagelis Papalexakis",
      "Greg Ver Steeg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/38d6af46cca4ce1f7d699bf11078cb84-Abstract-Conference.html": {
    "title": "Distribution Guidance Network for Weakly Supervised Point Cloud Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyi Pan",
      "Wei Gao",
      "Shan Liu",
      "Ge Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/38d88c3296eafb09d3971d091a19cf6b-Abstract-Conference.html": {
    "title": "Stabilizing Linear Passive-Aggressive Online Learning with Weighted Reservoir Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Skyler Wu",
      "Fred Lu",
      "Edward Raff",
      "James Holt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/38e491559eb9e4cf31b8cd3a4e222436-Abstract-Conference.html": {
    "title": "Separation and Bias of Deep Equilibrium Models on Expressivity and Learning Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhoutong Wu",
      "Yimu Zhang",
      "Cong Fang",
      "Zhouchen Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/39137b5a573126103c8812dcdb9d0187-Abstract-Conference.html": {
    "title": "MatrixNet: Learning over symmetry groups using learned group representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Laird",
      "Circe Hsu",
      "Asilata Bapat",
      "Robin Walters"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3918dd28e51ce20c38c8a1c05758e4a0-Abstract-Conference.html": {
    "title": "Statistical and Geometrical properties of the Kernel Kullback-Leibler divergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Korba",
      "Francis R. Bach",
      "Clémentine CHAZAL"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3926df1a00c9abf056df7bcf253d026a-Abstract-Conference.html": {
    "title": "Masked Pre-training Enables Universal Zero-shot Denoiser",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxiao Ma",
      "Zhixiang Wei",
      "Yi Jin",
      "Pengyang Ling",
      "Tianle Liu",
      "Ben Wang",
      "Junkang Dai",
      "Huaian Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/393367b4ccf1d56e51276243d5cb85c4-Abstract-Conference.html": {
    "title": "Adaptive Image Quality Assessment via Teaching Large Multimodal Model to Compare",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanwei Zhu",
      "Haoning Wu",
      "Yixuan Li",
      "Zicheng Zhang",
      "Baoliang Chen",
      "Lingyu Zhu",
      "Yuming Fang",
      "Guangtao Zhai",
      "Weisi Lin",
      "Shiqi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3950f6bf5c2eb7435ecf58eaa85cc8c2-Abstract-Conference.html": {
    "title": "Fast Best-of-N Decoding via Speculative Rejection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanshi Sun",
      "Momin Haider",
      "Ruiqi Zhang",
      "Huitao Yang",
      "Jiahao Qiu",
      "Ming Yin",
      "Mengdi Wang",
      "Peter L. Bartlett",
      "Andrea Zanette"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/395371f778ebd4854b88521100af30ad-Abstract-Conference.html": {
    "title": "PointMamba: A Simple State Space Model for Point Cloud Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingkang Liang",
      "Xin Zhou",
      "Wei Xu",
      "Xingkui Zhu",
      "Zhikang Zou",
      "Xiaoqing Ye",
      "Xiao Tan",
      "Xiang Bai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/39717429762da92201a750dd03386920-Abstract-Conference.html": {
    "title": "Distributional regression: CRPS-error bounds for model fitting, model selection and convex aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dombry Clement",
      "Ahmed Zaoui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/397271e11322fae8ba7f827c50ca8d9b-Abstract-Conference.html": {
    "title": "Non-convolutional graph neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanqing Wang",
      "Kyunghyun Cho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/39781da4b5d05bc2908ce08e43bc6404-Abstract-Conference.html": {
    "title": "TripletCLIP: Improving Compositional Reasoning of CLIP via Synthetic Vision-Language Negatives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maitreya Patel",
      "Naga Sai Abhiram Kusumba",
      "Sheng Cheng",
      "Changhoon Kim",
      "Tejas Gokhale",
      "Chitta Baral",
      "'YZ' Yezhou Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3979818cdc7bc8dbeec87170c11ee340-Abstract-Conference.html": {
    "title": "Where does In-context Learning Happen in Large Language Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suzanna Sia",
      "David Mueller",
      "Kevin Duh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/398b00a05b847ac65eb98c8e5e865fe8-Abstract-Conference.html": {
    "title": "Fine-grained Control of Generative Data Augmentation in IoT Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianshi Wang",
      "Qikai Yang",
      "Ruijie Wang",
      "Dachun Sun",
      "Jinyang Li",
      "Yizhuo Chen",
      "Yigong Hu",
      "Chaoqi Yang",
      "Tomoyoshi Kimura",
      "Denizhan Kara",
      "Tarek Abdelzaher"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/399888844ffc8fa95c2db196f3f1b60b-Abstract-Conference.html": {
    "title": "Geodesic Optimization for Predictive Shift Adaptation on EEG data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Apolline Mellot",
      "Antoine Collas",
      "Sylvain Chevallier",
      "Alex Gramfort",
      "Denis A. Engemann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/39a3aa9dfd0280ff8fbad1d330662cac-Abstract-Conference.html": {
    "title": "Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaosen Zheng",
      "Tianyu Pang",
      "Chao Du",
      "Qian Liu",
      "Jing Jiang",
      "Min Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/39a746f507dc5087bd85cc39ded8c52f-Abstract-Conference.html": {
    "title": "Stylus: Automatic Adapter Selection for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Luo",
      "Justin Wong",
      "Brandon Trabucco",
      "Yanping Huang",
      "Joseph E Gonzalez",
      "zhifeng Chen",
      "Ruslan Salakhutdinov",
      "Ion Stoica"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/39b1126cdba0a37986fa14f568471ae8-Abstract-Conference.html": {
    "title": "The Surprising Ineffectiveness of Pre-Trained Visual Representations for Model-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moritz Schneider",
      "Robert Krug",
      "Narunas Vaskevicius",
      "Luigi Palmieri",
      "Joschka Boedecker"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/39bc6e3cbf5a1991d33dc10ebff9a9cf-Abstract-Conference.html": {
    "title": "DiMSUM: Diffusion Mamba - A Scalable and Unified Spatial-Frequency Method for Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Phung",
      "Quan Dao",
      "Trung Dao",
      "Viet Hoang Phan",
      "Dimitris Metaxas",
      "Anh Tran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/39c5871aa13be86ab978cba7069cbcec-Abstract-Conference.html": {
    "title": "Graph-based Uncertainty Metrics for Long-form Language Model Generations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjian Jiang",
      "Yangjun Ruan",
      "Prasanna Sattigeri",
      "Salim Roukos",
      "Tatsunori B Hashimoto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/39ca8893ea38905a9d2ffe786e85af0f-Abstract-Conference.html": {
    "title": "Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Huguet",
      "James Vuckovic",
      "Kilian FATRAS",
      "Eric Thibodeau-Laufer",
      "Pablo Lemos",
      "Riashat Islam",
      "Chenghao Liu",
      "Jarrid Rector-Brooks",
      "Tara Akhound-Sadegh",
      "Michael Bronstein",
      "Alexander Tong",
      "Joey Bose"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/39cee562b91611c16ac0b100f0bc1ea1-Abstract-Conference.html": {
    "title": "GaussianMarker: Uncertainty-Aware Copyright Protection of 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiufeng Huang",
      "Ruiqi Li",
      "Yiu-ming Cheung",
      "Ka Chun Cheung",
      "Simon See",
      "Renjie Wan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/39ec972afab01e0d8ddc6834a9d12ac1-Abstract-Conference.html": {
    "title": "Warm-starting Push-Relabel",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sami Davies",
      "Sergei Vassilvitskii",
      "Yuyan Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/39f6d5c2e310a5a629dcfc4d517aa0d1-Abstract-Conference.html": {
    "title": "Latent Neural Operator for Solving Forward and Inverse PDE Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Wang",
      "Chuang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3a10c46572628d58cb44fb705f25cbbf-Abstract-Conference.html": {
    "title": "Dense Connector for MLLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huanjin Yao",
      "Wenhao Wu",
      "Taojiannan Yang",
      "YuXin Song",
      "Mengxi Zhang",
      "Haocheng Feng",
      "Yifan Sun",
      "Zhiheng Li",
      "Wanli Ouyang",
      "Jingdong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3a1de90699eec7d7f42c91d81f94af16-Abstract-Conference.html": {
    "title": "Learning to compute Gröbner bases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroshi Kera",
      "Yuki Ishihara",
      "Yuta Kambe",
      "Tristan Vaccon",
      "Kazuhiro Yokoyama"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3a1fc7b8e200a45110872c56f0569f61-Abstract-Conference.html": {
    "title": "Linearly Decomposing and Recomposing Vision Transformers for Diverse-Scale Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuxia Lin",
      "Miaosen Zhang",
      "Ruiming Chen",
      "Xu Yang",
      "Qiufeng Wang",
      "Xin Geng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3a2d96d2eb2902043c2db705ca03e9a2-Abstract-Conference.html": {
    "title": "Adaptive Depth Networks with Skippable Sub-Paths",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woochul Kang",
      "HYUNGSEOP LEE"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3a2ef31a1e45908901adc0ca853a8faf-Abstract-Conference.html": {
    "title": "Bridge the Points: Graph-based Few-shot Segment Anything Semantically",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Zhang",
      "Guangyu Gao",
      "Jianbo Jiao",
      "Chi Liu",
      "Yunchao Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3a40e042c66e84659249f3254460c123-Abstract-Conference.html": {
    "title": "DiffNorm: Self-Supervised Normalization for Non-autoregressive Speech-to-speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiting Tan",
      "Jingyu Zhang",
      "Lingfeng Shen",
      "Daniel Khashabi",
      "Philipp Koehn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3a509449a73fd0aab8c0cf5705827036-Abstract-Conference.html": {
    "title": "Neural Persistence Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Zeng",
      "Florian Graf",
      "Martin Uray",
      "Stefan Huber",
      "Roland Kwitt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3a6935d11910d6f9142b0a1e36fc6753-Abstract-Conference.html": {
    "title": "Ada-MSHyper: Adaptive Multi-Scale Hypergraph Transformer for Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongjiang Shang",
      "Ling Chen",
      "Binqing Wu",
      "Dongliang Cui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3a797b10ff20562b1ecee0d4e914c1c7-Abstract-Conference.html": {
    "title": "Pseudo-Private Data Guided Model Inversion Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiong Peng",
      "Bo Han",
      "Feng Liu",
      "Tongliang Liu",
      "Mingyuan Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3a899fa79bc4110bca1eaa6649e9a8fa-Abstract-Conference.html": {
    "title": "Cluster-wise Graph Transformer with Dual-granularity Kernelized Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Huang",
      "Yunchong Song",
      "Jiayue Zhou",
      "Zhouhan Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3acbe9dc3a1e8d48a57b16e9aef91879-Abstract-Conference.html": {
    "title": "Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Zurek",
      "Yudong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3acc054949b6948d4444b35d412cab56-Abstract-Conference.html": {
    "title": "Learning Spatially-Aware Language and Audio Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhavika Devnani",
      "Skyler Seto",
      "Zakaria Aldeneh",
      "Alessandro Toso",
      "Elena Menyaylenko",
      "Barry-John Theobald",
      "Jonathan Sheaffer",
      "Miguel Sarabia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3ae2d3297891cad0c56dd12d60ff7dde-Abstract-Conference.html": {
    "title": "A Neural Network Approach for Efficiently Answering Most Probable Explanation Queries in Probabilistic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivvrat Arya",
      "Tahrima Rahman",
      "Vibhav Gogate"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3b057de5a2e38bd8fa10201866c20dbf-Abstract-Conference.html": {
    "title": "RefDrop: Controllable Consistency in Image or Video Generation via Reference Feature Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaojiao Fan",
      "Haotian Xue",
      "Qinsheng Zhang",
      "Yongxin Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3b1675de6b49cc00084374213f8c38ae-Abstract-Conference.html": {
    "title": "Deep Equilibrium Algorithmic Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dobrik Georgiev",
      "Joseph Wilson",
      "Davide Buffelli",
      "Pietro Lió"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3b2fa36f85cda678363cc19cf62b7c5c-Abstract-Conference.html": {
    "title": "Disentangling the Roles of Distinct Cell Classes with Cell-Type Dynamical Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditi Jha",
      "Diksha Gupta",
      "Carlos D. Brody",
      "Jonathan W Pillow"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3b36fee06b8c4d66314122998a2f8ac0-Abstract-Conference.html": {
    "title": "MG-Net: Learn to Customize QAOA with Circuit Depth Awareness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Qian",
      "Xinbiao Wang",
      "Yuxuan Du",
      "Yong Luo",
      "Dacheng Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3b4e1336f775c3dba16ebbb8d2afd258-Abstract-Conference.html": {
    "title": "Provable Acceleration of Nesterov's Accelerated Gradient for Asymmetric Matrix Factorization and Linear Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao Xu",
      "Yuqing Wang",
      "Tuo Zhao",
      "Rachel Ward",
      "Molei Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3b576711b12ab036b45130fc8eb78504-Abstract-Conference.html": {
    "title": "S-STE: Continuous Pruning Function for Efficient 2:4 Sparse Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuezhou Hu",
      "Jun Zhu",
      "Jianfei Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3b62bca132cf5c8973b09a2fc6dc8ca6-Abstract-Conference.html": {
    "title": "Rethinking Score Distillation as a Bridge Between Image Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David McAllister",
      "Songwei Ge",
      "Jia-Bin Huang",
      "David Jacobs",
      "Alexei Efros",
      "Aleksander Holynski",
      "Angjoo Kanazawa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3b64416915026a6744bf10a819571041-Abstract-Conference.html": {
    "title": "Modeling Latent Neural Dynamics with Gaussian Process Switching Linear Dynamical Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amber Hu",
      "David Zoltowski",
      "Aditya Nair",
      "David Anderson",
      "Lea Duncker",
      "Scott Linderman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3b6aaffec941f98930753fa6d6de7263-Abstract-Conference.html": {
    "title": "Sparse maximal update parameterization: A holistic approach to sparse training dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nolan Dey",
      "Shane Bergsma",
      "Joel Hestness"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3b77109ad4dd4ba82d07cacd4b24207e-Abstract-Conference.html": {
    "title": "AHA: Human-Assisted Out-of-Distribution Generalization and Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyue Bai",
      "Jifan Zhang",
      "Robert Nowak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3b7db05c0e383518d789b6e93131f1f0-Abstract-Conference.html": {
    "title": "Efficient Sign-Based Optimization: Accelerating Convergence via Variance Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Jiang",
      "Sifan Yang",
      "Wenhao Yang",
      "Lijun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3b8db54b629e00537b59cbc6612026d7-Abstract-Conference.html": {
    "title": "DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Cui",
      "Hengkai Pan",
      "Aadhithya Iyer",
      "Siddhant Haldar",
      "Lerrel Pinto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3b930b4dc163c57b6a35aee5df89f9ed-Abstract-Conference.html": {
    "title": "Predicting Ground State Properties: Constant Sample Complexity and Deep Learning Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Wanner",
      "Laura Lewis",
      "Chiranjib Bhattacharyya",
      "Devdatt Dubhashi",
      "Alexandru Gheorghiu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3ba7560b4c3e66d760fbdd472cf4a5a9-Abstract-Conference.html": {
    "title": "Spiking Graph Neural Network on Riemannian Manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Sun",
      "Zhenhao Huang",
      "Qiqi Wan",
      "Hao Peng",
      "Philip S Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3baf4eeffad860ca9c54aeab632716b4-Abstract-Conference.html": {
    "title": "Vector Quantization Prompting for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Jiao",
      "Qiuxia LAI",
      "YU LI",
      "Qiang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3be14af22f0b311325664277f48111f4-Abstract-Conference.html": {
    "title": "Contextual Multinomial Logit Bandits with General Value Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxiao Zhang",
      "Haipeng Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3bf80b34f731313b8292f4578e820c90-Abstract-Conference.html": {
    "title": "MindMerger: Efficiently Boosting LLM Reasoning in non-English Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixian Huang",
      "Wenhao Zhu",
      "Gong Cheng",
      "Lei Li",
      "Fei Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3c1e1fdf305195cd620c118aaa9717ad-Abstract-Conference.html": {
    "title": "LLM-Check: Investigating Detection of Hallucinations in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurang Sriramanan",
      "Siddhant Bharti",
      "Vinu Sankar Sadasivan",
      "Shoumik Saha",
      "Priyatham Kattakinda",
      "Soheil Feizi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3c215225324f9988858602dc92219615-Abstract-Conference.html": {
    "title": "Shaping the distribution of neural responses with interneurons in a recurrent circuit model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Lipshutz",
      "Eero P. Simoncelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3c5ac360b070000646ce0490dab83cb7-Abstract-Conference.html": {
    "title": "Safe and Efficient: A Primal-Dual Method for Offline Convex CMDPs under Partial Data Coverage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobo Zhang",
      "Xiyue Peng",
      "Honghao Wei",
      "Xin Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3c9629e718d931e8d4d240378aa1d3bf-Abstract-Conference.html": {
    "title": "Accelerating Blockwise Parallel Language Models with Draft Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taehyeon Kim",
      "Ananda Theertha Suresh",
      "Kishore Papineni",
      "Michael D Riley",
      "Sanjiv Kumar",
      "Adrian Benton"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3cbf33008024aa1612ce853ef78e0e53-Abstract-Conference.html": {
    "title": "MotionBooth: Motion-Aware Customized Text-to-Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianzong Wu",
      "Xiangtai Li",
      "Yanhong Zeng",
      "Jiangning Zhang",
      "Qianyu Zhou",
      "Yining Li",
      "Yunhai Tong",
      "Kai Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3cbf627fa24fb6cb576e04e689b9428b-Abstract-Conference.html": {
    "title": "Neural Pose Representation Learning for Generating and Transferring Non-Rigid Object Poses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungwoo Yoo",
      "Juil Koo",
      "Kyeongmin Yeo",
      "Minhyuk Sung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3cd50f2922b7adaaa9e5113e35bae095-Abstract-Conference.html": {
    "title": "Energy-based Epistemic Uncertainty for Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Fuchsgruber",
      "Tom Wollschläger",
      "Stephan Günnemann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3d007df4ae13adf9001f8969555b11bd-Abstract-Conference.html": {
    "title": "Bridge the Modality and Capability Gaps in Vision-Language Model Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Yi",
      "Yuhang He",
      "De-Chuan Zhan",
      "Han-Jia Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3d03800841fa1bb2f43ef1750aafcce4-Abstract-Conference.html": {
    "title": "MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kailai Yang",
      "Zhiwei Liu",
      "Qianqian Xie",
      "Jimin Huang",
      "Tianlin Zhang",
      "Sophia Ananiadou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3d03b03197666b19c6a6e69812dd3e34-Abstract-Conference.html": {
    "title": "DOGS: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Chen",
      "Gim Hee Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3d13d910b48ac2e672a32cfdf98be1bf-Abstract-Conference.html": {
    "title": "Blind Image Restoration via Fast Diffusion Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamadi Chihaoui",
      "Abdelhak Lemkhenter",
      "Paolo Favaro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3d158f054ff0cb83397367234899db07-Abstract-Conference.html": {
    "title": "Web-Scale Visual Entity Recognition: An LLM-Driven Data Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathilde Caron",
      "Alireza Fathi",
      "Cordelia Schmid",
      "Ahmet Iscen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3d226fb8fbd6ee6ec70d0427f1319707-Abstract-Conference.html": {
    "title": "FedSSP: Federated Graph Learning with Spectral Knowledge and Personalized Preference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Tan",
      "Guancheng Wan",
      "Wenke Huang",
      "Mang Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3d27d607586984908900eaa8ce19c96c-Abstract-Conference.html": {
    "title": "Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Dangel",
      "Johannes Müller",
      "Marius Zeinhofer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3d3a9e085540c65dd3e5731361f9320e-Abstract-Conference.html": {
    "title": "SpeedLoader: An I/O efficient scheme for heterogeneous and distributed LLM operation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqi Zhang",
      "Yang You"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3d4c0a618d0acd7921493e4f30395c22-Abstract-Conference.html": {
    "title": "CoSy: Evaluating Textual Explanations of Neurons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laura Kopf",
      "Philine L Bommer",
      "Anna Hedström",
      "Sebastian Lapuschkin",
      "Marina Höhne",
      "Kirill Bykov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3d4dc72d715bd6415d356293079adf3d-Abstract-Conference.html": {
    "title": "Multistable Shape from Shading Emerges from Patch Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinran Han",
      "Todd Zickler",
      "Ko Nishino"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3d55170799265c03b37993e02b71b2cc-Abstract-Conference.html": {
    "title": "Exploring Behavior-Relevant and Disentangled Neural Dynamics with Generative Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yule Wang",
      "Chengrui Li",
      "Weihan Li",
      "Anqi Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3d77c6dcc7f143aa2154e7f4d5e22d68-Abstract-Conference.html": {
    "title": "Bidirectional Recurrence for Cardiac Motion Tracking with Gaussian Process Latent Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiewen Yang",
      "Yiqun Lin",
      "Bin Pu",
      "Xiaomeng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3d8f2fdc04fa66c9239f2eb14379546d-Abstract-Conference.html": {
    "title": "Algebraic Positional Encodings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantinos Kogkalidis",
      "Jean-Philippe Bernardy",
      "Vikas Garg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3d981a02079344710b96087d2bef8741-Abstract-Conference.html": {
    "title": "What is my quantum computer good for? Quantum capability learning with physics-aware neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Hothem",
      "Ashe Miller",
      "Timothy Proctor"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3d9ef68629089da055334c2d41dfcf93-Abstract-Conference.html": {
    "title": "Rule Extrapolation in Language Modeling: A Study of Compositional Generalization on OOD Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Mészáros",
      "Szilvia Ujváry",
      "Wieland Brendel",
      "Patrik Reizinger",
      "Ferenc Huszar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3db3e1f192877e47bf48c93cae238e51-Abstract-Conference.html": {
    "title": "Byzantine Robustness and Partial Participation Can Be Achieved at Once: Just Clip Gradient Differences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Grigory Malinovsky",
      "Peter Richtarik",
      "Samuel Horváth",
      "Eduard Gorbunov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3dbb8b6b5576b85afb3037e9630812dc-Abstract-Conference.html": {
    "title": "Is Your LiDAR Placement Optimized for 3D Scene Understanding?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Li",
      "Lingdong Kong",
      "Hanjiang Hu",
      "Xiaohao Xu",
      "Xiaonan Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3dbcadb7beedc2afe32bb23f75dd30ec-Abstract-Conference.html": {
    "title": "3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baohao Liao",
      "Christof Monz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3dc5d113ce0a746dfbcd90d3b6b463bc-Abstract-Conference.html": {
    "title": "Bridging OOD Detection and Generalization: A Graph-Theoretic View",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Wang",
      "Sharon Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3dd7c683eddc14f8cabcd6ce8d48cd41-Abstract-Conference.html": {
    "title": "Localize, Understand, Collaborate: Semantic-Aware Dragging via Intention Reasoner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xing Cui",
      "Peipei Li",
      "Zekun Li",
      "Xuannan Liu",
      "Yueying Zou",
      "Zhaofeng He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3df12798be2203f0a24a82c145f4de84-Abstract-Conference.html": {
    "title": "FashionR2R: Texture-preserving Rendered-to-Real Image Translation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Hu",
      "Qian He",
      "Gaofeng He",
      "Jiedong Zhuang",
      "Huang Chen",
      "Huafeng Liu",
      "Huamin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3df874367ce2c43891aab1ab23ae6959-Abstract-Conference.html": {
    "title": "Preference Alignment with Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minu Kim",
      "Yongsik Lee",
      "Sehyeok Kang",
      "Jihwan Oh",
      "Song Chong",
      "Se-Young Yun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3e0f495e21bdbdb4251792d0fff57928-Abstract-Conference.html": {
    "title": "Mean-Field Langevin Dynamics for Signed Measures via a Bilevel Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Wang",
      "Alireza Mousavi-Hosseini",
      "Lénaïc Chizat"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3e25d1aff47964c8409fd5c8dc0438d7-Abstract-Conference.html": {
    "title": "Advection Augmented Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niloufar Zakariaei",
      "Siddharth Rout",
      "Eldad Haber",
      "Moshe Eliasof"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3e2aeb66481dd63a32421bf032b70384-Abstract-Conference.html": {
    "title": "Extensive-Form Game Solving via Blackwell Approachability on Treeplexes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Darshan Chakrabarti",
      "Julien Grand-Clément",
      "Christian Kroer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3e2c12c1a41af7c19c5b38e0837a52d1-Abstract-Conference.html": {
    "title": "Quantum Algorithms for Non-smooth Non-convex Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengchang Liu",
      "Chaowen Guan",
      "Jianhao He",
      "John C. S. Lui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3e2d037e4e8e2c924d3c5e8cb4767150-Abstract-Conference.html": {
    "title": "Automatic Outlier Rectification via Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jose Blanchet",
      "Jiajin Li",
      "Markus Pelger",
      "Greg Zanotti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3e356b84727460df86c8d0eeff1ff85d-Abstract-Conference.html": {
    "title": "Topological obstruction to the training of shallow ReLU neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Nurisso",
      "Pierrick Leroy",
      "Francesco Vaccarino"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3e53d82a1113e3d240059a9195668edc-Abstract-Conference.html": {
    "title": "Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Li",
      "Qianshan Wei",
      "Chuanyi Zhang",
      "Guilin Qi",
      "Miaozeng Du",
      "Yongrui Chen",
      "Sheng Bi",
      "Fan Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3e9034dd5420660d86c8c360c35a895e-Abstract-Conference.html": {
    "title": "Mining and Transferring Feature-Geometry Coherence for Unsupervised Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "KeZheng Xiong",
      "Haoen Xiang",
      "Qingshan Xu",
      "Chenglu Wen",
      "Siqi Shen",
      "Jonathan Jun LI",
      "Cheng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3e9412a9c1d93810ef3ef7825115016b-Abstract-Conference.html": {
    "title": "Improving robustness to corruptions with multiplicative weight perturbations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quoc Trung Trinh",
      "Markus Heinonen",
      "Luigi Acerbi",
      "Samuel Kaski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3ea832724870c700f0a03c665572e2a9-Abstract-Conference.html": {
    "title": "LeDex: Training LLMs to Better Self-Debug and Explain Code",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Jiang",
      "Xiaopeng Li",
      "Shiqi Wang",
      "Qiang Zhou",
      "Soneya Hossain",
      "Baishakhi Ray",
      "Varun Kumar",
      "Xiaofei Ma",
      "Anoop Deoras"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3eb660055cdcdc9a545a0b16c1eff80d-Abstract-Conference.html": {
    "title": "Guiding Neural Collapse: Optimising Towards the Nearest Simplex Equiangular Tight Frame",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evan Markou",
      "Thalaiyasingam Ajanthan",
      "Stephen Gould"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3ec4f1d5759d6d3d340a66638a52944b-Abstract-Conference.html": {
    "title": "Time-Constrained Robust MDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adil Zouitine",
      "David Bertoin",
      "Pierre Clavier",
      "Matthieu Geist",
      "Emmanuel Rachelson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3ec6c6fc9065aa57785eb05dffe7c3db-Abstract-Conference.html": {
    "title": "CemiFace: Center-based Semi-hard Synthetic Face Generation for Face Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhonglin Sun",
      "Siyang Song",
      "Ioannis Patras",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3ec7806669b4048cdba4d1defc76ace3-Abstract-Conference.html": {
    "title": "Randomized Strategic Facility Location with Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Balkanski",
      "Vasilis Gkatzelis",
      "Golnoosh Shahkarami"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3eceb70f47690051d6769739fbf6294b-Abstract-Conference.html": {
    "title": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Owen Dugan",
      "Donato Jiménez-Benetó",
      "Charlotte Loh",
      "Zhuo Chen",
      "Rumen Dangovski",
      "Marin Soljacic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3ed57b293db0aab7cc30c44f45262348-Abstract-Conference.html": {
    "title": "ProSST: Protein Language Modeling with Quantized Structure and Disentangled Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingchen Li",
      "Yang Tan",
      "Xinzhu Ma",
      "Bozitao Zhong",
      "Huiqun Yu",
      "Ziyi Zhou",
      "Wanli Ouyang",
      "Bingxin Zhou",
      "Pan Tan",
      "Liang Hong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3eec5006051d9544e717067de3220198-Abstract-Conference.html": {
    "title": "The Dormant Neuron Phenomenon in Multi-Agent Reinforcement Learning Value Factorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyuan Qin",
      "Chennan Ma",
      "Deng",
      "Zhengzhu Liu",
      "Songzhu Mei",
      "Xinwang Liu",
      "Cheng Wang",
      "Siqi Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3eff068e195daace49955348de9f8398-Abstract-Conference.html": {
    "title": "Infinite Limits of Multi-head Transformer Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Blake Bordelon",
      "Hamza Chaudhry",
      "Cengiz Pehlevan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3effb91593c4fb42b1da1528328eff49-Abstract-Conference.html": {
    "title": "WeiPer: OOD Detection using Weight Perturbations of Class Projections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Granz",
      "Manuel Heurich",
      "Tim Landgraf"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3f0c8c5dc6b16e601b78e164a70d68a2-Abstract-Conference.html": {
    "title": "On the Impacts of the Random Initialization in the Neural Tangent Kernel Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guhan Chen",
      "Yicheng Li",
      "Qian Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3f52ab4322e967efd312c38a68d07f01-Abstract-Conference.html": {
    "title": "Boundary Matters: A Bi-Level Active Finetuning Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Lu",
      "Yichen Xie",
      "Xiaokang Yang",
      "Junchi Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3f54333743f508c891eaa4c8734d5235-Abstract-Conference.html": {
    "title": "Implicitly Guided Design with PropEn: Match your Data to Follow the Gradient",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nataša Tagasovska",
      "Vladimir Gligorijevic",
      "Kyunghyun Cho",
      "Andreas Loukas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3f630b20b7b3ac76d3a0016fe29b6dc0-Abstract-Conference.html": {
    "title": "Separations in the Representational Capabilities of Transformers and Recurrent Architectures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Satwik Bhattamishra",
      "Michael Hahn",
      "Phil Blunsom",
      "Varun Kanade"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3f66d5cdbe032bb750f2dc523357b7a5-Abstract-Conference.html": {
    "title": "Multistep Distillation of Diffusion Models via Moment Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Salimans",
      "Thomas Mensink",
      "Jonathan Heek",
      "Emiel Hoogeboom"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3f8c7eb848ffec848f3ed2b7ca44915d-Abstract-Conference.html": {
    "title": "AUC Maximization under Positive Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atsutoshi Kumagai",
      "Tomoharu Iwata",
      "Hiroshi Takahashi",
      "Taishi Nishiyama",
      "Yasuhiro Fujiwara"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3f9bbf77fbd858e5b6e39d39fe84ed2e-Abstract-Conference.html": {
    "title": "NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting-Hsuan Chen",
      "Jie Wen Chan",
      "Hau-Shiang Shiu",
      "Shih-Han Yen",
      "Changhan Yeh",
      "Yu-Lun Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3f9bf45ea04c98ad7cb857f951f499e2-Abstract-Conference.html": {
    "title": "Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadi Pouransari",
      "Chun-Liang Li",
      "Jen-Hao Chang",
      "Pavan Kumar Anasosalu Vasu",
      "Cem Koc",
      "Vaishaal Shankar",
      "Oncel Tuzel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3fa2d2b637122007845a2fbb7c21453b-Abstract-Conference.html": {
    "title": "Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Xu",
      "Yuejie Chi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3fb6c52aeb11e09053c16eabee74dd7b-Abstract-Conference.html": {
    "title": "Mobility-LLM: Learning Visiting Intentions and Travel Preference from Human Mobility Data with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Letian Gong",
      "Yan Lin",
      "zxy",
      "Yiwen Lu",
      "Xuedi Han",
      "Yichen Liu",
      "Shengnan Guo",
      "Youfang Lin",
      "Huaiyu Wan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3fb97beb678a1c46bf25f40b3a8c6f98-Abstract-Conference.html": {
    "title": "Probabilistic Conformal Distillation for Enhancing Missing Modality Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxi Chen",
      "Fei Zhang",
      "Zihua Zhao",
      "Jiangchao Yao",
      "Ya Zhang",
      "Yanfeng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3fcce87e6df22b2ab6f0be68af3ec714-Abstract-Conference.html": {
    "title": "Causal discovery with endogenous context variables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wiebke Günther",
      "Oana-Iuliana Popescu",
      "Martin Rabel",
      "Urmi Ninad",
      "Andreas Gerhardus",
      "Jakob Runge"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3fe2a777282299ecb4f9e7ebb531f0ab-Abstract-Conference.html": {
    "title": "Medformer: A Multi-Granularity Patching Transformer for Medical Time-Series Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihe Wang",
      "Nan Huang",
      "Taida Li",
      "Yujun Yan",
      "Xiang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/3fefebc2d4e3c1c6ee9b892bd293117d-Abstract-Conference.html": {
    "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hugo Cui",
      "Freya Behrens",
      "Florent Krzakala",
      "Lenka Zdeborová"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/401ece9f5d1cfa8600c22049ef43930e-Abstract-Conference.html": {
    "title": "Learning to Reason via Program Generation, Emulation, and Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathaniel Weir",
      "Muhammad Khalifa",
      "Linlu Qiu",
      "Orion Weller",
      "Peter Clark"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/40223e204a9e18dd8b01d7d11ea97939-Abstract-Conference.html": {
    "title": "Mutli-Armed Bandits with Network Interference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhineet Agarwal",
      "Anish Agarwal",
      "Lorenzo Masoero",
      "Justin Whitehouse"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/402542c2341e5d2eadc1dd0891275901-Abstract-Conference.html": {
    "title": "Bayesian Nonparametrics Meets Data-Driven Distributionally Robust Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicola Bariletto",
      "Nhat Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/40386e4770bebd63fdf47cbc67341c0b-Abstract-Conference.html": {
    "title": "On the Ability of Developers' Training Data Preservation of Learnware",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao-Yi Lei",
      "Zhi-Hao Tan",
      "Zhi-Hua Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4038c9208dfc22644c60ad39c24e5c53-Abstract-Conference.html": {
    "title": "Learning the Optimal Policy for Balancing Short-Term and Long-Term Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinwei Yang",
      "Xueqing Liu",
      "Yan Zeng",
      "Ruocheng Guo",
      "Yang Liu",
      "Peng Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/403bf224290de69c7d5dc856f5a99d9e-Abstract-Conference.html": {
    "title": "Interpreting Learned Feedback Patterns in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luke Marks",
      "Amir Abdullah",
      "Clement Neo",
      "Rauno Arike",
      "David Krueger",
      "Philip Torr",
      "Fazl Barez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/403d7aae69d2f2926dadb35499e1a105-Abstract-Conference.html": {
    "title": "What Makes CLIP More Robust to Long-Tailed Pre-Training Data? A Controlled Study for Transferable Insights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Wen",
      "Bingchen Zhao",
      "Yilun Chen",
      "Jiangmiao Pang",
      "Xiaojuan Qi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/404df2480b6eef0486a1679e371894b0-Abstract-Conference.html": {
    "title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamish Ivison",
      "Yizhong Wang",
      "Jiacheng Liu",
      "Zeqiu Wu",
      "Valentina Pyatkin",
      "Nathan Lambert",
      "Noah A. Smith",
      "Yejin Choi",
      "Hanna Hajishirzi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4054556fcaa934b0bf76da52cf4f92cb-Abstract-Conference.html": {
    "title": "Benign overfitting in leaky ReLU networks with moderate input dimension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kedar Karhadkar",
      "Erin George",
      "Michael Murray",
      "Guido F. Montufar",
      "Deanna Needell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/40889e2aedb47e64d42dd8b8de2fab4e-Abstract-Conference.html": {
    "title": "On the Computational Complexity of Private High-dimensional Model Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saptarshi Roy",
      "Zehua Wang",
      "Ambuj Tewari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/409334f42cbb57d07aa152f2d0433ec7-Abstract-Conference.html": {
    "title": "SpGesture: Source-Free Domain-adaptive sEMG-based Gesture Recognition with Jaccard Attentive Spiking Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyu Guo",
      "Ying Sun",
      "Yijie Xu",
      "Ziyue Qiao",
      "Yongkui Yang",
      "Hui Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/40954ac18a457dd5f11145bae6454cdf-Abstract-Conference.html": {
    "title": "Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yimeng Zhang",
      "Xin Chen",
      "Jinghan Jia",
      "Yihua Zhang",
      "Chongyu Fan",
      "Jiancheng Liu",
      "Mingyi Hong",
      "Ke Ding",
      "Sijia Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/409fcc9d24b549969b8b9be68b56a7be-Abstract-Conference.html": {
    "title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zinan Guo",
      "Yanze Wu",
      "Chen Zhuowei",
      "Lang chen",
      "Peng Zhang",
      "Qian HE"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/40cf27290cc2bd98a428b567ba25075c-Abstract-Conference.html": {
    "title": "Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Brenner",
      "Christoph Jürgen Hemmer",
      "Zahra Monfared",
      "Daniel Durstewitz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/40eff1670d6b08bb1bda48b0c5f30110-Abstract-Conference.html": {
    "title": "D2R2: Diffusion-based Representation with Random Distance Matching for Tabular Few-shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoxue Liu",
      "Linjiajie Fang",
      "Wenjia Wang",
      "Bingyi Jing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/411fa9d368b5485be4c6bb62615b365e-Abstract-Conference.html": {
    "title": "Learning Discrete Concepts in Latent Hierarchical Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingjing Kong",
      "Guangyi Chen",
      "Biwei Huang",
      "Eric P. Xing",
      "Yuejie Chi",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/412f33b7bfecf8d0318113a325339c96-Abstract-Conference.html": {
    "title": "Scaling White-Box Transformers for Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinrui Yang",
      "Xianhang Li",
      "Druv Pai",
      "Yuyin Zhou",
      "Yi Ma",
      "Yaodong Yu",
      "Cihang Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/412fb8623bf8b6d56fb6285ea295447e-Abstract-Conference.html": {
    "title": "Bridging Gaps: Federated Multi-View Clustering in Heterogeneous Hybrid Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyue Chen",
      "Yazhou Ren",
      "Jie Xu",
      "Fangfei Lin",
      "Xiaorong Pu",
      "Yang Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/413885e70482b95dcbeeddc1daf39177-Abstract-Conference.html": {
    "title": "Robust and Faster Zeroth-Order Minimax Optimization: Complexity and Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixin An",
      "Yuanyuan Liu",
      "Fanhua Shang",
      "Hongying Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4140fe26102db5fea1f40118afc7137b-Abstract-Conference.html": {
    "title": "Generalization Error Bounds for Two-stage Recommender Systems with Tree Structure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Zhang",
      "Ze Liu",
      "Defu Lian",
      "Enhong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4147dfaa46cd7e20a2aecb91097ae8cc-Abstract-Conference.html": {
    "title": "Group Robust Preference Optimization in Reward-free RLHF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shyam Sundhar Ramesh",
      "Yifan Hu",
      "Iason Chaimalas",
      "Viraj Mehta",
      "Pier Giuseppe Sessa",
      "Haitham Bou Ammar",
      "Ilija Bogunovic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4163873c9ad623a87989d0a6eefe9442-Abstract-Conference.html": {
    "title": "Polyhedral Complex Derivation from Piecewise Trilinear Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin-Hwa Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4175b70823a51e9c41f811555ee8bb53-Abstract-Conference.html": {
    "title": "Differentiable Quantum Computing for Large-scale Linear Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Connor Clayton",
      "Jiaqi Leng",
      "Gengzhi Yang",
      "Yi-Ling Qiao",
      "Ming Lin",
      "Xiaodi Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/419b6c974712adb884bfbbeea8e94d1b-Abstract-Conference.html": {
    "title": "TopoFR: A Closer Look at Topology Alignment on Face Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Dan",
      "Yang Liu",
      "Jiankang Deng",
      "Haoyu Xie",
      "Siyuan Li",
      "Baigui Sun",
      "Shan Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/419ce7cb77aa90868c16882ce4257a69-Abstract-Conference.html": {
    "title": "Fair Online Bilateral Trade",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "François Bachoc",
      "Nicolò Cesa-Bianchi",
      "Tom Cesari",
      "Roberto Colomboni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/419f72cbd568ad62183f8132a3605a2a-Abstract-Conference.html": {
    "title": "Towards Exact Gradient-based Training on Analog In-memory Computing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoxian Wu",
      "Tayfun Gokmen",
      "Malte Rasch",
      "Tianyi Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/41aa1c9f57ea83d7c41f0d3e98ed3dd4-Abstract-Conference.html": {
    "title": "Fast Tree-Field Integrators: From Low Displacement Rank to Topological Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krzysztof M Choromanski",
      "Arijit Sehanobish",
      "Somnath Basu Roy Chowdhury",
      "Han Lin",
      "Kumar Avinava Dubey",
      "Tamas Sarlos",
      "Snigdha Chaturvedi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/41b0c83e8bbfc706c14a016635415660-Abstract-Conference.html": {
    "title": "Barely Random Algorithms and Collective Metrical Task Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Romain Cosson",
      "Laurent Massoulié"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/41bba7b0f5c81e789a20bb16a370aeeb-Abstract-Conference.html": {
    "title": "Aligning Large Language Models with Representation Editing: A Control Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingkai Kong",
      "Haorui Wang",
      "Wenhao Mu",
      "Yuanqi Du",
      "Yuchen Zhuang",
      "Yifei Zhou",
      "Yue Song",
      "Rongzhi Zhang",
      "Kai Wang",
      "Chao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/41c38a83bd97ba28505b4def82676ba5-Abstract-Conference.html": {
    "title": "Adjust Pearson's $r$ to Measure Arbitrary Monotone Dependence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinbo Ai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/41ca8a0eb2bc4927a499b910934b9b81-Abstract-Conference.html": {
    "title": "Disentangling Interpretable Factors with Supervised Independent Subspace Principal Component Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayu Su",
      "David A Knowles",
      "Raúl Rabadán"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/41ed4bd197d0a5fa036d361c1fc606ad-Abstract-Conference.html": {
    "title": "Why the Metric Backbone Preserves Community Structure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilien Dreveton",
      "Charbel Chucri",
      "Matthias Grossglauser",
      "Patrick Thiran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/41efc12982eca6f8bb5e48dc3a84b843-Abstract-Conference.html": {
    "title": "Unified Graph Augmentations for Generalized Contrastive Learning on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Zhuo",
      "Yintong Lu",
      "Hui Ning",
      "Kun Fu",
      "bingxin niu",
      "Dongxiao He",
      "Chuan Wang",
      "Yuanfang Guo",
      "Zhen Wang",
      "Xiaochun Cao",
      "Liang Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/41f3347f8f47c17bbadaed584e68d8bd-Abstract-Conference.html": {
    "title": "MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Chen",
      "Zhilei Bei",
      "Xingyi Cheng",
      "Pan Li",
      "Jie Tang",
      "Le Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/41fb2ecb5b7d1b505bca787de0a603dc-Abstract-Conference.html": {
    "title": "DiffGS: Functional Gaussian Splatting Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junsheng Zhou",
      "Weiqi Zhang",
      "Yu-Shen Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/423d0909791493b7c10916fd328c2913-Abstract-Conference.html": {
    "title": "Towards Editing Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baoyu Jing",
      "Shuqi Gu",
      "Tianyu Chen",
      "Zhiyu Yang",
      "Dongsheng Li",
      "Jingrui He",
      "Kan Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4241c27d3161c7a7064bfc1a6e539563-Abstract-Conference.html": {
    "title": "ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Meng",
      "Kayhan Behdin",
      "Haoyue Wang",
      "Rahul Mazumder"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/42475c537936b2394b5015e871765056-Abstract-Conference.html": {
    "title": "From Linear to Linearizable Optimization: A Novel Framework with Applications to Stationary and Non-stationary DR-submodular Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Pedramfar",
      "Vaneet Aggarwal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4254e856d01a5e7b7ea050477c3ef9b9-Abstract-Conference.html": {
    "title": "Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model on Knowledge Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyi Chen",
      "Panrong Tong",
      "Zhongming Jin",
      "Ying Sun",
      "Jieping Ye",
      "Hui Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4268cf6ba253744befa41ffee5083fa0-Abstract-Conference.html": {
    "title": "Efficient Sketches for Training Data Attribution and Studying the Loss Landscape",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Schioppa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/426fc72e404c4dfafc1edd62f4971dc2-Abstract-Conference.html": {
    "title": "To Learn or Not to Learn, That is the Question — A Feature-Task Dual Learning Model of Perceptual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Liu",
      "Muyang Lyu",
      "Cong Yu",
      "Si Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/42770daf4a3384b712ea9c36e9279998-Abstract-Conference.html": {
    "title": "Efficient Temporal Action Segmentation via Boundary-aware Query Voting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiyao Wang",
      "Yuewei Lin",
      "Erik Blasch",
      "jie wei",
      "Haibin Ling"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/42a24a7a3e7cf7dd127a76daf487cffd-Abstract-Conference.html": {
    "title": "Beyond task diversity: provable representation transfer for sequential multitask linear bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thang Duong",
      "Zhi Wang",
      "Chicheng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/42aae0e655c77d93edad9171ad9f4717-Abstract-Conference.html": {
    "title": "Time Makes Space: Emergence of Place Fields in Networks Encoding Temporally Continuous Sensory Experiences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoze Wang",
      "Ronald Di Tullio",
      "Spencer Rooke",
      "Vijay Balasubramanian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/42c40aff7814e9796266e12053b1c610-Abstract-Conference.html": {
    "title": "Gradient-Variation Online Learning under Generalized Smoothness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan-Feng Xie",
      "Peng Zhao",
      "Zhi-Hua Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/43069caa6776eac8bca4bfd74d4a476d-Abstract-Conference.html": {
    "title": "Learnability Matters: Active Learning for Video Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqian Zhang",
      "Buyu Liu",
      "Jun Bao",
      "Qiang Huang",
      "Min Zhang",
      "Jun Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/430894999584d0bd358611e2ecf00b15-Abstract-Conference.html": {
    "title": "Soft ascent-descent as a stable and flexible alternative to flooding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Holland",
      "Kosuke Nakatani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4309616aaed8e848009bc4a7ef73b493-Abstract-Conference.html": {
    "title": "RanDumb: Random Representations Outperform Online Continually Learned Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ameya Prabhu",
      "Shiven Sinha",
      "Ponnurangam Kumaraguru",
      "Philip Torr",
      "Ozan Sener",
      "Puneet Dokania"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/434958fd1edf1f027306cb7f1b75fbf5-Abstract-Conference.html": {
    "title": "Randomized Truthful Auctions with Learning Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gagan Aggarwal",
      "Anupam Gupta",
      "Andres Perlroth",
      "Grigoris Velegkas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/435422305988b73c6cc00bcb29ba2531-Abstract-Conference.html": {
    "title": "ContactField: Implicit Field Representation for Multi-Person Interaction Geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hansol Lee",
      "Tackgeun You",
      "Hansoo Park",
      "Woohyeon Shim",
      "Sanghyeon Kim",
      "Hwasup Lim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4364fef031fdf7bfd9d1c9c56b287084-Abstract-Conference.html": {
    "title": "Active Perception for Grasp Detection via Neural Graspness Field",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxiang Ma",
      "Modi Shi",
      "Boyang Gao",
      "Di Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/437587c32b781fbb94026ac5905b439b-Abstract-Conference.html": {
    "title": "A Bayesian Approach to Data Point Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "XINNUO XU",
      "Minyoung Kim",
      "Royson Lee",
      "Brais Martinez",
      "Timothy Hospedales"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/437cd2749391ad40f67e4dd1d87c4596-Abstract-Conference.html": {
    "title": "Bayesian Optimization of Functions over Node Subsets in Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huidong Liang",
      "Xingchen Wan",
      "Xiaowen Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/439bf902de1807088d8b731ca20b0777-Abstract-Conference.html": {
    "title": "Mission Impossible: A Statistical Perspective on Jailbreaking LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingtong Su",
      "Julia Kempe",
      "Karen Ullrich"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/43ab1646052dab79731f5d70bf40f6dc-Abstract-Conference.html": {
    "title": "A Geometric View of Data Complexity: Efficient Local Intrinsic Dimension Estimation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamid Kamkari",
      "Brendan Ross",
      "Rasa Hosseinzadeh",
      "Jesse Cresswell",
      "Gabriel Loaiza-Ganem"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/43ba0466af2b1ac76aa85d8fbec714e3-Abstract-Conference.html": {
    "title": "Simplifying Latent Dynamics with Softly State-Invariant World Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tankred Saanum",
      "Peter Dayan",
      "Eric Schulz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/43c6d9b0d11696f89c75157919a0334a-Abstract-Conference.html": {
    "title": "When is Multicalibration Post-Processing Necessary?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dutch Hansen",
      "Siddartha Devic",
      "Preetum Nakkiran",
      "Vatsal Sharan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/43d1d3bdd92204c96fa4ac3c578f6a33-Abstract-Conference.html": {
    "title": "Learning Multimodal Behaviors from Scratch with Diffusion Policy Gradient",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steven Li",
      "Rickmer Krohn",
      "Tao Chen",
      "Anurag Ajay",
      "Pulkit Agrawal",
      "Georgia Chalvatzaki"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/43d33182360378d5c8e69dd706c24f2f-Abstract-Conference.html": {
    "title": "Attack-Resilient Image Watermarking Using Stable Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijun Zhang",
      "Xiao Liu",
      "Antoni Martin",
      "Cindy Bearfield",
      "Yuriy Brun",
      "Hui Guan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/43d7bc009cf5171e7af77a91ee4bb890-Abstract-Conference.html": {
    "title": "Activating Self-Attention for Multi-Scene Absolute Pose Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miso Lee",
      "Jihwan Kim",
      "Jae-Pil Heo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/43d8e5fc816c692f342493331d5e98fc-Abstract-Conference.html": {
    "title": "SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialu Li",
      "Jaemin Cho",
      "Yi-Lin Sung",
      "Jaehong Yoon",
      "Mohit Bansal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/43eeff8a47075418c3e2b8b204053448-Abstract-Conference.html": {
    "title": "Generalized Protein Pocket Generation with Prior-Informed Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZAIXI ZHANG",
      "Marinka Zitnik",
      "Qi Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/43f5f6c5cb333115914c8448b8506411-Abstract-Conference.html": {
    "title": "Reinforcement Learning Gradients as Vitamin for Online Finetuning Decision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Yan",
      "Alex Schwing",
      "Yu-Xiong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4418f6a54f4314202688d77956e731ce-Abstract-Conference.html": {
    "title": "Bias Amplification in Language Model Evolution: An Iterated Learning Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Ren",
      "Shangmin Guo",
      "Linlu Qiu",
      "Bailin Wang",
      "Danica J. Sutherland"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/442443dbe8c4c7e1df7eda140921de36-Abstract-Conference.html": {
    "title": "Credal Learning Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michele Caprio",
      "Maryam Sultana",
      "Eleni Elia",
      "Fabio Cuzzolin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/442553041ab2d860c013b870d5e8f167-Abstract-Conference.html": {
    "title": "On the Power of Small-size Graph Neural Networks for Linear Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Li",
      "Tian Ding",
      "Linxin Yang",
      "Minghui Ouyang",
      "Qingjiang Shi",
      "Ruoyu Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4426af45e987692abf1b80108951ff8a-Abstract-Conference.html": {
    "title": "An eye for an ear: zero-shot audio description leveraging an image captioner with audio-visual token distribution matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hugo Malard",
      "Michel Olvera",
      "Stéphane Lathuilière",
      "Slim Essid"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4462db5eee6823b2abad0d1f955e187a-Abstract-Conference.html": {
    "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yabin Zhang",
      "Lei Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4476dd7320e0eba63961990d73525064-Abstract-Conference.html": {
    "title": "Maximizing utility in multi-agent environments by anticipating the behavior of other learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angelos Assos",
      "Yuval Dagan",
      "Constantinos Daskalakis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/447d012bd95b6767a4bfdebf96cdfcc9-Abstract-Conference.html": {
    "title": "Generative Hierarchical Materials Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sherry Yang",
      "Simon Batzner",
      "Ruiqi Gao",
      "Muratahan Aykol",
      "Alexander Gaunt",
      "Brendan C McMorrow",
      "Danilo Jimenez Rezende",
      "Dale Schuurmans",
      "Igor Mordatch",
      "Ekin Dogus Cubuk"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/448444518637da106d978ae7409d9789-Abstract-Conference.html": {
    "title": "Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhao Yao",
      "Xiaolin Hu",
      "Shenzhi Yang",
      "Yong Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/448abd486677165ceedfa790e9a61802-Abstract-Conference.html": {
    "title": "Certified Machine Unlearning via Noisy Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eli Chien",
      "Haoyu Wang",
      "Ziang Chen",
      "Pan Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/449a016a6ce6fba3fe50d05482abf836-Abstract-Conference.html": {
    "title": "$\\boldsymbol{\\mu}\\mathbf{P^2}$: Effective Sharpness Aware Minimization Requires Layerwise Perturbation Scaling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moritz Haas",
      "Jin Xu",
      "Volkan Cevher",
      "Leena Chennuru Vankadara"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/44af065477781e7f8a8589b14a62c489-Abstract-Conference.html": {
    "title": "Leveraging Environment Interaction for Automated PDDL Translation and Planning with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sadegh Mahdavi",
      "Raquel Aoki",
      "Keyi Tang",
      "Yanshuai Cao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/44b61c5c0ba06d55ab5a1cfb9cfff763-Abstract-Conference.html": {
    "title": "BiDM: Pushing the Limit of Quantization for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Zheng",
      "Xianglong Liu",
      "Yichen Bian",
      "Xudong Ma",
      "Yulun Zhang",
      "Jiakai Wang",
      "Jinyang Guo",
      "Haotong Qin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/44cb9aa2897a288f7e6d9dd66659d523-Abstract-Conference.html": {
    "title": "Gated Inference Network: Inference and Learning State-Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamidreza Hashempoorikderi",
      "Wan Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/44e67d5eea7c7cf7e9958b32eaa52775-Abstract-Conference.html": {
    "title": "FedGTST: Boosting Global Transferability of Federated Models via Statistics Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evelyn Ma",
      "Chao Pan",
      "S. Rasoul Etesami",
      "Han Zhao",
      "Olgica Milenkovic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/450c10220669b024e46d638bfe347558-Abstract-Conference.html": {
    "title": "Zero-Shot Scene Reconstruction from Single Images with Deep Prior Assembly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junsheng Zhou",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4526cfacdbca6b6e184568dac91bf070-Abstract-Conference.html": {
    "title": "Binding in hippocampal-entorhinal circuits enables compositionality in cognitive maps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Kymn",
      "Sonia Mazelet",
      "Anthony Thomas",
      "Denis Kleyko",
      "Edward Frady",
      "Fritz Sommer",
      "Bruno A. Olshausen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/45326c2df19fee16fc1ebc44941fea8e-Abstract-Conference.html": {
    "title": "GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZAITANG LI",
      "Pin-Yu Chen",
      "Tsung-Yi Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4537592f9594a0522da99566b90380cc-Abstract-Conference.html": {
    "title": "Multivariate Stochastic Dominance via Optimal Transport and Applications to Models Benchmarking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Rioux",
      "Apoorva Nitsure",
      "Mattia Rigotti",
      "Kristjan Greenewald",
      "Youssef Mroueh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/453a27b717972ef94a9a9113d236ad2f-Abstract-Conference.html": {
    "title": "Rethinking Misalignment in Vision-Language Model Adaptation from a Causal Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanan Zhang",
      "Jiangmeng Li",
      "Lixiang Liu",
      "Wenwen Qiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4540d267eeec4e5dbd9dae9448f0b739-Abstract-Conference.html": {
    "title": "EEGPT: Pretrained Transformer for Universal and Reliable Representation of EEG Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyu Wang",
      "Wenchao Liu",
      "Yuhong He",
      "Cong Xu",
      "Lin Ma",
      "Haifeng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/456ce0476c9b4689a74918b851cecd5a-Abstract-Conference.html": {
    "title": "DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongpai Gao",
      "Benjamin Planche",
      "Meng Zheng",
      "Xiao Chen",
      "Terrence Chen",
      "Ziyan Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/458567910b6d21f438f22aa20c036723-Abstract-Conference.html": {
    "title": "ChatTracker: Enhancing Visual Tracking Performance via Chatting with Multimodal Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Sun",
      "Fan Yu",
      "Shaoxiang Chen",
      "Yu Zhang",
      "Junwei Huang",
      "Yang Li",
      "Chenhui Li",
      "Changbo Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/458d9f2dd5c7565af60143630dc62f10-Abstract-Conference.html": {
    "title": "L-TTA: Lightweight Test-Time Adaptation Using a Versatile Stem Layer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Shin",
      "Hyun Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/458fa8ee331566383d8e74bdb647f829-Abstract-Conference.html": {
    "title": "Sample Complexity of Interventional Causal Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emre Acartürk",
      "Burak Varıcı",
      "Karthikeyan Shanmugam",
      "Ali Tajer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/459d93dad139eb084c365d40a57eada3-Abstract-Conference.html": {
    "title": "GAMap: Zero-Shot Object Goal Navigation with Multi-Scale Geometric-Affordance Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "shuaihang yuan",
      "Hao Huang",
      "Yu Hao",
      "Congcong Wen",
      "Anthony Tzes",
      "Yi Fang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/45a30141c6719e9cfedfb51f1c665a37-Abstract-Conference.html": {
    "title": "Dual-Personalizing Adapter for Federated Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "yiyuan yang",
      "Guodong Long",
      "Tao Shen",
      "Jing Jiang",
      "Michael Blumenstein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/45a7ca247462d9e465ee88c8a302ca70-Abstract-Conference.html": {
    "title": "SlimSAM: 0.1% Data Makes Segment Anything Slim",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zigeng Chen",
      "Gongfan Fang",
      "Xinyin Ma",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/45afdc1958befe9b60af7b445e768b10-Abstract-Conference.html": {
    "title": "Group-wise oracle-efficient algorithms for online multi-group learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Deng",
      "Jingwen Liu",
      "Daniel J. Hsu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/45cc967d7899616f51993b7b363d35b5-Abstract-Conference.html": {
    "title": "Diff-eRank: A Novel Rank-Based Metric for Evaluating Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lai Wei",
      "Zhiquan Tan",
      "Chenghai Li",
      "Jindong Wang",
      "Weiran Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/45d4924460c37853d57885d8af0b8d5c-Abstract-Conference.html": {
    "title": "LG-CAV: Train Any Concept Activation Vector with Language Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihan Huang",
      "Jie Song",
      "Mengqi Xue",
      "Haofei Zhang",
      "Bingde Hu",
      "Huiqiong Wang",
      "Hao Jiang",
      "Xingen Wang",
      "Mingli Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/45e409b46bebd648e9041a628a1a9964-Abstract-Conference.html": {
    "title": "Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhang",
      "Lei Cao",
      "Jiayi Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/45ed1a72597594c097152ef9cc187762-Abstract-Conference.html": {
    "title": "Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Min",
      "Yawei Luo",
      "Jianwen Sun",
      "Yi Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/45f7927942098d14e473fc5d000031e2-Abstract-Conference.html": {
    "title": "Interpreting and Analysing CLIP's Zero-Shot Image Classification via Mutual Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fawaz Sammani",
      "Nikos Deligiannis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/45f7ad60c01f17711ccd8ac2f2fb77e3-Abstract-Conference.html": {
    "title": "An exactly solvable model for emergence and scaling laws in the multitask sparse parity problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "yoonsoo nam",
      "Nayara Fonseca",
      "Seok Hyeong Lee",
      "Chris Mingard",
      "Ard Louis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/45fc4a0da7e7f6fbabaabe2d20a441d1-Abstract-Conference.html": {
    "title": "The Sample-Communication Complexity Trade-off in Federated Q-Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudeep Salgia",
      "Yuejie Chi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/460e1f983103d38832e6c79cbaa91471-Abstract-Conference.html": {
    "title": "Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Yang",
      "Jie Wang",
      "Guoping Wu",
      "Bin Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4620a66570e554a3ff0e39dc59bcb07a-Abstract-Conference.html": {
    "title": "Harmonizing Stochasticity and Determinism: Scene-responsive Diverse Human Motion Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Lou",
      "Qiongjie Cui",
      "Tuo Wang",
      "Zhenbo Song",
      "Luoming Zhang",
      "Cheng Cheng",
      "Haofan Wang",
      "Xu Tang",
      "Huaxia Li",
      "Hong Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/463a91da3c832bd28912cd0d1b8d9974-Abstract-Conference.html": {
    "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin-Bin Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/464d8334dfca732d538ac050e45c6dc7-Abstract-Conference.html": {
    "title": "Harnessing small projectors and multiple views for efficient vision pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arna Ghosh",
      "Kumar Krishna Agrawal",
      "Shagun Sodhani",
      "Adam Oberman",
      "Blake Richards"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/465a13a95741fab2e912f98adb07df1d-Abstract-Conference.html": {
    "title": "AttnDreamBooth: Towards Text-Aligned Personalized Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lianyu Pang",
      "Jian Yin",
      "Baoquan Zhao",
      "Feize Wu",
      "Fu Lee Wang",
      "Qing Li",
      "Xudong Mao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/466a8a8ae45c488b2bd312699ba1e5ee-Abstract-Conference.html": {
    "title": "Bridge-IF: Learning Inverse Protein Folding with Markov Bridges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiheng Zhu",
      "Jialu Wu",
      "Qiuyi Li",
      "Jiahuan Yan",
      "Mingze Yin",
      "Wei Wu",
      "Mingyang Li",
      "Jieping Ye",
      "Zheng Wang",
      "Jian Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/467e109270678b3e2b3c2e7fa6b3d575-Abstract-Conference.html": {
    "title": "On Differentially Private Subspace Estimation in a Distribution-Free Setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eliad Tsfadia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4685275b9a6a2c55d78135563dfd50bb-Abstract-Conference.html": {
    "title": "Optimal Scalarizations for Sublinear Hypervolume Regret",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiuyi (Richard) Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/468c9ec4215e05b6488ac307d377662e-Abstract-Conference.html": {
    "title": "PromptFix: You Prompt and We Fix the Photo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "yongsheng yu",
      "Ziyun Zeng",
      "Hang Hua",
      "Jianlong Fu",
      "Jiebo Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/46907c2ff9fafd618095161d76461842-Abstract-Conference.html": {
    "title": "DenoiseRep: Denoising Model for Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "zhengrui Xu",
      "Guan&#x27;an Wang",
      "Xiaowen Huang",
      "Jitao Sang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/469eb28a0a67dba79f79cbb03f84cd90-Abstract-Conference.html": {
    "title": "Revealing Distribution Discrepancy by Sampling Transfer in Unlabeled Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhilin Zhao",
      "Longbing Cao",
      "Xuhui Fan",
      "Wei-Shi Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/46a126492ea6fb87410e55a58df2e189-Abstract-Conference.html": {
    "title": "RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Liu",
      "Mengzhen Liu",
      "Zhenyu Wang",
      "Pengju An",
      "Xiaoqi Li",
      "Kaichen Zhou",
      "Senqiao Yang",
      "Renrui Zhang",
      "Yandong Guo",
      "Shanghang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/46e3b98045760c8cd9a0728d9e9f158d-Abstract-Conference.html": {
    "title": "Interfacing Foundation Models' Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyan Zou",
      "Linjie Li",
      "Jianfeng Wang",
      "Jianwei Yang",
      "Mingyu Ding",
      "Junyi Wei",
      "Zhengyuan Yang",
      "Feng Li",
      "Hao Zhang",
      "Shilong Liu",
      "Arul Aravinthan",
      "Yong Jae Lee",
      "Lijuan Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/46ed503889ab232c21c1162340ee17b2-Abstract-Conference.html": {
    "title": "Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andy Zhou",
      "Bo Li",
      "Haohan Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/46fd43174b82660f24e3ba11cf5e1340-Abstract-Conference.html": {
    "title": "Large Spatial Model: End-to-end Unposed Images to Semantic 3D",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwen Fan",
      "Jian Zhang",
      "Wenyan Cong",
      "Peihao Wang",
      "Renjie Li",
      "Kairun Wen",
      "Shijie Zhou",
      "Achuta Kadambi",
      "Zhangyang &quot;Atlas&quot; Wang",
      "Danfei Xu",
      "Boris Ivanovic",
      "Marco Pavone",
      "Yue Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/470629a47e2d65ce0606c40055df5d26-Abstract-Conference.html": {
    "title": "Long-tailed Object Detection Pretraining: Dynamic Rebalancing Contrastive Learning with Dual Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Long Duan",
      "Yong Li",
      "Xiu-Shen Wei",
      "Lin Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/47387bb0aeb97785f608c11f2f4bb091-Abstract-Conference.html": {
    "title": "HydraViT: Stacking Heads for a Scalable ViT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Janek Haberer",
      "Ali Hojjat",
      "Olaf Landsiedel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/473a9a75edc46eff5ff224d53d5f7294-Abstract-Conference.html": {
    "title": "Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byung-Kwan Lee",
      "Chae Won Kim",
      "Beomchan Park",
      "Yong Man Ro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/473c578564ed9fe8041abfef772d37db-Abstract-Conference.html": {
    "title": "Learning predictable and robust neural representations by straightening image sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julie Xueyan Niu",
      "Cristina Savin",
      "Eero P. Simoncelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/47482e8241e820bfcb1c599b3009c90d-Abstract-Conference.html": {
    "title": "Quasi-Bayes meets Vines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Huk",
      "Yuanhe Zhang",
      "Ritabrata Dutta",
      "Mark Steel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/47795c4ae2f7d07ea2fb0d11fa2c3c90-Abstract-Conference.html": {
    "title": "Online Learning with Sublinear Best-Action Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Russo",
      "Andrea Celli",
      "Riccardo Colini Baldeschi",
      "Federico Fusco",
      "Daniel Haimovich",
      "Dima Karamshuk",
      "Stefano Leonardi",
      "Niek Tax"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/477cde5ca7ab9254d8fc83b97e834446-Abstract-Conference.html": {
    "title": "On the Target-kernel Alignment: a Unified Analysis with Kernel Complexity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Wang",
      "Xin HE",
      "Yuwen Wang",
      "Junhui Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/47811ee68103bfcde7ca2223fccefb3a-Abstract-Conference.html": {
    "title": "Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Nagler",
      "Lennart Schneider",
      "Bernd Bischl",
      "Matthias Feurer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/478b06f60662d3cdc1d4f15d4587173a-Abstract-Conference.html": {
    "title": "Generative Modeling of Molecular Dynamics Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Jing",
      "Hannes Stärk",
      "Tommi Jaakkola",
      "Bonnie Berger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/47908cab4e5b696d7af5c7de69f3b7d2-Abstract-Conference.html": {
    "title": "On the Sparsity of the Strong Lottery Ticket Hypothesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emanuele Natale",
      "Davide Ferre",
      "Giordano Giambartolomei",
      "Frederic Giroire",
      "Frederik Mallmann-Trenn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/47a287298e7887d1c25d4aabb918bd54-Abstract-Conference.html": {
    "title": "Progressive Exploration-Conformal Learning for Sparsely Annotated Object Detection in Aerial Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Lu",
      "Chenxu Wang",
      "Chunyan Xu",
      "Xiangwei Zheng",
      "Zhen Cui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/47a6e9e2c3019f13ad94a0f259fe4970-Abstract-Conference.html": {
    "title": "MotionTTT: 2D Test-Time-Training Motion Estimation for 3D Motion Corrected MRI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobit Klug",
      "Kun Wang",
      "Stefan Ruschke",
      "Reinhard Heckel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/47bb4eff6321ae7a11fb6e3352c63125-Abstract-Conference.html": {
    "title": "Wasserstein convergence of Cech persistence diagrams for samplings of submanifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles Arnal",
      "David Cohen-Steiner",
      "Vincent Divol"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/47c7edadfee365b394b2a3bd416048da-Abstract-Conference.html": {
    "title": "LLM Circuit Analyses Are Consistent Across Training and Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Curt Tigges",
      "Michael Hanna",
      "Qinan Yu",
      "Stella Biderman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/47dfa401aa7f51bd16783fc62c0684ee-Abstract-Conference.html": {
    "title": "Discovering plasticity rules that organize and maintain neural circuits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Bell",
      "Alison Duffy",
      "Adrienne L. Fairhall"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/47e288629a6996a17ce50b90a056a0e1-Abstract-Conference.html": {
    "title": "On Weak Regret Analysis for Dueling Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "El Mehdi Saad",
      "Alexandra Carpentier",
      "Tomáš Kocák",
      "Nicolas Verzelen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/47eb2874a790d5b1f554b9bb93b3de9d-Abstract-Conference.html": {
    "title": "HGDL: Heterogeneous Graph Label Distribution Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Jin",
      "Heng Lian",
      "Yi He",
      "Xingquan Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/47ee3941a6f1d23c39b788e0f450e2a7-Abstract-Conference.html": {
    "title": "Simple and Fast Distillation of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Zhou",
      "Defang Chen",
      "Can Wang",
      "Chun Chen",
      "Siwei Lyu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/47f30d67bce3e9824928267e9355420f-Abstract-Conference.html": {
    "title": "Semi-supervised Knowledge Transfer Across Multi-omic Single-cell Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Zhang",
      "Tianyu Liu",
      "Zihao Chen",
      "Xiaojiang Peng",
      "Chong Chen",
      "Xian-Sheng Hua",
      "Xiao Luo",
      "Hongyu Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/480150047ecb2187a3a8b8dccfd8f2de-Abstract-Conference.html": {
    "title": "A Siamese Transformer with Hierarchical Refinement for Lane Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zinan Lv",
      "Dong Han",
      "Wenzhe Wang",
      "Danny Z Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/48088756ec0ce6ba362bddc7ebeb3915-Abstract-Conference.html": {
    "title": "Yo'LLaVA: Your Personalized Language and Vision Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thao Nguyen",
      "Haotian Liu",
      "Yuheng Li",
      "Mu Cai",
      "Utkarsh Ojha",
      "Yong Jae Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4809dd4b628b6253d0aad0154014f7a3-Abstract-Conference.html": {
    "title": "SEL-BALD: Deep Bayesian Active Learning with Selective Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijiang Gao",
      "Mingzhang Yin",
      "Maytal Saar-Tsechansky"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/480e563034dbb6d1dd622d8eab7d129b-Abstract-Conference.html": {
    "title": "Improved Sample Complexity Bounds for Diffusion Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivam Gupta",
      "Aditya Parulekar",
      "ecprice",
      "Zhiyang Xun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/480eb35745feb11c9120b666f640893e-Abstract-Conference.html": {
    "title": "Inexact Augmented Lagrangian Methods for Conic Optimization: Quadratic Growth and Linear Convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng-Yi Liao",
      "Lijun Ding",
      "Yang Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/481c70828a4ff20d31a646cc6cc95f3d-Abstract-Conference.html": {
    "title": "VidMan: Exploiting Implicit Dynamics from Video Diffusion Model for Effective Robot Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youpeng Wen",
      "Junfan Lin",
      "Yi Zhu",
      "Jianhua Han",
      "Hang Xu",
      "Shen Zhao",
      "Xiaodan Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4822991365c962105b1b95b1107d30e5-Abstract-Conference.html": {
    "title": "Compact Language Models via Pruning and Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saurav Muralidharan",
      "Sharath Turuvekere Sreenivas",
      "Raviraj Joshi",
      "Marcin Chochowski",
      "Mostofa Patwary",
      "Mohammad Shoeybi",
      "Bryan Catanzaro",
      "Jan Kautz",
      "Pavlo Molchanov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4822d9adc9cec7a39e254d007aa78276-Abstract-Conference.html": {
    "title": "Graphcode: Learning from multiparameter persistent homology using graph neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Russold",
      "Michael Kerber"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4830a9b95a2f63fc4b3fe09abc18f045-Abstract-Conference.html": {
    "title": "Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josh McClellan",
      "Naveed Haghani",
      "John Winder",
      "Furong Huang",
      "Pratap Tokekar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/484201c6ad50072c99baa1b26228f714-Abstract-Conference.html": {
    "title": "TurboHopp: Accelerated Molecule Scaffold Hopping with Consistency Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiwoong Yoo",
      "Owen Oertell",
      "Junhyun Lee",
      "Sanghoon Lee",
      "Jaewoo Kang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/485e0981e81766248b61fd1ec43c118f-Abstract-Conference.html": {
    "title": "The Expressive Capacity of State Space Models: A Formal Language Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Sarrof",
      "Yana Veitsman",
      "Michael Hahn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/48644509339cb3076f7b0407c7588af6-Abstract-Conference.html": {
    "title": "Is One GPU Enough? Pushing Image Generation at Higher-Resolutions with Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Athanasios Tragakis",
      "Marco Aversa",
      "Chaitanya Kaul",
      "Roderick Murray-Smith",
      "Daniele Faccio"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/486ff0b164cf92b0255fe39863bcf99e-Abstract-Conference.html": {
    "title": "Optimal Algorithms for Online Convex Optimization with Adversarial Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhishek Sinha",
      "Rahul Vaze"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/48726631f87322012c6be38e00c72a47-Abstract-Conference.html": {
    "title": "Unveiling the Bias Impact on Symmetric Moral Consistency of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Zhou",
      "Xinwei Guo",
      "Jiashi Gao",
      "Xiangyu Zhao",
      "Shiyao Zhang",
      "Xin Yao",
      "Xuetao Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/487667c56596138d36bbaa3bd8aac6df-Abstract-Conference.html": {
    "title": "Accelerated Regularized Learning in Finite N-Person Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyriakos Lotidis",
      "Angeliki Giannou",
      "Panayotis Mertikopoulos",
      "Nicholas Bambos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/487c9d6ef55e73aa9dfd4b48fe3713a6-Abstract-Conference.html": {
    "title": "Embedding Dimension of Contrastive Learning and $k$-Nearest Neighbors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitrii Avdiukhin",
      "Vaggos Chatziafratis",
      "Orr Fischer",
      "Grigory Yaroslavtsev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/48bb60a0c0aebb4142bf314bd1a5c6a0-Abstract-Conference.html": {
    "title": "Verified Code Transpilation with LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahil Bhatia",
      "Jie Qiu",
      "Niranjan Hasabnis",
      "Sanjit Seshia",
      "Alvin Cheung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/48c368f105e8145b945227b73255635a-Abstract-Conference.html": {
    "title": "SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vijay Chandra Lingam",
      "Atula Neerkaje",
      "Aditya Vavre",
      "Aneesh Shetty",
      "Gautham Krishna Gudur",
      "Joydeep Ghosh",
      "Eunsol Choi",
      "Alex Dimakis",
      "Aleksandar Bojchevski",
      "Sujay Sanghavi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/48dfc849640344e2d58df0b5bb78c33b-Abstract-Conference.html": {
    "title": "Interpretable Image Classification with Adaptive Prototype-based Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiyu Ma",
      "Jon Donnelly",
      "Wenjun Liu",
      "Soroush Vosoughi",
      "Cynthia Rudin",
      "Chaofan Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/48fecef47b19fe501d27d338b6d52582-Abstract-Conference.html": {
    "title": "I2EBench: A Comprehensive Benchmark for Instruction-based Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Ma",
      "Jiayi Ji",
      "Ke Ye",
      "Weihuang Lin",
      "Zhibin Wang",
      "Yonghan Zheng",
      "Qiang Zhou",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/48ffa38c13078d6ce26b328e7f373243-Abstract-Conference.html": {
    "title": "F-OAL: Forward-only Online Analytic Learning with Fast Training and Low Memory Footprint in Class Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HUIPING ZHUANG",
      "Yuchen Liu",
      "Run He",
      "Kai Tong",
      "Ziqian Zeng",
      "Cen Chen",
      "Yi Wang",
      "Lap-Pui Chau"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4902603fe8cb095b9ada707a19bd151c-Abstract-Conference.html": {
    "title": "Debiasing Synthetic Data Generated by Deep Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Decruyenaere",
      "Heidelinde Dehaene",
      "Paloma Rabaey",
      "Johan Decruyenaere",
      "Christiaan Polet",
      "Thomas Demeester",
      "Stijn Vansteelandt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/492592890311679d7f71559148358973-Abstract-Conference.html": {
    "title": "Probabilistic Weather Forecasting with Hierarchical Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joel Oskarsson",
      "Tomas Landelius",
      "Marc Deisenroth",
      "Fredrik Lindsten"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/494f876fad056843f310ad647274dd99-Abstract-Conference.html": {
    "title": "Learning Plaintext-Ciphertext Cryptographic Problems via ANF-based SAT Instance Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhao Zheng",
      "Yang Li",
      "Cunxin Fan",
      "Huaijin Wu",
      "Xinhao Song",
      "Junchi Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4964cd3f89ee5b57f07f4bc1824d2beb-Abstract-Conference.html": {
    "title": "A Universal Growth Rate for Learning with Smooth Surrogate Losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Mao",
      "Mehryar Mohri",
      "Yutao Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/496720b3c860111b95ac8634349dcc88-Abstract-Conference.html": {
    "title": "Exploiting LLM Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazuki Egashira",
      "Mark Vero",
      "Robin Staab",
      "Jingxuan He",
      "Martin Vechev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/499b12df1531fe8ee0febcf08381f3a4-Abstract-Conference.html": {
    "title": "Learning Social Welfare Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kanad Pardeshi",
      "Itai Shapira",
      "Ariel D Procaccia",
      "Aarti Singh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/49abf767d606b72f74ea6009176fafeb-Abstract-Conference.html": {
    "title": "MaNo: Exploiting Matrix Norm for Unsupervised Accuracy Estimation Under Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "RENCHUNZI XIE",
      "Ambroise Odonnat",
      "Vasilii Feofanov",
      "Weijian Deng",
      "Jianfeng Zhang",
      "Bo An"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/49be51578b507f37cd8b5fad379af183-Abstract-Conference.html": {
    "title": "Learning 3D Garment Animation from Trajectories of A Piece of Cloth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YIDI SHAO",
      "Chen Change Loy",
      "Bo Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/49c466ccc038f39b08b1980a2b06673c-Abstract-Conference.html": {
    "title": "DOPPLER: Differentially Private Optimizers with Low-pass Filter for Privacy Noise Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinwei Zhang",
      "Zhiqi Bu",
      "Mingyi Hong",
      "Meisam Razaviyayn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/49f07f600ca532a89ef4fa0618bb78c1-Abstract-Conference.html": {
    "title": "The Challenges of the Nonlinear Regime for Physics-Informed Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Bonfanti",
      "Giuseppe Bruno",
      "Cristina Cipriani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/49f80e4d2471ad4f2edf4f5f1ab62339-Abstract-Conference.html": {
    "title": "Transferability Bound Theory: Exploring Relationship between Adversarial Transferability and Flatness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyuan Fan",
      "Xiaodan Li",
      "Cen Chen",
      "Wenmeng Zhou",
      "Yaliang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/49fb58cfd482a33619d48a5c5910cf3c-Abstract-Conference.html": {
    "title": "Physics-Regularized Multi-Modal Image Assimilation for Brain Tumor Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Balcerak",
      "Tamaz Amiranashvili",
      "Andreas Wagner",
      "Jonas Weidner",
      "Petr Karnakov",
      "Johannes C. Paetzold",
      "Ivan Ezhov",
      "Petros Koumoutsakos",
      "Benedikt Wiestler",
      "bjoern menze"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4a17cd29ced0443bcff689fbb0d32d5e-Abstract-Conference.html": {
    "title": "Mechanism design augmented with output advice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "George Christodoulou",
      "Alkmini Sgouritsa",
      "Ioannis Vlachos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4a32a646254d2e37fc74a38d65796552-Abstract-Conference.html": {
    "title": "Persistent Homology for High-dimensional Data Based on Spectral Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Damrich",
      "Philipp Berens",
      "Dmitry Kobak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4a36c3c51af11ed9f34615b81edb5bbc-Abstract-Conference.html": {
    "title": "Sparsity-Agnostic Linear Bandits with Adaptive Adversaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyuan Jin",
      "Kyoungseok Jang",
      "Nicolò Cesa-Bianchi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4a3a14b9536806a0522930007c5512f7-Abstract-Conference.html": {
    "title": "MoME: Mixture of Multimodal Experts for Generalist Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leyang Shen",
      "Gongwei Chen",
      "Rui Shao",
      "Weili Guan",
      "Liqiang Nie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4a3a96231b8240f11483afd196227278-Abstract-Conference.html": {
    "title": "OW-VISCapTor: Abstractors for Open-World Video Instance Segmentation and Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anwesa Choudhuri",
      "Girish Chowdhary",
      "Alex Schwing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4a3ffc1a0074901718ab9dcef41909e9-Abstract-Conference.html": {
    "title": "Unveiling and Mitigating Backdoor Vulnerabilities based on Unlearning Weight Changes and Backdoor Activeness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weilin Lin",
      "Li Liu",
      "Shaokui Wei",
      "Jianze Li",
      "Hui Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4a6824f8f137e78f18e73d9cfc1d22ed-Abstract-Conference.html": {
    "title": "Lookback Prophet Inequalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyad Benomar",
      "Dorian Baudry",
      "Vianney Perchet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4a68bb0e075a3215d4ff37382ea598dc-Abstract-Conference.html": {
    "title": "Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Havrilla",
      "Wenjing Liao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4a86ec12e94ef1fe306362e7bdcd5894-Abstract-Conference.html": {
    "title": "Self-Healing Machine Learning: A Framework for Autonomous Adaptation in Real-World Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paulius Rauba",
      "Nabeel Seedat",
      "Krzysztof Kacprzyk",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4a8bc86ca475c229dc1fd0f4d5cf8f63-Abstract-Conference.html": {
    "title": "Learning Transferable Features for Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kushal Kardam Vyas",
      "Imtiaz Humayun",
      "Aniket Dashpute",
      "Richard Baraniuk",
      "Ashok Veeraraghavan",
      "Guha Balakrishnan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4a9eaf6dff3fdac9ab1aaf4c0fe2d563-Abstract-Conference.html": {
    "title": "VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roy Miles",
      "Pradyumna Reddy",
      "Ismail Elezi",
      "Jiankang Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4aab410455f9aab47c4e312dd855ee3a-Abstract-Conference.html": {
    "title": "Who's Gaming the System? A Causally-Motivated Approach for Detecting Strategic Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trenton Chang",
      "Lindsay Warrenburg",
      "Sae-Hwan Park",
      "Ravi Parikh",
      "Maggie Makar",
      "Jenna Wiens"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ac4365b98bc242acd5ab974a05c68a8-Abstract-Conference.html": {
    "title": "Interactive Deep Clustering via Value Mining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honglin Liu",
      "Peng Hu",
      "Changqing Zhang",
      "Yunfan Li",
      "Xi Peng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4acb23d5d9b4bea566799afac0ee3125-Abstract-Conference.html": {
    "title": "General Detection-based Text Line Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raphael Baena",
      "Syrine Kalleli",
      "Mathieu Aubry"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ae163cb8788970e53b4fd9578141139-Abstract-Conference.html": {
    "title": "Identifying General Mechanism Shifts in Linear Causal Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Chen",
      "Kevin Bello",
      "Francesco Locatello",
      "Bryon Aragam",
      "Pradeep K. Ravikumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ae5cdacb9152d95661bc3c620b75aba-Abstract-Conference.html": {
    "title": "Wasserstein Distributionally Robust Optimization through the Lens of Structural Causal Models and Individual Fairness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad-Reza Ehyaei",
      "Golnoosh Farnadi",
      "Samira Samadi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ae74049d83240399df9772017bfa046-Abstract-Conference.html": {
    "title": "Accurate and Steady Inertial Pose Estimation through Sequence Structure Learning and Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinghao Wu",
      "chaoran wang",
      "Lu Yin",
      "Shihui Guo",
      "Yipeng Qin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ae7d78ebbe48f772e31c5c3fcc04c43-Abstract-Conference.html": {
    "title": "Approximated Orthogonal Projection Unit: Stabilizing Regression Network Training Using Natural Gradient",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoqi Wang",
      "Chunjie Yang",
      "Siwei Lou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b06cdddb1cde6624c0be1465c7b800f-Abstract-Conference.html": {
    "title": "InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyi Dong",
      "Pan Zhang",
      "Yuhang Zang",
      "Yuhang Cao",
      "Bin Wang",
      "Linke Ouyang",
      "Songyang Zhang",
      "Haodong Duan",
      "Wenwei Zhang",
      "Yining Li",
      "Hang Yan",
      "Yang Gao",
      "Zhe Chen",
      "xinyue zhang",
      "Wei Li",
      "Li Jingwen",
      "Wenhai Wang",
      "Kai Chen",
      "Conghui He",
      "Xingcheng ZHANG",
      "Jifeng Dai",
      "Yu Qiao",
      "Dahua Lin",
      "Jiaqi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b1d9a1fbf7b2a93bea08e18792fe436-Abstract-Conference.html": {
    "title": "IllumiNeRF: 3D Relighting Without Inverse Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoming Zhao",
      "Pratul Srinivasan",
      "Dor Verbin",
      "Keunhong Park",
      "Ricardo Martin Brualla",
      "Philipp Henzler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b25c000967af9036fb9b207b198a626-Abstract-Conference.html": {
    "title": "Genetic-guided GFlowNets for Sample Efficient Molecular Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonah Kim",
      "Minsu Kim",
      "Sanghyeok Choi",
      "Jinkyoo Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b296975eaa172a94e03538094da6a66-Abstract-Conference.html": {
    "title": "Slicing Vision Transformer for Flexible Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yitian Zhang",
      "n n",
      "Xu Ma",
      "Huan Wang",
      "Ke Ma",
      "Stephen Chen",
      "Derek Hu",
      "Yun Fu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b2a917e30e1bb1aff055b4d8c6c081c-Abstract-Conference.html": {
    "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muzhi Zhu",
      "Yang Liu",
      "Zekai Luo",
      "Chenchen Jing",
      "Hao Chen",
      "Guangkai Xu",
      "Xinlong Wang",
      "Chunhua Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b32c2943a02331792877cc6b5205f49-Abstract-Conference.html": {
    "title": "This Too Shall Pass: Removing Stale Observations in Dynamic Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anthony Bardou",
      "Patrick Thiran",
      "Giovanni Ranieri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b34143d8a1e651f00002e31b3b72e95-Abstract-Conference.html": {
    "title": "Learning symmetries via weight-sharing with doubly stochastic tensors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Putri van der Linden",
      "Alejandro García-Castellanos",
      "Sharvaree Vadgama",
      "Thijs Kuipers",
      "Erik Bekkers"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b4d25dc0c52d3cf43d5b203cdfdf241-Abstract-Conference.html": {
    "title": "Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "qinpeng cui",
      "yixuan liu",
      "Xinyi Zhang",
      "Qiqi Bao",
      "Qingmin Liao",
      "liwang Amd",
      "Lu Tian",
      "Zicheng Liu",
      "Zhongdao Wang",
      "Emad Barsoum"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b5d47949866d06ab5c03022b4a5a551-Abstract-Conference.html": {
    "title": "Improved Sample Complexity for Multiclass PAC Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steve Hanneke",
      "Shay Moran",
      "Qian Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b6908c6695009dd84846e273d6c2607-Abstract-Conference.html": {
    "title": "Online Estimation via Offline Estimation: An Information-Theoretic Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dylan J Foster",
      "Yanjun Han",
      "Jian Qian",
      "Alexander Rakhlin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b70484ebef62484e0c8cdd269e482fd-Abstract-Conference.html": {
    "title": "Revisiting the Integration of Convolution and Attention for Vision Backbone",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Zhu",
      "Xinjiang Wang",
      "Wayne Zhang",
      "Rynson Lau"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b734e95f0788a030a69caa987516186-Abstract-Conference.html": {
    "title": "Embedding Trajectory for Out-of-Distribution Detection in Mathematical Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Wang",
      "Pei Zhang",
      "Baosong Yang",
      "Derek Wong",
      "Zhuosheng Zhang",
      "Rui Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b77d5b896c321a29277524a98a50215-Abstract-Conference.html": {
    "title": "RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amrith Setlur",
      "Saurabh Garg",
      "Xinyang Geng",
      "Naman Garg",
      "Virginia Smith",
      "Aviral Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b8001fc75f0532827472ea5a16af9ca-Abstract-Conference.html": {
    "title": "Learning Formal Mathematics From Intrinsic Motivation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Poesia",
      "David Broman",
      "Nick Haber",
      "Noah Goodman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b814aa0d7c7955a0581e819981e2f81-Abstract-Conference.html": {
    "title": "Hollowed Net for On-Device Personalization of Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonguk Cho",
      "Seokeon Choi",
      "Debasmit Das",
      "Matthias Reisser",
      "Taesup Kim",
      "Sungrack Yun",
      "Fatih Porikli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b8eaf3bcdc105423a972ed90eb07217-Abstract-Conference.html": {
    "title": "LACIE: Listener-Aware Finetuning for Calibration in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elias Stengel-Eskin",
      "Peter Hase",
      "Mohit Bansal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b96695d9885f038110b8b16ef50e882-Abstract-Conference.html": {
    "title": "Temporal Sentence Grounding with Relevance Feedback in Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianfeng Dong",
      "Xiaoman Peng",
      "Daizong Liu",
      "Xiaoye Qu",
      "Xun Yang",
      "Cuizhu Bao",
      "Meng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4b9e2c66b07c89116d2de0a2b7d03db3-Abstract-Conference.html": {
    "title": "Revisiting Self-Supervised Heterogeneous Graph Learning from Spectral Clustering Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YUJIE MO",
      "Zhihe Lu",
      "Runpeng Yu",
      "Xiaofeng Zhu",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4bbeef01d9753fd5a29e9fd02d275698-Abstract-Conference.html": {
    "title": "Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tang Li",
      "Mengmeng Ma",
      "Xi Peng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4bfcebedf7a2967c410b64670f27f904-Abstract-Conference.html": {
    "title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Deng",
      "Kunlei Jing",
      "Shengmei Chen",
      "Cheng Liu",
      "Jiawei Ru",
      "Bo Jiang",
      "Lin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4c11f5dc728d29460f2eb7c096536726-Abstract-Conference.html": {
    "title": "Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziang Chen",
      "Rong Ge"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4c2092ec0b1370cce3fb5965ab255fae-Abstract-Conference.html": {
    "title": "Exploiting Activation Sparsity with Dense to Dynamic-k Mixture-of-Experts Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filip Szatkowski",
      "Bartosz Wójcik",
      "Mikołaj Piórczyński",
      "Simone Scardapane"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4c454d34f3a4c8d6b4ca85a918e5d7ba-Abstract-Conference.html": {
    "title": "TinyTTA: Efficient Test-time Adaptation via Early-exit Ensembles on Edge Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong Jia",
      "Young Kwon",
      "Alessio Orsino",
      "Ting Dang",
      "DOMENICO TALIA",
      "Cecilia Mascolo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4c5ce1fc8895076f49935951a630be5c-Abstract-Conference.html": {
    "title": "DASH: Warm-Starting Neural Network Training in Stationary Settings without Loss of Plasticity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baekrok Shin",
      "Junsoo Oh",
      "Hanseul Cho",
      "Chulhee Yun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4c5e2bcbf21bdf40d75fddad0bd43dc9-Abstract-Conference.html": {
    "title": "Initializing Variable-sized Vision Transformers from Learngene with Learnable Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyu Xia",
      "Yuankun Zu",
      "Xu Yang",
      "Xin Geng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4c79c359b3c5f077c0b955f93cb0f53e-Abstract-Conference.html": {
    "title": "Sketchy Moment Matching: Toward Fast and Provable Data Selection for Finetuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijun Dong",
      "Viet Hoang Phan",
      "Xiang Pan",
      "Qi Lei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4c9477b9e2c7ec0ad3f4f15077aaf85a-Abstract-Conference.html": {
    "title": "QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharath Girish",
      "Tianye Li",
      "Amrita Mazumdar",
      "Abhinav Shrivastava",
      "david luebke",
      "Shalini De Mello"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ca9b0906de992ae32c0001915fbea22-Abstract-Conference.html": {
    "title": "A General Protocol to Probe Large Vision Models for 3D Physical Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanqi Zhan",
      "Chuanxia Zheng",
      "Weidi Xie",
      "Andrew Zisserman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4cc4cc789849230a4f495a2060b45c87-Abstract-Conference.html": {
    "title": "Are nuclear masks all you need for improved out-of-domain generalisation? A closer look at cancer classification in histopathology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhananjay Tomar",
      "Alexander Binder",
      "Andreas Kleppe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ce18228ececb78bca04cbce069891b1-Abstract-Conference.html": {
    "title": "WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Xia",
      "Zhihao Yue",
      "Yingbo Zhou",
      "Zhiwei Ling",
      "Yiyu Shi",
      "Xian Wei",
      "Mingsong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ced59d480e07d290b6f29fc8798f195-Abstract-Conference.html": {
    "title": "ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Ye",
      "Jiarui Wang",
      "Zhiguang Cao",
      "Federico Berto",
      "Chuanbo Hua",
      "HAEYEON KIM",
      "Jinkyoo Park",
      "Guojie Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4cf53c9fa86318d37bbeadac59c1a34d-Abstract-Conference.html": {
    "title": "Optimal-state Dynamics Estimation for Physics-based Human Motion Capture from Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cuong Le",
      "John Viktor Johansson",
      "Manon Kok",
      "Bastian Wandt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4d10e8c4a691ec3c7732cd6d7ffa13a9-Abstract-Conference.html": {
    "title": "A Simple Remedy for Dataset Bias via Self-Influence: A Mislabeled Sample Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeonsung Jung",
      "Jaeyun Song",
      "June Yong Yang",
      "Jin-Hwa Kim",
      "Sung-Yub Kim",
      "Eunho Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4d254c96c5ff500dda2ac0a58987aeba-Abstract-Conference.html": {
    "title": "DiffPO: A causal diffusion model for learning distributions of potential outcomes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Ma",
      "Valentyn Melnychuk",
      "Jonas Schweisthal",
      "Stefan Feuerriegel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4d2aa4c034745f558bfea34643c8d6a6-Abstract-Conference.html": {
    "title": "(FL)$^2$: Overcoming Few Labels in Federated Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungjoo Lee",
      "Thanh-Long V. Le",
      "Jaemin Shin",
      "Sung-Ju Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4d4f7cf206bb00f9a38a5b6ae92cf79a-Abstract-Conference.html": {
    "title": "Policy Improvement using Language Feedback Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Zhong",
      "Dipendra Misra",
      "Xingdi Yuan",
      "Marc-Alexandre Côté"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4d5ce4a7ebf588834db127965cdb5ccb-Abstract-Conference.html": {
    "title": "RAMP: Boosting Adversarial Robustness Against Multiple $l_p$ Perturbations for Universal Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enyi Jiang",
      "Gagandeep Singh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4d65fc9de1051c382fd258dbafd8cde9-Abstract-Conference.html": {
    "title": "Self-Supervised Adversarial Training via Diverse Augmented Queries and Self-Supervised Double Perturbation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruize Zhang",
      "Sheng Tang",
      "Juan Cao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4d9032fdc73c822b703c8a897a3e3753-Abstract-Conference.html": {
    "title": "Causal Effect Identification in a Sub-Population with Latent Variables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Mohammad Abouei",
      "Ehsan Mokhtarian",
      "Negar Kiyavash",
      "Matthias Grossglauser"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4d90363e96781894b1327b87d1ade17e-Abstract-Conference.html": {
    "title": "Generating compositional scenes via Text-to-image RGBA Instance Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro Fontanella",
      "Petru-Daniel Tudosiu",
      "Yongxin Yang",
      "Shifeng Zhang",
      "Sarah Parisot"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4da1f5e3e2bf7f3d8f3fa116877dc6d0-Abstract-Conference.html": {
    "title": "Tight Rates for Bandit Control Beyond Quadratics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Y. Jennifer Sun",
      "Zhou Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4da8ae12143b0f8f7505498f39ecb1f8-Abstract-Conference.html": {
    "title": "DEX: Data Channel Extension for Efficient CNN Inference on Tiny AI Accelerators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taesik Gong",
      "Fahim Kawsar",
      "Chulhong Min"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4dc37a7bc61057252ce043fa3b83aac2-Abstract-Conference.html": {
    "title": "Out-of-Distribution Detection with a Single Unconditional Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alvin Heng",
      "alexandre thiery",
      "Harold Soh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4dc57702c987e1e72f0dd2921edb5ded-Abstract-Conference.html": {
    "title": "When are dynamical systems learned from time series data statistically accurate?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongjin Park",
      "Nicole Yang",
      "Nisha Chandramoorthy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4dd0a016d7d253d02473e4778414ab0b-Abstract-Conference.html": {
    "title": "Generative Adversarial Model-Based Optimization via Source Critic Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Yao",
      "Yimeng Zeng",
      "Hamsa Bastani",
      "Jacob Gardner",
      "James Gee",
      "Osbert Bastani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ddfe69f164eae70abc86f0f9cbed7e8-Abstract-Conference.html": {
    "title": "Aligning Target-Aware Molecule Diffusion Models with Exact Energy Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyi Gu",
      "Minkai Xu",
      "Alexander Powers",
      "Weili Nie",
      "Tomas Geffner",
      "Karsten Kreis",
      "Jure Leskovec",
      "Arash Vahdat",
      "Stefano Ermon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4df1cc5a7528b7197ad8ae76ff30107a-Abstract-Conference.html": {
    "title": "OTTER: Effortless Label Distribution Adaptation of Zero-shot Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changho Shin",
      "Jitian Zhao",
      "Sonia Cromp",
      "Harit Vishwakarma",
      "Frederic Sala"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4df3510ad02a86d69dc32388d91606f8-Abstract-Conference.html": {
    "title": "A Closer Look at AUROC and AUPRC under Class Imbalance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew McDermott",
      "Haoran Zhang",
      "Lasse Hansen",
      "Giovanni Angelotti",
      "Jack Gallifant"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4e2acb1e1c8e297d394ae29ed9535172-Abstract-Conference.html": {
    "title": "The GAN is dead; long live the GAN! A Modern GAN Baseline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nick Huang",
      "Aaron Gokaslan",
      "Volodymyr Kuleshov",
      "James Tompkin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4e2c6423d724370b36c3a7612f25b78c-Abstract-Conference.html": {
    "title": "Amortized Active Causal Induction with Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yashas Annadani",
      "Panagiotis Tigas",
      "Stefan Bauer",
      "Adam Foster"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4e392aa9bc70ed731d3c9c32810f92fb-Abstract-Conference.html": {
    "title": "Long-range Meta-path Search on Large-scale Heterogeneous Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Li",
      "Zijie Guo",
      "qiuting he",
      "Kun He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4e5a47c427ec16ec863e547bc1aeb70c-Abstract-Conference.html": {
    "title": "A Single-Step, Sharpness-Aware Minimization is All You Need to Achieve Efficient and Accurate Sparse Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Ji",
      "Gen Li",
      "Jingjing Fu",
      "Fatemeh Afghah",
      "Linke Guo",
      "Xiaoyong Yuan",
      "Xiaolong Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4e5cd83b9a7dc3fc36ce1b76b145f8ca-Abstract-Conference.html": {
    "title": "Generalized Eigenvalue Problems with Generative Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoqiang Liu",
      "Wen Li",
      "Junren Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4e8f257e054abd24c550d55e57cec274-Abstract-Conference.html": {
    "title": "MALT Powers Up Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Odelia Melamed",
      "Gilad Yehudai",
      "Adi Shamir"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ea4a1ea4d9ff273688c8e92bd087112-Abstract-Conference.html": {
    "title": "Multi-Object Hallucination in Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuweiyi Chen",
      "Ziqiao Ma",
      "Xuejun Zhang",
      "Sihan Xu",
      "Shengyi Qian",
      "Jianing Yang",
      "David Fouhey",
      "Joyce Chai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ea6932c9845d6b2cfb89c72b41df3c5-Abstract-Conference.html": {
    "title": "Diffusing Differentiable Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Savani",
      "Marc Finzi",
      "J. Zico Kolter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4eb2c0adafbe71269f3a772c130f9e53-Abstract-Conference.html": {
    "title": "Implicit Regularization of Sharpness-Aware Minimization for Scale-Invariant Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingcong Li",
      "Liang Zhang",
      "Niao He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4eb32e1569085c8f8883163665bf3c0a-Abstract-Conference.html": {
    "title": "Reimagining Mutual Information for Enhanced Defense against Data Leakage in Collaborative Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Duan",
      "Jingwei Sun",
      "Jinyuan Jia",
      "Yiran Chen",
      "Maria Gorlatova"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4eb5cc598a6271528ed9b84bb9879e9b-Abstract-Conference.html": {
    "title": "Simplifying Constraint Inference with Inverse Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adriana Hugessen",
      "Harley Wiltzer",
      "Glen Berseth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4eb91efe090f72f7cf42c69aab03fe85-Abstract-Conference.html": {
    "title": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoming Cai",
      "Jingxi Chen",
      "Brandon Feng",
      "Weiyun Jiang",
      "Mingyang Xie",
      "Kevin Zhang",
      "Cornelia Fermuller",
      "Yiannis Aloimonos",
      "Ashok Veeraraghavan",
      "Chris Metzler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ebba705ffdee81e0a638c99fe066ce2-Abstract-Conference.html": {
    "title": "Parameter-free Clipped Gradient Descent Meets Polyak",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Takezawa",
      "Han Bao",
      "Ryoma Sato",
      "Kenta Niwa",
      "Makoto Yamada"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ebf0617b32da2cd083c3b17c7285cce-Abstract-Conference.html": {
    "title": "Acoustic Volume Rendering for Neural Impulse Response Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zitong Lan",
      "Chenhao Zheng",
      "Zhiwei Zheng",
      "Mingmin Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ec160e5bf92ce97ad307500e0db9151-Abstract-Conference.html": {
    "title": "DOFEN: Deep Oblivious Forest ENsemble",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "KuanYu Chen",
      "Ping-Han Chiang",
      "Hsin-Rung Chou",
      "Chih-Sheng Chen",
      "Tien-Hao Chang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ec3ddc465c6d650c9c419fb91f1c00a-Abstract-Conference.html": {
    "title": "Symbolic Regression with a Learned Concept Library",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arya Grayeli",
      "Atharva Sehgal",
      "Omar Costilla Reyes",
      "Miles Cranmer",
      "Swarat Chaudhuri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ee22ead36c9625396e353a6a9fca822-Abstract-Conference.html": {
    "title": "Learning to Embed Distributions via Maximum Kernel Entropy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oleksii Kachaiev",
      "Stefano Recanatesi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4ee3ac2cd119023c79b0d21c4a464dc7-Abstract-Conference.html": {
    "title": "Aligning Individual and Collective Objectives in Multi-Agent Cooperation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Li",
      "Wenhao Zhang",
      "Jianhong Wang",
      "Shao Zhang",
      "Yali Du",
      "Ying Wen",
      "Wei Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4eff61b79274124bc71efe2ee9772f95-Abstract-Conference.html": {
    "title": "Is Programming by Example Solved by LLMs?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen-Ding Li",
      "Kevin Ellis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4f1150c8c5f49af270555ad0c7db76d0-Abstract-Conference.html": {
    "title": "Referring Human Pose and Mask Estimation In the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Miao",
      "Mingtao Feng",
      "Zijie Wu",
      "Mohammed Bennamoun",
      "Yongsheng Gao",
      "Ajmal Mian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4f1fbd5ab8d58d0ecf33c95fd46b900e-Abstract-Conference.html": {
    "title": "Be Confident in What You Know: Bayesian Parameter Efficient Fine-Tuning of Vision Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deep Pandey",
      "Spandan Pyakurel",
      "Qi Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4f3820576130a8f796ddbf204c841487-Abstract-Conference.html": {
    "title": "Meta-DT: Offline Meta-RL as Conditional Sequence Modeling with World Model Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Wang",
      "Li Zhang",
      "Wenhao Wu",
      "Yuanheng Zhu",
      "Dongbin Zhao",
      "Chunlin Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4f3cb9576dc99d62b80726690453716f-Abstract-Conference.html": {
    "title": "Poisson Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadi Vafaii",
      "Dekel Galor",
      "Jacob Yates"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4f550cb7b30b59553e50cd08a9dbf068-Abstract-Conference.html": {
    "title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruyi Zha",
      "Tao Jun Lin",
      "Yuanhao Cai",
      "Jiwen Cao",
      "Yanhao Zhang",
      "Hongdong Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4f84e81c0a4eed2024cebcfb8f9d6e7f-Abstract-Conference.html": {
    "title": "Data-faithful Feature Attribution: Mitigating Unobservable Confounders via Instrumental Variables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiheng Sun",
      "Haocheng Xia",
      "Jinfei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4f92d2f498b88f1bd43732312272967a-Abstract-Conference.html": {
    "title": "FreqBlender: Enhancing DeepFake Detection by Blending Frequency Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "hanzhe li",
      "Jiaran Zhou",
      "Yuezun Li",
      "Baoyuan Wu",
      "Bin Li",
      "Junyu Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4fa8c1f39d3872d3a38d910e2c37f4c0-Abstract-Conference.html": {
    "title": "Reliable Learning of Halfspaces under Gaussian Marginals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilias Diakonikolas",
      "Lisheng Ren",
      "Nikos Zarifis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4fac0e32088db2fd2948cfaacc4fe108-Abstract-Conference.html": {
    "title": "EM Distillation for One-step Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sirui Xie",
      "Zhisheng Xiao",
      "Diederik Kingma",
      "Tingbo Hou",
      "Ying Nian Wu",
      "Kevin P. Murphy",
      "Tim Salimans",
      "Ben Poole",
      "Ruiqi Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4fc03d122a7e08d21aa92573113790a3-Abstract-Conference.html": {
    "title": "A Motion-aware Spatio-temporal Graph for Video Salient Object Ranking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Chen",
      "Zhu Yufei",
      "Yongjian Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4fc81f4cd2715d995018e0799262176b-Abstract-Conference.html": {
    "title": "An Accelerated Gradient Method for Convex Smooth Simple Bilevel Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jincheng Cao",
      "Ruichen Jiang",
      "Erfan Yazdandoost Hamedani",
      "Aryan Mokhtari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4fd5cfd2e31bebbccfa5ffa354c04bdc-Abstract-Conference.html": {
    "title": "Large Scale Transfer Learning for Tabular Data via Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josh Gardner",
      "Juan Perdomo",
      "Ludwig Schmidt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4fd96b997454b5b02698595df70fccaf-Abstract-Conference.html": {
    "title": "ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingrui Wu",
      "Xinyue Cai",
      "Jiayi Ji",
      "Jiale Li",
      "Oucheng Huang",
      "Gen Luo",
      "Hao Fei",
      "Guannan Jiang",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/4fe1859112230a032c7143a9adc3be78-Abstract-Conference.html": {
    "title": "Continuous Spatiotemporal Events Decoupling through Spike-based Bayesian Computation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yajing Zheng",
      "Jiyuan Zhang",
      "Zhaofei Yu",
      "Tiejun Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/50358459632f7fc1c7e9f9f0ad0cc026-Abstract-Conference.html": {
    "title": "4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng Yu",
      "Chaoyang Wang",
      "Peiye Zhuang",
      "Willi Menapace",
      "Aliaksandr Siarohin",
      "Junli Cao",
      "László Jeni",
      "Sergey Tulyakov",
      "Hsin-Ying Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5035a409f5798e188079e236f437e522-Abstract-Conference.html": {
    "title": "Transferable Boltzmann Generators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leon Klein",
      "Frank Noe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5039a14b703c4fda9c304a193dfd6d1e-Abstract-Conference.html": {
    "title": "Biologically Inspired Learning Model for Instructed Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roy Abel",
      "Shimon Ullman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/504fa7e518da9d1b53a233ed20a38b46-Abstract-Conference.html": {
    "title": "Can Language Models Learn to Skip Steps?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tengxiao Liu",
      "Qipeng Guo",
      "Xiangkun Hu",
      "Cheng Jiayang",
      "Yue Zhang",
      "Xipeng Qiu",
      "Zheng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/507c24f0dcc69d8fc99789a29ff20f25-Abstract-Conference.html": {
    "title": "Learning Noisy Halfspaces with a Margin: Massart is No Harder than Random",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gautam Chandrasekaran",
      "Vasilis Kontonis",
      "Konstantinos Stavropoulos",
      "Kevin Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5087b25434cb4a9aca928f2a12f5c47b-Abstract-Conference.html": {
    "title": "GraphCroc: Cross-Correlation Autoencoder for Graph Structural Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijin Duan",
      "Ruyi Ding",
      "Jiaxing He",
      "Aidong Ding",
      "Yunsi Fei",
      "Xiaolin Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/50be7e77b9c883144940be925b608acc-Abstract-Conference.html": {
    "title": "Gradient-Free Methods for Nonconvex Nonsmooth Stochastic Compositional Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuanghua Liu",
      "Luo Luo",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/50d277e84b2bcbaadcd84548a87e8cc4-Abstract-Conference.html": {
    "title": "A versatile informative diffusion model for single-cell ATAC-seq data generation and analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Huang",
      "Lei Xiong",
      "Na Sun",
      "Zunpeng Liu",
      "Ka-Chun Wong",
      "Manolis Kellis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/50d4f8ff0416cedfe0771b7ad947a197-Abstract-Conference.html": {
    "title": "Dual-frame Fluid Motion Estimation with Test-time Optimization and Zero-divergence Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Zhang",
      "Huan-ang Gao",
      "zhou jiang",
      "Hao Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/50ea4cf434c84a50203d7145afc7b853-Abstract-Conference.html": {
    "title": "Enhancing Domain Adaptation through Prompt Gradient Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viet Hoang Phan",
      "Tung Lam Tran",
      "Quyen Tran",
      "Trung Le"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/50ea4dbd1cff6bd3daef939eff10c092-Abstract-Conference.html": {
    "title": "From Instance Training to Instruction Learning: Task Adapters Generation from Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huanxuan Liao",
      "Shizhu He",
      "Yao Xu",
      "Yuanzhe Zhang",
      "Yanchao Hao",
      "Shengping Liu",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/50ee6db59fca8643dc625829d4a0eab9-Abstract-Conference.html": {
    "title": "Flaws can be Applause: Unleashing Potential of Segmenting Ambiguous Objects in SAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxin Li",
      "Yuzhihuang",
      "WUYANG LI",
      "Hengyu Liu",
      "Xinyu Liu",
      "Qing Xu",
      "Zhen Chen",
      "Yue Huang",
      "Yixuan Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/50f1dd93c57ba57fe246fba9e64f5b20-Abstract-Conference.html": {
    "title": "HairFastGAN: Realistic and Robust Hair Transfer with a Fast Encoder-Based Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxim Nikolaev",
      "Mikhail Kuznetsov",
      "Dmitry P Vetrov",
      "Aibek Alanov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/510950c4e75d8bbe430dbe01c8ad2426-Abstract-Conference.html": {
    "title": "Adaptive Important Region Selection with Reinforced Hierarchical Search for Dense Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingrong Wang",
      "Hitesh Sapkota",
      "Qi Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/510ad3018bbdc5b6e3b10646e2e35771-Abstract-Conference.html": {
    "title": "PLIP: Language-Image Pre-training for Person Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialong Zuo",
      "Jiahao Hong",
      "Feng Zhang",
      "Changqian Yu",
      "Hanyu Zhou",
      "Changxin Gao",
      "Nong Sang",
      "Jingdong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/510cfd9945f8bde6f0cf9b27ff1f8a76-Abstract-Conference.html": {
    "title": "DEL: Discrete Element Learner for Learning 3D Particle Dynamics with Neural Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JIAXU WANG",
      "Jingkai SUN",
      "ziyi Zhang",
      "Junhao He",
      "Qiang Zhang",
      "Mingyuan Sun",
      "Renjing Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/510d0935b543a29d686f93fa52d1c288-Abstract-Conference.html": {
    "title": "In-N-Out: Lifting 2D Diffusion Prior for 3D Object Removal via Tuning-Free Latents Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongting Hu",
      "Huan Fu",
      "Jiaxian Guo",
      "Liuhua Peng",
      "Tingjin Chu",
      "Feng Liu",
      "Tongliang Liu",
      "Mingming Gong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/51173cf34c5faac9796a47dc2fdd3a71-Abstract-Conference.html": {
    "title": "Are More LLM Calls All You Need? Towards the Scaling Properties of Compound AI Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingjiao Chen",
      "Jared Quincy Davis",
      "Boris Hanin",
      "Peter Bailis",
      "Ion Stoica",
      "Matei A Zaharia",
      "James Y Zou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5141f6bc105d30edbae48f1d2e0b1e66-Abstract-Conference.html": {
    "title": "Federated Transformer: Multi-Party Vertical Federated Learning on Practical Fuzzily Linked Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaomin Wu",
      "Junyi Hou",
      "Yiqun Diao",
      "Bingsheng He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/514b48017505dc778c6db2d845cbbe24-Abstract-Conference.html": {
    "title": "MMSite: A Multi-modal Framework for the Identification of Active Sites in Proteins",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Ouyang",
      "Huiyu Cai",
      "Yong Luo",
      "Kehua Su",
      "Lefei Zhang",
      "Bo Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/515c62809e0a29729d7eec26e2916fc0-Abstract-Conference.html": {
    "title": "Questioning the Survey Responses of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ricardo Dominguez-Olmedo",
      "Moritz Hardt",
      "Celestine Mendler-Dünner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/518a75257f37b32f711082dff33c2ffc-Abstract-Conference.html": {
    "title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via Adaptive Heads Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilong Chen",
      "Linhao Zhang",
      "Junyuan Shang",
      "Zhenyu Zhang",
      "Tingwen Liu",
      "Shuohuan Wang",
      "YU SUN"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5195825ee60d7efc1e42b7f3f3137040-Abstract-Conference.html": {
    "title": "Connectivity Shapes Implicit Regularization in Matrix Factorization Models for Matrix Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Bai",
      "Jiajie Zhao",
      "Yaoyu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/51aaa93cf41101da5dd1ce239184fb46-Abstract-Conference.html": {
    "title": "Diversify, Contextualize, and Adapt: Efficient Entropy Modeling for Neural Image Codec",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun-Hyuk Kim",
      "Seungeon Kim",
      "Won-Hee Lee",
      "Dokwan Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/51c6e143b5da2bd6e4a618d8a5d7f38b-Abstract-Conference.html": {
    "title": "Goal-Conditioned On-Policy Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Gong",
      "Feng Dawei",
      "Kele Xu",
      "Bo Ding",
      "Huaimin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/51d317df78eded9eb3c9d3fb1091c279-Abstract-Conference.html": {
    "title": "FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anuroop Sriram",
      "Benjamin Miller",
      "Ricky T. Q. Chen",
      "Brandon Wood"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/520416e27d3b0cef3cd70a083e2991c7-Abstract-Conference.html": {
    "title": "Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongru Yang",
      "Bhavya Kailkhura",
      "Zhangyang &quot;Atlas&quot; Wang",
      "Yingbin Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/520425a5a4c2fb7f7fc345078b188201-Abstract-Conference.html": {
    "title": "BELM: Bidirectional Explicit Linear Multi-step Sampler for Exact Inversion in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangyikang Wang",
      "Hubery Yin",
      "Yue-Jiang Dong",
      "Huminhao Zhu",
      "zhang chao",
      "Hanbin Zhao",
      "Hui Qian",
      "Chen Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/520b379123d16e41f85472e766846486-Abstract-Conference.html": {
    "title": "Learning to Understand: Identifying Interactions via the Möbius Transform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Kang",
      "Yigit Efe Erginbas",
      "Landon Butler",
      "Ramtin Pedarsani",
      "Kannan Ramchandran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/522134ee1c52c7a2b929bc87cfe1781c-Abstract-Conference.html": {
    "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangji Bai",
      "Yijiang Li",
      "Chen LING",
      "Kibaek Kim",
      "Liang Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5227ce00add5aa0a12d1c4ee92fcd2dc-Abstract-Conference.html": {
    "title": "Text-Aware Diffusion for Policy Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Calvin Luo",
      "Mandy He",
      "Zilai Zeng",
      "Chen Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/522ef98b1e52f5918e5abc868651175d-Abstract-Conference.html": {
    "title": "Private Geometric Median",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahdi Haghifam",
      "Thomas Steinke",
      "Jonathan Ullman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5255f5dcf1bd6532aed9470bb556c64a-Abstract-Conference.html": {
    "title": "Goal Conditioned Reinforcement Learning for Photo Finishing Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarui Wu",
      "Yujin Wang",
      "Lingen Li",
      "Zhang Fan",
      "Tianfan Xue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/526dad6de50a4a4d829d6b4e37e5a8f5-Abstract-Conference.html": {
    "title": "Private and Personalized Frequency Estimation in a Federated Setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amrith Setlur",
      "Vitaly Feldman",
      "Kunal Talwar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/527d9d8f89aec80d634e366a97f49ba8-Abstract-Conference.html": {
    "title": "How does Gradient Descent Learn Features --- A Local Analysis for Regularized Two-Layer Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mo Zhou",
      "Rong Ge"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/527dad0b9159805289906d5740a0bdd3-Abstract-Conference.html": {
    "title": "Pipeline Parallelism with Controllable Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Penghui Qi",
      "Xinyi Wan",
      "Nyamdavaa Amar",
      "Min Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/52a599ccd90f6830ef316c65a60de0a7-Abstract-Conference.html": {
    "title": "ReFIR: Grounding Large Restoration Models with Retrieval Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Guo",
      "Tao Dai",
      "Zhihao Ouyang",
      "Taolin Zhang",
      "Yaohua Zha",
      "Bin Chen",
      "Shu-Tao Xia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/52c21a32429a7d6050430b606a286a75-Abstract-Conference.html": {
    "title": "Grounded Answers for Multi-agent Decision-making Problem through Generative World Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyang Liu",
      "Xinrui Yang",
      "Shiguang Sun",
      "Long Qian",
      "Lipeng Wan",
      "Xingyu Chen",
      "Xuguang Lan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/52edb6eb7ddf239839e6e28874a842bb-Abstract-Conference.html": {
    "title": "AlterMOMA: Fusion Redundancy Pruning for Camera-LiDAR Fusion Models with Alternative Modality Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiqi Sun",
      "Yantao Lu",
      "Ning Liu",
      "Bo Jiang",
      "Jinchao Chen",
      "Ying Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/52f050499cf82fa8efb588e263f6f3a7-Abstract-Conference.html": {
    "title": "Loss Landscape Characterization of Neural Networks without Over-Parametrization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rustem Islamov",
      "Niccolò Ajroldi",
      "Antonio Orvieto",
      "Aurelien Lucchi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5301c49207917c5c870131959971851c-Abstract-Conference.html": {
    "title": "Gradient-based Discrete Sampling with Automatic Cyclical Scheduling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Pynadath",
      "Riddhiman Bhattacharya",
      "ARUN HARIHARAN",
      "Ruqi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/531998dc1fc858b5857a90b74d96ecab-Abstract-Conference.html": {
    "title": "2D-OOB: Attributing Data Contribution Through Joint Valuation Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Sun",
      "Jingyan Shen",
      "Yongchan Kwon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/532ce4fcf853023c4cf2ac38cbc5d002-Abstract-Conference.html": {
    "title": "Zero-Shot Tokenizer Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Minixhofer",
      "Edoardo Maria Ponti",
      "Ivan Vulić"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5358d1e6138dff5718c5e9790f5fa593-Abstract-Conference.html": {
    "title": "Theoretical Analysis of Weak-to-Strong Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hunter Lang",
      "David Sontag",
      "Aravindan Vijayaraghavan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5367f6d58cc98dc929e1b27fcaf2b0a6-Abstract-Conference.html": {
    "title": "Grasp as You Say: Language-guided Dexterous Grasp Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Lin Wei",
      "Jian-Jian Jiang",
      "Chengyi Xing",
      "Xian-Tuo Tan",
      "Xiao-Ming Wu",
      "Hao Li",
      "Mark Cutkosky",
      "Wei-Shi Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/53704142f230054140418ecd8857f391-Abstract-Conference.html": {
    "title": "RGFN: Synthesizable Molecular Generation Using GFlowNets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michał Koziarski",
      "Andrei Rekesh",
      "Dmytro Shevchuk",
      "Almer van der Sloot",
      "Piotr Gaiński",
      "Yoshua Bengio",
      "Chenghao Liu",
      "Mike Tyers",
      "Robert Batey"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/53923bb44655a7defb31c7744c01b62b-Abstract-Conference.html": {
    "title": "Exploring Molecular Pretraining Model at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "xiaohong ji",
      "Zhen Wang",
      "Zhifeng Gao",
      "Hang Zheng",
      "Linfeng Zhang",
      "Guolin Ke",
      "Weinan E"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/53dbd7e34fab703a639964e2d3ee9e84-Abstract-Conference.html": {
    "title": "Model Collapse Demystified: The Case of Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elvis Dohmatob",
      "Yunzhen Feng",
      "Julia Kempe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/53f2c82c6b165a963b353194113ee71e-Abstract-Conference.html": {
    "title": "Metric Transforms and Low Rank Representations of Kernels for Fast Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timothy Chu",
      "Josh Alman",
      "Gary L. Miller",
      "Shyam Narayanan",
      "Mark Sellke",
      "Zhao Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/53fba4404ebecf9730dc8919b71d4d22-Abstract-Conference.html": {
    "title": "Generalizable Person Re-identification via Balancing Alignment and Uniformity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoonki Cho",
      "Jaeyoon Kim",
      "Woo Jae Kim",
      "Junsik Jung",
      "Sung-eui Yoon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/54024fca0cef9911be36319e622cde38-Abstract-Conference.html": {
    "title": "WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liwei Jiang",
      "Kavel Rao",
      "Seungju Han",
      "Allyson Ettinger",
      "Faeze Brahman",
      "Sachin Kumar",
      "Niloofar Mireshghallah",
      "Ximing Lu",
      "Maarten Sap",
      "Yejin Choi",
      "Nouha Dziri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/540a6eefb60428c8547a27253f9a2a59-Abstract-Conference.html": {
    "title": "Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Hu",
      "Tuo Liang",
      "Jing Li",
      "Yiren Lu",
      "Yunlai Zhou",
      "Yiran Qiao",
      "Jing Ma",
      "Yu Yin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/54191f424e9013fc1d7b923f6e45dff4-Abstract-Conference.html": {
    "title": "Controlling Continuous Relaxation for Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuma Ichikawa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5424f6b74f475ad738b54888d609283a-Abstract-Conference.html": {
    "title": "Reinforced Cross-Domain Knowledge Distillation on Time Series Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "QING XU",
      "Min Wu",
      "Xiaoli Li",
      "Kezhi Mao",
      "Zhenghua Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/547108084f0c2af39b956f8eadb75d1b-Abstract-Conference.html": {
    "title": "3DET-Mamba: Causal Sequence Modelling for End-to-End 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingsheng Li",
      "Jiakang Yuan",
      "Sijin Chen",
      "Lin Zhang",
      "Anyu Zhu",
      "Xin Chen",
      "Tao Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/54801e196796134a2b0ae5e8adef502f-Abstract-Conference.html": {
    "title": "DisCEdit: Model Editing by Identifying Discriminative Components",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaitanya Murti",
      "Chiranjib Bhattacharyya"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/548551c07a68c8f0a87d67c6167cedb1-Abstract-Conference.html": {
    "title": "Prompt Tuning Strikes Back: Customizing Foundation Models with Low-Rank Prompt Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhinav Jain",
      "Swarat Chaudhuri",
      "Thomas Reps",
      "Christopher Jermaine"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5499193036671a1ff6693fdb5a863d22-Abstract-Conference.html": {
    "title": "SEA: State-Exchange Attention for High-Fidelity Physics Based Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parsa Esmati",
      "Amirhossein Dadashzadeh",
      "Vahid Ardakani",
      "Nicolas Larrosa",
      "Nicolò Grilli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/54c67d3db2df24a31cf045525f9460b9-Abstract-Conference.html": {
    "title": "Referencing Where to Focus: Improving Visual Grounding with Referential Query",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yabing Wang",
      "Zhuotao Tian",
      "Qingpei Guo",
      "Zheng Qin",
      "Sanping Zhou",
      "Ming Yang",
      "Le Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/54ca2c45ce17dbd9ebad7ab0f39c825a-Abstract-Conference.html": {
    "title": "Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayantan Choudhury",
      "Nazarii Tupitsa",
      "Nicolas Loizou",
      "Samuel Horváth",
      "Martin Takac",
      "Eduard Gorbunov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/54d689d58fe54c92aee2d732fc49fca8-Abstract-Conference.html": {
    "title": "CausalStock: Deep End-to-end Causal Discovery for News-driven Multi-stock Movement Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuqi Li",
      "Yuebo Sun",
      "Yuxin Lin",
      "Xin Gao",
      "Shuo Shang",
      "Rui Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/54dcf25318f9de5a7a01f0a4125c541e-Abstract-Conference.html": {
    "title": "Improved Distribution Matching Distillation for Fast Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianwei Yin",
      "Michaël Gharbi",
      "Taesung Park",
      "Richard Zhang",
      "Eli Shechtman",
      "Fredo Durand",
      "Bill Freeman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/54dd9e0cff6d9214e20d97eb2a3bae49-Abstract-Conference.html": {
    "title": "Expert-level protocol translation for self-driving labs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Zhe Shi",
      "Fanxu Meng",
      "Haofei Hou",
      "Zhangqian Bi",
      "Qiao Xu",
      "Lecheng Ruan",
      "Qining Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/54ebbbbb1dec165816e7bb601db57a2c-Abstract-Conference.html": {
    "title": "Satformer: Accurate and Robust Traffic Data Estimation for Satellite Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Qin",
      "Xiyuan Liu",
      "Wenting Wei",
      "Liang Chengbin",
      "Huaxi Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/54ece32fe923c26b3de15d0da182e008-Abstract-Conference.html": {
    "title": "Generalized Tensor Decomposition for Understanding Multi-Output Regression under Combinatorial Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andong Wang",
      "Yuning Qiu",
      "Mingyuan Bai",
      "Zhong Jin",
      "Guoxu Zhou",
      "Qibin Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/54fa8255cdf30736ecad38e842725e7f-Abstract-Conference.html": {
    "title": "Unleashing the Denoising Capability of Diffusion Prior for Solving Inverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Zhang",
      "Jiaxin Zhuang",
      "Cheng Jin",
      "Gen Li",
      "Yuantao Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5503a7c69d48a2f86fc00b3dc09de686-Abstract-Conference.html": {
    "title": "Task Confusion and Catastrophic Forgetting in Class-Incremental Learning: A Mathematical Framework for Discriminative and Generative Modelings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milad Khademi Nori",
      "Il-Min Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/555479a201da27c97aaeed842d16ca49-Abstract-Conference.html": {
    "title": "Stepping on the Edge: Curvature Aware Learning Rate Tuners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Roulet",
      "Atish Agarwala",
      "Jean-Bastien Grill",
      "Grzegorz Swirszcz",
      "Mathieu Blondel",
      "Fabian Pedregosa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/55576bcdf386ba73859fb71766f85758-Abstract-Conference.html": {
    "title": "Do's and Don'ts: Learning Desirable Skills with Instruction Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HYUNSEUNG KIM",
      "BYUNG KUN LEE",
      "Hojoon Lee",
      "Dongyoon Hwang",
      "Donghu Kim",
      "Jaegul Choo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5564f3974ad0e49778dfdcee8e5dc385-Abstract-Conference.html": {
    "title": "Towards Accurate and Fair Cognitive Diagnosis via Monotonic Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "zheng zhang",
      "Wei Song",
      "Qi Liu",
      "Qingyang Mao",
      "Yiyan Wang",
      "Weibo Gao",
      "Zhenya Huang",
      "Shijin Wang",
      "Enhong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5572bc595de865c1450868fd5391e9c5-Abstract-Conference.html": {
    "title": "MAGNET: Improving the Multilingual Fairness of Language Models with Adaptive Gradient-Based Tokenization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Orevaoghene Ahia",
      "Sachin Kumar",
      "Hila Gonen",
      "Valentin Hofmann",
      "Tomasz Limisiewicz",
      "Yulia Tsvetkov",
      "Noah A. Smith"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/55769e1208c7f45e9acc98f06279c10c-Abstract-Conference.html": {
    "title": "Action Gaps and Advantages in Continuous-Time Distributional Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harley Wiltzer",
      "Marc Bellemare",
      "David Meger",
      "Patrick Shafto",
      "Yash Jhaveri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/558d48c1f08675daa636e09bfe94a89e-Abstract-Conference.html": {
    "title": "Does Egalitarian Fairness Lead to Instability? The Fairness Bounds in Stable Federated Learning Under Altruistic Behaviors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiashi Gao",
      "Ziwei Wang",
      "Xiangyu Zhao",
      "Xin Yao",
      "Xuetao Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/558da25ef295121e2d79209a95e45436-Abstract-Conference.html": {
    "title": "Counterfactual Fairness by Combining Factual and Counterfactual Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Zhou",
      "Tianci Liu",
      "Ruqi Bai",
      "Jing Gao",
      "Murat Kocaoglu",
      "David I Inouye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5598cf1b2905a26ddb863e6705588327-Abstract-Conference.html": {
    "title": "Does Video-Text Pretraining Help Open-Vocabulary Online Action Detection?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "qingsong zhao",
      "Yi Wang",
      "Jilan Xu",
      "Yinan He",
      "Zifan Song",
      "Limin Wang",
      "Yu Qiao",
      "Cairong Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/55aeba84b402008d3ed10440d906b4e1-Abstract-Conference.html": {
    "title": "Sample-efficient Bayesian Optimisation Using Known Invariances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Theodore Brown",
      "Alexandru Cioba",
      "Ilija Bogunovic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/55c944c9d908af245fa658b0e02b9003-Abstract-Conference.html": {
    "title": "Enhancing Consistency-Based Image Generation via Adversarialy-Trained Classification and Energy-Based Discrimination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shelly Golan",
      "Roy Ganz",
      "Michael Elad"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/55cb562b1f5af71f6707f3ff3c7941e6-Abstract-Conference.html": {
    "title": "Multi-view Masked Contrastive Representation Learning for Endoscopic Video Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Hu",
      "Ye Xiao",
      "Yuan Zhang",
      "Xieping Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/55d16334943f8728073e17139e5baa3d-Abstract-Conference.html": {
    "title": "WATT: Weight Average Test Time Adaptation of CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David OSOWIECHI",
      "Mehrdad Noori",
      "Gustavo Vargas Hakim",
      "Moslem Yazdanpanah",
      "Ali Bahri",
      "Milad Cheraghalikhani",
      "Sahar Dastani",
      "Farzad Beizaee",
      "Ismail Ayed",
      "Christian Desrosiers"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/55f5aa8d997a8071d0651e0009ec49a2-Abstract-Conference.html": {
    "title": "Taming Heavy-Tailed Losses in Adversarial Bandits and the Best-of-Both-Worlds Setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duo Cheng",
      "Xingyu Zhou",
      "Bo Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/560f7d557a41e54a64b43cb052766557-Abstract-Conference.html": {
    "title": "Online Control with Adversarial Disturbance for Continuous-time Linear Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwei Li",
      "Jing Dong",
      "Can Chang",
      "Baoxiang Wang",
      "Jingzhao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5623c35f3ab5e2c72aeb3abce27dc28f-Abstract-Conference.html": {
    "title": "Graph Neural Networks Do Not Always Oversmooth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bastian Epping",
      "Alexandre René",
      "Moritz Helias",
      "Michael T Schaub"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5635925cf9d2274f338eb0dd5971e845-Abstract-Conference.html": {
    "title": "HLM-Cite: Hybrid Language Model Workflow for Text-based Scientific Citation Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianyue Hao",
      "Jingyang Fan",
      "Fengli Xu",
      "Jian Yuan",
      "Yong Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5681251fa039cf49d6d11b906eded1b3-Abstract-Conference.html": {
    "title": "Identification and Estimation of the Bi-Directional MR with Some Invalid Instruments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Xie",
      "Zhen Yao",
      "Lin Xie",
      "Yan Zeng",
      "Zhi Geng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5687bf1b132e2e26200c53d04d398e86-Abstract-Conference.html": {
    "title": "STONE: A Submodular Optimization Framework for Active 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "RUIYU MAO",
      "Sarthak Kumar Maharana",
      "Rishabh Iyer",
      "Yunhui Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/56a033d57583623b4034d0ba80119601-Abstract-Conference.html": {
    "title": "Estimating Generalization Performance Along the Trajectory of Proximal SGD in Robust Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Tan",
      "Pierre C Bellec"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/56a225639da77e8f7c0409f6d5ba996b-Abstract-Conference.html": {
    "title": "Learning Better Representations From Less Data For Propositional Satisfiability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Ghanem",
      "Frederik Schmitt",
      "Julian Siber",
      "Bernd Finkbeiner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/56ad264ac7448239145606cf4106042f-Abstract-Conference.html": {
    "title": "SARAD: Spatial Association-Aware Anomaly Detection and Diagnosis for Multivariate Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Dai",
      "Ligang He",
      "Shuanghua Yang",
      "Matthew Leeke"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/56bd21259e28ebdc4d7e1503733bf421-Abstract-Conference.html": {
    "title": "Decoupling Semantic Similarity from Spatial Alignment for Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tassilo Wald",
      "Constantin Ulrich",
      "Priyank Jaini",
      "Gregor Koehler",
      "David Zimmerer",
      "Stefan Denner",
      "Fabian Isensee",
      "Michael Baumgartner",
      "Klaus Maier-Hein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/56ed2bd15b66f709cd81cb1aaa0496b9-Abstract-Conference.html": {
    "title": "Measuring Per-Unit Interpretability at Scale Without Humans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roland S. Zimmermann",
      "David Klindt",
      "Wieland Brendel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5706668422bd0d82588998ebe1067133-Abstract-Conference.html": {
    "title": "Efficient Recurrent Off-Policy RL Requires a Context-Encoder-Specific Learning Rate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan-Ming Luo",
      "Zuolin Tu",
      "Zefang Huang",
      "Yang Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/571082ea18d30060177dfcaf662ff0e5-Abstract-Conference.html": {
    "title": "Model Based Inference of Synaptic Plasticity Rules",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Mehta",
      "Danil Tyulmankov",
      "Adithya Rajagopalan",
      "Glenn Turner",
      "James Fitzgerald",
      "Jan Funke"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/571c7e164fb1ffbcf2f84a63784451ec-Abstract-Conference.html": {
    "title": "Similarity-Navigated Conformal Prediction for Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianqing Song",
      "Jianguo Huang",
      "Wenyu Jiang",
      "Baoming Zhang",
      "Shuangjie Li",
      "Chongjun Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/571dd493cf500fc5bde887a6b5d4c941-Abstract-Conference.html": {
    "title": "ReplaceAnything3D: Text-Guided Object Replacement in 3D Scenes with Compositional Scene Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edward Bartrum",
      "Thu H Nguyen-Phuoc",
      "Christopher Xie",
      "Zhengqin Li",
      "Numair Khan",
      "Armen Avetisyan",
      "Douglas Lanman",
      "Lei Xiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/572a6f16ec44f794fb3e0f8a310acbc6-Abstract-Conference.html": {
    "title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eli Sennesh",
      "Hao Wu",
      "Tommaso Salvatori"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/577cd5863ec73be4e6871340be0936ae-Abstract-Conference.html": {
    "title": "Monomial Matrix Group Equivariant Neural Functional Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoang Tran",
      "Thieu Vo",
      "Tho Huu",
      "An Nguyen The",
      "Tan Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5783212d85c205ef823b8974d44872c5-Abstract-Conference.html": {
    "title": "Error Analysis of Spherically Constrained Least Squares Reformulation in Solving the Stackelberg Prediction Game",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyuan Li",
      "Weiwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/578e65cdee35d00c708d4c64bce32971-Abstract-Conference.html": {
    "title": "On the cohesion and separability of average-link for hierarchical agglomerative clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eduardo Laber",
      "Miguel Batista"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/579b5b84311e122584dedab1d3b7613f-Abstract-Conference.html": {
    "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Namgyu Ho",
      "Sangmin Bae",
      "Taehyeon Kim",
      "Hyunjik Jo",
      "Yireun Kim",
      "Tal Schuster",
      "Adam Fisch",
      "James Thorne",
      "Se-Young Yun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/57a3c602f0a1c8980cc5ed07e49d9490-Abstract-Conference.html": {
    "title": "Linear Transformers are Versatile In-Context Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Vladymyrov",
      "Johannes von Oswald",
      "Mark Sandler",
      "Rong Ge"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/57c56985d9afe89bf78a8264c91071aa-Abstract-Conference.html": {
    "title": "CountGD: Multi-Modal Open-World Counting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niki Amini-Naieni",
      "Tengda Han",
      "Andrew Zisserman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/57c5a7c83b056d74bc97b7db36bd3649-Abstract-Conference.html": {
    "title": "SimGen: Simulator-conditioned Driving Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunsong Zhou",
      "Michael Simon",
      "Zhenghao (Mark) Peng",
      "Sicheng Mo",
      "Hongzi Zhu",
      "Minyi Guo",
      "Bolei Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/57c89126d60c209f48d0e6395c766bb3-Abstract-Conference.html": {
    "title": "Decoding-Time Language Model Alignment with Multiple Objectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruizhe Shi",
      "Yifang Chen",
      "Yushi Hu",
      "Alisa Liu",
      "Hanna Hajishirzi",
      "Noah A. Smith",
      "Simon S Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/57ef0373c890b30407eadfe6e06c8c84-Abstract-Conference.html": {
    "title": "SS1: Accelerating Inference with Fast and Expressive Sketch Structured Transform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Desai",
      "Kimia Saedi",
      "Apoorv Walia",
      "Jihyeong Lee",
      "Keren Zhou",
      "Anshumali Shrivastava"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/57fabaa549352c52d5d312171b16970e-Abstract-Conference.html": {
    "title": "FedGMark: Certifiably Robust Watermarking for Federated Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Yang",
      "Qiang Li",
      "Yuan Hong",
      "Binghui Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5808ba2d4643885430fc8d0f69fd6d73-Abstract-Conference.html": {
    "title": "Learning Macroscopic Dynamics from Partial Microscopic Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyi Chen",
      "Qianxiao Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/580c4ec4738ff61d5862a122cdf139b6-Abstract-Conference.html": {
    "title": "Spatio-Spectral Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Markus Geisler",
      "Arthur Kosmala",
      "Daniel Herbst",
      "Stephan Günnemann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/581e1a06fa20f2c079dc5fb2db236335-Abstract-Conference.html": {
    "title": "On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyu Zheng",
      "Wei Huang",
      "Rongzhen Wang",
      "Guoqiang Wu",
      "Jun Zhu",
      "Chongxuan LI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5828b7516d6f117e8301120519366cdb-Abstract-Conference.html": {
    "title": "Enhancing Protein Mutation Effect Prediction through a Retrieval-Augmented Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihan Guo",
      "Rui Wang",
      "Ruidong Wu",
      "Zhizhou Ren",
      "Jiahan Li",
      "Shitong Luo",
      "Zuofan Wu",
      "Qiang Liu",
      "Jian Peng",
      "Jianzhu Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/582e9771ac8527cb6390e5e9444a0fee-Abstract-Conference.html": {
    "title": "Key-Grid: Unsupervised 3D Keypoints Detection using Grid Heatmap Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengkai Hou",
      "Zhengrong Xue",
      "Bingyang Zhou",
      "Jinghan Ke",
      "Lin Shao",
      "Huazhe Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/583b87de2073d6bdc2762e6c804bb89e-Abstract-Conference.html": {
    "title": "Active Learning of General Halfspaces: Label Queries vs Membership Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilias Diakonikolas",
      "Daniel Kane",
      "Mingchen Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/584b78c26916e5947c5b0c4ff8e7c960-Abstract-Conference.html": {
    "title": "Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runlin Lei",
      "Yuwei Hu",
      "Yuchen Ren",
      "Zhewei Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/585e9cf25585612ac27b535457116513-Abstract-Conference.html": {
    "title": "FlexPlanner: Flexible 3D Floorplanning via Deep Reinforcement Learning in Hybrid Action Space with Multi-Modality Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruizhe Zhong",
      "Xingbo Du",
      "Shixiong Kai",
      "Zhentao Tang",
      "Siyuan Xu",
      "Jianye Hao",
      "Mingxuan Yuan",
      "Junchi Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/58692a1701314e09cbd7a5f5f3871cc9-Abstract-Conference.html": {
    "title": "Learning to Shape In-distribution Feature Space for Out-of-distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yonggang Zhang",
      "Jie Lu",
      "Bo Peng",
      "Zhen Fang",
      "Yiu-ming Cheung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/587b3f360588143a751c37fcb3b5db7f-Abstract-Conference.html": {
    "title": "Equivariant Neural Diffusion for Molecule Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "François Cornet",
      "Grigory Bartosh",
      "Mikkel Schmidt",
      "Christian Andersson Naesseth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/58acaf7b9aecc9604c7a6aac2eb81035-Abstract-Conference.html": {
    "title": "SyncVIS: Synchronized Video Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongkun Zheng",
      "Lu Qi",
      "Xi Chen",
      "Yi Wang",
      "Kun Wang",
      "Yu Qiao",
      "Hengshuang Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/58b286aea34a91a3d33e58af0586fa40-Abstract-Conference.html": {
    "title": "Gradients of Functions of Large Matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Krämer",
      "Pablo Moreno-Muñoz",
      "Hrittik Roy",
      "Søren Hauberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/58cbe393b4254da8966780a40d023c0b-Abstract-Conference.html": {
    "title": "Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanpu Cao",
      "Tianrong Zhang",
      "Bochuan Cao",
      "Ziyi Yin",
      "Lu Lin",
      "Fenglong Ma",
      "Jinghui Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/58cc11cda2a2679e8af5c6317aed0af8-Abstract-Conference.html": {
    "title": "Achieving Domain-Independent Certified Robustness via Knowledge Continuity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alan Sun",
      "Chiyu Ma",
      "Kenneth Ge",
      "Soroush Vosoughi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/58cd3b02902d79aea4b3b603fb0d0941-Abstract-Conference.html": {
    "title": "Monte Carlo Tree Search based Space Transfer for Black Box Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shukuan Wang",
      "Ke Xue",
      "Song Lei",
      "Xiaobin Huang",
      "Chao Qian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/58ce6a4b9c16d11975f11e4a23871041-Abstract-Conference.html": {
    "title": "Discovering Creative Behaviors through DUPLEX: Diverse Universal Features for Policy Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Borja G. Leon",
      "Francesco Riccio",
      "Kaushik Subramanian",
      "Peter Wurman",
      "Peter Stone"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/58dfad018974500457f84bb845b66776-Abstract-Conference.html": {
    "title": "Efficient Centroid-Linkage Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammadhossein Bateni",
      "Laxman Dhulipala",
      "Willem Fletcher",
      "Kishen N. Gowda",
      "D Ellis Hershkowitz",
      "Rajesh Jayaram",
      "Jakub Lacki"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/58e6c003c9fb3992265005ff6aef1913-Abstract-Conference.html": {
    "title": "Everyday Object Meets Vision-and-Language Navigation Agent via Backdoor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keji He",
      "Kehan Chen",
      "Jiawang Bai",
      "Yan Huang",
      "Qi Wu",
      "Shu-Tao Xia",
      "Liang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/59091e8226128dfad5f1d75a58f18906-Abstract-Conference.html": {
    "title": "GS-Hider: Hiding Messages into 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanyu Zhang",
      "Jiarui Meng",
      "Runyi Li",
      "Zhipei Xu",
      "yongbing zhang",
      "Jian Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5934ef82865c4456fc708c311801169d-Abstract-Conference.html": {
    "title": "Rethinking Decoders for Transformer-based Semantic Segmentation: A Compression Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qishuai Wen",
      "Chun-Guang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5942d10ae51b6bd07648e54df07ef9cd-Abstract-Conference.html": {
    "title": "LLMs Can Evolve Continually on Modality for $\\mathbb{X}$-Modal Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazuo Yu",
      "Haomiao Xiong",
      "Lu Zhang",
      "Haiwen Diao",
      "Yunzhi Zhuge",
      "Lanqing Hong",
      "Dong Wang",
      "Huchuan Lu",
      "You He",
      "Long Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/594743d814bc534c20381c0871ed384e-Abstract-Conference.html": {
    "title": "DarkSAM: Fooling Segment Anything Model to Segment Nothing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Zhou",
      "Yufei Song",
      "Minghui Li",
      "Shengshan Hu",
      "Xianlong Wang",
      "Leo Yu Zhang",
      "Dezhong Yao",
      "Hai Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5949a8750a110ce1f0631b1776c500a2-Abstract-Conference.html": {
    "title": "Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zaijing Li",
      "Yuquan Xie",
      "Rui Shao",
      "Gongwei Chen",
      "Dongmei Jiang",
      "Liqiang Nie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5966e7c5c017d8449d215faeb1db4173-Abstract-Conference.html": {
    "title": "Contextual Active Model Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuefeng Liu",
      "Fangfang Xia",
      "Rick Stevens",
      "Yuxin Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/597254dc45be8c166d3ccf0ba2d56325-Abstract-Conference.html": {
    "title": "Learning Successor Features the Simple Way",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raymond Chua",
      "Arna Ghosh",
      "Christos Kaplanis",
      "Blake Richards",
      "Doina Precup"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5993213b3f4ef528234836ecfb3b2933-Abstract-Conference.html": {
    "title": "Probabilistic size-and-shape functional mixed models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangyi Wang",
      "Karthik Bharath",
      "Oksana Chkrebtii",
      "Sebastian A. Kurtek"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/59a3444d39b97ba01a17994f938e1ccc-Abstract-Conference.html": {
    "title": "Diffusion Models are Certifiably Robust Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huanran Chen",
      "Yinpeng Dong",
      "Shitong Shao",
      "Hao Zhongkai",
      "Xiao Yang",
      "Hang Su",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/59a48c111f97f2174709ea9ed8e920d1-Abstract-Conference.html": {
    "title": "Diffusion Policies Creating a Trust Region for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Chen",
      "Zhendong Wang",
      "Mingyuan Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/59a9cc95f046e9125d8816ef971873e7-Abstract-Conference.html": {
    "title": "CA-SSLR: Condition-Aware Self-Supervised Learning Representation for Generalized Speech Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yen-Ju Lu",
      "Jing Liu",
      "Thomas Thebaud",
      "Laureano Moro-Velazquez",
      "Ariya Rastrow",
      "Najim Dehak",
      "Jesus Villalba"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/59c147c7d4fdb732daea3064eab949bf-Abstract-Conference.html": {
    "title": "Matryoshka Query Transformer for Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Hu",
      "Zi-Yi Dou",
      "Liunian Li",
      "Amita Kamath",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/59d4e18a60490b9ed9913f3be2b14839-Abstract-Conference.html": {
    "title": "Interpolating Item and User Fairness in Multi-Sided Recommendations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinyi Chen",
      "Jason Cheuk Nam Liang",
      "Negin Golrezaei",
      "Djallel Bouneffouf"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/59e73ff865b56cba6ab7f6b2cce1425d-Abstract-Conference.html": {
    "title": "TFGDA: Exploring Topology and Feature Alignment in Semi-supervised Graph Domain Adaptation through Robust Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Dan",
      "Weiming Liu",
      "Xie",
      "Hua Yu",
      "Shunjie Dong",
      "Yanchao Tan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/59eb2d8ce0e4830f80780f7f78c67dec-Abstract-Conference.html": {
    "title": "Towards Multi-Domain Learning for Generalizable Video Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MyeongAh Cho",
      "Taeoh Kim",
      "Minho Shim",
      "Dongyoon Wee",
      "Sangyoun Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/59f278de1619bdb6b53fd04e8e0976e0-Abstract-Conference.html": {
    "title": "GDeR: Safeguarding Efficiency, Balancing, and Robustness via Prototypical Graph Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guibin Zhang",
      "Haonan Dong",
      "yuchen zhang",
      "Zhixun Li",
      "Dingshuo Chen",
      "Kai Wang",
      "Tianlong Chen",
      "Yuxuan Liang",
      "Dawei Cheng",
      "Kun Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5a016da670821af25f151f523a2e563f-Abstract-Conference.html": {
    "title": "SpeechAlign: Aligning Speech Generation to Human Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Zhang",
      "Zhaowei Li",
      "Shimin Li",
      "Xin Zhang",
      "Pengyu Wang",
      "Yaqian Zhou",
      "Xipeng Qiu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5a05d477fef5a43c805d3377700efa40-Abstract-Conference.html": {
    "title": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjun Zhang",
      "Xin Fei",
      "Fangfu Liu",
      "Haixu Song",
      "Yueqi Duan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5a461bdff86cc07e976bb6c518810398-Abstract-Conference.html": {
    "title": "Splatter a Video: Video Gaussian Representation for Versatile Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang-Tian Sun",
      "Yihua Huang",
      "Lin Ma",
      "Xiaoyang Lyu",
      "Yan-Pei Cao",
      "Xiaojuan Qi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5a5acfd0876c940d81619c1dc60e7748-Abstract-Conference.html": {
    "title": "Approaching Human-Level Forecasting with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danny Halawi",
      "Fred Zhang",
      "Chen Yueh-Han",
      "Jacob Steinhardt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5a5ddf0ab751861025c00700093c5677-Abstract-Conference.html": {
    "title": "HYDRA-FL: Hybrid Knowledge Distillation for Robust and Accurate Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Momin Ahmad Khan",
      "Yasra Chandio",
      "Fatima Anwar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5a6815122f533193a022cbc41786c1cc-Abstract-Conference.html": {
    "title": "Selective Generation for Controllable Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minjae Lee",
      "Kyungmin Kim",
      "Taesoo Kim",
      "Sangdon Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5a7c947568c1b1328ccc5230172e1e7c-Abstract-Conference.html": {
    "title": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Yang",
      "Carlos Jimenez",
      "Alexander Wettig",
      "Kilian Lieret",
      "Shunyu Yao",
      "Karthik Narasimhan",
      "Ofir Press"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5aa96d1caa0d0b99d534b67df06be2ff-Abstract-Conference.html": {
    "title": "Towards the Dynamics of a DNN Learning Symbolic Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihan Ren",
      "Junpeng Zhang",
      "Yang Xu",
      "Yue Xin",
      "Dongrui Liu",
      "Quanshi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5aad86aa2a3c00b70c71e19bc4780319-Abstract-Conference.html": {
    "title": "Communication Bounds for the Distributed Experts Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Jia",
      "Qi Pang",
      "Trung Tran",
      "David Woodruff",
      "Zhihao Zhang",
      "Wenting Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5ab6f836f464d0f4e4f6aaa523249280-Abstract-Conference.html": {
    "title": "Déjà Vu Memorization in Vision–Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bargav Jayaraman",
      "Chuan Guo",
      "Kamalika Chaudhuri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5acb720a361eecb34ee62d356859d246-Abstract-Conference.html": {
    "title": "ARC: A Generalist Graph Anomaly Detector with In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Liu",
      "Shiyuan Li",
      "Yu Zheng",
      "Qingfeng Chen",
      "Chengqi Zhang",
      "Shirui Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5adff4d5402703418f7210a4004e1314-Abstract-Conference.html": {
    "title": "Exploiting Descriptive Completeness Prior for Cross Modal Hashing with Incomplete Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyang Luo",
      "Zheng Zhang",
      "Yadan Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5ae0f7cfd65d8e2b39da4177fef82015-Abstract-Conference.html": {
    "title": "Happy: A Debiased Learning Framework for Continual Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Ma",
      "Fei Zhu",
      "Zhun Zhong",
      "Wenzhuo Liu",
      "Xu-yao Zhang",
      "Cheng-lin Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5aea56eefab60e06f35016478e21aae6-Abstract-Conference.html": {
    "title": "Predicting Future Actions of Reinforcement Learning Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen Chung",
      "Scott Niekum",
      "David Krueger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5af741d487c5f0b08bfe56e11d1883e4-Abstract-Conference.html": {
    "title": "Exploring Token Pruning in Vision State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Zhan",
      "Zhenglun Kong",
      "Yifan Gong",
      "Yushu Wu",
      "Zichong Meng",
      "Hangyu Zheng",
      "Xuan Shen",
      "Stratis Ioannidis",
      "Wei Niu",
      "Pu Zhao",
      "Yanzhi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5b0c0b2c2efdd736a53688ebfdc3bcdb-Abstract-Conference.html": {
    "title": "ShowMaker: Creating High-Fidelity 2D Human Video via Fine-Grained Diffusion Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanwei Yang",
      "Jiazhi Guan",
      "Kaisiyuan Wang",
      "Lingyun Yu",
      "Wenqing Chu",
      "Hang Zhou",
      "ZhiQiang Feng",
      "Haocheng Feng",
      "Errui Ding",
      "Jingdong Wang",
      "Hongtao Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5b26b9e634ba10f6c51c6db7365c4c28-Abstract-Conference.html": {
    "title": "HEPrune: Fast Private Training of Deep Neural Networks With Encrypted Data Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yancheng Zhang",
      "Mengxin Zheng",
      "Yuzhang Shang",
      "Xun Chen",
      "Qian Lou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5b6346a05a537d4cdb2f50323452a9fe-Abstract-Conference.html": {
    "title": "Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqing Sun",
      "Longhui Yu",
      "Yikang Shen",
      "Weiyang Liu",
      "Yiming Yang",
      "Sean Welleck",
      "Chuang Gan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5b76d77e7095c6480ed827b85f0c2878-Abstract-Conference.html": {
    "title": "Fast TRAC: A Parameter-Free Optimizer for Lifelong Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aneesh Muppidi",
      "Zhiyu Zhang",
      "Heng Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5b93ce41ac6de2bf9aca7e4ba5ba01d5-Abstract-Conference.html": {
    "title": "Incentivizing Quality Text Generation via Statistical Contracts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eden Saig",
      "Ohad Einav",
      "Inbal Talgam-Cohen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5ba9ad2e7abefb68a7165396583038da-Abstract-Conference.html": {
    "title": "Practical $0.385$-Approximation for Submodular Maximization Subject to a Cardinality Constraint",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Morad Tukan",
      "Loay Mualem",
      "Moran Feldman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5bd9fbb3a5a985f80c16ddd0ec1dfc43-Abstract-Conference.html": {
    "title": "Fast Proxy Experiment Design for Causal Effect Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sepehr Elahi",
      "Sina Akbari",
      "Jalal Etesami",
      "Negar Kiyavash",
      "Patrick Thiran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5bed8703db85ab27dc32f6a42f8fbdb6-Abstract-Conference.html": {
    "title": "Dealing with Synthetic Data Contamination in Online Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maorong Wang",
      "Nicolas MICHEL",
      "Jiafeng Mao",
      "Toshihiko Yamasaki"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5bede447be3282f2459070475093fb76-Abstract-Conference.html": {
    "title": "Recognize Any Regions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haosen Yang",
      "Chuofan Ma",
      "Bin Wen",
      "Yi Jiang",
      "Zehuan Yuan",
      "Xiatian Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5bf2b802e24106064dc547ae9283bb0c-Abstract-Conference.html": {
    "title": "SAMPa: Sharpness-aware Minimization Parallelized",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanyun Xie",
      "Thomas Pethick",
      "Volkan Cevher"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c1170c249cd8e1bde5848a4fc10cb9a-Abstract-Conference.html": {
    "title": "Continuous Contrastive Learning for Long-Tailed Semi-Supervised Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi-Hao Zhou",
      "Siyuan Fang",
      "Zi-Jing Zhou",
      "Tong Wei",
      "Yuanyu Wan",
      "Min-Ling Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c14b3ee78d09e8b3240ffb1fb6cc819-Abstract-Conference.html": {
    "title": "Doubly Mild Generalization for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiu Mao",
      "Qi Wang",
      "Yun Qu",
      "Yuhang Jiang",
      "Xiangyang Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c186016d0844767209dc36e9e61441b-Abstract-Conference.html": {
    "title": "Is Mamba Compatible with Trajectory Optimization in Offline Reinforcement Learning?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Dai",
      "Oubo Ma",
      "Longfei Zhang",
      "Xingxing Liang",
      "Shengchao Hu",
      "Mengzhu Wang",
      "Shouling Ji",
      "Jincai Huang",
      "Li Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c20c00504e0c049ec2370d0cceaf3c4-Abstract-Conference.html": {
    "title": "Calibrated Self-Rewarding Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Zhou",
      "Zhiyuan Fan",
      "Dongjie Cheng",
      "Sihan Yang",
      "Zhaorun Chen",
      "Chenhang Cui",
      "Xiyao Wang",
      "Yun Li",
      "Linjun Zhang",
      "Huaxiu Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c20ca4b0b20b0bd2f1d839dc605e70f-Abstract-Conference.html": {
    "title": "ContextGS : Compact 3D Gaussian Splatting with Anchor Level Context Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Wang",
      "Zhihao Li",
      "Lanqing Guo",
      "Wenhan Yang",
      "Alex Kot",
      "Bihan Wen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c34c2a3462cbfd5e258c29974d60cca-Abstract-Conference.html": {
    "title": "Advancing Cross-domain Discriminability in Continual Learning of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yicheng Xu",
      "Yuxin Chen",
      "Jiahao Nie",
      "Yusong Wang",
      "HUIPING ZHUANG",
      "Manabu Okumura"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c4b6c3580d80e08565c8b66b3599c52-Abstract-Conference.html": {
    "title": "DG-SLAM: Robust Dynamic Gaussian Splatting SLAM with Hybrid Pose Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueming Xu",
      "Haochen Jiang",
      "Zhongyang Xiao",
      "Jianfeng Feng",
      "Li Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c4e0e38691e2aa08bba4cefc4c6e852-Abstract-Conference.html": {
    "title": "KOALA: Empirical Lessons Toward Memory-Efficient and Fast Diffusion Models for Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngwan Lee",
      "Kwanyong Park",
      "Yoorhim Cho",
      "Yong-Ju Lee",
      "Sung Ju Hwang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c54e016197805946481d786d80a662e-Abstract-Conference.html": {
    "title": "In Pursuit of Causal Label Correlations for Multi-label Image Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhao-Min Chen",
      "Xin Jin",
      "YisuGe",
      "Sixian Chan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c594bf6223b67109441c9e0c97542ed-Abstract-Conference.html": {
    "title": "NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Gong",
      "Guangyin Bao",
      "Qi Zhang",
      "Zhongwei Wan",
      "Duoqian Miao",
      "Shoujin Wang",
      "Lei Zhu",
      "Changwei Wang",
      "Rongtao Xu",
      "Liang Hu",
      "Ke Liu",
      "Yu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c6bcabf220f35df0b90f3d1d0ac1ad0-Abstract-Conference.html": {
    "title": "Active Classification with Few Queries under Misspecification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vasilis Kontonis",
      "Mingchen Ma",
      "Christos Tzamos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c6f928e3fc5f32ee29a1d916b68e6f5-Abstract-Conference.html": {
    "title": "EnOF-SNN: Training Accurate Spiking Neural Networks via Enhancing the Output Feature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Guo",
      "Weihang Peng",
      "Xiaode Liu",
      "Yuanpei Chen",
      "Yuhan Zhang",
      "Xin Tong",
      "Zhou Jie",
      "Zhe Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c7024041be305c94d7311cfcc53d93e-Abstract-Conference.html": {
    "title": "Why are Visually-Grounded Language Models Bad at Image Classification?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Zhang",
      "Alyssa Unell",
      "Xiaohan Wang",
      "Dhruba Ghosh",
      "Yuchang Su",
      "Ludwig Schmidt",
      "Serena Yeung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c7465e4a236b56226ca221f3d16bf2b-Abstract-Conference.html": {
    "title": "Association of Objects May Engender Stereotypes: Mitigating Association-Engendered Stereotypes in Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junlei Zhou",
      "Jiashi Gao",
      "Xiangyu Zhao",
      "Xin Yao",
      "Xuetao Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c7894ac8788555f1cecf536f1e0fd35-Abstract-Conference.html": {
    "title": "Stochastic Concept Bottleneck Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moritz Vandenhirtz",
      "Sonia Laguna",
      "Ričards Marcinkevičs",
      "Julia Vogt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c8168a8eca2eb23f6b1f5019371043e-Abstract-Conference.html": {
    "title": "Stealth edits to large language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oliver Sutton",
      "Qinghua Zhou",
      "Wei Wang",
      "Desmond Higham",
      "Alexander N Gorban",
      "Alexander Bastounis",
      "Ivan Tyukin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5c882988ce5fac487974ee4f415b96a9-Abstract-Conference.html": {
    "title": "Rejection via Learning Density Ratios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Soen",
      "Hisham Husain",
      "Philip Schulz",
      "Vu Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5cc19d8570ebe171ffad101fef1e4dde-Abstract-Conference.html": {
    "title": "The Many Faces of Optimal Weak-to-Strong Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikael Møller Høgsgaard",
      "Kasper Green Larsen",
      "Markus Engelund Mathiasen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5ce377d14a21ef1fea0400049ad324b4-Abstract-Conference.html": {
    "title": "Vision Mamba Mender",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacong Hu",
      "Anda Cao",
      "Zunlei Feng",
      "Shengxuming Zhang",
      "Yi Wang",
      "Lingxiang Jia",
      "Mingli Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5d1233f819202ade06023346df80a6d2-Abstract-Conference.html": {
    "title": "Online Non-convex Learning in Dynamic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipan Xu",
      "Lijun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5d1f02132ef51602adf07000ca5b6138-Abstract-Conference.html": {
    "title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Tao",
      "Yucheng Zhou",
      "Yanlin Wang",
      "Wenqiang Zhang",
      "Hongyu Zhang",
      "Yu Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5d2e24df9cfaad3189833b819c40b392-Abstract-Conference.html": {
    "title": "U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchuan Tian",
      "Zhijun Tu",
      "Hanting Chen",
      "Jie Hu",
      "Chao Xu",
      "Yunhe Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5d3b57e06e3fc45f077eb5c9f28156d4-Abstract-Conference.html": {
    "title": "Generalize or Detect? Towards Robust Semantic Segmentation Under Multiple Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhitong Gao",
      "Bingnan Li",
      "Mathieu Salzmann",
      "Xuming He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5d4f5a2de6320641566be8722d5f78dc-Abstract-Conference.html": {
    "title": "Equivariant spatio-hemispherical networks for diffusion MRI deconvolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Axel Elaldi",
      "Guido Gerig",
      "Neel Dey"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5d516fc09b53e9a7fade4fbad703e686-Abstract-Conference.html": {
    "title": "Pandora's Box: Towards Building Universal Attackers against Real-World Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daizong Liu",
      "Mingyu Yang",
      "Xiaoye Qu",
      "Pan Zhou",
      "Xiang Fang",
      "Keke Tang",
      "Yao Wan",
      "Lichao Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5d632bbfde3f580d8183dc48ed87b418-Abstract-Conference.html": {
    "title": "Generalization Bounds via Conditional $f$-Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqiao Wang",
      "Yongyi Mao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5d68a3f05ee2aae6a0fb2d94959082a0-Abstract-Conference.html": {
    "title": "Outlier-Robust Distributionally Robust Optimization via Unbalanced Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zifan Wang",
      "Yi Shen",
      "Michael Zavlanos",
      "Karl H. Johansson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5d7c739c8e0383d02bb0addf8f29cd19-Abstract-Conference.html": {
    "title": "Separate and Reconstruct: Asymmetric Encoder-Decoder for Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ui-Hyeop Shin",
      "Sangyoun Lee",
      "Taehan Kim",
      "Hyung-Min Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5ddcfaad1cb72ce6f1a365e8f1ecf791-Abstract-Conference.html": {
    "title": "DeiSAM: Segment Anything with Deictic Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hikaru Shindo",
      "Manuel Brack",
      "Gopika Sudhakaran",
      "Devendra S Dhami",
      "Patrick Schramowski",
      "Kristian Kersting"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5ddfb189c022a317ff1c72e6639079de-Abstract-Conference.html": {
    "title": "E2E-MFD: Towards End-to-End Synchronous Multimodal Fusion Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqing Zhang",
      "Mingxiang Cao",
      "Weiying Xie",
      "Jie Lei",
      "Daixun Li",
      "Wenbo Huang",
      "Yunsong Li",
      "Xue Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5de11e930c1bbfda5d4fc9d2b0924032-Abstract-Conference.html": {
    "title": "AdvAD: Exploring Non-Parametric Diffusion for Imperceptible Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Li",
      "Ziqiang He",
      "Anwei Luo",
      "Jian-Fang Hu",
      "Z. Jane Wang",
      "Xiangui Kang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5df1d92b478716877f774b82943477b3-Abstract-Conference.html": {
    "title": "REBEL: Reinforcement Learning via Regressing Relative Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaolin Gao",
      "Jonathan Chang",
      "Wenhao Zhan",
      "Owen Oertell",
      "Gokul Swamy",
      "Kianté Brantley",
      "Thorsten Joachims",
      "Drew Bagnell",
      "Jason Lee",
      "Wen Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5df4313ecd4875931fbdacc486cc1fcf-Abstract-Conference.html": {
    "title": "Predictive Attractor Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramy Mounir",
      "Sudeep Sarkar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5df5b1f121c915d8bdd00db6aac20827-Abstract-Conference.html": {
    "title": "Improving Adaptivity via Over-Parameterization in Sequence Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yicheng Li",
      "Qian Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5dfbe6f5671e82c76841ba687a8a9ecb-Abstract-Conference.html": {
    "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiqiang Jiang",
      "Yucheng LI",
      "Chengruidong Zhang",
      "Qianhui Wu",
      "Xufang Luo",
      "Surin Ahn",
      "Zhenhua Han",
      "Amir Abdi",
      "Dongsheng Li",
      "Chin-Yew Lin",
      "Yuqing Yang",
      "Lili Qiu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5e1c255653eb98cef13f45b2d337c882-Abstract-Conference.html": {
    "title": "Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sriyash Poddar",
      "Yanming Wan",
      "Hamish Ivison",
      "Abhishek Gupta",
      "Natasha Jaques"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5e2217482fa75556f1970be809acd3f8-Abstract-Conference.html": {
    "title": "Unveiling Encoder-Free Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiwen Diao",
      "Yufeng Cui",
      "Xiaotong Li",
      "Yueze Wang",
      "Huchuan Lu",
      "Xinlong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5e28c085a8cd881a4c7e5cf13391aac5-Abstract-Conference.html": {
    "title": "Density-based User Representation using Gaussian Process Regression for Multi-interest Personalized Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haolun Wu",
      "Ofer Meshi",
      "Masrour Zoghi",
      "Fernando Diaz",
      "Xue (Steve) Liu",
      "Craig Boutilier",
      "Maryam Karimzadehgan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5e364212327fa3a59ae3595b025c469f-Abstract-Conference.html": {
    "title": "Frequency-aware Generative Models for Multivariate Time Series Imputation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "XINYU YANG",
      "Yu Sun",
      "Yuan xiaojie",
      "Xinyang Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5e460d0374fea1e9f83febeb45a1d287-Abstract-Conference.html": {
    "title": "Overfitting Behaviour of Gaussian Kernel Ridgeless Regression: Varying Bandwidth or Dimensionality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marko Medvedev",
      "Gal Vardi",
      "Nati Srebro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5e5853f35164e434015716a8c2a66543-Abstract-Conference.html": {
    "title": "Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Tian",
      "Baolin Peng",
      "Linfeng Song",
      "Lifeng Jin",
      "Dian Yu",
      "Lei Han",
      "Haitao Mi",
      "Dong Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5e5d6f9ac33ba9349ba7b2be9f21bad9-Abstract-Conference.html": {
    "title": "A Theory of Optimistically Universal Online Learnability for General Concept Classes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steve Hanneke",
      "Hongao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5e5e93825f92192a701323f4e7105341-Abstract-Conference.html": {
    "title": "Belief-State Query Policies for User-Aligned POMDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Bramblett",
      "Siddharth Srivastava"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5e88ccc6d03f08e426de9bb918aa1bca-Abstract-Conference.html": {
    "title": "Adversarially Trained Weighted Actor-Critic for Safe Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honghao Wei",
      "Xiyue Peng",
      "Arnob Ghosh",
      "Xin Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5eaa54503005d9125ad6aa3044e912d8-Abstract-Conference.html": {
    "title": "Fast Iterative Hard Thresholding Methods with Pruning Gradient Computations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasutoshi Ida",
      "Sekitoshi Kanai",
      "Atsutoshi Kumagai",
      "Tomoharu Iwata",
      "Yasuhiro Fujiwara"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5eca2e4fe7858cbbfef4e08573cfcb25-Abstract-Conference.html": {
    "title": "InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sirui Xu",
      "ziyin wang",
      "Yu-Xiong Wang",
      "Liangyan Gui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5eceb48c3bc8b5d936c05ff8e2ece65e-Abstract-Conference.html": {
    "title": "Graph Convolutions Enrich the Self-Attention in Transformers!",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongwhan Choi",
      "Hyowon Wi",
      "Jayoung Kim",
      "Yehjin Shin",
      "Kookjin Lee",
      "Nathaniel Trask",
      "Noseong Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5edb0d52b6959d946afac7600f9f1e0c-Abstract-Conference.html": {
    "title": "FineStyle: Fine-grained Controllable Style Personalization for Text-to-image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gong Zhang",
      "Kihyuk Sohn",
      "Meera Hahn",
      "Humphrey Shi",
      "Irfan A. Essa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5ee553ec47c31e46a1209bb858b30aa5-Abstract-Conference.html": {
    "title": "Alias-Free Mamba Neural Operator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Zheng",
      "Wei Li",
      "Ni Xu",
      "Junwei Zhu",
      "XiaoxuLin",
      "Xiaoqin Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5ee7ed60a7e8169012224dec5fe0d27f-Abstract-Conference.html": {
    "title": "Guiding a Diffusion Model with a Bad Version of Itself",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tero Karras",
      "Miika Aittala",
      "Tuomas Kynkäänniemi",
      "Jaakko Lehtinen",
      "Timo Aila",
      "Samuli Laine"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5eeb693f46d753e5fe24c97212c22bd2-Abstract-Conference.html": {
    "title": "Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Oldfield",
      "Markos Georgopoulos",
      "Grigorios Chrysos",
      "Christos Tzelepis",
      "Yannis Panagakis",
      "Mihalis Nicolaou",
      "Jiankang Deng",
      "Ioannis Patras"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5f1b350fc0c2affd56f465faa36be343-Abstract-Conference.html": {
    "title": "Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Top Piriyakulkij",
      "Cassidy Langenfeld",
      "Tuan Anh Le",
      "Kevin Ellis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5f1eee2509599faeeb3570a887016a64-Abstract-Conference.html": {
    "title": "Understanding Emergent Abilities of Language Models from the Loss Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengxiao Du",
      "Aohan Zeng",
      "Yuxiao Dong",
      "Jie Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5f2f5882d6166d814629ada0cd95f9a0-Abstract-Conference.html": {
    "title": "G3: An Effective and Adaptive Framework for Worldwide Geolocalization Using Large Multi-Modality Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyue Jia",
      "Yiding Liu",
      "Xiaopeng Li",
      "Xiangyu Zhao",
      "Yuhao Wang",
      "Yantong Du",
      "Xiao Han",
      "Xuetao Wei",
      "Shuaiqiang Wang",
      "Dawei Yin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5f5f7b6080dcadced61cf5d96f7c6dde-Abstract-Conference.html": {
    "title": "Color-Oriented Redundancy Reduction in Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Yuan",
      "Zijian Wang",
      "Mahsa Baktashmotlagh",
      "Yadan Luo",
      "Zi Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5f940d94d83fe6ad374822312d5711a5-Abstract-Conference.html": {
    "title": "Euclidean distance compression via deep random features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brett Leroux",
      "Luis Rademacher"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5f96a21345c138da929e99871fda138e-Abstract-Conference.html": {
    "title": "LRM-Zero: Training Large Reconstruction Models with Synthesized Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Desai Xie",
      "Sai Bi",
      "Zhixin Shu",
      "Kai Zhang",
      "Zexiang Xu",
      "Yi Zhou",
      "Soeren Pirk",
      "Arie Kaufman",
      "Xin Sun",
      "Hao Tan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5f999632c48f87cffb214e575581e4a9-Abstract-Conference.html": {
    "title": "Uncertainty-aware Fine-tuning of Segmentation Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangning Liu",
      "Brian Price",
      "Jason Kuen",
      "Yifei Fan",
      "Zijun Wei",
      "Luis Figueroa",
      "Krzysztof Geras",
      "Carlos Fernandez-Granda"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5fba70900a84a8fb755c48ba99420c95-Abstract-Conference.html": {
    "title": "Learning via Surrogate PAC-Bayes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Picard",
      "Roman Moscoviz",
      "Benjamin Guedj"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5fcedec09977357f32e8e0ec8957073b-Abstract-Conference.html": {
    "title": "MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Lei",
      "Yi Zhang",
      "Shan Zuo",
      "Ali Payani",
      "Caiwen Ding"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5fd68e1c262099f846733435d620d574-Abstract-Conference.html": {
    "title": "ECMamba: Consolidating Selective State Space Model with Retinex Guidance for Efficient Multiple Exposure Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Dong",
      "Han Zhou",
      "Yulun Zhang",
      "Xiaohong Liu",
      "Jun Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/5fff164c04811174e1836dc3e66c0aba-Abstract-Conference.html": {
    "title": "Provable Tempered Overfitting of Minimal Nets and Typical Nets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Itamar Harel",
      "William Hoza",
      "Gal Vardi",
      "Itay Evron",
      "Nati Srebro",
      "Daniel Soudry"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6008facedac886858acdb76c09da2097-Abstract-Conference.html": {
    "title": "Dynamic Service Fee Pricing under Strategic Behavior: Actions as Instruments and Phase Transition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Ai",
      "David Simchi-Levi",
      "Feng Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6034a661584af6c28fd97a6f23e56c0a-Abstract-Conference.html": {
    "title": "Autoformalize Mathematical Statements by Symbolic Equivalence and Semantic Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zenan Li",
      "Yifan Wu",
      "Zhaoyu Li",
      "Xinming Wei",
      "Xian Zhang",
      "Fan Yang",
      "Xiaoxing Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6075fc6540b9a3cb951752099efd86ef-Abstract-Conference.html": {
    "title": "Only Strict Saddles in the Energy Landscape of Predictive Coding Networks?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Innocenti",
      "El Mehdi Achour",
      "Ryan Singh",
      "Christopher L Buckley"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6087a60306544be7ba0d0cf34aa93c8f-Abstract-Conference.html": {
    "title": "Unelicitable Backdoors via Cryptographic Transformer Circuits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andis Draguns",
      "Andrew Gritsevskiy",
      "Sumeet Motwani",
      "Christian Schroeder de Witt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/608fe7e32f7b773545cc1d656a0fdc98-Abstract-Conference.html": {
    "title": "Accelerating Greedy Coordinate Gradient and General Prompt Optimization via Probe Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiran Zhao",
      "Wenyue Zheng",
      "Tianle Cai",
      "Do Xuan Long",
      "Kenji Kawaguchi",
      "Anirudh Goyal",
      "Michael Qizhe Shieh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6091bf1542b118287db4088bc16be8d9-Abstract-Conference.html": {
    "title": "Flow Snapshot Neurons in Action: Deep Neural Networks Generalize to Biological Motion Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuangpeng Han",
      "Ziyu Wang",
      "Mengmi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/60960ad78868fce5c165295fbd895060-Abstract-Conference.html": {
    "title": "WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Wang",
      "Zexi Li",
      "Ningyu Zhang",
      "Ziwen Xu",
      "Yunzhi Yao",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Huajun Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/60dc7fa827f5f761ad481e2ad40b5573-Abstract-Conference.html": {
    "title": "Harnessing Multiple Correlated Networks for Exact Community Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miklos Z. Racz",
      "Jifan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/60f81431bdf32f139438c69787e12b3f-Abstract-Conference.html": {
    "title": "Active Learning for Derivative-Based Global Sensitivity Analysis with Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Syrine Belakaria",
      "Ben Letham",
      "Jana Doppa",
      "Barbara Engelhardt",
      "Stefano Ermon",
      "Eytan Bakshy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/60fb8cf8000f0386063fb24ead366330-Abstract-Conference.html": {
    "title": "FlexSBDD: Structure-Based Drug Design with Flexible Protein Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZAIXI ZHANG",
      "Mengdi Wang",
      "Qi Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6111371a868af8dcfba0f96ad9e25ae3-Abstract-Conference.html": {
    "title": "Diffusion-based Reinforcement Learning via Q-weighted Variational Policy Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shutong Ding",
      "Ke Hu",
      "Zhenhao Zhang",
      "Kan Ren",
      "Weinan Zhang",
      "Jingyi Yu",
      "Jingya Wang",
      "Ye Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/611e84703eac7cc03f78339df8aae2ed-Abstract-Conference.html": {
    "title": "EAI: Emotional Decision-Making of LLMs in Strategic Games and Ethical Dilemmas",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikhail Mozikov",
      "Nikita Severin",
      "Valeria Bodishtianu",
      "Maria Glushanina",
      "Ivan Nasonov",
      "Daniil Orekhov",
      "Pekhotin Vladislav",
      "Ivan Makovetskiy",
      "Mikhail Baklashkin",
      "Vasily Lavrentyev",
      "Akim Tsvigun",
      "Denis Turdakov",
      "Tatiana Shavrina",
      "Andrey Savchenko",
      "Ilya Makarov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/61355b9c218505505d1bedede9da56b2-Abstract-Conference.html": {
    "title": "Denoising Diffusion Path: Attribution Noise Reduction with An Auxiliary Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Lei",
      "Zilong Li",
      "Junping Zhang",
      "Hongming Shan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/614f8eba720cfc7ff00274bd64fb0a3f-Abstract-Conference.html": {
    "title": "Transformer Doctor: Diagnosing and Treating Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacong Hu",
      "Hao Chen",
      "Kejia Chen",
      "Yang Gao",
      "Jingwen Ye",
      "Xingen Wang",
      "Mingli Song",
      "Zunlei Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/615675cc6e94ddb1a783904fb178b5f6-Abstract-Conference.html": {
    "title": "StepbaQ: Stepping backward as Correction for Quantized Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Chung Chen",
      "Zhi-Kai Huang",
      "Jing-Ren Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6158e152498f8d8b83d14388a7ec1963-Abstract-Conference.html": {
    "title": "Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Hoon Lee",
      "Seunghoon Hong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6164b6e5352c139e9ddc1a98c09e4e4a-Abstract-Conference.html": {
    "title": "PIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaidong Zhang",
      "Pengzhen Ren",
      "Bingqian Lin",
      "Junfan Lin",
      "Shikui Ma",
      "Hang Xu",
      "Xiaodan Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/616521c3cf15f9f7018565c427d40e3b-Abstract-Conference.html": {
    "title": "A Foundation Model for Zero-shot Logical Query Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Galkin",
      "Jincheng Zhou",
      "Bruno Ribeiro",
      "Jian Tang",
      "Zhaocheng Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/61674667d642ae52f6bb281bea90ee29-Abstract-Conference.html": {
    "title": "LSH-MoE: Communication-efficient MoE Training via Locality-Sensitive Hashing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaonan Nie",
      "Liu Qibin",
      "Fangcheng Fu",
      "Shenhan Zhu",
      "Xupeng Miao",
      "Xiaoyang Li",
      "Yang Zhang",
      "Shouda Liu",
      "Bin CUI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6174c67b136621f3f2e4a6b1d3286f6b-Abstract-Conference.html": {
    "title": "Diffusion Actor-Critic with Entropy Regulator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinuo Wang",
      "Likun Wang",
      "Yuxuan Jiang",
      "Wenjun Zou",
      "Tong Liu",
      "Xujie Song",
      "Wenxuan Wang",
      "Liming Xiao",
      "Jiang Wu",
      "Jingliang Duan",
      "Shengbo Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/617ffb01ea5b57769b0d63d5e9fefd3f-Abstract-Conference.html": {
    "title": "Can LLMs Implicitly Learn Numeric Parameter Constraints in Data Science APIs?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinlin Deng",
      "Chunqiu Steven Xia",
      "Zhezhen Cao",
      "Meiziniu Li",
      "LINGMING ZHANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/618c8af8efd19b4ce90b8864a764d0fa-Abstract-Conference.html": {
    "title": "Freya PAGE: First Optimal Time Complexity for Large-Scale Nonconvex Finite-Sum Optimization with Heterogeneous Asynchronous Computations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Tyurin",
      "Kaja Gruntkowska",
      "Peter Richtarik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/619b8e3ead58dce90bc615f2a7d5d102-Abstract-Conference.html": {
    "title": "Multivariate Probabilistic Time Series Forecasting with Correlated Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Zhihao Zheng",
      "Lijun Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/61a18c8a7a1ea7445375dd7255905bc3-Abstract-Conference.html": {
    "title": "Variational Delayed Policy Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyuan Wu",
      "Simon Zhan",
      "Yixuan Wang",
      "Yuhui Wang",
      "Chung-Wei Lin",
      "Chen Lv",
      "Qi Zhu",
      "Chao Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/61a8faabae5704ce6f095e89856d7c79-Abstract-Conference.html": {
    "title": "MILP-StuDio: MILP Instance Generation via Block Structure Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyang Liu",
      "Jie Wang",
      "Wanbo Zhang",
      "Zijie Geng",
      "Yufei Kuang",
      "Xijun Li",
      "Bin Li",
      "Yongdong Zhang",
      "Feng Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/61c00c07e6d27285e4b952e96cc65666-Abstract-Conference.html": {
    "title": "BrainBits: How Much of the Brain are Generative Reconstruction Methods Using?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Mayo",
      "Christopher Wang",
      "Asa Harbin",
      "Abdulrahman Alabdulkareem",
      "Albert Shaw",
      "Boris Katz",
      "Andrei Barbu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/61caa89f7a5366023db6f5736b2c579d-Abstract-Conference.html": {
    "title": "Q-Distribution guided Q-learning for offline reinforcement learning: Uncertainty penalized Q-value via consistency model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Zhang",
      "Linjiajie Fang",
      "Kexin SHI",
      "Wenjia Wang",
      "Bingyi Jing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/61dad41cb106aef1154a82965f42df0e-Abstract-Conference.html": {
    "title": "ChatCam: Empowering Camera Control through Conversational AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhang Liu",
      "Yu-Wing Tai",
      "Chi-Keung Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/61f425da6e0a201b8fe1454601abfba5-Abstract-Conference.html": {
    "title": "What Rotary Position Embedding Can Tell Us: Identifying Query and Key Weights Corresponding to Basic Syntactic or High-level Semantic Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiting Chen",
      "Junchi Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/61f9cc83f684a4979929804ba8abf027-Abstract-Conference.html": {
    "title": "EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Yang",
      "Wei Zhai",
      "Chengfeng Wang",
      "Chengjun Yu",
      "Yang Cao",
      "Zheng-Jun Zha"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/62133b95efaa6f10a5395eb10ca087cd-Abstract-Conference.html": {
    "title": "NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao-Lun Sun",
      "Lei Hsiung",
      "Nandhini Chandramoorthy",
      "Pin-Yu Chen",
      "Tsung-Yi Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6216515a5e0b3257c49dcb1647e497d1-Abstract-Conference.html": {
    "title": "Dissecting Query-Key Interaction in Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Pan",
      "Aaron Philip",
      "Ziqian Xie",
      "Odelia Schwartz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/62203a74e233e933b160711e791e1a02-Abstract-Conference.html": {
    "title": "PEAC: Unsupervised Pre-training for Cross-Embodiment Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyang Ying",
      "Hao Zhongkai",
      "Xinning Zhou",
      "Xuezhou Xu",
      "Hang Su",
      "Xingxing Zhang",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6225ca7eaef477a05d01e90b0615ab4f-Abstract-Conference.html": {
    "title": "Idiographic Personality Gaussian Process for Psychological Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yehu Chen",
      "Muchen Xi",
      "Joshua Jackson",
      "Jacob Montgomery",
      "Roman Garnett"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6246e04dcf42baf7c71e3a65d3d93b55-Abstract-Conference.html": {
    "title": "pFedClub: Controllable Heterogeneous Model Aggregation for Personalized Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Wang",
      "Qi Li",
      "Lingjuan Lyu",
      "Fenglong Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/626ab938fe19200324b368f5ee816868-Abstract-Conference.html": {
    "title": "Efficient Minimum Bayes Risk Decoding using Low-Rank Matrix Completion Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Firas Trabelsi",
      "David Vilar",
      "Mara Finkelstein",
      "Markus Freitag"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6294e6ec40d3599dcc9e9d584d959f5c-Abstract-Conference.html": {
    "title": "Taming the Long Tail in Human Mobility Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohang Xu",
      "Renhe Jiang",
      "Chuang Yang",
      "zipei fan",
      "Kaoru Sezaki"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/62a58f2130894e44e8a272c563a2c6f1-Abstract-Conference.html": {
    "title": "Categorical Flow Matching on Statistical Manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoran Cheng",
      "Jiahan Li",
      "Jian Peng",
      "Ge Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/62a9c80248963f348778a9c0bec060dd-Abstract-Conference.html": {
    "title": "How does Inverse RL Scale to Large State Spaces? A Provably Efficient Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filippo Lazzati",
      "Mirco Mutti",
      "Alberto Maria Metelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/62ab1c2cb4b03e717005479efb211841-Abstract-Conference.html": {
    "title": "Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiguang Chen",
      "Libo Qin",
      "Jiaqi Wang",
      "Jingxuan Zhou",
      "Wanxiang Che"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/62c4718cc334f6a0a62fb81c4a2095a1-Abstract-Conference.html": {
    "title": "LoRA-GA: Low-Rank Adaptation with Gradient Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaowen Wang",
      "Linxi Yu",
      "Jian Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/62e5f22b5dae99ec700be622df4fbe0d-Abstract-Conference.html": {
    "title": "DALD: Improving Logits-based Detector without Logits from Black-box LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Zeng",
      "Shengkun Tang",
      "Xianjun Yang",
      "Yuanzhou Chen",
      "Yiyou Sun",
      "Zhiqiang Xu",
      "Yao Li",
      "Haifeng Chen",
      "Wei Cheng",
      "Dongkuan (DK) Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/62ffefbe8dfc8548d22564f3c1d21488-Abstract-Conference.html": {
    "title": "Free-Rider and Conflict Aware Collaboration Formation for Cross-Silo Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengmeng Chen",
      "Xiaohu Wu",
      "Xiaoli Tang",
      "Tiantian He",
      "Yew Soon Ong",
      "QIQI LIU",
      "Qicheng Lao",
      "Han Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/630d25bf5c0862dc1c49ba2096cedc2d-Abstract-Conference.html": {
    "title": "Distributionally Robust Performative Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songkai Xue",
      "Yuekai Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/630d293833e09e1ecd892a898a20b074-Abstract-Conference.html": {
    "title": "Abrupt Learning in Transformers: A Case Study on Matrix Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pulkit Gopalani",
      "Ekdeep S Lubana",
      "Wei Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/631bb9434d718ea309af82566347d607-Abstract-Conference.html": {
    "title": "Pre-Trained Multi-Goal Transformers with Prompt Optimization for Efficient Online Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoqi Yuan",
      "Yuhui Fu",
      "Feiyang Xie",
      "Zongqing Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6323d96f79d5d49e0d3fc88835c082cd-Abstract-Conference.html": {
    "title": "FilterNet: Harnessing Frequency Filters for Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Yi",
      "Jingru Fei",
      "Qi Zhang",
      "Hui He",
      "Shufeng Hao",
      "Defu Lian",
      "Wei Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/633780c1344d0c95e4d2dd3431fe08d9-Abstract-Conference.html": {
    "title": "Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benyuan Meng",
      "Qianqian Xu",
      "Zitai Wang",
      "Xiaochun Cao",
      "Qingming Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/633b0e871a48d542280c3ad03928e60d-Abstract-Conference.html": {
    "title": "Estimating Ego-Body Pose from Doubly Sparse Egocentric Video Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunggeun Chi",
      "Pin-Hao Huang",
      "Enna Sachdeva",
      "Hengbo Ma",
      "Karthik Ramani",
      "Kwonjoon Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/63460cef7f8e76fb2479c2f1500ecbfb-Abstract-Conference.html": {
    "title": "DECRL: A Deep Evolutionary Clustering Jointed Temporal Knowledge Graph Representation Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Chen",
      "Ling Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/637b1d029aa1a626fe6d4734d8a98869-Abstract-Conference.html": {
    "title": "Single Image Reflection Separation via Dual-Stream Interactive Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiming Hu",
      "Hainuo Wang",
      "Xiaojie Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/639d992f819c2b40387d4d5170b8ffd7-Abstract-Conference.html": {
    "title": "Recursive Introspection: Teaching Language Model Agents How to Self-Improve",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiao Qu",
      "Tianjun Zhang",
      "Naman Garg",
      "Aviral Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/63b2b056f48653b7cff0d8d233c96a4d-Abstract-Conference.html": {
    "title": "START: A Generalized State Space Model with Saliency-Driven Token-Aware Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jintao Guo",
      "Lei Qi",
      "Yinghuan Shi",
      "Yang Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/63ba665e01f39233674426ba36d6e177-Abstract-Conference.html": {
    "title": "When does perceptual alignment benefit vision representations?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shobhita Sundaram",
      "Stephanie Fu",
      "Lukas Muttenthaler",
      "Netanel Tamir",
      "Lucy Chai",
      "Simon Kornblith",
      "Trevor Darrell",
      "Phillip Isola"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/63e8bc7bbf1cfea36d1d1b6538aecce5-Abstract-Conference.html": {
    "title": "Towards Understanding the Working Mechanism of Text-to-Image Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyang Yi",
      "Aoxue Li",
      "Yi Xin",
      "Zhenguo Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6415b5dc9aa4bf4e6404cb221a109ec7-Abstract-Conference.html": {
    "title": "Inevitable Trade-off between Watermark Strength and Speculative Sampling Efficiency for Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengmian Hu",
      "Heng Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/641ce6c0e22483f34cd58625fcc7630e-Abstract-Conference.html": {
    "title": "MoTE: Reconciling Generalization with Specialization for Visual-Language to Video Knowledge Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghao Zhu",
      "Zhengpu Wang",
      "Mengxian Hu",
      "Ronghao Dang",
      "Xiao Lin",
      "Xun Zhou",
      "Chengju Liu",
      "Qijun Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6452474601429509f3035dc81c233226-Abstract-Conference.html": {
    "title": "DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuang Ai",
      "Xiaoqiang Zhou",
      "Huaibo Huang",
      "Xiaotian Han",
      "Zhengyu Chen",
      "Quanzeng You",
      "Hongxia Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6454dcd80b5373daaa97e53ce32c78a1-Abstract-Conference.html": {
    "title": "Revisiting Differentially Private ReLU Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Ding",
      "Mingxi Lei",
      "Liyang Zhu",
      "Shaowei Wang",
      "Di Wang",
      "Jinhui Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6462073c6bdf864ebfbbb11e80619f3e-Abstract-Conference.html": {
    "title": "Empowering Active Learning for 3D Molecular Graphs with Geometric Graph Isomorphism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronast Subedi",
      "Lu Wei",
      "Wenhan Gao",
      "Shayok Chakraborty",
      "Yi Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/646d2edf873df99d36aaeeaf058acdb8-Abstract-Conference.html": {
    "title": "Dual Lagrangian Learning for Conic Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathieu Tanneau",
      "Pascal Van Hentenryck"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/647af5f6b2538524f6c047c1d9170fd9-Abstract-Conference.html": {
    "title": "Selective Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Monteiro Paes",
      "Dennis Wei",
      "Flavio Calmon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/648a5a590ca6f2bb5de53f938e230160-Abstract-Conference.html": {
    "title": "Cascade of phase transitions in the training of energy-based models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitrios Bachtis",
      "Giulio Biroli",
      "Aurélien Decelle",
      "Beatriz Seoane"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/649ad92e7067b3553a0f15acac68806d-Abstract-Conference.html": {
    "title": "WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghan Jia",
      "Jiancheng Liu",
      "Yihua Zhang",
      "Parikshit Ram",
      "Nathalie Baracaldo",
      "Sijia Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/649f080d8891ab4d4b262cb9cd52e69a-Abstract-Conference.html": {
    "title": "Shuffling Gradient-Based Methods for Nonconvex-Concave Minimax Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quoc Tran Dinh",
      "Trang H. Tran",
      "Lam Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/64e2449d74f84e5b1a5c96ba7b3d308e-Abstract-Conference.html": {
    "title": "Preferential Normalizing Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Petrus Mikkola",
      "Luigi Acerbi",
      "Arto Klami"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/64ff8d0bf0b0fe2b872a42a0de9668f8-Abstract-Conference.html": {
    "title": "SceneDiffuser: Efficient and Controllable Driving Simulation Initialization and Rollout",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Jiang",
      "Yijing Bai",
      "Andre Cornman",
      "Christopher Davis",
      "XIUKUN HUANG",
      "Hong Jeon",
      "Sakshum Kulshrestha",
      "John Lambert",
      "Shuangyu Li",
      "Xuanyu Zhou",
      "Carlos Fuertes",
      "Chang Yuan",
      "Mingxing Tan",
      "Yin Zhou",
      "Dragomir Anguelov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6506964d22ede4d36adae956e6a9919a-Abstract-Conference.html": {
    "title": "GeoLRM: Geometry-Aware Large Reconstruction Model for High-Quality 3D Gaussian Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chubin Zhang",
      "Hongliang Song",
      "Yi Wei",
      "Chen Yu",
      "Jiwen Lu",
      "Yansong Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6521937507d78f327cd402401be73bf2-Abstract-Conference.html": {
    "title": "CryoSPIN: Improving Ab-Initio Cryo-EM Reconstruction with Semi-Amortized Pose Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shayan Shekarforoush",
      "David Lindell",
      "Marcus A Brubaker",
      "David J Fleet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/652efa5ba15928e48577f76a786faf27-Abstract-Conference.html": {
    "title": "Disentangled Style Domain for Implicit $z$-Watermark Towards Copyright Protection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junqiang Huang",
      "Zhaojun Guo",
      "Ge Luo",
      "Zhenxing Qian",
      "Sheng Li",
      "Xinpeng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/654f61ecd998c9095d30d42c03b832aa-Abstract-Conference.html": {
    "title": "EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinqian Lei",
      "Bo Wang",
      "Robby Tan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/656678aa961a99a6a3d59bfbf88daf77-Abstract-Conference.html": {
    "title": "Near-Optimal Dynamic Regret for Adversarial Linear Mixture MDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long-Fei Li",
      "Peng Zhao",
      "Zhi-Hua Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/657ce8a0bb63ef46a81a76bfc82d261a-Abstract-Conference.html": {
    "title": "Online Classification with Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinod Raman",
      "Ambuj Tewari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6599417a0b34d6ab7836cf86f8dd138c-Abstract-Conference.html": {
    "title": "QWO: Speeding Up Permutation-Based Causal Discovery in LiGAMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Shahverdikondori",
      "Ehsan Mokhtarian",
      "Negar Kiyavash"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/65a39213d7d0e1eb5d192aa77e77eeb7-Abstract-Conference.html": {
    "title": "Towards Robust Multimodal Sentiment Analysis with Incomplete Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Zhang",
      "Wenbin Wang",
      "Tianshu Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/65a723bf7d8dad838c09178270d30e80-Abstract-Conference.html": {
    "title": "Era3D: High-Resolution Multiview Diffusion using Efficient Row-wise Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Li",
      "Yuan Liu",
      "Xiaoxiao Long",
      "Feihu Zhang",
      "Cheng Lin",
      "Mengfei Li",
      "Xingqun Qi",
      "Shanghang Zhang",
      "Wei Xue",
      "Wenhan Luo",
      "Ping Tan",
      "Wenping Wang",
      "Qifeng Liu",
      "Yike Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/65ae674df2fb642518ae8d2b5435e1b8-Abstract-Conference.html": {
    "title": "Piecewise-Stationary Bandits with Knapsacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xilin Zhang",
      "Wang Chi Cheung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/65ec8f5cbc66b6a192c40a3ef1c05702-Abstract-Conference.html": {
    "title": "Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Zhou",
      "Xuandong Zhao",
      "Xiaolin Xu",
      "Shaolei Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/65f54fdf62cd5614dc5715ae7ece4ef6-Abstract-Conference.html": {
    "title": "Deep Homomorphism Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takanori Maehara",
      "Hoang NT"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/660cf2a1eabe448920a3ab6754555adb-Abstract-Conference.html": {
    "title": "When to Act and When to Ask: Policy Learning With Deferral Under Hidden Confounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marah Ghoummaid",
      "Uri Shalit"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/661c37f3b098bdee53fd7d9c4ef6964a-Abstract-Conference.html": {
    "title": "Maximum Entropy Reinforcement Learning via Energy-Based Normalizing Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Hao Chao",
      "Chien Feng",
      "Wei-Fang Sun",
      "Cheng-Kuang Lee",
      "Simon See",
      "Chun-Yi Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/661de50100d3115cf4317bb8b5219e56-Abstract-Conference.html": {
    "title": "FasterDiT: Towards Faster Diffusion Transformers Training without Architecture Modification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JINGFENG YAO",
      "Cheng Wang",
      "Wenyu Liu",
      "Xinggang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/662dcc5c2b9aca77b2a0ec8a98aefae9-Abstract-Conference.html": {
    "title": "Large Pre-trained time series models for cross-domain Time series analysis tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harshavardhan Prabhakar Kamarthi",
      "B. Aditya Prakash"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/663bce02a0050c4a11f1eb8a7f1429d3-Abstract-Conference.html": {
    "title": "Iteratively Refined Behavior Regularization for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Ma",
      "Jianye Hao",
      "Xiaohan Hu",
      "YAN ZHENG",
      "Chenjun Xiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/66453d578afae006252d2ea090e151c9-Abstract-Conference.html": {
    "title": "Rethinking LLM Memorization through the Lens of Adversarial Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avi Schwarzschild",
      "Zhili Feng",
      "Pratyush Maini",
      "Zachary Lipton",
      "J. Zico Kolter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/665654759cdf2114c0cbe2b8e501e00e-Abstract-Conference.html": {
    "title": "Learning in Markov Games with Adaptive Adversaries: Policy Regret, Fundamental Barriers, and Efficient Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thanh Nguyen-Tang",
      "Raman Arora"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/665bb142d4b9f55660cb89bb56a66fe1-Abstract-Conference.html": {
    "title": "BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qizhen (Irene) Zhang",
      "Nikolas Gritsch",
      "Dwaraknath Gnaneshwar Talupuru",
      "Simon Guo",
      "David Cairuz",
      "Bharat Venkitesh",
      "Jakob Foerster",
      "Phil Blunsom",
      "Sebastian Ruder",
      "Ahmet Üstün",
      "Acyr Locatelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/666e5e1df2d04dbe2b545ea3a3e3f7d3-Abstract-Conference.html": {
    "title": "Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geng Yu",
      "Jianing Zhu",
      "Jiangchao Yao",
      "Bo Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/66944d3a3e77ebe366793f6a6126f3a4-Abstract-Conference.html": {
    "title": "An Efficient Recipe for Long Context Extension via Middle-Focused Positional Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Wu",
      "Yanpeng Zhao",
      "Zilong Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/669b0bb64ff9eff7a81a7858a54fe7a0-Abstract-Conference.html": {
    "title": "Precipitation Downscaling with Spatiotemporal Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prakhar Srivastava",
      "Ruihan Yang",
      "Gavin Kerrigan",
      "Gideon Dresdner",
      "Jeremy McGibbon",
      "Christopher S. Bretherton",
      "Stephan Mandt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/66b35d2e8d524706f39cc21f5337b002-Abstract-Conference.html": {
    "title": "Diversity Is Not All You Need: Training A Robust Cooperative Agent Needs Specialist Partners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rujikorn Charakorn",
      "Poramate Manoonpong",
      "Nat Dilokthanakul"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/66e226469f20625aaebddbe47f0ca997-Abstract-Conference.html": {
    "title": "Autoregressive Image Generation without Vector Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhong Li",
      "Yonglong Tian",
      "He Li",
      "Mingyang Deng",
      "Kaiming He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/66f09010d989c83faeeac2617464b6a4-Abstract-Conference.html": {
    "title": "Clustering with Non-adaptive Subset Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadley Black",
      "Euiwoong Lee",
      "Arya Mazumdar",
      "Barna Saha"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6701e9c94bc7c7d6b5fc47c0fc13ab5b-Abstract-Conference.html": {
    "title": "What Variables Affect Out-of-Distribution Generalization in Pretrained Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Yousuf Harun",
      "Kyungbok Lee",
      "Gianmarco Gallardo",
      "Giri Krishnan",
      "Christopher Kanan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/673b9f8cba4ee9f15b88656a69761631-Abstract-Conference.html": {
    "title": "Bayesian Adaptive Calibration and Optimal Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rafael Oliveira",
      "Dino Sejdinovic",
      "David Howard",
      "Edwin V. Bonilla"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/677e39f949e8b92065957a8976345aba-Abstract-Conference.html": {
    "title": "Deep Submodular Peripteral Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gantavya Bhatt",
      "Arnav Das",
      "Jeff A Bilmes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6782c18960808848174cfe60742b415a-Abstract-Conference.html": {
    "title": "Parametric model reduction of mean-field and stochastic systems via higher-order action matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jules Berman",
      "Tobias Blickhan",
      "Benjamin Peherstorfer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/67b0e7c7c2a5780aeefe3b79caac106e-Abstract-Conference.html": {
    "title": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yue",
      "Yulin Wang",
      "Bingyi Kang",
      "Yizeng Han",
      "Shenzhi Wang",
      "Shiji Song",
      "Jiashi Feng",
      "Gao Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/67b2e2e895380fa6acd537c2894e490e-Abstract-Conference.html": {
    "title": "Can Simple Averaging Defeat Modern Watermarks?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei Yang",
      "Hai Ci",
      "Yiren Song",
      "Mike Zheng Shou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/67b31ca159553d5593e62d7b998d63ea-Abstract-Conference.html": {
    "title": "Causal language modeling can elicit search and reasoning capabilities on logic puzzles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kulin Shah",
      "Nishanth Dikkala",
      "Xin Wang",
      "Rina Panigrahy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/67c15da4a9340140c60783d9a175fd3f-Abstract-Conference.html": {
    "title": "Local and Adaptive Mirror Descents in Extensive-Form Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Côme Fiegel",
      "Pierre Ménard",
      "Tadashi Kozuno",
      "Remi Munos",
      "Vianney Perchet",
      "Michal Valko"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/67e79c8e9b11f068a7cafd79505175c0-Abstract-Conference.html": {
    "title": "Single-Loop Stochastic Algorithms for Difference of Max-Structured Weakly Convex Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanqi Hu",
      "Qi Qi",
      "Zhaosong Lu",
      "Tianbao Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/67ea314d1df751bbf99ab664ae3049a5-Abstract-Conference.html": {
    "title": "GTA: Generative Trajectory Augmentation with Guidance for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewoo Lee",
      "Sujin Yun",
      "Taeyoung Yun",
      "Jinkyoo Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6801fa3fd290229efc490ee0cf1c5687-Abstract-Conference.html": {
    "title": "UniAudio 1.5: Large Language Model-Driven Audio Codec is A Few-Shot Audio Task Learner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongchao Yang",
      "Haohan Guo",
      "Yuanyuan Wang",
      "Rongjie Huang",
      "Xiang Li",
      "Xu Tan",
      "Xixin Wu",
      "Helen Meng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6808f2c57d9564a2639a4710e3bbd9b9-Abstract-Conference.html": {
    "title": "L4GM: Large 4D Gaussian Reconstruction Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Ren",
      "Cheng Xie",
      "Ashkan Mirzaei",
      "hanxue liang",
      "xiaohui zeng",
      "Karsten Kreis",
      "Ziwei Liu",
      "Antonio Torralba",
      "Sanja Fidler",
      "Seung Wook Kim",
      "Huan Ling"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/680b2a8135b9c71278a09cafb605869e-Abstract-Conference.html": {
    "title": "One for All: Multi-Domain Joint Training for Point Cloud Based 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Wang",
      "Ya-Li Li",
      "Hengshuang Zhao",
      "Shengjin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/680da2fd0331deecc2e5b7cf0e55e832-Abstract-Conference.html": {
    "title": "Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanpeng Ye",
      "Jie Ren",
      "Shaozhou Wang",
      "Yuwei Wan",
      "Imran Razzak",
      "Bram Hoex",
      "Haofen Wang",
      "Tong Xie",
      "Wenjie Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6828259348d99d5e8994028bfdf15d09-Abstract-Conference.html": {
    "title": "Contrasting with Symile: Simple Model-Agnostic Representation Learning for Unlimited Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adriel Saporta",
      "Aahlad Manas Puli",
      "Mark Goldstein",
      "Rajesh Ranganath"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/684c59d614fe6ae74a3be8c3ef07e061-Abstract-Conference.html": {
    "title": "Spectral Editing of Activations for Large Language Model Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifu QIU",
      "Zheng Zhao",
      "Yftah Ziser",
      "Anna Korhonen",
      "Edoardo Maria Ponti",
      "Shay B. Cohen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6860d98b7ba3f331eaf03f5199c2b0be-Abstract-Conference.html": {
    "title": "Mixture of neural fields for heterogeneous reconstruction in cryo-EM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Axel Levy",
      "Rishwanth Raghu",
      "David Shustin",
      "Adele Peng",
      "Huan Li",
      "Oliver Clarke",
      "Gordon Wetzstein",
      "Ellen Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/687163285b8affc8ee933bdca8e75747-Abstract-Conference.html": {
    "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Pan",
      "Xiang Liu",
      "SHIZHE DIAO",
      "Renjie Pi",
      "Jipeng Zhang",
      "Chi Han",
      "Tong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6882dbdc34bcd094e6f858c06ce30edb-Abstract-Conference.html": {
    "title": "ODGS: 3D Scene Reconstruction from Omnidirectional Images with 3D Gaussian Splattings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suyoung Lee",
      "Jaeyoung Chung",
      "Jaeyoo Huh",
      "Kyoung Mu Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/688c7a82e31653e7c256c6c29fd3b438-Abstract-Conference.html": {
    "title": "Geometric-Averaged Preference Optimization for Soft Preference Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroki Furuta",
      "Kuang-Huei Lee",
      "Shixiang (Shane) Gu",
      "Yutaka Matsuo",
      "Aleksandra Faust",
      "Heiga Zen",
      "Izzeddin Gur"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/688ffe062732aabd87dfe57bcb0bf3ae-Abstract-Conference.html": {
    "title": "Magnet: We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyi Zhuang",
      "Ying Hu",
      "Pan Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/68a3919db3858f548dea769f2dbba611-Abstract-Conference.html": {
    "title": "Enhancing Graph Transformers with Hierarchical Distance Structural Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuankai Luo",
      "Hongkang Li",
      "Lei Shi",
      "Xiao-Ming Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/68b8d2bc77268facfc75a78782da9559-Abstract-Conference.html": {
    "title": "Graph Neural Flows for Unveiling Systemic Interactions Among Irregularly Sampled Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giangiacomo Mercatali",
      "Andre Freitas",
      "Jie Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/68bad5506f0f9eea7ae75f01ae00d5e2-Abstract-Conference.html": {
    "title": "Vitron: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Fei",
      "Shengqiong Wu",
      "Hanwang Zhang",
      "Tat-Seng Chua",
      "Shuicheng Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/692ae28fda9bfbde7c01b13bf5a03395-Abstract-Conference.html": {
    "title": "PANORAMIA: Privacy Auditing of Machine Learning Models without Retraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mishaal Kazmi",
      "Hadrien Lautraite",
      "Alireza Akbari",
      "Qiaoyue Tang",
      "Mauricio Soroco",
      "Tao Wang",
      "Sébastien Gambs",
      "Mathias Lécuyer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/693189d9b2d1949a28a9b2abf9fe7aec-Abstract-Conference.html": {
    "title": "A Metalearned Neural Circuit for Nonparametric Bayesian Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jake Snell",
      "Gianluca Bencomo",
      "Tom Griffiths"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/694be3548697e9cc8999d45e8d16fe1e-Abstract-Conference.html": {
    "title": "MSA Generation with Seqs2Seqs Pretraining: Advancing Protein Structure Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "LE ZHANG",
      "Jiayang Chen",
      "Tao Shen",
      "Yu Li",
      "Siqi Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6953cb03692e187e3acd5c3aada984e3-Abstract-Conference.html": {
    "title": "Learning on Large Graphs using Intersecting Communities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Finkelshtein",
      "Ismail Ceylan",
      "Michael Bronstein",
      "Ron Levie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/698570ae5ec88e07e9f4547f831b4593-Abstract-Conference.html": {
    "title": "Flow Priors for Linear Inverse Problems via Iterative Corrupted Trajectory Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasi Zhang",
      "Peiyu Yu",
      "Yaxuan Zhu",
      "Yingshan CHANG",
      "Feng Gao",
      "Ying Nian Wu",
      "Oscar Leong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/698cfaf72a208aef2e78bcac55b74328-Abstract-Conference.html": {
    "title": "No-Regret M${}^{\\natural}$-Concave Function Maximization: Stochastic Bandit Algorithms and NP-Hardness of Adversarial Full-Information Setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taihei Oki",
      "Shinsaku Sakaue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/69e68611ec4d8c0ae4a4b2bece165f5f-Abstract-Conference.html": {
    "title": "Understanding Generalizability of Diffusion Models Requires Rethinking the Hidden Gaussian Structure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Li",
      "Yixiang Dai",
      "Qing Qu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/69f3eb242c7c9df9ea2f2b66ea8b3c0f-Abstract-Conference.html": {
    "title": "MGF: Mixed Gaussian Flow for Diverse Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahe Chen",
      "Jinkun Cao",
      "Dahua Lin",
      "Kris Kitani",
      "Jiangmiao Pang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/69f5b860d6dc469ac6e52f03866b73c4-Abstract-Conference.html": {
    "title": "DeltaDEQ: Exploiting Heterogeneous Convergence for Accelerating Deep Equilibrium Iterations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zuowen Wang",
      "Longbiao Cheng",
      "Pehuen Moure",
      "Niklas Hahn",
      "Shih-Chii Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/69f98acf161316ed896047e45da3bc0c-Abstract-Conference.html": {
    "title": "Functionally Constrained Algorithm Solves Convex Simple Bilevel Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huaqing Zhang",
      "Lesi Chen",
      "Jing Xu",
      "Jingzhao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6a14c7f9fb3f42645cfa6bd5aa446819-Abstract-Conference.html": {
    "title": "Generalizable and Animatable Gaussian Head Avatar",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuangeng Chu",
      "Tatsuya Harada"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6a183ab792c56627d4c9dc45c1f5a616-Abstract-Conference.html": {
    "title": "No-Regret Learning for Fair Multi-Agent Social Welfare Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxiao Zhang",
      "Ramiro Deo-Campo Vuong",
      "Haipeng Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6a1e959d76d9974caa46ced84b4cd6c1-Abstract-Conference.html": {
    "title": "Initializing Services in Interactive ML Systems for Diverse Users",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avinandan Bose",
      "Mihaela Curmei",
      "Daniel Jiang",
      "Jamie H Morgenstern",
      "Sarah Dean",
      "Lillian Ratliff",
      "Maryam Fazel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6a2e30664b9647f97d7b9275358d083c-Abstract-Conference.html": {
    "title": "Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuancheng Xu",
      "Jiarui Yao",
      "Manli Shu",
      "Yanchao Sun",
      "Zichu Wu",
      "Ning Yu",
      "Tom Goldstein",
      "Furong Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6a39cf3b666f8bdb2223f253981f3869-Abstract-Conference.html": {
    "title": "Rethinking Exploration in Reinforcement Learning with Effective Metric-Based Exploration Bonus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Wang",
      "Kaiyan Zhao",
      "Furui Liu",
      "Leong Hou U"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6a412f0037b0df295a39a198666ea6a6-Abstract-Conference.html": {
    "title": "Local Curvature Smoothing with Stein's Identity for Efficient Score Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "GENKI OSADA",
      "Makoto Shing",
      "Takashi Nishide"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6a5c38a730b17f3827c09cf6b192be04-Abstract-Conference.html": {
    "title": "Model-based Diffusion for Trajectory Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoyi Pan",
      "Zeji Yi",
      "Guanya Shi",
      "Guannan Qu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6a69d44b3386e50c06f7107ef4f29302-Abstract-Conference.html": {
    "title": "A Cat Is A Cat (Not A Dog!): Unraveling Information Mix-ups in Text-to-Image Encoders through Causal Analysis and Embedding Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chieh-Yun Chen",
      "Chiang Tseng",
      "Li-Wu Tsao",
      "Hong-Han Shuai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6a8e10164a90d5c3660c3949289f969a-Abstract-Conference.html": {
    "title": "Learning De-Biased Representations for Remote-Sensing Imagery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Tian",
      "Zhaozheng CHEN",
      "QIANRU SUN"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6ac807c9b296964409b277369e55621a-Abstract-Conference.html": {
    "title": "Challenges of Generating Structurally Diverse Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fedor Velikonivtsev",
      "Mikhail Mironov",
      "Liudmila Prokhorenkova"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6ad7e3de1776ba5ed1a6aadc9c1724a5-Abstract-Conference.html": {
    "title": "The Price of Implicit Bias in Adversarially Robust Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolaos Tsilivis",
      "Natalie Frank",
      "Nati Srebro",
      "Julia Kempe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6adaf0cbeba11705d4ea67a62044f63d-Abstract-Conference.html": {
    "title": "A probability contrastive learning framework for 3D molecular representation learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayu Qin",
      "Jian Chen",
      "Rohan Sharma",
      "Jingchen Sun",
      "Changyou Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6aebba00fff5b6de7b488e496f80edd7-Abstract-Conference.html": {
    "title": "To Believe or Not to Believe Your LLM: Iterative Prompting for Estimating Epistemic Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasin Abbasi Yadkori",
      "Ilja Kuzborskij",
      "András György",
      "Csaba Szepesvari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6aef8bffb372096ee73d98da30119f89-Abstract-Conference.html": {
    "title": "From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinlei Wang",
      "Maike Feng",
      "Jing Qiu",
      "JINJIN GU",
      "Junhua Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6af08ba9468f0daca4b8dd388cb95824-Abstract-Conference.html": {
    "title": "Vision-Language Models are Strong Noisy Label Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Wei",
      "Hao-Tian Li",
      "ChunShu Li",
      "Jiang-Xin Shi",
      "Yu-Feng Li",
      "Min-Ling Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6b031defd145b02bed031093d8797bb3-Abstract-Conference.html": {
    "title": "Teach Better or Show Smarter? On Instructions and Exemplars in Automatic Prompt Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingchen Wan",
      "Ruoxi Sun",
      "Hootan Nakhost",
      "Sercan Arik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6b066da6a23bc55f9b887e7298102884-Abstract-Conference.html": {
    "title": "Algorithmic progress in language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wing Hin Anson Ho",
      "Tamay Besiroglu",
      "Ege Erdil",
      "Zifan Guo",
      "David Owen",
      "Robi Rahman",
      "David Atkinson",
      "Neil Thompson",
      "Jaime Sevilla"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6b111780a4a1c3beecb43b708ad7415e-Abstract-Conference.html": {
    "title": "Train-Attention: Meta-Learning Where to Focus in Continual Knowledge Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seo Yeongbin",
      "Dongha Lee",
      "Jinyoung Yeo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6b295b08549c0441914e391651423477-Abstract-Conference.html": {
    "title": "One Sample Fits All: Approximating All Probabilistic Values Simultaneously and Efficiently",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weida Li",
      "Yaoliang Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6b44ee74539ea77d6a0d50d468724371-Abstract-Conference.html": {
    "title": "Forgetting, Ignorance or Myopia: Revisiting Key Challenges in Online Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Xinrui",
      "Chuanxing Geng",
      "Wenhai Wan",
      "Shao-Yuan Li",
      "Songcan Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6b4cb812f234a92ab757d1544912b4a8-Abstract-Conference.html": {
    "title": "Sample and Computationally Efficient Robust Learning of Gaussian Single-Index Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Puqian Wang",
      "Nikos Zarifis",
      "Ilias Diakonikolas",
      "Jelena Diakonikolas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6b6492cd06db22bac024506e9ed0925e-Abstract-Conference.html": {
    "title": "Towards Unsupervised Model Selection for Domain Adaptive Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengfu Yu",
      "Jinhong Deng",
      "Wen Li",
      "Lixin Duan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6b7375226d4742ff910618a56ae72b7d-Abstract-Conference.html": {
    "title": "Where Do Large Learning Rates Lead Us?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ildus Sadrtdinov",
      "Maxim Kodryan",
      "Eduard Pokonechny",
      "Ekaterina Lobacheva",
      "Dmitry P Vetrov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6b768359d0e8925164f61f381a748441-Abstract-Conference.html": {
    "title": "Mixture of Nested Experts: Adaptive Processing of Visual Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gagan Jain",
      "Nidhi Hegde",
      "Aditya Kusupati",
      "Arsha Nagrani",
      "Shyamal Buch",
      "Prateek Jain",
      "Anurag Arnab",
      "Sujoy Paul"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6b7e1e96243c9edc378f85e7d232e415-Abstract-Conference.html": {
    "title": "Unified Domain Generalization and Adaptation for Multi-View 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyusam Chang",
      "Jiwon Lee",
      "Donghyun Kim",
      "Jinkyu Kim",
      "Dongwook Lee",
      "Daehyun Ji",
      "Sujin Jang",
      "Sangpil Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6b9320bee0e8dc092e8d780b9afd686f-Abstract-Conference.html": {
    "title": "Provably Efficient Reinforcement Learning with Multinomial Logit Function Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long-Fei Li",
      "Yu-Jie Zhang",
      "Peng Zhao",
      "Zhi-Hua Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6b99cfe8f27a30d013f49a970aacd6e8-Abstract-Conference.html": {
    "title": "Conformalized Multiple Testing after Data-dependent Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoning Wang",
      "Yuyang Huo",
      "Liuhua Peng",
      "Changliang Zou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6ba9e7ddd48e6db2dcaa7ec3806714b3-Abstract-Conference.html": {
    "title": "GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuseung Lee",
      "Taehoon Yoon",
      "Minhyuk Sung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6bbda0824bcc20749f21510fd8b28de5-Abstract-Conference.html": {
    "title": "Reconstruction of Manipulated Garment with Guided Deformation Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ren Li",
      "Corentin Dumery",
      "Zhantao Deng",
      "Pascal Fua"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6bbefb73c0ede70635823a18426b9208-Abstract-Conference.html": {
    "title": "Navigating Chemical Space with Latent Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanghao Wei",
      "Yining Huang",
      "Chenru Duan",
      "Yue Song",
      "Yuanqi Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6bd5fca2074dcd9ede9de50f71f7ec28-Abstract-Conference.html": {
    "title": "Neural network learns low-dimensional polynomials with SGD near the information-theoretic limit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jason Lee",
      "Kazusato Oko",
      "Taiji Suzuki",
      "Denny Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6bdde0373d53d4a501249547084bed43-Abstract-Conference.html": {
    "title": "Diffusion for World Modeling: Visual Details Matter in Atari",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eloi Alonso",
      "Adam Jelley",
      "Vincent Micheli",
      "Anssi Kanervisto",
      "Amos J. Storkey",
      "Tim Pearce",
      "François Fleuret"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6be46e85b1e3ff727c107f2673ce8027-Abstract-Conference.html": {
    "title": "Black-Box Forgetting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuke Kuwana",
      "Yuta Goto",
      "Takashi Shibata",
      "Go Irie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6bf333d4ca7c7f6fe6e301b2a3160163-Abstract-Conference.html": {
    "title": "Continual learning with the neural tangent ensemble",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ari Benjamin",
      "Christian-Gernot Pehle",
      "Kyle Daruwalla"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6c0ff499edc529c7d8c9f05c7c0ccb82-Abstract-Conference.html": {
    "title": "Aligning Audio-Visual Joint Representations with an Agentic Workflow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shentong Mo",
      "Yibing Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6c49d2ad55e50c5ebc1002fdc50e48e5-Abstract-Conference.html": {
    "title": "Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Eysenbach",
      "Vivek Myers",
      "Ruslan Salakhutdinov",
      "Sergey Levine"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6c4a1a3cbe70ef36d7d6332166bba77d-Abstract-Conference.html": {
    "title": "Self-Guided Masked Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongwoo Shin",
      "Inseo Lee",
      "Junho Lee",
      "Joonseok Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6c5f877b2d78e093860ce9715e251dec-Abstract-Conference.html": {
    "title": "Robust Offline Active Learning on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanchen Wu",
      "Yubai Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6c6c5fccf3c8661fcae219be7ca226f7-Abstract-Conference.html": {
    "title": "OnlineTAS: An Online Baseline for Temporal Action Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Zhong",
      "Guodong Ding",
      "Angela Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6c8985579293e0209bdaa4f21bb1d237-Abstract-Conference.html": {
    "title": "Efficient LLM Scheduling by Learning to Rank",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichao Fu",
      "Siqi Zhu",
      "Runlong Su",
      "Aurick Qiao",
      "Ion Stoica",
      "Hao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6c9dcffe0b9cc3b05d83bcdddb250690-Abstract-Conference.html": {
    "title": "Swift Sampler: Efficient Learning of Sampler by 10 Parameters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Yao",
      "Chuming Li",
      "Canran Xiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6ca5691fad39acd4ad984791b1338da8-Abstract-Conference.html": {
    "title": "Identifiability Analysis of Linear ODE Systems with Hidden Confounders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyuan Wang",
      "Biwei Huang",
      "Wei Huang",
      "Xi Geng",
      "Mingming Gong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6ca5d2665de83394f437dad0c3746907-Abstract-Conference.html": {
    "title": "EGODE: An Event-attended Graph ODE Framework for Modeling Rigid Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyang Yuan",
      "Gongbo Sun",
      "Zhiping Xiao",
      "Hang Zhou",
      "Xiao Luo",
      "Junyu Luo",
      "Yusheng Zhao",
      "Wei Ju",
      "Ming Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6cac74e7bb50d1f21626800f5b49a869-Abstract-Conference.html": {
    "title": "Validating Climate Models with Spherical Convolutional Wasserstein Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert Garrett",
      "Trevor Harris",
      "Zhuo Wang",
      "Bo Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6cb81234ab47027e991728ed7dd76735-Abstract-Conference.html": {
    "title": "Optical Diffusion Models for Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilker Oguz",
      "Niyazi Dinc",
      "Mustafa Yildirim",
      "Junjie Ke",
      "Innfarn Yoo",
      "Qifei Wang",
      "Feng Yang",
      "Christophe Moser",
      "Demetri Psaltis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6cc066e48b391a749c2161f271fda80b-Abstract-Conference.html": {
    "title": "Private Stochastic Convex Optimization with Heavy Tails: Near-Optimality from Simple Reductions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hilal Asi",
      "Daogao Liu",
      "Kevin Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6cca3481ae66707958b824d37df40177-Abstract-Conference.html": {
    "title": "Learning World Models for Unconstrained Goal Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanlin Duan",
      "Wensen Mao",
      "He Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6cf7a37e761f55b642cf0939b4c64bb8-Abstract-Conference.html": {
    "title": "An engine not a camera: Measuring performative power of online search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Celestine Mendler-Dünner",
      "Gabriele Carovano",
      "Moritz Hardt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6d0942e288ce41db8d4ebd041e7d1100-Abstract-Conference.html": {
    "title": "DFA-GNN: Forward Learning of Graph Neural Networks by Direct Feedback Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gongpei Zhao",
      "Tao Wang",
      "Congyan Lang",
      "Yi Jin",
      "Yidong Li",
      "Haibin Ling"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6d09ef61aeb76be676b358f6f87b3484-Abstract-Conference.html": {
    "title": "MeshFormer : High-Quality Mesh Generation with 3D-Guided Reconstruction Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghua Liu",
      "Chong Zeng",
      "Xinyue Wei",
      "Ruoxi Shi",
      "Linghao Chen",
      "Chao Xu",
      "Mengqi Zhang",
      "Zhaoning Wang",
      "Xiaoshuai Zhang",
      "Isabella Liu",
      "Hongzhi Wu",
      "Hao Su"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6d19163eaec3b0f0accbe462a0139466-Abstract-Conference.html": {
    "title": "Differentiable Task Graph Learning: Procedural Activity Representation and Online Mistake Detection from Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luigi Seminara",
      "Giovanni Maria Farinella",
      "Antonino Furnari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6d56bc83ae9a4fafdce050bb36f04174-Abstract-Conference.html": {
    "title": "Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haibo Jin",
      "Andy Zhou",
      "Joe Menke",
      "Haohan Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6d96950e6ce0b92e3d0fb2afa634e0f2-Abstract-Conference.html": {
    "title": "A Unified Framework for 3D Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Xu",
      "Chunsheng Shi",
      "Sifan Tu",
      "Xin Zhou",
      "Dingkang Liang",
      "Xiang Bai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6da1f7ec03ef87c83a8914173688666a-Abstract-Conference.html": {
    "title": "Reciprocal Reward Influence Encourages Cooperation From Self-Interested Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John L Zhou",
      "Weizhe Hong",
      "Jonathan Kao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6dc63b4063c978cf195bc15178e8152a-Abstract-Conference.html": {
    "title": "Navigable Graphs for High-Dimensional Nearest Neighbor Search: Constructions and Limits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haya Diwan",
      "Jinrui Gou",
      "Cameron Musco",
      "Christopher Musco",
      "Torsten Suel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6ddc001d07ca4f319af96a3024f6dbd1-Abstract-Conference.html": {
    "title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bernal Jimenez Gutierrez",
      "Yiheng Shu",
      "Yu Gu",
      "Michihiro Yasunaga",
      "Yu Su"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6dddcff5b115b40c998a08fbd1cea4d7-Abstract-Conference.html": {
    "title": "Breaking Semantic Artifacts for Generalized AI-generated Image Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chende Zheng",
      "Chenhao Lin",
      "Zhengyu Zhao",
      "Hang Wang",
      "Xu Guo",
      "Shuai Liu",
      "Chao Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6de2e84b8da47bb2eb5e2ac96c63d2b0-Abstract-Conference.html": {
    "title": "QTIP: Quantization with Trellises and Incoherence Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Albert Tseng",
      "Qingyao Sun",
      "David Hou",
      "Christopher M De Sa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6de71272e0be559af3f76b884e94794b-Abstract-Conference.html": {
    "title": "Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Chen",
      "Ankit Shah",
      "Jindong Wang",
      "Ran Tao",
      "Yidong Wang",
      "Xiang Li",
      "Xing Xie",
      "Masashi Sugiyama",
      "Rita Singh",
      "Bhiksha Raj"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6df1b2b45e64d402588746f79b68b82c-Abstract-Conference.html": {
    "title": "Minimum Entropy Coupling with Bottleneck",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Ebrahimi",
      "Jun Chen",
      "Ashish Khisti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6df3a719d99bd2479c04114d357003d0-Abstract-Conference.html": {
    "title": "Artificial Generational Intelligence: Cultural Accumulation in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Cook",
      "Chris Lu",
      "Edward Hughes",
      "Joel Z. Leibo",
      "Jakob Foerster"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6e016d123b093571bfd086f51d209b8a-Abstract-Conference.html": {
    "title": "RMLR: Extending Multinomial Logistic Regression into General Geometries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziheng Chen",
      "Yue Song",
      "Rui Wang",
      "Xiaojun Wu",
      "Nicu Sebe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6e0274fdb05f21654421db43cd8de042-Abstract-Conference.html": {
    "title": "Wide Two-Layer Networks can Learn from Adversarial Perturbations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soichiro Kumano",
      "Hiroshi Kera",
      "Toshihiko Yamasaki"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6e09c213ac18d6375704a4f3ea75c4f8-Abstract-Conference.html": {
    "title": "Coupled Mamba: Enhanced Multimodal Fusion with Coupled State Space Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbing Li",
      "Hang Zhou",
      "Junqing Yu",
      "Zikai Song",
      "Wei Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6e2861dabad3fe21a71914ccfbfff976-Abstract-Conference.html": {
    "title": "Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeming Wen",
      "Swarat Chaudhuri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6e3b9fb0c0c56cf6e1ee61e6a068fca4-Abstract-Conference.html": {
    "title": "S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Yang",
      "Jixuan Leng",
      "Geyang Guo",
      "Jiawei Zhao",
      "Ryumei Nakada",
      "Linjun Zhang",
      "Huaxiu Yao",
      "Beidi Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6e4b14e76d0d4f42a9dff031a7a8417b-Abstract-Conference.html": {
    "title": "OPEL: Optimal Transport Guided ProcedurE Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayeed Shafayet Chowdhury",
      "Soumyadeep Chandra",
      "Kaushik Roy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6e4cdfdd909ea4e34bfc85a12774cba0-Abstract-Conference.html": {
    "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhe Gu",
      "Ziwei Ji",
      "Wenwei Zhang",
      "Chengqi Lyu",
      "Dahua Lin",
      "Kai Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6e60a9023d2c63f7f0856910129ae753-Abstract-Conference.html": {
    "title": "Unveiling The Matthew Effect Across Channels: Assessing Layer Width Sufficiency via Weight Norm Variance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiting Chen",
      "Jiazi Bu",
      "Junchi Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6e9f99809b6df59e8fe5c9ffe269bf40-Abstract-Conference.html": {
    "title": "Learning to Cooperate with Humans using Generative Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yancheng Liang",
      "Daphne Chen",
      "Abhishek Gupta",
      "Simon S Du",
      "Natasha Jaques"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6eb05d8bc6bd7bb6868c64b5802125bd-Abstract-Conference.html": {
    "title": "Active, anytime-valid risk controlling prediction sets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Xu",
      "Nikos Karampatziakis",
      "Paul Mineiro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6ec81faa568317949b0ff3be4d87cced-Abstract-Conference.html": {
    "title": "SGD vs GD: Rank Deficiency in Linear Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Vardhan Varre",
      "Margarita Sagitova",
      "Nicolas Flammarion"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6ed5bf446f59e2c6646d23058c86424b-Abstract-Conference.html": {
    "title": "Are Language Models Actually Useful for Time Series Forecasting?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingtian Tan",
      "Mike Merrill",
      "Vinayak Gupta",
      "Tim Althoff",
      "Tom Hartvigsen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6ee0ebd0983f0926496b7d4e0d48b8e3-Abstract-Conference.html": {
    "title": "Parameterized Approximation Schemes for Fair-Range Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Zhang",
      "Xiaohong Chen",
      "Limei Liu",
      "Jie Chen",
      "Junyu Huang",
      "Qilong Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6eee1da520ac78c56f3a3e0353a5da34-Abstract-Conference.html": {
    "title": "Simulation-Free Training of Neural ODEs on Paired Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Semin Kim",
      "Jaehoon Yoo",
      "Jinwoo Kim",
      "Yeonwoo Cha",
      "Saehoon Kim",
      "Seunghoon Hong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6efa2dea10bd60e68c9a98b35a1099f3-Abstract-Conference.html": {
    "title": "ETO:Efficient Transformer-based Local Feature Matching by Organizing Multiple Homography Hypotheses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Ni",
      "Guofeng Zhang",
      "Guanglin Li",
      "Yijin Li",
      "Xinyang Liu",
      "Zhaoyang Huang",
      "Hujun Bao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6efcc7fd8efeee29a050a79c843c90e0-Abstract-Conference.html": {
    "title": "SemCoder: Training Code Language Models with Comprehensive Semantics Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangruibo Ding",
      "Jinjun Peng",
      "Marcus Min",
      "Gail Kaiser",
      "Junfeng Yang",
      "Baishakhi Ray"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6f098c5618e4289f5a6b642c94199f34-Abstract-Conference.html": {
    "title": "On the Limitations of Fractal Dimension as a Measure of Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charlie Tan",
      "Inés García-Redondo",
      "Qiquan Wang",
      "Michael Bronstein",
      "Anthea Monod"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6f11132f6ecbbcafafdf6decfc98f7be-Abstract-Conference.html": {
    "title": "ColJailBreak: Collaborative Generation and Editing for Jailbreaking Text-to-Image Deep Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhuo Ma",
      "Shanmin Pang",
      "Qi Guo",
      "Tianyu Wei",
      "Qing Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6f1346bac8b02f76a631400e2799b24b-Abstract-Conference.html": {
    "title": "DI-MaskDINO: A Joint Object Detection and Instance Segmentation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixiong Nan",
      "Li Xianghong",
      "Tao Xiang",
      "Jifeng Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6f1cacd88ac241bf51266d9d6594ab32-Abstract-Conference.html": {
    "title": "Leveraging partial stragglers within gradient coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya RAMAMOORTHY",
      "Ruoyu Meng",
      "Vrinda Girimaji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6f244818d72b2a4be9b1225d1344e950-Abstract-Conference.html": {
    "title": "Multiclass Transductive Online Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steve Hanneke",
      "Vinod Raman",
      "Amirreza Shaeiri",
      "Unique Subedi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6f479ea488e0908ac8b1b37b27fd134c-Abstract-Conference.html": {
    "title": "Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicola Dainese",
      "Matteo Merler",
      "Minttu Alakuijala",
      "Pekka Marttinen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6f6af59b11f3919965b9811c6c9ad6df-Abstract-Conference.html": {
    "title": "Localizing Memorization in SSL Vision Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Wang",
      "Adam Dziedzic",
      "Michael Backes",
      "Franziska Boenisch"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6fb9ea5197c0b8ece8a64220fb82cdfe-Abstract-Conference.html": {
    "title": "Opponent Modeling based on Subgoal Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "XiaoPeng Yu",
      "Jiechuan Jiang",
      "Zongqing Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6fcb1afcc1e9c2c82c8ddddf03bcf0f6-Abstract-Conference.html": {
    "title": "Scaling Laws in Linear Regression: Compute, Parameters, and Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Licong Lin",
      "Jingfeng Wu",
      "Sham Kakade",
      "Peter L. Bartlett",
      "Jason Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6fcc2190f456464160921e98393bf50e-Abstract-Conference.html": {
    "title": "Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanyun Cui",
      "Qianle Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6fdf57c71bc1f1ee29014b8dc52e723f-Abstract-Conference.html": {
    "title": "How Do Large Language Models Acquire Factual Knowledge During Pretraining?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoyeon Chang",
      "Jinho Park",
      "Seonghyeon Ye",
      "Sohee Yang",
      "Youngkyung Seo",
      "Du-Seong Chang",
      "Minjoon Seo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6fe5d7a2de090168917425fe89a6c1b8-Abstract-Conference.html": {
    "title": "YouDream: Generating Anatomically Controllable Consistent Text-to-3D Animals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sandeep Mishra",
      "Oindrila Saha",
      "Alan Bovik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6feb9b30798abcfae937760d183605e1-Abstract-Conference.html": {
    "title": "Online Consistency of the Nearest Neighbor Rule",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geelon So",
      "Sanjoy Dasgupta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6ffc307731cd1d6784c35c6c2875c122-Abstract-Conference.html": {
    "title": "Constrained Binary Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Průša",
      "Vojtech Franc"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/6ffdbbe354893979367f93e2121e37dd-Abstract-Conference.html": {
    "title": "Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Zhang",
      "Runjin Chen",
      "Shiwei Liu",
      "Zhewei Yao",
      "Olatunji Ruwase",
      "Beidi Chen",
      "Xiaoxia Wu",
      "Zhangyang &quot;Atlas&quot; Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7016d7b7b6e3c05b2128ac5b3aae492d-Abstract-Conference.html": {
    "title": "RA-PbRL: Provably Efficient Risk-Aware Preference-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Zhao",
      "Jose Aguilar Escamilla",
      "Weyl Lu",
      "Huazheng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/701ec28790b29a5bc33832b7bdc4c3b6-Abstract-Conference.html": {
    "title": "Inferring Neural Signed Distance Functions by Overfitting on Single Noisy Point Clouds through Finetuning Data-Driven based Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Chen",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/70270a1bc28ecb2a2aefad566c5e556b-Abstract-Conference.html": {
    "title": "Flipped Classroom: Aligning Teacher Attention with Student in Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Lin",
      "Wenbin An",
      "Jiahao Wang",
      "Yan Chen",
      "Feng Tian",
      "Mengmeng Wang",
      "QianYing Wang",
      "Guang Dai",
      "Jingdong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/702b67152ec4435795f681865b67999c-Abstract-Conference.html": {
    "title": "A Canonicalization Perspective on Invariant and Equivariant Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "George Ma",
      "Yifei Wang",
      "Derek Lim",
      "Stefanie Jegelka",
      "Yisen Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/705b97ecb07ae86524d438abac97a3e2-Abstract-Conference.html": {
    "title": "Neural Conditional Probability for Uncertainty Quantification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladimir Kostic",
      "Grégoire Pacreau",
      "Giacomo Turri",
      "Pietro Novelli",
      "Karim Lounici",
      "Massimiliano Pontil"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/70681152122b1f4c5fa6b6ba823eea9a-Abstract-Conference.html": {
    "title": "DDR: Exploiting Deep Degradation Response as Flexible Image Descriptor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juncheng Wu",
      "Zhangkai Ni",
      "Hanli Wang",
      "Wenhan Yang",
      "Yuyin Zhou",
      "Shiqi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/70702e8cbb4890b4a467b984ae59828a-Abstract-Conference.html": {
    "title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anay Mehrotra",
      "Manolis Zampetakis",
      "Paul Kassianik",
      "Blaine Nelson",
      "Hyrum Anderson",
      "Yaron Singer",
      "Amin Karbasi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/707a2d58641b2192203b4bf4c532cfe1-Abstract-Conference.html": {
    "title": "Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Albert Q. Jiang",
      "Alicja Ziarko",
      "Bartosz Piotrowski",
      "Wenda Li",
      "Mateja Jamnik",
      "Piotr Miłoś"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7087c949df293f13c0052ac825936e6f-Abstract-Conference.html": {
    "title": "Diffusion Priors for Variational Likelihood Estimation and Image Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Cheng",
      "Shan Tan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7089237e828d55febd85a8d07f89e102-Abstract-Conference.html": {
    "title": "Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanlin Duan",
      "Guofeng Cui",
      "He Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/708e0d691a22212e1e373dc8779cbe53-Abstract-Conference.html": {
    "title": "Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Yang",
      "Xinyu Gao",
      "Yang-Tian Sun",
      "Yihua Huang",
      "Xiaoyang Lyu",
      "Wen Zhou",
      "Shaohui Jiao",
      "Xiaojuan Qi",
      "Xiaogang Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/708fdc7911f11585ee7161518e509ae6-Abstract-Conference.html": {
    "title": "$\\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runqian Wang",
      "Soumya Ghosh",
      "David Cox",
      "Diego Antognini",
      "Aude Oliva",
      "Rogerio Feris",
      "Leonid Karlinsky"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/70a06501001e1820fd1eb9ee821302d2-Abstract-Conference.html": {
    "title": "PACE: Marrying generalization in PArameter-efficient fine-tuning with Consistency rEgularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Ni",
      "Shan Zhang",
      "Piotr Koniusz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/70b8505ac79e3e131756f793cd80eb8d-Abstract-Conference.html": {
    "title": "Real-Time Selection Under General Constraints via Predictive Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Huo",
      "Lin Lu",
      "Haojie Ren",
      "Changliang Zou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/70d4ef44dc973586cfa3ea92b4868b72-Abstract-Conference.html": {
    "title": "Unsupervised Homography Estimation on Multimodal Image Pair via Alternating Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghyeob Song",
      "Jaihyun Lew",
      "Hyemi Jang",
      "Sungroh Yoon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/70d638f3177d2f0bbdd9f400b43f0683-Abstract-Conference.html": {
    "title": "Self-Supervised Alignment with Mutual Information: Learning to Follow Principles without Preference Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan-Philipp Fraenken",
      "Eric Zelikman",
      "Rafael Rafailov",
      "Kanishk Gandhi",
      "Tobias Gerstenberg",
      "Noah Goodman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/70e5444e5f331f7f5431f302110b97af-Abstract-Conference.html": {
    "title": "Talking Heads: Understanding Inter-Layer Communication in Transformer Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack Merullo",
      "Carsten Eickhoff",
      "Ellie Pavlick"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/70fa5df8e3300dc30bf19bee44a56155-Abstract-Conference.html": {
    "title": "The Fine-Grained Complexity of Gradient Computation for Training Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josh Alman",
      "Zhao Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/71008846945765893f43abe829090bf8-Abstract-Conference.html": {
    "title": "Learning Cut Generating Functions for Integer Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Cheng",
      "Amitabh Basu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7103444259031cc58051f8c9a4868533-Abstract-Conference.html": {
    "title": "Worst-Case Offline Reinforcement Learning with Arbitrary Data Support",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kohei Miyaguchi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7103cd82de95a7b30983fcf74ba499ac-Abstract-Conference.html": {
    "title": "Fairness without Harm: An Influence-Guided Active Sampling Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinlong Pang",
      "Jialu Wang",
      "Zhaowei Zhu",
      "Yuanshun Yao",
      "Chen Qian",
      "Yang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/710445227fa8c1b6a9ceada902dd4741-Abstract-Conference.html": {
    "title": "Opponent Modeling with In-context Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Jing",
      "Bingyun Liu",
      "Kai Li",
      "Yifan Zang",
      "Haobo Fu",
      "Qiang Fu",
      "Junliang Xing",
      "Jian Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7107d4d2e837bde2171c6b71b5bde954-Abstract-Conference.html": {
    "title": "On Sampling Strategies for Spectral Model Sharding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Korzhenkov",
      "Christos Louizos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/710f3f8473b93394505a082f9a8f3ba2-Abstract-Conference.html": {
    "title": "S-SOS: Stochastic Sum-Of-Squares for Parametric Polynomial Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Licheng Zhu",
      "Mathias Oster",
      "Yuehaw Khoo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7116cda41d75d580bae15d9e484a8466-Abstract-Conference.html": {
    "title": "TopoLogic: An Interpretable Pipeline for Lane Topology Reasoning on Driving Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanping Fu",
      "Wenbin Liao",
      "Xinyuan Liu",
      "Hang Xu",
      "Yike Ma",
      "Yucheng Zhang",
      "Feng Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/711ebaa5fcc8321d0075c1b42870b425-Abstract-Conference.html": {
    "title": "Detecting and Measuring Confounding Using Causal Mechanism Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abbavaram Gowtham Reddy",
      "Vineeth N Balasubramanian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7124771cf8c9b0f6e9c7fec0a66c5866-Abstract-Conference.html": {
    "title": "Robust Gaussian Processes via Relevance Pursuit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Ament",
      "Elizabeth Santorella",
      "David Eriksson",
      "Ben Letham",
      "Maximilian Balandat",
      "Eytan Bakshy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/715b78ccfb6f4cada5528ac9b5278def-Abstract-Conference.html": {
    "title": "No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishaal Udandarao",
      "Ameya Prabhu",
      "Adhiraj Ghosh",
      "Yash Sharma",
      "Philip Torr",
      "Adel Bibi",
      "Samuel Albanie",
      "Matthias Bethge"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7169e48889a07662f4168e3911c4a69e-Abstract-Conference.html": {
    "title": "On the Minimax Regret for Contextual Linear Bandits and Multi-Armed Bandits with Expert Advice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shinji Ito"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/716ddcbb5aa8802f56a7dfd94c4df3db-Abstract-Conference.html": {
    "title": "Gradient Rewiring for Editable Graph Neural Network Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhimeng Jiang",
      "Zirui Liu",
      "Xiaotian Han",
      "Qizhang Feng",
      "Hongye Jin",
      "Qiaoyu Tan",
      "Kaixiong Zhou",
      "Na Zou",
      "Xia Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7172e147d916eef4cb1eb30016ce725f-Abstract-Conference.html": {
    "title": "Understanding Bias in Large-Scale Visual Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boya Zeng",
      "Yida Yin",
      "Zhuang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/71883294314045d60c900113a359934b-Abstract-Conference.html": {
    "title": "4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roman Bachmann",
      "Oguzhan Fatih Kar",
      "David Mizrahi",
      "Ali Garjani",
      "Mingfei Gao",
      "David Griffiths",
      "Jiaming Hu",
      "Afshin Dehghan",
      "Amir Zamir"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/718a3c5cf135894db6e718725f52ef9a-Abstract-Conference.html": {
    "title": "BOLD: Boolean Logic Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Van Minh NGUYEN",
      "Cristian Ocampo-Blandon",
      "Aymen Askri",
      "Louis Leconte",
      "Ba-Hien Tran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/718d02a76d69686a36eccc8cde3e6a41-Abstract-Conference.html": {
    "title": "Unifying Generation and Prediction on Graphs with Latent Graph Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cai Zhou",
      "Xiyuan Wang",
      "Muhan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7195b4ffa1803de9eb34447032f94234-Abstract-Conference.html": {
    "title": "Template-free Articulated Gaussian Splatting for Real-time Reposable Dynamic View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diwen Wan",
      "Yuxiang Wang",
      "Ruijie Lu",
      "Gang Zeng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/71ad539a57b1fd49b19e5c80070cb8b9-Abstract-Conference.html": {
    "title": "Rethinking The Training And Evaluation of Rich-Context Layout-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Cheng",
      "ZIXU ZHAO",
      "Tong He",
      "Tianjun Xiao",
      "Zheng Zhang",
      "Yicong Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/71b17f00017da0d73823ccf7fbce2d4f-Abstract-Conference.html": {
    "title": "Facilitating Multimodal Classification via Dynamically Learning Modality Gap",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yang",
      "Fengqiang Wan",
      "Qing-Yuan Jiang",
      "Yi Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/71b52a5b3fe2e9303433a174b60e160d-Abstract-Conference.html": {
    "title": "Periodic agent-state based Q-learning for POMDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amit Sinha",
      "Matthieu Geist",
      "Aditya Mahajan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/71c3451f6cd6a4f82bb822db25cea4fd-Abstract-Conference.html": {
    "title": "Make Your LLM Fully Utilize the Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengnan An",
      "Zexiong Ma",
      "Zeqi Lin",
      "Nanning Zheng",
      "Jian-Guang Lou",
      "Weizhu Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/71c9eb0913e6c7fda3afd69c914b1a0c-Abstract-Conference.html": {
    "title": "Vivid-ZOO: Multi-View Video Generation with Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bing Li",
      "Cheng Zheng",
      "Wenxuan Zhu",
      "Jinjie Mai",
      "Biao Zhang",
      "Peter Wonka",
      "Bernard Ghanem"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/71d7dbe2652bd4662d29fa269f059db4-Abstract-Conference.html": {
    "title": "Boosting Vision-Language Models with Transduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Zanella",
      "Benoît Gérin",
      "Ismail Ayed"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/71d9a840a7f04339ca271c10a0f4fbd4-Abstract-Conference.html": {
    "title": "Nature-Inspired Local Propagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro Betti",
      "Marco Gori"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/71f7154547c748c8041505521ca433ab-Abstract-Conference.html": {
    "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Yang",
      "Ruomeng Ding",
      "Yong Lin",
      "Huan Zhang",
      "Tong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/71f88122d414cfeb455ac0ed932fbe1f-Abstract-Conference.html": {
    "title": "OSLO: One-Shot Label-Only Membership Inference Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuefeng Peng",
      "Jaechul Roh",
      "Subhransu Maji",
      "Amir Houmansadr"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/720991812855c99df50bc8b36966cd81-Abstract-Conference.html": {
    "title": "DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by Distilling Neural Fields and Foundation Model Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Letian Wang",
      "Seung Wook Kim",
      "Jiawei Yang",
      "Cunjun Yu",
      "Boris Ivanovic",
      "Steven Waslander",
      "Yue Wang",
      "Sanja Fidler",
      "Marco Pavone",
      "Peter Karkus"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/720e7ebc49c84252abc0754dddf80976-Abstract-Conference.html": {
    "title": "The Feature Speed Formula: a flexible approach to scale hyper-parameters of deep neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lénaïc Chizat",
      "Praneeth Netrapalli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/72176f95680c3fb27a0966f36d5d0c53-Abstract-Conference.html": {
    "title": "On the Power of Decision Trees in Auto-Regressive Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulu Gan",
      "Tomer Galanti",
      "Tomaso Poggio",
      "Eran Malach"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/722968fb52c75947371c6b02a58c959a-Abstract-Conference.html": {
    "title": "HardCore Generation: Generating Hard UNSAT Problems for Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Cotnareanu",
      "Zhanguang Zhang",
      "Hui-Ling Zhen",
      "Yingxue Zhang",
      "Mark Coates"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/723933067ad315269b620bc0d2c05cba-Abstract-Conference.html": {
    "title": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junxiong Wang",
      "Daniele Paliotta",
      "Avner May",
      "Alexander Rush",
      "Tri Dao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/72393bd47a35f5b3bee4c609e7bba733-Abstract-Conference.html": {
    "title": "Expectation Alignment: Handling Reward Misspecification in the Presence of Expectation Mismatch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Malek Mechergui",
      "Sarath Sreedharan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/72462f451057f9e782af2c3c04c5a4b0-Abstract-Conference.html": {
    "title": "BendVLM: Test-Time Debiasing of Vision-Language Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Walter Gerych",
      "Haoran Zhang",
      "Kimia Hamidieh",
      "Eileen Pan",
      "Maanas K. Sharma",
      "Tom Hartvigsen",
      "Marzyeh Ghassemi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/724afcaae4ae92a9220a077ffe80088d-Abstract-Conference.html": {
    "title": "An Analysis of Tokenization: Transformers under Markov Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nived Rajaraman",
      "Jiantao Jiao",
      "Kannan Ramchandran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/724be4472168f31ba1c9ac630f15dec8-Abstract-Conference.html": {
    "title": "SGLang: Efficient Execution of Structured Language Model Programs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lianmin Zheng",
      "Liangsheng Yin",
      "Zhiqiang Xie",
      "Chuyue (Livia) Sun",
      "Jeff Huang",
      "Cody Hao Yu",
      "Shiyi Cao",
      "Christos Kozyrakis",
      "Ion Stoica",
      "Joseph E Gonzalez",
      "Clark Barrett",
      "Ying Sheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/726ab29b61a749b36d2593648716ae3c-Abstract-Conference.html": {
    "title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Li",
      "Zhao Song",
      "Yu Xia",
      "Tong Yu",
      "Tianyi Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7279908471a7dd4898d2715f7c6a7413-Abstract-Conference.html": {
    "title": "Induced Model Matching: Restricted Models Help Train Full-Featured Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Usama Muneeb",
      "Mesrob I Ohannessian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/72802bef5cf1a3449e909b20c2ae18d5-Abstract-Conference.html": {
    "title": "The ALCHEmist: Automated Labeling 500x CHEaper than LLM Data Annotators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tzu-Heng Huang",
      "Catherine Cao",
      "Vaishnavi Bhargava",
      "Frederic Sala"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7280f65ed571b7b28321f2c7cf4c60c8-Abstract-Conference.html": {
    "title": "SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang Chen",
      "Shiyan Chen",
      "Jiyuan Zhang",
      "Baoyue Zhang",
      "Yajing Zheng",
      "Tiejun Huang",
      "Zhaofei Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/72d32f4fe0b7af03732bd227bf1c4a5f-Abstract-Conference.html": {
    "title": "PTQ4DiT: Post-training Quantization for Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Wu",
      "Haoxuan Wang",
      "Yuzhang Shang",
      "Mubarak Shah",
      "Yan Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/72d50a87b218d84c175d16f4557f7e12-Abstract-Conference.html": {
    "title": "B-cosification: Transforming Deep Neural Networks to be Inherently Interpretable",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shreyash Arya",
      "Sukrut Rao",
      "Moritz Böhle",
      "Bernt Schiele"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/72da102da91a8042a0b2aa968429a9f9-Abstract-Conference.html": {
    "title": "SelfCodeAlign: Self-Alignment for Code Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Wei",
      "Federico Cassano",
      "Jiawei Liu",
      "Yifeng Ding",
      "Naman Jain",
      "Zachary Mueller",
      "Harm de Vries",
      "Leandro Von Werra",
      "Arjun Guha",
      "LINGMING ZHANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/72f9c316440c384a95c88022fd78f066-Abstract-Conference.html": {
    "title": "SPARKLE: A Unified Single-Loop Primal-Dual Framework for Decentralized Bilevel Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuchen Zhu",
      "Boao Kong",
      "Songtao Lu",
      "Xinmeng Huang",
      "Kun Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/73073ccb3bc559fd001e66b9079d6d5e-Abstract-Conference.html": {
    "title": "Mitigating Partial Observability in Sequential Decision Processes via the Lambda Discrepancy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cameron Allen",
      "Aaron Kirtland",
      "Ruo Yu Tao",
      "Sam Lobel",
      "Daniel Scott",
      "Nicholas Petrocelli",
      "Omer Gottesman",
      "Ronald Parr",
      "Michael L. Littman",
      "George Konidaris"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7314e20a73542bbfff25030d1185ce88-Abstract-Conference.html": {
    "title": "The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anya Sims",
      "Cong Lu",
      "Jakob Foerster",
      "Yee Whye Teh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/733209a1f12071a7ec979e8ffaeb1d99-Abstract-Conference.html": {
    "title": "UGC: Universal Graph Coarsening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohit Kataria",
      "Sandeep Kumar",
      "Jayadeva"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7343a5c976f8399880b695267f1f9e9f-Abstract-Conference.html": {
    "title": "Improving the Training of Rectified Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangyun Lee",
      "Zinan Lin",
      "Giulia Fanti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/735c847a07bf6dd4486ca1ace242a88c-Abstract-Conference.html": {
    "title": "A robust inlier identification algorithm for point cloud registration via $\\mathbf{\\ell_0}$-minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinuo Jiang",
      "Xiuchuan Tang",
      "Cheng Cheng",
      "Ye Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7371b5f5fab9fd401c4ebd352f29dc48-Abstract-Conference.html": {
    "title": "Score-based generative models are provably robust: an uncertainty quantification perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikiforos Mimikos-Stamatopoulos",
      "Benjamin Zhang",
      "Markos Katsoulakis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7371ee6a40da2951303ec7ebdb2150ce-Abstract-Conference.html": {
    "title": "Distributional Reinforcement Learning with Regularized Wasserstein Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Sun",
      "Yingnan Zhao",
      "Wulong Liu",
      "Bei Jiang",
      "Linglong Kong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/73950f0eb4ac0925dc71ba2406893320-Abstract-Conference.html": {
    "title": "CryoGEM: Physics-Informed Generative Cryo-Electron Microscopy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiakai Zhang",
      "Qihe Chen",
      "Yan Zeng",
      "Wenyuan Gao",
      "Xuming He",
      "Zhijie Liu",
      "Jingyi Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/73a2e3284e26730555101a2fb4bbc54e-Abstract-Conference.html": {
    "title": "Semi-Open 3D Object Retrieval via Hierarchical Equilibrium on Hypergraph",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Xu",
      "Yifan Feng",
      "Jun Zhang",
      "Jun-Hai Yong",
      "Yue Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/73ba81c7b25134a559c8a9c39ec1a4c3-Abstract-Conference.html": {
    "title": "Task-recency bias strikes back: Adapting covariances in Exemplar-Free Class Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Grzegorz Rypeść",
      "Sebastian Cygert",
      "Tomasz Trzciński",
      "Bartłomiej Twardowski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/73bda9a20f6f9f6074ce822e76f126bb-Abstract-Conference.html": {
    "title": "ScaleKD: Strong Vision Transformers Could Be Excellent Teachers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Fan",
      "Chao Li",
      "Xiaolong Liu",
      "Anbang Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/73c362193e3e23188b2a74de7824cc4c-Abstract-Conference.html": {
    "title": "Unscrambling disease progression at scale: fast inference of event permutations with optimal transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Wijeratne",
      "Daniel Alexander"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/73efab19ebde03ff0958f4f155483f57-Abstract-Conference.html": {
    "title": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dake Bu",
      "Wei Huang",
      "Andi Han",
      "Atsushi Nitanda",
      "Taiji Suzuki",
      "Qingfu Zhang",
      "Hau-San Wong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/74057f0c21b553b4adbaeb394a15617f-Abstract-Conference.html": {
    "title": "Differentially Private Optimization with Sparse Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Badih Ghazi",
      "Cristóbal Guzmán",
      "Pritish Kamath",
      "Ravi Kumar",
      "Pasin Manurangsi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/74088c68894b99383c12399c9c637be9-Abstract-Conference.html": {
    "title": "Fourier-enhanced Implicit Neural Fusion Network for Multispectral and Hyperspectral Image Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YuJie Liang",
      "ZiHan Cao",
      "Shangqi Deng",
      "Hong-Xia Dou",
      "Liang-Jian Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/741aab8b41a2987867acc9939ad50383-Abstract-Conference.html": {
    "title": "Learning Linear Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jikai Jin",
      "Vasilis Syrgkanis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/741ad162ab0f3da6f9aad60e9e34f5f1-Abstract-Conference.html": {
    "title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiaoyu Tang",
      "Jiawei Chen",
      "Zhuoqun Li",
      "Bowen Yu",
      "Yaojie Lu",
      "ChengFu",
      "Haiyang Yu",
      "Hongyu Lin",
      "Fei Huang",
      "Ben He",
      "Xianpei Han",
      "Le Sun",
      "Yongbin Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7423902b5534e2b267438c85444a54b1-Abstract-Conference.html": {
    "title": "Conformal Inverse Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Lin",
      "Erick Delage",
      "Timothy Chan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7428e6db752171d6b832c53b2ed297ab-Abstract-Conference.html": {
    "title": "Alignment for Honesty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqing Yang",
      "Ethan Chern",
      "Xipeng Qiu",
      "Graham Neubig",
      "Pengfei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/743771397cef2aa0ef497c428c3a46b7-Abstract-Conference.html": {
    "title": "ODGEN: Domain-specific Object Detection Data Generation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyuan Zhu",
      "Shiyu Li",
      "Yuxuan (Andy) Liu",
      "Jian Yuan",
      "Ping Huang",
      "Jiulong Shan",
      "Huimin Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/744e09d3213ee53de88f04164a9b06ab-Abstract-Conference.html": {
    "title": "Language Models as Zero-shot Lossless Gradient Compressors: Towards General Neural Parameter Prior Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui-Po Wang",
      "Mario Fritz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/746b0a1e6e3cff8d968ba6d2e6fff049-Abstract-Conference.html": {
    "title": "When to Sense and Control? A Time-adaptive Approach for Continuous-Time RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lenart Treven",
      "Bhavya",
      "Yarden As",
      "Florian Dorfler",
      "Andreas Krause"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/747dc7c6566c74eb9a663bcd8d057c78-Abstract-Conference.html": {
    "title": "Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Xia",
      "Fangcheng Fu",
      "Wentao Zhang",
      "Jiawei Jiang",
      "Bin CUI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/74953ef4abd9c436344e59d687ad34d3-Abstract-Conference.html": {
    "title": "Adversarial Environment Design via Regret-Guided Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hojun Chung",
      "Junseo Lee",
      "Minsoo Kim",
      "Dohyeong Kim",
      "Songhwai Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7495fa446f10e9edef6e47b2d327596e-Abstract-Conference.html": {
    "title": "Generalizable Implicit Motion Modeling for Video Frame Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zujin Guo",
      "Wei Li",
      "Chen Change Loy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/74d188c51d97fcfbc0269f584d6a53b7-Abstract-Conference.html": {
    "title": "Provable Partially Observable Reinforcement Learning with Privileged Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Cai",
      "Xiangyu Liu",
      "Argyris Oikonomou",
      "Kaiqing Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/74fa9e6bc36aa567fe7cf002b733a30d-Abstract-Conference.html": {
    "title": "Noise-Aware Differentially Private Regression via Meta-Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ossi Räisä",
      "Stratis Markou",
      "Matthew Ashman",
      "Wessel Bruinsma",
      "Marlon Tobaben",
      "Antti Honkela",
      "Richard Turner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/75008a0fba53bf13b0bb3b7bff986e0e-Abstract-Conference.html": {
    "title": "ReFT: Representation Finetuning for Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengxuan Wu",
      "Aryaman Arora",
      "Zheng Wang",
      "Atticus Geiger",
      "Dan Jurafsky",
      "Christopher D Manning",
      "Christopher Potts"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/750a56383caf20b92fe070732f969300-Abstract-Conference.html": {
    "title": "Training Binary Neural Networks via Gaussian Variational Inference and Low-Rank Semidefinite Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Orecchia",
      "Jiawei Hu",
      "Xue He",
      "Wang Mark",
      "Xulei Yang",
      "Min Wu",
      "Xue Geng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/751ef1e7f557a8a88f0837b61bf5070f-Abstract-Conference.html": {
    "title": "SMART: Towards Pre-trained Missing-Aware Model for Patient Health Status Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Yu",
      "Chu Xu",
      "Yujie Jin",
      "Yasha Wang",
      "Junfeng Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7538e7a7a589a267f49527fe6fbc640e-Abstract-Conference.html": {
    "title": "BMRS: Bayesian Model Reduction for Structured Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dustin Wright",
      "Christian Igel",
      "Raghavendra Selvan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/754612bde73a8b65ad8743f1f6d8ddf6-Abstract-Conference.html": {
    "title": "SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Lu",
      "Xu-Yang Chen",
      "Han-Jia Ye",
      "De-Chuan Zhan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7565f036ceb20a2c74d341bfaa9fffad-Abstract-Conference.html": {
    "title": "MO-DDN: A Coarse-to-Fine Attribute-based Exploration Agent for Multi-Object Demand-driven Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongcheng Wang",
      "Peiqi Liu",
      "Wenzhe Cai",
      "Mingdong Wu",
      "Zhengyu Qian",
      "Hao Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7571c9d44179c7988178593c5b62a9b6-Abstract-Conference.html": {
    "title": "The Sample Complexity of Gradient Descent in Stochastic Convex Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roi Livni",
      "Amir",
      "Koren",
      "Livni",
      "Schliserman",
      "Sherman",
      "Koren",
      "Shalev-Shwartz",
      "Shamir",
      "Srebro",
      "Sridharan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/759ca99a82e2a9137c6bef4811c8d378-Abstract-Conference.html": {
    "title": "Fight Back Against Jailbreaking via Prompt Adversarial Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichuan Mo",
      "Yuji Wang",
      "Zeming Wei",
      "Yisen Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/75b0edb869e2cd509d64d0e8ff446bc1-Abstract-Conference.html": {
    "title": "The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ezra Edelman",
      "Nikolaos Tsilivis",
      "Benjamin Edelman",
      "Eran Malach",
      "Surbhi Goel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/760341adc5632de3f1cf2e8d22215a93-Abstract-Conference.html": {
    "title": "Leveraging Tumor Heterogeneity: Heterogeneous Graph Representation Learning for Cancer Survival Prediction in Whole Slide Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junxian Wu",
      "Xinyi Ke",
      "XIAOMING JIANG",
      "Huanwen Wu",
      "Youyong Kong",
      "Lizhi Shao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/760b5def8dcb1156aac454e9c0f5f406-Abstract-Conference.html": {
    "title": "Flipping-based Policy for Chance-Constrained Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xun Shen",
      "Shuo Jiang",
      "Akifumi Wachi",
      "Kazumune Hashimoto",
      "Sebastien Gros"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/760d09bcc06b949f5ac4b6a918739aa8-Abstract-Conference.html": {
    "title": "Memory-Efficient LLM Training with Online Subspace Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaizhao Liang",
      "Bo Liu",
      "Lizhang Chen",
      "Qiang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/76227feb18ea0ee40bd15cf02c33e18e-Abstract-Conference.html": {
    "title": "Learning Distinguishable Trajectory Representation with Contrastive Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianxu Li",
      "Kun Zhu",
      "Juan Li",
      "Yang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/765871e77d2ca65126d3d64d31aa6908-Abstract-Conference.html": {
    "title": "HEALNet: Multimodal Fusion for Heterogeneous Biomedical Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantin Hemker",
      "Nikola Simidjievski",
      "Mateja Jamnik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7659d52f74f46dabf53f221a92bc97ab-Abstract-Conference.html": {
    "title": "Stochastic contextual bandits with graph feedback: from independence number to MAS number",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiao Wen",
      "Yanjun Han",
      "Zhengyuan Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7664a7e946a84ac5e97649a967717cf2-Abstract-Conference.html": {
    "title": "Reinforcement Learning with Lookahead Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nadav Merlis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/76954b4a44e158e738b4c64494977c6a-Abstract-Conference.html": {
    "title": "SearchLVLMs: A Plug-and-Play Framework for Augmenting Large Vision-Language Models by Searching Up-to-Date Internet Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanhao Li",
      "Zhen Li",
      "Chenchen Jing",
      "Shuo Liu",
      "Wenqi Shao",
      "Yuwei Wu",
      "Ping Luo",
      "Yu Qiao",
      "Kaipeng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/76b878f34e43c5faeba770c840bec394-Abstract-Conference.html": {
    "title": "Drones Help Drones: A Collaborative Framework for Multi-Drone Object Trajectory Prediction and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhechao Wang",
      "Peirui Cheng",
      "Minxing Chen",
      "Pengju Tian",
      "Zhirui Wang",
      "Xinming Li",
      "Xue Yang",
      "Xian Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/76bea0a1cf7bf9b78f842009f6de15a1-Abstract-Conference.html": {
    "title": "DCDepth: Progressive Monocular Depth Estimation in Discrete Cosine Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Wang",
      "Zhiqiang Yan",
      "Junkai Fan",
      "Wanlu Zhu",
      "Xiang Li",
      "Jun Li",
      "Jian Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/76d2f8e328e1081c22a77ca0fa330ca5-Abstract-Conference.html": {
    "title": "Queueing Matching Bandits with Preference Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jung-hun Kim",
      "Min-hwan Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/76e952a4e83d97186d3f55eef6a3a367-Abstract-Conference.html": {
    "title": "Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Hassani",
      "Wen-Mei Hwu",
      "Humphrey Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/76ec4dc30e9faaf0e4b6093eaa377218-Abstract-Conference.html": {
    "title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Zhang",
      "Sining Zhoubian",
      "Ziniu Hu",
      "Yisong Yue",
      "Yuxiao Dong",
      "Jie Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/770b5223b47d4304042826b29733e864-Abstract-Conference.html": {
    "title": "A Global Depth-Range-Free Multi-View Stereo Transformer Network with Pose Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yitong Dong",
      "Yijin Li",
      "Zhaoyang Huang",
      "Weikang Bian",
      "Jingbo Liu",
      "Hujun Bao",
      "Zhaopeng Cui",
      "Hongsheng Li",
      "Guofeng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7718914dfe7d5a657bf6261b5f431021-Abstract-Conference.html": {
    "title": "Multi-Stage Predict+Optimize for (Mixed Integer) Linear Programs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi HU",
      "Jasper Lee",
      "Jimmy Lee",
      "Peter Stuckey"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7740c0dc79dc282a29384ceb9d3b2408-Abstract-Conference.html": {
    "title": "Optimization Can Learn Johnson Lindenstrauss Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikos Tsikouras",
      "Constantine Caramanis",
      "Christos Tzamos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/774164b966cc277c82a960934445140d-Abstract-Conference.html": {
    "title": "Federated Graph Learning for Cross-Domain Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Yang",
      "Zhaopeng Peng",
      "Zihui Wang",
      "Jianzhong Qi",
      "Chaochao Chen",
      "Weike Pan",
      "Chenglu Wen",
      "Cheng Wang",
      "Xiaoliang Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/775226eaa2a36c543e2bd6cc9eae1b6a-Abstract-Conference.html": {
    "title": "Co-occurrence is not Factual Association in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Zhang",
      "Miao Li",
      "Ji Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7755fafa64beceac16d3eab17f8ed3d6-Abstract-Conference.html": {
    "title": "LLM-AutoDA: Large Language Model-Driven Automatic Data Augmentation for Long-tailed Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengkun Wang",
      "Zhe Zhao",
      "HaiBin Wen",
      "Fanfu Wang",
      "Binwu Wang",
      "Qingfu Zhang",
      "Yang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/776a5f2c7d6dd4b0d83145fc044e2726-Abstract-Conference.html": {
    "title": "Designs for Enabling Collaboration in Human-Machine Teaming via Interactive and Explainable Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohan Paleja",
      "Michael Munje",
      "Kimberlee Chang",
      "Reed Jensen",
      "Matthew Gombolay"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/777cc2af0ab984e6dc48d168ce7b754f-Abstract-Conference.html": {
    "title": "Data Augmentation with Diffusion for Open-Set Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonghyun Ban",
      "Heesan Kong",
      "Kee-Eung Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/77828623211df05497ce3658300dafd9-Abstract-Conference.html": {
    "title": "LoTLIP: Improving Language-Image Pre-training for Long Text Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Wu",
      "Kecheng Zheng",
      "Shuailei Ma",
      "Fan Lu",
      "Yuxin Guo",
      "Yifei Zhang",
      "Wei Chen",
      "Qingpei Guo",
      "Yujun Shen",
      "Zheng-Jun Zha"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7798f18a1e91aff7ad1df4841d6a01a8-Abstract-Conference.html": {
    "title": "Rethinking Parity Check Enhanced Symmetry-Preserving Ansatz",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ge Yan",
      "Mengfei Ran",
      "Ruocheng Wang",
      "Kaisen Pan",
      "Junchi Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/779cb405b8b916f7db70e73d51650ed2-Abstract-Conference.html": {
    "title": "Dynamic Neural Regeneration: Enhancing Deep Learning Generalization on Small Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vijaya Raghavan Ramkumar",
      "Elahe Arani",
      "Bahram Zonooz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/77baa7c2a3a675823e89131698fd6e19-Abstract-Conference.html": {
    "title": "Safe LoRA: The Silver Lining of Reducing Safety Risks when Finetuning Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chia-Yi Hsu",
      "Yu-Lin Tsai",
      "Chih-Hsun Lin",
      "Pin-Yu Chen",
      "Chia-Mu Yu",
      "Chun-Ying Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/77c87a15bbf0aad017c53995b832cf84-Abstract-Conference.html": {
    "title": "Learning Structure-Aware Representations of Dependent Types",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantinos Kogkalidis",
      "Orestis Melkonian",
      "Jean-Philippe Bernardy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/77dd8e90fe833eba5fae86cf017d7a56-Abstract-Conference.html": {
    "title": "AsCAN: Asymmetric Convolution-Attention Networks for Efficient Recognition and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anil Kag",
      "n n",
      "Jierun Chen",
      "Junli Cao",
      "Willi Menapace",
      "Aliaksandr Siarohin",
      "Sergey Tulyakov",
      "Jian Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/77e6814d32a86b76123bd10aa7e2ad81-Abstract-Conference.html": {
    "title": "Microstructures and Accuracy of Graph Recall by Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanbang Wang",
      "Hejie Cui",
      "Jon M. Kleinberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/77f089cd16dbc36ddd1caeb18446fbdd-Abstract-Conference.html": {
    "title": "CulturePark: Boosting Cross-cultural Understanding in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Li",
      "Damien Teney",
      "Linyi Yang",
      "Qingsong Wen",
      "Xing Xie",
      "Jindong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/77f2d0c271e508278ea13e24cd8773d5-Abstract-Conference.html": {
    "title": "Robust group and simultaneous inferences for high-dimensional single index model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weichao Yang",
      "Hongwei Shi",
      "Xu Guo",
      "Changliang Zou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/77f33e8bd80345de4aea8554bbe5a4da-Abstract-Conference.html": {
    "title": "A Separation in Heavy-Tailed Sampling: Gaussian vs. Stable Oracles for Proximal Samplers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye He",
      "Alireza Mousavi-Hosseini",
      "Krishnakumar Balasubramanian",
      "Murat A Erdogdu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/77fa0e7d45c6687f1958de0b31e9fc05-Abstract-Conference.html": {
    "title": "Symmetry-Informed Governing Equation Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianke Yang",
      "Wang Rao",
      "Nima Dehmamy",
      "Robin Walters",
      "Rose Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/77fa8253adfc8b33209639f3e9985741-Abstract-Conference.html": {
    "title": "The Limits of Differential Privacy in Online Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Li",
      "Wei Wang",
      "Peng Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/77fd93190fdcfc26023c3cbb1ce8bcf0-Abstract-Conference.html": {
    "title": "Listenable Maps for Zero-Shot Audio Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Paissan",
      "Luca Della Libera",
      "Mirco Ravanelli",
      "Cem Subakan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/783c5986e1d6112cb4688d9b2105609a-Abstract-Conference.html": {
    "title": "LaSCal: Label-Shift Calibration without target labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teodora Popordanoska",
      "Gorjan Radevski",
      "Tinne Tuytelaars",
      "Matthew Blaschko"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/784fd5a46dfe303e5b51c8621b84cf3f-Abstract-Conference.html": {
    "title": "Beyond Optimism: Exploration With Partially Observable Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simone Parisi",
      "Alireza Kazemipour",
      "Michael Bowling"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/78526d7ad4a2532bd91416e948b9644c-Abstract-Conference.html": {
    "title": "Wasserstein Distance Rivals Kullback-Leibler Divergence for Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Lv",
      "Haoyuan Yang",
      "Peihua Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7878585bb03092b0cf23732de3590d90-Abstract-Conference.html": {
    "title": "Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vahid Balazadeh",
      "Keertana Chidambaram",
      "Viet Nguyen",
      "Rahul G Krishnan",
      "Vasilis Syrgkanis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7886b89aced4d37dd25a6f32854bf3f9-Abstract-Conference.html": {
    "title": "CoFie: Learning Compact Neural Surface Representations with Coordinate Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanwen Jiang",
      "Haitao Yang",
      "Georgios Pavlakos",
      "Qixing Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7886b9bafe76c52fd568db10ff9772df-Abstract-Conference.html": {
    "title": "Heterogeneity-Guided Client Sampling: Towards Fast and Efficient Non-IID Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huancheng Chen",
      "Haris Vikalo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/788f9787ec93e3e1f34658fb003a1f44-Abstract-Conference.html": {
    "title": "Learning Versatile Skills with Curriculum Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Tang",
      "Zhihui Xie",
      "Zichuan Lin",
      "Deheng Ye",
      "Shuai Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/78a2ed8e7da1829d64fb3561adbd706b-Abstract-Conference.html": {
    "title": "Neural Collapse To Multiple Centers For Imbalanced Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongren Yan",
      "Yuhua Qian",
      "Furong Peng",
      "Jiachen Luo",
      "zheqing zhu",
      "Feijiang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/78b6beab44f92adc74ac1fdb212ac3a0-Abstract-Conference.html": {
    "title": "NeuMA: Neural Material Adaptor for Visual Grounding of Intrinsic Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Cao",
      "Shanyan Guan",
      "Yanhao Ge",
      "Wei Li",
      "Xiaokang Yang",
      "Chao Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/78b9d95f6bb13b080c2a68bdea54cdbb-Abstract-Conference.html": {
    "title": "Stable-Pose: Leveraging Transformers for Pose-Guided Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun Wang",
      "Morteza Ghahremani Boozandani",
      "Yitong Li",
      "Björn Ommer",
      "Christian Wachinger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/78df0f831fbe5854349dbdfccde7ee5d-Abstract-Conference.html": {
    "title": "Generalized Fast Exact Conformalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diyang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/78e839f96568985d18463044a064ea0f-Abstract-Conference.html": {
    "title": "Unleashing Multispectral Video's Potential in Semantic Segmentation: A Semi-supervised Viewpoint and New UAV-View Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Ji",
      "Jingjing Li",
      "Wenbo Li",
      "Yilin Shen",
      "Li cheng",
      "Hongxia Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/78ed45281dd746a265fff16ff75a02e5-Abstract-Conference.html": {
    "title": "Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Zhang",
      "Kaiqi Zhang",
      "Minshuo Chen",
      "Yuma Takeda",
      "Mengdi Wang",
      "Tuo Zhao",
      "Yu-Xiang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/78f0db30c39c850de728c769f42fc903-Abstract-Conference.html": {
    "title": "Amortized Planning with Large-Scale Transformers: A Case Study on Chess",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anian Ruoss",
      "Grégoire Delétang",
      "Sourabh Medapati",
      "Jordi Grau-Moya",
      "Kevin Li",
      "Elliot Catt",
      "John Reid",
      "Cannada Lewis",
      "Joel Veness",
      "Tim Genewein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/78fa1d5646bc285940281cfc1386efae-Abstract-Conference.html": {
    "title": "Doob's Lagrangian: A Sample-Efficient Variational Approach to Transition Path Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanqi Du",
      "Michael Plainer",
      "Rob Brekelmans",
      "Chenru Duan",
      "Frank Noe",
      "Carla P. Gomes",
      "Alan Aspuru-Guzik",
      "Kirill Neklyudov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/79353864175b6b8c3d073cde84d7014a-Abstract-Conference.html": {
    "title": "On the Impact of Feature Heterophily on Link Prediction with Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiong Zhu",
      "Gaotang Li",
      "Yao-An Yang",
      "Jing Zhu",
      "Xuehao Cui",
      "Danai Koutra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/79358587d84628728199059f648824e6-Abstract-Conference.html": {
    "title": "Breaking the curse of dimensionality in structured density estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert A. Vandermeulen",
      "Wai Ming Tai",
      "Bryon Aragam"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/794108d801c7762b427a8367efe906c5-Abstract-Conference.html": {
    "title": "Zipper: Addressing Degeneracy in Algorithm-Agnostic Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geng Chen",
      "Yinxu Jia",
      "Guanghui Wang",
      "Changliang Zou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/794a72b1a9d5fc4c040eb3110d94c8a1-Abstract-Conference.html": {
    "title": "Adaptive Exploration for Data-Efficient General Value Function Evaluations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arushi Jain",
      "Josiah Hanna",
      "Doina Precup"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/796455f65fd2cbe049112a2d2d4488cb-Abstract-Conference.html": {
    "title": "The Poisson Midpoint Method for Langevin Dynamics: Provably Efficient Discretization for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saravanan Kandasamy",
      "Dheeraj Nagaraj"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7972f3735e104a54715922aa416fde1b-Abstract-Conference.html": {
    "title": "Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning Zero-Shot Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaican Li",
      "Weiyan Xie",
      "Yongxiang Huang",
      "Didan Deng",
      "Lanqing Hong",
      "Zhenguo Li",
      "Ricardo Silva",
      "Nevin L. Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7988e9b3876ad689e921ce05d711442f-Abstract-Conference.html": {
    "title": "Language Generation in the Limit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jon M. Kleinberg",
      "Sendhil Mullainathan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/79af547fa22cdcb0facd0b31dcd4bdb0-Abstract-Conference.html": {
    "title": "Model Decides How to Tokenize: Adaptive DNA Sequence Tokenization with MxDNA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lifeng Qiao",
      "Peng Ye",
      "Yuchen Ren",
      "Weiqiang Bai",
      "Chaoqi Liang",
      "Xinzhu Ma",
      "Nanqing Dong",
      "Wanli Ouyang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/79b90b4c2ee23cc35fdd8de2969dc4e8-Abstract-Conference.html": {
    "title": "Provably and Practically Efficient Adversarial Imitation Learning with General Function Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Xu",
      "Zhilong Zhang",
      "Ruishuo Chen",
      "Yihao Sun",
      "Yang Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/79ba1b827d3fc58e129d1cbfc8ff69f2-Abstract-Conference.html": {
    "title": "EGonc : Energy-based Open-Set Node Classification with substitute Unknowns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qin Zhang",
      "Zelin Shi",
      "Shirui Pan",
      "Junyang Chen",
      "Huisi Wu",
      "Xiaojun Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/79be41d858841037987964e3f5caf76d-Abstract-Conference.html": {
    "title": "Latent Functional Maps: a spectral framework for representation alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Fumero",
      "Marco Pegoraro",
      "Valentino Maiorca",
      "Francesco Locatello",
      "Emanuele Rodolà"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/79cafa874121a3435d8a54f454b646b4-Abstract-Conference.html": {
    "title": "Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sean Jaffe",
      "Alexander Davydov",
      "Deniz Lapsekili",
      "Ambuj K Singh",
      "Francesco Bullo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7a0f7e9d9b42b26e5bfc9ba4c6e5287c-Abstract-Conference.html": {
    "title": "Just Add $100 More: Augmenting Pseudo-LiDAR Point Cloud for Resolving Class-imbalance Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mincheol Chang",
      "Siyeong Lee",
      "Jinkyu Kim",
      "Namil Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7a0f8055c838df8e62329a76c7c6403d-Abstract-Conference.html": {
    "title": "A Novel Unified Architecture for Low-Shot Counting by Detection and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jer Pelhan",
      "Alan Lukezic",
      "Vitjan Zavrtanik",
      "Matej Kristan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7a641b8ec86162fc875fb9f6456a542f-Abstract-Conference.html": {
    "title": "RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhao Chen",
      "Weisen Jiang",
      "Baijiong Lin",
      "James T. Kwok",
      "Yu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7a7a3f53faafc0161be0fcb57e5fa078-Abstract-Conference.html": {
    "title": "OneBit: Towards Extremely Low-bit Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhuang Xu",
      "Xu Han",
      "Zonghan Yang",
      "Shuo Wang",
      "Qingfu Zhu",
      "Zhiyuan Liu",
      "Weidong Liu",
      "Wanxiang Che"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7a8e7fd295aa04eac4b470ae27f8785c-Abstract-Conference.html": {
    "title": "Chain-of-Thought Reasoning Without Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuezhi Wang",
      "Denny Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7a9745f251508a053425a256490b0665-Abstract-Conference.html": {
    "title": "Slack-Free Spiking Neural Network Formulation for Hypergraph Minimum Vertex Cover",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tam Nguyen",
      "Anh-Dzung Doan",
      "zhipeng cai",
      "Tat-Jun Chin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7a9ee756fb71d401bcb5230db02c11db-Abstract-Conference.html": {
    "title": "Federated Model Heterogeneous Matryoshka Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liping Yi",
      "Han Yu",
      "Chao Ren",
      "Gang Wang",
      "xiaoguang Liu",
      "Xiaoxiao Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7aad0cdc7e140778ad944f17a266e1bc-Abstract-Conference.html": {
    "title": "MaskFactory: Towards High-quality Synthetic Data Generation for Dichotomous Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Qian",
      "Yinda Chen",
      "Shengtao Lou",
      "Fahad Shahbaz Khan",
      "Xiaogang Jin",
      "Deng-Ping Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7aae9e3ec211249e05bd07271a6b1441-Abstract-Conference.html": {
    "title": "Unveiling Induction Heads: Provable Training Dynamics and Feature Learning in Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyu Chen",
      "Heejune Sheen",
      "Tianhao Wang",
      "Zhuoran Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7aebeabf3c031ab5395db836e0b73473-Abstract-Conference.html": {
    "title": "Optimal Top-Two Method for Best Arm Identification and Fluid Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Agniv Bandyopadhyay",
      "Sandeep Juneja",
      "Shubhada Agrawal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7af60ccb99c7a434a0d9d9c1fb00ca94-Abstract-Conference.html": {
    "title": "Training Data Attribution via Approximate Unrolling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juhan Bae",
      "Wu Lin",
      "Jonathan Lorraine",
      "Roger B Grosse"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7b122d0a0dcb1a86ffa25ccba154652b-Abstract-Conference.html": {
    "title": "Parsimony or Capability? Decomposition Delivers Both in Long-term Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinliang Deng",
      "Feiyang Ye",
      "Du Yin",
      "Xuan Song",
      "Ivor W. Tsang",
      "Hui Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7b24015f3af598e1d9179f6e06353780-Abstract-Conference.html": {
    "title": "The Impact of Geometric Complexity on Neural Collapse in Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Munn",
      "Benoit Dherin",
      "Javier Gonzalvo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7b3f7b6670fdab2933411b5b922cdcc3-Abstract-Conference.html": {
    "title": "Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy Curvature of Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Susung Hong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7b5d3047939b63ed97fcbbee23f8eb77-Abstract-Conference.html": {
    "title": "Cross-modal Representation Flattening for Multi-modal Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfeng FAN",
      "Wenchao Xu",
      "Haozhao Wang",
      "Song Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7b75a7339dfb256ee4b4bec028a6890b-Abstract-Conference.html": {
    "title": "FM-Delta: Lossless Compression for Storing Massive Fine-tuned Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanyi Ning",
      "Jingyu Wang",
      "Qi Qi",
      "Mengde Zhu",
      "Haifeng Sun",
      "Daixuan Cheng",
      "Jianxin Liao",
      "Ce Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7b76eea0c3683e440c3d362620f578cd-Abstract-Conference.html": {
    "title": "Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quentin Delfosse",
      "Sebastian Sztwiertnia",
      "Mark Rothermel",
      "Wolfgang Stammer",
      "Kristian Kersting"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7b78a2a7360d5a9ad750834dc5a33bfb-Abstract-Conference.html": {
    "title": "Back to the Continuous Attractor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ábel Ságodi",
      "Guillermo Martín-Sánchez",
      "Piotr Sokol",
      "Memming Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7b7d7985f62284060d65f532ed2ea5fa-Abstract-Conference.html": {
    "title": "CriticEval: Evaluating Large-scale Language Model as Critic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Lan",
      "Wenwei Zhang",
      "Chen Xu",
      "Heyan Huang",
      "Dahua Lin",
      "Kai Chen",
      "Xian-Ling Mao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7b7db41ea66d624587f211aa15c07e45-Abstract-Conference.html": {
    "title": "Learning Distributions on Manifolds with Free-Form Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Sorrenson",
      "Felix Draxler",
      "Armand Rousselot",
      "Sander Hummerich",
      "Ullrich Köthe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7b871f53ba2ee540ef76e6220b462e16-Abstract-Conference.html": {
    "title": "Truthful High Dimensional Sparse Linear Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyang Zhu",
      "Amina Manseur",
      "Meng Ding",
      "Jinyan Liu",
      "Jinhui Xu",
      "Di Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7bacd0ebd061d4694583ae0eb69ad15f-Abstract-Conference.html": {
    "title": "Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minjong Yoo",
      "Jinwoo Jang",
      "Wei-Jin Park",
      "Honguk Woo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7bc4f74e35bcfe8cfe43b0a860786d6a-Abstract-Conference.html": {
    "title": "Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingchang Chen",
      "Hongxuan Tang",
      "Zheng Chu",
      "Qianglong Chen",
      "Zekun Wang",
      "Ming Liu",
      "Bing Qin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7bf421a1370d5d3fae9ddbcbaf746143-Abstract-Conference.html": {
    "title": "TrackIME: Enhanced Video Point Tracking via Instance Motion Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seong Hyeon Park",
      "Huiwon Jang",
      "Byungwoo Jeon",
      "Sukmin Yun",
      "Paul Hongsuck Seo",
      "Jinwoo Shin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7c04aea54c2a60a632a47bd451cd2849-Abstract-Conference.html": {
    "title": "Knowledge-Empowered Dynamic Graph Network for Irregularly Sampled Medical Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yicheng Luo",
      "Zhen Liu",
      "Linghao Wang",
      "Binquan Wu",
      "Junhao Zheng",
      "Qianli Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7c06d57601163b03fbd132dbf925eff7-Abstract-Conference.html": {
    "title": "OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Shu",
      "Jiongfeng Fang",
      "Ying He",
      "Fei Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7c35478c8829e5b8baa0e69f3240994a-Abstract-Conference.html": {
    "title": "HHD-GP: Incorporating Helmholtz-Hodge Decomposition into Gaussian Processes for Learning Dynamical Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Xu",
      "Jia Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7c7baa87e763a7e2fa2527e7bf105508-Abstract-Conference.html": {
    "title": "Knowledge Composition using Task Vectors with Learned Anisotropic Scaling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederic Z. Zhang",
      "Paul Albert",
      "Cristian Rodriguez-Opazo",
      "Anton van den Hengel",
      "Ehsan Abbasnejad"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7c8365d335ebf55beca714122d6c882f-Abstract-Conference.html": {
    "title": "UniFL: Improve Latent Diffusion Model via Unified Feedback Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Zhang",
      "Jie Wu",
      "Yuxi Ren",
      "Xin Xia",
      "Huafeng Kuang",
      "Pan Xie",
      "Jiashi Li",
      "Xuefeng Xiao",
      "Weilin Huang",
      "Shilei Wen",
      "Lean Fu",
      "Guanbin Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7c8c0db90a14c7a2db9475e52dc343a9-Abstract-Conference.html": {
    "title": "Learning a Single Neuron Robustly to Distributional Shifts and Adversarial Label Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyao Li",
      "Sushrut Karmalkar",
      "Ilias Diakonikolas",
      "Jelena Diakonikolas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7c9341ad0263428b5057d92f4d88dfa0-Abstract-Conference.html": {
    "title": "SpikedAttention: Training-Free and Fully Spike-Driven Transformer-to-SNN Conversion with Winner-Oriented Spike Shift for Softmax Operation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangwoo Hwang",
      "Seunghyun Lee",
      "Dahoon Park",
      "Donghun Lee",
      "Jaeha Kung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7ca55c8276acf1f0aa996cd3622d1df4-Abstract-Conference.html": {
    "title": "Deep Correlated Prompting for Visual Recognition with Missing Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "lianyu hu",
      "Tongkai Shi",
      "Wei Feng",
      "Fanhua Shang",
      "Liang Wan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7caf9d251b546bc78078b35b4a6f3b7e-Abstract-Conference.html": {
    "title": "Partial observation can induce mechanistic mismatches in data-constrained models of neural dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Qian",
      "Jacob Zavatone-Veth",
      "Ben Ruben",
      "Cengiz Pehlevan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7cb04f510593c9ba30da398f5e0a7e7b-Abstract-Conference.html": {
    "title": "GraphVis: Boosting LLMs with Visual Knowledge Graph Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihe Deng",
      "Chenchen Ye",
      "Zijie Huang",
      "Mingyu Derek Ma",
      "Yiwen Kou",
      "Wei Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7cc16e8635e6f27c295355bd214ef8d8-Abstract-Conference.html": {
    "title": "PACE: Pacing Operator Learning to Accurate Optical Field Simulation for Complicated Photonic Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanqing Zhu",
      "Wenyan Cong",
      "Guojin Chen",
      "Shupeng Ning",
      "Ray Chen",
      "Jiaqi Gu",
      "David Z. Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7cdbd53dfbcf9a5263227555aac5b9cd-Abstract-Conference.html": {
    "title": "Evidential Stochastic Differential Equations for Time-Aware Sequential Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krishna Neupane",
      "Ervine Zheng",
      "Qi Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7cdf000d22c6cda21f3cbd7467aaf26f-Abstract-Conference.html": {
    "title": "Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yian Wang",
      "Xiaowen Qiu",
      "Jiageng Liu",
      "Zhehuan Chen",
      "Jiting Cai",
      "Yufei Wang",
      "Tsun-Hsuan Johnson Wang",
      "Zhou Xian",
      "Chuang Gan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7ce9df1d14adc2806df18c8e168d7ef9-Abstract-Conference.html": {
    "title": "Zero-Shot Transfer of Neural ODEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tyler Ingebrand",
      "Adam Thorpe",
      "Ufuk Topcu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7d0c6ff18f16797b92e77d7cc95b3c53-Abstract-Conference.html": {
    "title": "ACES: Generating a Diversity of Challenging Programming Puzzles with Autotelic Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julien Pourcel",
      "Cédric Colas",
      "Gaia Molinaro",
      "Pierre-Yves Oudeyer",
      "Laetitia Teodorescu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7d0c7a899224cd178b2e0cbecf39b5a5-Abstract-Conference.html": {
    "title": "Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Yunis",
      "Justin Jung",
      "Falcon Dai",
      "Matthew Walter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7d26958422928e08465d5dd6cf0cb4cb-Abstract-Conference.html": {
    "title": "Beware of Road Markings: A New Adversarial Patch Attack to Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hangcheng Liu",
      "Zhenhu Wu",
      "Hao Wang",
      "Xingshuo Han",
      "Shangwei Guo",
      "Tao Xiang",
      "Tianwei Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7d3626b603cac298c9f7573b1df00cac-Abstract-Conference.html": {
    "title": "Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibo Jiang",
      "Goutham Rajendran",
      "Pradeep K. Ravikumar",
      "Bryon Aragam"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7d53575463291ea6b5a23cf6e571f59b-Abstract-Conference.html": {
    "title": "BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibin Wang",
      "Haizhou Shi",
      "Ligong Han",
      "Dimitris Metaxas",
      "Hao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7d60bfd8458b67acbbaf18b892338d00-Abstract-Conference.html": {
    "title": "BoostAdapter: Improving Vision-Language Test-Time Adaptation via Regional Bootstrapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taolin Zhang",
      "Jinpeng Wang",
      "Hang Guo",
      "Tao Dai",
      "Bin Chen",
      "Shu-Tao Xia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7d60f19a8e5766910fd1798dc953869a-Abstract-Conference.html": {
    "title": "$\\textit{NeuroPath}$: A Neural Pathway Transformer for Joining the Dots of Human Connectomes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziquan Wei",
      "Tingting Dan",
      "Jiaqi Ding",
      "Guorong Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7d62a85ebfed2f680eb5544beae93191-Abstract-Conference.html": {
    "title": "FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xing Han",
      "Huy Nguyen",
      "Carl Harris",
      "Nhat Ho",
      "Suchi Saria"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7d6930fd71740eae21224a5ffb70cb8c-Abstract-Conference.html": {
    "title": "SLIM: Style-Linguistics Mismatch Model for Generalized Audio Deepfake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zhu",
      "Surya Koppisetti",
      "Trang Tran",
      "Gaurav Bharaj"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7d6e85e88495104442af94c98e899659-Abstract-Conference.html": {
    "title": "Long-Horizon Planning for Multi-Agent Robots in Partially Observable Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sid Nayak",
      "Adelmo Morrison Orozco",
      "Marina Have",
      "Jackson Zhang",
      "Vittal Thirumalai",
      "Darren Chen",
      "Aditya Kapoor",
      "Eric Robinson",
      "Karthik Gopalakrishnan",
      "James Harrison",
      "Anuj Mahajan",
      "Brian Ichter",
      "Hamsa Balakrishnan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7d7020e945935214d756cd9a65c43170-Abstract-Conference.html": {
    "title": "Abductive Reasoning in Logical Credal Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Radu Marinescu",
      "Junkyu Lee",
      "Debarun Bhattacharjya",
      "Fabio Cozman",
      "Alexander G. Gray"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7d90c28e7820709792d969211815a2b3-Abstract-Conference.html": {
    "title": "GSGAN: Adversarial Learning for Hierarchical Generation of 3D Gaussian Splats",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangeek Hyun",
      "Jae-Pil Heo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7db2ffcbfd0bd361d47b7fa612bd2ba2-Abstract-Conference.html": {
    "title": "Optimization Algorithm Design via Electric Circuits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen Boyd",
      "Tetiana Parshakova",
      "Ernest Ryu",
      "Jaewook J. Suh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7dbb5bfab324e3b86af9bd0df15498dd-Abstract-Conference.html": {
    "title": "iVideoGPT: Interactive VideoGPTs are Scalable World Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialong Wu",
      "Shaofeng Yin",
      "Ningya Feng",
      "Xu He",
      "Dong Li",
      "Jianye Hao",
      "Mingsheng Long"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7ddf0b49a46f57b8d7835e942c45f7fe-Abstract-Conference.html": {
    "title": "Smoothed Online Classification can be Harder than Batch Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinod Raman",
      "Unique Subedi",
      "Ambuj Tewari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7de665476d0adc8a54d3b8744f932bbf-Abstract-Conference.html": {
    "title": "Optimal Classification under Performative Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edwige Cyffers",
      "Muni Sreenivas Pydi",
      "Jamal Atif",
      "Olivier Cappé"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7e0af0d1bc0ec2a90fc294be2e00447e-Abstract-Conference.html": {
    "title": "Improving Robustness of 3D Point Cloud Recognition from a Fourier Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibo Miao",
      "Yinpeng Dong",
      "Jinlai Zhang",
      "Lijia Yu",
      "Xiao Yang",
      "Xiao-Shan Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7e0dc9ccba0f1333be13a3f9dc2b3138-Abstract-Conference.html": {
    "title": "Deep Graph Neural Networks via Posteriori-Sampling-based Node-Adaptative Residual Module",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingbo Zhou",
      "Yixuan Du",
      "Ruqiong Zhang",
      "Jun Xia",
      "Zhizhi Yu",
      "Zelin Zang",
      "Di Jin",
      "Carl Yang",
      "Rui Zhang",
      "Stan Z. Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7e1854dc0e983210dc12feb6c2016b98-Abstract-Conference.html": {
    "title": "John Ellipsoids via Lazy Updates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Woodruff",
      "Taisuke Yasuda"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7e3491e922bfd199ea34ecafeb7380f0-Abstract-Conference.html": {
    "title": "Inversion-based Latent Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewon Chu",
      "Jinyoung Park",
      "Seunghun Lee",
      "Hyunwoo J Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7e57131fdeb815764434b65162c88895-Abstract-Conference.html": {
    "title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yefei He",
      "Luoming Zhang",
      "Weijia Wu",
      "Jing Liu",
      "Hong Zhou",
      "Bohan Zhuang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7e670825a578392891ad40e93931b1e3-Abstract-Conference.html": {
    "title": "Policy Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parand A. Alamdari",
      "Soroush Ebadian",
      "Ariel D Procaccia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7e6f445a74cdb71931aac64f1e3f49c9-Abstract-Conference.html": {
    "title": "When is an Embedding Model More Promising than Another?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Darrin",
      "Philippe Formont",
      "Ismail Ayed",
      "Jackie CK Cheung",
      "Pablo Piantanida"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7e7b768198d24d883d69704eee57efb0-Abstract-Conference.html": {
    "title": "Sample-Efficient Constrained Reinforcement Learning with General Parameterization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Washim Mondal",
      "Vaneet Aggarwal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7e810b2c75d69be186cadd2fe3febeab-Abstract-Conference.html": {
    "title": "What Is Missing For Graph Homophily? Disentangling Graph Homophily For Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Zheng",
      "Sitao Luan",
      "Lihui Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7e83fd2ff7ae58485d418685521c9608-Abstract-Conference.html": {
    "title": "HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhao Cai",
      "Zihao Xiao",
      "Yixun Liang",
      "Minghan Qin",
      "Yulun Zhang",
      "Xiaokang Yang",
      "Yaoyao Liu",
      "Alan L. Yuille"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7e991aa4cd2fdf0014fba2f000f542d0-Abstract-Conference.html": {
    "title": "GraphMorph: Tubular Structure Extraction by Morphing Predicted Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhao Zhang",
      "Ziwei Zhao",
      "Dong Wang",
      "Liwei Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7ea46207ec9bda974b140fe11d8dd727-Abstract-Conference.html": {
    "title": "Revisiting Ensembling in One-Shot Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youssef Allouah",
      "Akash Dhasade",
      "Rachid Guerraoui",
      "Nirupam Gupta",
      "Anne-marie Kermarrec",
      "Rafael Pinot",
      "Rafael Pires",
      "Rishi Sharma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7eb6233e02f7d9efbb84acd839a996fb-Abstract-Conference.html": {
    "title": "Few-Shot Diffusion Models Escape the Curse of Dimensionality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruofeng Yang",
      "Bo Jiang",
      "Cheng Chen",
      "ruinan Jin",
      "Baoxiang Wang",
      "Shuai Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7eca17ef54789b0663cab421f2e9dbf5-Abstract-Conference.html": {
    "title": "Binocular-Guided 3D Gaussian Splatting with View Consistency for Sparse View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Han",
      "Junsheng Zhou",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7ed89d8e0b346994b064ba804443689e-Abstract-Conference.html": {
    "title": "Solving Zero-Sum Markov Games with Continuous State via Spectral Dynamic Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhao Zhou",
      "Zebang Shen",
      "zhang chao",
      "Hanbin Zhao",
      "Hui Qian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7ede97c3e082c6df10a8d6103a2eebd2-Abstract-Conference.html": {
    "title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jay Shah",
      "Ganesh Bikshandi",
      "Ying Zhang",
      "Vijay Thakkar",
      "Pradeep Ramani",
      "Tri Dao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7f0e8d7a5ac159b351c2433300fd67c6-Abstract-Conference.html": {
    "title": "Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Yuan",
      "Michael Maire"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7f0f24deb34c21ee590d8cece365710b-Abstract-Conference.html": {
    "title": "Group and Shuffle: Efficient Structured Orthogonal Parametrization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikhail Gorbunov",
      "Nikolay Yudin",
      "Vera Soboleva",
      "Aibek Alanov",
      "Alexey Naumov",
      "Maxim Rakhuba"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7f19b99e63762d20e9df91144056f1ee-Abstract-Conference.html": {
    "title": "Scaling transformer neural networks for skillful and reliable medium-range weather forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tung Nguyen",
      "Rohan Shah",
      "Hritik Bansal",
      "Troy Arcomano",
      "Romit Maulik",
      "Rao Kotamarthi",
      "Ian Foster",
      "Sandeep Madireddy",
      "Aditya Grover"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7f1f0218e45f5414c79c0679633e47bc-Abstract-Conference.html": {
    "title": "LLM Evaluators Recognize and Favor Their Own Generations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arjun Panickssery",
      "Samuel Bowman",
      "Shi Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7f2257d2b291b8d7e712c70b67e09412-Abstract-Conference.html": {
    "title": "SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyang Ma",
      "Kai Lu",
      "Ta-Ying Cheng",
      "Niki Trigoni",
      "Andrew Markham"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7f588e59e9ae6138d3ea9e4fdaa7e040-Abstract-Conference.html": {
    "title": "Reasons and Solutions for the Decline in Model Performance after Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiusheng Huang",
      "Jiaxiang Liu",
      "Yequan Wang",
      "Kang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7f5f9a88c6516469c83d074c6f2976fb-Abstract-Conference.html": {
    "title": "Changing the Training Data Distribution to Reduce Simplicity Bias Improves In-distribution Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dang Nguyen",
      "Paymon Haddad",
      "Eric Gan",
      "Baharan Mirzasoleiman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7f605d59a0dbde101518b552cb616ddf-Abstract-Conference.html": {
    "title": "P$^2$C$^2$Net: PDE-Preserved Coarse Correction Network for efficient prediction of spatiotemporal dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Wang",
      "Pu Ren",
      "Hao Zhou",
      "Xin-Yang Liu",
      "Zhiwen Deng",
      "Yi Zhang",
      "Zeruizhi Cheng",
      "Hongsheng Liu",
      "Zidong Wang",
      "Jian-Xun Wang",
      "Ji-Rong Wen",
      "Hao Sun",
      "Yang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7f64034009f4a5fa417a57e1a987c5cd-Abstract-Conference.html": {
    "title": "Approximation Rate of the Transformer Architecture for Sequence Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Jiang",
      "Qianxiao Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7f9220f90cc85b0da693643add6618e6-Abstract-Conference.html": {
    "title": "Consistency of Neural Causal Partial Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyuan Tan",
      "Jose Blanchet",
      "Vasilis Syrgkanis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7fa46657df480226112d5be3faf096c4-Abstract-Conference.html": {
    "title": "SpeAr: A Spectral Approach for Zero-Shot Node Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Guo",
      "Da Wang",
      "Jiye Liang",
      "Kaihan Zhang",
      "Jianchao Zeng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7fa5a377b7ffabcce43cd00231bb3f9c-Abstract-Conference.html": {
    "title": "On the Worst Prompt Performance of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Cao",
      "Deng Cai",
      "Zhisong Zhang",
      "Yuexian Zou",
      "Wai Lam"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7fc0aac0d700873ac520af9b05e3f2b6-Abstract-Conference.html": {
    "title": "Putting Gale & Shapley to Work: Guaranteeing Stability Through Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadi Hosseini",
      "Sanjukta Roy",
      "Duohan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7fc914993440219b64254e0c27964e11-Abstract-Conference.html": {
    "title": "Thinking Forward: Memory-Efficient Federated Finetuning of Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunjal Panchal",
      "Nisarg Parikh",
      "Sunav Choudhary",
      "Lijun Zhang",
      "Yuriy Brun",
      "Hui Guan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7ff7cdb97a08b2511107ee30ef4fb208-Abstract-Conference.html": {
    "title": "Bias Detection via Signaling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiling Chen",
      "Tao Lin",
      "Ariel D Procaccia",
      "Aaditya Ramdas",
      "Itai Shapira"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7ff97417474268e6b5a38bcbfae04944-Abstract-Conference.html": {
    "title": "Stress-Testing Capability Elicitation With Password-Locked Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Greenblatt",
      "Fabien Roger",
      "Dmitrii Krasheninnikov",
      "David Krueger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/7ffb43adf37b3eeaba559098bc084cc6-Abstract-Conference.html": {
    "title": "Instruction Tuning With Loss Over Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengxiang Shi",
      "Adam Yang",
      "Bin Wu",
      "Laurence Aitchison",
      "Emine Yilmaz",
      "Aldo Lipani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/803485352e61e3ebf41221e4776c9fd4-Abstract-Conference.html": {
    "title": "FACT or Fiction: Can Truthful Mechanisms Eliminate Federated Free Riding?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Bornstein",
      "Amrit Singh Bedi",
      "Abdirisak Mohamed",
      "Furong Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/80364062ec10e7bd976fc2d7a22730c3-Abstract-Conference.html": {
    "title": "Log-concave Sampling from a Convex Body with a Barrier: a Robust and Unified Dikin Walk",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhou Gu",
      "Nikki Lijing Kuang",
      "Yian Ma",
      "Zhao Song",
      "Lichen Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/804dbf8d3b8eee1ef875c6857efc64eb-Abstract-Conference.html": {
    "title": "United We Stand, Divided We Fall: Fingerprinting Deep Neural Networks via Adversarial Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianlong Xu",
      "Chen Wang",
      "Gaoyang Liu",
      "Yang Yang",
      "Kai Peng",
      "Wei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/806288e682d8a38c0bf21e37ab38af0a-Abstract-Conference.html": {
    "title": "Can Graph Neural Networks Expose Training Data Properties? An Efficient Risk Assessment Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyang Yuan",
      "Jiarong Xu",
      "Renhong Huang",
      "Mingli Song",
      "Chunping Wang",
      "YANG YANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8066ae1446b2bbccb5159587cc3b3bcc-Abstract-Conference.html": {
    "title": "Training Compute-Optimal Protein Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyi Cheng",
      "Bo Chen",
      "Pan Li",
      "Jing Gong",
      "Jie Tang",
      "Le Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8078e76f913e31b8467e85b4c0f0d22b-Abstract-Conference.html": {
    "title": "CALANet: Cheap All-Layer Aggregation for Human Activity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaegyun Park",
      "Dae-Won Kim",
      "Jaesung Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/80b3224c35ad857b951b8bfccfd32fa5-Abstract-Conference.html": {
    "title": "Over-parameterized Student Model via Tensor Decomposition Boosted Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Liang Zhan",
      "Zhong-Yi Lu",
      "Hao Sun",
      "Ze-Feng Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/80b7bec60081f95d900973509744a306-Abstract-Conference.html": {
    "title": "Qualitative Mechanism Independence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oliver Richardson",
      "Spencer J Peters",
      "Joseph Halpern"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/80cddcdd52c84d19b8b4a27a8e8c17d8-Abstract-Conference.html": {
    "title": "Tensor-Based Synchronization and the Low-Rankness of the Block Trifocal Tensor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Miao",
      "Gilad Lerman",
      "Joe Kileel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/80e354fdac2c7fbf439a51f4853edbac-Abstract-Conference.html": {
    "title": "DINTR: Tracking via Diffusion-based Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pha Nguyen",
      "Ngan Le",
      "Jackson Cothren",
      "Alper Yilmaz",
      "Khoa Luu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/80f48ffa8022773973a4a5cec7cce19c-Abstract-Conference.html": {
    "title": "Training-Free Open-Ended Object Detection and Segmentation via Attention as Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Lin",
      "Yongtao Wang",
      "Zhi Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/80f628f21b040cfd281b10628ba7e6c0-Abstract-Conference.html": {
    "title": "Multi-Agent Domain Calibration with a Handful of Offline Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Jiang",
      "Lei Yuan",
      "Lihe Li",
      "Cong Guan",
      "Zongzhang Zhang",
      "Yang Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/81166fbd9cc5adf14031cdb69d3fd6a8-Abstract-Conference.html": {
    "title": "No Representation, No Trust: Connecting Representation, Collapse, and Trust Issues in PPO",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Skander Moalla",
      "Andrea Miele",
      "Daniil Pyatko",
      "Razvan Pascanu",
      "Caglar Gulcehre"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8143b8c73073a9a23b9c18e400066471-Abstract-Conference.html": {
    "title": "The Best of Both Worlds: On the Dilemma of Out-of-distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyang Zhang",
      "Qiuxuan Feng",
      "Joey Tianyi Zhou",
      "Yatao Bian",
      "Qinghua Hu",
      "Changqing Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8147a43d030b43a01020774ae1d3e3bb-Abstract-Conference.html": {
    "title": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikayel Samvelyan",
      "Sharath Chandra Raparthy",
      "Andrei Lupu",
      "Eric Hambro",
      "Aram Markosyan",
      "Manish Bhatt",
      "Yuning Mao",
      "Minqi Jiang",
      "Jack Parker-Holder",
      "Jakob Foerster",
      "Tim Rocktäschel",
      "Roberta Raileanu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/81858558b55a8c63763cfe088090242a-Abstract-Conference.html": {
    "title": "Externally Valid Policy Evaluation from Randomized Trials Using Additional Observational Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sofia Ek",
      "Dave Zachariah"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8189d86a5d8dea0694d43bb90e01c14d-Abstract-Conference.html": {
    "title": "Semidefinite Relaxations of the Gromov-Wasserstein Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Chen",
      "Binh T. Nguyen",
      "Shang Koh",
      "Yong Sheng Soh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8198901c33d24a391b4c6b3f30f253ce-Abstract-Conference.html": {
    "title": "Exploring the Precise Dynamics of Single-Layer GAN Models: Leveraging Multi-Feature Discriminators for High-Dimensional Subspace Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Bond",
      "Zafer Dogan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/819977c0a95458911bbfd9e5b5115018-Abstract-Conference.html": {
    "title": "Understanding Visual Feature Reliance through the Lens of Complexity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Fel",
      "Louis Béthune",
      "Andrew Lampinen",
      "Thomas Serre",
      "Katherine Hermann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/81a60d18e010b27b36cd465c6604b915-Abstract-Conference.html": {
    "title": "VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiannan Wu",
      "Muyan Zhong",
      "Sen Xing",
      "Zeqiang Lai",
      "Zhaoyang Liu",
      "Zhe Chen",
      "Wenhai Wang",
      "Xizhou Zhu",
      "Lewei Lu",
      "Tong Lu",
      "Ping Luo",
      "Yu Qiao",
      "Jifeng Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/81b00efcbc755bd0b8dc6c0d15e9d0b1-Abstract-Conference.html": {
    "title": "Continual Learning with Global Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueying Bai",
      "Jinghuan Shang",
      "Yifan Sun",
      "Niranjan Balasubramanian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/81b8390039b7302c909cb769f8b6cd93-Abstract-Conference.html": {
    "title": "Self-Distilled Depth Refinement with Noisy Poisson Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Li",
      "Yiran Wang",
      "Jinghong Zheng",
      "Zihao Huang",
      "Ke Xian",
      "Zhiguo Cao",
      "Jianming Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/81c252b45b3bd7d9bf080eb27794b762-Abstract-Conference.html": {
    "title": "Scalable DP-SGD: Shuffling vs. Poisson Subsampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lynn Chua",
      "Badih Ghazi",
      "Pritish Kamath",
      "Ravi Kumar",
      "Pasin Manurangsi",
      "Amer Sinha",
      "Chiyuan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/81c565e605161fcf25d08aa230431eba-Abstract-Conference.html": {
    "title": "Improving Generalization of Dynamic Graph Learning via Environment Prompt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuo Yang",
      "Zhengyang Zhou",
      "Qihe Huang",
      "Limin Li",
      "Yuxuan Liang",
      "Yang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/81f19c0e9f3e06c831630ab6662fd8ea-Abstract-Conference.html": {
    "title": "Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "PENGHUI RUAN",
      "Pichao WANG",
      "Divya Saxena",
      "Jiannong Cao",
      "Yuhui Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/81f554467f27759e88de14ba2fbafb47-Abstract-Conference.html": {
    "title": "ProxyFusion: Face Feature Aggregation Through Sparse Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhavin Jawade",
      "Alexander Stone",
      "Deen Dayal Mohan",
      "Xiao Wang",
      "Srirangaraj Setlur",
      "Venu Govindaraju"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/820c61a0cd419163ccbd2c33b268816e-Abstract-Conference.html": {
    "title": "WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Tang",
      "Darren Key",
      "Kevin Ellis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/821a6e5681b072351fd3c21fac44739a-Abstract-Conference.html": {
    "title": "A Consistency-Aware Spot-Guided Transformer for Versatile and Hierarchical Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renlang Huang",
      "Yufan Tang",
      "Jiming Chen",
      "Liang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/821d5c03ff3bbb574c876bb6beef98b2-Abstract-Conference.html": {
    "title": "Perceptual Fairness in Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guy Ohayon",
      "Michael Elad",
      "Tomer Michaeli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/82240d93542b74d0c4fdffca39cb779f-Abstract-Conference.html": {
    "title": "Deterministic Uncertainty Propagation for Improved Model-Based Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdullah Akgül",
      "Manuel Haussmann",
      "Melih Kandemir"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/822621aaa88635437ea51023afdeaec2-Abstract-Conference.html": {
    "title": "Building a stable classifier with the inflated argmax",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jake Soloff",
      "Rina Barber",
      "Rebecca Willett"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/82389fbff376d1e8aec510916d50d054-Abstract-Conference.html": {
    "title": "Constrained Latent Action Policies for Model-Based Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marvin Alles",
      "Philip Becker-Ehmck",
      "Patrick van der Smagt",
      "Maximilian Karl"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/823e43f5537d8c1894afd1f6ab00a927-Abstract-Conference.html": {
    "title": "LaSe-E2V: Towards Language-guided Semantic-aware Event-to-Video Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kanghao Chen",
      "Hangyu Li",
      "Jiazhou Zhou",
      "Zeyu Wang",
      "Lin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8249b30d877c91611fd8c7aa6ac2b5fe-Abstract-Conference.html": {
    "title": "Predicting Label Distribution from Ternary Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunan Lu",
      "Xiuyi Jia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/824acc60310a838dd15565a2f5fbf376-Abstract-Conference.html": {
    "title": "VQ-Map: Bird's-Eye-View Map Layout Estimation in Tokenized Discrete Space via Vector Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Zhang",
      "Jin Gao",
      "Fudong Ge",
      "Guan Luo",
      "Bing Li",
      "ZHAO-XIANG ZHANG",
      "Haibin Ling",
      "Weiming Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/824f98c4e9bb301b0b96fa2ae071360b-Abstract-Conference.html": {
    "title": "Dual Defense: Enhancing Privacy and Mitigating Poisoning Attacks in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runhua Xu",
      "Shiqi Gao",
      "Chao Li",
      "James Joshi",
      "Jianxin Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/825341ab91db01bf063add41ac022702-Abstract-Conference.html": {
    "title": "Dual-Perspective Activation: Efficient Channel Denoising via Joint Forward-Backward Criterion for Artificial Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Qiu",
      "Chenchao Gao",
      "Zunlei Feng",
      "Jie Lei",
      "Bingde Hu",
      "Xingen Wang",
      "Yi Gao",
      "Mingli Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8258e815fa93aa59441ad526cc828c53-Abstract-Conference.html": {
    "title": "Provably Faster Algorithms for Bilevel Optimization via Without-Replacement Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Li",
      "Heng Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/82764461a05e933cc2fd9d312e107d12-Abstract-Conference.html": {
    "title": "Offline Multitask Representation Learning for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haque Ishfaq",
      "Thanh Nguyen-Tang",
      "Songtao Feng",
      "Raman Arora",
      "Mengdi Wang",
      "Ming Yin",
      "Doina Precup"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/827cb489449ea216e4a257c47e407d18-Abstract-Conference.html": {
    "title": "Performative Control for Linear Dynamical Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songfu Cai",
      "Fei Han",
      "Xuanyu Cao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/82844e428d9163a9f94830dc03af4f9c-Abstract-Conference.html": {
    "title": "Boosting Generalization in Parametric PDE Neural Solvers through Adaptive Conditioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armand Kassaï Koupaï",
      "Jorge Mifsut Benet",
      "Yuan Yin",
      "Jean-Noël Vittaut",
      "Patrick Gallinari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/82846e19e6d42ebfd4ace4361def29ae-Abstract-Conference.html": {
    "title": "Provably Optimal Memory Capacity for Modern Hopfield Models: Transformer-Compatible Dense Associative Memories as Spherical Codes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Dennis Wu",
      "Han Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/829300b183943850930c3ce36fe04268-Abstract-Conference.html": {
    "title": "Aligning Embeddings and Geometric Random Graphs: Informational Results and Computational Approaches for the Procrustes-Wasserstein Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathieu Even",
      "Luca Ganassali",
      "Jakob Maier",
      "Laurent Massoulié"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/82a9fb94035dad3ec007de4ad13c6748-Abstract-Conference.html": {
    "title": "Pin-Tuning: Parameter-Efficient In-Context Tuning for Few-Shot Molecular Property Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Wang",
      "Qiang Liu",
      "Shaozhen Liu",
      "Xin Sun",
      "Shu Wu",
      "Liang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/82acbbc04435f6c1e7f656b1cbe4ad82-Abstract-Conference.html": {
    "title": "Improving Context-Aware Preference Modeling for Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Silviu Pitis",
      "Ziang Xiao",
      "Nicolas Le Roux",
      "Alessandro Sordoni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/82c1fee06391ef7c22b108b8611cf266-Abstract-Conference.html": {
    "title": "Almost Free: Self-concordance in Natural Exponential Families and an Application to Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Liu",
      "Alex Ayoub",
      "Flore Sentenac",
      "Xiaoqi Tan",
      "Csaba Szepesvari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/82eac7050fb44b662062ac64aa6637c3-Abstract-Conference.html": {
    "title": "Neural Cover Selection for Image Steganography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karl Chahine",
      "Hyeji Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/82f05a105c928c10706213952bf0c8b7-Abstract-Conference.html": {
    "title": "Counter-Current Learning: A Biologically Plausible Dual Network Approach for Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chia-Hsiang Kao",
      "Bharath Hariharan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/82f68b38747c406672f7f9f6bab86775-Abstract-Conference.html": {
    "title": "Faster Differentially Private Top-$k$ Selection: A Joint Exponential Mechanism with Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao WU",
      "Hanwen Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8309b3636e1a27c1003a6ae4a93a0793-Abstract-Conference.html": {
    "title": "A Tractable Inference Perspective of Offline RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuejie Liu",
      "Anji Liu",
      "Guy Van den Broeck",
      "Yitao Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/830c927e4e9c9a457d476c145514fb56-Abstract-Conference.html": {
    "title": "Adaptive Labeling for Efficient Out-of-distribution Model Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daksh Mittal",
      "Yuanzhe Ma",
      "Shalmali Joshi",
      "Hongseok Namkoong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/831406cfe7e4a0aed5ac5c8a8389d1f5-Abstract-Conference.html": {
    "title": "The Ladder in Chaos: Improving Policy Learning by Harnessing the Parameter Evolving Path in A Low-dimensional Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyao Tang",
      "Min Zhang",
      "Chen Chen",
      "Jianye Hao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8329a45669017898bb0cc09d27f8d2bb-Abstract-Conference.html": {
    "title": "MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YUHONG CHOU",
      "Man Yao",
      "Kexin Wang",
      "Yuqi Pan",
      "Rui-Jie Zhu",
      "Jibin Wu",
      "Yiran Zhong",
      "Yu Qiao",
      "Bo Xu",
      "Guoqi Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/832b20b65f655587e9c0447860406a82-Abstract-Conference.html": {
    "title": "2DQuant: Low-bit Post-Training Quantization for Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Liu",
      "Haotong Qin",
      "Yong Guo",
      "Xin Yuan",
      "Linghe Kong",
      "Guihai Chen",
      "Yulun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/833295cb9278a3ba973842a94ea68e3c-Abstract-Conference.html": {
    "title": "Using Surrogates in Covariate-adjusted Response-adaptive Randomization Experiments with Delayed Outcomes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Shi",
      "Waverly Wei",
      "Jingshen Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8337b176767d948275d0b6d17cf4221e-Abstract-Conference.html": {
    "title": "Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye He",
      "Kevin Rojas",
      "Molei Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/833b21da1956c6b92f6df253bf655cf5-Abstract-Conference.html": {
    "title": "MambaAD: Exploring State Space Models for Multi-class Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyang He",
      "Yuhu Bai",
      "Jiangning Zhang",
      "Qingdong He",
      "Hongxu Chen",
      "Zhenye Gan",
      "Chengjie Wang",
      "Xiangtai Li",
      "Guanzhong Tian",
      "Lei Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8340b085045cf13f1f0b6c2c4cc0a89c-Abstract-Conference.html": {
    "title": "Can LLMs Learn by Teaching for Better Reasoning? A Preliminary Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuefei Ning",
      "Zifu Wang",
      "Shiyao Li",
      "Zinan Lin",
      "Peiran Yao",
      "Tianyu Fu",
      "Matthew Blaschko",
      "Guohao Dai",
      "Huazhong Yang",
      "Yu Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/834d4d24d223e5dc05f9971eaf30d4a6-Abstract-Conference.html": {
    "title": "The Star Geometry of Critic-Based Regularizer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oscar Leong",
      "Eliza O&#x27;Reilly",
      "Yong Sheng Soh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/834fe9eecf8d1839f406a79c6982be05-Abstract-Conference.html": {
    "title": "Even Sparser Graph Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamed Shirzad",
      "Honghao Lin",
      "Balaji Venkatachalam",
      "Ameya Velingker",
      "David Woodruff",
      "Danica J. Sutherland"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/835a0185f61867a1ea0f86155489839a-Abstract-Conference.html": {
    "title": "Large Stepsize Gradient Descent for Non-Homogeneous Two-Layer Networks: Margin Improvement and Fast Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Cai",
      "Jingfeng Wu",
      "Song Mei",
      "Michael Lindsey",
      "Peter L. Bartlett"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8364ad56291e22ec6041794355a46802-Abstract-Conference.html": {
    "title": "Taming \"data-hungry\" reinforcement learning? Stability in continuous state-action spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaqi Duan",
      "Martin J. Wainwright"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/836cf992a71f7a0bda218c180f942902-Abstract-Conference.html": {
    "title": "Risk-sensitive control as inference with Rényi divergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaito Ito",
      "Kenji Kashima"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/837b396039248acb08c385bebb6291b4-Abstract-Conference.html": {
    "title": "Adaptive Domain Learning for Cross-domain Image Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zian Qian",
      "Chenyang Qi",
      "Ka Law",
      "Hao Fu",
      "Chenyang Lei",
      "Qifeng Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/837bc5db12f3d394d220815a7687340c-Abstract-Conference.html": {
    "title": "On the Inductive Bias of Stacking Towards Improving Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikunj Saunshi",
      "Stefani Karp",
      "Shankar Krishnan",
      "Sobhan Miryoosefi",
      "Sashank Jakkam Reddi",
      "Sanjiv Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8395fdf356059eaa92afd39e3952a677-Abstract-Conference.html": {
    "title": "All-in-One Image Coding for Joint Human-Machine Vision with Multi-Path Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Zhang",
      "Peiyao Guo",
      "Ming Lu",
      "Zhan Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/83960718b4d12f799985206f1b1cf00f-Abstract-Conference.html": {
    "title": "FouRA: Fourier Low-Rank Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubhankar Borse",
      "Shreya Kadambi",
      "Nilesh Pandey",
      "Kartikeya Bhardwaj",
      "Viswanath Ganapathy",
      "Sweta Priyadarshi",
      "Risheek Garrepalli",
      "Rafael Esteves",
      "Munawar Hayat",
      "Fatih Porikli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/83a4ea71b13bc86308a2bd0b5e07fb61-Abstract-Conference.html": {
    "title": "Learning to Assist Humans without Inferring Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivek Myers",
      "Evan Ellis",
      "Sergey Levine",
      "Benjamin Eysenbach",
      "Anca Dragan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/83b7da3ed13f06c13ce82235c8eedf35-Abstract-Conference.html": {
    "title": "Are Your Models Still Fair? Fairness Attacks on Graph Neural Networks via Node Injections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Luo",
      "Hong Huang",
      "Yongkang Zhou",
      "Jiping Zhang",
      "Nuo Chen",
      "Hai Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/83ba7056bce2c3c3c27e17397cf3e1f0-Abstract-Conference.html": {
    "title": "Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ching-An Cheng",
      "Allen Nie",
      "Adith Swaminathan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/83ce241ce40aef32225eb2833ca2363c-Abstract-Conference.html": {
    "title": "Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arvind Vepa",
      "Zukang Yang",
      "Andrew Choi",
      "Jungseock Joo",
      "Fabien Scalzo",
      "Yizhou Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/83e77607638c4fb17fba4a9b7844800c-Abstract-Conference.html": {
    "title": "Great Minds Think Alike: The Universal Convergence Trend of Input Salience",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yipei Wang",
      "Jeffrey M. Siskind",
      "Xiaoqian Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/83eb339ed42297658fa24b5cec939285-Abstract-Conference.html": {
    "title": "Marrying Causal Representation Learning with Dynamical Systems for Science",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingling Yao",
      "Caroline Muller",
      "Francesco Locatello"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/83eb86be3e2f9fd66c44d9073c51ba4d-Abstract-Conference.html": {
    "title": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Zhang",
      "Xiangtai Li",
      "Hao Fei",
      "Haobo Yuan",
      "Shengqiong Wu",
      "Shunping Ji",
      "Chen Change Loy",
      "Shuicheng Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/83f95bb0ac5046338ea2afe3390e9f4b-Abstract-Conference.html": {
    "title": "CorDA: Context-Oriented Decomposition Adaptation of Large Language Models for Task-Aware Parameter-Efficient Fine-tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibo Yang",
      "Xiaojie Li",
      "Zhongzhu Zhou",
      "Shuaiwen Song",
      "Jianlong Wu",
      "Liqiang Nie",
      "Bernard Ghanem"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8402cf3031d649066ada24514739f0dd-Abstract-Conference.html": {
    "title": "Neural Concept Binder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wolfgang Stammer",
      "Antonia Wüst",
      "David Steinmann",
      "Kristian Kersting"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/840abfadd04c967feaa2a49aba94a32d-Abstract-Conference.html": {
    "title": "Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuezhe Ma",
      "Xiaomeng Yang",
      "Wenhan Xiong",
      "Beidi Chen",
      "LILI YU",
      "Hao Zhang",
      "Jonathan May",
      "Luke Zettlemoyer",
      "Omer Levy",
      "Chunting Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/842714f78c95096e20ac7d2591c5a24b-Abstract-Conference.html": {
    "title": "Boosted Conformal Prediction Intervals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ran Xie",
      "Rina Barber",
      "Emmanuel Candes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/84409c45a0defe347c895b004b1c675b-Abstract-Conference.html": {
    "title": "Enriching Disentanglement: From Logical Definitions to Quantitative Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yivan Zhang",
      "Masashi Sugiyama"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8443219a991f068c34d9491ad68ffa94-Abstract-Conference.html": {
    "title": "Active preference learning for ordering items in- and out-of-sample",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Herman Bergström",
      "Emil Carlsson",
      "Devdatt Dubhashi",
      "Fredrik D Johansson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8451a20c5a7e0ee5671dda28f7daf7f3-Abstract-Conference.html": {
    "title": "Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiqu Liang",
      "Zixu Zhang",
      "Jaime Fisac"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/848350afc07c02edea5be06d3390d865-Abstract-Conference.html": {
    "title": "Can Transformers Smell Like Humans?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farzaneh Taleb",
      "Miguel Vasco",
      "Antonio Ribeiro",
      "Mårten Björkman",
      "Danica Kragic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/848784373188ddf641079524e89e0ac9-Abstract-Conference.html": {
    "title": "Universal In-Context Approximation By Prompting Fully Recurrent Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandar Petrov",
      "Tom Lamb",
      "Alasdair Paren",
      "Philip Torr",
      "Adel Bibi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8488454077cb0fd9d31772274c78115d-Abstract-Conference.html": {
    "title": "TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrei Margeloiu",
      "Xiangjian Jiang",
      "Nikola Simidjievski",
      "Mateja Jamnik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8492211e9176b8abdaeb1f7aa4c223ea-Abstract-Conference.html": {
    "title": "Training an Open-Vocabulary Monocular 3D Detection Model without 3D Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Huang",
      "Henry Zheng",
      "Yan Wang",
      "Zhuofan Xia",
      "Marco Pavone",
      "Gao Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/849b84c0038e5856f2887e5bfe6ced41-Abstract-Conference.html": {
    "title": "Automatically Learning Hybrid Digital Twins of Dynamical Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Holt",
      "Tennison Liu",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/84a7fc24ed52e8eff514c33e8ac76ea3-Abstract-Conference.html": {
    "title": "DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangqian Gao",
      "Chi-Heng Lin",
      "Ting Hua",
      "Zheng Tang",
      "Yilin Shen",
      "Hongxia Jin",
      "Yen-Chang Hsu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/84bad835faaf48f24d990072bb5b80ee-Abstract-Conference.html": {
    "title": "EEG2Video: Towards Decoding Dynamic Visual Perception from EEG Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan-Hao Liu",
      "Yan-Kai Liu",
      "Yansen Wang",
      "Kan Ren",
      "Hanwen Shi",
      "Zilong Wang",
      "Dongsheng Li",
      "Bao-Liang Lu",
      "Wei-Long Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/84c6c76526783f4afd82687829d3aa97-Abstract-Conference.html": {
    "title": "Physics-informed Neural Networks for Functional Differential Equations: Cylindrical Approximation and Its Convergence Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taiki Miyagawa",
      "Takeru Yokota"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/84ca3f2d9d9bfca13f69b48ea63eb4a5-Abstract-Conference.html": {
    "title": "PPLNs: Parametric Piecewise Linear Networks for Event-Based Temporal Modeling and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Song",
      "Zhenxiao Liang",
      "Bo Sun",
      "Qixing Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/84d286e32bbee8fa3a86ee9c50e00081-Abstract-Conference.html": {
    "title": "ADOPT: Modified Adam Can Converge with Any $\\beta_2$ with the Optimal Rate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shohei Taniguchi",
      "Keno Harada",
      "Gouki Minegishi",
      "Yuta Oshima",
      "Seong Cheol Jeong",
      "Go Nagahara",
      "Tomoshi Iiyama",
      "Masahiro Suzuki",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/84d395725a9b40cb4a49d84478ac24c7-Abstract-Conference.html": {
    "title": "Tighter Convergence Bounds for Shuffled SGD via Primal-Dual Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xufeng Cai",
      "Cheuk Yin Lin",
      "Jelena Diakonikolas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/84e1b1ec17bb11c57234e96433022a9a-Abstract-Conference.html": {
    "title": "Poseidon: Efficient Foundation Models for PDEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Herde",
      "Bogdan Raonic",
      "Tobias Rohner",
      "Roger Käppeli",
      "Roberto Molinaro",
      "Emmanuel de Bezenac",
      "Siddhartha Mishra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/84f44b36ceb4fbc9bb269959f4796eed-Abstract-Conference.html": {
    "title": "Integrating GNN and Neural ODEs for Estimating Non-Reciprocal Two-Body Interactions in Mixed-Species Collective Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masahito Uwamichi",
      "Simon Schnyder",
      "Tetsuya J. Kobayashi",
      "Satoshi Sawai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/850d6cd6cca1398d9251e5ae870fad0e-Abstract-Conference.html": {
    "title": "Accelerating ERM for data-driven algorithm design using output-sensitive techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria-Florina F Balcan",
      "Christopher Seiler",
      "Dravyansh Sharma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/850e8063d902e0825d3c5504d183bafe-Abstract-Conference.html": {
    "title": "Decision Mamba: Reinforcement Learning via Hybrid Selective Sequence Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sili Huang",
      "Jifeng Hu",
      "Zhejian Yang",
      "Liwei Yang",
      "Tao Luo",
      "Hechang Chen",
      "Lichao Sun",
      "Bo Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8523a98265ceae12afd34113aa6c5cca-Abstract-Conference.html": {
    "title": "Dissecting the Interplay of Attention Paths in a Statistical Mechanics Theory of Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Tiberi",
      "Francesca Mignacco",
      "Kazuki Irie",
      "Haim Sompolinsky"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/852f50969a9e523ec41d26f2f68bd456-Abstract-Conference.html": {
    "title": "Does Worst-Performing Agent Lead the Pack? Analyzing Agent Dynamics in Unified Distributed SGD",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Hu",
      "Yi-Ting Ma",
      "Do-Young Eun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/853e781cb2af58956ed5c89aa59da3fc-Abstract-Conference.html": {
    "title": "Generative Retrieval Meets Multi-Graded Relevance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yubao Tang",
      "Ruqing Zhang",
      "Jiafeng Guo",
      "Maarten Rijke",
      "Wei Chen",
      "Xueqi Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/85529bc995777a74072ef63c05bedd30-Abstract-Conference.html": {
    "title": "Order-Independence Without Fine Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reid McIlroy-Young",
      "Katrina Brown",
      "Conlan Olson",
      "Linjun Zhang",
      "Cynthia Dwork"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8555cf308efef02adacb0f9e09bd5f10-Abstract-Conference.html": {
    "title": "SeTAR: Out-of-Distribution Detection with Selective Low-Rank Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixia Li",
      "Boya Xiong",
      "Guanhua Chen",
      "Yun Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/85826ad1eb4602a2962b7cdbe129b341-Abstract-Conference.html": {
    "title": "Efficient Availability Attacks against Supervised and Contrastive Learning Simultaneously",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Wang",
      "Yifan Zhu",
      "Xiao-Shan Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/858e773f5c99a4186b548f3f531fb07e-Abstract-Conference.html": {
    "title": "Safe Exploitative Play with Untrusted Type Beliefs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongxin Li",
      "Tinashe Handina",
      "Shaolei Ren",
      "Adam Wierman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/858fc542b70d3b39067f7d3b1cd77635-Abstract-Conference.html": {
    "title": "Compositional Automata Embeddings for Goal-Conditioned Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beyazit Yalcinkaya",
      "Niklas Lauffer",
      "Marcell Vazquez-Chanlatte",
      "Sanjit Seshia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/859358cace308ff7bae91ed477e9ed35-Abstract-Conference.html": {
    "title": "Improved Bayes Regret Bounds for Multi-Task Hierarchical Bayesian Bandit Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiechao Guan",
      "Hui Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/85970f7bbc821852c1d17052b88c2451-Abstract-Conference.html": {
    "title": "Pure Message Passing Can Estimate Common Neighbor for Link Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Dong",
      "Zhichun Guo",
      "Nitesh Chawla"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/859b6564b04959833fdf52ae6f726f84-Abstract-Conference.html": {
    "title": "Bridging Multicalibration and Out-of-distribution Generalization Beyond Covariate Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayun Wu",
      "Jiashuo Liu",
      "Peng Cui",
      "Steven Z. Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/85be6406bc3b93649a12b4074100c00b-Abstract-Conference.html": {
    "title": "Towards Stable Representations for Protein Interface Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Gao",
      "Zijing Liu",
      "Yu Li",
      "Jia Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/85db52cc08c5e00cfb1d216b1c85ba35-Abstract-Conference.html": {
    "title": "Are Graph Neural Networks Optimal Approximation Algorithms?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Morris Yau",
      "Nikolaos Karalias",
      "Eric Lu",
      "Jessica Xu",
      "Stefanie Jegelka"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/85dbd2fb8b355e4231b51e454c08ec1c-Abstract-Conference.html": {
    "title": "Symmetries in Overparametrized Neural Networks: A Mean Field View",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javier Maass",
      "Joaquin Fontbona"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/85dca095c00c71adb23e76ab3e1038ea-Abstract-Conference.html": {
    "title": "Physical Consistency Bridges Heterogeneous Data in Molecular Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Ren",
      "Dihan Zheng",
      "Chang Liu",
      "Peiran Jin",
      "Yu Shi",
      "Lin Huang",
      "Jiyan He",
      "Shengjie Luo",
      "Tao Qin",
      "Tie-Yan Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/85f1225db986e629289f402c46eff1a4-Abstract-Conference.html": {
    "title": "OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Wang",
      "Shaofei Cai",
      "Zhancun Mu",
      "Haowei Lin",
      "Ceyao Zhang",
      "Xuejie Liu",
      "Qing Li",
      "Anji Liu",
      "Xiaojian (Shawn) Ma",
      "Yitao Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8600a9df1a087a9a66900cc8c948c3f0-Abstract-Conference.html": {
    "title": "State Chrono Representation for Enhancing Generalization in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianda Chen",
      "Wen zheng terence Ng",
      "Zichen Chen",
      "Sinno Pan",
      "Tianwei Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/86040ae1ecc64655bdbdfbbf774ead26-Abstract-Conference.html": {
    "title": "On Affine Homotopy between Language Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robin Chan",
      "Reda Boumasmoud",
      "Anej Svete",
      "Yuxin Ren",
      "Qipeng Guo",
      "Zhijing Jin",
      "Shauli Ravfogel",
      "Mrinmaya Sachan",
      "Bernhard Schölkopf",
      "Mennatallah El-Assady",
      "Ryan Cotterell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/860c1c657deafe09f64c013c2888bd7b-Abstract-Conference.html": {
    "title": "Self-Play Fine-tuning of Diffusion Models for Text-to-image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huizhuo Yuan",
      "Zixiang Chen",
      "Kaixuan Ji",
      "Quanquan Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/860e5b214c842eaedaa6b4026ee91aac-Abstract-Conference.html": {
    "title": "Graph Edit Distance with General Costs Using Neural Set Divergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eeshaan Jain",
      "Indradyumna Roy",
      "Saswat Meher",
      "Soumen Chakrabarti",
      "Abir De"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/861f7dad098aec1c3560fb7add468d41-Abstract-Conference.html": {
    "title": "Secret Collusion among AI Agents: Multi-Agent Deception via Steganography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumeet Motwani",
      "Mikhail Baranchuk",
      "Martin Strohmeier",
      "Vijay Bolina",
      "Philip Torr",
      "Lewis Hammond",
      "Christian Schroeder de Witt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8629b0fff229b8a27efb1422e990605f-Abstract-Conference.html": {
    "title": "Scalable Optimization in the Modular Norm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Large",
      "Yang Liu",
      "Jacob Huh",
      "Hyojin Bahng",
      "Phillip Isola",
      "Jeremy Bernstein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/86655bc516148e311bcfcf88f1744de7-Abstract-Conference.html": {
    "title": "Solving Inverse Problems via Diffusion Optimal Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henry Li",
      "Marcus Pereira"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8678da90126aa58326b2fc0254b33a8c-Abstract-Conference.html": {
    "title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Terufumi Morishita",
      "Gaku Morio",
      "Atsuki Yamaguchi",
      "Yasuhiro Sogawa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/869bfd807a513755bef25e3896a19a21-Abstract-Conference.html": {
    "title": "Optimal Aggregation of Prediction Intervals under Unsupervised Domain Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Ge",
      "Debarghya Mukherjee",
      "Jianqing Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/86a11cc08e221cfef301dabc735bbe92-Abstract-Conference.html": {
    "title": "Realizable $H$-Consistent and Bayes-Consistent Loss Functions for Learning to Defer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Mao",
      "Mehryar Mohri",
      "Yutao Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/86ab6927ee4ae9bde4247793c46797c7-Abstract-Conference.html": {
    "title": "GeoNLF: Geometry guided Pose-Free Neural LiDAR Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyi Xue",
      "Zehan Zheng",
      "Fan Lu",
      "Haiyun Wei",
      "Guang Chen",
      "changjun jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/86b3697c4eb7792c951831636bfdacd5-Abstract-Conference.html": {
    "title": "Gradual Domain Adaptation via Manifold-Constrained Distributionally Robust Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirhossein Saberi",
      "Amir Najafi",
      "Amin Behjati",
      "Ala Emrani",
      "Yasaman Zolfit",
      "Mahdi Shadrooy",
      "Abolfazl Motahari",
      "Babak H. Khalaj"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/86c1fd74fa25bd6be0072937803e0bd1-Abstract-Conference.html": {
    "title": "Efficient Leverage Score Sampling for Tensor Train Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivek Bharadwaj",
      "Beheshteh Toloueirakhshan",
      "Osman Asif Malik",
      "Guillaume Rabusseau"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/86c283920335ed1fec3edee227e05fbf-Abstract-Conference.html": {
    "title": "Distributed Least Squares in Small Space via Sketching and Bias Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sachin Garg",
      "Kevin Tan",
      "Michal Derezinski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/86c9df30129f7663ad4d429b6f80d461-Abstract-Conference.html": {
    "title": "Aligning to Thousands of Preferences via System Message Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongyun Lee",
      "Sue Hyun Park",
      "Seungone Kim",
      "Minjoon Seo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/86cbfa52bb97fbeb3df53f300d7df870-Abstract-Conference.html": {
    "title": "On the Expressive Power of Tree-Structured Probabilistic Circuits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lang Yin",
      "Han Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/86fe62e3b315d2578721562d9fd1a433-Abstract-Conference.html": {
    "title": "Hybrid Mamba for Few-Shot Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianxiong Xu",
      "Xuanyi Liu",
      "Lanyun Zhu",
      "Guosheng Lin",
      "Cheng Long",
      "Ziyue Li",
      "Rui Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/870ccde24673d3970a680bb48496ed63-Abstract-Conference.html": {
    "title": "Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Gui",
      "Ying Jin",
      "Zhimei Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8716aa6a02bcc3c8e69a3a42be192236-Abstract-Conference.html": {
    "title": "Time-Varying LoRA: Towards Effective Cross-Domain Fine-Tuning of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhan Zhuang",
      "Yulong Zhang",
      "Xuehao Wang",
      "Jiangang Lu",
      "Ying Wei",
      "Yu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/871a8ccb9232487366feb5e2d9069915-Abstract-Conference.html": {
    "title": "Neural Flow Diffusion Models: Learnable Forward Process for Improved Diffusion Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Grigory Bartosh",
      "Dmitry P Vetrov",
      "Christian Andersson Naesseth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/871ed095b734818cfba48db6aeb25a62-Abstract-Conference.html": {
    "title": "Efficient Large Multi-modal Models via Visual Context Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieneng Chen",
      "Luoxin Ye",
      "Ju He",
      "Zhaoyang Wang",
      "Daniel Khashabi",
      "Alan L. Yuille"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8735e0793cfd43327eceaacf39466a01-Abstract-Conference.html": {
    "title": "SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taisuke Yasuda",
      "Kyriakos Axiotis",
      "Gang Fu",
      "Mohammadhossein Bateni",
      "Vahab Mirrokni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/873af132332ac0be6d64e9ceb8967014-Abstract-Conference.html": {
    "title": "Contrastive dimension reduction: when and how?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sam Hawke",
      "YueEn Ma",
      "Didong Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/873c86d9a979ab80d8e2919510d4446b-Abstract-Conference.html": {
    "title": "Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiansheng Huang",
      "Sihao Hu",
      "Ling Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/874067cee9c98dc9b9980fa6ef70176a-Abstract-Conference.html": {
    "title": "Efficient Lifelong Model Evaluation in an Era of Rapid Progress",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ameya Prabhu",
      "Vishaal Udandarao",
      "Philip Torr",
      "Matthias Bethge",
      "Adel Bibi",
      "Samuel Albanie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/874411a224a1934b80d499068384808b-Abstract-Conference.html": {
    "title": "Membership Inference on Text-to-Image Diffusion Models via Conditional Likelihood Discrepancy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengfang ZHAI",
      "Huanran Chen",
      "Yinpeng Dong",
      "Jiajun Li",
      "Qingni Shen",
      "Yansong Gao",
      "Hang Su",
      "Yang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/874a4d89f2d04b4bcf9a2c19545cf040-Abstract-Conference.html": {
    "title": "Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vijay Ekambaram",
      "Arindam Jati",
      "Pankaj Dayama",
      "Sumanta Mukherjee",
      "Nam Nguyen",
      "Wesley M Gifford",
      "Chandra Reddy",
      "Jayant Kalagnanam"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8754ad68d557e633e5ce9dd02781e9cc-Abstract-Conference.html": {
    "title": "Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gunshi Gupta",
      "Karmesh Yadav",
      "Yarin Gal",
      "Dhruv Batra",
      "Zsolt Kira",
      "Cong Lu",
      "Tim G. J. Rudner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/87571720167f7e88827c40e468e3101f-Abstract-Conference.html": {
    "title": "TALoS: Enhancing Semantic Scene Completion via Test-time Adaptation on the Line of Sight",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyun-Kurl Jang",
      "Jihun Kim",
      "Hyeokjun Kweon",
      "Kuk-Jin Yoon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8766fbc68e1ed1cdef712ce273e0a363-Abstract-Conference.html": {
    "title": "UMB: Understanding Model Behavior for Open-World Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xing Xi",
      "Yangyang Huang",
      "Zhijie Zhong",
      "Ronghua Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/87726969ce38e9a676ca1fd4459ba77d-Abstract-Conference.html": {
    "title": "Unity by Diversity: Improved Representation Learning for Multimodal VAEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Sutter",
      "Yang Meng",
      "Andrea Agostini",
      "Daphné Chopard",
      "Norbert Fortin",
      "Julia Vogt",
      "Babak Shahbaba",
      "Stephan Mandt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8773cdaf02c5af3528e05f1cee816129-Abstract-Conference.html": {
    "title": "Relationship Prompt Learning is Enough for Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Li",
      "Yang Lu",
      "Yuan Xie",
      "Yanyun Qu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/879c5890a9d2ecdcb590c9674cda4a59-Abstract-Conference.html": {
    "title": "Mitigating Spurious Correlations via Disagreement Probability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonggeun Han",
      "Sehwan Kim",
      "Hyungjun Joo",
      "Sangwoo Hong",
      "Jungwoo Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/87affd2029375d1be123ccdab5334c55-Abstract-Conference.html": {
    "title": "HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Panwang Pan",
      "Zhuo Su",
      "Chenguo Lin",
      "Zhen Fan",
      "Yongjie Zhang",
      "Zeming Li",
      "Tingting Shen",
      "Yadong Mu",
      "Yebin Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/87be61bf9338389702712f5e9754a986-Abstract-Conference.html": {
    "title": "SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert Csordas",
      "Piotr Piękos",
      "Kazuki Irie",
      "Jürgen Schmidhuber"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/87ccd80753f787e81d4c8da135385b4e-Abstract-Conference.html": {
    "title": "FastDrag: Manipulate Anything in One Step",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanjia Zhao",
      "Jian Guan",
      "Congyi Fan",
      "Dongli Xu",
      "Youtian Lin",
      "Haiwei Pan",
      "Pengming Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/87ee1bbac4635e7c948f3eea83c1f262-Abstract-Conference.html": {
    "title": "Decoupled Kullback-Leibler Divergence Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiequan Cui",
      "Zhuotao Tian",
      "Zhisheng Zhong",
      "Xiaojuan Qi",
      "Bei Yu",
      "Hanwang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8815c983a1f46b477fe4fbf7042a3ba3-Abstract-Conference.html": {
    "title": "Small steps no more: Global convergence of stochastic gradient bandits for arbitrary learning rates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jincheng Mei",
      "Bo Dai",
      "Alekh Agarwal",
      "Sharan Vaswani",
      "Anant Raj",
      "Csaba Szepesvari",
      "Dale Schuurmans"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8829f586a1ac0e6c41143f5d57b63c4b-Abstract-Conference.html": {
    "title": "Oja's Algorithm for Streaming Sparse PCA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Syamantak Kumar",
      "Purnamrita Sarkar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/883105b282fe15275991b411e6b200c5-Abstract-Conference.html": {
    "title": "Fair and Welfare-Efficient Constrained Multi-Matchings under Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elita Lobo",
      "Justin Payan",
      "Cyrus Cousins",
      "Yair Zick"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8839556afa4efe2a7dc5aae5e0a22fd4-Abstract-Conference.html": {
    "title": "Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao-Lun Hsu",
      "Weixin Wang",
      "Miroslav Pajic",
      "Pan Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/883f66687a521536c505f9b2fbdcbf1e-Abstract-Conference.html": {
    "title": "Stochastic Kernel Regularisation Improves Generalisation in Deep Kernel Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edward Milsom",
      "Ben Anson",
      "Laurence Aitchison"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8861183dd25469d5706db9731aa86e49-Abstract-Conference.html": {
    "title": "S-MolSearch: 3D Semi-supervised Contrastive Learning for Bioactive Molecule Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gengmo Zhou",
      "Zhen Wang",
      "Feng Yu",
      "Guolin Ke",
      "Zhewei Wei",
      "Zhifeng Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8874a21bfb43738f5af2eb348f2bc693-Abstract-Conference.html": {
    "title": "Gaussian Process Bandits for Top-k Recommendations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohit Yadav",
      "Cameron Musco",
      "Daniel R. Sheldon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/88a129e44f25a571ae8b838057c46855-Abstract-Conference.html": {
    "title": "Universal Rates for Active Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steve Hanneke",
      "Amin Karbasi",
      "Shay Moran",
      "Grigoris Velegkas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/88ad9774ffcb7a272457e9396f793a07-Abstract-Conference.html": {
    "title": "Full-Atom Peptide Design with Geometric Latent Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangzhe Kong",
      "Yinjun Jia",
      "Wenbing Huang",
      "Yang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/88be023075a5a3ff3dc3b5d26623fa22-Abstract-Conference.html": {
    "title": "ReF-LDM: A Latent Diffusion Model for Reference-based Face Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi-Wei Hsiao",
      "Yu-Lun Liu",
      "Cheng-Kun Yang",
      "Sheng-Po Kuo",
      "Kevin Jou",
      "Chia-Ping Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/88c2fb9ae705e143c99156ba37926837-Abstract-Conference.html": {
    "title": "The Implicit Bias of Gradient Descent toward Collaboration between Layers: A Dynamic Analysis of Multilayer Perceptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Wang",
      "Geyong Min",
      "Wenjie Ruan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/88dd7aa6979e352fda7c4952ca8eac59-Abstract-Conference.html": {
    "title": "TableRAG: Million-Token Table Understanding with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Si-An Chen",
      "Lesly Miculicich",
      "Julian Eisenschlos",
      "Zifeng Wang",
      "Zilong Wang",
      "Yanfei Chen",
      "YASUHISA FUJII",
      "Hsuan-Tien Lin",
      "Chen-Yu Lee",
      "Tomas Pfister"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/88f00376aedb947af123a7868fce3e58-Abstract-Conference.html": {
    "title": "XMask3D: Cross-modal Mask Reasoning for Open Vocabulary 3D Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Wang",
      "Yanbo Wang",
      "Xumin Yu",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8900e600e4f55be0a10cee4a14d7c3de-Abstract-Conference.html": {
    "title": "Towards Global Optimal Visual In-Context Learning Prompt Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengming Xu",
      "Chen Liu",
      "Yikai Wang",
      "Yuan Yao",
      "Yanwei Fu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8926246e46868353aa422d3629893d20-Abstract-Conference.html": {
    "title": "NeoRL: Efficient Exploration for Nonepisodic RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhavya",
      "Lenart Treven",
      "Florian Dorfler",
      "Stelian Coros",
      "Andreas Krause"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8936fa1691764912d9519e1b5673ea66-Abstract-Conference.html": {
    "title": "Transformers Represent Belief State Geometry in their Residual Stream",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Shai",
      "Lucas Teixeira",
      "Alexander Oldenziel",
      "Sarah Marzen",
      "Paul Riechers"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/89379d5fc6eb34ff98488202fb52b9d0-Abstract-Conference.html": {
    "title": "GIC: Gaussian-Informed Continuum for Physical Property Identification and Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Cai",
      "Yuji Yang",
      "Weihao Yuan",
      "Yisheng HE",
      "Zilong Dong",
      "Liefeng Bo",
      "Hui Cheng",
      "Qifeng Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/893a5db6100028ec814cfd99fe92c31b-Abstract-Conference.html": {
    "title": "Spiking Neural Network as Adaptive Event Stream Slicer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahang Cao",
      "Mingyuan Sun",
      "Ziqing Wang",
      "Hao Cheng",
      "Qiang Zhang",
      "shibo zhou",
      "Renjing Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8951f484e8242b7f74817fdc390dd954-Abstract-Conference.html": {
    "title": "IntraMix: Intra-Class Mixup Generation for Accurate Labels and Neighbors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghe Zheng",
      "Hongzhi Wang",
      "Xianglong Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/89566c18d5b3e9836e8e16fde010b41d-Abstract-Conference.html": {
    "title": "Director3D: Real-world Camera Trajectory and 3D Scene Generation from Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyang Li",
      "Zhangyu Lai",
      "Linning Xu",
      "Yansong Qu",
      "Liujuan Cao",
      "ShengChuan Zhang",
      "Bo Dai",
      "Rongrong Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8957cfb29ae633d6e8c3615be038d4d6-Abstract-Conference.html": {
    "title": "Fast Rates for Bandit PAC Multiclass Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liad Erez",
      "Alon Peled-Cohen",
      "Tomer Koren",
      "Yishay Mansour",
      "Shay Moran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/89667cd3cf308eb8a9cfdc1f53578d42-Abstract-Conference.html": {
    "title": "Prune and Repaint: Content-Aware Image Retargeting for any Ratio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feihong Shen",
      "Chao Li",
      "Yifeng Geng",
      "Yongjian Deng",
      "Hao Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/898387e745a0b95e17017d2c795444b6-Abstract-Conference.html": {
    "title": "BricksRL: A Platform for Democratizing Robotics and Reinforcement Learning Research and Education with LEGO",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Dittert",
      "Vincent Moens",
      "Gianni De Fabritiis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/899511e37a8e01e1bd6f6f1d377cc250-Abstract-Conference.html": {
    "title": "On scalable oversight with weak LLMs judging strong LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zachary Kenton",
      "Noah Siegel",
      "Janos Kramar",
      "Jonah Brown-Cohen",
      "Samuel Albanie",
      "Jannis Bulian",
      "Rishabh Agarwal",
      "David Lindner",
      "Yunhao Tang",
      "Noah Goodman",
      "Rohin Shah"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8997f9fff71e14a2c01bae9c6e226f44-Abstract-Conference.html": {
    "title": "Exploration by Learning Diverse Skills through Successor State Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul-Antoine LE TOLGUENEC",
      "Yann BESSE",
      "Florent Teichteil-Koenigsbuch",
      "Dennis Wilson",
      "Emmanuel Rachelson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/89af8e4eb696738f2c9e589522968a09-Abstract-Conference.html": {
    "title": "DePLM: Denoising Protein Language Models for Property Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyuan Wang",
      "Keyan Ding",
      "Ming Qin",
      "Xiaotong Li",
      "Xiang Zhuang",
      "Yu Zhao",
      "Jianhua Yao",
      "Qiang Zhang",
      "Huajun Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/89b89c04f55ea7c7ca989992bb6a98c0-Abstract-Conference.html": {
    "title": "MambaTree: Tree Topology is All You Need in State Space Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yicheng Xiao",
      "Lin Song",
      "shaoli huang",
      "Jiangshan Wang",
      "Siyu Song",
      "Yixiao Ge",
      "Xiu Li",
      "Ying Shan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/89c61fce5a8b73871d1c4073f486b134-Abstract-Conference.html": {
    "title": "Structural Inference of Dynamical Systems with Conjoined State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aoran Wang",
      "Jun Pang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/89cc5e613d34f90de90c21e996e60b30-Abstract-Conference.html": {
    "title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayu Wang",
      "Yifei Ming",
      "Zhenmei Shi",
      "Vibhav Vineet",
      "Xin Wang",
      "Sharon Li",
      "Neel Joshi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/89d0d5c2f720921df93bbb8fef514571-Abstract-Conference.html": {
    "title": "Domain Adaptation for Large-Vocabulary Object Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Jiang",
      "Jiaxing Huang",
      "Weiying Xie",
      "Jie Lei",
      "Yunsong Li",
      "Ling Shao",
      "Shijian Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/89e4433fec4b99f1d859db57af1e0a0f-Abstract-Conference.html": {
    "title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Gao",
      "Aleksander Holynski",
      "Philipp Henzler",
      "Arthur Brussee",
      "Ricardo Martin Brualla",
      "Pratul Srinivasan",
      "Jonathan Barron",
      "Ben Poole"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/89edef87915d31de3437b6b2ac5f79e7-Abstract-Conference.html": {
    "title": "From Biased to Unbiased Dynamics: An Infinitesimal Generator Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timothée Devergne",
      "Vladimir Kostic",
      "Michele Parrinello",
      "Massimiliano Pontil"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/89f39d0b3d49a47606a165eefba2778c-Abstract-Conference.html": {
    "title": "Panacea: Pareto Alignment via Preference Adaptation for LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhong",
      "Chengdong Ma",
      "Xiaoyuan Zhang",
      "Ziran Yang",
      "Haojun Chen",
      "Qingfu Zhang",
      "Siyuan Qi",
      "Yaodong Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8a18e17fd522d0b7080a00ebfda118f8-Abstract-Conference.html": {
    "title": "IR-CM: The Fast and General-purpose Image Restoration Method Based on Consistency Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxuan Gong",
      "Jie Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8a23a95e26d016711c0d70f79ade3c95-Abstract-Conference.html": {
    "title": "Assouad, Fano, and Le Cam with Interaction: A Unifying Lower Bound Framework and Characterization for Bandit Learnability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Chen",
      "Dylan J Foster",
      "Yanjun Han",
      "Jian Qian",
      "Alexander Rakhlin",
      "Yunbei Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8a30aba6514b56d02976f49797f6338a-Abstract-Conference.html": {
    "title": "Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lanqing Li",
      "Hai Zhang",
      "Xinyu Zhang",
      "Shatong Zhu",
      "Yang YU",
      "Junqiao Zhao",
      "Pheng-Ann Heng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8a54a80ffc2834689ffdd0920202018e-Abstract-Conference.html": {
    "title": "Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyuan Huang",
      "Kaixiang Ji",
      "Biao Gong",
      "Zhiwu Qing",
      "Qinglong Zhang",
      "Kecheng Zheng",
      "Jian Wang",
      "Jingdong Chen",
      "Ming Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8a57aa8e8b57e64a42e95f7dceb0adb9-Abstract-Conference.html": {
    "title": "T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Li",
      "Weixi Feng",
      "Tsu-Jui Fu",
      "Xinyi Wang",
      "S Basu",
      "Wenhu Chen",
      "William Yang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8a6eb62e87a287f91734ca33e1585e6e-Abstract-Conference.html": {
    "title": "Pricing and Competition for Generative AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rafid Mahmood"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8a8a2ca52c541e02bff3c6c02b99f464-Abstract-Conference.html": {
    "title": "Accelerating Relative Entropy Coding with Space Partitioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun He",
      "Gergely Flamich",
      "José Miguel Hernández-Lobato"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8aa0c4d28021c0b273480f9e2aab83a6-Abstract-Conference.html": {
    "title": "Samba: Severity-aware Recurrent Modeling for Cross-domain Medical Image Grading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Bi",
      "Jingjun Yi",
      "Hao Zheng",
      "Wei Ji",
      "Haolan Zhan",
      "Yawen Huang",
      "Yuexiang Li",
      "Yefeng Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8aaae73117c9266b35eb977e70fcf95f-Abstract-Conference.html": {
    "title": "On Divergence Measures for Training GFlowNets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiago Silva",
      "Eliezer de Souza da Silva",
      "Diego Mesquita"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8ab385402ef6611c22e92f38570b9576-Abstract-Conference.html": {
    "title": "Conditional Synthesis of 3D Molecules with Time Correction Sampler",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hojung Jung",
      "Youngrok Park",
      "Laura Schmid",
      "Jaehyeong Jo",
      "Dongkyu Lee",
      "Bongsang Kim",
      "Se-Young Yun",
      "Jinwoo Shin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8ac50fd0a4eeeb1f077b17bb7c5353c3-Abstract-Conference.html": {
    "title": "VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Sarch",
      "Lawrence Jang",
      "Michael Tarr",
      "William W. Cohen",
      "Kenneth Marino",
      "Katerina Fragkiadaki"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8ac80f6cdd1fffc43a4ba4ba49f40186-Abstract-Conference.html": {
    "title": "Local Linearity: the Key for No-regret Reinforcement Learning in Continuous MDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Maran",
      "Alberto Maria Metelli",
      "Matteo Papini",
      "Marcello Restelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8acb1688c994bee3e17df1bff8fedd62-Abstract-Conference.html": {
    "title": "The Reliability of OKRidge Method in Solving Sparse Ridge Regression Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyuan Li",
      "Youjun Wang",
      "Weiwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8af52d7acc4f0013661d4223d7e12b4c-Abstract-Conference.html": {
    "title": "Continual Audio-Visual Sound Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiguo Pian",
      "Yiyang Nan",
      "Shijian Deng",
      "Shentong Mo",
      "Yunhui Guo",
      "Yapeng Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8b21a7ea42cbcd1c29a7a88c444cce45-Abstract-Conference.html": {
    "title": "Amortizing intractable inference in diffusion models for vision, language, and control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddarth Venkatraman",
      "Moksh Jain",
      "Luca Scimeca",
      "Minsu Kim",
      "Marcin Sendera",
      "Mohsin Hasan",
      "Luke Rowe",
      "Sarthak Mittal",
      "Pablo Lemos",
      "Emmanuel Bengio",
      "Alexandre Adam",
      "Jarrid Rector-Brooks",
      "Yoshua Bengio",
      "Glen Berseth",
      "Nikolay Malkin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8b2fc235787852ead92da2268cd9e90c-Abstract-Conference.html": {
    "title": "Video Diffusion Models are Training-free Motion Interpreter and Controller",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqi Xiao",
      "Yifan Zhou",
      "Shuai Yang",
      "Xingang Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8b4e529225152b2fa6749c10c1d616dd-Abstract-Conference.html": {
    "title": "Confident Natural Policy Gradient for Local Planning in $q_\\pi$-realizable Constrained MDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Tian",
      "Lin Yang",
      "Csaba Szepesvari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8b54ecd9823fff6d37e61ece8f87e534-Abstract-Conference.html": {
    "title": "CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "DONGZHI JIANG",
      "Guanglu Song",
      "Xiaoshi Wu",
      "Renrui Zhang",
      "Dazhong Shen",
      "ZHUOFAN ZONG",
      "Yu Liu",
      "Hongsheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8b9062635eafbc3677429496a23e424b-Abstract-Conference.html": {
    "title": "Convergence of No-Swap-Regret Dynamics in Self-Play",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renato Leme",
      "Georgios Piliouras",
      "Jon Schneider"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8b970e15a89bf5d12542810df8eae8fc-Abstract-Conference.html": {
    "title": "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Hägele",
      "Elie Bakouch",
      "Atli Kosson",
      "Loubna Ben allal",
      "Leandro Von Werra",
      "Martin Jaggi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8bb7d93ee3ce2c75da68ebeb51508111-Abstract-Conference.html": {
    "title": "Measuring Mutual Policy Divergence for Multi-Agent Sequential Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haowen Dou",
      "Lujuan Dang",
      "Zhirong Luan",
      "Badong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8bc74514d554a90c996576f6c373f5f3-Abstract-Conference.html": {
    "title": "Neural Assets: 3D-Aware Multi-Object Scene Synthesis with Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Wu",
      "Yulia Rubanova",
      "Rishabh Kabra",
      "Drew Hudson",
      "Igor Gilitschenski",
      "Yusuf Aytar",
      "Sjoerd van Steenkiste",
      "Kelsey Allen",
      "Thomas Kipf"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8bd31288ad8e9a31d519fdeede7ee47d-Abstract-Conference.html": {
    "title": "Persistence Homology Distillation for Semi-supervised Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YanFan",
      "Yu Wang",
      "Pengfei Zhu",
      "Dongyue Chen",
      "Qinghua Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8bd4f1dbc7a70c6b80ce81b8b4fdc0b2-Abstract-Conference.html": {
    "title": "Normal-GS: 3D Gaussian Splatting with Normal-Involved Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Wei",
      "Qianyi Wu",
      "Jianmin Zheng",
      "Hamid Rezatofighi",
      "Jianfei Cai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8bea36ac39e11ebe49e9eddbd4b8bd3a-Abstract-Conference.html": {
    "title": "GuardT2I: Defending Text-to-Image Models from Adversarial Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijun Yang",
      "Ruiyuan Gao",
      "Xiao Yang",
      "Jianyuan Zhong",
      "Qiang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8c1075ffea0abbf0bb94c9de43638e58-Abstract-Conference.html": {
    "title": "Online Bayesian Persuasion Without a Clue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Bacchiocchi",
      "Matteo Bollini",
      "Matteo Castiglioni",
      "Alberto Marchesi",
      "Nicola Gatti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8c1df8153bc1b1366fe27f0785e5fdd4-Abstract-Conference.html": {
    "title": "Testing Semantic Importance via Betting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacopo Teneggi",
      "Jeremias Sulam"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8c22e5e918198702765ecff4b20d0a90-Abstract-Conference.html": {
    "title": "Set-based Neural Network Encoding Without Weight Tying",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bruno Andreis",
      "Bedionita Soro",
      "Philip Torr",
      "Sung Ju Hwang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8c263f70550cc7d69dba3fc170a23e77-Abstract-Conference.html": {
    "title": "Disentangled Unsupervised Skill Discovery for Efficient Hierarchical Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaheng Hu",
      "Zizhao Wang",
      "Peter Stone",
      "Roberto Martín-Martín"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8c2de4155634a20d903c2ab0b1784886-Abstract-Conference.html": {
    "title": "Space-Time Continuous PDE Forecasting using Equivariant Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Knigge",
      "David Wessels",
      "Riccardo Valperga",
      "Samuele Papa",
      "Jan-jakob Sonke",
      "Erik Bekkers",
      "Efstratios Gavves"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8c420176b45e923cf99dee1d7356a763-Abstract-Conference.html": {
    "title": "Expanding Sparse Tuning for Low Memory Usage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shufan Shen",
      "Junshu Sun",
      "Xiangyang Ji",
      "Qingming Huang",
      "Shuhui Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8c54e9bfed4119c873f575d1d1e2f0a0-Abstract-Conference.html": {
    "title": "Randomized Exploration for Reinforcement Learning with Multinomial Logistic Function Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wooseong Cho",
      "Taehyun Hwang",
      "Joongkyu Lee",
      "Min-hwan Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8c64bc3f7796d31caa7c3e6b969bf7da-Abstract-Conference.html": {
    "title": "BitsFusion: 1.99 bits Weight Quantization of Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Sui",
      "Yanyu Li",
      "Anil Kag",
      "Yerlan Idelbayev",
      "Junli Cao",
      "Ju Hu",
      "Dhritiman Sagar",
      "Bo Yuan",
      "Sergey Tulyakov",
      "Jian Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8c67fc501a50977947c5bebbc39ca8f6-Abstract-Conference.html": {
    "title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunze Man",
      "Shuhong Zheng",
      "Zhipeng Bao",
      "Martial Hebert",
      "Liangyan Gui",
      "Yu-Xiong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8c976a95df6a229551cd28c76627edc9-Abstract-Conference.html": {
    "title": "Deep linear networks for regression are implicitly regularized towards flat minima",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Marion",
      "Lénaïc Chizat"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8caa0e69d7e5bef025e118da9ff35a6c-Abstract-Conference.html": {
    "title": "Enabling Adaptive Agent Training in Open-Ended Simulators by Targeting Diversity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robby Costales",
      "Stefanos Nikolaidis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8cb564df771e9eacbfe9d72bd46a24a9-Abstract-Conference.html": {
    "title": "Many-Shot In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishabh Agarwal",
      "Avi Singh",
      "Lei Zhang",
      "Bernd Bohnet",
      "Luis Rosias",
      "Stephanie Chan",
      "Biao Zhang",
      "Ankesh Anand",
      "Zaheer Abbas",
      "Azade Nova",
      "John Co-Reyes",
      "Eric Chu",
      "Feryal Behbahani",
      "Aleksandra Faust",
      "Hugo Larochelle"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8cc7e1509fbfee9cabaacd3ab0bfe2b1-Abstract-Conference.html": {
    "title": "Robust Fine-tuning of Zero-shot Models via Variance Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beier Zhu",
      "Jiequan Cui",
      "Hanwang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8ccc5ec30a8d46793d790e2216efd40d-Abstract-Conference.html": {
    "title": "Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahdi Karami",
      "Ali Ghodsi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8cd23ec650193ea59a249dbdfdde18cb-Abstract-Conference.html": {
    "title": "TrajCLIP: Pedestrian trajectory prediction method using contrastive learning and idempotent networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengfei Yao",
      "Yinglong Zhu",
      "Huikun Bi",
      "Tianlu Mao",
      "Zhaoqi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8ce6c5450ccddbe6adee4b3749893587-Abstract-Conference.html": {
    "title": "Personalized Federated Learning via Feature Distribution Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Connor Mclaughlin",
      "Lili Su"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8cea78701eb986f3ec357eb9b7c6badd-Abstract-Conference.html": {
    "title": "Learning to Discuss Strategically: A Case Study on One Night Ultimate Werewolf",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanfa Jin",
      "Ziyan Wang",
      "Yali Du",
      "Meng Fang",
      "Haifeng Zhang",
      "Jun Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8cef4e4bcb85f7d4a1005a9db018d6b6-Abstract-Conference.html": {
    "title": "Causal Temporal Representation Learning with Nonstationary Sparse Transition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangchen Song",
      "Zijian Li",
      "Guangyi Chen",
      "Yujia Zheng",
      "Yewen Fan",
      "Xinshuai Dong",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8d17fe1feff1a6a619b1e3229eb94712-Abstract-Conference.html": {
    "title": "HOI-Swap: Swapping Objects in Videos with Hand-Object Interaction Awareness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihui (Sherry) Xue",
      "Romy Luo",
      "Changan Chen",
      "Kristen Grauman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8d2aad53fa148791ffd004a7c9cce5ad-Abstract-Conference.html": {
    "title": "End-To-End Causal Effect Estimation from Unstructured Natural Language Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Dhawan",
      "Leonardo Cotta",
      "Karen Ullrich",
      "Rahul G Krishnan",
      "Chris J Maddison"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8d35d225230a9d77b29c1dd300e48ad9-Abstract-Conference.html": {
    "title": "CoMERA: Computing- and Memory-Efficient Training via Rank-Adaptive Tensor Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi Yang",
      "Ziyue Liu",
      "Samridhi Choudhary",
      "Xinfeng Xie",
      "Cao Gao",
      "Siegfried Kunzmann",
      "Zheng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8d57f138d14fdfdc520eb29804116d9e-Abstract-Conference.html": {
    "title": "Sample-Efficient Geometry Reconstruction from Euclidean Distances using Non-Convex Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ipsita Ghosh",
      "Abiy Tasissa",
      "Christian Kümmerle"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8d6233829f97fe8eb3fa7d2ab774aedc-Abstract-Conference.html": {
    "title": "Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Musfiqur Rahman",
      "Matt Jordan",
      "Murat Kocaoglu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8d6c356c82252032a20c749dda36f2d8-Abstract-Conference.html": {
    "title": "Pretrained Transformer Efficiently Learns Low-Dimensional Target Functions In-Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazusato Oko",
      "Yujin Song",
      "Taiji Suzuki",
      "Denny Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8d7c8a3a0ed04006d129b3cebcac7a3e-Abstract-Conference.html": {
    "title": "Contracting with a Learning Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guru Guruganesh",
      "Yoav Kolumbus",
      "Jon Schneider",
      "Inbal Talgam-Cohen",
      "Emmanouil-Vasileios Vlatakis-Gkaragkounis",
      "Joshua R. Wang",
      "S. Matthew Weinberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8d8e060d9a3312ae12f42adf0da6ec7c-Abstract-Conference.html": {
    "title": "Faster Accelerated First-order Methods for Convex Optimization with Strongly Convex Function Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "zhenwei lin",
      "Qi Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8db12f7214d3a1a0c450ba751163e0fd-Abstract-Conference.html": {
    "title": "BPQP: A Differentiable Convex Optimization Framework for Efficient End-to-End Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianming Pan",
      "Zeqi Ye",
      "Xiao Yang",
      "Xu Yang",
      "Weiqing Liu",
      "Lewen Wang",
      "Jiang Bian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8db9279f593652ee9bb2223b4a2c43fa-Abstract-Conference.html": {
    "title": "Sm: enhanced localization in Multiple Instance Learning for medical imaging classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francisco M. Castro-Macías",
      "Pablo Morales Alvarez",
      "Yunan Wu",
      "Rafael Molina",
      "Aggelos Katsaggelos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8dbd2780192078711c0f31e10a819031-Abstract-Conference.html": {
    "title": "Local Anti-Concentration Class: Logarithmic Regret for Greedy Linear Contextual Bandit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seok-Jin Kim",
      "Min-hwan Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8dc8a46f3981224217d32eb3f8362998-Abstract-Conference.html": {
    "title": "Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashwin Ramachandran",
      "Vaibhav Raj",
      "Indradyumna Roy",
      "Soumen Chakrabarti",
      "Abir De"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8de48448bf7434e65b3a3473006893c0-Abstract-Conference.html": {
    "title": "Solving Sparse \\& High-Dimensional-Output Regression via Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renyuan Li",
      "Zhehui Chen",
      "Guanyi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8deff86036ee6a73cda883dc7d2d6b07-Abstract-Conference.html": {
    "title": "Relational Concept Bottleneck Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pietro Barbiero",
      "Francesco Giannini",
      "Gabriele Ciravegna",
      "Michelangelo Diligenti",
      "Giuseppe Marra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8df80d7115a55eb81010c967a247b1ae-Abstract-Conference.html": {
    "title": "Confidence Calibration of Classifiers with Many Classes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrien Le Coz",
      "Stéphane Herbin",
      "Faouzi Adjed"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8e1419dfa476017e0ab5d1ac5813e297-Abstract-Conference.html": {
    "title": "An Analytical Study of Utility Functions in Multi-Objective Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manel Rodríguez Soto",
      "Juan A Rodríguez-Aguilar",
      "Maite López-Sánchez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8e3b2fb66db1930ae7fcee712770e48a-Abstract-Conference.html": {
    "title": "SkiLD: Unsupervised Skill Discovery Guided by Factor Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zizhao Wang",
      "Jiaheng Hu",
      "Caleb Chuck",
      "Stephen Chen",
      "Roberto Martín-Martín",
      "Amy Zhang",
      "Scott Niekum",
      "Peter Stone"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8e5b9dc3ff7172ff7689f932047e7852-Abstract-Conference.html": {
    "title": "$\\text{ID}^3$: Identity-Preserving-yet-Diversified Diffusion Models for Synthetic Face Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianqing Xu",
      "Shen Li",
      "Jiaying Wu",
      "Miao Xiong",
      "Ailin Deng",
      "Jiazhen Ji",
      "Yuge Huang",
      "Guodong Mu",
      "Wenjie Feng",
      "Shouhong Ding",
      "Bryan Hooi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8e63972d4d9d81b31459d787466ce271-Abstract-Conference.html": {
    "title": "Language-Driven Interactive Traffic Trajectory Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junkai XIA",
      "Chenxin Xu",
      "Qingyao Xu",
      "Yanfeng Wang",
      "Siheng Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8e6f3d53b2bef98fce17e699557f5f11-Abstract-Conference.html": {
    "title": "VLMimic: Vision Language Models are Visual Imitation Learner for Fine-grained Actions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyan Chen",
      "Meiling Wang",
      "Te Cui",
      "Yao Mu",
      "Haoyang Lu",
      "Tianxing Zhou",
      "Zicai Peng",
      "Mengxiao Hu",
      "Haizhou Li",
      "Li Yuan",
      "Yi Yang",
      "Yufeng Yue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8e8399e5e7aed601c9f135f40be26564-Abstract-Conference.html": {
    "title": "Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Min",
      "Zeyu Qin",
      "Nevin L. Zhang",
      "Li Shen",
      "Minhao Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8e851181b937e2c519ff4c9adbe32be8-Abstract-Conference.html": {
    "title": "SSDiff: Spatial-spectral Integrated Diffusion Model for Remote Sensing Pansharpening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhong",
      "Xiao Wu",
      "Liang-Jian Deng",
      "ZiHan Cao",
      "Hong-Xia Dou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8e906a69bcdfc3b9be15995b525ddda7-Abstract-Conference.html": {
    "title": "Generating Highly Designable Proteins with Geometric Algebra Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Wagner",
      "Leif Seute",
      "Vsevolod Viliuga",
      "Nicolas Wolf",
      "Frauke Gräter",
      "Jan Stühmer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8e9c7d4a48bdac81a58f983a64aaf42b-Abstract-Conference.html": {
    "title": "Covariate Shift Corrected Conditional Randomization Test",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Xu",
      "Yiwen Huang",
      "Chuan Hong",
      "Shuangning Li",
      "Molei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8ea50bf458f6070548b11babbe0bf89b-Abstract-Conference.html": {
    "title": "Dual-Diffusion for Binocular 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyue Wan",
      "Zhuo Chen",
      "Bingzhi Duan",
      "Xu Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8ec078518dcce6be1324cfd3de11ed24-Abstract-Conference.html": {
    "title": "Motion Forecasting in Continuous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Song",
      "Bozhou Zhang",
      "Xiatian Zhu",
      "Li Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8ecf25e3a462e9bb1e29662dfe7258fe-Abstract-Conference.html": {
    "title": "Two-way Deconfounder for Off-policy Evaluation in Causal Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuguang Yu",
      "Shuxing Fang",
      "Ruixin Peng",
      "Zhengling Qi",
      "Fan Zhou",
      "Chengchun Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8ee4dfc3bac3d53d36fb9c0c72ec2a9f-Abstract-Conference.html": {
    "title": "An Accelerated Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaochuan Gong",
      "Jie Hao",
      "Mingrui Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8eec8d7bcecf034304174e6b57dbc19a-Abstract-Conference.html": {
    "title": "Structured Unrestricted-Rank Matrices for Parameter Efficient Finetuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arijit Sehanobish",
      "Kumar Avinava Dubey",
      "Krzysztof M Choromanski",
      "Somnath Basu Roy Chowdhury",
      "Deepali Jain",
      "Vikas Sindhwani",
      "Snigdha Chaturvedi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8eed150084dc3534f01ba63f9b7d32d2-Abstract-Conference.html": {
    "title": "Online Convex Optimisation: The Optimal Switching Regret for all Segmentations Simultaneously",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen Pasteris",
      "Chris Hicks",
      "Vasilios Mavroudis",
      "Mark Herbster"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8f11e548311c7fd3f33596a4d1dd41f0-Abstract-Conference.html": {
    "title": "AmoebaLLM: Constructing Any-Shape Large Language Models for Efficient and Instant Deployment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yonggan Fu",
      "Zhongzhi Yu",
      "Junwei Li",
      "Jiayi Qian",
      "Yongan Zhang",
      "Xiangchi Yuan",
      "Dachuan Shi",
      "Roman Yakunin",
      "Yingyan (Celine) Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8f395480c04ac6dfb2c2326a639df88e-Abstract-Conference.html": {
    "title": "Understanding Transformer Reasoning Capabilities via Graph Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clayton Sanford",
      "Bahare Fatemi",
      "Ethan Hall",
      "Anton Tsitsulin",
      "Mehran Kazemi",
      "Jonathan Halcrow",
      "Bryan Perozzi",
      "Vahab Mirrokni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8f4477b086a9c97e30d1a0621ea6b2f5-Abstract-Conference.html": {
    "title": "One-to-Normal: Anomaly Personalization for Few-shot Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyue Li",
      "Shaoting Zhang",
      "Kang Li",
      "Qicheng Lao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8f6b3692297e49e5d5c91ba00281379c-Abstract-Conference.html": {
    "title": "Multi-modal Transfer Learning between Biological Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan Jose Garau-Luis",
      "Patrick Bordes",
      "Liam Gonzalez",
      "Maša Roller",
      "Bernardo de Almeida",
      "Christopher Blum",
      "Lorenz Hexemer",
      "Stefan Laurent",
      "Maren Lang",
      "Thomas Pierrot",
      "Guillaume Richard"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8f75af4704feac629a560f4ad6b67cef-Abstract-Conference.html": {
    "title": "OASIS: Conditional Distribution Shaping for Offline Safe Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihang Yao",
      "Zhepeng Cen",
      "Wenhao Ding",
      "Haohong Lin",
      "Shiqi Liu",
      "Tingnan Zhang",
      "Wenhao Yu",
      "DING ZHAO"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8f8af4eebc4e50994e0490898d891c96-Abstract-Conference.html": {
    "title": "Learning from higher-order correlations, efficiently: hypothesis tests, random features, and neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eszter Szekely",
      "Lorenzo Bardone",
      "Federica Gerace",
      "Sebastian Goldt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8f9a3fdb29de4c3fd030a18a505223c6-Abstract-Conference.html": {
    "title": "Unified Mechanism-Specific Amplification by Subsampling and Group Privacy Amplification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Schuchardt",
      "Mihail Stoian",
      "Arthur Kosmala",
      "Stephan Günnemann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8f9d1362a386e80a723e98451a8c7564-Abstract-Conference.html": {
    "title": "PeRFlow: Piecewise Rectified Flow as Universal Plug-and-Play Accelerator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanshu Yan",
      "Xingchao Liu",
      "Jiachun Pan",
      "Jun Hao Liew",
      "Qiang Liu",
      "Jiashi Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8f9d459c19b59b5400ce396e0f8c23e0-Abstract-Conference.html": {
    "title": "Online Weighted Paging with Unknown Weights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Orin Levy",
      "Noam Touitou",
      "Aviv Rosenberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8f9f4eb32b9081a90f2a0b2627eb2a24-Abstract-Conference.html": {
    "title": "Achieving $\\tilde{O}(1/\\epsilon)$ Sample Complexity for Constrained Markov Decision Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiashuo Jiang",
      "Yinyu Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8fa068ffe59817175d176bd75641fe16-Abstract-Conference.html": {
    "title": "Overcoming the Sim-to-Real Gap: Leveraging Simulation to Learn to Explore for Real-World RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Wagenmaker",
      "Kevin Huang",
      "Liyiming Ke",
      "Kevin G. Jamieson",
      "Abhishek Gupta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8fa604a81e5a236e2f38e917109571a3-Abstract-Conference.html": {
    "title": "Animal-Bench: Benchmarking Multimodal Video Models for Animal-centric Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinuo Jing",
      "Ruxu Zhang",
      "Kongming Liang",
      "Yongxiang Li",
      "Zhongjiang He",
      "Zhanyu Ma",
      "Jun Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8fad1ce6d16f654936b98662fce84c5d-Abstract-Conference.html": {
    "title": "Transfer Learning for Latent Variable Network Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akhil Jalan",
      "Arya Mazumdar",
      "Soumendu Sundar Mukherjee",
      "Purnamrita Sarkar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8fae6a68aaf1e05bfd90375755b63821-Abstract-Conference.html": {
    "title": "Beyond Single Stationary Policies: Meta-Task Players as Naturally Superior Collaborators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Haoming",
      "Zhaoming Tian",
      "Yunpeng Song",
      "Xiangliang Zhang",
      "Zhongmin Cai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8fb96e8d0fbf591b1fa1ad85653d8417-Abstract-Conference.html": {
    "title": "Hierarchical Federated Learning with Multi-Timescale Gradient Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhi Fang",
      "Dong-Jun Han",
      "Evan Chen",
      "Shiqiang Wang",
      "Christopher Brinton"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8fcd17eb91bae20d9826786d7d6be799-Abstract-Conference.html": {
    "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyi Lu",
      "Chenghao Fan",
      "Wei Wei",
      "Xiaoye Qu",
      "Dangyang Chen",
      "Yu Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.html": {
    "title": "Connectivity-Driven Pseudo-Labeling Makes Stronger Cross-Domain Segmenters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Zhao",
      "Qi Zang",
      "Shuang Wang",
      "Nicu Sebe",
      "Zhun Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8fd5bc08e744fe0dfe798c61d1575a22-Abstract-Conference.html": {
    "title": "Lambda: Learning Matchable Prior For Entity Alignment with Unlabeled Dangling Cases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Yin",
      "Liyao Xiang",
      "Dong Ding",
      "Yuheng He",
      "Yihan Wu",
      "Pengzhi Chu",
      "Xinbing Wang",
      "Chenghu Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8ff87c96935244b63503f542472462b3-Abstract-Conference.html": {
    "title": "Replicability in Learning: Geometric Partitions and KKM-Sperner Lemma",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jason Vander Woude",
      "Peter Dixon",
      "A. Pavan",
      "Jamie Radcliffe",
      "N. V. Vinodchandran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/8ffb4e3118280a66b192b6f06e0e2596-Abstract-Conference.html": {
    "title": "Is Value Learning Really the Main Bottleneck in Offline RL?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seohong Park",
      "Kevin Frans",
      "Sergey Levine",
      "Aviral Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/90043ebd68500f9efe84fedf860a64f3-Abstract-Conference.html": {
    "title": "VLG-CBM: Training Concept Bottleneck Models with Vision-Language Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Divyansh Srivastava",
      "Ge Yan",
      "Lily Weng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/90080022263cddafddd4a0726f1fb186-Abstract-Conference.html": {
    "title": "ActSort: An active-learning accelerated cell sorting algorithm for large-scale calcium imaging datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqi Jiang",
      "Hakki Akengin",
      "Ji Zhou",
      "Mehmet Aslihak",
      "Yang Li",
      "Radoslaw Chrapkiewicz",
      "Oscar Hernandez",
      "sadegh ebrahimi",
      "Omar Jaidar",
      "Yanping Zhang",
      "Hakan Inan",
      "Christopher Miranda",
      "Fatih Dinc",
      "Marta Pozo",
      "Mark Schnitzer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9062b7d6e522dadf4f7d85d49b60d81e-Abstract-Conference.html": {
    "title": "Learnability of high-dimensional targets by two-parameter models and gradient flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitry Yarotsky"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/906727703322b2e33b37c855fc15e8b7-Abstract-Conference.html": {
    "title": "UnSeg: One Universal Unlearnable Example Generator is Enough against All Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Sun",
      "Hao Zhang",
      "Tiehua Zhang",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/907a9fb75a408f6c3a2ae1bf84c39e44-Abstract-Conference.html": {
    "title": "Decision-Focused Learning with Directional Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Huang",
      "Vishal Gupta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/90800f46f84381b7891e1378ee850013-Abstract-Conference.html": {
    "title": "Bridging the Divide: Reconsidering Softmax and Linear Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongchen Han",
      "Yifan Pu",
      "Zhuofan Xia",
      "Yizeng Han",
      "Xuran Pan",
      "Xiu Li",
      "Jiwen Lu",
      "Shiji Song",
      "Gao Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/90812824c8b36622e6f61803d03b2926-Abstract-Conference.html": {
    "title": "Energy-Based Modelling for Discrete and Mixed Data via Heat Equations on Structured Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Schröder",
      "Zijing Ou",
      "Yingzhen Li",
      "Andrew Duncan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/909f526db5127f8bd8158af32d9e313a-Abstract-Conference.html": {
    "title": "RegExplainer: Generating Explanations for Graph Neural Networks in Regression Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxing Zhang",
      "Zhuomin Chen",
      "hao mei",
      "Longchao Da",
      "Dongsheng Luo",
      "Hua Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/90afd20dc776bc8849c31d61a0763a0b-Abstract-Conference.html": {
    "title": "Humanoid Locomotion as Next Token Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilija Radosavovic",
      "Bike Zhang",
      "Baifeng Shi",
      "Jathushan Rajasegaran",
      "Sarthak Kamat",
      "Trevor Darrell",
      "Koushil Sreenath",
      "Jitendra Malik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/90b31ad371165eaac2dc6de8993fded7-Abstract-Conference.html": {
    "title": "Dendritic Integration Inspired Artificial Neural Networks Capture Data Correlation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongming Liu",
      "Jingyang Ma",
      "Songting Li",
      "Douglas (Dongzhuo) Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/90c4537a301e9545bb4c60219f2992b1-Abstract-Conference.html": {
    "title": "Explaining Datasets in Words: Statistical Models with Natural Language Parameters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Zhong",
      "Heng Wang",
      "Dan Klein",
      "Jacob Steinhardt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/90caeb952bd4b03c7d8e7a0e31fc9a8b-Abstract-Conference.html": {
    "title": "Learning with Fitzpatrick Losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seta Rakotomandimby",
      "Jean-Philippe Chancelier",
      "Michel De Lara",
      "Mathieu Blondel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/90d1fc07f46e31387978b88e7e057a31-Abstract-Conference.html": {
    "title": "MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yubin Kim",
      "Chanwoo Park",
      "Hyewon Jeong",
      "Yik Siu Chan",
      "Xuhai \"Orson\" Xu",
      "Daniel McDuff",
      "Hyeonhoon Lee",
      "Marzyeh Ghassemi",
      "Cynthia Breazeal",
      "Hae Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/90e73f3cf1a6c84c723a2e8b7fb2b2c1-Abstract-Conference.html": {
    "title": "Compact Proofs of Model Performance via Mechanistic Interpretability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jason Gross",
      "Rajashree Agrawal",
      "Thomas Kwa",
      "Euan Ong",
      "Chun Hei Yip",
      "Alex Gibson",
      "Soufiane Noubir",
      "Lawrence Chan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/911dd89c81efc624c4e1c39381179505-Abstract-Conference.html": {
    "title": "Semantic Feature Learning for Universal Unsupervised Cross-Domain Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lixu Wang",
      "Xinyu Du",
      "Qi Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/911fc798523e7d4c2e9587129fcf88fc-Abstract-Conference.html": {
    "title": "Credal Deep Ensembles for Uncertainty Quantification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaizheng Wang",
      "Fabio Cuzzolin",
      "Shireen Kudukkil Manchingal -",
      "Keivan Shariatmadar",
      "David Moens",
      "Hans Hallez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/91315fbb83ce353ae5538cba395f70d1-Abstract-Conference.html": {
    "title": "Mixture of In-Context Experts Enhance LLMs' Long Context Awareness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongzhan Lin",
      "Ang Lv",
      "yuhan chen",
      "chen zhu",
      "Yang Song",
      "Hengshu Zhu",
      "Rui Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9144c7c014bf4c30e88f650bef8f68dd-Abstract-Conference.html": {
    "title": "BehaviorGPT: Smart Agent Simulation for Autonomous Driving with Next-Patch Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zikang Zhou",
      "HU Haibo",
      "Xinhong Chen",
      "Jianping Wang",
      "Nan Guan",
      "Kui Wu",
      "Yung-Hui Li",
      "Yu-Kai Huang",
      "Chun Jason Xue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9149fc44c95ce58e3ca529a1e34c2691-Abstract-Conference.html": {
    "title": "OctreeOcc: Efficient and Multi-Granularity Occupancy Prediction Using Octree Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Lu",
      "Xinge ZHU",
      "Tai WANG",
      "Yuexin Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/914b372356d58d9e9357b29332cb8fdc-Abstract-Conference.html": {
    "title": "Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Chen",
      "Bowen Zhang",
      "Gang Wang",
      "Qi Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9161a3e810cc0542718abddb791e8aed-Abstract-Conference.html": {
    "title": "Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eli Chien",
      "Haoyu Wang",
      "Ziang Chen",
      "Pan Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/91813e5ddd9658b99be4c532e274b49c-Abstract-Conference.html": {
    "title": "Discrete-state Continuous-time Diffusion for Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Xu",
      "Ruizhong Qiu",
      "Yuzhong Chen",
      "Huiyuan Chen",
      "Xiran Fan",
      "Menghai Pan",
      "Zhichen Zeng",
      "Mahashweta Das",
      "Hanghang Tong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/918b9487f8ea4661e8ba5a02b2126658-Abstract-Conference.html": {
    "title": "CooHOI: Learning Cooperative Human-Object Interaction with Manipulated Object Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Gao",
      "Ziqin Wang",
      "Zeqi Xiao",
      "Jingbo Wang",
      "Tai WANG",
      "Jinkun Cao",
      "Xiaolin Hu",
      "Si Liu",
      "Jifeng Dai",
      "Jiangmiao Pang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/91a5742235f70ae846436d9780e9f1d4-Abstract-Conference.html": {
    "title": "A Boosting-Type Convergence Result for AdaBoost.MH with Factorized Multi-Class Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zou",
      "Zhengyu Zhou",
      "Jingyuan Xu",
      "Weiwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/91a5bb5e5939cb075f5f2464d7b8bbf0-Abstract-Conference.html": {
    "title": "MutaPLM: Protein Language Modeling for Mutation Explanation and Engineering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhen Luo",
      "Zikun Nie",
      "Massimo Hong",
      "Suyuan Zhao",
      "Hao Zhou",
      "Zaiqing Nie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/91b1966914203338495a76d13b23599c-Abstract-Conference.html": {
    "title": "Graph Neural Networks Need Cluster-Normalize-Activate Modules",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arseny Skryagin",
      "Felix Divo",
      "Mohammad Amin Ali",
      "Devendra S Dhami",
      "Kristian Kersting"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/91d193b65d0b120d29503590827de1ea-Abstract-Conference.html": {
    "title": "Meta-Diffu$B$: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun-Yen Chuang",
      "Hung-Min Hsu",
      "Kevin Lin",
      "Chen-Sheng Gu",
      "Ling-Zhen Li",
      "Ray-I Chang",
      "Hung-yi Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html": {
    "title": "DRIP: Unleashing Diffusion Priors for Joint Foreground and Alpha Prediction in Image Matting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaodi Li",
      "Zongxin Yang",
      "Ruijie Quan",
      "Yi Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/92440ec643f4e9f17409557b6516566e-Abstract-Conference.html": {
    "title": "Revive Re-weighting in Imbalanced Learning by Density Ratio Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JIAAN LUO",
      "Feng Hong",
      "Jiangchao Yao",
      "Bo Han",
      "Ya Zhang",
      "Yanfeng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9246aa822579d9b29a140ecdac36ad60-Abstract-Conference.html": {
    "title": "AutoPSV: Automated Process-Supervised Verifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianqiao Lu",
      "Zhiyang Dou",
      "Hongru WANG",
      "Zeyu Cao",
      "Jianbo Dai",
      "Yunlong Feng",
      "Zhijiang Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9251fa7fcc356bbfc16c040f4c030d83-Abstract-Conference.html": {
    "title": "Free Lunch in Pathology Foundation Model: Task-specific Model Adaptation with Concept-Guided Feature Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanyan Huang",
      "Weiqin Zhao",
      "Yihang Chen",
      "Yu Fu",
      "Lequan Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/92559987ee79e42a2b01d534a54682ee-Abstract-Conference.html": {
    "title": "Du-IN: Discrete units-guided mask modeling for decoding speech from Intracranial Neural signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Zheng",
      "Haiteng Wang",
      "Weibang Jiang",
      "Zhongtao Chen",
      "Li He",
      "Peiyang Lin",
      "Penghu Wei",
      "Guoguang Zhao",
      "Yunzhe Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/925869234d3aa2a3aad5f05b643974aa-Abstract-Conference.html": {
    "title": "Reference Trustable Decoding: A Training-Free Augmentation Paradigm for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luohe Shi",
      "Yao Yao",
      "Zuchao Li",
      "Lefei Zhang",
      "Hai Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/926f9a8eb94ee02cb519b0ddddb67fc6-Abstract-Conference.html": {
    "title": "MambaSCI: Efficient Mamba-UNet for Quad-Bayer Patterned Video Snapshot Compressive Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao Pan",
      "Haijin Zeng",
      "Jiezhang Cao",
      "Yongyong Chen",
      "Kai Zhang",
      "Yong Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/929814ae1e76ae131a473e3f30609fef-Abstract-Conference.html": {
    "title": "Learning from Snapshots of Discrete and Continuous Data Streams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pramith Devulapalli",
      "Steve Hanneke"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/92a7a03e1c716970848a4a86cc8243ee-Abstract-Conference.html": {
    "title": "SongCreator: Lyrics-based Universal Song Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shun Lei",
      "Yixuan Zhou",
      "Boshi Tang",
      "Max W. Y. Lam",
      "Feng liu",
      "Hangyu Liu",
      "Jingcheng Wu",
      "Shiyin Kang",
      "Zhiyong Wu",
      "Helen Meng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/92af0c8c2664429de2bb44c2692d84ae-Abstract-Conference.html": {
    "title": "Automated Label Unification for Multi-Dataset Semantic Segmentation with GNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ma Rong",
      "Jie Chen",
      "Xiangyang Xue",
      "Jian Pu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/92ce40962b4098f7bf6eed33128fc606-Abstract-Conference.html": {
    "title": "Tetrahedron Splatting for 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun Gu",
      "Zeyu Yang",
      "Zijie Pan",
      "Xiatian Zhu",
      "Li Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/92dd1adab39f362046f99dfe3c39d90f-Abstract-Conference.html": {
    "title": "Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Jin",
      "Pengyi Jiao",
      "Zheng-Peng Duan",
      "Xingchao Yang",
      "Chongyi Li",
      "Chun-Le Guo",
      "Bo Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/92e886487a8354b03d8bf4416eae6d7d-Abstract-Conference.html": {
    "title": "GenWarp: Single Image to Novel Views with Semantic-Preserving Generative Warping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyoung Seo",
      "Kazumi Fukuda",
      "Takashi Shibuya",
      "Takuya Narihira",
      "Naoki Murata",
      "Shoukang Hu",
      "Chieh-Hsin Lai",
      "Seungryong Kim",
      "Yuki Mitsufuji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/92f43b1d33fae4aa1958f75317f0cec1-Abstract-Conference.html": {
    "title": "Direct Unlearning Optimization for Robust and Safe Text-to-Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yong-Hyun Park",
      "Sangdoo Yun",
      "Jin-Hwa Kim",
      "Junho Kim",
      "Geonhui Jang",
      "Yonghyun Jeong",
      "Junghyo Jo",
      "Gayoung Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/92f79f493ca2d6c0ba04c3af76bb3368-Abstract-Conference.html": {
    "title": "Balancing Context Length and Mixing Times for Reinforcement Learning at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Riemer",
      "Khimya Khetarpal",
      "Janarthanan Rajendran",
      "Sarath Chandar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9302e68244b2a9aa8cf6e3bf560d2de5-Abstract-Conference.html": {
    "title": "PCP-MAE: Learning to Predict Centers for Point Masked Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangdong Zhang",
      "Shaofeng Zhang",
      "Junchi Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9304fe9848ca4db73c79ac0bac414aef-Abstract-Conference.html": {
    "title": "A scalable generative model for dynamical system reconstruction from neuroimaging data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Volkmann",
      "Alena Brändle",
      "Daniel Durstewitz",
      "Georgia Koppe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9316769afaaeeaad42a9e3633b14e801-Abstract-Conference.html": {
    "title": "FFAM: Feature Factorization Activation Map for Explanation of 3D Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Liu",
      "Boyang Li",
      "Zhiyu Fang",
      "Mingyue Cui",
      "Kai Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/932147114c48f8b04d41aebc0c631158-Abstract-Conference.html": {
    "title": "Dissecting the Failure of Invariant Learning on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qixun Wang",
      "Yifei Wang",
      "Yisen Wang",
      "Xianghua Ying"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9328208f88ec69420031647e6ff97727-Abstract-Conference.html": {
    "title": "Axioms for AI Alignment from Human Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luise Ge",
      "Daniel Halpern",
      "Evi Micha",
      "Ariel D Procaccia",
      "Itai Shapira",
      "Yevgeniy Vorobeychik",
      "Junlin Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/93397b489222f1e66a862f795fc32b96-Abstract-Conference.html": {
    "title": "Dynamic 3D Gaussian Fields for Urban Areas",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Fischer",
      "Jonas Kulhanek",
      "Samuel Rota Bulò",
      "Lorenzo Porzi",
      "Marc Pollefeys",
      "Peter Kontschieder"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/934eb45b99eff8f16b5cb8e4d3cb5641-Abstract-Conference.html": {
    "title": "Towards a \"Universal Translator\" for Neural Dynamics at Single-Cell, Single-Spike Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizi Zhang",
      "Yanchen Wang",
      "Donato Jiménez-Benetó",
      "Zixuan Wang",
      "Mehdi Azabou",
      "Blake Richards",
      "Renee Tung",
      "Olivier Winter",
      "Brain Laboratory International",
      "Eva Dyer",
      "Liam Paninski",
      "Cole Hurwitz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/936ce22b767cf1a1496083e4725d3b21-Abstract-Conference.html": {
    "title": "Bandits with Ranking Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Maran",
      "Francesco Bacchiocchi",
      "Francesco Emanuele Stradi",
      "Matteo Castiglioni",
      "Nicola Gatti",
      "Marcello Restelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9370cc8d438b9ed8cb4344818a251d9a-Abstract-Conference.html": {
    "title": "HiCoM: Hierarchical Coherent Motion for Dynamic Streamable Scenes with 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiankun Gao",
      "Jiarui Meng",
      "Chengxiang Wen",
      "Jie Chen",
      "Jian Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9370fa016d6a14af78f5048bfcb0582b-Abstract-Conference.html": {
    "title": "Non-asymptotic Convergence of Training Transformers for Next-token Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiquan Huang",
      "Yingbin Liang",
      "Jing Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9374af323abb65ce551168d44b09ad5f-Abstract-Conference.html": {
    "title": "Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaokui Wei",
      "Hongyuan Zha",
      "Baoyuan Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9379ea6ba7a61a402c7750833848b99f-Abstract-Conference.html": {
    "title": "Logarithmic Smoothing for Pessimistic Off-Policy Evaluation, Selection and Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Otmane Sakhi",
      "Imad Aouali",
      "Pierre Alquier",
      "Nicolas Chopin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/937ae0e83eb08d2cb8627fe1def8c751-Abstract-Conference.html": {
    "title": "Long-form factuality in large language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jerry Wei",
      "Chengrun Yang",
      "Xinying Song",
      "Yifeng Lu",
      "Nathan Hu",
      "Jie Huang",
      "Dustin Tran",
      "Daiyi Peng",
      "Ruibo Liu",
      "Da Huang",
      "Cosmo Du",
      "Quoc V Le"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/938ac7bb9a997b60b6be0348c486aaef-Abstract-Conference.html": {
    "title": "Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Xu",
      "Jun Wang",
      "Jianyong Wang",
      "Wei Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/939f20cc178460749e4ab5fa28fd3b10-Abstract-Conference.html": {
    "title": "FuseAnyPart: Diffusion-Driven Facial Parts Swapping via Multiple Reference Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "zheng yu",
      "Yaohua Wang",
      "Siying Cui",
      "Aixi Zhang",
      "Wei-Long Zheng",
      "Senzhang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/93a4fc360b66fd23d6276156ca69eb50-Abstract-Conference.html": {
    "title": "Multi-hypotheses Conditioned Point Cloud Diffusion for 3D Human Reconstruction from Occluded Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghwan Kim",
      "Tae-Kyun Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/93ab29256d64d936345d90d4b6840700-Abstract-Conference.html": {
    "title": "Towards Harmless Rawlsian Fairness Regardless of Demographic Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanqian Wang",
      "Jing Li",
      "Ivor W. Tsang",
      "Yew Soon Ong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/93b7e2780c4f6599837fdd3718c51fad-Abstract-Conference.html": {
    "title": "Towards Dynamic Message Passing on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junshu Sun",
      "Chenxue Yang",
      "Xiangyang Ji",
      "Qingming Huang",
      "Shuhui Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/93be245fce00a9bb2333c17ceae4b732-Abstract-Conference.html": {
    "title": "3D Gaussian Splatting as Markov Chain Monte Carlo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shakiba Kheradmand",
      "Daniel Rebain",
      "Gopal Sharma",
      "Weiwei Sun",
      "Yang-Che Tseng",
      "Hossam Isack",
      "Abhishek Kar",
      "Andrea Tagliasacchi",
      "Kwang Moo Yi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/93c099bb4cde51b724eaa6d6d4a4b5e4-Abstract-Conference.html": {
    "title": "Nearest Neighbor Speculative Decoding for LLM Generation and Attribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghan Li",
      "Xilun Chen",
      "Ari Holtzman",
      "Beidi Chen",
      "Jimmy Lin",
      "Scott Yih",
      "Victoria Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/93c936b9e492def9c00782cab79dbc6d-Abstract-Conference.html": {
    "title": "Improved off-policy training of diffusion samplers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcin Sendera",
      "Minsu Kim",
      "Sarthak Mittal",
      "Pablo Lemos",
      "Luca Scimeca",
      "Jarrid Rector-Brooks",
      "Alexandre Adam",
      "Yoshua Bengio",
      "Nikolay Malkin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/93e45db754dd0f82339763055c6cda56-Abstract-Conference.html": {
    "title": "Decomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sriram Balasubramanian",
      "Samyadeep Basu",
      "Soheil Feizi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/93e67adb632d636bb2bcedf4eef1a6fe-Abstract-Conference.html": {
    "title": "Aggregate-and-Adapt Natural Language Prompts for Downstream Generalization of CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Huang",
      "Skyler Seto",
      "Samira Abnar",
      "David Grangier",
      "Navdeep Jaitly",
      "Joshua Susskind"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/93f250215e4889119807b6fac3a57aec-Abstract-Conference.html": {
    "title": "UPS: Unified Projection Sharing for Lightweight Single-Image Super-resolution and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Zhou",
      "Xinyu Lin",
      "Zhonghang LIU",
      "Xiaoguang Han",
      "Jiangbo Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/93fab021315170101c92e8330a56fbdb-Abstract-Conference.html": {
    "title": "Transforming Vision Transformer: Towards Efficient Multi-Task Asynchronous Learner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanwen Zhong",
      "Jiaxin Chen",
      "Yutong Zhang",
      "Di Huang",
      "Yunhong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/94074dd5a072d28ff75a76dabed43767-Abstract-Conference.html": {
    "title": "Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Kunin",
      "Allan Raventós",
      "Clémentine Dominé",
      "Feng Chen",
      "David Klindt",
      "Andrew Saxe",
      "Surya Ganguli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9415416201aa201902d1743c7e65787b-Abstract-Conference.html": {
    "title": "Segment Any Change",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuo Zheng",
      "Yanfei Zhong",
      "Liangpei Zhang",
      "Stefano Ermon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/941938d6c3c57b4ef4a518965e238a6d-Abstract-Conference.html": {
    "title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xun Zhu",
      "Ying Hu",
      "Fanbin Mo",
      "Miao Li",
      "Ji Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/94205e76ba4a5077ad0fac02b17bd46f-Abstract-Conference.html": {
    "title": "The surprising efficiency of temporal difference learning for rare event prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoou Cheng",
      "Jonathan Weare"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9421261e06f1a63a352b068f1ac90609-Abstract-Conference.html": {
    "title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xun Wu",
      "Shaohan Huang",
      "Guolong Wang",
      "Jing Xiong",
      "Furu Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/942d5e890e4e0ab0e7a5f8cbcef5b9ff-Abstract-Conference.html": {
    "title": "The Implicit Bias of Gradient Descent on Separable Multiclass Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hrithik Ravi",
      "Clay Scott",
      "Daniel Soudry",
      "Yutong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/942e820be4aa112509b3a281ff398851-Abstract-Conference.html": {
    "title": "Classifier Clustering and Feature Alignment for Federated Learning under Distributed Concept Drift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junbao Chen",
      "Jingfeng Xue",
      "Yong Wang",
      "Zhenyan Liu",
      "Lu Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9446c291a8744a125a0bda5b18f4d5a1-Abstract-Conference.html": {
    "title": "Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Ma",
      "Yang Liu",
      "Jingjing Liu",
      "Xiaoxu Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/946ecab300b0695fe24b53a92e632935-Abstract-Conference.html": {
    "title": "Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Jiao",
      "Shaoxiang Chen",
      "Zequn Jie",
      "Jingjing Chen",
      "Lin Ma",
      "Yu-Gang Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/947b63838c90f1485188b9c673bc3a14-Abstract-Conference.html": {
    "title": "Voxel Mamba: Group-Free State Space Models for Point Cloud based 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guowen Zhang",
      "Lue Fan",
      "Chenhang HE",
      "Zhen Lei",
      "ZHAO-XIANG ZHANG",
      "Lei Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9482f45fdd89aba9130bb04c44f788a9-Abstract-Conference.html": {
    "title": "FedLPA: One-shot Federated Learning with Layer-Wise Posterior Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Liu",
      "Liangxi Liu",
      "Feiyang Ye",
      "Yunheng Shen",
      "Xia Li",
      "Linshan Jiang",
      "Jialin Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/94840c41497ead6a84f493f029eba7fa-Abstract-Conference.html": {
    "title": "On the Comparison between Multi-modal and Single-modal Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Huang",
      "Andi Han",
      "Yongqiang Chen",
      "Yuan Cao",
      "Zhiqiang Xu",
      "Taiji Suzuki"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/948b1c9d660d7286dd767cd07dabd487-Abstract-Conference.html": {
    "title": "Stability and Generalizability in SDE Diffusion Models with Measure-Preserving Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weitong Zhang",
      "Chengqi Zang",
      "Liu Li",
      "Sarah Cechnicka",
      "Cheng Ouyang",
      "Bernhard Kainz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/948d8ba4e30c8c3a800cf436b31f376e-Abstract-Conference.html": {
    "title": "QGFN: Controllable Greediness with Action Values",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elaine Lau",
      "Stephen Lu",
      "Ling Pan",
      "Doina Precup",
      "Emmanuel Bengio"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/94a1fa3346407b2318ae7f8891d9f550-Abstract-Conference.html": {
    "title": "Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing Reliability, Reproducibility, and Practicality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianle Zhang",
      "Langtian Ma",
      "Yuchen Yan",
      "yuchen zhang",
      "yue yang",
      "Ziyao Guo",
      "Wenqi Shao",
      "Kai Wang",
      "Yang You",
      "Yu Qiao",
      "Ping Luo",
      "Kaipeng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/94bbcb744bbada8808fda05b9d9290d6-Abstract-Conference.html": {
    "title": "Mitigating Reward Overoptimization via Lightweight Uncertainty Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoying Zhang",
      "Jean-Francois Ton",
      "Wei Shen",
      "Hongning Wang",
      "Yang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/94bd1b8c906494bada774096bd0fdd73-Abstract-Conference.html": {
    "title": "A Structure-Aware Framework for Learning Device Placements on Computation Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shukai Duan",
      "Heng Ping",
      "Nikos Kanakaris",
      "Xiongye Xiao",
      "Panagiotis Kyriakis",
      "Nesreen K. Ahmed",
      "Peiyu Zhang",
      "Guixiang Ma",
      "Mihai Capotă",
      "Shahin Nazarian",
      "Theodore Willke",
      "Paul Bogdan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/94d13c2401fe119e57ba325b6fe526e0-Abstract-Conference.html": {
    "title": "Online Iterative Reinforcement Learning from Human Feedback with General Preference Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenlu Ye",
      "Wei Xiong",
      "Yuheng Zhang",
      "Hanze Dong",
      "Nan Jiang",
      "Tong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/94ef721705ea95d6981632be62bb66e2-Abstract-Conference.html": {
    "title": "SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Nie",
      "Dan Ding",
      "Chunwei Wang",
      "Yuanfan Guo",
      "Jianhua Han",
      "Hang Xu",
      "Li Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/94f008bcc2a5eb785fb8e0ad7aedd4fc-Abstract-Conference.html": {
    "title": "Higher-Order Causal Message Passing for Experimentation with Complex Interference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohsen Bayati",
      "Yuwei Luo",
      "William Overman",
      "Mohamad Sadegh Shirani Faradonbeh",
      "Ruoxuan Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/94f093b41fc2666376fb1f667fe282f3-Abstract-Conference.html": {
    "title": "SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niels Mündler",
      "Mark Müller",
      "Jingxuan He",
      "Martin Vechev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/94ffbc2939e18adac36eedf8d38d1ef3-Abstract-Conference.html": {
    "title": "Differentially Private Set Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarvar Patel",
      "Giuseppe Persiano",
      "Joon Young Seo",
      "Kevin Yeo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/951539113efa57260d88b76f79a800e9-Abstract-Conference.html": {
    "title": "realSEUDO for real-time calcium imaging analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iuliia Dmitrieva",
      "Sergey Babkin",
      "Adam S Charles"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/951877b24b376c5f4612e850251ee85b-Abstract-Conference.html": {
    "title": "Probabilistic Federated Prompt-Tuning with Non-IID and Imbalanced Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei-Yau Weng",
      "Minh Hoang",
      "Lam Nguyen",
      "My T. Thai",
      "Lily Weng",
      "Nghia Hoang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/95249d42f559a0cfaf282fdf26fe2e69-Abstract-Conference.html": {
    "title": "Monoculture in Matching Markets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenny Peng",
      "Nikhil Garg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/952ddaa9299d81cc307427edab034784-Abstract-Conference.html": {
    "title": "Input-to-State Stable Coupled Oscillator Networks for Closed-form Model-based Control in Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Stölzle",
      "Cosimo Della Santina"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/953d276d037e701fcd97dbb34ebb2394-Abstract-Conference.html": {
    "title": "SceneCraft: Layout-Guided 3D Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuyu Yang",
      "Yunze Man",
      "Junkun Chen",
      "Yu-Xiong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/953e742190ca02fc8f9f710052f2fead-Abstract-Conference.html": {
    "title": "Bayes-optimal learning of an extensive-width neural network from quadratically many samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Maillard",
      "Emanuele Troiani",
      "Simon Martin",
      "Florent Krzakala",
      "Lenka Zdeborová"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/95827e011b9e899f189a01fe2f4ef316-Abstract-Conference.html": {
    "title": "One-Layer Transformer Provably Learns One-Nearest Neighbor In Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Li",
      "Yuan Cao",
      "Cheng Gao",
      "Yihan He",
      "Han Liu",
      "Jason Klusowski",
      "Jianqing Fan",
      "Mengdi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/959f70ee50044bed305e48e3484005a7-Abstract-Conference.html": {
    "title": "Promoting Fairness Among Dynamic Agents in Online-Matching Markets under Known Stationary Arrival Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Will Ma",
      "Pan Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/95c6ae3f3393786203a4b6dcb9df1036-Abstract-Conference.html": {
    "title": "RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maya Varma",
      "Jean-Benoit Delbrouck",
      "Zhihong Chen",
      "Akshay Chaudhari",
      "Curtis Langlotz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/95dcc1f6463491d37a8918c1d38380a7-Abstract-Conference.html": {
    "title": "From Trojan Horses to Castle Walls: Unveiling Bilateral Data Poisoning Effects in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoshi Pan",
      "Yuguang Yao",
      "Gaowen Liu",
      "Bingquan Shen",
      "H. Vicky Zhao",
      "Ramana Kompella",
      "Sijia Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/95dd2f8d32badf8959844ef0c2528d31-Abstract-Conference.html": {
    "title": "Sample Complexity of Posted Pricing for a Single Item",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Billy Jin",
      "Thomas Kesselheim",
      "Will Ma",
      "Sahil Singla"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/95ddc8f24ad9698e3f2c4a014602ee74-Abstract-Conference.html": {
    "title": "Generalizing CNNs to graphs with learnable neighborhood quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isaac Osafo Nkansah",
      "Neil Gallagher",
      "Ruchi Sandilya",
      "Conor Liston",
      "Logan Grosenick"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/96189e90e599ccc43f00434ff3ed0312-Abstract-Conference.html": {
    "title": "Leveraging Separated World Model for Exploration in Visually Distracted Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaichen Huang",
      "Shenghua Wan",
      "Minghao Shao",
      "Hai-Hang Sun",
      "Le Gan",
      "Shuai Feng",
      "De-Chuan Zhan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9626a58529367967b71c17c9b2db72f1-Abstract-Conference.html": {
    "title": "Preventing Model Collapse in Deep Canonical Correlation Analysis by Noise Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junlin He",
      "Jinxiao Du",
      "Susu Xu",
      "Wei Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9628b97f74cd8f83ad12b631378b81e2-Abstract-Conference.html": {
    "title": "Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanlai Yang",
      "Matt Jones",
      "Michael Mozer",
      "Mengye Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9647157086adf5aa2c0217fb7f82bb19-Abstract-Conference.html": {
    "title": "Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Yang",
      "Cuiling Lan",
      "Yan Lu",
      "Nanning Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/964ce727dccdc18aa9bad54604699bbe-Abstract-Conference.html": {
    "title": "Error Correction Output Codes for Robust Neural Networks against Weight-errors: A Neural Tangent Kernel Point of View",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anlan Yu",
      "Shusen Jing",
      "Ning Lyu",
      "Wujie Wen",
      "Zhiyuan Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/966bf8492dbaa8376f5e3756c09d7edb-Abstract-Conference.html": {
    "title": "Optimal Rates for Vector-Valued Spectral Regularization Learning Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitri Meunier",
      "Zikai Shen",
      "Mattes Mollenhauer",
      "Arthur Gretton",
      "Zhu Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/967017dbe801dc95f5a2587c6d6a1ef3-Abstract-Conference.html": {
    "title": "Conjugated Semantic Pool Improves OOD Detection with Pre-trained Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyuan Chen",
      "Junyu Gao",
      "Changsheng Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/96af549bb29d01b1713e88e5c3899ceb-Abstract-Conference.html": {
    "title": "Learning from Noisy Labels via Conditional Distributionally Robust Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui GUO",
      "Grace Yi",
      "Boyu Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/96b42e02294543f0b226f862be75472e-Abstract-Conference.html": {
    "title": "CE-NAS: An End-to-End Carbon-Efficient Neural Architecture Search Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Zhao",
      "Yunzhuo Liu",
      "Bo Jiang",
      "Tian Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/96b8167534ef3cc30c230bbeb55a524d-Abstract-Conference.html": {
    "title": "Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Yao",
      "Qi Qian",
      "Juhua Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/96bbdd0ed2a9e7cd2fb7caf2fae15f3d-Abstract-Conference.html": {
    "title": "RL in Latent MDPs is Tractable: Online Guarantees via Off-Policy Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongyeol Kwon",
      "Shie Mannor",
      "Constantine Caramanis",
      "Yonathan Efroni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/96ddbf813f042e8ff891b4d6f7149bb6-Abstract-Conference.html": {
    "title": "REDUCR: Robust Data Downsampling using Class Priority Reweighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Bankes",
      "George Hughes",
      "Ilija Bogunovic",
      "Zi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/97005e2b425c93015a91c312ba5c5298-Abstract-Conference.html": {
    "title": "Auditing Privacy Mechanisms via Label Inference Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Róbert Busa-Fekete",
      "Travis Dick",
      "Claudio Gentile",
      "Andres Munoz Medina",
      "Adam Smith",
      "Marika Swanberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9705eec26e511d7844a591ce10c33a3f-Abstract-Conference.html": {
    "title": "Provable Posterior Sampling with Denoising Oracles via Tilted Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joan Bruna",
      "Jiequn Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9709b85c06f72243cc2ea335c0110a94-Abstract-Conference.html": {
    "title": "Theoretical Investigations and Practical Enhancements on Tail Task Risk Minimization in Meta Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqin Lv",
      "Qi Wang",
      "Dong Liang",
      "Zheng Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/970cbc9be823f381392db75c0b00e17c-Abstract-Conference.html": {
    "title": "SAM-Guided Masked Token Prediction for 3D Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhimin Chen",
      "Liang Yang",
      "Yingwei Li",
      "Longlong Jing",
      "Bing Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/970f59b22f4c72aec75174aae63c7459-Abstract-Conference.html": {
    "title": "Gradient-free Decoder Inversion in Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongmin Hong",
      "Suh Yoon Jeon",
      "Kyeonghyun Lee",
      "Ernest Ryu",
      "Se Young Chun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9713d53ee4f31781304b1ca43266f8d1-Abstract-Conference.html": {
    "title": "GACL: Exemplar-Free Generalized Analytic Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HUIPING ZHUANG",
      "Yizhu Chen",
      "Di Fang",
      "Run He",
      "Kai Tong",
      "Hongxin Wei",
      "Ziqian Zeng",
      "Cen Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/971f1e59cd956cc094da4e2f78c6ea7c-Abstract-Conference.html": {
    "title": "QT-ViT: Improving Linear Attention in ViT with Quadratic Taylor Expansion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixing Xu",
      "Chao Li",
      "Dong Li",
      "Xiao Sheng",
      "Fan Jiang",
      "Lu Tian",
      "Emad Barsoum"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/972cd27c994a806e187ef1c2f5254059-Abstract-Conference.html": {
    "title": "Self-supervised Transformation Learning for Equivariant Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaemyung Yu",
      "Jaehyun Choi",
      "DongJae Lee",
      "HyeongGwon Hong",
      "Junmo Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9736acf007760cc2b47948ae3cf06274-Abstract-Conference.html": {
    "title": "Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Karvonen",
      "Benjamin Wright",
      "Can Rager",
      "Rico Angell",
      "Jannik Brinkmann",
      "Logan Smith",
      "Claudio Mayrink Verdun",
      "David Bau",
      "Samuel Marks"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9740da1c07c7b451af14e11523f95271-Abstract-Conference.html": {
    "title": "Towards a theory of how the structure of language is acquired by deep neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Cagnetta",
      "Matthieu Wyart"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/976cc04f0cbaad7790ce0d665e44f90f-Abstract-Conference.html": {
    "title": "AnyFit: Controllable Virtual Try-on for Any Combination of Attire Across Any Scenario",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Li",
      "Hao Zhou",
      "Wenxiang Shang",
      "Ran Lin",
      "Xuanhong Chen",
      "Bingbing Ni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/978cc34c539fd26f0e8afb7e3905f34a-Abstract-Conference.html": {
    "title": "Non-Stationary Learning of Neural Networks with Automatic Soft Parameter Reset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Galashov",
      "Michalis K. Titsias",
      "András György",
      "Clare Lyle",
      "Razvan Pascanu",
      "Yee Whye Teh",
      "Maneesh Sahani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/97a0f7d7fc2c26732943860241af1c7a-Abstract-Conference.html": {
    "title": "Fairness-Aware Meta-Learning via Nash Bargaining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zeng",
      "Xuelin Yang",
      "Li Chen",
      "Cristian Ferrer",
      "Ming Jin",
      "Michael I. Jordan",
      "Ruoxi Jia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/97aa3f6600add4b521e1f2ae297653ad-Abstract-Conference.html": {
    "title": "Trajectory Data Suffices for Statistically Efficient Learning in Offline RL with Linear $q^\\pi$-Realizability and Concentrability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Volodymyr Tkachuk",
      "Gellert Weisz",
      "Csaba Szepesvari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/97c2f0fac182353062d304d0322ae285-Abstract-Conference.html": {
    "title": "Scaling Law for Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingzhe Shi",
      "Qinwei Ma",
      "Huan Ma",
      "Lei Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/97ca7168c2c333df5ea61ece3b3276e1-Abstract-Conference.html": {
    "title": "Improving Alignment and Robustness with Circuit Breakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andy Zou",
      "Long Phan",
      "Justin Wang",
      "Derek Duenas",
      "Maxwell Lin",
      "Maksym Andriushchenko",
      "J. Zico Kolter",
      "Matt Fredrikson",
      "Dan Hendrycks"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/97d008f7873b8dd55cb6dd343fc4386f-Abstract-Conference.html": {
    "title": "Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Wen",
      "Leo Marchyok",
      "Sanghyun Hong",
      "Jonas Geiping",
      "Tom Goldstein",
      "Nicholas Carlini"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/97d539aa02b82c52d4a9eda62e1c6435-Abstract-Conference.html": {
    "title": "Model Reconstruction Using Counterfactual Explanations: A Perspective From Polytope Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pasan Dissanayake",
      "Sanghamitra Dutta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/97dc07f1253ab33ee514f395a82fa7cc-Abstract-Conference.html": {
    "title": "TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Feuer",
      "Robin Schirrmeister",
      "Valeriia Cherepanova",
      "Chinmay Hegde",
      "Frank Hutter",
      "Micah Goldblum",
      "Niv Cohen",
      "Colin White"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/97fe251c25b6f99a2a23b330a75b11d4-Abstract-Conference.html": {
    "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Yang",
      "Siddhartha Mishra",
      "Jeffrey Chiang",
      "Baharan Mirzasoleiman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/98082e6b4b97ab7d3af1134a5013304e-Abstract-Conference.html": {
    "title": "Improving Equivariant Model Training via Constraint Relaxation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefanos Pertigkiozoglou",
      "Evangelos Chatzipantazis",
      "Shubhendu Trivedi",
      "Kostas Daniilidis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/983876577ec81db17ecfae1521df9208-Abstract-Conference.html": {
    "title": "ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Pang",
      "Masoumeh Shafieinejad",
      "Lucy Liu",
      "Stephanie Hazlewood",
      "Xi He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/984de836e696ba653bbfbbbfce31d3bc-Abstract-Conference.html": {
    "title": "Multi-language Diversity Benefits Autoformalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Albert Q. Jiang",
      "Wenda Li",
      "Mateja Jamnik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/984f0eb750c5ea4793e56414df028454-Abstract-Conference.html": {
    "title": "The Value of Reward Lookahead in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nadav Merlis",
      "Dorian Baudry",
      "Vianney Perchet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/984fa4634385c48ab3722d825c57ede0-Abstract-Conference.html": {
    "title": "On the Stability and Generalization of Meta-Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunjuan Wang",
      "Raman Arora"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9856b5d30ac61ab744fdab6f67d874e4-Abstract-Conference.html": {
    "title": "Cross-model Control: Improving Multiple Large Language Models in One-time Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Wu",
      "Hao Sun",
      "Hengyi Cai",
      "Lixin Su",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Xiang Li",
      "Ming Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9861a7c3972ed5d36dda3826d44bb246-Abstract-Conference.html": {
    "title": "How Does Variance Shape the Regret in Contextual Bandits?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Jia",
      "Jian Qian",
      "Alexander Rakhlin",
      "Chen-Yu Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/986292a930c3692168b177a770025ab3-Abstract-Conference.html": {
    "title": "Understanding and Minimising Outlier Features in Transformer Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bobby He",
      "Lorenzo Noci",
      "Daniele Paliotta",
      "Imanol Schlag",
      "Thomas Hofmann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/98904db124b2a2463e8c59ec33fc7150-Abstract-Conference.html": {
    "title": "Guided Trajectory Generation with Diffusion Models for Offline Model-based Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taeyoung Yun",
      "Sujin Yun",
      "Jaewoo Lee",
      "Jinkyoo Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/989463e7e34dc152bef08156dac95569-Abstract-Conference.html": {
    "title": "Towards training digitally-tied analog blocks via hybrid gradient computation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timothy Nest",
      "Maxence Ernoult"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/989be684b6315b788a0cc18e2775f045-Abstract-Conference.html": {
    "title": "Achievable distributional robustness when the robust risk is only partially identified",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julia Kostin",
      "Nicola Gnecco",
      "Fanny Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/98a29475083c502c34949f9baa1aa2ef-Abstract-Conference.html": {
    "title": "Phased Consistency Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu-Yun Wang",
      "Zhaoyang Huang",
      "Alexander Bergman",
      "Dazhong Shen",
      "Peng Gao",
      "Michael Lingelbach",
      "Keqiang Sun",
      "Weikang Bian",
      "Guanglu Song",
      "Yu Liu",
      "Xiaogang Wang",
      "Hongsheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/98b2b307aa4aa323df2ba3a83460f25e-Abstract-Conference.html": {
    "title": "Zero-shot Image Editing with Reference Imitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Chen",
      "Yutong Feng",
      "Mengting Chen",
      "Yiyang Wang",
      "Shilong Zhang",
      "Yu Liu",
      "Yujun Shen",
      "Hengshuang Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/98bf3b8505c611ac21055dd9d355c66e-Abstract-Conference.html": {
    "title": "Toward Efficient Inference for Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Huang",
      "Newsha Ardalani",
      "Anna Sun",
      "Liu Ke",
      "Shruti Bhosale",
      "Hsien-Hsin Lee",
      "Carole-Jean Wu",
      "Benjamin Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/98d112a7111970b1638dafc97d96d9ee-Abstract-Conference.html": {
    "title": "Learning Disentangled Representations for Perceptual Point Cloud Quality Assessment via Mutual Information Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Shan",
      "Yujie Zhang",
      "Yipeng Liu",
      "YILING XU"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/98d17a9632e1534bae96793e99dc3c2d-Abstract-Conference.html": {
    "title": "Practical Shuffle Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julius Kunze",
      "Daniel Severo",
      "Jan-Willem van de Meent",
      "James Townsend"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/990488efad2c59c619d2b1006a1986d9-Abstract-Conference.html": {
    "title": "DN-4DGS: Denoised Deformable Network with Temporal-Spatial Aggregation for Dynamic Scene Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Lu",
      "Jiacheng Deng",
      "Ruijie Zhu",
      "Yanzhe Liang",
      "Wenfei Yang",
      "Xu Zhou",
      "Tianzhu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/990641d09f71bcee0060a8f1704ab8e2-Abstract-Conference.html": {
    "title": "PGN: The RNN's New Successor is Effective for Long-Range Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Jia",
      "Youfang Lin",
      "Jing Yu",
      "Shuo Wang",
      "Tianhao Liu",
      "Huaiyu Wan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/99261adc8a6356b38bcf999bba9a26dc-Abstract-Conference.html": {
    "title": "Dissect Black Box: Interpreting for Rule-Based Explanations in Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhang",
      "Ruoyu Li",
      "Nengwu Wu",
      "Qing Li",
      "Xinhan Lin",
      "Yang Hu",
      "Tao Li",
      "Yong Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9952369f49ecc064d169fe6612cbf204-Abstract-Conference.html": {
    "title": "Vision Transformer Neural Architecture Search for Out-of-Distribution Generalization: Benchmark and Insights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sy-Tuyen Ho",
      "Tuan Van Vo",
      "Somayeh Ebrahimkhani",
      "Ngai-Man (Man) Cheung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9961e42624a6c083279303767c73269d-Abstract-Conference.html": {
    "title": "Information-theoretic Generalization Analysis for Expected Calibration Error",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Futoshi Futami",
      "Masahiro Fujisawa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/996bef37d8a638f37bdfcac2789e835d-Abstract-Conference.html": {
    "title": "Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Usha Bhalla",
      "Alex Oesterling",
      "Suraj Srinivas",
      "Flavio Calmon",
      "Himabindu Lakkaraju"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/997089469acbeb410405e43f0011be1f-Abstract-Conference.html": {
    "title": "Small coresets via negative dependence: DPPs, linear statistics, and concentration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rémi Bardenet",
      "Subhroshekhar Ghosh",
      "Hugo Simon-Onfroy",
      "Hoang Son Tran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9979a69d2613ab98ad25d3849068f9f0-Abstract-Conference.html": {
    "title": "One-Shot Safety Alignment for Large Language Models via Optimal Dualization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinmeng Huang",
      "Shuo Li",
      "Edgar Dobriban",
      "Osbert Bastani",
      "Hamed Hassani",
      "Dongsheng Ding"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/99831104028c3b7e6079fd8bdcc42c8f-Abstract-Conference.html": {
    "title": "Mind the Gap: A Causal Perspective on Bias Amplification in Prediction & Decision-Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Drago Plecko",
      "Elias Bareinboim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9988f2c8e07c1f98af7ba9ca31ccae0b-Abstract-Conference.html": {
    "title": "Leveraging an ECG Beat Diffusion Model for Morphological Reconstruction from Indirect Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lisa Bedin",
      "Gabriel Cardoso",
      "Josselin Duchateau",
      "Remi Dubois",
      "Eric Moulines"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/998f8d8ff2697da2eae5f87143668754-Abstract-Conference.html": {
    "title": "FASTopic: Pretrained Transformer is a Fast, Adaptive, Stable, and Transferable Topic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobao Wu",
      "Thong Nguyen",
      "Delvin Zhang",
      "William Yang Wang",
      "Anh Tuan Luu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/99c66755871ae101a4cef87c67fb29e8-Abstract-Conference.html": {
    "title": "EffiLearner: Enhancing Efficiency of Generated Code via Self-Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong HUANG",
      "Jianbo Dai",
      "Han Weng",
      "Puzhen Wu",
      "Yuhao QING",
      "Heming Cui",
      "Zhijiang Guo",
      "Jie Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/99c80ceb10cb674110f03b2def6a5b76-Abstract-Conference.html": {
    "title": "Don't Compress Gradients in Random Reshuffling: Compress Gradient Differences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdurakhmon Sadiev",
      "Grigory Malinovsky",
      "Eduard Gorbunov",
      "Igor Sokolov",
      "Ahmed Khaled",
      "Konstantin Burlachenko",
      "Peter Richtarik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/99d27a4939a1dfcdde8394b9137bd885-Abstract-Conference.html": {
    "title": "Hybrid Generative AI for De Novo Design of Co-Crystals with Enhanced Tabletability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nina Gubina",
      "Andrei Dmitrenko",
      "Gleb Solovev",
      "Lyubov Yamshchikova",
      "Oleg Petrov",
      "Ivan Lebedev",
      "Nikita Serov",
      "Grigorii Kirgizov",
      "Nikolay Nikitin",
      "Vladimir Vinogradov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/99d4ceebdf75b64e8ed608a245b63416-Abstract-Conference.html": {
    "title": "Hyper-opinion Evidential Deep Learning for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingen Qu",
      "Yufei Chen",
      "Xiaodong Yue",
      "Wei Fu",
      "Qiguang Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/99d7a578d72ed133203d1f88c9d39044-Abstract-Conference.html": {
    "title": "Mimicking To Dominate: Imitation Learning Strategies for Success in Multiagent Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "The Viet Bui",
      "Tien Mai",
      "Thanh Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/99e6bcf460ea36818cf236da29311e73-Abstract-Conference.html": {
    "title": "Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Core Francisco Park",
      "Maya Okawa",
      "Andrew Lee",
      "Ekdeep S Lubana",
      "Hidenori Tanaka"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/99fc8bc48b917c301a80cb74d91c0c06-Abstract-Conference.html": {
    "title": "Parameter Competition Balancing for Model Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guodong DU",
      "Junlin Lee",
      "Jing Li",
      "Runhua Jiang",
      "Yifei Guo",
      "Shuyang Yu",
      "Hanting Liu",
      "Sim Kuan Goh",
      "Ho-Kin Tang",
      "Daojing He",
      "Min Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/99fecf765ecf62c3e3175ef2278f3315-Abstract-Conference.html": {
    "title": "Deep Learning for Computing Convergence Rates of Markov Chains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanlin Qu",
      "Jose Blanchet",
      "Peter W Glynn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9a16935bf54c4af233e25d998b7f4a2c-Abstract-Conference.html": {
    "title": "CultureLLM: Incorporating Cultural Differences into Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Li",
      "Mengzhuo Chen",
      "Jindong Wang",
      "Sunayana Sitaram",
      "Xing Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9a24e284b187f662681440ba15c416fb-Abstract-Conference.html": {
    "title": "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keyu Tian",
      "Yi Jiang",
      "Zehuan Yuan",
      "BINGYUE PENG",
      "Liwei Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9a263e235f6d1521d13a8531c7974951-Abstract-Conference.html": {
    "title": "PointAD: Comprehending 3D Anomalies from Points and Pixels for Zero-shot 3D Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Zhou",
      "Jiangtao Yan",
      "Shibo He",
      "Wenchao Meng",
      "Jiming Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9a2b834905136e2b67136df3183a9032-Abstract-Conference.html": {
    "title": "Personalized Adapter for Large Meteorology Model on Devices: Towards Weather Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengchao Chen",
      "Guodong Long",
      "Jing Jiang",
      "Chengqi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9a379c1b05793d1c42dc832269834515-Abstract-Conference.html": {
    "title": "NeuralFluid: Nueral Fluidic System Design and Control with Differentiable Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Li",
      "Yuchen Sun",
      "Pingchuan Ma",
      "Eftychios Sifakis",
      "Tao Du",
      "Bo Zhu",
      "Wojciech Matusik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9a3942c235daa9c5f62a8598ae81a946-Abstract-Conference.html": {
    "title": "What does guidance do? A fine-grained analysis in a simple setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muthu Chidambaram",
      "Khashayar Gatmiry",
      "Sitan Chen",
      "Holden Lee",
      "Jianfeng Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9a439efaa34fe37177eba00737624824-Abstract-Conference.html": {
    "title": "Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sonia Laguna",
      "Ričards Marcinkevičs",
      "Moritz Vandenhirtz",
      "Julia Vogt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9a6f6e0d6781d1cb8689192408946d73-Abstract-Conference.html": {
    "title": "Images that Sound: Composing Images and Sounds on a Single Canvas",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Chen",
      "Daniel Geng",
      "Andrew Owens"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9a8d52eb05eb7b13f54b3d9eada667b7-Abstract-Conference.html": {
    "title": "DeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Belouadi",
      "Simone Ponzetto",
      "Steffen Eger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9a987c98a7f36cc83f9065df3ca4f9e0-Abstract-Conference.html": {
    "title": "MagR: Weight Magnitude Reduction for Enhancing Post-Training Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aozhong Zhang",
      "Naigang Wang",
      "Yanxia Deng",
      "Xin Li",
      "Zi Yang",
      "Penghang Yin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9aa51796f8bede2ea947d6b6e3087ab8-Abstract-Conference.html": {
    "title": "Boosting Alignment for Post-Unlearning Text-to-Image Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myeongseob Ko",
      "Henry Li",
      "Zhun Wang",
      "Jonathan Patsenker",
      "Jiachen (Tianhao) Wang",
      "Qinbin Li",
      "Ming Jin",
      "Dawn Song",
      "Ruoxi Jia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9abc1d4bcdb4e61e7c0e01a11ab7c375-Abstract-Conference.html": {
    "title": "Federated Learning over Connected Modes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dennis Grinwald",
      "Philipp Wiesner",
      "Shinichi Nakajima"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9ad996b5c45130de2bc00b60d8607904-Abstract-Conference.html": {
    "title": "Faster Diffusion: Rethinking the Role of the Encoder for Diffusion Model Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Senmao Li",
      "Taihang Hu",
      "Joost van de Weijer",
      "Fahad Shahbaz Khan",
      "Tao Liu",
      "Linxuan Li",
      "Shiqi Yang",
      "Yaxing Wang",
      "Ming-Ming Cheng",
      "jian Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9adc8ada9183f4b9a007a02773fd8114-Abstract-Conference.html": {
    "title": "A provable control of sensitivity of neural networks through a direct parameterization of the overall bi-Lipschitzness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuri Kinoshita",
      "Taro Toyoizumi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9af09f8aa60f81dad5dcdbafff605b0d-Abstract-Conference.html": {
    "title": "Dynamic Subgroup Identification in Covariate-adjusted Response-adaptive Randomization Experiments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanping Li",
      "Jingshen Wang",
      "Waverly Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9b01c4a7d3fc49875dad3c13848bcd9e-Abstract-Conference.html": {
    "title": "TinyLUT: Tiny Look-Up Table for Efficient Image Restoration at the Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huanan LI",
      "Juntao Guan",
      "Lai Rui",
      "Sijun Ma",
      "Lin Gu",
      "Noperson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9b14a88c6e50068a97256696902521bf-Abstract-Conference.html": {
    "title": "PRODuctive bandits: Importance Weighting No More",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Zimmert",
      "Teodor Vanislavov Marinov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9b224ace8963c9385ad5e2b5c9039b97-Abstract-Conference.html": {
    "title": "Continual Learning in the Frequency Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "RuiQi Liu",
      "Boyu Diao",
      "Libo Huang",
      "Zijia An",
      "Zhulin An",
      "Yongjun Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9b28dbda50ac90dcc73b2ba7d0dd2381-Abstract-Conference.html": {
    "title": "Advancing Open-Set Domain Generalization Using Evidential Bi-Level Hardest Domain Scheduler",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunyu Peng",
      "Di Wen",
      "Kailun Yang",
      "Ao Luo",
      "Yufan Chen",
      "Jia Fu",
      "M. Saquib Sarfraz",
      "Alina Roitberg",
      "Rainer Stiefelhagen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9b333cc1c9eb36e479b27f8c19f0873c-Abstract-Conference.html": {
    "title": "Flexible mapping of abstract domains by grid cells via self-supervised extraction and projection of generalized velocity signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhiram Iyer",
      "Sarthak Chandra",
      "Sugandha Sharma",
      "Ila Fiete"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9b35a0a20d617dc68ae98a7a57df2f51-Abstract-Conference.html": {
    "title": "Protected Test-Time Adaptation via Online Entropy Matching: A Betting Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yarin Bar",
      "Shalev Shaer",
      "Yaniv Romano"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9b3c956210432122aa5385c355fc7c6e-Abstract-Conference.html": {
    "title": "Identification of Analytic Nonlinear Dynamical Systems with Non-asymptotic Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Negin Musavi",
      "Ziyao Guo",
      "Geir Dullerud",
      "Yingying Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9b77f07301b1ef1fe810aae96c12cb7b-Abstract-Conference.html": {
    "title": "A Closer Look at the CLS Token for Cross-Domain Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiong Zou",
      "Shuai Yi",
      "Yuhua Li",
      "Ruixuan Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9b812ee4b831c21e14156ced8659197c-Abstract-Conference.html": {
    "title": "INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hung Le",
      "Yingbo Zhou",
      "Caiming Xiong",
      "Silvio Savarese",
      "Doyen Sahoo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9b9aa183c3ab49380e0f306e9f130acf-Abstract-Conference.html": {
    "title": "On the Necessity of Collaboration for Online Model Selection with Decentralized Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfan Li",
      "Zheshun Wu",
      "Zenglin Xu",
      "Irwin King"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9b9cfd5428153ccfbd4ba34b7e007305-Abstract-Conference.html": {
    "title": "Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Saxon",
      "Fatima Jahara",
      "Mahsa Khoshnoodi",
      "Yujie Lu",
      "Aditya Sharma",
      "William Yang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9baded199ea27b236a9d2582f756c34f-Abstract-Conference.html": {
    "title": "Exploiting the Replay Memory Before Exploring the Environment: Enhancing Reinforcement Learning Through Empirical MDP Iteration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongming Zhang",
      "Chenjun Xiao",
      "Chao Gao",
      "Han Wang",
      "bo xu",
      "Martin Müller"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9bae399d1f34b8650351c1bd3692aeae-Abstract-Conference.html": {
    "title": "SocraticLM: Exploring Socratic Personalized Teaching with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayu Liu",
      "Zhenya Huang",
      "Tong Xiao",
      "Jing Sha",
      "Jinze Wu",
      "Qi Liu",
      "Shijin Wang",
      "Enhong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9baf31febefde7bd76023c2d2f13cbd7-Abstract-Conference.html": {
    "title": "IWBVT: Instance Weighting-based Bias-Variance Trade-off for Crowdsourcing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjun Zhang",
      "Liangxiao Jiang",
      "Chaoqun Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9bb4d1b3fbeed86a0e854d736ea1d293-Abstract-Conference.html": {
    "title": "A-FedPD: Aligning Dual-Drift is All Federated Primal-Dual Learning Needs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Sun",
      "Li Shen",
      "Dacheng Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9bcd1fa0c05e5f25ba7a1261f1852e82-Abstract-Conference.html": {
    "title": "Sub-optimal Experts mitigate Ambiguity in Inverse Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Poiani",
      "Curti Gabriele",
      "Alberto Maria Metelli",
      "Marcello Restelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9be39b35906526b8d240056daac72c6f-Abstract-Conference.html": {
    "title": "Leveraging Contrastive Learning for Enhanced Node Representations in Tokenized Graph Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsong Chen",
      "Hanpeng Liu",
      "John Hopcroft",
      "Kun He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9be9407431b7ff8cc04cae5460fcb7ab-Abstract-Conference.html": {
    "title": "DoFIT: Domain-aware Federated Instruction Tuning with Alleviated Catastrophic Forgetting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binqian Xu",
      "Xiangbo Shu",
      "Haiyang Mei",
      "Zechen Bai",
      "Basura Fernando",
      "Mike Zheng Shou",
      "Jinhui Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9bfa0c155653e24120760a5ead819376-Abstract-Conference.html": {
    "title": "In-Context Learning with Representations: Contextual Generalization of Trained Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Yang",
      "Yu Huang",
      "Yingbin Liang",
      "Yuejie Chi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9c1c1c2be31aa04da26a65057ad95a6b-Abstract-Conference.html": {
    "title": "Multi-Winner Reconfiguration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiehua Chen",
      "Christian Hatschka",
      "Sofia Simola"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9c20f16b05f5e5e70fa07e2a4364b80e-Abstract-Conference.html": {
    "title": "Large Language Models Must Be Taught to Know What They Don't Know",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanyam Kapoor",
      "Nate Gruver",
      "Manley Roberts",
      "Katie Collins",
      "Arka Pal",
      "Umang Bhatt",
      "Adrian Weller",
      "Samuel Dooley",
      "Micah Goldblum",
      "Andrew G Wilson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9c26a48b7ebd37bc3e979c8651fc3649-Abstract-Conference.html": {
    "title": "Learning Infinitesimal Generators of Continuous Symmetries from Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyeonghoon Ko",
      "Hyunsu Kim",
      "Juho Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9c3828adf1500f5de3c56f6550dfe43c-Abstract-Conference.html": {
    "title": "Brain-JEPA: Brain Dynamics Foundation Model with Gradient Positioning and Spatiotemporal Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Dong",
      "Ruilin Li",
      "Yilei Wu",
      "Thuan Tinh Nguyen",
      "Joanna Chong",
      "Fang Ji",
      "Nathanael Tong",
      "Christopher Chen",
      "Juan Helen Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9c43057f39d49b8b5c989cc1aac70ab7-Abstract-Conference.html": {
    "title": "StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JIANAN LI",
      "Quan Tu",
      "Cunli Mao",
      "Zhengtao Yu",
      "Ji-Rong Wen",
      "Rui Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9c4bbdad95f6ffed1a15c06b491e0a3e-Abstract-Conference.html": {
    "title": "Enhancing Robustness in Deep Reinforcement Learning: A Lyapunov Exponent Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rory Young",
      "Nicolas Pugeault"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9c7900fac04a701cbed83256b76dbaa3-Abstract-Conference.html": {
    "title": "SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yachao Liang",
      "Min Yu",
      "Gang Li",
      "Jianguo Jiang",
      "Boquan Li",
      "Feng Yu",
      "Ning Zhang",
      "Xiang Meng",
      "Weiqing Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9c7eeda2dc98e61baa9a5884afd231bc-Abstract-Conference.html": {
    "title": "On Feature Learning in Structured State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leena Chennuru Vankadara",
      "Jin Xu",
      "Moritz Haas",
      "Volkan Cevher"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9c8df8de46c1a1b39b30b9f74be69c02-Abstract-Conference.html": {
    "title": "Toward Conditional Distribution Calibration in Survival Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shi-ang Qi",
      "Yakun Yu",
      "Russell Greiner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9cb5b083ba4f5ca6bd05dd307a2fb354-Abstract-Conference.html": {
    "title": "Cascade Speculative Drafting for Even Faster LLM Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Chen",
      "Xiaocong Yang",
      "Jiacheng Lin",
      "Chenkai Sun",
      "Kevin Chang",
      "Jie Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9cdb4f8c4dfa13284d2d5a6e7853e5a2-Abstract-Conference.html": {
    "title": "Local to Global: Learning Dynamics and Effect of Initialization for Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashok Vardhan Makkuva",
      "Marco Bondaschi",
      "Adway Girish",
      "Alliot Nagle",
      "Hyeji Kim",
      "Michael Gastpar",
      "Chanakya Ekbote"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9cef1316eaef9bd99da46f63334dc031-Abstract-Conference.html": {
    "title": "Localized Zeroth-Order Prompt Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyang Hu",
      "Yao Shu",
      "Zongmin Yu",
      "Zhaoxuan Wu",
      "Xiaoqiang Lin",
      "Zhongxiang Dai",
      "See-Kiong Ng",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9cf904c86cc5f9ac95646c07d2cfa241-Abstract-Conference.html": {
    "title": "Bayesian Optimisation with Unknown Hyperparameters: Regret Bounds Logarithmically Closer to Optimal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juliusz Ziomek",
      "Masaki Adachi",
      "Michael A Osborne"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9d0947107ea92d6ce369dce7749180dd-Abstract-Conference.html": {
    "title": "Neural Model Checking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mirco Giacobbe",
      "Daniel Kroening",
      "Abhinandan Pal",
      "Michael Tautschnig"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9d3faa41886997cfc2128b930077fa49-Abstract-Conference.html": {
    "title": "Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miaosen Zhang",
      "Yixuan Wei",
      "Zhen Xing",
      "Yifei Ma",
      "Zuxuan Wu",
      "Ji Li",
      "Zheng Zhang",
      "Qi Dai",
      "Chong Luo",
      "Xin Geng",
      "Baining Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9d4e58d9c6abac29374bedbc5b6f4758-Abstract-Conference.html": {
    "title": "How many classifiers do we need?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunsuk Kim",
      "Liam Hodgkinson",
      "Ryan Theisen",
      "Michael W. Mahoney"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9d66f74820f11ce037fb5f711ab9acd4-Abstract-Conference.html": {
    "title": "HENASY: Learning to Assemble Scene-Entities for Interpretable Egocentric Video-Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khoa Vo",
      "Thinh Phan",
      "Kashu Yamazaki",
      "Minh Tran",
      "Ngan Le"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9d6d351ba8028a50382f42a065d31bf0-Abstract-Conference.html": {
    "title": "SpaFL: Communication-Efficient Federated Learning With Sparse Models And Low Computational Overhead",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsu Kim",
      "Walid Saad",
      "Merouane DEBBAH",
      "Choong Hong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9d88b87b31986f8293bb0067a841579e-Abstract-Conference.html": {
    "title": "Discovering Preference Optimization Algorithms with and for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Lu",
      "Samuel Holt",
      "Claudio Fanconi",
      "Alex Chan",
      "Jakob Foerster",
      "Mihaela van der Schaar",
      "Robert Lange"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9d8cf1247786d6dfeefeeb53b8b5f6d7-Abstract-Conference.html": {
    "title": "Interventional Causal Discovery in a Mixture of DAGs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Burak Varıcı",
      "Dmitriy Katz",
      "Dennis Wei",
      "Prasanna Sattigeri",
      "Ali Tajer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9dcc5038eb565e625c7ebdde0435d958-Abstract-Conference.html": {
    "title": "Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhishuai Liu",
      "Pan Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9ddb899a45c889af822fbc49957807ec-Abstract-Conference.html": {
    "title": "Neur2BiLO: Neural Bilevel Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Dumouchelle",
      "Esther Julien",
      "Jannis Kurtz",
      "Elias Khalil"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9de7a49945898da86e062e7029baa284-Abstract-Conference.html": {
    "title": "Proving Theorems Recursively",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiming Wang",
      "Huajian Xin",
      "Zhengying Liu",
      "Wenda Li",
      "Yinya Huang",
      "Jianqiao Lu",
      "Zhicheng Yang",
      "Jing Tang",
      "Jian Yin",
      "Zhenguo Li",
      "Xiaodan Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9df56a345b2a9c64c294986a5a63b8a6-Abstract-Conference.html": {
    "title": "Analytically deriving Partial Information Decomposition for affine systems of stable and convolution-closed distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaitanya Goswami",
      "Amanda Merkley"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9df6a759e01b871a009138bcc0471607-Abstract-Conference.html": {
    "title": "UDON: Universal Dynamic Online distillatioN for generic image representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolaos-Antonios Ypsilantis",
      "Kaifeng Chen",
      "Andre Araujo",
      "Ondrej Chum"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9dff3b83d463fab213941bfee23341ba-Abstract-Conference.html": {
    "title": "Disentangling Linear Quadratic Control with Untrusted ML Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongxin Li",
      "Hao Liu",
      "Yisong Yue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9e23d020c18e4c40d81c6a0fc7a46f68-Abstract-Conference.html": {
    "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Brandon",
      "Mayank Mishra",
      "Aniruddha Nrusimha",
      "Rameswar Panda",
      "Jonathan Ragan-Kelley"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9e2b548fd8d6c555fb79fa34a27592f9-Abstract-Conference.html": {
    "title": "Unveiling User Satisfaction and Creator Productivity Trade-Offs in Recommendation Platforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Yao",
      "Yiming Liao",
      "Jingzhou Liu",
      "Shaoliang Nie",
      "Qifan Wang",
      "Haifeng Xu",
      "Hongning Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9e3bba153aa362f961dc43de5cababac-Abstract-Conference.html": {
    "title": "Transcendence: Generative Models Can Outperform The Experts That Train Them",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edwin Zhang",
      "Vincent Zhu",
      "Naomi Saphra",
      "Anat Kleiman",
      "Benjamin Edelman",
      "Milind Tambe",
      "Sham Kakade",
      "Eran Malach"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9e4b14eb6f16fe7b5818a8d633a0606a-Abstract-Conference.html": {
    "title": "Identifying Causal Effects Under Functional Dependencies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizuo Chen",
      "Adnan Darwiche"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9e5f7743a4e753452f73d32da1190202-Abstract-Conference.html": {
    "title": "Style Adaptation and Uncertainty Estimation for Multi-Source Blended-Target Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwu Lu",
      "Haoyu Huang",
      "Xue Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9e688be0330d81a0b60fa18385c304c1-Abstract-Conference.html": {
    "title": "Linear Regression using Heterogeneous Data Batches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayush Jain",
      "Rajat Sen",
      "Weihao Kong",
      "Abhimanyu Das",
      "Alon Orlitsky"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9e72fc628edeb29f7aa64ac81b7ec6ce-Abstract-Conference.html": {
    "title": "Adaptive $Q$-Aid for Conditional Supervised Learning in Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghye Kim",
      "Suyoung Lee",
      "Woojun Kim",
      "Youngchul Sung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9e74900c3f6100c56add4bf417547848-Abstract-Conference.html": {
    "title": "You Only Look Around: Learning Illumination-Invariant Feature for Low-light Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingbo Hong",
      "Shen Cheng",
      "Haibin Huang",
      "Haoqiang Fan",
      "Shuaicheng Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9e770fcdb456400325c11d58b3a04d08-Abstract-Conference.html": {
    "title": "Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image Classification and Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongsu Song",
      "Daehwa Ko",
      "Jay Hoon Jung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9e89f068a62f6858c661a8abecf5bb0a-Abstract-Conference.html": {
    "title": "End-to-End Ontology Learning with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andy Lo",
      "Albert Q. Jiang",
      "Wenda Li",
      "Mateja Jamnik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9ebc79569f5e356b1ecfd1892d1b0a2e-Abstract-Conference.html": {
    "title": "Bayesian Domain Adaptation with Gaussian Mixture Domain-Indexing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanfang Ling",
      "Jiyong Li",
      "Lingbo Li",
      "Shangsong Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9ebf3213e396975cce47f2762e87e166-Abstract-Conference.html": {
    "title": "Reprogramming Pretrained Target-Specific Diffusion Models for Dual-Target Drug Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangxin Zhou",
      "Jiaqi Guan",
      "Yijia Zhang",
      "Xingang Peng",
      "Liang Wang",
      "Jianzhu Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9ec86860d30b362a4471105c3261d4ae-Abstract-Conference.html": {
    "title": "Upping the Game: How 2D U-Net Skip Connections Flip 3D Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingru Huang",
      "yihao guo",
      "Jian Huang",
      "Tianyun Zhang",
      "HE HONG",
      "Shaowei Jiang",
      "Yaoqi Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9ee3a664ccfeabc0da16ac6f1f1cfe59-Abstract-Conference.html": {
    "title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Tong",
      "Ellis Brown",
      "Penghao Wu",
      "Sanghyun Woo",
      "Adithya Jairam Vedagiri IYER",
      "Sai Charitha Akula",
      "Shusheng Yang",
      "Jihan Yang",
      "Manoj Middepogu",
      "Ziteng Wang",
      "Xichen Pan",
      "Rob Fergus",
      "Yann LeCun",
      "Saining Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9f0b1220028dfa2ee82ca0a0e0fc52d1-Abstract-Conference.html": {
    "title": "Dual Encoder GAN Inversion for High-Fidelity 3D Head Reconstruction from Single Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bahri Batuhan Bilecen",
      "Ahmet Gökmen",
      "Aysegul Dundar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9f12dd32d552f3ad9eaa0e9dfec291be-Abstract-Conference.html": {
    "title": "Base of RoPE Bounds Context Length",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyu Xu",
      "Xin Men",
      "Bingning Wang",
      "Qingyu Zhang",
      "Hongyu Lin",
      "Xianpei Han",
      "weipeng chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9f2b171fd3f4ca8dd71d1998f65c356a-Abstract-Conference.html": {
    "title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Lu",
      "Yanyan Zhao",
      "Bing Qin",
      "Liangyu Huo",
      "Qing Yang",
      "Dongliang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9f35ec2f7f403ef2c83d65b581df10bc-Abstract-Conference.html": {
    "title": "FactorSim: Generative Simulation via Factorized Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan-Yun Sun",
      "Harini S I",
      "Angela Yi",
      "Yihan Zhou",
      "Alex Zook",
      "Jonathan Tremblay",
      "Logan Cross",
      "Jiajun Wu",
      "Nick Haber"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9f40d2612d0b3b6f6c2a77da21b1067f-Abstract-Conference.html": {
    "title": "AMAGO-2: Breaking the Multi-Task Barrier in Meta-Reinforcement Learning with Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jake Grigsby",
      "Justin Sasek",
      "Samyak Parajuli",
      "Ikechukwu D. Adebi",
      "Amy Zhang",
      "Yuke Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9f42f06a54ce3b709ad78d34c73e4363-Abstract-Conference.html": {
    "title": "Alleviate Anchor-Shift: Explore Blind Spots with Cross-View Reconstruction for Incomplete Multi-View Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suyuan Liu",
      "Siwei Wang",
      "KE LIANG",
      "Junpu Zhang",
      "Zhibin Dong",
      "Tianrui Liu",
      "En Zhu",
      "Xinwang Liu",
      "Kunlun He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9f73d65a4186198152357be871345771-Abstract-Conference.html": {
    "title": "Transferable Adversarial Attacks on SAM and Its Downstream Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Xia",
      "Wenhan Yang",
      "Yi Yu",
      "Xun Lin",
      "Henghui Ding",
      "LINGYU DUAN",
      "Xudong Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9f7f063144103bf6debb09a3f15e00fb-Abstract-Conference.html": {
    "title": "MoGU: A Framework for Enhancing Safety of LLMs While Preserving Their Usability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YANRUI DU",
      "Sendong Zhao",
      "Danyang Zhao",
      "Ming Ma",
      "Yuhan Chen",
      "Liangyu Huo",
      "Qing Yang",
      "Dongliang Xu",
      "Bing Qin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9f7f2f57d8eaf44b2f09020f64ff6d96-Abstract-Conference.html": {
    "title": "Causal Imitation for Markov Decision Processes: a Partial Identification Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangrui Ruan",
      "Junzhe Zhang",
      "Xuan Di",
      "Elias Bareinboim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9f94298bac4668db4dc77ddb0a244301-Abstract-Conference.html": {
    "title": "Learning Diffusion Priors from Observations by Expectation Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "François Rozet",
      "Gerome Andry",
      "Francois Lanusse",
      "Gilles Louppe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9fdb856e9340634dd322e1438431e291-Abstract-Conference.html": {
    "title": "FastSurvival: Hidden Computational Blessings in Training Cox Proportional Hazards Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachang Liu",
      "Rui Zhang",
      "Cynthia Rudin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9febda1c8344cc5f2d51713964864e93-Abstract-Conference.html": {
    "title": "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haokun Lin",
      "Haobo Xu",
      "Yichen WU",
      "Jingzhi Cui",
      "Yingtao Zhang",
      "Linzhan Mou",
      "Linqi Song",
      "Zhenan Sun",
      "Ying Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/9ff1577a1f8308df1ccea6b4f64a103f-Abstract-Conference.html": {
    "title": "DAGER: Exact Gradient Inversion for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivo Petrov",
      "Dimitar I. Dimitrov",
      "Maximilian Baader",
      "Mark Müller",
      "Martin Vechev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a01e69aa9c3c61fcb40ea378e71fc780-Abstract-Conference.html": {
    "title": "Posture-Informed Muscular Force Learning for Robust Hand Pressure Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyungjin Seo",
      "Junghoon Seo",
      "Hanseok Jeong",
      "Sangpil Kim",
      "Sang Ho Yoon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a03037317560b8c5f2fb4b6466d4c439-Abstract-Conference.html": {
    "title": "What matters when building vision-language models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hugo Laurençon",
      "Leo Tronchon",
      "Matthieu Cord",
      "Victor Sanh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a06e129e01e0d2ef853e9ff67b911360-Abstract-Conference.html": {
    "title": "Language Grounded Multi-agent Reinforcement Learning with Human-interpretable Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huao Li",
      "Hossein Nourkhiz Mahjoub",
      "Behdad Chalaki",
      "Vaishnav Tadiparthi",
      "Kwonjoon Lee",
      "Ehsan Moradi Pari",
      "Charles Lewis",
      "Katia Sycara"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a076d0d1ed77364fc57693bdee1958fb-Abstract-Conference.html": {
    "title": "Variational Multi-scale Representation for Estimating Uncertainty in 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Li",
      "Yiu-ming Cheung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a07e5160196058120105ad7cb3505d3c-Abstract-Conference.html": {
    "title": "Exploring DCN-like architecture for fast image generation with arbitrary resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Wang",
      "Zexian Li",
      "Tianhui Song",
      "Xubin Li",
      "Tiezheng Ge",
      "Bo Zheng",
      "Limin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a07e87ecfa8a651d62257571669b0150-Abstract-Conference.html": {
    "title": "Replay-and-Forget-Free Graph Class-Incremental Learning: A Task Profiling and Prompting Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoxi Niu",
      "Guansong Pang",
      "Ling Chen",
      "Bing Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a0b1082fc7823c4c68abcab4fa850e9c-Abstract-Conference.html": {
    "title": "DeformableTST: Transformer for Time Series Forecasting without Over-reliance on Patching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghao Luo",
      "Xue Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a0cd56b91305239e2580dd9440b2e155-Abstract-Conference.html": {
    "title": "An Efficient High-dimensional Gradient Estimator for Stochastic Differential Equations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengbo Wang",
      "Jose Blanchet",
      "Peter W Glynn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a0da098e0031f58269efdcba40eedf47-Abstract-Conference.html": {
    "title": "Mixture of Demonstrations for In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Wang",
      "Zihan Chen",
      "Chengshuai Shi",
      "Cong Shen",
      "Jundong Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a0da690a47b2f52faa63f6fe054057b5-Abstract-Conference.html": {
    "title": "Differentially Private Reinforcement Learning with Self-Play",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Qiao",
      "Yu-Xiang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a0e1c2c40fc245b5fe7251ea33fbb045-Abstract-Conference.html": {
    "title": "Contextual Decision-Making with Knapsacks Beyond the Worst Case",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaohua Chen",
      "Rui Ai",
      "Mingwei Yang",
      "Yuqi Pan",
      "Chang Wang",
      "Xiaotie Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a0ebf1246e2e25b90902184c72250ab1-Abstract-Conference.html": {
    "title": "Transition Constrained Bayesian Optimization via Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jose Pablo Folch",
      "Calvin Tsay",
      "Robert Lee",
      "Behrang Shafei",
      "Weronika Ormaniec",
      "Andreas Krause",
      "Mark van der Wilk",
      "Ruth Misener",
      "Mojmir Mutny"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a102dd5931da01e1b40205490513304c-Abstract-Conference.html": {
    "title": "Finding NeMo: Localizing Neurons Responsible For Memorization in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Hintersdorf",
      "Lukas Struppek",
      "Kristian Kersting",
      "Adam Dziedzic",
      "Franziska Boenisch"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a10c3d85879c29331ba4d73ede56c7d3-Abstract-Conference.html": {
    "title": "Model-Based Transfer Learning for Contextual Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jung-Hoon Cho",
      "Vindula Jayawardana",
      "Sirui Li",
      "Cathy Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a117a3cd54b7affad04618c77c2fb18b-Abstract-Conference.html": {
    "title": "DeTeCtive: Detecting AI-generated Text via Multi-Level Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xun Guo",
      "Yongxin He",
      "Shan Zhang",
      "Ting Zhang",
      "Wanquan Feng",
      "Haibin Huang",
      "Chongyang Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a11e42a37c6bc926d6dc57e0cca0e825-Abstract-Conference.html": {
    "title": "Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Wang",
      "Jieming Bian",
      "Letian Zhang",
      "Chen Chen",
      "Jie Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a120382cf4e2e06d94d7ae7ac96fbe25-Abstract-Conference.html": {
    "title": "On the Optimality of Dilated Entropy and Lower Bounds for Online Learning in Extensive-Form Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Fan",
      "Christian Kroer",
      "Gabriele Farina"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a124b5e7385d35e5c8ad05d192106e19-Abstract-Conference.html": {
    "title": "Renovating Names in Open-Vocabulary Segmentation Benchmarks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiwen Huang",
      "Songyou Peng",
      "Dan Zhang",
      "Andreas Geiger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a128e43ed4708862aec51549b0050fc6-Abstract-Conference.html": {
    "title": "bit2bit: 1-bit quanta video reconstruction via self-supervised photon prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yehe Liu",
      "Alexander Krull",
      "Hector Basevi",
      "Ales Leonardis",
      "Michael Jenkins"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a12e362d89d4e0b40760f839f91550ee-Abstract-Conference.html": {
    "title": "Trap-MID: Trapdoor-based Defense against Model Inversion Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZhenTing Liu",
      "ShangTse Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a14ac7911df7d369919a687909a14a8b-Abstract-Conference.html": {
    "title": "IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Lin",
      "Shuyi Xie",
      "Yong Dai",
      "Wenlin Yao",
      "TianJiao Lang",
      "Yu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a1722a6bd1023c026a3d6a570fb3af75-Abstract-Conference.html": {
    "title": "ProtGO: Function-Guided Protein Modeling for Unified Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bozhen Hu",
      "Cheng Tan",
      "Yongjie Xu",
      "Zhangyang Gao",
      "Jun Xia",
      "Lirong Wu",
      "Stan Z. Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a178bd942c97f51ec069ddceeb55591b-Abstract-Conference.html": {
    "title": "A Near-optimal Algorithm for Learning Margin Halfspaces with Massart Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilias Diakonikolas",
      "Nikos Zarifis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a19940b01b77b6acd41ff8b32b334e7c-Abstract-Conference.html": {
    "title": "Speculative Monte-Carlo Tree Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Scott Cheng",
      "Mahmut T Kandemir",
      "Ding-Yong Hong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a1b390267538cd0af235dd5b0f1dc4a3-Abstract-Conference.html": {
    "title": "Learning to Price Homogeneous Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keran Chen",
      "Joon Suk Huh",
      "Kirthevasan Kandasamy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a1c716638d9b618a1a40a96f473c8250-Abstract-Conference.html": {
    "title": "Provable and Efficient Dataset Distillation for Kernel Ridge Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilan Chen",
      "Wei Huang",
      "Lily Weng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a1caa7bf30226ac9370889d00cbd74da-Abstract-Conference.html": {
    "title": "Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julius Vetter",
      "Guy Moss",
      "Cornelius Schröder",
      "Richard Gao",
      "Jakob H Macke"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a1d20cc72a21ef971d7e49a90d8fa56f-Abstract-Conference.html": {
    "title": "Improving the Worst-Case Bidirectional Communication Complexity for Nonconvex Distributed Optimization under Function Similarity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaja Gruntkowska",
      "Alexander Tyurin",
      "Peter Richtarik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a1d2eca2159af6978ff2633454d8db81-Abstract-Conference.html": {
    "title": "On Socially Fair Low-Rank Approximation and Column Subset Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhao Song",
      "Ali Vakilian",
      "David Woodruff",
      "Samson Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a1e0d6fa0c30b7d4f75dd9c7ed6189f2-Abstract-Conference.html": {
    "title": "Perception of Knowledge Boundary for Large Language Models through Semi-open-ended Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihua Wen",
      "Zhiliang Tian",
      "Zexin Jian",
      "Zhen Huang",
      "Pei Ke",
      "Yifu Gao",
      "Minlie Huang",
      "Dongsheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a1f0d96171c8e79019ae35ee439c2938-Abstract-Conference.html": {
    "title": "Decomposable Transformer Point Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aristeidis Panos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a1f12d8d3cc1b4789ff4ebec46e27609-Abstract-Conference.html": {
    "title": "Once Read is Enough: Domain-specific Pretraining-free Language Models with Cluster-guided Sparse Experts for Long-tail Domain Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fang Dong",
      "Mengyi Chen",
      "Jixian Zhou",
      "Yubin Shi",
      "Yixuan Chen",
      "Mingzhi Dong",
      "Yujiang Wang",
      "Dongsheng Li",
      "Xiaochen Yang",
      "Rui Zhu",
      "Robert Dick",
      "Qin Lv",
      "Fan Yang",
      "Tun Lu",
      "Ning Gu",
      "Li Shang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a20e8451ffb07ad25282c21945ad4f19-Abstract-Conference.html": {
    "title": "Compressing Large Language Models using Low Rank and Low Precision Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajarshi Saha",
      "Naomi Sagan",
      "Varun Srivastava",
      "Andrea Goldsmith",
      "Mert Pilanci"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a2118322165fffb648d1e341ff5a5b05-Abstract-Conference.html": {
    "title": "Intervention and Conditioning in Causal Bayesian Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sainyam Galhotra",
      "Joseph Halpern"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a221d22ff6a33599142c8299c7ed06bb-Abstract-Conference.html": {
    "title": "QUEST: Quality-Aware Metropolis-Hastings Sampling for Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gonçalo Faria",
      "Sweta Agrawal",
      "António Farinhas",
      "Ricardo Rei",
      "José de Souza",
      "André Martins"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a229cb89a98a84b2373496bb3cfc3570-Abstract-Conference.html": {
    "title": "Molecule Design by Latent Prompt Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deqian Kong",
      "Yuhao Huang",
      "Jianwen Xie",
      "Edouardo Honig",
      "Ming Xu",
      "Shuanghong Xue",
      "Pei Lin",
      "Sanping Zhou",
      "Sheng Zhong",
      "Nanning Zheng",
      "Ying Nian Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a2370db7c99791ad5d9f3ef48ad6d464-Abstract-Conference.html": {
    "title": "Improving Adversarial Robust Fairness via Anti-Bias Soft Label Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiji Zhao",
      "Ranjie Duan",
      "xizhewang",
      "Xingxing Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a267f6a1ea2bc9e0ec15ed4af7d5ae3f-Abstract-Conference.html": {
    "title": "Spectral Learning of Shared Dynamics Between Generalized-Linear Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucine L Oganesian",
      "Omid G. Sani",
      "Maryam Shanechi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a26f3dc32a913b77fa70c33ffa5dcb37-Abstract-Conference.html": {
    "title": "GaussianCut: Interactive segmentation via graph cut for 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Umangi Jain",
      "Ashkan Mirzaei",
      "Igor Gilitschenski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a2753c86334b9b4a21dd9d8e191a8bbf-Abstract-Conference.html": {
    "title": "Class Distribution Shifts in Zero-Shot Learning: Learning Robust Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuli Slavutsky",
      "Yuval Benjamini"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a2825fc792a1ebfdd45051b01aa5a180-Abstract-Conference.html": {
    "title": "Improved Guarantees for Fully Dynamic $k$-Center Clustering with Outliers in General Metric Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leyla Biabani",
      "Annika Hennes",
      "Denise La Gordt Dillie",
      "Morteza Monemizadeh",
      "Melanie Schmidt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a28af221f2f70be183afc16797a56b91-Abstract-Conference.html": {
    "title": "Constrained Synthesis with Projected Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob K Christopher",
      "Stephen Baek",
      "Nando Fioretto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a29f0fc2127c9d8cb1c9d86e423241af-Abstract-Conference.html": {
    "title": "Revisiting motion information for RGB-Event tracking with MOT philosophy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianlu Zhang",
      "Kurt Debattista",
      "Qiang Zhang",
      "guiguang ding",
      "Jungong Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a2e707354da36956945dbb288efe82b3-Abstract-Conference.html": {
    "title": "Thompson Sampling For Combinatorial Bandits: Polynomial Regret and Mismatched Sampling Paradox",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raymond Zhang",
      "Richard Combes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a2f1a33355610e271b8a27f09dac83d2-Abstract-Conference.html": {
    "title": "What Makes Partial-Label Learning Algorithms Effective?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Lv",
      "Yangfan Liu",
      "Shiyu Xia",
      "Ning Xu",
      "Miao Xu",
      "Gang Niu",
      "Min-Ling Zhang",
      "Masashi Sugiyama",
      "Xin Geng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a2fac827e992d55dcfdd4263e98528f4-Abstract-Conference.html": {
    "title": "DiffHammer: Rethinking the Robustness of Diffusion-Based Adversarial Purification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaibo Wang",
      "Xiaowen Fu",
      "Yuxuan Han",
      "Yang Xiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a3017a8d202a433be56a3dfdcac6c8eb-Abstract-Conference.html": {
    "title": "Multidimensional Fractional Programming for Normalized Cuts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yannan Chen",
      "Beichen Huang",
      "Licheng Zhao",
      "Kaiming Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a30769d9b62c9b94b72e21e0ca73f338-Abstract-Conference.html": {
    "title": "DiffusionBlend: Learning 3D Image Prior through Position-aware Diffusion Score Blending for 3D Computed Tomography Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Song",
      "Jason Hu",
      "Zhaoxu Luo",
      "Jeffrey Fessler",
      "Liyue Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a30d82c09902ced8aa3f49c5ee4a3d9b-Abstract-Conference.html": {
    "title": "Adversarial Schrödinger Bridge Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Gushchin",
      "Daniil Selikhanovych",
      "Sergei Kholkin",
      "Evgeny Burnaev",
      "Aleksandr Korotin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a31d1942f4f4f52a73bfd1d67856bed7-Abstract-Conference.html": {
    "title": "Infusing Self-Consistency into Density Functional Theory Hamiltonian Prediction via Deep Equilibrium Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zun Wang",
      "Chang Liu",
      "Nianlong Zou",
      "He Zhang",
      "Xinran Wei",
      "Lin Huang",
      "Lijun Wu",
      "Bin Shao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a32539cb16274581a17e679f6046f4bf-Abstract-Conference.html": {
    "title": "TransVIP: Speech to Speech Translation System with Voice and Isochrony Preservation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyang Le",
      "Yao Qian",
      "Dongmei Wang",
      "Long Zhou",
      "Shujie LIU",
      "Xiaofei Wang",
      "Midia Yousefi",
      "Yanmin Qian",
      "Jinyu Li",
      "Michael Zeng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a337c82579ca723415417fdd8ea3a485-Abstract-Conference.html": {
    "title": "Doubly Hierarchical Geometric Representations for Strand-based Human Hairstyle Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunlu Chen",
      "Francisco Vicente Carrasco",
      "Christian Häne",
      "Giljoo Nam",
      "Jean-Charles Bazin",
      "Fernando D De la Torre"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a344cef5eceea9c55baad6fdc49bcaed-Abstract-Conference.html": {
    "title": "Untrained Neural Nets for Snapshot Compressive Imaging: Theory and Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyu Zhao",
      "Xi Chen",
      "Xin Yuan",
      "Shirin Jalali"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a376fb45a0396c6fbf2f914dce8f7316-Abstract-Conference.html": {
    "title": "Enhancing Feature Diversity Boosts Channel-Adaptive Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chau Pham",
      "Bryan Plummer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a397986e0f34d4b1f0b640686ceaeff7-Abstract-Conference.html": {
    "title": "FIFO-Diffusion: Generating Infinite Videos from Text without Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihwan Kim",
      "Junoh Kang",
      "Jinyoung Choi",
      "Bohyung Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a399456a191ca36c7c78dff367887f0a-Abstract-Conference.html": {
    "title": "A Theoretical Understanding of Self-Correction through In-context Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Wang",
      "Yuyang Wu",
      "Zeming Wei",
      "Stefanie Jegelka",
      "Yisen Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a3a661eb3308d0bb686f6a4bac521032-Abstract-Conference.html": {
    "title": "ActFusion: a Unified Diffusion Model for Action Segmentation and Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dayoung Gong",
      "Suha Kwak",
      "Minsu Cho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a3cf318fbeec1126da21e9185ae9908c-Abstract-Conference.html": {
    "title": "DeepITE: Designing Variational Graph Autoencoders for Intervention Target Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyuan Tao",
      "Hang Yu",
      "Jianguo Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a3eadeebbc9eecd621086f6978865a85-Abstract-Conference.html": {
    "title": "Octopus: A Multi-modal LLM with Parallel Recognition and Sequential Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuyang Zhao",
      "YuXin Song",
      "Junru Chen",
      "KANG RONG",
      "Haocheng Feng",
      "Gang Zhang",
      "Shufan Ji",
      "Jingdong Wang",
      "Errui Ding",
      "Yifan Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a3f0196243c0c96e03733253ef29b34a-Abstract-Conference.html": {
    "title": "Constant Acceleration Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dogyun Park",
      "Sojin Lee",
      "Sihyeon Kim",
      "Taehoon Lee",
      "Youngjoon Hong",
      "Hyunwoo J Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a406c9f8eb70032a21110a4d86735ab9-Abstract-Conference.html": {
    "title": "Instance-Optimal Private Density Estimation in the Wasserstein Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vitaly Feldman",
      "Audra McMillan",
      "Satchit Sivakumar",
      "Kunal Talwar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a40ff56daab9f4808b1e18350c8a11ce-Abstract-Conference.html": {
    "title": "Optimal Design for Human Preference Elicitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhojyoti Mukherjee",
      "Anusha Lalitha",
      "Kousha Kalantari",
      "Aniket Anand Deshmukh",
      "Ge Liu",
      "Yifei Ma",
      "Branislav Kveton"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a421e88bb7db67cd0401475a0ed4a17d-Abstract-Conference.html": {
    "title": "Low Precision Local Training is Enough for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Li",
      "Yiqiu LI",
      "Binbin Lin",
      "Zhongming Jin",
      "Weizhong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a422a2f016c14406a01ddba731c0969a-Abstract-Conference.html": {
    "title": "Immiscible Diffusion: Accelerating Diffusion Training with Noise Assignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiheng Li",
      "Heyang Jiang",
      "Akio Kodaira",
      "Masayoshi TOMIZUKA",
      "Kurt Keutzer",
      "Chenfeng Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a4352e2c9d93582898a2a20e1f514e8f-Abstract-Conference.html": {
    "title": "Continuous Product Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aref Einizade",
      "Fragkiskos Malliaros",
      "Jhony H. Giraldo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a439259e78294c38d157a51a2c40486b-Abstract-Conference.html": {
    "title": "Reinforcement Learning with Euclidean Data Augmentation for State-Based Continuous Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinzhu Luo",
      "Dingyang Chen",
      "Qi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a45296e83b19f656392e0130d9e53cb1-Abstract-Conference.html": {
    "title": "Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenshan Wu",
      "Shaoguang Mao",
      "Yadong Zhang",
      "Yan Xia",
      "Li Dong",
      "Lei Cui",
      "Furu Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a4628e9fbd3002a554923642f74d5d6b-Abstract-Conference.html": {
    "title": "D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Que",
      "Jiaheng Liu",
      "Ge Zhang",
      "Chenchen Zhang",
      "Xingwei Qu",
      "Yinghao Ma",
      "Feiyu Duan",
      "ZhiqiBai zhiqi",
      "JiakaiWang",
      "Yuanxing Zhang",
      "Xu Tan",
      "Jie Fu",
      "Jiamang Wang",
      "Lin Qu",
      "Wenbo Su",
      "Bo Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a467e68e57e9645f83a682fb33645f45-Abstract-Conference.html": {
    "title": "CYCLO: Cyclic Graph Transformer Approach to Multi-Object Relationship Modeling in Aerial Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trong-Thuan Nguyen",
      "Pha Nguyen",
      "Xin Li",
      "Jackson Cothren",
      "Alper Yilmaz",
      "Khoa Luu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a47f5cdff1469751597d78e803fc590f-Abstract-Conference.html": {
    "title": "Prior-itizing Privacy: A Bayesian Approach to Setting the Privacy Budget in Differential Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeki Kazan",
      "Jerome Reiter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a488aa1a0c00d76db8a922ef7815a786-Abstract-Conference.html": {
    "title": "ENAT: Rethinking Spatial-temporal Interactions in Token-based Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zanlin Ni",
      "Yulin Wang",
      "Renping Zhou",
      "Yizeng Han",
      "Jiayi Guo",
      "Zhiyuan Liu",
      "Yuan Yao",
      "Gao Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a4b293979b8b521e9222d30c40246911-Abstract-Conference.html": {
    "title": "Binary Search with Distributional Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Dinitz",
      "Sungjin Im",
      "Thomas Lavastida",
      "Ben Moseley",
      "Aidin Niaparast",
      "Sergei Vassilvitskii"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a4b95476f673e6e538f80862f622ba2f-Abstract-Conference.html": {
    "title": "Towards a Theoretical Understanding of the 'Reversal Curse' via Training Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanlin Zhu",
      "Baihe Huang",
      "Shaolun Zhang",
      "Michael I. Jordan",
      "Jiantao Jiao",
      "Yuandong Tian",
      "Stuart J Russell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a4c17d9b88eaefc9bdf7c656ffc8ce55-Abstract-Conference.html": {
    "title": "GRANOLA: Adaptive Normalization for Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moshe Eliasof",
      "Beatrice Bevilacqua",
      "Carola-Bibiane Schönlieb",
      "Haggai Maron"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a4c3a66ed818455b8bbe591b6a5d0f56-Abstract-Conference.html": {
    "title": "The Intelligible and Effective Graph Neural Additive Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maya Bechler-Speicher",
      "Amir Globerson",
      "Ran Gilad-Bachrach"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a4c680b456b9750003c8a494a5b165dc-Abstract-Conference.html": {
    "title": "DeTrack: In-model Latent Denoising Learning for Visual Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Zhou",
      "Jinglun Li",
      "Lingyi Hong",
      "Kaixun Jiang",
      "Pinxue Guo",
      "Weifeng Ge",
      "Wenqiang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a4ca07aa108036f80cbb5b82285fd4b1-Abstract-Conference.html": {
    "title": "MAmmoTH2: Scaling Instructions from the Web",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Yue",
      "Tianyu Zheng",
      "Ge Zhang",
      "Wenhu Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a4e683f0ce6b91e7fbdae9d32642d88f-Abstract-Conference.html": {
    "title": "A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Yang",
      "Mona Gandhi",
      "Yufei Wang",
      "Yifan Wu",
      "Michael Yao",
      "Chris Callison-Burch",
      "James Gee",
      "Mark Yatskar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a5036c166e44b731f214f41813364d01-Abstract-Conference.html": {
    "title": "Imitating Language via Scalable Inverse Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Markus Wulfmeier",
      "Michael Bloesch",
      "Nino Vieillard",
      "Arun Ahuja",
      "Jorg Bornschein",
      "Sandy Huang",
      "Artem Sokolov",
      "Matt Barnes",
      "Guillaume Desjardins",
      "Alex Bewley",
      "Sarah Bechtle",
      "Jost Springenberg",
      "Nikola Momchev",
      "Olivier Bachem",
      "Matthieu Geist",
      "Martin Riedmiller"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a5059a9a389ccc76da85760ea79490d8-Abstract-Conference.html": {
    "title": "Gradient Guidance for Diffusion Models: An Optimization Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingqing Guo",
      "Hui Yuan",
      "Yukang Yang",
      "Minshuo Chen",
      "Mengdi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a51937290b8ada2dc1404ce4f9fe2c9d-Abstract-Conference.html": {
    "title": "Private Edge Density Estimation for Random Graphs: Optimal, Efficient and Robust",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjie Chen",
      "Jingqiu Ding",
      "Yiding Hua",
      "David Steurer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a51a0d42d794c8adf416196aae9f0974-Abstract-Conference.html": {
    "title": "Stochastic Newton Proximal Extragradient Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruichen Jiang",
      "Michal Derezinski",
      "Aryan Mokhtari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a51a74b2d71387dc71cc29181b5519bb-Abstract-Conference.html": {
    "title": "Aligner: Efficient Alignment by Learning to Correct",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Ji",
      "Boyuan Chen",
      "Hantao Lou",
      "Donghai Hong",
      "Borong Zhang",
      "Xuehai Pan",
      "Tianyi (Alex) Qiu",
      "Juntao Dai",
      "Yaodong Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a526cc8f6ffb74bedb6ff313e3fdb450-Abstract-Conference.html": {
    "title": "On provable privacy vulnerabilities of graph representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruofan Wu",
      "Guanhua Fang",
      "Mingyang Zhang",
      "Qiying Pan",
      "Tengfei LIU",
      "Weiqiang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a5321f64005b0d4a94d0b18e84e19f48-Abstract-Conference.html": {
    "title": "Memory-Efficient Gradient Unrolling for Large-Scale Bi-level Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianli Shen",
      "Yezhen Wang",
      "Zhouhao Yang",
      "Xiang Li",
      "Haonan Wang",
      "Yang Zhang",
      "Jonathan Scarlett",
      "Zhanxing Zhu",
      "Kenji Kawaguchi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a540b17fb2295c736d5afd6c507acf66-Abstract-Conference.html": {
    "title": "On the Adversarial Robustness of Benjamini Hochberg",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Louis Chen",
      "Roberto Szechtman",
      "Matan Seri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a547d86953a4e36aa8a1390e6f4708e2-Abstract-Conference.html": {
    "title": "The Space Complexity of Approximating Logistic Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gregory Dexter",
      "Petros Drineas",
      "Rajiv Khanna"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a55dd7fc416d91fbb3817defe6a75df6-Abstract-Conference.html": {
    "title": "Rethinking 3D Convolution in $\\ell_p$-norm Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Zhang",
      "Yan Zhong",
      "Jianan Wang",
      "Zhe Min",
      "RujingWang",
      "Liu Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a55f589b1d54edd513cf1c8b9dd1ed47-Abstract-Conference.html": {
    "title": "Near-Optimality of Contrastive Divergence Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Glaser",
      "Kevin Han Huang",
      "Arthur Gretton"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a5a3b1ef79520b7cd122d888673a3ebc-Abstract-Conference.html": {
    "title": "A Generative Model of Symmetry Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Allingham",
      "Bruno Mlodozeniec",
      "Shreyas Padhy",
      "Javier Antorán",
      "David Krueger",
      "Richard Turner",
      "Eric Nalisnick",
      "José Miguel Hernández-Lobato"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a5a5b0ff87c59172a13342d428b1e033-Abstract-Conference.html": {
    "title": "Lips Are Lying: Spotting the Temporal Inconsistency between Audio and Visual in Lip-Syncing DeepFakes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weifeng Liu",
      "Tianyi She",
      "Jiawei Liu",
      "Boheng Li",
      "Dongyu Yao",
      "子游 梁",
      "Run Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a5c47c1b7adf19e8dc633812a4acf6d2-Abstract-Conference.html": {
    "title": "Graph Learning for Numeric Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dillon Chen",
      "Sylvie Thiebaux"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a5c7206fd66e8314bb21a04492359353-Abstract-Conference.html": {
    "title": "ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanlin Li",
      "Kangjie Chen",
      "Shudong Zhang",
      "Jie Zhang",
      "Tianwei Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a5d2da376bab7624b3caeb9f78fcaa2f-Abstract-Conference.html": {
    "title": "Learning Structured Representations with Hyperbolic Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Sinha",
      "Siqi Zeng",
      "Makoto Yamada",
      "Han Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a5d8aba27dfef4e849e8cb03fb87a954-Abstract-Conference.html": {
    "title": "Scaling Retrieval-Based Language Models with a Trillion-Token Datastore",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rulin Shao",
      "Jacqueline He",
      "Akari Asai",
      "Weijia Shi",
      "Tim Dettmers",
      "Sewon Min",
      "Luke Zettlemoyer",
      "Pang Wei W Koh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a600f0f740605205133553cb74a1c107-Abstract-Conference.html": {
    "title": "How JEPA Avoids Noisy Features: The Implicit Bias of Deep Linear Self Distillation Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Etai Littwin",
      "Omid Saremi",
      "Madhu Advani",
      "Vimal Thilak",
      "Preetum Nakkiran",
      "Chen Huang",
      "Joshua Susskind"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a627810151be4d13f907ac898ff7e948-Abstract-Conference.html": {
    "title": "How Sparse Can We Prune A Deep Network: A Fundamental Limit Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiaozhe Zhang",
      "Ruijie Zhang",
      "Jun Sun",
      "Yingzhuang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a6678e2be4ce7aef9d2192e03cd586b7-Abstract-Conference.html": {
    "title": "Multilingual Diversity Improves Vision-Language Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thao Nguyen",
      "Matthew Wallingford",
      "Sebastin Santy",
      "Wei-Chiu Ma",
      "Sewoong Oh",
      "Ludwig Schmidt",
      "Pang Wei W Koh",
      "Ranjay Krishna"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a6805b5564bd8d813a81c4b5a97e5ca6-Abstract-Conference.html": {
    "title": "Post-Hoc Reversal: Are We Selecting Models Prematurely?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishabh Ranjan",
      "Saurabh Garg",
      "Mrigank Raman",
      "Carlos Guestrin",
      "Zachary Lipton"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a68120d2eb2f53f7d9e71547591aef11-Abstract-Conference.html": {
    "title": "Universal Exact Compression of Differentially Private Mechanisms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanxiao Liu",
      "Wei-Ning Chen",
      "Ayfer Ozgur",
      "Cheuk Ting Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a69d7f3a1340d55c720e572742439eaf-Abstract-Conference.html": {
    "title": "DisenGCD: A Meta Multigraph-assisted Disentangled Graph Learning Framework for Cognitive Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangshang Yang",
      "Mingyang Chen",
      "Ziwen Wang",
      "Xiaoshan Yu",
      "Panpan Zhang",
      "Haiping Ma",
      "Xingyi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a6a066fb44f2fe0d36cf740c873b8890-Abstract-Conference.html": {
    "title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenyuan Gao",
      "Jiazhi Yang",
      "Li Chen",
      "Kashyap Chitta",
      "Yihang Qiu",
      "Andreas Geiger",
      "Jun Zhang",
      "Hongyang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a6a21421022d1da16eadbba533980530-Abstract-Conference.html": {
    "title": "Your contrastive learning problem is secretly a distribution alignment problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Chen",
      "Chi-Heng Lin",
      "Ran Liu",
      "Jingyun Xiao",
      "Eva Dyer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a6a6891cf1dfc64d664f086cf5976e93-Abstract-Conference.html": {
    "title": "Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agents at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyue Ou",
      "Frank F. Xu",
      "Aman Madaan",
      "Jiarui Liu",
      "Robert Lo",
      "Abishek Sridhar",
      "Sudipta Sengupta",
      "Dan Roth",
      "Graham Neubig",
      "Shuyan Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a6deba3b2408af45b3f9994c2152b862-Abstract-Conference.html": {
    "title": "Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuechen Zhang",
      "Zijian Huang",
      "Ege Onur Taga",
      "Carlee Joe-Wong",
      "Samet Oymak",
      "Jiasi Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a6e1f6963f65bcc4854691a15460dbd8-Abstract-Conference.html": {
    "title": "Noisy Dual Mirror Descent: A Near Optimal Algorithm for Jointly-DP Convex Resource Allocation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Du Chen",
      "Geoffrey A. Chua"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a6eeb95990fc2704bacbba9b5aca3cff-Abstract-Conference.html": {
    "title": "Markov Equivalence and Consistency in Differentiable Structure Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Deng",
      "Kevin Bello",
      "Pradeep K. Ravikumar",
      "Bryon Aragam"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a70ee7ea485e4fd36abbfc4adf591c28-Abstract-Conference.html": {
    "title": "Multi-scale Consistency for Robust 3D Registration via Hierarchical Sinkhorn Tree",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengwei Ren",
      "Yifan Feng",
      "Weixiang Zhang",
      "Xiao-Ping (Steven) Zhang",
      "Yue Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a71c1931d3fb8ba564f7458d0657d0b1-Abstract-Conference.html": {
    "title": "A Kernel Perspective on Distillation-based Collaborative Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sejun Park",
      "Kihun Hong",
      "Ganguk Hwang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a71df365f872a39e58475f1fa7950879-Abstract-Conference.html": {
    "title": "Extending Multi-modal Contrastive Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziang Zhang",
      "Zehan Wang",
      "Luping Liu",
      "Rongjie Huang",
      "Xize Cheng",
      "Zhenhui Ye",
      "wang lin",
      "Huadai Liu",
      "Haifeng Huang",
      "Yang Zhao",
      "Tao Jin",
      "Siqi Zheng",
      "Zhou Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a730abbcd6cf4a371ca9545db5922442-Abstract-Conference.html": {
    "title": "Improved Analysis for Bandit Learning in Matching Markets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fang Kong",
      "Zilong Wang",
      "Shuai Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a73474c359ed523e6cd3174ed29a4d56-Abstract-Conference.html": {
    "title": "FEEL-SNN: Robust Spiking Neural Networks with Frequency Encoding and Evolutionary Leak Factor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengting Xu",
      "De Ma",
      "Huajin Tang",
      "Qian Zheng",
      "Gang Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a76653a4209b5d098107e5e7e9295ba8-Abstract-Conference.html": {
    "title": "COSMIC: Compress Satellite Image Efficiently via Diffusion Compensation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyuan Zhang",
      "Han Qiu",
      "Maosen Zhang",
      "Jun Liu",
      "Bin Chen",
      "Tianwei Zhang",
      "Hewu Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a7667ee5d545a43d2f0fda98863c260e-Abstract-Conference.html": {
    "title": "Variance estimation in compound decision theory under boundedness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhodh Kotekal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a76ed4a8ef522c823d73925e7fff16d4-Abstract-Conference.html": {
    "title": "Mitigating Object Hallucination via Concentric Causal Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Xing",
      "Yiheng Li",
      "Ivan Laptev",
      "Shijian Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a789f19f6454f858ad74bc8b7581fe36-Abstract-Conference.html": {
    "title": "Physics-Constrained Comprehensive Optical Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanbing Liu",
      "JIANWEI QIN",
      "Yan Liu",
      "Xi Yue",
      "Xun Liu",
      "Guoqing Wang",
      "Tianyu Li",
      "Ye",
      "Wei Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a78f142aec481e68c75276756e0a0d91-Abstract-Conference.html": {
    "title": "Recovering Complete Actions for Cross-dataset Skeleton Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanchao Liu",
      "Yujiang Li",
      "Tai-Jiang Mu",
      "Shi-min Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a79054a9da91d73ed3cb1a9e87d7cd2d-Abstract-Conference.html": {
    "title": "Speculative Decoding with CTC-based Draft Model for LLM Inference Acceleration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuofan Wen",
      "Shangtong Gui",
      "Yang Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a791a086d7643ecf53608e57cd5889f0-Abstract-Conference.html": {
    "title": "Second-order forward-mode optimization of recurrent neural networks for neuroscience",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youjing Yu",
      "Rui Xia",
      "Qingxi Ma",
      "Mate Lengyel",
      "Guillaume Hennequin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a7a6465b9344c5ecc691c02af40661a7-Abstract-Conference.html": {
    "title": "Personalized Federated Learning with Mixture of Models for Adaptive Prediction and Model Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pouya M. Ghari",
      "Yanning Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a7bfdee9544cea324cf183ac03c7d5c0-Abstract-Conference.html": {
    "title": "OccFusion: Rendering Occluded Humans with Generative Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Sun",
      "Tiange Xiang",
      "Scott Delp",
      "Fei-Fei Li",
      "Ehsan Adeli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a7c17115db36193f6b83b71b0fe1d416-Abstract-Conference.html": {
    "title": "QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuo Chen",
      "Rumen Dangovski",
      "Charlotte Loh",
      "Owen Dugan",
      "Di Luo",
      "Marin Soljacic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a7c57c8942b7d2adba1ac615ba0e60b9-Abstract-Conference.html": {
    "title": "Abstracted Shapes as Tokens - A Generalizable and Interpretable Model for Time-series Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunshi Wen",
      "Tengfei Ma",
      "Lily Weng",
      "Lam Nguyen",
      "Anak Agung Julius"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a7d36e5cb41a1f21c46db25cb1aafab9-Abstract-Conference.html": {
    "title": "Unraveling the Gradient Descent Dynamics of Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingqing Song",
      "Boran Han",
      "Shuai Zhang",
      "Jie Ding",
      "Mingyi Hong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a7ebe2e8d8cfd2fcec6cd77f9e6fd34d-Abstract-Conference.html": {
    "title": "Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehyun Nam",
      "Kyuyoung Kim",
      "Seunghyuk Oh",
      "Jihoon Tack",
      "Jaehyung Kim",
      "Jinwoo Shin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a7efd7734bb2e79997f8d1137ece3f09-Abstract-Conference.html": {
    "title": "Infinite-Dimensional Feature Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhui Xu",
      "FUXUN YU",
      "Maoliang Li",
      "Zihao Zheng",
      "Zirui Xu",
      "Jinjun Xiong",
      "Xiang Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a7f89793b9e6f8c6568dbbb6ff727b9b-Abstract-Conference.html": {
    "title": "ConStat: Performance-Based Contamination Detection in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jasper Dekoninck",
      "Mark Müller",
      "Martin Vechev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a7fbf054c80d26e5b4ed67588ea384f0-Abstract-Conference.html": {
    "title": "Spectral-Risk Safe Reinforcement Learning with Convergence Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dohyeong Kim",
      "Taehyun Cho",
      "Seungyub Han",
      "Hojun Chung",
      "Kyungjae Lee",
      "Songhwai Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a8223b0ad64007423ffb308b0dd92298-Abstract-Conference.html": {
    "title": "One-Step Effective Diffusion Network for Real-World Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongyuan Wu",
      "Lingchen Sun",
      "Zhiyuan Ma",
      "Lei Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a8320b6b9d95798dc286a867c44742a1-Abstract-Conference.html": {
    "title": "MECD: Unlocking Multi-Event Causal Discovery in Video Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tieyuan Chen",
      "Huabin Liu",
      "Tianyao He",
      "Yihang Chen",
      "chaofan gan",
      "Xiao Ma",
      "Cheng Zhong",
      "Yang Zhang",
      "Yingxue Wang",
      "Hui Lin",
      "Weiyao Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a845fdc3f87751710218718adb634fe7-Abstract-Conference.html": {
    "title": "DiP-GO: A Diffusion Pruner via Few-step Gradient Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haowei Zhu",
      "Dehua Tang",
      "Ji Liu",
      "Mingjie Lu",
      "Jintu Zheng",
      "Jinzhang Peng",
      "Dong Li",
      "Yu Wang",
      "Fan Jiang",
      "Lu Tian",
      "Spandan Tiwari",
      "Ashish Sirasao",
      "Jun-Hai Yong",
      "Bin Wang",
      "Emad Barsoum"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a862f5788fd09bb6843c694d8120d50c-Abstract-Conference.html": {
    "title": "Reasoning Multi-Agent Behavioral Topology for Interactive Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochen Liu",
      "Li Chen",
      "Yu Qiao",
      "Chen Lv",
      "Hongyang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a8633d27d782f66fe660c2fb4bae446e-Abstract-Conference.html": {
    "title": "In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liam Collins",
      "Advait Parulekar",
      "Aryan Mokhtari",
      "Sujay Sanghavi",
      "Sanjay Shakkottai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a867a94a427bacbe3f4de16c7ac10ba8-Abstract-Conference.html": {
    "title": "Rethinking the Membrane Dynamics and Optimization Objectives of Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hangchi Shen",
      "Qian Zheng",
      "Huamin Wang",
      "Gang Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a8808b75b299d64a23255bc8d30fb786-Abstract-Conference.html": {
    "title": "When Is Inductive Inference Possible?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhou Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a8809ae67a7aad49a64d615468d72808-Abstract-Conference.html": {
    "title": "Collaborative Refining for Learning from Inaccurate Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "BIN HAN",
      "Yi-Xuan Sun",
      "Ya-Lin Zhang",
      "Libang Zhang",
      "Haoran Hu",
      "Longfei Li",
      "Jun Zhou",
      "Guo Ye",
      "HUIMEI HE"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a89e938d2dde9cce748a26bf84627d1c-Abstract-Conference.html": {
    "title": "Improved Regret of Linear Ensemble Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harin Lee",
      "Min-hwan Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a8bc668b1559d705221b0e7510c45e48-Abstract-Conference.html": {
    "title": "Learning Equilibria in Adversarial Team Markov Games: A Nonconvex-Hidden-Concave Min-Max Optimization Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fivos Kalogiannis",
      "Jingming Yan",
      "Ioannis Panageas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a921f335253add9996d5175ad30896ec-Abstract-Conference.html": {
    "title": "Understanding Model Selection for Learning in Strategic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tinashe Handina",
      "Eric Mazumdar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a922b7121007768f78f770c404415375-Abstract-Conference.html": {
    "title": "CALVIN: Improved Contextual Video Captioning via Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gowthami Somepalli",
      "Arkabandhu Chowdhury",
      "Jonas Geiping",
      "Ronen Basri",
      "Tom Goldstein",
      "David Jacobs"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a9338cd6e092ff1f96c3749b08cdc537-Abstract-Conference.html": {
    "title": "Homology Consistency Constrained Efficient Tuning for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huatian Zhang",
      "Lei Zhang",
      "Yongdong Zhang",
      "Zhendong Mao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a9619dd0f0d54a5cf7734add1dc38cd1-Abstract-Conference.html": {
    "title": "Designing Cell-Type-Specific Promoter Sequences Using Conservative Model-Based Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniketh Janardhan Reddy",
      "Xinyang Geng",
      "Michael Herschl",
      "Sathvik Kolli",
      "Aviral Kumar",
      "Patrick Hsu",
      "Sergey Levine",
      "Nilah Ioannidis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a96368eb38bce0956a1132154d70d72d-Abstract-Conference.html": {
    "title": "Matrix Denoising with Doubly Heteroscedastic Noise: Fundamental Limits and Optimal Spectral Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Zhang",
      "Marco Mondelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a96d4fda3017f1773b261a52a3efc8dd-Abstract-Conference.html": {
    "title": "Batched Energy-Entropy acquisition for Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Teufel",
      "Carsten Stahlhut",
      "Jesper Ferkinghoff-Borg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a97b58c4f7551053b0512f92244b0810-Abstract-Conference.html": {
    "title": "SLowcalSGD : Slow Query Points Improve Local-SGD for Stochastic Convex Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tehila Dahan",
      "Kfir Y. Levy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a97f0218b49bc17ea3f121a0e724f028-Abstract-Conference.html": {
    "title": "Boosting Weakly Supervised Referring Image Segmentation via Progressive Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zaiquan Yang",
      "Yuhao LIU",
      "Jiaying Lin",
      "Gerhard Hancke",
      "Rynson Lau"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a995960dd0193654d6b18eca4ac5b936-Abstract-Conference.html": {
    "title": "When Your AIs Deceive You: Challenges of Partial Observability in Reinforcement Learning from Human Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leon Lang",
      "Davis Foote",
      "Stuart J Russell",
      "Anca Dragan",
      "Erik Jenner",
      "Scott Emmons"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a9a3f0e4a95cb273867931369c8fc3b1-Abstract-Conference.html": {
    "title": "Identifiable Object-Centric Representation Learning via Probabilistic Slot Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avinash Kori",
      "Francesco Locatello",
      "Ainkaran Santhirasekaram",
      "Francesca Toni",
      "Ben Glocker",
      "Fabio De Sousa Ribeiro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a9b50b051cd5f17455458d922e52768b-Abstract-Conference.html": {
    "title": "Piecewise deterministic generative models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Bertazzi",
      "Dario Shariatian",
      "Umut Simsekli",
      "Eric Moulines",
      "Alain Durmus"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a9b938e79504889f905d549f8d53e405-Abstract-Conference.html": {
    "title": "Contrastive losses as generalized models of global epistasis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Brookes",
      "Jakub Otwinowski",
      "Sam Sinai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a9bef53eb7b0e5950d4f2d9c74a16006-Abstract-Conference.html": {
    "title": "What Makes and Breaks Safety Fine-tuning? A Mechanistic Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samyak Jain",
      "Ekdeep S Lubana",
      "Kemal Oksuz",
      "Tom Joy",
      "Philip Torr",
      "Amartya Sanyal",
      "Puneet Dokania"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a9d2a5fd12d34250c21b5e4fa8d906b0-Abstract-Conference.html": {
    "title": "Learning to Handle Complex Constraints for Vehicle Routing Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieyi Bi",
      "Yining Ma",
      "Jianan Zhou",
      "Wen Song",
      "Zhiguang Cao",
      "Yaoxin Wu",
      "Jie Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a9d419ef12fb34105424fa3166716139-Abstract-Conference.html": {
    "title": "Normalization Layer Per-Example Gradients are Sufficient to Predict Gradient Noise Scale in Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gavia Gray",
      "aman tiwari",
      "Shane Bergsma",
      "Joel Hestness"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a9db2b121c3517fd559ecbe5038701ee-Abstract-Conference.html": {
    "title": "Unifying Homophily and Heterophily for Spectral Graph Neural Networks via Triple Filter Ensembles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Duan",
      "Mingjian Guang",
      "Junli Wang",
      "Chungang Yan",
      "Hongda Qi",
      "Wenkang Su",
      "Can Tian",
      "Haoran Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/a9f3457fa97f106f1756885237787789-Abstract-Conference.html": {
    "title": "Offline Reinforcement Learning with OOD State Correction and OOD Action Suppression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiu Mao",
      "Qi Wang",
      "Chen Chen",
      "Yun Qu",
      "Xiangyang Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aa1d8cf866c4a684ef2e066dd200f8e4-Abstract-Conference.html": {
    "title": "Dynamic Conditional Optimal Transport through Simulation-Free Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gavin Kerrigan",
      "Giosue Migliorini",
      "Padhraic Smyth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aa280e73c4e23e765fde232571116d3b-Abstract-Conference.html": {
    "title": "Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alberto Alfarano",
      "Francois Charton",
      "Amaury Hayat"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aa56c74513a5e35768a11f4e82dd7ffb-Abstract-Conference.html": {
    "title": "Neural Collapse Inspired Feature Alignment for Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhikang Chen",
      "Min Zhang",
      "Sen Cui",
      "Haoxuan Li",
      "Gang Niu",
      "Mingming Gong",
      "Changshui Zhang",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aa5a581a221f1e431fcad9d7ccfedaa2-Abstract-Conference.html": {
    "title": "SSA-Seg: Semantic and Spatial Adaptive Pixel-level Classifier for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaowen Ma",
      "Zhen-Liang Ni",
      "Xinghao Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aa7259c82d642e47d5661f3218cdcad2-Abstract-Conference.html": {
    "title": "Identifying Spatio-Temporal Drivers of Extreme Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamad Hakam Shams Eddin",
      "Jürgen Gall"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aa76025af7f8d69338c4b5ee29f66e70-Abstract-Conference.html": {
    "title": "LiT: Unifying LiDAR \"Languages\" with LiDAR Translator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixing Lao",
      "Tao Tang",
      "Xiaoyang Wu",
      "Peng Chen",
      "Kaicheng Yu",
      "Hengshuang Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aa76039e10ccb71d2922e87a87240f72-Abstract-Conference.html": {
    "title": "Accelerating Matroid Optimization through Fast Imprecise Oracles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Franziska Eberle",
      "Felix Hommelsheim",
      "Alexander Lindermayr",
      "Zhenwei Liu",
      "Nicole Megow",
      "Jens Schlöter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aa7eb65738b5bc71c81848fba9111c97-Abstract-Conference.html": {
    "title": "MonkeySee: Space-time-resolved reconstructions of natural images from macaque multi-unit activity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lynn Le",
      "Paolo Papale",
      "Katja Seeliger",
      "Antonio Lozano",
      "Thirza Dado",
      "Feng Wang",
      "Pieter Roelfsema",
      "Marcel A. J. van Gerven",
      "Yağmur Güçlütürk",
      "Umut Güçlü"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aa8164b8a8e484fe31885657fea14997-Abstract-Conference.html": {
    "title": "Constrained Diffusion with Trust Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Y. Huang",
      "Yifeng Jiang",
      "Tom Van Wouwe",
      "Karen Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aa933b5abc1be30baece1d230ec575a7-Abstract-Conference.html": {
    "title": "Parameter Symmetry and Noise Equilibrium of Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liu Ziyin",
      "Mingze Wang",
      "Hongchao Li",
      "Lei Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aa93a55655e49cc8bf8e6e9295d9b295-Abstract-Conference.html": {
    "title": "Light Unbalanced Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milena Gazdieva",
      "Arip Asadulaev",
      "Evgeny Burnaev",
      "Aleksandr Korotin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aa9508b2577e0d9c824da9e2e27fe9d6-Abstract-Conference.html": {
    "title": "NeuroGauss4D-PCI: 4D Neural Fields and Gaussian Deformation Fields for Point Cloud Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaokang Jiang",
      "Dalong Du",
      "Jiuming Liu",
      "Siting Zhu",
      "Zhenqiang Liu",
      "Zhuang Ma",
      "Zhujin Liang",
      "Jie Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aa9796d525f6614898297847d8efea17-Abstract-Conference.html": {
    "title": "Unsupervised Modality Adaptation with Text-to-Image Diffusion Models for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihao Xia",
      "Yu Liang",
      "Peng-Tao Jiang",
      "Hao Zhang",
      "Bo Li",
      "Yang Tang",
      "Pan Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aaa0ac4253da75faf9b0dc0dda062612-Abstract-Conference.html": {
    "title": "Sequential Signal Mixing Aggregation for Message Passing Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mitchell Keren Taraday",
      "Almog David",
      "Chaim Baskin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aaa9215b743f3df666d73cde9dc357c3-Abstract-Conference.html": {
    "title": "Learning from Highly Sparse Spatio-temporal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leyan Deng",
      "Chenwang Wu",
      "Defu Lian",
      "Enhong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aaf50c91c3fc018f6a476032d02114d9-Abstract-Conference.html": {
    "title": "Learning Frequency-Adapted Vision Foundation Model for Domain Generalized Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Bi",
      "Jingjun Yi",
      "Hao Zheng",
      "Haolan Zhan",
      "Yawen Huang",
      "Wei Ji",
      "Yuexiang Li",
      "Yefeng Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ab05dc8bf36a9f66edbff6992ec86f56-Abstract-Conference.html": {
    "title": "Multi-Head Mixture-of-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xun Wu",
      "Shaohan Huang",
      "Wenhui Wang",
      "Shuming Ma",
      "Li Dong",
      "Furu Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ab1ee157f7804a13f980414b644a9460-Abstract-Conference.html": {
    "title": "Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Markus Hiller",
      "Krista A. Ehinger",
      "Tom Drummond"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ab41313eaa3cbedbe491c24cbfe6547d-Abstract-Conference.html": {
    "title": "Adaptive and Optimal Second-order Optimistic Methods for Minimax Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruichen Jiang",
      "Ali Kavis",
      "Qiujiang Jin",
      "Sujay Sanghavi",
      "Aryan Mokhtari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ab54dd3cd5e273dbb1e75a5e6203fb42-Abstract-Conference.html": {
    "title": "Stable Minima Cannot Overfit in Univariate ReLU Networks: Generalization by Large Step Sizes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Qiao",
      "Kaiqi Zhang",
      "Esha Singh",
      "Daniel Soudry",
      "Yu-Xiang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ab6022d3d669b5baafa24c91d7c407a6-Abstract-Conference.html": {
    "title": "OxonFair: A Flexible Toolkit for Algorithmic Fairness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eoin Delaney",
      "Zihao Fu",
      "Sandra Wachter",
      "Brent Mittelstadt",
      "Chris Russell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ab63d1eb181e920273504411fe0942dc-Abstract-Conference.html": {
    "title": "Derandomizing Multi-Distribution Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kasper Green Larsen",
      "Omar Montasser",
      "Nikita Zhivotovskiy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ab6a2c6ee757afe43882121281f6065c-Abstract-Conference.html": {
    "title": "Optimal and Approximate Adaptive Stochastic Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ran Ben-Basat",
      "Yaniv Ben-Itzhak",
      "Michael Mitzenmacher",
      "Shay Vargaftik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ab6eba9a853087993addff937c8cec87-Abstract-Conference.html": {
    "title": "Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhan Kim",
      "Chungman Lee",
      "Eulrang Cho",
      "Kyungphil Park",
      "Ho-young Kim",
      "Joonyoung Kim",
      "Yongkweon Jeon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ab9f9cfe97da3665e08f50ade9f8c4d6-Abstract-Conference.html": {
    "title": "Deterministic Policies for Constrained Reinforcement Learning in Polynomial Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy McMahan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/abbbb25cddb2c2cd08714e6bfa2f0634-Abstract-Conference.html": {
    "title": "Improving Subgroup Robustness via Data Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saachi Jain",
      "Kimia Hamidieh",
      "Kristian Georgiev",
      "Andrew Ilyas",
      "Marzyeh Ghassemi",
      "Aleksander Madry"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/abc1943857a42935ceacff03c524bb44-Abstract-Conference.html": {
    "title": "Time-FFM: Towards LM-Empowered Federated Foundation Model for Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingxiang Liu",
      "Xu Liu",
      "Chenghao Liu",
      "Qingsong Wen",
      "Yuxuan Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/abccb8a90b30d45b948360ba41f5a20f-Abstract-Conference.html": {
    "title": "Hypothesis Testing the Circuit Hypothesis in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Claudia Shi",
      "Nicolas Beltran Velez",
      "Achille Nazaret",
      "Carolina Zheng",
      "Adrià Garriga-Alonso",
      "Andrew Jesson",
      "Maggie Makar",
      "David M. Blei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/abd3c6b90e474ec50a52c446926b00be-Abstract-Conference.html": {
    "title": "Temporal Graph Neural Tangent Kernel with Graphon-Guaranteed",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katherine Tieu",
      "Dongqi Fu",
      "Yada Zhu",
      "Hendrik Hamann",
      "Jingrui He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/abe31a12e83111fdf2cfd54deed5a2ce-Abstract-Conference.html": {
    "title": "Test-time Adaptation in Non-stationary Environments via Adaptive Representation Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen-Yu Zhang",
      "Zhiyu Xie",
      "Huaxiu Yao",
      "Masashi Sugiyama"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/abf731c2993f9b1ee417cc3734787d7a-Abstract-Conference.html": {
    "title": "Accelerating Augmentation Invariance Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhong Lin",
      "Cheng-En Wu",
      "Yibing Wei",
      "Pedro Morgado"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ac04e54e0a2d1927d60709019e4e7870-Abstract-Conference.html": {
    "title": "Pre-training Differentially Private Models with Limited Public Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqi Bu",
      "Xinwei Zhang",
      "Sheng Zha",
      "Mingyi Hong",
      "George Karypis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ac3cea0be817ebac21299b77fd114ddf-Abstract-Conference.html": {
    "title": "A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning with General Function Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heyang Zhao",
      "Jiafan He",
      "Quanquan Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ac4106bcfff33140de7799d03daeb8a4-Abstract-Conference.html": {
    "title": "Is Cross-validation the Gold Standard to Estimate Out-of-sample Model Performance?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Garud Iyengar",
      "Henry Lam",
      "Tianyu Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ac4c13a5c6ec1bba68c9ce1d908321d2-Abstract-Conference.html": {
    "title": "Soft-Label Integration for Robust Toxicity Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelei Cheng",
      "Xian Wu",
      "Jiahao Yu",
      "Shuo Han",
      "Xin-Qiang Cai",
      "Xinyu Xing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ac662d74829e4407ce1d126477f4a03a-Abstract-Conference.html": {
    "title": "Online Composite Optimization Between Stochastic and Adversarial Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibo Wang",
      "SIJIA CHEN",
      "Wei Jiang",
      "Wenhao Yang",
      "Yuanyu Wan",
      "Lijun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ac6c452efdf063644d330caba2ab1e57-Abstract-Conference.html": {
    "title": "Stepping Forward on the Last Mile",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Feng",
      "Jay Zhuo",
      "Parker Zhang",
      "Ramchalam Kinattinkara Ramakrishnan",
      "Zhaocong Yuan",
      "Andrew Zou Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ac8ec9b4d94c03f0af8c4fe3d5fad4fd-Abstract-Conference.html": {
    "title": "Adam with model exponential moving average is effective for nonconvex optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwangjun Ahn",
      "Ashok Cutkosky"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ac8fbba029dadca99d6b8c3f913d3ed6-Abstract-Conference.html": {
    "title": "Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alliot Nagle",
      "Adway Girish",
      "Marco Bondaschi",
      "Michael Gastpar",
      "Ashok Vardhan Makkuva",
      "Hyeji Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aca97732e30bcf1303bc22ac3924fd16-Abstract-Conference.html": {
    "title": "From Chaos to Clarity: 3DGS in the Dark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Li",
      "Yufei Wang",
      "Alex Kot",
      "Bihan Wen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/acb94e709f02895fd98b5867f0b184f3-Abstract-Conference.html": {
    "title": "RandNet-Parareal: a time-parallel PDE solver using Random Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guglielmo Gattiglio",
      "Lyudmila Grigoryeva",
      "Massimiliano Tamborrino"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/acccb791886d5d811fe4e16e98c26633-Abstract-Conference.html": {
    "title": "A Primal-Dual-Assisted Penalty Approach to Bilevel Optimization with Coupled Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liuyuan Jiang",
      "Quan Xiao",
      "Victor Tenorio",
      "Fernando Real-Rojas",
      "Antonio G. Marques",
      "Tianyi Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/acf4a08f67724e9d2de34099f57a9c25-Abstract-Conference.html": {
    "title": "Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roland Stolz",
      "Hanna Krasowski",
      "Jakob Thumm",
      "Michael Eichelbeck",
      "Philipp Gassert",
      "Matthias Althoff"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ad15848baa3932c0d2deabf0e11d1dcd-Abstract-Conference.html": {
    "title": "AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zigeng Chen",
      "Xinyin Ma",
      "Gongfan Fang",
      "Zhenxiong Tan",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ad1efab57a04d93f097e7fbb2d4fc054-Abstract-Conference.html": {
    "title": "SyncTweedies: A General Generative Framework Based on Synchronized Diffusions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaihoon Kim",
      "Juil Koo",
      "Kyeongmin Yeo",
      "Minhyuk Sung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ad217e0c7fecc71bdf48660ad6714b07-Abstract-Conference.html": {
    "title": "Grokking of Implicit Reasoning in Transformers: A Mechanistic Journey to the Edge of Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boshi Wang",
      "Xiang Yue",
      "Yu Su",
      "Huan Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ad2b3b0963114adddbd2c97d6e74abb9-Abstract-Conference.html": {
    "title": "Effective Exploration Based on the Structural Information Principles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianghua Zeng",
      "Hao Peng",
      "Angsheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ad30b5f8274286c4ee12de843f03a07f-Abstract-Conference.html": {
    "title": "Safe Time-Varying Optimization based on Gaussian Processes with Spatio-Temporal Kernel",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialin Li",
      "Marta Zagorowska",
      "Giulia De Pasquale",
      "Alisa Rupenyan",
      "John Lygeros"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ad350eaaa93c8c3ab762cdc119d12889-Abstract-Conference.html": {
    "title": "Nonstationary Sparse Spectral Permanental Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zicheng Sun",
      "Yixuan Zhang",
      "Zenan Ling",
      "Xuhui Fan",
      "Feng Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ad3d0ac42b4b5cc3b5f0ca10107d5c84-Abstract-Conference.html": {
    "title": "Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avelina Hadji-Kyriacou",
      "Ognjen Arandjelovic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ad405c243d2940db5b24057ed26b2b2f-Abstract-Conference.html": {
    "title": "On the Use of Anchoring for Training Vision Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivek Sivaraman Narayanaswamy",
      "Kowshik Thopalli",
      "Rushil Anirudh",
      "Yamen Mubarka",
      "Wesam Sakla",
      "Jay Thiagarajan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ad47b1801557e4be37d30baf623de426-Abstract-Conference.html": {
    "title": "Diffusion-Reward Adversarial Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun-Mao Lai",
      "Hsiang-Chun Wang",
      "Ping-Chun Hsieh",
      "Frank Wang",
      "Min-Hung Chen",
      "Shao-Hua Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ad48f017e6c3d474caf511208e600459-Abstract-Conference.html": {
    "title": "Computerized Adaptive Testing via Collaborative Ranking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Liu",
      "Yan Zhuang",
      "Qi Liu",
      "Jiatong Li",
      "Yuren Zhang",
      "Zhenya Huang",
      "Jinze Wu",
      "Shijin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ad62d81bb6edee5f9b33f5e0d34a7943-Abstract-Conference.html": {
    "title": "Reverse Transition Kernel: A Flexible Framework to Accelerate Diffusion Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xunpeng Huang",
      "Difan Zou",
      "Hanze Dong",
      "Zhang",
      "Yian Ma",
      "Tong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ad7922fd4650f8aba5d8b067e622ca84-Abstract-Conference.html": {
    "title": "Preventing Dimensional Collapse in Self-Supervised Learning via Orthogonality Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junlin He",
      "Jinxiao Du",
      "Wei Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ad86418f7bdfa685cd089e028efd75cd-Abstract-Conference.html": {
    "title": "Shape analysis for time series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibaut Germain",
      "Samuel Gruffaz",
      "Charles Truong",
      "Alain Durmus",
      "Laurent Oudre"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ad885a9caafff30ee9cafdf0ee42fda2-Abstract-Conference.html": {
    "title": "Optimistic Verifiable Training by Controlling Hardware Nondeterminism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Megha Srivastava",
      "Simran Arora",
      "Dan Boneh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ad9964b731bc7b5621a83c7869fc653b-Abstract-Conference.html": {
    "title": "Graph neural networks and non-commuting operators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mauricio Velasco",
      "Kaiying O&#x27;Hare",
      "Bernardo Rychtenberg",
      "Soledad Villar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ada93fa6643735f294be51dc31eebbd4-Abstract-Conference.html": {
    "title": "Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ShengYun Peng",
      "Pin-Yu Chen",
      "Matthew Hull",
      "Duen Horng Chau"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/adb77ecc8ba1c2d3135c86a46b8f2496-Abstract-Conference.html": {
    "title": "Real-time Core-Periphery Guided ViT with Smart Data Layout Selection on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Shu",
      "Xiaowei Yu",
      "Zihao Wu",
      "Wenqi Jia",
      "Yinchen Shi",
      "Miao Yin",
      "Tianming Liu",
      "Dajiang Zhu",
      "Wei Niu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/adbea136219b64db96a9941e4249a857-Abstract-Conference.html": {
    "title": "ContextCite: Attributing Model Generation to Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Cohen-Wang",
      "Harshay Shah",
      "Kristian Georgiev",
      "Aleksander Madry"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ade039c1db0391106a3375bd2feb310a-Abstract-Conference.html": {
    "title": "Randomized algorithms and PAC bounds for inverse reinforcement learning in continuous spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angeliki Kamoutsi",
      "Peter Schmitt-Förster",
      "Tobias Sutter",
      "Volkan Cevher",
      "John Lygeros"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/adf7fa39d65e2983d724ff7da57f00ac-Abstract-Conference.html": {
    "title": "Improving Neural ODE Training with Temporal Adaptive Batch Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Su Zheng",
      "Zhengqi Gao",
      "Fan-Keng Sun",
      "Duane Boning",
      "Bei Yu",
      "Martin D. Wong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ae03bdef276132fae089692445725635-Abstract-Conference.html": {
    "title": "Rethinking Reconstruction-based Graph-Level Anomaly Detection: Limitations and a Simple Remedy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunwoo Kim",
      "Soo Yong Lee",
      "Fanchen Bu",
      "Shinhwan Kang",
      "Kyungho Kim",
      "Jaemin Yoo",
      "Kijung Shin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ae1752315811107ee07aa5ab06cead71-Abstract-Conference.html": {
    "title": "Achieving Linear Convergence with Parameter-Free Algorithms in Decentralized Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilya Kuruzov",
      "Gesualdo Scutari",
      "Alexander Gasnikov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ae28c7bc9414ffd8ffd2b3d454e6ef3e-Abstract-Conference.html": {
    "title": "Contrastive-Equivariant Self-Supervised Learning Improves Alignment with Primate Visual Area IT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Yerxa",
      "Jenelle Feather",
      "Eero P. Simoncelli",
      "SueYeon Chung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ae338b3fe6624798efa80b6581924028-Abstract-Conference.html": {
    "title": "Faster Algorithms for User-Level Private Stochastic Convex Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Lowy",
      "Daogao Liu",
      "Hilal Asi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ae485c4579564ca17b643d45eb598930-Abstract-Conference.html": {
    "title": "Identify Then Recommend: Towards Unsupervised Group Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Liu",
      "Shihao Zhu",
      "Tianyuan Yang",
      "Jian Ma",
      "Wenliang Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ae6a89006985492a3b9bf5605d6ad6c5-Abstract-Conference.html": {
    "title": "Fast Encoder-Based 3D from Casual Videos via Point Track Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoni Kasten",
      "Wuyue Lu",
      "Haggai Maron"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ae6c7dbd9429b3a75c41b5fb47e57c9e-Abstract-Conference.html": {
    "title": "DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen-Hsuan Chu",
      "Lei Ke",
      "Katerina Fragkiadaki"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ae6c81a39079ddeb88b034b6ef18c7fe-Abstract-Conference.html": {
    "title": "A distributional simplicity bias in the learning dynamics of transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Rende",
      "Federica Gerace",
      "Alessandro Laio",
      "Sebastian Goldt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ae8b0b5838ba510daff1198474e7b984-Abstract-Conference.html": {
    "title": "PCoTTA: Continual Test-Time Adaptation for Multi-Task Point Cloud Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jincen Jiang",
      "Qianyu Zhou",
      "Yuhang Li",
      "Xinkui Zhao",
      "Meili Wang",
      "Lizhuang Ma",
      "Jian Chang",
      "Jian.J Zhang",
      "Xuequan Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ae8d4084f418bb51575c2ca6c658a05b-Abstract-Conference.html": {
    "title": "Nearly Minimax Optimal Submodular Maximization with Bandit Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artin Tajdini",
      "Lalit Jain",
      "Kevin G. Jamieson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ae90d88755e0eaeb9121712fbac4e8de-Abstract-Conference.html": {
    "title": "Latent Representation Matters: Human-like Sketches in One-shot Drawing Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Boutin",
      "Rishav Mukherji",
      "Aditya Agrawal",
      "Sabine Muzellec",
      "Thomas Fel",
      "Thomas Serre",
      "Rufin VanRullen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aea8bdc42d8ba3a67a69b3f18be93f69-Abstract-Conference.html": {
    "title": "Data-Driven Discovery of Dynamical Systems in Pharmacology using Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Holt",
      "Zhaozhi Qian",
      "Tennison Liu",
      "Jim Weatherall",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aeae9df8e6bfe7350160bb42965dbd44-Abstract-Conference.html": {
    "title": "Improved Generation of Adversarial Examples Against Safety-aligned LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qizhang Li",
      "Yiwen Guo",
      "Wangmeng Zuo",
      "Hao Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aec2dfc4f5e19acf05c15587c889dbc4-Abstract-Conference.html": {
    "title": "Text-Guided Attention is All You Need for Zero-Shot Robustness in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Yu",
      "Haiyang Zhang",
      "Changsheng Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aee5298251a418aad89618cf6b5e7ccc-Abstract-Conference.html": {
    "title": "Classification Done Right for Vision-Language Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilong Huang",
      "Qinghao Ye",
      "Bingyi Kang",
      "Jiashi Feng",
      "Haoqi Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/aefb164890f80762128efc135205a308-Abstract-Conference.html": {
    "title": "COVE: Unleashing the Diffusion Feature Correspondence for Consistent Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangshan Wang",
      "Yue Ma",
      "Jiayi Guo",
      "Yicheng Xiao",
      "Gao Huang",
      "Xiu Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/af10f27d0a48175e486a647c06c7a9c6-Abstract-Conference.html": {
    "title": "Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitry Kovalev",
      "Ekaterina Borodich",
      "Alexander Gasnikov",
      "Dmitrii Feoktistov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/af1c61e4dd59596f033d826419870602-Abstract-Conference.html": {
    "title": "FactorizePhys: Matrix Factorization for Multidimensional Attention in Remote Physiological Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jitesh Joshi",
      "Sos Agaian",
      "Youngjun Cho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/af2bb2b2280d36f8842e440b4e275152-Abstract-Conference.html": {
    "title": "Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoang Chi",
      "He Li",
      "Wenjing Yang",
      "Feng Liu",
      "Long Lan",
      "Xiaoguang Ren",
      "Tongliang Liu",
      "Bo Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/af45937b353f539d0a518b3bb890a01d-Abstract-Conference.html": {
    "title": "Convolutions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Dangel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/af536fa4312b5b2e5c541156cd31e93b-Abstract-Conference.html": {
    "title": "Lightweight Frequency Masker for Cross-Domain Few-Shot Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jintao Tong",
      "Yixiong Zou",
      "Yuhua Li",
      "Ruixuan Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/af58a33861ac45472ea1cc5860d2b13e-Abstract-Conference.html": {
    "title": "The Power of Hard Attention Transformers on Data Sequences: A formal language theoretic perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pascal Bergsträßer",
      "Chris Köcher",
      "Anthony Lin",
      "Georg Zetzsche"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/af79bcbd1d229a527c8f10d8d41c589d-Abstract-Conference.html": {
    "title": "Latent Intrinsics Emerge from Training to Relight",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Zhang",
      "William Gao",
      "Seemandhar Jain",
      "Michael Maire",
      "David A. Forsyth",
      "Anand Bhattad"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/af7cc9e2366b8f8837a6ef339810277a-Abstract-Conference.html": {
    "title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Gao",
      "Haiyun Jiang",
      "Deng Cai",
      "Shuming Shi",
      "Wai Lam"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/af85ade5e70a6e1eb07a9541fb529baf-Abstract-Conference.html": {
    "title": "Inflationary Flows: Calibrated Bayesian Inference with Diffusion-Based Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniela de Albuquerque",
      "John C. Pearson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/af8af7a2346099926b5c0d9e974df03e-Abstract-Conference.html": {
    "title": "Trading Place for Space: Increasing Location Resolution Reduces Contextual Capacity in Hippocampal Codes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Spencer Rooke",
      "Zhaoze Wang",
      "Ronald Di Tullio",
      "Vijay Balasubramanian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/af9ac087ed9123957bb3a45dca56b9d4-Abstract-Conference.html": {
    "title": "RealCompo: Balancing Realism and Compositionality Improves Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinchen Zhang",
      "Ling Yang",
      "YaQi Cai",
      "Zhaochen Yu",
      "Kai-Ni Wang",
      "xie jiake",
      "Ye Tian",
      "Minkai Xu",
      "Yong Tang",
      "Yujiu Yang",
      "Bin CUI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/afa58a5b6adc0845e0fd632132a64c39-Abstract-Conference.html": {
    "title": "RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhicheng Sun",
      "Zhenhao Yang",
      "Yang Jin",
      "Haozhe Chi",
      "Kun Xu",
      "Liwei Chen",
      "Hao Jiang",
      "Yang Song",
      "Kun Gai",
      "Yadong Mu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/afbe068bd0469f4cd778c0f8106181b6-Abstract-Conference.html": {
    "title": "Rethinking Deep Thinking: Stable Learning of Algorithms using Lipschitz Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jay Bear",
      "Adam Prugel-Bennett",
      "Jonathon Hare"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b0049c3f9c53fb06f674ae66c2cf2376-Abstract-Conference.html": {
    "title": "Apathetic or Empathetic? Evaluating LLMs' Emotional Alignments with Humans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jen-Tse Huang",
      "Man Ho LAM",
      "Eric John Li",
      "Shujie Ren",
      "Wenxuan Wang",
      "Wenxiang Jiao",
      "Zhaopeng Tu",
      "Michael R Lyu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b00ef390dcd5f147fd7c5c2bb35f09be-Abstract-Conference.html": {
    "title": "Approximately Equivariant Neural Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Ashman",
      "Cristiana Diaconu",
      "Adrian Weller",
      "Wessel Bruinsma",
      "Richard Turner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b00efe6d4dbf119e2d0d44186cdfe7c8-Abstract-Conference.html": {
    "title": "A Simple yet Scalable Granger Causal Structural Learning Approach for Topological Event Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjia Li",
      "Shuo Liu",
      "Hong Qian",
      "Aimin Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b0131b6ee02a00b03fc3320176fec8f5-Abstract-Conference.html": {
    "title": "MeshXL: Neural Coordinate Field for Generative 3D Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijin Chen",
      "Xin Chen",
      "Anqi Pang",
      "Xianfang Zeng",
      "Wei Cheng",
      "Yijun Fu",
      "Fukun Yin",
      "Billzb Wang",
      "Jingyi Yu",
      "Gang Yu",
      "BIN FU",
      "Tao Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b0171a5965e0b58afcd4ce1adb0baa56-Abstract-Conference.html": {
    "title": "ID-to-3D: Expressive ID-guided 3D Heads via Score Distillation Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesca Babiloni",
      "Alexandros Lattas",
      "Jiankang Deng",
      "Stefanos Zafeiriou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b0313c2f4501a81d0e0d4a1e8fbf4995-Abstract-Conference.html": {
    "title": "SfPUEL: Shape from Polarization under Unknown Environment Light",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youwei Lyu",
      "Heng Guo",
      "Kailong Zhang",
      "Si Li",
      "Boxin Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b041cbfcc3f282a9b3c8eb9c16177529-Abstract-Conference.html": {
    "title": "Making Offline RL Online: Collaborative World Models for Offline Visual Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Wang",
      "Junming Yang",
      "Yunbo Wang",
      "Xin Jin",
      "Wenjun Zeng",
      "Xiaokang Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b0499a1aecf036d42074d03f621d7864-Abstract-Conference.html": {
    "title": "Unravelling in Collaborative Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aymeric Capitaine",
      "Etienne Boursier",
      "Antoine Scheid",
      "Eric Moulines",
      "Michael I. Jordan",
      "El-Mahdi El-Mhamdi",
      "Alain Durmus"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b05e6a11df6eacb600074a42bb28ae52-Abstract-Conference.html": {
    "title": "Noisy Label Learning with Instance-Dependent Outliers: Identifiability via Crowd Wisdom",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tri Nguyen",
      "Shahana Ibrahim",
      "Xiao Fu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b063829b922fdeb4fa3472dd3471ff43-Abstract-Conference.html": {
    "title": "Mini-Sequence Transformers: Optimizing Intermediate Memory for Long Sequences Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "cheng Luo",
      "Jiawei Zhao",
      "Zhuoming Chen",
      "Beidi Chen",
      "Animashree Anandkumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b09df3a10e26204136540ca59bc5a646-Abstract-Conference.html": {
    "title": "Structure Consistent Gaussian Splatting with Matching Prior for Few-shot Novel View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Peng",
      "Wangze Xu",
      "Luyang Tang",
      "levio leo",
      "Jianbo Jiao",
      "Ronggang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b09e445cd23b356589e8a0e39e84549f-Abstract-Conference.html": {
    "title": "MC-DiT: Contextual Enhancement via Clean-to-Clean Reconstruction for Masked Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanghao Zheng",
      "Yuchen Liu",
      "Wenrui Dai",
      "Chenglin Li",
      "Junni Zou",
      "Hongkai Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b0ae046e198a5e43141519868a959c74-Abstract-Conference.html": {
    "title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yazid Janati",
      "Badr MOUFAD",
      "Alain Durmus",
      "Eric Moulines",
      "Jimmy Olsson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b0b750c4189f19d0cd71375e9e17f83f-Abstract-Conference.html": {
    "title": "GaussianCube: A Structured and Explicit Radiance Representation for 3D Generative Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Zhang",
      "Yiji Cheng",
      "Jiaolong Yang",
      "Chunyu Wang",
      "Feng Zhao",
      "Yansong Tang",
      "Dong Chen",
      "Baining Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b0bc711f48724237b38823c4d9cee10b-Abstract-Conference.html": {
    "title": "Learn more, but bother less: parameter efficient continual learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuli Qiao",
      "Mehrdad Mahdavi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b0ca717599b7ba84d5e4f4c8b1ef6657-Abstract-Conference.html": {
    "title": "Harmonizing Visual Text Comprehension and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Zhao",
      "Jingqun Tang",
      "Binghong Wu",
      "Chunhui Lin",
      "Shu Wei",
      "Hao Liu",
      "Xin Tan",
      "zhizhong zhang",
      "Can Huang",
      "Yuan Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b0d4b9d3dc41b36cad1d36ddd27858ab-Abstract-Conference.html": {
    "title": "B-ary Tree Push-Pull Method is Provably Efficient for Distributed Learning on Heterogeneous Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runze You",
      "Shi Pu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b0e40256fe5d90b78525312564a2de64-Abstract-Conference.html": {
    "title": "Neural Characteristic Activation Analysis and Geometric Parameterization for ReLU Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenlin Chen",
      "Hong Ge"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b0e89a49af1fb2ebea69bfc39df0be4a-Abstract-Conference.html": {
    "title": "Diffusion-based Curriculum Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erdi Sayar",
      "Giovanni Iacca",
      "Ozgur S. Oguz",
      "Alois Knoll"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b0f25f0a63cc544d506e4c1374a3c807-Abstract-Conference.html": {
    "title": "CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Brandfonbrener",
      "Hanlin Zhang",
      "Andreas Kirsch",
      "Jonathan Richard Schwarz",
      "Sham Kakade"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b111aa30ab71255946b19b6bd4e68939-Abstract-Conference.html": {
    "title": "SCube: Instant Large-Scale Scene Reconstruction using VoxSplats",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanchi Ren",
      "Yifan Lu",
      "hanxue liang",
      "Jay Zhangjie Wu",
      "Huan Ling",
      "Mike Chen",
      "Sanja Fidler",
      "Francis Williams",
      "Jiahui Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b125999bde7e80910cbdbd323087df8f-Abstract-Conference.html": {
    "title": "Dueling over Dessert, Mastering the Art of Repeated Cake Cutting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simina Branzei",
      "MohammadTaghi Hajiaghayi",
      "Reed Phillips",
      "Suho Shin",
      "Kun Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b12a1d1014e952e676f5d6931d03241a-Abstract-Conference.html": {
    "title": "AID: Attention Interpolation of Text-to-Image Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "He Qiyuan",
      "Jinghao Wang",
      "Ziwei Liu",
      "Angela Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b130a5691815f550977e331f8bec08ae-Abstract-Conference.html": {
    "title": "SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware Self-Reflection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangxin Liu",
      "Xuebo Liu",
      "Derek Wong",
      "Dongfang Li",
      "Ziyi Wang",
      "Baotian Hu",
      "Min Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b157cfde6794e93b2353b9712bbd45a5-Abstract-Conference.html": {
    "title": "Real-world Image Dehazing with Coherence-based Pseudo Labeling and Cooperative Unfolding Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyu Fang",
      "Chunming He",
      "Fengyang Xiao",
      "Yulun Zhang",
      "Longxiang Tang",
      "Yuelin Zhang",
      "Kai Li",
      "Xiu Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b1762f00cbbcd8dd879ddcea41b04b91-Abstract-Conference.html": {
    "title": "Semi-supervised Multi-label Learning with Balanced Binary Angular Margin Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ximing Li",
      "Silong Liang",
      "Changchun Li",
      "pengfei wang",
      "Fangming Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b176e960edbb8afe77f77f9039d2738a-Abstract-Conference.html": {
    "title": "Statistical-Computational Trade-offs for Density Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anders Aamand",
      "Alexandr Andoni",
      "Justin Chen",
      "Piotr Indyk",
      "Shyam Narayanan",
      "Sandeep Silwal",
      "Haike Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b17e1642998a8214be240b3056f5faa5-Abstract-Conference.html": {
    "title": "Extracting Training Data from Molecular Pre-trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renhong Huang",
      "Jiarong Xu",
      "Zhiming Yang",
      "Xiang Si",
      "Xin Jiang",
      "Hanyang Yuan",
      "Chunping Wang",
      "YANG YANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b1a72c79ac7512df8d7b573f38143ac4-Abstract-Conference.html": {
    "title": "Learning-Augmented Algorithms with Explicit Predictors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marek Elias",
      "Haim Kaplan",
      "Yishay Mansour",
      "Shay Moran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b1b16c4b875eb84d3585cb70d23970ca-Abstract-Conference.html": {
    "title": "Do LLMs Build World Representations? Probing Through the Lens of State Abstraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichao Li",
      "Yanshuai Cao",
      "Jackie CK Cheung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b1c446eebd9a317dd0e96b16908c821a-Abstract-Conference.html": {
    "title": "Understanding Transformers via N-Gram Statistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timothy Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b1c62bdeee97b38c34dcda152c829511-Abstract-Conference.html": {
    "title": "Neuro-Vision to Language: Enhancing Brain Recording-based Visual Reconstruction and Language Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guobin Shen",
      "Dongcheng Zhao",
      "Xiang He",
      "Linghao Feng",
      "Yiting Dong",
      "Jihang Wang",
      "Qian Zhang",
      "Yi Zeng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b1d35561c4a4a0e0b6012b2af531e149-Abstract-Conference.html": {
    "title": "Transformers need glasses! Information over-squashing in language tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Barbero",
      "Andrea Banino",
      "Steven Kapturowski",
      "Dharshan Kumaran",
      "João Madeira Araújo",
      "Oleksandr Vitvitskyi",
      "Razvan Pascanu",
      "Petar Veličković"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b1f140eeee243db24e9e006481b91cf1-Abstract-Conference.html": {
    "title": "Statistical Multicriteria Benchmarking via the GSD-Front",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christoph Jansen",
      "Georg Schollmeyer",
      "Julian Rodemann",
      "Hannah Blocher",
      "Thomas Augustin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b1f34d7b4a03a3d80be8e72eb430dd81-Abstract-Conference.html": {
    "title": "MixEval: Deriving Wisdom of the Crowd from LLM Benchmark Mixtures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinjie Ni",
      "Fuzhao Xue",
      "Xiang Yue",
      "Yuntian Deng",
      "Mahir Shah",
      "Kabir Jain",
      "Graham Neubig",
      "Yang You"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b204de7078301292a8876a762eed3dcb-Abstract-Conference.html": {
    "title": "Stopping Bayesian Optimization with Probabilistic Regret Bounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Wilson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b206d54ffbb803b5c51d85f405d422e4-Abstract-Conference.html": {
    "title": "DDK: Distilling Domain Knowledge for Efficient Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaheng Liu",
      "Chenchen Zhang",
      "Jinyang Guo",
      "Yuanxing Zhang",
      "Haoran Que",
      "Ken Deng",
      "ZhiqiBai zhiqi",
      "Jie Liu",
      "Ge Zhang",
      "JiakaiWang",
      "Yanan Wu",
      "Congnan Liu",
      "Jiamang Wang",
      "Lin Qu",
      "Wenbo Su",
      "Bo Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b2077e6d66da612fcb701589efa9ce88-Abstract-Conference.html": {
    "title": "AP-Adapter: Improving Generalization of Automatic Prompts on Unseen Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Fu",
      "Zhiwei Jiang",
      "Yuliang Liu",
      "Cong Wang",
      "Zexuan Deng",
      "Zhaoling Chen",
      "Qing Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b213d9a999b82cb6fcd03a0d5a7498be-Abstract-Conference.html": {
    "title": "A Modular Conditional Diffusion Framework for Image Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Magauiya Zhussip",
      "Iaroslav Koshelev",
      "Stamatios Lefkimmiatis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b225f5c7cd13615e9558c3931fa4e66f-Abstract-Conference.html": {
    "title": "Scale-invariant Optimal Sampling for Rare-events Data and Sparse Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wang",
      "HaiYing Wang",
      "Hao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b235f0b417e8bf270c0cb19fe0b82c1e-Abstract-Conference.html": {
    "title": "TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Guo",
      "Shaobin Zhuang",
      "Kunchang Li",
      "Yu Qiao",
      "Yali Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b25222d2d405e0768d218e7fc90070b2-Abstract-Conference.html": {
    "title": "Few-Shot Task Learning through Inverse Generative Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aviv Netanyahu",
      "Yilun Du",
      "Antonia Bronars",
      "Jyothish Pari",
      "Josh Tenenbaum",
      "Tianmin Shu",
      "Pulkit Agrawal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b288470688e72f58c02031304ad6339f-Abstract-Conference.html": {
    "title": "Scene Graph Disentanglement and Composition for Generalizable Complex Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunnan Wang",
      "Ziqiang Li",
      "Wenyao Zhang",
      "Zequn Zhang",
      "Baao Xie",
      "Xihui Liu",
      "Wenjun Zeng",
      "Xin Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b2913cff905a649c5bda3ce2cd19088c-Abstract-Conference.html": {
    "title": "Physics-Informed Variational State-Space Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oliver Hamelijnck",
      "Arno Solin",
      "Theodoros Damoulas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b2b3e1d9840eba17ad9bbf073e009afe-Abstract-Conference.html": {
    "title": "SuperDeepFool: a new fast and accurate minimal adversarial attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "alireza abdollahpour",
      "Mahed Abroshan",
      "Seyed-Mohsen Moosavi-Dezfooli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b2b781badeeb49896c4b324c466ec442-Abstract-Conference.html": {
    "title": "Dual Cone Gradient Descent for Training Physics-Informed Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngsik Hwang",
      "Dongyoung Lim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b2c39fe6ce838440faf03a0f780e7a63-Abstract-Conference.html": {
    "title": "On the Efficiency of ERM in Feature Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayoub El Hanchi",
      "Chris J Maddison",
      "Murat A Erdogdu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b2c4b7d34b3d96b9dc12f7bce424b7ae-Abstract-Conference.html": {
    "title": "Uncovering the Redundancy in Graph Self-supervised Learning Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhibiao Wang",
      "Xiao Wang",
      "Haoyue Deng",
      "Nian Liu",
      "Shirui Pan",
      "Chunming Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b2c892312af07f8a77afbeed188391f4-Abstract-Conference.html": {
    "title": "Membership Inference Attacks against Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhan Li",
      "Yongtao Wu",
      "Yihang Chen",
      "Francesco Tonin",
      "Elias Abad Rocamora",
      "Volkan Cevher"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b2d4051f03a7038a2771dfbbe5c7b54e-Abstract-Conference.html": {
    "title": "Transformers Learn to Achieve Second-Order Convergence Rates for In-Context Linear Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deqing Fu",
      "Tian-qi Chen",
      "Robin Jia",
      "Vatsal Sharan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b2e20d7402c9985eae4ba924c65370a8-Abstract-Conference.html": {
    "title": "$\\text{Di}^2\\text{Pose}$: Discrete Diffusion Model for Occluded 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiquan Wang",
      "Jun Xiao",
      "Chunping Wang",
      "Wei Liu",
      "Zhao Wang",
      "Long Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b2e2774c8e76afe191b5bf518f5cb727-Abstract-Conference.html": {
    "title": "Drift-Resilient TabPFN: In-Context Learning Temporal Distribution Shifts on Tabular Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Helli",
      "David Schnurr",
      "Noah Hollmann",
      "Samuel Müller",
      "Frank Hutter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b2f2af5403042b1344f4e93b35fb67d9-Abstract-Conference.html": {
    "title": "Flex-MoE: Modeling Arbitrary Modality Combination via the Flexible Mixture-of-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sukwon Yun",
      "Inyoung Choi",
      "Jie Peng",
      "Yangfan Wu",
      "Jingxuan Bao",
      "Qiyiwen Zhang",
      "Jiayi Xin",
      "Qi Long",
      "Tianlong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b2fea79b1137d917e8b7cce9434ab5fa-Abstract-Conference.html": {
    "title": "Diffusion-DICE: In-Sample Diffusion Guidance for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyuan Mao",
      "Haoran Xu",
      "Xianyuan Zhan",
      "Weinan Zhang",
      "Amy Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b304a04c6a4ac168aa692923814ace17-Abstract-Conference.html": {
    "title": "An Equivalence Between Static and Dynamic Regret Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Jacobsen",
      "Francesco Orabona"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b319c0e8092ba726cb22e718d7d9a95e-Abstract-Conference.html": {
    "title": "Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoqi Zhang",
      "Ziwei Luo",
      "Jens Sjölund",
      "Thomas Schön",
      "Per Mattsson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b31c332c4cebcec31b788400b47c94b3-Abstract-Conference.html": {
    "title": "Hierarchy-Agnostic Unsupervised Segmentation: Parsing Semantic Image Structure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simone Rossetti",
      "Fiora Pirri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b31f6d65f2584b3c4347148db36fe07f-Abstract-Conference.html": {
    "title": "Can neural operators always be continuously discretized?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takashi Furuya",
      "Michael Puthawala",
      "Matti Lassas",
      "Maarten V. de Hoop"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b350d52dc4794c9e5349e15be69f70be-Abstract-Conference.html": {
    "title": "Sample-Efficient Private Learning of Mixtures of Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hassan Ashtiani",
      "Mahbod Majid",
      "Shyam Narayanan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b35c38f70065ac6c694089ca93a015bb-Abstract-Conference.html": {
    "title": "ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "jingnan zheng",
      "Han Wang",
      "An Zhang",
      "Nguyen Duy Tai",
      "Jun Sun",
      "Tat-Seng Chua"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b360fd42ced877429882a2a68b4a4343-Abstract-Conference.html": {
    "title": "Non-asymptotic Approximation Error Bounds of Parameterized Quantum Circuits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhan Yu",
      "Qiuhao Chen",
      "Yuling Jiao",
      "Yinan Li",
      "Xiliang Lu",
      "Xin Wang",
      "Jerry Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b3a08d179347e33414badadf100e4e8d-Abstract-Conference.html": {
    "title": "ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Field",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiyohiro Nakayama",
      "Mikaela Angelina Uy",
      "Yang You",
      "Ke Li",
      "Leonidas Guibas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b3ac2a18635fb355100c85f0f2044b89-Abstract-Conference.html": {
    "title": "Elucidating the Design Space of Dataset Condensation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shitong Shao",
      "Zikai Zhou",
      "Huanran Chen",
      "Zhiqiang Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b3ac808c09f98444090a8f6c2d4bd1dc-Abstract-Conference.html": {
    "title": "Towards Next-Generation Logic Synthesis: A Scalable Neural Circuit Generation Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihai Wang",
      "Jie Wang",
      "Qingyue Yang",
      "Yinqi Bai",
      "Xing Li",
      "Lei Chen",
      "Jianye Hao",
      "Mingxuan Yuan",
      "Bin Li",
      "Yongdong Zhang",
      "Feng Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b3b0ea507520e3db70f7219b59fd5fd9-Abstract-Conference.html": {
    "title": "ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed Convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Mei",
      "Dongzhe Zheng",
      "Shihua Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b3b55c366d641c07180c40e4f978f311-Abstract-Conference.html": {
    "title": "Make-it-Real: Unleashing Large Multimodal Model for Painting 3D Objects with Realistic Materials",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Fang",
      "Zeyi Sun",
      "Tong Wu",
      "Jiaqi Wang",
      "Ziwei Liu",
      "Gordon Wetzstein",
      "Dahua Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b3bac97f3227c52c0179a6d967480867-Abstract-Conference.html": {
    "title": "MVInpainter: Learning Multi-View Consistent Inpainting to Bridge 2D and 3D Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenjie Cao",
      "Chaohui Yu",
      "Fan Wang",
      "Xiangyang Xue",
      "Yanwei Fu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b3cca813dcd78fe75e4d4df2e6a0b1a7-Abstract-Conference.html": {
    "title": "PaCE: Parsimonious Concept Engineering for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinqi Luo",
      "Tianjiao Ding",
      "Kwan Ho Ryan Chan",
      "Darshan Thaker",
      "Aditya Chattopadhyay",
      "Chris Callison-Burch",
      "Rene Vidal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b3cff9947cde08b4fa70652cb8ac9209-Abstract-Conference.html": {
    "title": "SHED: Shapley-Based Automated Dataset Refinement for Instruction Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yexiao He",
      "Ziyao Wang",
      "Zheyu Shen",
      "Guoheng Sun",
      "Yucong Dai",
      "Yongkai Wu",
      "Hongyi Wang",
      "Ang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b3d868b4b5b61b35a849ba6e7a1d4449-Abstract-Conference.html": {
    "title": "Unlearnable 3D Point Clouds: Class-wise Transformation Is All You Need",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianlong Wang",
      "Minghui Li",
      "Wei Liu",
      "Hangtao Zhang",
      "Shengshan Hu",
      "Yechao Zhang",
      "Ziqi Zhou",
      "Hai Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b3edfc1950c30c42e2ecf6637ab7fb09-Abstract-Conference.html": {
    "title": "How to Solve Contextual Goal-Oriented Problems with Offline Datasets?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Fan",
      "Jingling Li",
      "Adith Swaminathan",
      "Aditya Modi",
      "Ching-An Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b3f269d610fc0a0972bd4c2f4905165a-Abstract-Conference.html": {
    "title": "Scalable Kernel Inverse Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youyuan Long",
      "Tolga Ok",
      "Pedro Zattoni Scroccaro",
      "Peyman Mohajerin Esfahani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b40d5797756800c97f3d525c2e4c8357-Abstract-Conference.html": {
    "title": "DeNetDM: Debiasing by Network Depth Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Silpa Vadakkeeveetil Sreelatha",
      "Adarsh Kappiyath",
      "ABHRA CHAUDHURI",
      "Anjan Dutta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b41907dd4df5c60f86216b73fe0c7465-Abstract-Conference.html": {
    "title": "Length Optimization in Conformal Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shayan Kiyani",
      "George J. Pappas",
      "Hamed Hassani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b444ad72520a5f5c467343be88e352ed-Abstract-Conference.html": {
    "title": "Stochastic Zeroth-Order Optimization under Strongly Convexity and Lipschitz Hessian: Minimax Sample Complexity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Yu",
      "Yining Wang",
      "Baihe Huang",
      "Qi Lei",
      "Jason Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b46aaf640bc8659e65a1a573971ba5a2-Abstract-Conference.html": {
    "title": "Human-3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Xue",
      "Xianghui Xie",
      "Riccardo Marin",
      "Gerard Pons-Moll"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b46bc1449205888e1883f692aff1a252-Abstract-Conference.html": {
    "title": "Efficient Prompt Optimization Through the Lens of Best Arm Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengshuai Shi",
      "Kun Yang",
      "Zihan Chen",
      "Jundong Li",
      "Jing Yang",
      "Cong Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b48bb4ad31718381838f68bf5f0d4ff1-Abstract-Conference.html": {
    "title": "Fair Kernel K-Means: from Single Kernel to Multiple Kernel",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Zhou",
      "Rongwen Li",
      "Liang Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b4995cbd0b7f6a1a366ee65f9c4b02ac-Abstract-Conference.html": {
    "title": "Learning Commonality, Divergence and Variety for Unsupervised Visible-Infrared Person Re-identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangming Shi",
      "Xiangbo Yin",
      "Yachao Zhang",
      "zhizhong zhang",
      "Yuan Xie",
      "Yanyun Qu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b4b75092bb44a14815be33d052aa47f5-Abstract-Conference.html": {
    "title": "Stabilized Proximal-Point Methods for Federated Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaowen Jiang",
      "Anton Rodomanov",
      "Sebastian U Stich"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b4baac5d3f7508a4eb2b65376470a5a2-Abstract-Conference.html": {
    "title": "Why Go Full? Elevating Federated Learning Through Partial Network Updates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haolin Wang",
      "Xuefeng Liu",
      "Jianwei Niu",
      "Wenkai Guo",
      "Shaojie Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b4f1b80392bd582c8835b62cfb0bd523-Abstract-Conference.html": {
    "title": "Efficiently Learning Significant Fourier Feature Pairs for Statistical Independence Testing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Ren",
      "Yewei Xia",
      "Hao Zhang",
      "Jihong Guan",
      "Shuigeng Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b4fd162d3e2d015233486a2e313828a7-Abstract-Conference.html": {
    "title": "OwMatch: Conditional Self-Labeling with Consistency for Open-World Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjie Niu",
      "Lifan Lin",
      "Jian Huang",
      "Chao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b51693c2ba5b5ddf67429966576fb962-Abstract-Conference.html": {
    "title": "Can an AI Agent Safely Run a Government? Existence of Probably Approximately Aligned Policies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frédéric Berdoz",
      "Roger Wattenhofer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b51b50262b492dd89bb9cd3105a46702-Abstract-Conference.html": {
    "title": "DC-Gaussian: Improving 3D Gaussian Splatting for Reflective Dash Cam Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linhan Wang",
      "Kai Cheng",
      "Shuo Lei",
      "Shengkun Wang",
      "Wei Yin",
      "Chenyang Lei",
      "Xiaoxiao Long",
      "Chang-Tien Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b528459c99e929718a7d7e1697253d7f-Abstract-Conference.html": {
    "title": "Relating Hopfield Networks to Episodic Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hugo Chateau-Laurent",
      "Frederic Alexandre"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b53a96991e9fe42922261bd1ebbf899c-Abstract-Conference.html": {
    "title": "Explanations that reveal all through the deﬁnition of encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aahlad Manas Puli",
      "Nhi Nguyen",
      "Rajesh Ranganath"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b54532b0e57eb963b19e00583376cda3-Abstract-Conference.html": {
    "title": "Fourier Amplitude and Correlation Loss: Beyond Using L2 Loss for Skillful Precipitation Nowcasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiu-Wai Yan",
      "Shi Quan Foo",
      "Van Hoan Trinh",
      "Dit-Yan Yeung",
      "Ka-Hing Wong",
      "Wai-kin Wong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b577c062bd4f894b7e05fab6440373ed-Abstract-Conference.html": {
    "title": "Robust Mixture Learning when Outliers Overwhelm Small Groups",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniil Dmitriev",
      "Rares-Darius Buhai",
      "Stefan Tiegel",
      "Alexander Wolters",
      "Gleb Novikov",
      "Amartya Sanyal",
      "David Steurer",
      "Fanny Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b57b9aafccf6bf6a76d01079e316e14d-Abstract-Conference.html": {
    "title": "Target-Guided Adversarial Point Cloud Transformer Towards Recognition Against Real-world Corruptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Wang",
      "Tingfa Xu",
      "Lihe Ding",
      "Jianan Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b588d9b67932b459ea66ff6e2804c6b3-Abstract-Conference.html": {
    "title": "On the Complexity of Identification in Linear Structural Causal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Dörfler",
      "Benito van der Zander",
      "Markus Bläser",
      "Maciej Liskiewicz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b589d92785e39486e978fa273d0dc343-Abstract-Conference.html": {
    "title": "Dynamic Model Predictive Shielding for Provably Safe Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arko Banerjee",
      "Kia Rahmani",
      "Joydeep Biswas",
      "Isil Dillig"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b5a412531110b92961fa13c90938806a-Abstract-Conference.html": {
    "title": "A theoretical design of concept sets: improving the predictability of concept bottleneck models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Ruiz Luyten",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b5b939436789f76f08b9d0da5e81af7c-Abstract-Conference.html": {
    "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saleh Ashkboos",
      "Amirkeivan Mohtashami",
      "Maximilian Croci",
      "Bo Li",
      "Pashmina Cameron",
      "Martin Jaggi",
      "Dan Alistarh",
      "Torsten Hoefler",
      "James Hensman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b5cc526f12164b2144bb2e06f2e84864-Abstract-Conference.html": {
    "title": "Global Distortions from Local Rewards: Neural Coding Strategies in Path-Integrating Neural Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francisco Acosta",
      "Fatih Dinc",
      "William Redman",
      "Manu Madhav",
      "David Klindt",
      "Nina Miolane"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b5e5a6c0ab7078e5c21e7c9e46360480-Abstract-Conference.html": {
    "title": "Is O(log N) practical? Near-Equivalence Between Delay Robustness and Bounded Regret in Bandits and RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enoch H. Kang",
      "P. R. Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b5fd95d6b16d3172e307103a97f19e1b-Abstract-Conference.html": {
    "title": "CoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leying Zhang",
      "Yao Qian",
      "Long Zhou",
      "Shujie LIU",
      "Dongmei Wang",
      "Xiaofei Wang",
      "Midia Yousefi",
      "Yanmin Qian",
      "Jinyu Li",
      "Lei He",
      "sheng zhao",
      "Michael Zeng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b6145836acf918800b4b025f842512de-Abstract-Conference.html": {
    "title": "Aligner-Encoders: Self-Attention Transformers Can Be Self-Transducers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Stooke",
      "Rohit Prabhavalkar",
      "Khe Sim",
      "Pedro Moreno Mengibar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b619cd6dcc986856b8a8da2b08d89396-Abstract-Conference.html": {
    "title": "Lever LM: Configuring In-Context Sequence to Lever Large Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Yang",
      "Yingzhe Peng",
      "Haoxuan Ma",
      "Shuo Xu",
      "Chi Zhang",
      "Yucheng Han",
      "Hanwang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b61da4f02b271cb7b5e3d538e2b78fb9-Abstract-Conference.html": {
    "title": "Boosting Transferability and Discriminability for Time Series Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyang Liu",
      "Xinyang Chen",
      "Yang Shu",
      "Xiucheng Li",
      "Weili Guan",
      "Liqiang Nie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b6341525cd84f3be0ef203e4d7cd8556-Abstract-Conference.html": {
    "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomer Porian",
      "Mitchell Wortsman",
      "Jenia Jitsev",
      "Ludwig Schmidt",
      "Yair Carmon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b63a24a1832bd14fa945c71f535c0095-Abstract-Conference.html": {
    "title": "Sample-Efficient Agnostic Boosting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Udaya Ghai",
      "Karan Singh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b63dba40303ae7adb3e2b16e5dd5fd0b-Abstract-Conference.html": {
    "title": "Geometry of naturalistic object representations in recurrent neural network models of working memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxuan Lei",
      "Takuya Ito",
      "Pouya Bashivan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b676cbd80be73a4a7af178f12035a801-Abstract-Conference.html": {
    "title": "Adaptive Proximal Gradient Method for Convex Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yura Malitsky",
      "Konstantin Mishchenko"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b6b4906c1334656e97cc9968ccfca073-Abstract-Conference.html": {
    "title": "HYDRA: Model Factorization Framework for Black-Box LLM Personalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Zhuang",
      "Haotian Sun",
      "Yue Yu",
      "Rushi Qiang",
      "Qifan Wang",
      "Chao Zhang",
      "Bo Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b6d0df730c5976ad918bbf4fb30afe7d-Abstract-Conference.html": {
    "title": "Fine-Tuning Personalization in Federated Learning to Mitigate Adversarial Clients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youssef Allouah",
      "Abdellah El Mrini",
      "Rachid Guerraoui",
      "Nirupam Gupta",
      "Rafael Pinot"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b6e118c759c16f2424997bbb6a1ffd61-Abstract-Conference.html": {
    "title": "MOTE-NAS: Multi-Objective Training-based Estimate for Efficient Neural Architecture Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuming Zhang",
      "Jun Hsieh",
      "Xin Li",
      "Ming-Ching Chang",
      "Chun-Chieh Lee",
      "Kuo-Chin Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b6e271e596574f2b2dfadec6b3ba22a4-Abstract-Conference.html": {
    "title": "Paths to Equilibrium in Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bora Yongacoglu",
      "Gurdal Arslan",
      "Lacra Pavel",
      "Serdar Yuksel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b6e67ae290635d0874c4cb43ba2a2cfb-Abstract-Conference.html": {
    "title": "Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangwon Jang",
      "Jaehyeong Jo",
      "Kimin Lee",
      "Sung Ju Hwang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b6e9d6f4f3428cd5f3f9e9bbae2cab10-Abstract-Conference.html": {
    "title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenkai Yang",
      "Xiaohan Bi",
      "Yankai Lin",
      "Sishuo Chen",
      "Jie Zhou",
      "Xu Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b6edb87876bec4ac2260bffa083cb992-Abstract-Conference.html": {
    "title": "CodeRosetta: Pushing the Boundaries of Unsupervised Code Translation for Parallel Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Tehrani",
      "Arijit Bhattacharjee",
      "Le Chen",
      "Nesreen K. Ahmed",
      "Amir Yazdanbakhsh",
      "Ali Jannesari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b6fa3ed9624c184bd73e435123bd576a-Abstract-Conference.html": {
    "title": "An Information Theoretic Perspective on Conformal Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alvaro Correia",
      "Fabio Valerio Massoli",
      "Christos Louizos",
      "Arash Behboodi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b71cfefae46909178603b5bc6c11d3ae-Abstract-Conference.html": {
    "title": "MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajesh Jayaram",
      "Laxman Dhulipala",
      "Majid Hadian",
      "Jason Lee",
      "Vahab Mirrokni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b7216f4a324864e1f592c18de4d83d10-Abstract-Conference.html": {
    "title": "TAPTRv2: Attention-based Position Update Improves Tracking Any Point",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyang Li",
      "Hao Zhang",
      "Shilong Liu",
      "Zhaoyang Zeng",
      "Feng Li",
      "Bohan Li",
      "Tianhe Ren",
      "Lei Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b734c30b9c955c535e333f0301f5e45c-Abstract-Conference.html": {
    "title": "Voxel Proposal Network via Multi-Frame Knowledge Distillation for Semantic Scene Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lubo Wang",
      "Di Lin",
      "Kairui Yang",
      "Ruonan Liu",
      "Qing Guo",
      "Wuyuan Xie",
      "Miaohui Wang",
      "Lingyu Liang",
      "Yi Wang",
      "Ping Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b736c4b0b38876c9249db9bd900c1a86-Abstract-Conference.html": {
    "title": "Warped Diffusion: Solving Video Inverse Problems with Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giannis Daras",
      "Weili Nie",
      "Karsten Kreis",
      "Alex Dimakis",
      "Morteza Mardani",
      "Nikola Kovachki",
      "Arash Vahdat"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b739cbaa0e94bfd7669d9573e9235411-Abstract-Conference.html": {
    "title": "OpenDlign: Open-World Point Cloud Understanding with Depth-Aligned Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Mao",
      "JUNPENG JING",
      "Krystian Mikolajczyk"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b739d7ae14c0dd4c7619476f3f80ec98-Abstract-Conference.html": {
    "title": "A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guy Bar-Shalom",
      "Yam Eitan",
      "Fabrizio Frasca",
      "Haggai Maron"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b762632135b16f1225672f9fe2a9740b-Abstract-Conference.html": {
    "title": "UniDSeg: Unified Cross-Domain 3D Semantic Segmentation via Visual Foundation Models Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Wu",
      "Mingwei Xing",
      "Yachao Zhang",
      "Xiaotong Luo",
      "Yuan Xie",
      "Yanyun Qu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b76a9959151d377ddd2c77a275a97475-Abstract-Conference.html": {
    "title": "From Causal to Concept-Based Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Goutham Rajendran",
      "Simon Buchholz",
      "Bryon Aragam",
      "Bernhard Schölkopf",
      "Pradeep K. Ravikumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b76bec34ef5e0c0ceedff6edfbefc9f5-Abstract-Conference.html": {
    "title": "Foundations of Multivariate Distributional Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harley Wiltzer",
      "Jesse Farebrother",
      "Arthur Gretton",
      "Mark Rowland"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b782a3462ee9d566291cff148333ea9b-Abstract-Conference.html": {
    "title": "Tell What You Hear From What You See - Video to Audio Generation Through Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiulong Liu",
      "Kun Su",
      "Eli Shlizerman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b7868dedad7192f83c9efb042da43317-Abstract-Conference.html": {
    "title": "SEEV: Synthesis with Efficient Exact Verification for ReLU Neural Barrier Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongchao Zhang",
      "Zhizhen Qin",
      "Sicun Gao",
      "Andrew Clark"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b78adc1c1558bf344809275854d40fd6-Abstract-Conference.html": {
    "title": "Unveiling the Hidden Structure of Self-Attention via Kernel Principal Component Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rachel S.Y. Teo",
      "Tan Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b78c69286221bbce7d12f98b5b45f337-Abstract-Conference.html": {
    "title": "Prototypical Hash Encoding for On-the-Fly Fine-Grained Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Zheng",
      "Nan Pu",
      "Wenjing Li",
      "Nicu Sebe",
      "Zhun Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b7d795e655c1463d7299688d489e8ef4-Abstract-Conference.html": {
    "title": "Multi-Scale Representation Learning for Protein Fitness Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zuobai Zhang",
      "Pascal Notin",
      "Yining Huang",
      "Aurelie C. Lozano",
      "Vijil Chenthamarakshan",
      "Debora Marks",
      "Payel Das",
      "Jian Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b7d9b1d4a9464d5d1ece82198e351349-Abstract-Conference.html": {
    "title": "DiffusionFake: Enhancing Generalization in Deepfake Detection via Guided Stable Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ke sun",
      "Shen Chen",
      "Taiping Yao",
      "Hong Liu",
      "Xiaoshuai Sun",
      "Shouhong Ding",
      "Rongrong Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b7eecb72574b043ad0c69ea296212450-Abstract-Conference.html": {
    "title": "Rethinking Transformer for Long Contextual Histopathology Whole Slide Image Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honglin Li",
      "Yunlong Zhang",
      "Pingyi Chen",
      "Zhongyi Shui",
      "Chenglu Zhu",
      "Lin Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b7f09d26f9b64b5430402860158c2e19-Abstract-Conference.html": {
    "title": "Consensus Learning with Deep Sets for Essential Matrix Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dror Moran",
      "Yuval Margalit",
      "Guy Trostianetsky",
      "Fadi Khatib",
      "Meirav Galun",
      "Ronen Basri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b81a352c156ca123c30c740f147a4496-Abstract-Conference.html": {
    "title": "Any2Graph: Deep End-To-End Supervised Graph Prediction With An Optimal Transport Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Krzakala",
      "Junjie Yang",
      "Rémi Flamary",
      "Florence d'Alché-Buc",
      "Charlotte Laclau",
      "Matthieu Labeau"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b81d83165e3145a2e7d33bb5e33ea913-Abstract-Conference.html": {
    "title": "RadarOcc: Robust 3D Occupancy Prediction with 4D Imaging Radar",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangqiang Ding",
      "Xiangyu Wen",
      "Yunzhou Zhu",
      "Yiming Li",
      "Chris Xiaoxuan Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b83fae17d73b079b1b98ab200276db9f-Abstract-Conference.html": {
    "title": "Neural Experts: Mixture of Experts for Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhak Ben-Shabat",
      "Chamin Hewa Koneputugodage",
      "Sameera Ramasinghe",
      "Stephen Gould"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b84cbefc3bebbb88811e2ead03c9ef0c-Abstract-Conference.html": {
    "title": "Fast Rates in Stochastic Online Convex Optimization by Exploiting the Curvature of Feasible Sets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taira Tsuchiya",
      "Shinji Ito"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b85bbf8b91266a027d21f33eba4e54a6-Abstract-Conference.html": {
    "title": "Strategic Littlestone Dimension: Improved Bounds on Online Strategic Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saba Ahmadi",
      "Kunhe Yang",
      "Hanrui Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b8700a8a005032fe869c741b0a75274b-Abstract-Conference.html": {
    "title": "Transfer Q-star : Principled Decoding for LLM Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Souradip Chakraborty",
      "Soumya Suvra Ghosal",
      "Ming Yin",
      "Dinesh Manocha",
      "Mengdi Wang",
      "Amrit Singh Bedi",
      "Furong Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b87bdcf963cad3d0b265fcb78ae7d11e-Abstract-Conference.html": {
    "title": "Learning to Decouple the Lights for 3D Face Texture Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianxin Huang",
      "Zhenyu Zhang",
      "Ying Tai",
      "Gim Hee Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b88318174aad2cc174a4e05ab6bfad80-Abstract-Conference.html": {
    "title": "MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijie Zhu",
      "Yanzhe Liang",
      "Hanzhi Chang",
      "Jiacheng Deng",
      "Jiahao Lu",
      "Wenfei Yang",
      "Tianzhu Zhang",
      "Yongdong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b8adf038ba1da7a58afa2f88f0f0fb8e-Abstract-Conference.html": {
    "title": "SSDM: Scalable Speech Dysfluency Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Lian",
      "Xuanru Zhou",
      "Zoe Ezzes",
      "Jet Vonk",
      "Brittany Morin",
      "David Paul Baquirin",
      "Zachary Miller",
      "Maria Luisa Gorno Tempini",
      "Gopala Anumanchipalli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b8b34328ae68ccc931265bcd21ae5f16-Abstract-Conference.html": {
    "title": "Neural Signed Distance Function Inference through Splatting 3D Gaussians Pulled on Zero-Level Set",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyuan Zhang",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b8bf2c0dd0b48511889b7d3b2c5fc8f5-Abstract-Conference.html": {
    "title": "LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zecheng Hao",
      "Xinyu Shi",
      "Yujia Liu",
      "Zhaofei Yu",
      "Tiejun Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b8ce770a6b25e603fbff4a37f9e31edc-Abstract-Conference.html": {
    "title": "Preference Learning Algorithms Do Not Learn Preference Rankings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angelica Chen",
      "Sadhika Malladi",
      "Lily Zhang",
      "Xinyi Chen",
      "Qiuyi (Richard) Zhang",
      "Rajesh Ranganath",
      "Kyunghyun Cho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b8f10193cab43d45df9bb810637333fd-Abstract-Conference.html": {
    "title": "Learn To be Efficient: Build Structured Sparsity in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haizhong Zheng",
      "Xiaoyan Bai",
      "Xueshen Liu",
      "Zhuoqing Morley Mao",
      "Beidi Chen",
      "Fan Lai",
      "Atul Prakash"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b8f21f324ff277ba26aed2e944b7576b-Abstract-Conference.html": {
    "title": "MaVEn: An Effective Multi-granularity Hybrid Visual Encoding Framework for Multimodal Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoya Jiang",
      "Hongrui Jia",
      "Haiyang Xu",
      "Wei Ye",
      "Mengfan Dong",
      "Ming Yan",
      "Ji Zhang",
      "Fei Huang",
      "Shikun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b9079234614422c036c5a92bb8ec04c4-Abstract-Conference.html": {
    "title": "CigTime: Corrective Instruction Generation Through Inverse Motion Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Fang",
      "Chengcheng Tang",
      "Bugra Tekin",
      "Yanchao Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b90cb10d4dae058dd167388e76168c1b-Abstract-Conference.html": {
    "title": "Continuous Heatmap Regression for Pose Estimation via Implicit Neural Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengxiang Hu",
      "Huaijiang Sun",
      "Dong Wei",
      "Xiaoning Sun",
      "Jin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b91cebea9292bfc45a2e674f0b9d4a51-Abstract-Conference.html": {
    "title": "Efficient Adaptation of Pre-trained Vision Transformer via Householder Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Dong",
      "Yuan Sun",
      "Yiting Yang",
      "Xing Zhang",
      "Zhijun Lin",
      "Qingsen Yan",
      "Haokui Zhang",
      "Peng Wang",
      "Yang Yang",
      "Hengtao Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b939da3932e88ded5e9b08026e35069d-Abstract-Conference.html": {
    "title": "LoRANN: Low-Rank Matrix Factorization for Approximate Nearest Neighbor Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elias Jääsaari",
      "Ville Hyvönen",
      "Teemu Roos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b93fda2862db7a7ac4a5c412adfb1ac2-Abstract-Conference.html": {
    "title": "Random Cycle Coding: Lossless Compression of Cluster Assignments via Bits-Back Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Severo",
      "Ashish Khisti",
      "Alireza Makhzani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b956d55b4d15eb3f024c67f8415822e4-Abstract-Conference.html": {
    "title": "UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanzhang Zhou",
      "Zijian Feng",
      "Zixiao Zhu",
      "Junlang Qian",
      "Kezhi Mao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b95cb2d3f647dae571203bab285077e7-Abstract-Conference.html": {
    "title": "Structured Multi-Track Accompaniment Arrangement via Style Prior Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwei Zhao",
      "Gus Xia",
      "Ziyu Wang",
      "Ye Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b9603de9e49d0838e53b6c9cf9d06556-Abstract-Conference.html": {
    "title": "TPR: Topology-Preserving Reservoirs for Generalized Zero-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Chen",
      "Yanbin Liu",
      "Yongqiang Ma",
      "Nanning Zheng",
      "Xin Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b96ce7d38339874a8704e8895f743284-Abstract-Conference.html": {
    "title": "On the Scalability of Certified Adversarial Robustness with Generated Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Altstidl",
      "David Dobre",
      "Arthur Kosmala",
      "Bjoern Eskofier",
      "Gauthier Gidel",
      "Leo Schwinn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b97fc02c9e536d68300d82be05c23aa2-Abstract-Conference.html": {
    "title": "No Free Delivery Service: Epistemic limits of passive data collection in complex social systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Nickel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b99a07486702417d3b1bd64ec2cf74ad-Abstract-Conference.html": {
    "title": "Discovery of the Hidden World with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxi Liu",
      "Yongqiang Chen",
      "Tongliang Liu",
      "Mingming Gong",
      "James Cheng",
      "Bo Han",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b99d6cc40b05809c3d84b57a165448cd-Abstract-Conference.html": {
    "title": "Full-Distance Evasion of Pedestrian Detectors in the Physical World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Cheng",
      "Zhanhao Hu",
      "Yuqiu Liu",
      "Jianmin Li",
      "Hang Su",
      "Xiaolin Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b99fd30559b520cc97447ba905040677-Abstract-Conference.html": {
    "title": "Can Models Learn Skill Composition from Examples?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Zhao",
      "Simran Kaur",
      "Dingli Yu",
      "Anirudh Goyal",
      "Sanjeev Arora"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b9a159b81a0535e3944ffdbd07ae3ba1-Abstract-Conference.html": {
    "title": "A Bayesian Approach for Personalized Federated Learning in Heterogeneous Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Disha Makhija",
      "Joydeep Ghosh",
      "Nhat Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b9a17133e3943509243b5e197c1c23b2-Abstract-Conference.html": {
    "title": "A Fast Convoluted Story: Scaling Probabilistic Inference for Integer Arithmetics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lennert De Smet",
      "Pedro Zuidberg Dos Martires"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b9b1d29de259b82c944b03b7322eae45-Abstract-Conference.html": {
    "title": "Unsupervised Object Detection with Theoretical Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marian Longa",
      "João F. Henriques"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b9b228d28770dc2a18922de5cd49f1d9-Abstract-Conference.html": {
    "title": "Atlas3D: Physically Constrained Self-Supporting Text-to-3D for Simulation and Fabrication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunuo Chen",
      "Tianyi Xie",
      "Zeshun Zong",
      "Xuan Li",
      "Feng Gao",
      "Yin Yang",
      "Ying Nian Wu",
      "Chenfanfu Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/b9e88ae0308cf82d0b0f634ddbdf809a-Abstract-Conference.html": {
    "title": "Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damien Ferbach",
      "Quentin Bertrand",
      "Joey Bose",
      "Gauthier Gidel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba0578cd316b89709ac3dafa16311f7c-Abstract-Conference.html": {
    "title": "SA3DIP: Segment Any 3D Instance with Potential 3D Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Yang",
      "Xu Gu",
      "Xingyilang Yin",
      "Xinbo Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba06eb7c3ff7cb731ba04c906d212445-Abstract-Conference.html": {
    "title": "CODA: A Correlation-Oriented Disentanglement and Augmentation Modeling Scheme for Better Resisting Subpopulation Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziquan OU",
      "Zijun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba0ad9d1e0c737800b2340b9cd68c208-Abstract-Conference.html": {
    "title": "One-shot Federated Learning via Synthetic Distiller-Distillate Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyuan Zhang",
      "Songhua Liu",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba1293b663ddd4a29c6854e4d3bf766a-Abstract-Conference.html": {
    "title": "Conditional Outcome Equivalence: A Quantile Alternative to CATE",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josh Givens",
      "Henry Reeve",
      "Song Liu",
      "Katarzyna Reluga"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba1c5356d9164bb64c446a4b690226b0-Abstract-Conference.html": {
    "title": "Generate Universal Adversarial Perturbations for Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiman Hu",
      "Yixiong Zou",
      "Ruixuan Li",
      "Yuhua Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba1d33849b963efc6b5d3082ad68f480-Abstract-Conference.html": {
    "title": "Super Consistency of Neural Network Landscapes and Learning Rate Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Noci",
      "Alexandru Meterez",
      "Thomas Hofmann",
      "Antonio Orvieto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba29e3f830d039c3f1fa0b4dfcf19c54-Abstract-Conference.html": {
    "title": "Integrating Suboptimal Human Knowledge with Hierarchical Reinforcement Learning for Large-Scale Multiagent Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingbang Liu",
      "Shohei Kato",
      "Wen Gu",
      "Fenghui Ren",
      "Jun Yan",
      "Guoxin Su"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba2e53000899e45e6018f639cb7469fa-Abstract-Conference.html": {
    "title": "Distribution-Aware Data Expansion with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haowei Zhu",
      "Ling Yang",
      "Jun-Hai Yong",
      "Hongzhi Yin",
      "Jiawei Jiang",
      "Meng Xiao",
      "Wentao Zhang",
      "Bin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba2f8120cd723c63e6d72b5a2fd16803-Abstract-Conference.html": {
    "title": "Identifiability Guarantees for Causal Disentanglement from Purely Observational Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Welch",
      "Jiaqi Zhang",
      "Caroline Uhler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba5f1233efa77787ff9ec015877dbd1f-Abstract-Conference.html": {
    "title": "Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyang Li",
      "Chen Wei",
      "Shiying Li",
      "Jiachen Zou",
      "Quanying Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba63f9aaba08f39c70ffe19693ef470f-Abstract-Conference.html": {
    "title": "CIFD: Controlled Information Flow to Enhance Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yashas Malur Saidutta",
      "Rakshith Sharma Srinivasa",
      "Jaejin Cho",
      "Ching-Hua Lee",
      "Chouchang Yang",
      "Yilin Shen",
      "Hongxia Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba84da6921f3040b74ee163aa7451f53-Abstract-Conference.html": {
    "title": "Seeing Beyond the Crop: Using Language Priors for Out-of-Bounding Box Keypoint Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bavesh Balaji",
      "Jerrin Bright",
      "Yuhao Chen",
      "Sirisha Rambhatla",
      "John Zelek",
      "David Clausi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba8aee784ffe0813890288b334444eda-Abstract-Conference.html": {
    "title": "LFME: A Simple Framework for Learning from Multiple Experts in Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Chen",
      "Yong Zhang",
      "Yibing Song",
      "Zhiqiang Shen",
      "Lingqiao Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba92705991cfbbcedc26e27e833ebbae-Abstract-Conference.html": {
    "title": "HaloScope: Harnessing Unlabeled LLM Generations for Hallucination Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuefeng Du",
      "Chaowei Xiao",
      "Sharon Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba9e3d60610f3525717665966d86e0cd-Abstract-Conference.html": {
    "title": "CRONOS: Enhancing Deep Learning with Scalable GPU Accelerated Convex Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miria Feng",
      "Zachary Frangella",
      "Mert Pilanci"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ba9e6976157ee25a10c2e9d95102590f-Abstract-Conference.html": {
    "title": "Trading off Consistency and Dimensionality of Convex Surrogates for Multiclass Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enrique Nueve",
      "Dhamma Kimpara",
      "Bo Waggoner",
      "Jessica Finocchiaro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/baa2da9ae4bfed26520bb61d259a3653-Abstract-Conference.html": {
    "title": "VMamba: Visual State Space Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Liu",
      "Yunjie Tian",
      "Yuzhong Zhao",
      "Hongtian Yu",
      "Lingxi Xie",
      "Yaowei Wang",
      "Qixiang Ye",
      "Jianbin Jiao",
      "Yunfan Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bab1486cec466c980b40e7d633dd4bbc-Abstract-Conference.html": {
    "title": "CSPG: Crossing Sparse Proximity Graphs for Approximate Nearest Neighbor Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Yang",
      "Yuzheng Cai",
      "Weiguo Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bac4d92b3f6decfe47eab9a5893dd1f6-Abstract-Conference.html": {
    "title": "Graph-based Unsupervised Disentangled Representation Learning via Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baao Xie",
      "Qiuyu Chen",
      "Yunnan Wang",
      "Zequn Zhang",
      "Xin Jin",
      "Wenjun Zeng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bad233b9849f019aead5e5cc60cef70f-Abstract-Conference.html": {
    "title": "Simplified and Generalized Masked Diffusion for Discrete Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Shi",
      "Kehang Han",
      "Zhe Wang",
      "Arnaud Doucet",
      "Michalis K. Titsias"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bade15648d6c71df84988cfe042be5d8-Abstract-Conference.html": {
    "title": "NeuRodin: A Two-stage Framework for High-Fidelity Neural Surface Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Wang",
      "Di Huang",
      "Weicai Ye",
      "Guofeng Zhang",
      "Wanli Ouyang",
      "Tong He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/baf0fab890edc9dce805d7c518058712-Abstract-Conference.html": {
    "title": "On the Complexity of Learning Sparse Functions with Statistical and Gradient Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nirmit Joshi",
      "Theodor Misiakiewicz",
      "Nati Srebro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/baf583e395665636887b3bda9b5ec7a1-Abstract-Conference.html": {
    "title": "Direct Consistency Optimization for Robust Customization of Text-to-Image Diffusion models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyungmin Lee",
      "Sangkyung Kwak",
      "Kihyuk Sohn",
      "Jinwoo Shin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bb0fea29f7aa6ede17e906ac6a225f34-Abstract-Conference.html": {
    "title": "MoVA: Adapting Mixture of Vision Experts to Multimodal Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZHUOFAN ZONG",
      "Bingqi Ma",
      "Dazhong Shen",
      "Guanglu Song",
      "Hao Shao",
      "DONGZHI JIANG",
      "Hongsheng Li",
      "Yu Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bb11f79ad86f5e33e2a7c850cbdfed42-Abstract-Conference.html": {
    "title": "Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacong Xu",
      "Yiqun Mei",
      "Vishal Patel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bb1e9f32181a8d6a834670d5b3e1c48d-Abstract-Conference.html": {
    "title": "Saliency-driven Experience Replay for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giovanni Bellitto",
      "Federica Proietto Salanitri",
      "Matteo Pennisi",
      "Matteo Boschini",
      "Lorenzo Bonicelli",
      "Angelo Porrello",
      "SIMONE CALDERARA",
      "Simone Palazzo",
      "Concetto Spampinato"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bb3cfcb0284642a973dd631ec9184f2f-Abstract-Conference.html": {
    "title": "Schrodinger Bridge Flow for Unpaired Data Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valentin De Bortoli",
      "Iryna Korshunova",
      "Andriy Mnih",
      "Arnaud Doucet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bb49e1fb9845daf322bb92e0b0ce1c1a-Abstract-Conference.html": {
    "title": "Distributed-Order Fractional Graph Operating Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Zhao",
      "Xuhao Li",
      "Qiyu Kang",
      "Feng Ji",
      "Qinxu Ding",
      "Yanan Zhao",
      "Wenfei Liang",
      "Wee Peng Tay"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bb63841e1ad12370a34504f15c60db4f-Abstract-Conference.html": {
    "title": "Convergence Analysis of Split Federated Learning on Heterogeneous Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengchao Han",
      "Chao Huang",
      "Geng Tian",
      "Ming Tang",
      "Xin Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bb68f698772f14b6d8eaef4529fb9176-Abstract-Conference.html": {
    "title": "From Dictionary to Tensor: A Scalable Multi-View Subspace Clustering Framework with Triple Information Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhibin Gu",
      "Songhe Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bb71b5567ee985e0a4cee54ade19275c-Abstract-Conference.html": {
    "title": "DA-Ada: Learning Domain-Aware Adapter for Domain Adaptive Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochen Li",
      "Rui Zhang",
      "Hantao Yao",
      "Xin Zhang",
      "Yifan Hao",
      "Xinkai Song",
      "Xiaqing Li",
      "Yongwei Zhao",
      "Yunji Chen",
      "Ling Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bb80772833b5e7ce85321a4fb50a66e7-Abstract-Conference.html": {
    "title": "SF-V: Single Forward Video Generation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixing Zhang",
      "Yanyu Li",
      "Yushu Wu",
      "yanwu xu",
      "Anil Kag",
      "Ivan Skorokhodov",
      "Willi Menapace",
      "Aliaksandr Siarohin",
      "Junli Cao",
      "Dimitris Metaxas",
      "Sergey Tulyakov",
      "Jian Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bb97e9a7c811904c9b01f51fde66edcf-Abstract-Conference.html": {
    "title": "Private Attribute Inference from Images with Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Batuhan Tömekçe",
      "Mark Vero",
      "Robin Staab",
      "Martin Vechev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bba9be4bc526c5d515a9d3c16ccfe138-Abstract-Conference.html": {
    "title": "OPERA: Automatic Offline Policy Evaluation with Re-weighted Aggregates of Multiple Estimators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Allen Nie",
      "Yash Chandak",
      "Christina Yuan",
      "Anirudhan Badrinath",
      "Yannis Flet-Berliac",
      "Emma Brunskill"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bbd35a696d85afab1249423dbd6e1041-Abstract-Conference.html": {
    "title": "DiffuBox: Refining 3D Object Detection with Point Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Chen",
      "Zhenzhen Liu",
      "Katie Luo",
      "Siddhartha Datta",
      "Adhitya Polavaram",
      "Yan Wang",
      "Yurong You",
      "Boyi Li",
      "Marco Pavone",
      "Wei-Lun (Harry) Chao",
      "Mark Campbell",
      "Bharath Hariharan",
      "Kilian Q. Weinberger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bbd7d8bd780fcf7143add2317ba04638-Abstract-Conference.html": {
    "title": "Learning to Predict Structural Vibrations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan van Delden",
      "Julius Schultz",
      "Christopher Blech",
      "Sabine Langer",
      "Timo Lüddecke"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bbf9c6e203e2638d756929c1e5bbe85e-Abstract-Conference.html": {
    "title": "Bootstrapping Top-down Information for Self-modulating Slot Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongwon Kim",
      "Seoyeon Kim",
      "Suha Kwak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bc09efb501c801ed92e181e26a885c2d-Abstract-Conference.html": {
    "title": "Reinforcing LLM Agents via Policy Optimization with Action Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muning Wen",
      "Ziyu Wan",
      "Jun Wang",
      "Weinan Zhang",
      "Ying Wen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bc12914d66b41b6bfc2d3a5decdb498b-Abstract-Conference.html": {
    "title": "Cooperative Hardware-Prompt Learning for Snapshot Compressive Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JIAMIAN WANG",
      "Zongliang Wu",
      "Yulun Zhang",
      "Xin Yuan",
      "Tao Lin",
      "Zhiqiang Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bc1c5e5fb8ed1ef9b9b5abced2022e40-Abstract-Conference.html": {
    "title": "A Framework for Bilevel Optimization on Riemannian Manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andi Han",
      "Bamdev Mishra",
      "Pratik Kumar Jawanpuria",
      "Akiko Takeda"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bc427eb348789b190e3ea050cceff8a3-Abstract-Conference.html": {
    "title": "Mixture of Tokens: Continuous MoE through Cross-Example Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Szymon Antoniak",
      "Michał Krutul",
      "Maciej Pióro",
      "Jakub Krajewski",
      "Jan Ludziejewski",
      "Kamil Ciebiera",
      "Krystian Król",
      "Tomasz Odrzygóźdź",
      "Marek Cygan",
      "Sebastian Jaszczur"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bc46e29f91e676747c584ca181cb0ea1-Abstract-Conference.html": {
    "title": "GENOT: Entropic (Gromov) Wasserstein Flow Matching with Applications to Single-Cell Genomics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Klein",
      "Théo Uscidda",
      "Fabian J. Theis",
      "Marco Cuturi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bc46f5d72f432de4c58e1a311bad011b-Abstract-Conference.html": {
    "title": "On the Complexity of Teaching a Family of Linear Behavior Cloning Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubham Bharti",
      "Stephen Wright",
      "Adish Singla",
      "Xiaojin Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bc52716d13d2d72ea0f335667d86c0f8-Abstract-Conference.html": {
    "title": "TrAct: Making First-layer Pre-Activations Trainable",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Petersen",
      "Christian Borgelt",
      "Stefano Ermon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bc667ac84ef58f2b5022da97a465cbab-Abstract-Conference.html": {
    "title": "Improving Visual Prompt Tuning by Gaussian Neighborhood Minimization for Long-Tailed Visual Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengke Li",
      "Ye Liu",
      "Yang Lu",
      "Yiqun Zhang",
      "Yiu-ming Cheung",
      "Hui Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bc75254bc4b8b42f401d0ab5d6e9aa4b-Abstract-Conference.html": {
    "title": "Fine-Grained Dynamic Framework for Bias-Variance Joint Optimization on Data Missing Not at Random",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingming Ha",
      "Taoxuewen",
      "Wenfang Lin",
      "QIONGXU MA",
      "Wujiang Xu",
      "Linxun Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bc75fa9843a7905bbed9d83895a88f7f-Abstract-Conference.html": {
    "title": "Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Ashiqur Rahman",
      "Robert Joseph George",
      "Mogab Elleithy",
      "Daniel Leibovici",
      "Zongyi Li",
      "Boris Bonev",
      "Colin White",
      "Julius Berner",
      "Raymond A. Yeh",
      "Jean Kossaifi",
      "Kamyar Azizzadenesheli",
      "Animashree Anandkumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bc808cf2d2444b0abcceca366b771389-Abstract-Conference.html": {
    "title": "BiScope: AI-generated Text Detection by Checking Memorization of Preceding Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanxi Guo",
      "Siyuan Cheng",
      "Xiaolong Jin",
      "Zhuo Zhang",
      "Kaiyuan Zhang",
      "Guanhong Tao",
      "Guangyu Shen",
      "Xiangyu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bc82dbfbfa43232be85b8d9838f49c3e-Abstract-Conference.html": {
    "title": "Principled Bayesian Optimization in Collaboration with Human Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Xu",
      "Masaki Adachi",
      "Colin Jones",
      "Michael A Osborne"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bc8b2058fd96978a4146f18298cb2d39-Abstract-Conference.html": {
    "title": "SeeA*: Efficient Exploration-Enhanced A* Search by Selective Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dengwei Zhao",
      "Shikui Tu",
      "Lei Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bc8f76d9caadd48f77025b1c889d2e2d-Abstract-Conference.html": {
    "title": "Optimal Flow Matching: Learning Straight Trajectories in Just One Step",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Kornilov",
      "Petr Mokrov",
      "Alexander Gasnikov",
      "Aleksandr Korotin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bca7a9a0dd85e2a68420e5cae27eccfb-Abstract-Conference.html": {
    "title": "Logical characterizations of recurrent graph neural networks with reals and floats",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Veeti Ahvonen",
      "Damian Heiman",
      "Antti Kuusisto",
      "Carsten Lutz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bcad07d4bfab51243efaa08b8ed475b3-Abstract-Conference.html": {
    "title": "In-Context Symmetries: Self-Supervised Learning through Contextual World Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharut Gupta",
      "Chenyu Wang",
      "Yifei Wang",
      "Tommi Jaakkola",
      "Stefanie Jegelka"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bcaebb606df0c8c714756660e4a3bd7c-Abstract-Conference.html": {
    "title": "Efficient Federated Learning against Heterogeneous and Non-stationary Client Unavailability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Xiang",
      "Stratis Ioannidis",
      "Edmund Yeh",
      "Carlee Joe-Wong",
      "Lili Su"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bcbdc25dc4f0be5ae8ac07232df6e33a-Abstract-Conference.html": {
    "title": "RFLPA: A Robust Federated Learning Framework against Poisoning Attacks with Secure Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peihua Mai",
      "Ran Yan",
      "Yan Pang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bccdd196d798a51a4961989984a9ed4a-Abstract-Conference.html": {
    "title": "Algorithmic Capabilities of Random Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqian Zhong",
      "Jacob Andreas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bcd11db0b26d8fc2266b91d3ff982ed1-Abstract-Conference.html": {
    "title": "Markovian Flow Matching: Accelerating MCMC with Continuous Normalizing Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alberto Cabezas",
      "Louis Sharrock",
      "Christopher Nemeth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bce96750dcb64f3257ceabadf0b9c9bf-Abstract-Conference.html": {
    "title": "Distributional Preference Alignment of LLMs via Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Igor Melnyk",
      "Youssef Mroueh",
      "Brian Belgodere",
      "Mattia Rigotti",
      "Apoorva Nitsure",
      "Mikhail Yurochkin",
      "Kristjan Greenewald",
      "Jiri Navratil",
      "Jarret Ross"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bcf26768143c94bd36e363cd4bf5daf0-Abstract-Conference.html": {
    "title": "Probabilistic Decomposed Linear Dynamical Systems for Robust Discovery of Latent Neural Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yenho Chen",
      "Noga Mudrik",
      "Kyle A. Johnsen",
      "Sankaraleengam Alagapan",
      "Adam S Charles",
      "Christopher Rozell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bcfcf7232cb74e1ef82d751880ff835b-Abstract-Conference.html": {
    "title": "Stepwise Alignment for Constrained Language Model Policy Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akifumi Wachi",
      "Thien Tran",
      "Rei Sato",
      "Takumi Tanabe",
      "Youhei Akimoto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bcfdaf04b54a69f47623c973c864ee8d-Abstract-Conference.html": {
    "title": "Lisa: Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiansheng Huang",
      "Sihao Hu",
      "Fatih Ilhan",
      "Selim Tekin",
      "Ling Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bd0194a23b3b0787e54a0173d65b0b37-Abstract-Conference.html": {
    "title": "Towards Estimating Bounds on the Effect of Policies under Unobserved Confounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexis Bellot",
      "Silvia Chiappa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bd03e36b93d52d6abe3d05f9c4ef2770-Abstract-Conference.html": {
    "title": "Carrot and Stick: Eliciting Comparison Data and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiling Chen",
      "Shi Feng",
      "Fang-Yi Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bd18189308a4c45c7d71ca83acf3deaa-Abstract-Conference.html": {
    "title": "Dimension-free deterministic equivalents and scaling laws for random feature regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonardo Defilippis",
      "Bruno Loureiro",
      "Theodor Misiakiewicz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bd19ca8039547b339a6a37bd4df24405-Abstract-Conference.html": {
    "title": "Metric from Human: Zero-shot Monocular Metric Depth Estimation via Test-time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhou Zhao",
      "Hengwei Bian",
      "Kaihua Chen",
      "Pengliang Ji",
      "Liao Qu",
      "Shao-yu Lin",
      "Weichen Yu",
      "Haoran Li",
      "Hao Chen",
      "Jun Shen",
      "Bhiksha Raj",
      "Min Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bd20595c8e5802ba40ed418f4ec116f0-Abstract-Conference.html": {
    "title": "Universal Neural Functionals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Allan Zhou",
      "Chelsea Finn",
      "James Harrison"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bd31bfd4caa85bffe07a35568182cdfa-Abstract-Conference.html": {
    "title": "GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface Reconstruction in Open Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaochao Song",
      "Chong Cheng",
      "Hao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bd592fc74cb0c5089c3e38eb762a793d-Abstract-Conference.html": {
    "title": "LCM: Locally Constrained Compact Point Cloud Model for Masked Point Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaohua Zha",
      "Naiqi Li",
      "Yanzi Wang",
      "Tao Dai",
      "Hang Guo",
      "Bin Chen",
      "Zhi Wang",
      "Zhihao Ouyang",
      "Shu-Tao Xia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bd7ffcbd088b5e12f3b9eecc9c498b27-Abstract-Conference.html": {
    "title": "Disentangled Representation Learning in Non-Markovian Causal Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Li",
      "Yushu Pan",
      "Elias Bareinboim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bd8058b8580eb7f54dbacd8c8c1eb5ce-Abstract-Conference.html": {
    "title": "Generalization Analysis for Label-Specific Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Fan Zhang",
      "Min-Ling Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bd8b52c2fefdb37e3b3953a37408e9dc-Abstract-Conference.html": {
    "title": "GOMAA-Geo: GOal Modality Agnostic Active Geo-localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anindya Sarkar",
      "Srikumar Sastry",
      "Aleksis Pirinen",
      "Chongjie Zhang",
      "Nathan Jacobs",
      "Yevgeniy Vorobeychik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bd92debabb5e6eb881ef81d88e0f22ae-Abstract-Conference.html": {
    "title": "PURE: Prompt Evolution with Graph ODE for Out-of-distribution Fluid Dynamics Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Wu",
      "Changhu Wang",
      "Fan Xu",
      "Jinbao Xue",
      "Chong Chen",
      "Xian-Sheng Hua",
      "Xiao Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bd996108ed57d388866ca6deb7acf6cb-Abstract-Conference.html": {
    "title": "Reconstruction Attacks on Machine Unlearning: Simple Models are Vulnerable",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Bertran",
      "Shuai Tang",
      "Michael J. Kearns",
      "Jamie H Morgenstern",
      "Aaron Roth",
      "Steven Z. Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bda6df40126f791a144ac2bd500679db-Abstract-Conference.html": {
    "title": "Consistency Purification: Effective and Efficient Diffusion Purification towards Certified Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiquan Li",
      "Zhongzhu Chen",
      "Kun Jin",
      "Jiongxiao Wang",
      "Jiachen Lei",
      "Bo Li",
      "Chaowei Xiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bdabb5d4262bcfb6a1d529d690a6c82b-Abstract-Conference.html": {
    "title": "Quantifying Aleatoric Uncertainty of the Treatment Effect: A Novel Orthogonal Learner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valentyn Melnychuk",
      "Stefan Feuerriegel",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bdcdf38389d7fcefc73c4c3720217155-Abstract-Conference.html": {
    "title": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kexue Fu",
      "xiaoyuan luo",
      "Linhao Qu",
      "Shuo Wang",
      "Ying Xiong",
      "Ilias Maglogiannis",
      "Longxiang Gao",
      "Manning Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bdcfa850adac4a1088153881282ca972-Abstract-Conference.html": {
    "title": "Mutual Information Estimation via $f$-Divergence and Data Derangements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nunzio Alexandro Letizia",
      "Nicola Novello",
      "Andrea M Tonello"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bde1c75d7d89e64d6d93dce1c68dc3d8-Abstract-Conference.html": {
    "title": "Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource Programming and Formal Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Mora",
      "Justin Wong",
      "Haley Lepe",
      "Sahil Bhatia",
      "Karim Elmaaroufi",
      "George Varghese",
      "Joseph E Gonzalez",
      "Elizabeth Polgreen",
      "Sanjit Seshia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bdeab378efe6eb289714e2a5abc6ed42-Abstract-Conference.html": {
    "title": "Coarse-to-Fine Concept Bottleneck Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantinos Panousis",
      "Dino Ienco",
      "Diego Marcos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/be0a8ecf8b2743a4117557c5eca0fb79-Abstract-Conference.html": {
    "title": "TAIA: Large Language Models are Out-of-Distribution Data Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyang Jiang",
      "Yusheng Liao",
      "Ya Zhang",
      "Yanfeng Wang",
      "Yu Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/be30024e7fa2c29cac7a6dafcbb8571f-Abstract-Conference.html": {
    "title": "Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Ye",
      "Shansan Gong",
      "Liheng Chen",
      "Lin Zheng",
      "Jiahui Gao",
      "Han Shi",
      "Chuan Wu",
      "Xin Jiang",
      "Zhenguo Li",
      "Wei Bi",
      "Lingpeng Kong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/be36e50757bf9cd280aa74f89a7d1c23-Abstract-Conference.html": {
    "title": "A Polar coordinate system represents syntax in large language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pablo J. Diego Simon",
      "Stéphane d'Ascoli",
      "Emmanuel Chemla",
      "Yair Lakretz",
      "Jean-Remi King"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/be41269a9fe258f1ecba663b0b402322-Abstract-Conference.html": {
    "title": "InterControl: Zero-shot Human Interaction Generation by Controlling Every Joint",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenzhi Wang",
      "Jingbo Wang",
      "Yixuan Li",
      "Dahua Lin",
      "Bo Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/be52acf6bccf4a8c0a90fe2f5cfcead3-Abstract-Conference.html": {
    "title": "Large Language Model Unlearning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanshun Yao",
      "Xiaojun Xu",
      "YangLiu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/be5b3fe26158de0eb2eaa5903385ed53-Abstract-Conference.html": {
    "title": "Toward Approaches to Scalability in 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun-Hui Kim",
      "Seong-Whan Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/be6122d1c7dec017c2bb1b168637f146-Abstract-Conference.html": {
    "title": "Boosting Semi-Supervised Scene Text Recognition via Viewing and Summarizing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yadong Qu",
      "Yuxin Wang",
      "Bangbang Zhou",
      "Zixiao Wang",
      "Hongtao Xie",
      "Yongdong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/be8987a75afeeadfa65007cf7ee25de0-Abstract-Conference.html": {
    "title": "Self-Labeling the Job Shop Scheduling Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Corsini",
      "Angelo Porrello",
      "SIMONE CALDERARA",
      "Mauro Dell&#x27;Amico"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/be99227ef4a4de84bb45d7dc7b53f808-Abstract-Conference.html": {
    "title": "E-Motion: Future Motion Simulation via Event Sequence Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Wu",
      "Zhiyu Zhu",
      "Junhui Hou",
      "GUANGMING Shi",
      "Jinjian Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bec8b667016a73bb195b611aa1f41026-Abstract-Conference.html": {
    "title": "Multi-Reward Best Policy Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessio Russo",
      "Filippo Vannella"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/becd02b89259774da2ede23116a80648-Abstract-Conference.html": {
    "title": "SkipPredict: When to Invest in Predictions for Scheduling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rana Shahout",
      "Michael Mitzenmacher"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bece7e02455a628b770e49fcfa791147-Abstract-Conference.html": {
    "title": "SCOREQ: Speech Quality Assessment with Contrastive Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro Ragano",
      "Jan Skoglund",
      "Andrew Hines"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bee43378b65ec195a67f24709469dcaf-Abstract-Conference.html": {
    "title": "Diffusion Models With Learned Adaptive Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subham Sahoo",
      "Aaron Gokaslan",
      "Christopher M De Sa",
      "Volodymyr Kuleshov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bef7a072148e646fcb62641cc351e599-Abstract-Conference.html": {
    "title": "Fair Bilevel Neural Network (FairBiNN): On Balancing fairness and accuracy via Stackelberg Equilibrium",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehdi Yazdani-Jahromi",
      "Ali Khodabandeh Yalabadi",
      "Amirarsalan Rajabi",
      "Aida Tayebi",
      "Ivan Garibay",
      "OZLEM GARIBAY"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/befcb9fa78d078b6a06cf6f2ecf3a70d-Abstract-Conference.html": {
    "title": "Ask, Attend, Attack: An Effective Decision-Based Black-Box Targeted Attack for Image-to-Text Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyuan Zeng",
      "Zhenzhong Wang",
      "Yiu-ming Cheung",
      "Min JIANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bf2a5ce85aea9ff40d9bf8b2c2561cae-Abstract-Conference.html": {
    "title": "Tight Bounds for Learning RUMs from Small Slates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Flavio Chierichetti",
      "Mirko Giacchini",
      "Ravi Kumar",
      "Alessandro Panconesi",
      "Andrew Tomkins"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bf331c87e29f473b610336f00fe1cb51-Abstract-Conference.html": {
    "title": "On the Computational Landscape of Replicable Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alkis Kalavasis",
      "Amin Karbasi",
      "Grigoris Velegkas",
      "Felix Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bf5311df07f3efce97471921e6d2f159-Abstract-Conference.html": {
    "title": "SILENCE: Protecting privacy in offloaded speech understanding on resource-constrained devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongqi Cai",
      "Shangguang Wang",
      "Zeling Zhang",
      "Felix Xiaozhu Lin",
      "Mengwei Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bf7262e692f3a5c7d676e9e06a1d919a-Abstract-Conference.html": {
    "title": "Frozen-DETR: Enhancing DETR with Image Understanding from Frozen Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghao Fu",
      "Junkai Yan",
      "Qize Yang",
      "Xihan Wei",
      "Xiaohua Xie",
      "Wei-Shi Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bf85879363044ca21f7868a3d1b4021c-Abstract-Conference.html": {
    "title": "Hierarchical Visual Feature Aggregation for OCR-Free Document Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JaeYoo Park",
      "Jin Young Choi",
      "Jeonghyung Park",
      "Bohyung Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bfa629625fd35bf5b880df464b584a57-Abstract-Conference.html": {
    "title": "Asynchronous Perception Machine for Efficient Test Time Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajat Modi",
      "Yogesh Rawat"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bfa7d650ddfd7bd55866ee92f7a3393b-Abstract-Conference.html": {
    "title": "Mixed Dynamics In Linear Networks: Unifying the Lazy and Active Regimes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenfeng Tu",
      "Santiago Tomas Aranguri Diaz",
      "Arthur Jacot"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bfab6c120092a9bf530b6aff18e1436c-Abstract-Conference.html": {
    "title": "Strategic Multi-Armed Bandit Problems Under Debt-Free Reporting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Ben Yahmed",
      "Clément Calauzènes",
      "Vianney Perchet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bfba8efb806a970455b83b852c9cf846-Abstract-Conference.html": {
    "title": "Elo Uncovered: Robustness and Best Practices in Language Model Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meriem Boubdir",
      "Edward Kim",
      "Beyza Ermis",
      "Sara Hooker",
      "Marzieh Fadaee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bfbe51e142f2fa4b26ac60a40cc900f1-Abstract-Conference.html": {
    "title": "Task-Agnostic Machine-Learning-Assisted Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Miao",
      "Qiongshi Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bfdd520867b56af18dc9e80a8235b887-Abstract-Conference.html": {
    "title": "Asymptotics of Alpha-Divergence Variational Inference Algorithms with Exponential Families",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "François Bertholom",
      "randal douc",
      "François Roueff"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bfe167fee3be2d862a56af82dee77720-Abstract-Conference.html": {
    "title": "Rough Transformers: Lightweight and Continuous Time Series Modelling through Signature Patching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fernando Moreno-Pino",
      "Alvaro Arroyo",
      "Harrison Waldon",
      "Xiaowen Dong",
      "Alvaro Cartea"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bfe593e151614c691323c08c0079c1f6-Abstract-Conference.html": {
    "title": "Neuronal Competition Groups with Supervised STDP for Spike-Based Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaspard Goupy",
      "Pierre Tirilly",
      "Ioan Marius Bilasco"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bfe7998398779dde03cad7a73b1f81b6-Abstract-Conference.html": {
    "title": "CycleNet: Enhancing Time Series Forecasting through Modeling Periodic Patterns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengsheng Lin",
      "Weiwei Lin",
      "Xinyi HU",
      "Wentai Wu",
      "Ruichao Mo",
      "Haocheng Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/bff09ce4b210b185a265c9bcd58048bb-Abstract-Conference.html": {
    "title": "UniAR: A Unified model for predicting human Attention and Responses on visual content",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peizhao Li",
      "Junfeng He",
      "Gang Li",
      "Rachit Bhargava",
      "Shaolei Shen",
      "Nachiappan Valliappan",
      "Youwei Liang",
      "Hongxiang Gu",
      "Venky Ramachandran",
      "Golnaz farhadi",
      "Yang Li",
      "Kai Kohlhoff",
      "Vidhya Navalpakkam"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c01dfe01b97aadc10fc16a201faa80d8-Abstract-Conference.html": {
    "title": "Visual Data Diagnosis and Debiasing with Concept Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rwiddhi Chakraborty",
      "Yinong O Wang",
      "Jialu Gao",
      "Runkai Zheng",
      "Cheng Zhang",
      "Fernando D De la Torre"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c023f4ec4c567ad48188e9b2ce6bdba7-Abstract-Conference.html": {
    "title": "Mitigating Biases in Blackbox Feature Extractors for Image Classification Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhipsa Basu",
      "Saswat Subhajyoti Mallick",
      "Venkatesh Babu R"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c04d37be05ba74419d2d5705972a9d64-Abstract-Conference.html": {
    "title": "Normalization and effective learning rates in reinforcement learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clare Lyle",
      "Zeyu Zheng",
      "Khimya Khetarpal",
      "James Martens",
      "Hado P van Hasselt",
      "Razvan Pascanu",
      "Will Dabney"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c07d71ff0bc042e4b9acd626a79597fa-Abstract-Conference.html": {
    "title": "No Filter: Cultural and Socioeconomic Diversity in Contrastive Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angéline Pouget",
      "Lucas Beyer",
      "Emanuele Bugliarello",
      "Xiao Wang",
      "Andreas Steiner",
      "Xiaohua Zhai",
      "Ibrahim M Alabdulmohsin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c09916f6f0c428a97a09a53648e5002e-Abstract-Conference.html": {
    "title": "Learning Optimal Lattice Vector Quantizers for End-to-end Neural Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Zhang",
      "Xiaolin Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c0f7ee1901fef1da4dae2b88dfd43195-Abstract-Conference.html": {
    "title": "Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SIJIA CHEN",
      "Yibo Wang",
      "Yi-Feng Wu",
      "Qingguo Chen",
      "Zhao Xu",
      "Weihua Luo",
      "Kaifu Zhang",
      "Lijun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c0ff9e52e94ae331bc0f2d28be06a9ca-Abstract-Conference.html": {
    "title": "DeMo: Decoupling Motion Forecasting into Directional Intentions and Dynamic States",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bozhou Zhang",
      "Nan Song",
      "Li Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c11daad0a48ea5f3c5c6390c7b060720-Abstract-Conference.html": {
    "title": "Transformers are Minimax Optimal Nonparametric In-Context Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juno Kim",
      "Tai Nakamaki",
      "Taiji Suzuki"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c1233cb30ef11a3fe6ab10aa96beabaf-Abstract-Conference.html": {
    "title": "Robustly overfitting latents for flexible neural image compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yura Perugachi Diaz",
      "Arwin Gansekoele",
      "Sandjai Bhulai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c1366167e310460f2386c36152a897d6-Abstract-Conference.html": {
    "title": "CoSW: Conditional Sample Weighting for Smoke Segmentation with Label Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lujian Yao",
      "Haitao Zhao",
      "Zhongze Wang",
      "Kaijie Zhao",
      "Jingchao Peng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c13cd7feab4beb1a27981e19e2455916-Abstract-Conference.html": {
    "title": "SPEAR: Exact Gradient Inversion of Batches in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitar I. Dimitrov",
      "Maximilian Baader",
      "Mark Müller",
      "Martin Vechev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c13d5a10028586fdc15ee7da97b7563f-Abstract-Conference.html": {
    "title": "Scale Equivariant Graph Metanetworks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ioannis Kalogeropoulos",
      "Giorgos Bouritsas",
      "Yannis Panagakis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c14760740573001c0d18d58879a6a305-Abstract-Conference.html": {
    "title": "GFlowNet Assisted Biological Sequence Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pouya M. Ghari",
      "Alex Tseng",
      "Gokcen Eraslan",
      "Romain Lopez",
      "Tommaso Biancalani",
      "Gabriele Scalia",
      "Ehsan Hajiramezanali"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c153077e44a810cc8728460953af54f1-Abstract-Conference.html": {
    "title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixiang Chen",
      "Huizhuo Yuan",
      "Yongqian Li",
      "Yiwen Kou",
      "Junkai Zhang",
      "Quanquan Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c16a99558b0b4f6b10966ca9bdb98ade-Abstract-Conference.html": {
    "title": "DEPrune: Depth-wise Separable Convolution Pruning for Maximizing GPU Parallelism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheonjun Park",
      "Mincheol Park",
      "Hyunchan Moon",
      "Myung Kuk Yoon",
      "Seokjin Go",
      "Suhyun Kim",
      "Won Woo Ro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c16cb9e89527009a8e95c212f6199102-Abstract-Conference.html": {
    "title": "Online Feature Updates Improve Online (Generalized) Label Shift Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihan Wu",
      "Siddhartha Datta",
      "Yi Su",
      "Dheeraj Baby",
      "Yu-Xiang Wang",
      "Kilian Q. Weinberger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c17fab1bcef325d0d30989c9bf506c0b-Abstract-Conference.html": {
    "title": "Learning from Uncertain Data: From Possible Worlds to Possible Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiongli Zhu",
      "Su Feng",
      "Boris Glavic",
      "Babak Salimi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c182ec594f38926b7fcb827635b9a8f4-Abstract-Conference.html": {
    "title": "Risk-Averse Fine-tuning of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sapana Chaudhary",
      "Ujwal Dinesha",
      "Dileep Kalathil",
      "Srinivas Shakkottai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c196239c5f9481e0db2755f31fe4585f-Abstract-Conference.html": {
    "title": "MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuedong Chen",
      "Chuanxia Zheng",
      "Haofei Xu",
      "Bohan Zhuang",
      "Andrea Vedaldi",
      "Tat-Jen Cham",
      "Jianfei Cai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c1ab28d0fe0bfb53067a1af7e578cd7d-Abstract-Conference.html": {
    "title": "Pessimistic Backward Policy for GFlowNets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyosoon Jang",
      "Yunhui Jang",
      "Minsu Kim",
      "Jinkyoo Park",
      "Sungsoo Ahn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c1c44e46358e0fb94dc94ec495a7fb1a-Abstract-Conference.html": {
    "title": "SlimGPT: Layer-wise Structured Pruning for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gui Ling",
      "Ziyang Wang",
      "YuliangYan",
      "Qingwen Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c1c49aba08e6c90f2b1f85751f497a2f-Abstract-Conference.html": {
    "title": "Self-Calibrating Conformal Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lars van der Laan",
      "Ahmed M. Alaa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c1e1ad233411e25b54bb5df3a0576c2c-Abstract-Conference.html": {
    "title": "Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Hu",
      "Jiayi Lin",
      "Junchi Yan",
      "Shaogang Gong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c1e67cde895c3c91edb43569ad0df260-Abstract-Conference.html": {
    "title": "Trajectory Flow Matching with Applications to Clinical Time Series Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi (Nicole) Zhang",
      "Yuan Pu",
      "Yuki Kawamura",
      "Andrew Loza",
      "Yoshua Bengio",
      "Dennis Shung",
      "Alexander Tong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c1e9db5e1b04322963af91ac0c943568-Abstract-Conference.html": {
    "title": "VFIMamba: Video Frame Interpolation with State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guozhen Zhang",
      "Chuxnu Liu",
      "Yutao Cui",
      "Xiaotong Zhao",
      "Kai Ma",
      "Limin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c1f66abb52467443ba8fc70e0a32e061-Abstract-Conference.html": {
    "title": "Adaptive Preference Scaling for Reinforcement Learning with Human Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilgee Hong",
      "Zichong Li",
      "Alexander Bukharin",
      "Yixiao Li",
      "Haoming Jiang",
      "Tianbao Yang",
      "Tuo Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c212c1b88395ab68d5e1671c17883ec6-Abstract-Conference.html": {
    "title": "Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Braun",
      "Jordan Taylor",
      "Nicholas Goldowsky-Dill",
      "Lee Sharkey"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c2166d01fe4bcd694aba89f608737678-Abstract-Conference.html": {
    "title": "FreeSplat: Generalizable 3D Gaussian Splatting Towards Free View Synthesis of Indoor Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunsong Wang",
      "Tianxin Huang",
      "Hanlin Chen",
      "Gim Hee Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c223aaf2c89379cbde179858d3af1b0d-Abstract-Conference.html": {
    "title": "ManiPose: Manifold-Constrained Multi-Hypothesis 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cédric ROMMEL",
      "Victor Letzelter",
      "Nermin Samet",
      "Renaud Marlet",
      "Matthieu Cord",
      "Patrick Perez",
      "Eduardo Valle"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c225136cfe52a8fd66658bbcf9d894ab-Abstract-Conference.html": {
    "title": "Improving Gloss-free Sign Language Translation by Reducing Representation Density",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhui Ye",
      "Xing Wang",
      "Wenxiang Jiao",
      "Junwei Liang",
      "Hui Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c23ccf9eedf87e4380e92b75b24955bb-Abstract-Conference.html": {
    "title": "GFT: Graph Foundation Model with Transferable Tree Vocabulary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehong Wang",
      "Zheyuan Zhang",
      "Nitesh Chawla",
      "Chuxu Zhang",
      "Yanfang Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c26a8494fe31695db965ae8b7244b7c1-Abstract-Conference.html": {
    "title": "RankUp: Boosting Semi-Supervised Regression with an Auxiliary Ranking Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pin-Yen Huang",
      "Szu-Wei Fu",
      "Yu Tsao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c290d4373c495b2cad0625d6288260f0-Abstract-Conference.html": {
    "title": "UniMTS: Unified Pre-training for Motion Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyuan Zhang",
      "Diyan Teng",
      "Ranak Roy Chowdhury",
      "Shuheng Li",
      "Dezhi Hong",
      "Rajesh Gupta",
      "Jingbo Shang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c2ae6a5b08e85fb96e31ff7dba83d6c1-Abstract-Conference.html": {
    "title": "Remix-DiT: Mixing Diffusion Transformers for Multi-Expert Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gongfan Fang",
      "Xinyin Ma",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c2bd9242609219deb380f161682f4568-Abstract-Conference.html": {
    "title": "Energy-Guided Continuous Entropic Barycenter Estimation for General Costs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Kolesov",
      "Petr Mokrov",
      "Igor Udovichenko",
      "Milena Gazdieva",
      "Gudmund Pammer",
      "Anastasis Kratsios",
      "Evgeny Burnaev",
      "Aleksandr Korotin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c2ce2f2701c10a2b2f2ea0bfa43cfaa3-Abstract-Conference.html": {
    "title": "xLSTM: Extended Long Short-Term Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Beck",
      "Korbinian Pöppel",
      "Markus Spanring",
      "Andreas Auer",
      "Oleksandra Prudnikova",
      "Michael Kopp",
      "Günter Klambauer",
      "Johannes Brandstetter",
      "Sepp Hochreiter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c2d82a425af4c18a35049899fea5ee82-Abstract-Conference.html": {
    "title": "Causal Deciphering and Inpainting in Spatio-Temporal Dynamics via Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Duan",
      "Jian Zhao",
      "pengcheng",
      "Junyuan Mao",
      "Hao Wu",
      "Jingyu Xu",
      "Shilong Wang",
      "Caoyuan Ma",
      "Kai Wang",
      "Kun Wang",
      "Xuelong Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c2e065133af98888ab11a549abed2cc3-Abstract-Conference.html": {
    "title": "Towards Flexible Visual Relationship Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangrui Zhu",
      "Jianwei Yang",
      "Huaizu Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c2f63bec652c85bb335a5ee135f244c9-Abstract-Conference.html": {
    "title": "Leveraging Drift to Improve Sample Complexity of Variance Exploding Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruofeng Yang",
      "Zhijie Wang",
      "Bo Jiang",
      "Shuai Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c3010e98dc44b6f76df7cf82b5e12c77-Abstract-Conference.html": {
    "title": "How Does Black-Box Impact the Learning Guarantee of Stochastic Compositional Optimization?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Chen",
      "Hong Chen",
      "Bin Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c3070c3388552a08a3326f0d28dc2af9-Abstract-Conference.html": {
    "title": "VisMin: Visual Minimal-Change Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rabiul Awal",
      "Saba Ahmadi",
      "LE ZHANG",
      "Aishwarya Agrawal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c3177be226ee12e34d6ba3b5e6fe6a5b-Abstract-Conference.html": {
    "title": "Are Uncertainty Quantification Capabilities of Evidential Deep Learning a Mirage?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maohao Shen",
      "Jongha (Jon) Ryu",
      "Soumya Ghosh",
      "Yuheng Bu",
      "Prasanna Sattigeri",
      "Subhro Das",
      "Gregory Wornell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c32319f4868da7613d78af9993100e42-Abstract-Conference.html": {
    "title": "D-MiSo: Editing Dynamic 3D Scenes using Multi-Gaussians Soup",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joanna Waczynska",
      "Piotr Borycki",
      "Joanna Kaleta",
      "Slawomir Tadeja",
      "Przemysław Spurek"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c3266c14d7eb9715d9fad4306133aa4e-Abstract-Conference.html": {
    "title": "Compositional PAC-Bayes: Generalization of GNNs with persistence and beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kirill Brilliantov",
      "Amauri Souza",
      "Vikas Garg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c326bdcf0db3220897b00db5e05b8592-Abstract-Conference.html": {
    "title": "Fast Channel Simulation via Error-Correcting Codes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharang Sriramu",
      "Rochelle Barsz",
      "Elizabeth Polito",
      "Aaron Wagner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c34057f0cfb685eadf7d8d7cc99f734a-Abstract-Conference.html": {
    "title": "Score-Optimal Diffusion Schedules",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher K. I. Williams",
      "Andrew Campbell",
      "Arnaud Doucet",
      "Saifuddin Syed"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c34ddd05eb089991f06f3c5dc36836e0-Abstract-Conference.html": {
    "title": "YOLOv10: Real-Time End-to-End Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ao Wang",
      "Hui Chen",
      "Lihao Liu",
      "Kai CHEN",
      "Zijia Lin",
      "Jungong Han",
      "guiguang ding"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c35986bc1ee29b31c1011481b77fe540-Abstract-Conference.html": {
    "title": "Transformers Can Do Arithmetic with the Right Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sean McLeish",
      "Arpit Bansal",
      "Alex Stein",
      "Neel Jain",
      "John Kirchenbauer",
      "Brian Bartoldson",
      "Bhavya Kailkhura",
      "Abhinav Bhatele",
      "Jonas Geiping",
      "Avi Schwarzschild",
      "Tom Goldstein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c373d1939756109b26308c8d84753747-Abstract-Conference.html": {
    "title": "Accelerating Nash Equilibrium Convergence in Monte Carlo Settings Through Counterfactual Value Based Fictitious Play",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Ju",
      "Falin Hei",
      "Ting Feng",
      "Dengbing Yi",
      "Zhemei Fang",
      "YunFeng Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c3909e3abe8ebdb20c42a42ce0bc907d-Abstract-Conference.html": {
    "title": "Hierarchical Hybrid Sliced Wasserstein: A Scalable Metric for Heterogeneous Joint Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khai Nguyen",
      "Nhat Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c3b3c5f9297ad0012cfe7188c34cea0e-Abstract-Conference.html": {
    "title": "Optimistic Critic Reconstruction and Constrained Fine-Tuning for General Offline-to-Online RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qin-Wen Luo",
      "Ming-Kun Xie",
      "Yewen Wang",
      "Sheng-Jun Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c3ec89aed795f9deeff3f1390c3bd882-Abstract-Conference.html": {
    "title": "Vision-Language Navigation with Energy-Based Policy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Liu",
      "Wenguan Wang",
      "Yi Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c3fa3a7d50b34732c6d08f6f66380d75-Abstract-Conference.html": {
    "title": "The Limits of Transfer Reinforcement Learning with Latent Low-rank Structure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tyler Sam",
      "Yudong Chen",
      "Christina Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c3fe2a07ec47b89c50e89706d2e23358-Abstract-Conference.html": {
    "title": "Deep Learning in Medical Image Registration: Magic or Mirage?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohit Jena",
      "Deeksha Sethi",
      "Pratik Chaudhari",
      "James Gee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c4006ff54a7bbda74c09bad6f7586f5b-Abstract-Conference.html": {
    "title": "In-and-Out: Algorithmic Diffusion for Sampling Convex Bodies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunbum Kook",
      "Santosh Vempala",
      "Matthew Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c402501846f9fe03e2cac015b3f0e6b1-Abstract-Conference.html": {
    "title": "Memorize What Matters: Emergent Scene Decomposition from Multitraverse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Li",
      "Zehong Wang",
      "Yue Wang",
      "Zhiding Yu",
      "Zan Gojcic",
      "Marco Pavone",
      "Chen Feng",
      "Jose M. Alvarez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c40daf14d7a6469e65116507c21faeb7-Abstract-Conference.html": {
    "title": "Retrieval & Fine-Tuning for In-Context Tabular Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valentin Thomas",
      "Junwei Ma",
      "Rasa Hosseinzadeh",
      "Keyvan Golestan",
      "Guangwei Yu",
      "Maks Volkovs",
      "Anthony L Caterini"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c43bade4b146231b0149da3b9e7e286e-Abstract-Conference.html": {
    "title": "The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Kalinke",
      "Zoltan Szabo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c44c4afd77d5ee760e7f4bed0c50f878-Abstract-Conference.html": {
    "title": "DDN: Dual-domain Dynamic Normalization for Non-stationary Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Dai",
      "Beiliang Wu",
      "Peiyuan Liu",
      "Naiqi Li",
      "Xue Yuerong",
      "Shu-Tao Xia",
      "Zexuan Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c4546b4f9e1a44ed15c253dd43307dd5-Abstract-Conference.html": {
    "title": "Active Set Ordering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quoc Phong Nguyen",
      "Sunil Gupta",
      "Svetha Venkatesh",
      "Bryan Kian Hsiang Low",
      "Patrick Jaillet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c455e799c485252c147fcf81e3aee2b1-Abstract-Conference.html": {
    "title": "Learning Elastic Costs to Shape Monge Displacements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Klein",
      "Aram-Alexandre Pooladian",
      "Pierre Ablin",
      "Eugene Ndiaye",
      "Jonathan Niles-Weed",
      "Marco Cuturi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c46c759679acea07d7ea92823ea1e290-Abstract-Conference.html": {
    "title": "Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinbo Bai",
      "Washim Mondal",
      "Vaneet Aggarwal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c473b9c8897f50203fa23570687c6b30-Abstract-Conference.html": {
    "title": "Neural decoding from stereotactic EEG: accounting for electrode variability across subjects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Mentzelopoulos",
      "Evangelos Chatzipantazis",
      "Ashwin Ramayya",
      "Michelle Hedlund",
      "Vivek Buch",
      "Kostas Daniilidis",
      "Konrad Kording",
      "Flavia Vitale"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c49a28241640407b23bba8f2495f4bc9-Abstract-Conference.html": {
    "title": "CRT-Fusion: Camera, Radar, Temporal Fusion Using Motion Information for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jisong Kim",
      "Minjae Seong",
      "Jun Won Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c4a16c7ba098740ea734e9a270947d6e-Abstract-Conference.html": {
    "title": "Transformation-Invariant Learning and Theoretical Guarantees for OOD Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omar Montasser",
      "Han Shao",
      "Emmanuel Abbe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c4b652b7e228b18e1c65478da3a4a2cf-Abstract-Conference.html": {
    "title": "BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Zhang",
      "Bingxin Ke",
      "Hayko Riemenschneider",
      "Nando Metzger",
      "Anton Obukhov",
      "Markus Gross",
      "Konrad Schindler",
      "Christopher Schroers"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c4bb407aa5042727317f9856dace001c-Abstract-Conference.html": {
    "title": "Temporal-Difference Learning Using Distributed Error Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Guan",
      "Shon Verch",
      "Claas Voelcker",
      "Ethan Jackson",
      "Nicolas Papernot",
      "William Cunningham"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c4bec0d2fd217e6c2c3eafeced432582-Abstract-Conference.html": {
    "title": "MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichun Yu",
      "Spandan Das",
      "Chenyan Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c4bf73386022473a652a18941e9ea6f8-Abstract-Conference.html": {
    "title": "Conformal Classification with Equalized Coverage for Adaptively Selected Groups",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanfei Zhou",
      "Matteo Sesia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c4dbadd63360f0b9a79b1c541995b6cd-Abstract-Conference.html": {
    "title": "GenRec: Unifying Video Generation and Recognition with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zejia Weng",
      "Xitong Yang",
      "Zhen Xing",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c4e380fb74dec9da9c7212e834657aa9-Abstract-Conference.html": {
    "title": "Rule Based Rewards for Language Model Safety",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Mu",
      "Alec Helyar",
      "Johannes Heidecke",
      "Joshua Achiam",
      "Andrea Vallone",
      "Ian Kivlichan",
      "Molly Lin",
      "Alex Beutel",
      "John Schulman",
      "Lilian Weng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c4e40d310af1be55f2c0f32cb16a9a16-Abstract-Conference.html": {
    "title": "Discretely beyond $1/e$: Guided Combinatorial Algortihms for Submodular Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Chen",
      "Ankur Nath",
      "Chunli Peng",
      "Alan Kuhnle"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c4edc5113b4ffd4632718558fb66b9ef-Abstract-Conference.html": {
    "title": "Understanding and Improving Training-free Loss-based Diffusion Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Shen",
      "XINYANG JIANG",
      "Yifan Yang",
      "Yezhen Wang",
      "Dongqi Han",
      "Dongsheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c4f3846c9a230cfd3373ff419b202bd5-Abstract-Conference.html": {
    "title": "Nearly Minimax Optimal Regret for Multinomial Logistic Bandit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joongkyu Lee",
      "Min-hwan Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c50e3c72bf45a361afc7c16d26c21a1a-Abstract-Conference.html": {
    "title": "Partial Structure Discovery is Sufficient for No-regret Learning in Causal Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Qasim Elahi",
      "Mahsa Ghasemi",
      "Murat Kocaoglu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c50f8180ef34060ec59b75d6e1220f7a-Abstract-Conference.html": {
    "title": "Iteration Head: A Mechanistic Study of Chain-of-Thought",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivien Cabannes",
      "Charles Arnal",
      "Wassim Bouaziz",
      "Xingyu Yang",
      "Francois Charton",
      "Julia Kempe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c5433ab4056ca58db67be4578c384cba-Abstract-Conference.html": {
    "title": "Causal Inference in the Closed-Loop: Marginal Structural Models for Sequential Excursion Effects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Levis",
      "Gabriel Loewinger",
      "Francisco Pereira"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c554c1305b8f4f993db4738a9c633d14-Abstract-Conference.html": {
    "title": "Rethinking Out-of-Distribution Detection on Imbalanced Data Distribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Liu",
      "Zhihang Fu",
      "Sheng Jin",
      "Chao Chen",
      "Ze Chen",
      "Rongxin Jiang",
      "Fan Zhou",
      "Yaowu Chen",
      "Jieping Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c556da88a2665e6266453d8c9b8a552d-Abstract-Conference.html": {
    "title": "Mitigating Covariate Shift in Behavioral Cloning via Robust Stationary Distribution Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seokin Seo",
      "Byung-Jun Lee",
      "Jongmin Lee",
      "HyeongJoo Hwang",
      "Hongseok Yang",
      "Kee-Eung Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c55aeaa3a85954615319a7e55c81197a-Abstract-Conference.html": {
    "title": "DeBaRA: Denoising-Based 3D Room Arrangement Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Léopold Maillard",
      "Nicolas Sereyjol-Garros",
      "Tom Durand",
      "Maks Ovsjanikov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c55e6792923cc16fd6ed5c3f672420a5-Abstract-Conference.html": {
    "title": "Optimal ablation for interpretability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Li",
      "Lucas Janson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c565514eb8cf8808d82a612c5f7a1920-Abstract-Conference.html": {
    "title": "Bridging Geometric States via Geometric Diffusion Bridge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjie Luo",
      "Yixian Xu",
      "Di He",
      "Shuxin Zheng",
      "Tie-Yan Liu",
      "Liwei Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c56b929f3305eb268bcf2328c3133dd4-Abstract-Conference.html": {
    "title": "Visual Pinwheel Centers Act as Geometric Saliency Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haixin Zhong",
      "Mingyi Huang",
      "Wei Dai",
      "Haoyu Wang",
      "Anna Roe",
      "Yuguo Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c573258c38d0a3919d8c1364053c45df-Abstract-Conference.html": {
    "title": "Adaptive Layer Sparsity for Large Language Models via Activation Correlation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Li",
      "Lujun Li",
      "Mark Lee",
      "Shengjie Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c58437945392cec01e0c75ff6cef901a-Abstract-Conference.html": {
    "title": "Virtual Scanning: Unsupervised Non-line-of-sight Imaging from Irregularly Undersampled Transients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Cui",
      "Huanjing Yue",
      "Song Li",
      "Xiangjun Yin",
      "Yusen Hou",
      "Yun Meng",
      "Kai Zou",
      "Xiaolong Hu",
      "Jingyu Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c585301e1c4b7c50039de5413ef585b5-Abstract-Conference.html": {
    "title": "Learning to Edit Visual Programs with Self-Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "R. Kenny Jones",
      "Renhao Zhang",
      "Aditya Ganeshan",
      "Daniel Ritchie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c592fc7e6207f82560ed45fece8d6937-Abstract-Conference.html": {
    "title": "DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weikang Wan",
      "Ziyu Wang",
      "Yufei Wang",
      "Zackory Erickson",
      "David Held"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c59f05d7ab3638b138cc61f32e1a7cd1-Abstract-Conference.html": {
    "title": "Amortized Bayesian Experimental Design for Decision-Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daolang Huang",
      "Yujia Guo",
      "Luigi Acerbi",
      "Samuel Kaski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c5cf13bfd3762821ef7607e63ee90075-Abstract-Conference.html": {
    "title": "xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Cheng",
      "Xun Wang",
      "Xingxing Zhang",
      "Tao Ge",
      "Si-Qing Chen",
      "Furu Wei",
      "Huishuai Zhang",
      "Dongyan Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c5e2ea485874f398c75594257ea9d083-Abstract-Conference.html": {
    "title": "Approximately Pareto-optimal Solutions for Bi-Objective k-Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Arutyunova",
      "Jan Eube",
      "Heiko Röglin",
      "Melanie Schmidt",
      "Sarah Sturm",
      "Julian Wargalla"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c5e7867ce0baf950b53f0879af369599-Abstract-Conference.html": {
    "title": "Customized Subgraph Selection and Encoding for Drug-drug Interaction Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotong Du",
      "Quanming Yao",
      "Juzheng Zhang",
      "Yang Liu",
      "Zhen Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c5ec22711f3a4a2f4a0a8ffd92167190-Abstract-Conference.html": {
    "title": "LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Requeima",
      "John Bronskill",
      "Dami Choi",
      "Richard Turner",
      "David K. Duvenaud"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c615f9e5ad3b90ad58af2cc529bc1528-Abstract-Conference.html": {
    "title": "Generalizing Consistency Policy to Visual RL with Prioritized Proximal Experience Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Li",
      "Zhennan Jiang",
      "YUHUI CHEN",
      "Dongbin Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c63908a3e946af0e7978c23737229137-Abstract-Conference.html": {
    "title": "Elliptical Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Nielsen",
      "Laziz Abdullaev",
      "Rachel S.Y. Teo",
      "Tan Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c6483c8a68083af3383f91ee0dc6db95-Abstract-Conference.html": {
    "title": "Evaluation of Text-to-Video Generation Models: A Dynamics Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingxiang Liao",
      "hannan lu",
      "Qixiang Ye",
      "Wangmeng Zuo",
      "Fang Wan",
      "Tianyu Wang",
      "Yuzhong Zhao",
      "Jingdong Wang",
      "Xinyu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c6776239514e90cbaa56b575a9f9e05c-Abstract-Conference.html": {
    "title": "Nonconvex Federated Learning on Compact Smooth Submanifolds With Heterogeneous Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaojiao Zhang",
      "Jiang Hu",
      "Anthony Man-Cho So",
      "Mikael Johansson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c693c3ff83259aebcd55a41ab19a5d84-Abstract-Conference.html": {
    "title": "Estimating Epistemic and Aleatoric Uncertainty with a Single Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Chan",
      "Maria Molina",
      "Chris Metzler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c69465280855cfe25d566e359da140c1-Abstract-Conference.html": {
    "title": "LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zefan Qu",
      "Ke Xu",
      "Gerhard Hancke",
      "Rynson Lau"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c6a23b26eaaefd187973658f5001f4fe-Abstract-Conference.html": {
    "title": "IQA-EVAL: Automatic Evaluation of Human-Model Interactive Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruosen Li",
      "Ruochen Li",
      "Barry Wang",
      "Xinya Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c6a79e139ec4f371701ea8cc9e06018e-Abstract-Conference.html": {
    "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiwei Wu",
      "Joya Chen",
      "Kevin Qinghong Lin",
      "Qimeng Wang",
      "Yan Gao",
      "Qianli Xu",
      "Tong Xu",
      "Yao Hu",
      "Enhong Chen",
      "Mike Zheng Shou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c6b2921f24f82dc05cda53bbe50bab90-Abstract-Conference.html": {
    "title": "Finding good policies in average-reward Markov Decision Processes without prior knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrienne Tuynman",
      "Rémy Degenne",
      "Emilie Kaufmann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c6b96bf6380bd81bbfbce5db0e054a41-Abstract-Conference.html": {
    "title": "Con4m: Context-aware Consistency Learning Framework for Segmented Time Series Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junru Chen",
      "Tianyu Cao",
      "Jing Xu",
      "Jiahe Li",
      "Zhilong Chen",
      "Tao Xiao",
      "YANG YANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c6d8301d2235036e1d23081252158a79-Abstract-Conference.html": {
    "title": "Diffusion Spectral Representation for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitry Shribak",
      "Chen-Xiao Gao",
      "Yitong Li",
      "Chenjun Xiao",
      "Bo Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c6e03e056d9ae8e852ec3100dbe75357-Abstract-Conference.html": {
    "title": "Plant-and-Steal: Truthful Fair Allocations via Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilan Cohen",
      "Alon Eden",
      "Talya Eden",
      "Arsen Vasilyan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c6e31f86c1eb8dfc05190cf15ed52064-Abstract-Conference.html": {
    "title": "Robust Graph Neural Networks via Unbiased Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhichao Hou",
      "Ruiqi Feng",
      "Tyler Derr",
      "Xiaorui Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c6f83c27a2223d817f9f1ade48d281a2-Abstract-Conference.html": {
    "title": "Semantics and Spatiality of Emergent Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rotem Ben Zion",
      "Boaz Carmeli",
      "Orr Paradise",
      "Yonatan Belinkov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c6fdc94aeb2cb3a426d510d970045dab-Abstract-Conference.html": {
    "title": "3D Structure Prediction of Atomic Systems with Flow-based Direct Preference Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Jiao",
      "Xiangzhe Kong",
      "Wenbing Huang",
      "Yang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c700d47679e325215a3edb323b63f069-Abstract-Conference.html": {
    "title": "Knowledge Graph Completion by Intermediate Variables Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changyi Xiao",
      "Yixin Cao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c7038ec7eee8dc5a99d74d594f70aa3f-Abstract-Conference.html": {
    "title": "Scaling laws for learning with real and surrogate data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayush Jain",
      "Andrea Montanari",
      "Eren Sasoglu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c705ba25f183b875c9359ef83fa262e8-Abstract-Conference.html": {
    "title": "Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaihang Pan",
      "Zhaoyu Fan",
      "Juncheng Li",
      "Qifan Yu",
      "Hao Fei",
      "Siliang Tang",
      "Richang Hong",
      "Hanwang Zhang",
      "QIANRU SUN"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c7138635035501eb71b0adf6ddc319d6-Abstract-Conference.html": {
    "title": "StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yupeng Zhou",
      "Daquan Zhou",
      "Ming-Ming Cheng",
      "Jiashi Feng",
      "Qibin Hou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c71769e2715835d37c3e25cc1173bd62-Abstract-Conference.html": {
    "title": "Reward Machines for Deep RL in Noisy and Uncertain Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Li",
      "Zizhao Chen",
      "Toryn Klassen",
      "Pashootan Vaezipoor",
      "Rodrigo Toro Icarte",
      "Sheila McIlraith"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c7201deff8d507a8fe2e86d34094e154-Abstract-Conference.html": {
    "title": "The motion planning neural circuit in goal-directed navigation as Lie group operator search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfeng Zuo",
      "Ying Nian Wu",
      "Si Wu",
      "Wenhao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c72861451d6fa9dfa64831102b9bb71a-Abstract-Conference.html": {
    "title": "Trajectory Diffusion for ObjectGoal Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyao Yu",
      "Sixian Zhang",
      "Xinhang Song",
      "Xiaorong Qin",
      "Shuqiang Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c73052b864497e078db340c9f6fa4ab5-Abstract-Conference.html": {
    "title": "Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junha Hyung",
      "Susung Hong",
      "Sungwon Hwang",
      "Jaeseong Lee",
      "Jaegul Choo",
      "Jin-Hwa Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c7313c42bc9536889fc0af617c384fa6-Abstract-Conference.html": {
    "title": "Generating Origin-Destination Matrices in Neural Spatial Interaction Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ioannis Zachos",
      "Mark Girolami",
      "Theodoros Damoulas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c735853cfb9003002db68cdacb1b6e1b-Abstract-Conference.html": {
    "title": "Hamiltonian Score Matching and Generative Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Holderrieth",
      "Yilun Xu",
      "Tommi Jaakkola"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c745bfa5b50544882938ff4f89ff26ac-Abstract-Conference.html": {
    "title": "RoPINN: Region Optimized Physics-Informed Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haixu Wu",
      "Huakun Luo",
      "Yuezhou Ma",
      "Jianmin Wang",
      "Mingsheng Long"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c7649eeb93d2fad0ced9a3b974260710-Abstract-Conference.html": {
    "title": "NeuralClothSim: Neural Deformation Fields Meet the Thin Shell Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Navami Kairanda",
      "Marc Habermann",
      "Christian Theobalt",
      "Vladislav Golyanik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c7731511dd2a2931f767e6e51317f322-Abstract-Conference.html": {
    "title": "Skill-aware Mutual Information Optimisation for Zero-shot Generalisation in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuehui Yu",
      "Mhairi Dunion",
      "Xin Li",
      "Stefano Albrecht"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c78d9d035215039ab46e48c281e6a63d-Abstract-Conference.html": {
    "title": "Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengtao Jian",
      "Kai Yang",
      "Yang Jiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c78f639424b8d89ceb4f2efbb4dfe4f4-Abstract-Conference.html": {
    "title": "RestoreAgent: Autonomous Image Restoration Agent via Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Chen",
      "Wenbo Li",
      "JINJIN GU",
      "Jingjing Ren",
      "Sixiang Chen",
      "Tian Ye",
      "Renjing Pei",
      "Kaiwen Zhou",
      "Fenglong Song",
      "Lei Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c793577b644268259b1416464a6cdb8c-Abstract-Conference.html": {
    "title": "How Molecules Impact Cells: Unlocking Contrastive PhenoMolecular Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philip Fradkin",
      "Puria Azadi Moghadam",
      "Karush Suri",
      "Frederik Wenkel",
      "Ali Bashashati",
      "Maciej Sypetkowski",
      "Dominique Beaini"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c79625091a4f8b5d3abe29f3b14fa43a-Abstract-Conference.html": {
    "title": "Aligning Model Properties via Conformal Risk Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Overman",
      "Jacqueline Vallon",
      "Mohsen Bayati"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c7b04e4e13bb77996d3ae2ff667231ac-Abstract-Conference.html": {
    "title": "GAVEL: Generating Games via Evolution and Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Graham Todd",
      "Alexander G Padula",
      "Matthew Stephenson",
      "Eric Piette",
      "Dennis Soemers",
      "Julian Togelius"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c7c7cf10082e454b9662a686ce6f1b6f-Abstract-Conference.html": {
    "title": "SegVol: Universal and Interactive Volumetric Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Du",
      "Fan BAI",
      "Tiejun Huang",
      "Bo Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c7f43ada17acc234f568dc66da527418-Abstract-Conference.html": {
    "title": "Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zebang Cheng",
      "Zhi-Qi Cheng",
      "Jun-Yan He",
      "Kai Wang",
      "Yuxiang Lin",
      "Zheng Lian",
      "Xiaojiang Peng",
      "Alexander Hauptmann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c7f4dbb8f3739b36029ba71a47844696-Abstract-Conference.html": {
    "title": "Diffusion4D: Fast Spatial-temporal Consistent 4D generation via Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HANWEN LIANG",
      "Yuyang Yin",
      "Dejia Xu",
      "hanxue liang",
      "Zhangyang &quot;Atlas&quot; Wang",
      "Konstantinos N Plataniotis",
      "Yao Zhao",
      "Yunchao Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c7f795dc3b4eb6ae630695d90001a2f8-Abstract-Conference.html": {
    "title": "Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sukjun Hwang",
      "Aakash Sunil Lahoti",
      "Ratish Puduppully",
      "Tri Dao",
      "Albert Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c81d09a6a79c69354331cdd09e4d5a4e-Abstract-Conference.html": {
    "title": "e-COP : Episodic Constrained Optimization of Policies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akhil Agnihotri",
      "Rahul Jain",
      "Deepak Ramachandran",
      "Sahil Singla"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c848b7d3adc08fcd0bf1df3101ba6728-Abstract-Conference.html": {
    "title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Zhai",
      "Hao Bai",
      "Zipeng Lin",
      "Jiayi Pan",
      "Peter Tong",
      "Yifei Zhou",
      "Alane Suhr",
      "Saining Xie",
      "Yann LeCun",
      "Yi Ma",
      "Sergey Levine"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c84dace5a79d5f1ccc77b26c496f0ce9-Abstract-Conference.html": {
    "title": "RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changli Wu",
      "Qi Chen",
      "Jiayi Ji",
      "Haowei Wang",
      "Yiwei Ma",
      "You Huang",
      "Gen Luo",
      "Hao Fei",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c859b99b5d717c9035e79d43dfd69435-Abstract-Conference.html": {
    "title": "Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhao Zhai",
      "Kevin Lin",
      "Zhengyuan Yang",
      "Linjie Li",
      "Jianfeng Wang",
      "Chung-Ching Lin",
      "DAVID DOERMANN",
      "Junsong Yuan",
      "Lijuan Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c897d8f3be030344949de9bd93d8274e-Abstract-Conference.html": {
    "title": "Motion Graph Unleashed: A Novel Approach to Video Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqi Zhong",
      "Luming Liang",
      "Bohan Tang",
      "Ilya Zharkov",
      "Ulrich Neumann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c8b100b376a7b338c84801b699935098-Abstract-Conference.html": {
    "title": "Hierarchical Selective Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shani Goren",
      "Ido Galil",
      "Ran El-Yaniv"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c8b2f897e45770595656a79a9ad91e89-Abstract-Conference.html": {
    "title": "Reranking Laws for Language Generation: A Communication-Theoretic Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "António Farinhas",
      "Haau-Sing Li",
      "André Martins"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c9028f7874df04843e7bf435ee4cd3c3-Abstract-Conference.html": {
    "title": "UltraPixel: Advancing Ultra High-Resolution Image Synthesis to New Peaks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingjing Ren",
      "Wenbo Li",
      "Haoyu Chen",
      "Renjing Pei",
      "Bin Shao",
      "Yong Guo",
      "Long Peng",
      "Fenglong Song",
      "Lei Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c91b6f7e0152b7a95ee777e987fe811e-Abstract-Conference.html": {
    "title": "FlexCap: Describe Anything in Images in Controllable Detail",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debidatta Dwibedi",
      "Vidhi Jain",
      "Jonathan J Tompson",
      "Andrew Zisserman",
      "Yusuf Aytar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c959bb2cb164d37569a17fa67494d69a-Abstract-Conference.html": {
    "title": "Enhancing Semi-Supervised Learning via Representative and Diverse Sample Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Shao",
      "Jiangrui Kang",
      "Qiyuan Chen",
      "Zepeng Li",
      "Hongxia Xu",
      "Yiwen Cao",
      "JIAJUAN LIANG",
      "Jian Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c97cc8d9005fdd3ad090afa6d1a5025c-Abstract-Conference.html": {
    "title": "DiffSF: Diffusion Models for Scene Flow Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushan Zhang",
      "Bastian Wandt",
      "Maria Magnusson",
      "Michael Felsberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c980ad0fb46f0cfb0faabcd42b30a67a-Abstract-Conference.html": {
    "title": "Random Function Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Benning",
      "Leif Döring"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c981fd12b1d5703f19bd8289da9fc996-Abstract-Conference.html": {
    "title": "Proportional Fairness in Clustering: A Social Choice Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leon Kellerhals",
      "Jannik Peters"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c9850f81fb8b3a198ffb13b80da64e3f-Abstract-Conference.html": {
    "title": "Unveiling the Hidden: Online Vectorized HD Map Construction with Clip-Level Token Interaction and Propagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nayeon Kim",
      "Hongje Seong",
      "Daehyun Ji",
      "Sujin Jang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c98987c5ec4f30920d7190dc699e3daf-Abstract-Conference.html": {
    "title": "GO4Align: Group Optimization for Multi-Task Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Shen",
      "Qi Wang",
      "Zehao Xiao",
      "Nanne van Noord",
      "Marcel Worring"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c9b551a2e195a209fc0b280de2f7f781-Abstract-Conference.html": {
    "title": "Toward a Stable, Fair, and Comprehensive Evaluation of Object Hallucination in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongliang Wei",
      "Xingtao Wang",
      "Xianqi Zhang",
      "Xiaopeng Fan",
      "Debin Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c9da56addea9c977cf4ba873e1da979d-Abstract-Conference.html": {
    "title": "Operator World Models for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pietro Novelli",
      "Marco Pratticò",
      "Massimiliano Pontil",
      "Carlo Ciliberto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c9e20f70f049ac5be8955c8bb970d0a5-Abstract-Conference.html": {
    "title": "Non-Euclidean Mixture Model for Social Network Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roshni Iyer",
      "Yewen Wang",
      "Wei Wang",
      "Yizhou Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c9fcd02e6445c7dfbad6986abee53d0d-Abstract-Conference.html": {
    "title": "Stratified Prediction-Powered Inference for Effective Hybrid Evaluation of Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Fisch",
      "Joshua Maynez",
      "R. Hofer",
      "Bhuwan Dhingra",
      "Amir Globerson",
      "William W. Cohen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/c9fd326fd03eaf52f672c31cde9658af-Abstract-Conference.html": {
    "title": "Team-Fictitious Play for Reaching Team-Nash Equilibrium in Multi-team Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Dönmez",
      "Yüksel Arslantaş",
      "Muhammed Sayin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ca4aa9acc097e0f606af55bf986cb031-Abstract-Conference.html": {
    "title": "WizardArena: Post-training Large Language Models via Simulated Offline Chatbot Arena",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HAIPENG LUO",
      "Qingfeng Sun",
      "Can Xu",
      "Pu Zhao",
      "Qingwei Lin",
      "Jian-Guang Lou",
      "Shifeng Chen",
      "Yansong Tang",
      "Weizhu Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ca4f6e86453e4b117dd3263792053cf5-Abstract-Conference.html": {
    "title": "Online Control in Population Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noah Golowich",
      "Elad Hazan",
      "Zhou Lu",
      "Dhruv Rohatgi",
      "Y. Jennifer Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ca70528fb11dc8086c6a623da9f3fee6-Abstract-Conference.html": {
    "title": "Linear Causal Representation Learning from Unknown Multi-node Interventions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Burak Varıcı",
      "Emre Acartürk",
      "Karthikeyan Shanmugam",
      "Ali Tajer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ca774047bc3b46cc81e53ead34cd5d5a-Abstract-Conference.html": {
    "title": "Bayesian Strategic Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lee Cohen",
      "Saeed Sharifi-Malvajerdi",
      "Kevin Stangl",
      "Ali Vakilian",
      "Juba Ziani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ca92ff06d973ece92cecc561757d500e-Abstract-Conference.html": {
    "title": "SCaR: Refining Skill Chaining for Long-Horizon Robotic Manipulation via Dual Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Chen",
      "Ze Ji",
      "Jing Huo",
      "Yang Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ca9567d8ef6b2ea2da0d7eed57b933ee-Abstract-Conference.html": {
    "title": "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giorgio Piatti",
      "Zhijing Jin",
      "Max Kleiman-Weiner",
      "Bernhard Schölkopf",
      "Mrinmaya Sachan",
      "Rada Mihalcea"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ca98452d4e9ecbc18c40da2aa0da8b98-Abstract-Conference.html": {
    "title": "Why Warmup the Learning Rate? Underlying Mechanisms and Improvements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dayal Singh Kalra",
      "Maissam Barkeshli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ca9873918aa72e9033041f76e77b5c15-Abstract-Conference.html": {
    "title": "PrivCirNet: Efficient Private Inference via Block Circulant Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianshi Xu",
      "Lemeng Wu",
      "Runsheng Wang",
      "Meng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cabf611498431ad89a85ace75f790d93-Abstract-Conference.html": {
    "title": "N-agent Ad Hoc Teamwork",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caroline Wang",
      "Muhammad Arrasy Rahman",
      "Ishan Durugkar",
      "Elad Liebman",
      "Peter Stone"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cac9e747a1d480c78312226959566cef-Abstract-Conference.html": {
    "title": "Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Qiao",
      "Haodong Duan",
      "Xinyu Fang",
      "Junming Yang",
      "Lin Chen",
      "Songyang Zhang",
      "Jiaqi Wang",
      "Dahua Lin",
      "Kai Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cad4501fe7c1b53427b363daf1366b2f-Abstract-Conference.html": {
    "title": "Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qitao Zhao",
      "Shubham Tulsiani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cae00f05c4074758a6542823ae7bea99-Abstract-Conference.html": {
    "title": "Coded Computing for Resilient Distributed Computing: A Learning-Theoretic Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parsa Moradi",
      "Behrooz Tahmasebi",
      "Mohammad Maddah-Ali"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cae31fa913f140b3126036cc474fd461-Abstract-Conference.html": {
    "title": "Meta-Learning Universal Priors Using Non-Injective Change of Variables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilang Zhang",
      "Alireza Sadeghi",
      "Georgios Giannakis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cb03b5108f1c3a38c990ef0b45bc8b31-Abstract-Conference.html": {
    "title": "SleeperNets: Universal Backdoor Poisoning Attacks Against Reinforcement Learning Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ethan Rathbun",
      "Christopher Amato",
      "Alina Oprea"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cb0f9020c00fc52a9f6c9dbfacc6ac58-Abstract-Conference.html": {
    "title": "Robust Sleep Staging over Incomplete Multimodal Physiological Signals via Contrastive Imagination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Shen",
      "Junchang Xin",
      "Bing Dai",
      "Shudi Zhang",
      "Zhiqiong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cb1ba6a42814bf83974ed45ffdb72efa-Abstract-Conference.html": {
    "title": "Rethinking the Diffusion Models for Missing Data Imputation: A Gradient Flow Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhichao Chen",
      "Haoxuan Li",
      "Fangyikang Wang",
      "Odin Zhang",
      "Hu Xu",
      "Xiaoyu Jiang",
      "Zhihuan Song",
      "Hao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cb2b9198daacf5f434372122671450ef-Abstract-Conference.html": {
    "title": "Efficient Combinatorial Optimization via Heat Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengyuan Ma",
      "Wenlian Lu",
      "Jianfeng Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cb775ebc750e0454a470fc99c5accc85-Abstract-Conference.html": {
    "title": "Private Online Learning via Lazy Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hilal Asi",
      "Tomer Koren",
      "Daogao Liu",
      "Kunal Talwar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cbae8efcc23a0cb6d15a20f245514020-Abstract-Conference.html": {
    "title": "Adversarial Moment-Matching Distillation of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Jia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cbb1fa8e7f515e796cda6621a703492f-Abstract-Conference.html": {
    "title": "Evidential Mixture Machines: Deciphering Multi-Label Correlations for Active Learning Sensitivity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dayou Yu",
      "Minghao Li",
      "Weishi Shi",
      "Qi Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cbc4912b67d57e3932f56f3fa99faab3-Abstract-Conference.html": {
    "title": "FreqMark: Invisible Image Watermarking via Frequency Based Optimization in Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YiYang Guo",
      "Ruizhe Li",
      "Mude Hui",
      "Hanzhong Guo",
      "Chen Zhang",
      "Chuangjian Cai",
      "Le Wan",
      "shangfei wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cbc80272426028bd561f3889af65c704-Abstract-Conference.html": {
    "title": "DisC-GS: Discontinuity-aware Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxuan Qu",
      "Zhuoling Li",
      "Hossein Rahmani",
      "Yujun Cai",
      "Jun Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cbcce87f745072c819204529be843d16-Abstract-Conference.html": {
    "title": "The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ouail Kitouni",
      "Niklas S Nolte",
      "Adina Williams",
      "Michael Rabbat",
      "Diane Bouchacourt",
      "Mark Ibrahim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cbe1fd3136e0f049bb8bc104231ccb99-Abstract-Conference.html": {
    "title": "Functional Gradient Flows for Constrained Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyue Zhang",
      "Longlin Yu",
      "Ziheng Cheng",
      "Cheng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cbe25fa0e7c7084049276888a09acc8d-Abstract-Conference.html": {
    "title": "Prediction with Action: Visual Policy Learning via Joint Denoising Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanjiang Guo",
      "Yucheng Hu",
      "Jianke Zhang",
      "Yen-Jen Wang",
      "Xiaoyu Chen",
      "Chaochao Lu",
      "Jianyu Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cc32ec39a5073f61d38c338d963df30d-Abstract-Conference.html": {
    "title": "Stochastic Optimal Control Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carles Domingo i Enrich",
      "Jiequn Han",
      "Brandon Amos",
      "Joan Bruna",
      "Ricky T. Q. Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cc4d9cfc45325e460b455a820d5f212c-Abstract-Conference.html": {
    "title": "Context-Aware Testing: A New Paradigm for Model Testing with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paulius Rauba",
      "Nabeel Seedat",
      "Max Ruiz Luyten",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cc52950239c3129464b0a6e379e2a9b0-Abstract-Conference.html": {
    "title": "3D Focusing-and-Matching Network for Multi-Instance Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyuan Zhang",
      "Le Hui",
      "qi liu",
      "Bo Li",
      "Yuchao Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cc57fac10eacadb3b72a907ac48f9a98-Abstract-Conference.html": {
    "title": "Learning Segmentation from Point Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laurynas Karazija",
      "Iro Laina",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cc596a803bedc7a03a87e98c77a22efe-Abstract-Conference.html": {
    "title": "AdaptiveISP: Learning an Adaptive Image Signal Processor for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujin Wang",
      "Tianyi Xu",
      "Zhang Fan",
      "Tianfan Xue",
      "Jinwei Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cc83e97320000f4e08cb9e293b12cf7e-Abstract-Conference.html": {
    "title": "Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zian Su",
      "Xiangzhe Xu",
      "Ziyang Huang",
      "Kaiyuan Zhang",
      "Xiangyu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cc84bfabe6389d8883fc2071c848f62a-Abstract-Conference.html": {
    "title": "Causal Dependence Plots",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Loftus",
      "Lucius Bynum",
      "Sakina Hansen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cc92809cd8dfbd035801966ab4896741-Abstract-Conference.html": {
    "title": "RSA: Resolving Scale Ambiguities in Monocular Depth Estimators through Language Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyao Zeng",
      "Yangchao Wu",
      "Hyoungseob Park",
      "Daniel Wang",
      "Fengyu Yang",
      "Stefano Soatto",
      "DONG LAO",
      "Byung-Woo Hong",
      "Alex Wong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ccda3c632cc8590ee60ca5ba226a4c30-Abstract-Conference.html": {
    "title": "NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Zhang",
      "Jonah Yi",
      "Bowen Yao",
      "Zhaozhuo Xu",
      "Anshumali Shrivastava"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cce7576bf51b6a156346156c0976e7f6-Abstract-Conference.html": {
    "title": "On the Robustness of Spectral Algorithms for Semirandom Stochastic Block Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Bhaskara",
      "Agastya Jha",
      "Michael Kapralov",
      "Naren Manoj",
      "Davide Mazzali",
      "Weronika Wrzos-Kaminska"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cceb6b5d1781b6eb848f7e87bff5f74b-Abstract-Conference.html": {
    "title": "Federated Behavioural Planes: Explaining the Evolution of Client Behaviour in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dario Fenoglio",
      "Gabriele Dominici",
      "Pietro Barbiero",
      "Alberto Tonda",
      "Martin Gjoreski",
      "Marc Langheinrich"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ccf4a7323b9ee3e54bf77f0e876b3f8b-Abstract-Conference.html": {
    "title": "Federated Black-Box Adaptation for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jay Paranjape",
      "Shameema Sikder",
      "S. Vedula",
      "Vishal Patel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ccfa9ba5a84d0e4c620093d27102b7c5-Abstract-Conference.html": {
    "title": "Compositional Generalization Across Distributional Shifts with Sparse Tree Operations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Soulos",
      "Henry Conklin",
      "Mattia Opper",
      "Paul Smolensky",
      "Jianfeng Gao",
      "Roland Fernandez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cd004fa45fc1fe5c0218b7801d98d036-Abstract-Conference.html": {
    "title": "Fractal Patterns May Illuminate the Success of Next-Token Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ibrahim M Alabdulmohsin",
      "Vinh Tran",
      "Mostafa Dehghani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cd04ec5aebfbe397c7fd718c35d02e0b-Abstract-Conference.html": {
    "title": "Symmetry Discovery Beyond Affine Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Shaw",
      "Abram Magner",
      "Kevin J. Moon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cd091a4d8e97157d32940428f902c7b0-Abstract-Conference.html": {
    "title": "Double-Ended Synthesis Planning with Goal-Constrained Bidirectional Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Yu",
      "Jihye Roh",
      "Ziang Li",
      "Wenhao Gao",
      "Runzhong Wang",
      "Connor Coley"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cd1da8043ba5c1c144ab4e10a8de6e53-Abstract-Conference.html": {
    "title": "Efficient and Sharp Off-Policy Evaluation in Robust Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Bennett",
      "Nathan Kallus",
      "Miruna Oprescu",
      "Wen Sun",
      "Kaiwen Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cd38340309ed34d886b9cd6e35059606-Abstract-Conference.html": {
    "title": "Matching the Statistical Query Lower Bound for $k$-Sparse Parity Problems with Sign Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwen Kou",
      "Zixiang Chen",
      "Quanquan Gu",
      "Sham Kakade"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cd3b5d2ed967e906af24b33d6a356cac-Abstract-Conference.html": {
    "title": "Bigger, Regularized, Optimistic: scaling for compute and sample efficient continuous control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Nauman",
      "Mateusz Ostaszewski",
      "Krzysztof Jankowski",
      "Piotr Miłoś",
      "Marek Cygan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cd476d01692c508ddf1cb43c6279a704-Abstract-Conference.html": {
    "title": "Customizing Language Models with Instance-wise LoRA for Sequential Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Kong",
      "Jiancan Wu",
      "An Zhang",
      "Leheng Sheng",
      "Hui Lin",
      "Xiang Wang",
      "Xiangnan He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cd47cd67caa87f5b1944e00f6781598f-Abstract-Conference.html": {
    "title": "Prediction-Powered Ranking of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivi Chatzi",
      "Eleni Straitouri",
      "Suhas Thejaswi",
      "Manuel Rodriguez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cd4b49379efac6e84186a3ffce108c37-Abstract-Conference.html": {
    "title": "ArkVale: Efficient Generative LLM Inference with Recallable Key-Value Eviction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renze Chen",
      "Zhuofeng Wang",
      "Beiquan Cao",
      "Tong Wu",
      "Size Zheng",
      "Xiuhong Li",
      "Xuechao Wei",
      "Shengen Yan",
      "Meng Li",
      "Yun Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cd8b5de90ebfd6df2b703d2346370cba-Abstract-Conference.html": {
    "title": "Unsupervised Discovery of Formulas for Mathematical Constants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Shalyt",
      "Uri Seligmann",
      "Itay Beit Halachmi",
      "Ofir David",
      "Rotem Elimelech",
      "Ido Kaminer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cd9664c7094d90e512ce27f2fd58198b-Abstract-Conference.html": {
    "title": "Oracle-Efficient Differentially Private Learning with Public Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Block",
      "Mark Bun",
      "Rathin Desai",
      "Abhishek Shetty",
      "Steven Z. Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cd96cb9a239c37b39dbf34f3f5a4c56f-Abstract-Conference.html": {
    "title": "Learning-Augmented Algorithms for the Bahncard Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hailiang Zhao",
      "Xueyan Tang",
      "Peng Chen",
      "Shuiguang Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cd97da5366de69250442901abcdd4c0a-Abstract-Conference.html": {
    "title": "DeltaDock: A Unified Framework for Accurate, Efficient, and Physically Reliable Molecular Docking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxian Yan",
      "ZAIXI ZHANG",
      "Jintao Zhu",
      "Kai Zhang",
      "Jianfeng Pei",
      "Qi Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cd9b4a28fb9eebe0430c3312a4898a41-Abstract-Conference.html": {
    "title": "Generalizablity of Memorization Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijia Yu",
      "Xiao-Shan Gao",
      "Lijun Zhang",
      "Yibo Miao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cda04d7ea67ea1376bf8c6962d8541e0-Abstract-Conference.html": {
    "title": "Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingru (Jessica) Jia",
      "Zehua Yuan",
      "Junhao Pan",
      "Paul McNamara",
      "Deming Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cdc1d08ee82d4818758d229abb7f1ce8-Abstract-Conference.html": {
    "title": "Exclusively Penalized Q-learning for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junghyuk Yeom",
      "Yonghyeon Jo",
      "Jeongmo Kim",
      "Sanghyeon Lee",
      "Seungyul Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cdcc6d47c1627350014a3076112ab824-Abstract-Conference.html": {
    "title": "Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Declan Campbell",
      "Sunayana Rane",
      "Tyler Giallanza",
      "Camillo Nicolò De Sabbata",
      "Kia Ghods",
      "Amogh Joshi",
      "Alexander Ku",
      "Steven Frankland",
      "Tom Griffiths",
      "Jonathan D Cohen",
      "Taylor Webb"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cdce17de141c9fba3bdf175a0b721941-Abstract-Conference.html": {
    "title": "Approximating the Top Eigenvector in Random Order Streams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Praneeth Kacham",
      "David Woodruff"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cdd0640218a27e9e2c0e52e324e25db0-Abstract-Conference.html": {
    "title": "Bridging The Gap between Low-rank and Orthogonal Adaptation via Householder Reflection Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shen Yuan",
      "Haotian Liu",
      "Hongteng Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cde328b7bf6358f5ebb91fe9c539745e-Abstract-Conference.html": {
    "title": "Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling Yang",
      "Zhaochen Yu",
      "Tianjun Zhang",
      "Shiyi Cao",
      "Minkai Xu",
      "Wentao Zhang",
      "Joseph E Gonzalez",
      "Bin CUI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cdf00c97c0cb2cc35179f03363da6c4f-Abstract-Conference.html": {
    "title": "Enhancing Large Language Models through Adaptive Tokenizers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyu Zheng",
      "Hanting Chen",
      "Tianyu Guo",
      "Chong Zhu",
      "Binfan Zheng",
      "Chang Xu",
      "Yunhe Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ce26d21662c979d515164b416d4571fe-Abstract-Conference.html": {
    "title": "Achieving Optimal Clustering in Gaussian Mixture Models with Anisotropic Covariance Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Chen",
      "Anderson Ye Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ce31378e9f41d8907e97dab172b6c559-Abstract-Conference.html": {
    "title": "Rad-NeRF: Ray-decoupled Training of Neural Radiance Field",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lidong Guo",
      "Xuefei Ning",
      "Yonggan Fu",
      "Tianchen Zhao",
      "Zhuoliang Kang",
      "Jincheng Yu",
      "Yingyan (Celine) Lin",
      "Yu Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ce406fcba6401cd90cbded79567e2cf5-Abstract-Conference.html": {
    "title": "SAFE: Slow and Fast Parameter-Efficient Tuning for Continual Learning with Pre-Trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linglan Zhao",
      "Xuerui Zhang",
      "Ke Yan",
      "Shouhong Ding",
      "Weiran Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ce5b4f79f4752b7f8e983a80ebcd9c7a-Abstract-Conference.html": {
    "title": "Nonlocal Attention Operator: Materializing Hidden Knowledge Towards Interpretable Physics Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Yu",
      "Ning Liu",
      "Fei Lu",
      "Tian Gao",
      "Siavash Jafarzadeh",
      "Stewart A Silling"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ce5f18eba5e5e9c7c166062300c677c3-Abstract-Conference.html": {
    "title": "Improved Particle Approximation Error for Mean Field Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atsushi Nitanda"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ce6326ac4794bb04d5eb16f597446baf-Abstract-Conference.html": {
    "title": "Provable Editing of Deep Neural Networks using Parametric Linear Relaxation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Tao",
      "Aditya V Thakur"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ce7984e36d58659211a8dc7d5457cd6f-Abstract-Conference.html": {
    "title": "A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arthur Juliani",
      "Jordan Ash"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ce953d71deeb33d9ffa2c879b518d273-Abstract-Conference.html": {
    "title": "Spike-based Neuromorphic Model for Sound Source Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dehao Zhang",
      "Shuai Wang",
      "Ammar Belatreche",
      "Wenjie Wei",
      "Yichen Xiao",
      "Haorui Zheng",
      "Zijian Zhou",
      "Malu Zhang",
      "Yang Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cea5ebedc384c5f4b4ef6b6eb29c197f-Abstract-Conference.html": {
    "title": "Information-theoretic Limits of Online Classification with Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changlong Wu",
      "Ananth Grama",
      "Wojciech Szpankowski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ceb45c48e29e7f49b0b47edb98e43691-Abstract-Conference.html": {
    "title": "On the Surprising Effectiveness of Attention Transfer for Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Li",
      "Yuandong Tian",
      "Beidi Chen",
      "Deepak Pathak",
      "Xinlei Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cebbd24f1e50bcb63d015611fe0fe767-Abstract-Conference.html": {
    "title": "Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haifeng Huang",
      "Yilun Chen",
      "Zehan Wang",
      "Rongjie Huang",
      "Runsen Xu",
      "Tai WANG",
      "Luping Liu",
      "Xize Cheng",
      "Yang Zhao",
      "Jiangmiao Pang",
      "Zhou Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ced76a666704e381c3039871ffe558ee-Abstract-Conference.html": {
    "title": "Scaling Sign Language Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biao Zhang",
      "Garrett Tanzer",
      "Orhan Firat"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cef5c8dec67597b854f0162ad76d92d2-Abstract-Conference.html": {
    "title": "SMART: Scalable Multi-agent Real-time Motion Generation via Next-token Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Wu",
      "Xiaoxin Feng",
      "Ziyan Gao",
      "Yuheng KAN"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cf026564af33fed49b911b9532680b9d-Abstract-Conference.html": {
    "title": "UMFC: Unsupervised Multi-Domain Feature Calibration for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Liang",
      "RuiBing Hou",
      "Minyang Hu",
      "Hong Chang",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cf10920ac985275845247f865b452529-Abstract-Conference.html": {
    "title": "Implicit Bias of Mirror Flow on Separable Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Scott Pesme",
      "Radu-Alexandru Dragomir",
      "Nicolas Flammarion"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cf1129594f603fde9e1913d10b7dbf77-Abstract-Conference.html": {
    "title": "Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "RONGLONG FANG",
      "Yuesheng Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cf5a019ae9c11b4be88213ce3f85d85c-Abstract-Conference.html": {
    "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaofan Tao",
      "Qian Liu",
      "Longxu Dou",
      "Niklas Muennighoff",
      "Zhongwei Wan",
      "Ping Luo",
      "Min Lin",
      "Ngai Wong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cf66f995883298c4db2f0dcba28fb211-Abstract-Conference.html": {
    "title": "Are Self-Attentions Effective for Time Series Forecasting?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongbin Kim",
      "Jinseong Park",
      "Jaewook Lee",
      "Hoki Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cf836efd32fd53493e02d26670f04d46-Abstract-Conference.html": {
    "title": "User-item fairness tradeoffs in recommendations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sophie Greenwood",
      "Sudalakshmee Chiniah",
      "Nikhil Garg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cf8b2205e39f81726a8d828ecbe00ad0-Abstract-Conference.html": {
    "title": "Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teng Xiao",
      "Yige Yuan",
      "Huaisheng Zhu",
      "Mingxiao Li",
      "Vasant Honavar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cf8ec6e5eb9b52bae998dc534713848d-Abstract-Conference.html": {
    "title": "Artemis: Towards Referential Understanding in Complex Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihao Qiu",
      "Yuan Zhang",
      "Xi Tang",
      "Lingxi Xie",
      "Tianren Ma",
      "Pengyu Yan",
      "DAVID DOERMANN",
      "Qixiang Ye",
      "Yunjie Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cfaccbd9b5e62562779351ebcb140c94-Abstract-Conference.html": {
    "title": "BAN: Detecting Backdoors Activated by Adversarial Neuron Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyun Xu",
      "Zhuoran Liu",
      "Stefanos Koffas",
      "Shujian Yu",
      "Stjepan Picek"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cfc1924c62e72e2cb0e0feeecb963241-Abstract-Conference.html": {
    "title": "Low-Rank Optimal Transport through Factor Relaxation with Latent Coupling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Halmos",
      "Xinhao Liu",
      "Julian Gold",
      "Benjamin Raphael"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cfce727868dcaf5295c0125f9d6fbc0b-Abstract-Conference.html": {
    "title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asadullah Hill Galib",
      "Pang-Ning Tan",
      "Lifeng Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cfecf44f8c9527c03200827d67869fea-Abstract-Conference.html": {
    "title": "TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic Scene",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sandika Biswas",
      "Qianyi Wu",
      "Biplab Banerjee",
      "Hamid Rezatofighi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cfedcb3b2abdf0f519b0afd6a16c04da-Abstract-Conference.html": {
    "title": "Learning to Solve Quadratic Unconstrained Binary Optimization in a Classification Way",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Chen",
      "Jie Chun",
      "Shang Xiang",
      "Luona Wei",
      "Yonghao Du",
      "Qian Wan",
      "Yuning Chen",
      "Yingwu Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cff98e0b76e05fd1df5c9256724b3af1-Abstract-Conference.html": {
    "title": "BECAUSE: Bilinear Causal Representation for Generalizable Offline Model-based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haohong Lin",
      "Wenhao Ding",
      "Jian Chen",
      "Laixi Shi",
      "Jiacheng Zhu",
      "Bo Li",
      "DING ZHAO"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/cffbaf4f47546ece96bb42c0edda40ee-Abstract-Conference.html": {
    "title": "Q-VLM: Post-training Quantization for Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changyuan Wang",
      "Ziwei Wang",
      "Xiuwei Xu",
      "Yansong Tang",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d00904cebc0d5b69fada8ad33d0f1422-Abstract-Conference.html": {
    "title": "Diffusion Tuning: Transferring Diffusion Models via Chain of Forgetting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jincheng Zhong",
      "Xingzhuo Guo",
      "Jiaxiang Dong",
      "Mingsheng Long"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d00fcdd0629dabdf515b1e6425a261bb-Abstract-Conference.html": {
    "title": "Multi-Group Proportional Representation in Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Oesterling",
      "Claudio Mayrink Verdun",
      "Alexander Glynn",
      "Carol Long",
      "Lucas Monteiro Paes",
      "Sajani Vithana",
      "Martina Cardone",
      "Flavio Calmon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d01db5cd2555ba11f75da0454d57b903-Abstract-Conference.html": {
    "title": "Provable Benefit of Cutout and CutMix for Feature Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junsoo Oh",
      "Chulhee Yun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d022216357816e483433914204aa80ee-Abstract-Conference.html": {
    "title": "A PID Controller Approach for Adaptive Probability-dependent Gradient Decay in Model Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Zhang",
      "Linbo Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d0241a0fb1fc9be477bdfde5e0da276a-Abstract-Conference.html": {
    "title": "Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wangbo Zhao",
      "Jiasheng Tang",
      "Yizeng Han",
      "Yibing Song",
      "Kai Wang",
      "Gao Huang",
      "Fan Wang",
      "Yang You"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d02ff1aeaa5c268dc34790dd1ad21526-Abstract-Conference.html": {
    "title": "Large language model validity via enhanced conformal prediction methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Cherian",
      "Isaac Gibbs",
      "Emmanuel Candes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d032263772946dd5026e7f3cd22bce5b-Abstract-Conference.html": {
    "title": "Agent Planning with World Knowledge Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuofei Qiao",
      "Runnan Fang",
      "Ningyu Zhang",
      "Yuqi Zhu",
      "Xiang Chen",
      "Shumin Deng",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Huajun Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d037fd021c9aace128b8ce25001cdb6c-Abstract-Conference.html": {
    "title": "Calibrating Reasoning in Language Models with Internal Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihui Xie",
      "Jizhou Guo",
      "Tong Yu",
      "Shuai Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d041d6bb47c01a4ce327a1773703e9a0-Abstract-Conference.html": {
    "title": "Graph Coarsening with Message-Passing Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonin Joly",
      "Nicolas Keriven"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d06537b4b38ccf008a54559d2c56fa23-Abstract-Conference.html": {
    "title": "Breaking the False Sense of Security in Backdoor Defense through Re-Activation Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingli Zhu",
      "Siyuan Liang",
      "Baoyuan Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d067d16e3e5fe8fa8a3e62909907659a-Abstract-Conference.html": {
    "title": "Theoretical Characterisation of the Gauss Newton Conditioning in Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jim Zhao",
      "Sidak Pal Singh",
      "Aurelien Lucchi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d06a797c436cd5136a6f45b063316278-Abstract-Conference.html": {
    "title": "Amortized Fourier Neural Operators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zipeng Xiao",
      "Siqi Kou",
      "Hao Zhongkai",
      "Bokai Lin",
      "Zhijie Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d06c529fe328638e6ce420b89999f636-Abstract-Conference.html": {
    "title": "Analysing Multi-Task Regression via Random Matrix Theory with Application to Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Romain Ilbert",
      "Malik Tiomoko",
      "Cosme Louart",
      "Ambroise Odonnat",
      "Vasilii Feofanov",
      "Themis Palpanas",
      "Ievgen Redko"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d0724f5d6108517c3eab35f77f156967-Abstract-Conference.html": {
    "title": "Offline Behavior Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiye Lei",
      "Sen Zhang",
      "Dacheng Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d07a9fc7da2e2ec0574c38d5f504d105-Abstract-Conference.html": {
    "title": "AutoSurvey: Large Language Models Can Automatically Write Surveys",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yidong Wang",
      "Qi Guo",
      "Wenjin Yao",
      "Hongbo Zhang",
      "Xin Zhang",
      "Zhen Wu",
      "Meishan Zhang",
      "Xinyu Dai",
      "Min zhang",
      "Qingsong Wen",
      "Wei Ye",
      "Shikun Zhang",
      "Yue Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d0822540916cd716add52e1846a6e18d-Abstract-Conference.html": {
    "title": "LOVA3: Learning to Visual Question Answering, Asking and Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henry Hengyuan Zhao",
      "Pan Zhou",
      "Difei Gao",
      "Zechen Bai",
      "Mike Zheng Shou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d085b2d2d6dee1dd596bc3b8ecd5aaa6-Abstract-Conference.html": {
    "title": "Mixture of Adversarial LoRAs: Boosting Robust Generalization in Meta-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Yang",
      "Chen Liu",
      "Ying Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d0949cbcec31c09431610553a284f94a-Abstract-Conference.html": {
    "title": "Leveraging Catastrophic Forgetting to Develop Safe Diffusion Models against Malicious Finetuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiadong Pan",
      "Hongcheng Gao",
      "Zongyu Wu",
      "Taihang Hu",
      "Li Su",
      "Qingming Huang",
      "Liang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d0a2279c9f7ded859bcbf878c3c3d1ed-Abstract-Conference.html": {
    "title": "PowerPM: Foundation Model for Power Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihao Tu",
      "Yupeng Zhang",
      "Jing Zhang",
      "Zhendong Fu",
      "Yin Zhang",
      "YANG YANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d0aafec03d59db29a92fa683bd783374-Abstract-Conference.html": {
    "title": "Cost-efficient Knowledge-based Question Answering with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junnan Dong",
      "Qinggang Zhang",
      "Chuang Zhou",
      "Hao Chen",
      "Daochen Zha",
      "Xiao Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d0c6bc641a56bebee9d985b937307367-Abstract-Conference.html": {
    "title": "LoQT: Low-Rank Adapters for Quantized Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Loeschcke",
      "Mads Toftrup",
      "Michael Kastoryano",
      "Serge Belongie",
      "Vésteinn Snæbjarnarson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d0cf89927acd9136d27ebf08f9e8a888-Abstract-Conference.html": {
    "title": "High Rank Path Development: an approach to learning the filtration of stochastic processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajie Tao",
      "Hao Ni",
      "Chong Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d0ffb35aaa7faa894afe5060c694d674-Abstract-Conference.html": {
    "title": "Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongxiao He",
      "Lianze Shan",
      "Jitao Zhao",
      "Hengrui Zhang",
      "Zhen Wang",
      "Weixiong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d107ca794d83c8242e357e6a43a068f4-Abstract-Conference.html": {
    "title": "One-Step Diffusion Distillation through Score Implicit Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijian Luo",
      "Zemin Huang",
      "Zhengyang Geng",
      "J. Zico Kolter",
      "Guo-Jun Qi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d10c7e24c96db4b222688efd11b02940-Abstract-Conference.html": {
    "title": "Structured Learning of Compositional Sequential Interventions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialin Yu",
      "Andreas Koukorinis",
      "Nicolo Colombo",
      "Yuchen Zhu",
      "Ricardo Silva"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d10f451f973846d20dae8674c493fa3c-Abstract-Conference.html": {
    "title": "Last-Iterate Convergence for Generalized Frank-Wolfe in Monotone Variational Inequalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zaiwei Chen",
      "Eric Mazumdar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d1346a0712aec24a0779995e946d6072-Abstract-Conference.html": {
    "title": "Real-time Stereo-based 3D Object Detection for Streaming Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changcai Li",
      "Zonghua Gu",
      "Gang Chen",
      "Libo Huang",
      "Wei Zhang",
      "Huihui Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d13a3eae72366e61dfdc7eea82eeb685-Abstract-Conference.html": {
    "title": "Parallelizing Linear Transformers with the Delta Rule over Sequence Length",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songlin Yang",
      "Bailin Wang",
      "Yu Zhang",
      "Yikang Shen",
      "Yoon Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d14c355d5e88cff437a6303d2d716252-Abstract-Conference.html": {
    "title": "Cost-aware Bayesian Optimization via the Pandora's Box Gittins Index",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Xie",
      "Raul Astudillo",
      "Peter Frazier",
      "Ziv Scully",
      "Alexander Terenin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d15c16cf5619a2b1606da5fc88e3f1a9-Abstract-Conference.html": {
    "title": "Adaptable Logical Control for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honghua Zhang",
      "Po-Nien Kung",
      "Masahiro Yoshida",
      "Guy Van den Broeck",
      "Nanyun Peng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d16152d53088ad779ffa634e7bf66166-Abstract-Conference.html": {
    "title": "FLAME : Factuality-Aware Alignment for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng-Chieh Lin",
      "Luyu Gao",
      "Barlas Oguz",
      "Wenhan Xiong",
      "Jimmy Lin",
      "Scott Yih",
      "Xilun Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d17477a5ff8b7ddb49e53f9c04305da5-Abstract-Conference.html": {
    "title": "FIARSE: Model-Heterogeneous Federated Learning via Importance-Aware Submodel Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feijie Wu",
      "Xingchen Wang",
      "Yaqing Wang",
      "Tianci Liu",
      "Lu Su",
      "Jing Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d18d208fa9c333483e5724ade7beff0f-Abstract-Conference.html": {
    "title": "Clustering in Causal Attention Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Karagodin",
      "Yury Polyanskiy",
      "Philippe Rigollet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d1a1e8713fcd5626656553c82f7c3b26-Abstract-Conference.html": {
    "title": "The Selective $G$-Bispectrum and its Inversion: Applications to $G$-Invariant Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Mataigne",
      "Johan Mathe",
      "Sophia Sanborn",
      "Christopher Hillar",
      "Nina Miolane"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d1a25d7e93f06cb422b3a74a0aa3bf3f-Abstract-Conference.html": {
    "title": "Schedule Your Edit: A Simple yet Effective Diffusion Noise Schedule for Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Lin",
      "Yan Chen",
      "Jiahao Wang",
      "Wenbin An",
      "Mengmeng Wang",
      "Feng Tian",
      "Yong Liu",
      "Guang Dai",
      "Jingdong Wang",
      "QianYing Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d1b7d6244356456476231f6bece2c046-Abstract-Conference.html": {
    "title": "Geometry Awakening: Cross-Geometry Learning Exhibits Superiority over Individual Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YADONG SUN",
      "Xiaofeng Cao",
      "Yu Wang",
      "Wei Ye",
      "Jingcai Guo",
      "Qing Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d1bdc488ec18f64177b2275a03984683-Abstract-Conference.html": {
    "title": "Advancing Training Efficiency of Deep Spiking Neural Networks through Rate-based Backpropagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengting Yu",
      "Lei Liu",
      "Gaoang Wang",
      "Erping Li",
      "Aili Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d1ca7877cdaf3201ecfa95b1240f7942-Abstract-Conference.html": {
    "title": "Exploring Jacobian Inexactness in Second-Order Methods for Variational Inequalities: Lower Bounds, Optimal Algorithms and Quasi-Newton Approximations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artem Agafonov",
      "Petr Ostroukhov",
      "Roman Mozhaev",
      "Konstantin Yakovlev",
      "Eduard Gorbunov",
      "Martin Takac",
      "Alexander Gasnikov",
      "Dmitry Kamzolov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d1e6739f319be4ba8e748cc05f23bba1-Abstract-Conference.html": {
    "title": "Provable Benefits of Complex Parameterizations for Structured State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuval Ran-Milo",
      "Eden Lumbroso",
      "Edo Cohen-Karlik",
      "Raja Giryes",
      "Amir Globerson",
      "Nadav Cohen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d1ebc73cd4c88866b97a6851ece739d1-Abstract-Conference.html": {
    "title": "A teacher-teacher framework for clinical language representation learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiqing Huang",
      "Shenghan Zhang",
      "Sara Sweet",
      "Tianxi Cai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d20e3c35bedb17a9f6f01fc434a30fa3-Abstract-Conference.html": {
    "title": "Testing Calibration in Nearly-Linear Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lunjia Hu",
      "Arun Jambulapati",
      "Kevin Tian",
      "Chutong Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d21f0a94b4c4d933a3ad1391b9e11c24-Abstract-Conference.html": {
    "title": "Towards Flexible 3D Perception: Object-Centric Occupancy Completion Augments 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoda Zheng",
      "Feng Wang",
      "Naiyan Wang",
      "Shuguang Cui",
      "Zhen Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d28077e5ff52034cd35b4aa15320caea-Abstract-Conference.html": {
    "title": "Jointly Modeling Inter- & Intra-Modality Dependencies for Multi-modal Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Divyam Madaan",
      "Taro Makino",
      "Sumit Chopra",
      "Kyunghyun Cho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d2964af7bd4e891297a39dc9085fa754-Abstract-Conference.html": {
    "title": "You Don't Need Domain-Specific Data Augmentations When Scaling Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Théo Moutakanni",
      "Maxime Oquab",
      "Marc Szafraniec",
      "Maria Vakalopoulou",
      "Piotr Bojanowski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d298cf34e4539f9134db7f38b42f69fe-Abstract-Conference.html": {
    "title": "$\\textit{Read-ME}$: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruisi Cai",
      "Yeonju Ro",
      "Geon-Woo Kim",
      "Peihao Wang",
      "Babak Ehteshami Bejnordi",
      "Aditya Akella",
      "Zhangyang &quot;Atlas&quot; Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d2bdcd4f51eea138365af22b50f3bf0a-Abstract-Conference.html": {
    "title": "Exploring and Exploiting the Asymmetric Valley of Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin-Chun Li",
      "Jin-Lin Tang",
      "Bo Zhang",
      "Lan Li",
      "De-Chuan Zhan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d2c50c5b2e3fcb33e8554d44b84eb52f-Abstract-Conference.html": {
    "title": "Instructor-inspired Machine Learning for Robust Molecular Property Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fang Wu",
      "Shuting Jin",
      "Siyuan Li",
      "Stan Z. Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d2cc447db9e56c13b993c11b45956281-Abstract-Conference.html": {
    "title": "Nuclear Norm Regularization for Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Scarvelis",
      "Justin M Solomon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d2d3ca53fd8fbd564bb948f8c09c0d85-Abstract-Conference.html": {
    "title": "SE(3)-bi-equivariant Transformers for Point Cloud Assembly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziming Wang",
      "Rebecka Jörnsten"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d2e48b68e0b91f8b1c2ac8acb929c5e5-Abstract-Conference.html": {
    "title": "Universal Rates of Empirical Risk Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steve Hanneke",
      "Mingyue Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d2fe3a5711a6d488da9e9a78b84ee24c-Abstract-Conference.html": {
    "title": "Attention Temperature Matters in ViT-Based Cross-Domain Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiong Zou",
      "Ran Ma",
      "Yuhua Li",
      "Ruixuan Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d303b4f1ef8d8274ae6b152df70f5406-Abstract-Conference.html": {
    "title": "LocCa: Visual Pretraining with Location-aware Captioners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Wan",
      "Michael Tschannen",
      "Yongqin Xian",
      "Filip Pavetic",
      "Ibrahim M Alabdulmohsin",
      "Xiao Wang",
      "André Susano Pinto",
      "Andreas Steiner",
      "Lucas Beyer",
      "Xiaohua Zhai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d33b177b69425e7685b0b1c05bd2a5e4-Abstract-Conference.html": {
    "title": "Efficient Graph Matching for Correlated Stochastic Block Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuwen Chai",
      "Miklos Z. Racz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d354fcb1ce9b10580b1e14a529e8bf6f-Abstract-Conference.html": {
    "title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjun Zhang",
      "Liangxiao Jiang",
      "Chaoqun Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d3602fc92fb8b9e0d55356c9e8815e2b-Abstract-Conference.html": {
    "title": "Understanding the Transferability of Representations via Task-Relatedness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Mehra",
      "Yunbei Zhang",
      "Jihun Hamm"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d368aba36f74776cc7a1079332a31973-Abstract-Conference.html": {
    "title": "S2HPruner: Soft-to-Hard Distillation Bridges the Discretization Gap in Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihao Lin",
      "Shengji Tang",
      "Chong Yu",
      "Peng Ye",
      "Tao Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d3696c79d572c995a74eac78037551a8-Abstract-Conference.html": {
    "title": "Label Noise: Ignorance Is Bliss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Zhu",
      "Jianxin Zhang",
      "Aditya Gangrade",
      "Clay Scott"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d37c9ad425fe5b65304d500c6edcba00-Abstract-Conference.html": {
    "title": "Iterative Reasoning Preference Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richard Yuanzhe Pang",
      "Weizhe Yuan",
      "He He",
      "Kyunghyun Cho",
      "Sainbayar Sukhbaatar",
      "Jason Weston"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d390199c28b467315b454789b6584f19-Abstract-Conference.html": {
    "title": "Strategic Linear Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Kleine Buening",
      "Aadirupa Saha",
      "Christos Dimitrakakis",
      "Haifeng Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d399b67fa017f0f7670102c88507720c-Abstract-Conference.html": {
    "title": "Dual Critic Reinforcement Learning under Partial Observability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinqiu Li",
      "Enmin Zhao",
      "Tong Wei",
      "Junliang Xing",
      "SHIMING XIANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d39e3ae9a11b79691709a7a6e06a63d9-Abstract-Conference.html": {
    "title": "What type of inference is planning?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miguel Lazaro-Gredilla",
      "Li Ku",
      "Kevin P. Murphy",
      "Dileep George"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d3a230d716e65afab578a8eb31a8d25f-Abstract-Conference.html": {
    "title": "Uncovering Safety Risks of Large Language Models through Concept Activation Vector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Xu",
      "Ruixuan HUANG",
      "Changyu Chen",
      "Xiting Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d3b8ce5e27b1c622d1b3da22b215e59b-Abstract-Conference.html": {
    "title": "Meta-Exploiting Frequency Prior for Cross-Domain Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Zhou",
      "Peng Wang",
      "Lei Zhang",
      "Zhenghua Chen",
      "Wei Wei",
      "Chen Ding",
      "Guosheng Lin",
      "Yanning Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d3d1947ba438c758790b18d5fcf69e8f-Abstract-Conference.html": {
    "title": "An In-depth Investigation of Sparse Rate Reduction in Transformer-like Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhe Hu",
      "Difan Zou",
      "Dong Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d3ee2816ae19c9e689c3352397c93a22-Abstract-Conference.html": {
    "title": "OT4P: Unlocking Effective Orthogonal Group Path for Permutation Relaxation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaming Guo",
      "chen zhu",
      "Hengshu Zhu",
      "Tieru Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d3f39e51f5f634fb16cc3e658f8512b9-Abstract-Conference.html": {
    "title": "Gated Slot Attention for Efficient Linear-Time Sequence Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhang",
      "Songlin Yang",
      "Rui-Jie Zhu",
      "Yue Zhang",
      "Leyang Cui",
      "Yiqiao Wang",
      "Bolun Wang",
      "Freda Shi",
      "Bailin Wang",
      "Wei Bi",
      "Peng Zhou",
      "Guohong Fu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d412468559da4e2ac41a0285b3a079cd-Abstract-Conference.html": {
    "title": "Tracing Hyperparameter Dependencies for Model Parsing via Learnable Graph Pooling Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Guo",
      "Vishal Asnani",
      "Sijia Liu",
      "Xiaoming Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d41f8403e9bb5141bc2c81fad7658185-Abstract-Conference.html": {
    "title": "Soft Superpixel Neighborhood Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kent W Gauen",
      "Stanley H Chan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d42523d621194ba54dda098669645f91-Abstract-Conference.html": {
    "title": "Focus On What Matters: Separated Models For Visual-Based RL Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Zhang",
      "Bowen Lv",
      "Hai Zhang",
      "Feifan Yang",
      "Junqiao Zhao",
      "Hang Yu",
      "Chang Huang",
      "Hongtu Zhou",
      "Chen Ye",
      "changjun jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d42a8bf2f40555d4a5120300f98c88f6-Abstract-Conference.html": {
    "title": "Conformalized Credal Set Predictors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alireza Javanmardi",
      "David Stutz",
      "Eyke Hüllermeier"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d4387c37b3b06e55f86eccdb8cd1f829-Abstract-Conference.html": {
    "title": "The Impact of Initialization on LoRA Finetuning Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soufiane Hayou",
      "Nikhil Ghosh",
      "Bin Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d450dceeacd6083d1d550247377f2320-Abstract-Conference.html": {
    "title": "DropEdge not Foolproof: Effective Augmentation Method for Signed Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZEYU ZHANG",
      "Lu Li",
      "Shuyan Wan",
      "Wang",
      "Zhiyi Wang",
      "Zhiyuan Lu",
      "Dong Hao",
      "Wanli Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d46f127a80dc58cbc0732a717285c43a-Abstract-Conference.html": {
    "title": "Learning Goal-Conditioned Representations for Language Reward Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vaskar Nath",
      "Dylan Slack",
      "Jeff Da",
      "Yuntao Ma",
      "Hugh Zhang",
      "Spencer Whitehead",
      "Sean Hendryx"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d4857f724cc1af4c8f1e18032426aa2e-Abstract-Conference.html": {
    "title": "Reinforcement Learning with LTL and $\\omega$-Regular Objectives via Optimality-Preserving Translation to Average Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Bach Le",
      "Dominik Wagner",
      "Leon Witzman",
      "Alexander Rabinovich",
      "Luke Ong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d4ab6d24758a0fe33604e1f4224ceea1-Abstract-Conference.html": {
    "title": "Learning Low-Rank Feature for Thorax Disease Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yancheng Wang",
      "Rajeev Goel",
      "Utkarsh Nath",
      "Alvin Silva",
      "Teresa Wu",
      "Yingzhen Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d4b6ccf3acd6ccbc1093e093df345ba2-Abstract-Conference.html": {
    "title": "In-Trajectory Inverse Reinforcement Learning: Learn Incrementally Before an Ongoing Trajectory Terminates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shicheng Liu",
      "Minghui Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d4bdeed749a437de2cbe2e2c7e5a6a8a-Abstract-Conference.html": {
    "title": "Navigating Extremes: Dynamic Sparsity in Large Output Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nasibullah Nasibullah",
      "Erik Schultheis",
      "Mike Lasby",
      "Yani Ioannou",
      "Rohit Babbar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d4cbcae8cfc8aa3ae897a1296e4e0cac-Abstract-Conference.html": {
    "title": "Truthfulness of Calibration Measures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nika Haghtalab",
      "Mingda Qiao",
      "Kunhe Yang",
      "Eric Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d4ce6738e84876aa79f13c8bc8b7c5eb-Abstract-Conference.html": {
    "title": "IODA: Instance-Guided One-shot Domain Adaptation for Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zaizuo Tang",
      "Yu-Bin Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d4e1983985392a9a46bd8f19bb51ba48-Abstract-Conference.html": {
    "title": "Conditional Density Estimation with Histogram Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lincen Yang",
      "Matthijs van Leeuwen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d4e1c24ac41ff0b82ca1b171731f0b23-Abstract-Conference.html": {
    "title": "Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxi Ren",
      "Xin Xia",
      "Yanzuo Lu",
      "Jiacheng Zhang",
      "Jie Wu",
      "Pan Xie",
      "XING WANG",
      "Xuefeng Xiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d4ee9e805cc90f636c66778225181036-Abstract-Conference.html": {
    "title": "Empowering Visible-Infrared Person Re-Identification with Large Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangyi Hu",
      "Bin Yang",
      "Mang Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d51cd79a85833b022841f7a2383b32d3-Abstract-Conference.html": {
    "title": "Differentiable Structure Learning with Partial Orders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taiyu Ban",
      "Lyuzhou Chen",
      "Xiangyu Wang",
      "Xin Wang",
      "Derui Lyu",
      "Huanhuan Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d51ceadaf09a4699f18986702df24987-Abstract-Conference.html": {
    "title": "Neural Residual Diffusion Models for Deep Scalable Vision Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Ma",
      "Liangliang Zhao",
      "Biqing Qi",
      "Bowen Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d529b943af3dba734f8a7d49efcb6d09-Abstract-Conference.html": {
    "title": "Truncated Variance Reduced Value Iteration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujia Jin",
      "Ishani Karmarkar",
      "Aaron Sidford",
      "Jiayi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d52ae52af3e3fcf1266fb1f282fd7206-Abstract-Conference.html": {
    "title": "The Fairness-Quality Tradeoff in Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rashida Hakim",
      "Ana-Andreea Stoica",
      "Christos Papadimitriou",
      "Mihalis Yannakakis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d52d2281babd36913643392a09a56832-Abstract-Conference.html": {
    "title": "Pruning neural network models for gene regulatory dynamics using data and domain knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Intekhab Hossain",
      "Jonas Fischer",
      "Rebekka Burkholz",
      "John Quackenbush"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d52dbd66219dc4e432e0bd4f9c25c4c3-Abstract-Conference.html": {
    "title": "Adaptive Sampling for Efficient Softmax Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tavor Baharav",
      "Ryan Kang",
      "Colin Sullivan",
      "Mo Tiwari",
      "Eric Luxenberg",
      "David Tse",
      "Mert Pilanci"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d53d46e19ce21c5fdc7664d4522999f1-Abstract-Conference.html": {
    "title": "InstructG2I: Synthesizing Images from Multimodal Attributed Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Jin",
      "Ziqi Pang",
      "Bingjun Guo",
      "Yu-Xiong Wang",
      "Jiaxuan You",
      "Jiawei Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d53d51e88d92d3723755f6d425bc513b-Abstract-Conference.html": {
    "title": "Is the MMI Criterion Necessary for Interpretability? Degenerating Non-causal Features to Plain Noise for Self-Rationalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Liu",
      "Zhiying Deng",
      "Zhongyu Niu",
      "Jun Wang",
      "Haozhao Wang",
      "YuanKai Zhang",
      "Ruixuan Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d560f94c582033e6d8eb0c97cdd4f721-Abstract-Conference.html": {
    "title": "Oracle-Efficient Reinforcement Learning for Max Value Ensembles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcel Hussing",
      "Michael J. Kearns",
      "Aaron Roth",
      "Sikata Sengupta",
      "Jessica Sorrell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d592bfaedf2f0861d7084cceba208d18-Abstract-Conference.html": {
    "title": "QuadMamba: Learning Quadtree-based Selective Scan for Visual State Space Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Xie",
      "Weijia Zhang",
      "Zhongdao Wang",
      "Chao Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d5938eb823784a2d422ebcf0ac86e47e-Abstract-Conference.html": {
    "title": "State-free Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyu Chen",
      "Aldo Pacchiano",
      "Xuezhou Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d5a1f97d2b922da92e880d13b7d2bf02-Abstract-Conference.html": {
    "title": "Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javier Gonzalez",
      "Aditya Nori"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d5a25f990ffa1cb759cc88916d7fbe96-Abstract-Conference.html": {
    "title": "Verified Safe Reinforcement Learning for Neural Network Dynamic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junlin Wu",
      "Huan Zhang",
      "Yevgeniy Vorobeychik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d5a58d198afa370a3dff0e1ca4fe1802-Abstract-Conference.html": {
    "title": "Noise Contrastive Alignment of Language Models with Explicit Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huayu Chen",
      "Guande He",
      "Lifan Yuan",
      "Ganqu Cui",
      "Hang Su",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d5ae1c5167be330b90f4c0a1eed7f8f0-Abstract-Conference.html": {
    "title": "Robust Conformal Prediction Using Privileged Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shai Feldman",
      "Yaniv Romano"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d5aed68fde8e934d0ae4aadb57acc6c0-Abstract-Conference.html": {
    "title": "PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghui Lu",
      "Yanjie Wang",
      "Ziwei Yang",
      "Xuejing Liu",
      "Brian Mac Namee",
      "Can Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d5c04aa72b92c53bda5b525b60958295-Abstract-Conference.html": {
    "title": "DMPlug: A Plug-in Method for Solving Inverse Problems with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengkang Wang",
      "Xu Zhang",
      "Taihui Li",
      "Yuxiang Wan",
      "Tiancong Chen",
      "Ju Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d5c3ecf397fff63419bb5f5f2d8afe33-Abstract-Conference.html": {
    "title": "Regression under demographic parity constraints via unlabeled post-processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gayane Taturyan",
      "Evgenii Chzhen",
      "Mohamed Hebiri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d5c56ec4f69c9a473089b16000d3f8cd-Abstract-Conference.html": {
    "title": "Code Repair with LLMs gives an Exploration-Exploitation Tradeoff",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Tang",
      "Keya Hu",
      "Jin Zhou",
      "Si Cheng Zhong",
      "Wei-Long Zheng",
      "Xujie Si",
      "Kevin Ellis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d5cd70b708f726737e2ebace18c3f71b-Abstract-Conference.html": {
    "title": "Efficient Multi-task Reinforcement Learning with Cross-Task Policy Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinmin He",
      "Kai Li",
      "Yifan Zang",
      "Haobo Fu",
      "Qiang Fu",
      "Junliang Xing",
      "Jian Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d5e256c988bdee59a0f4d7a9bc1dd6d9-Abstract-Conference.html": {
    "title": "Deep Bayesian Active Learning for Preference Modeling in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luckeciano Carvalho Melo",
      "Panagiotis Tigas",
      "Alessandro Abate",
      "Yarin Gal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d5e8326bbec25e1c608787d24488521b-Abstract-Conference.html": {
    "title": "Data Acquisition via Experimental Design for Data Markets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles Lu",
      "Baihe Huang",
      "Sai Praneeth Karimireddy",
      "Praneeth Vepakomma",
      "Michael I. Jordan",
      "Ramesh Raskar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d60b6b7f0ba6bf07d975b3bbdacea702-Abstract-Conference.html": {
    "title": "Latent Diffusion for Neural Spiking Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaivardhan Kapoor",
      "Auguste Schulz",
      "Julius Vetter",
      "Felix Pei",
      "Richard Gao",
      "Jakob H Macke"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d60e14c19cd6e0fc38556ad29ac8fbc9-Abstract-Conference.html": {
    "title": "Deep Support Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JunHoo Lee",
      "Hyunho Lee",
      "Kyomin Hwang",
      "Nojun Kwak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d6359156e0e30b1caa116a4306b12688-Abstract-Conference.html": {
    "title": "Large Language Model Unlearning via Embedding-Corrupted Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Liu",
      "Yaxuan Wang",
      "Jeffrey Flanigan",
      "Yang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d63cf0622eed012a17fe88fced64dcb8-Abstract-Conference.html": {
    "title": "SLTrain: a sparse plus low rank approach for parameter and memory efficient pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andi Han",
      "Jiaxiang Li",
      "Wei Huang",
      "Mingyi Hong",
      "Akiko Takeda",
      "Pratik Kumar Jawanpuria",
      "Bamdev Mishra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d64a008c2fa8392afaa7d7036a017042-Abstract-Conference.html": {
    "title": "Treeffuser: probabilistic prediction via conditional diffusions with gradient-boosted trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Beltran Velez",
      "Alessandro A Grande",
      "Achille Nazaret",
      "Alp Kucukelbir",
      "David M. Blei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d6520fa7f71dc8e09ed5939a60a64218-Abstract-Conference.html": {
    "title": "FedGMKD: An Efficient Prototype Federated Learning Framework through Knowledge Distillation and Discrepancy-Aware Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianqiao Zhang",
      "Caifeng Shan",
      "Jungong Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d65befe6b80ecf7f180b4def503d7776-Abstract-Conference.html": {
    "title": "Sigmoid Gating is More Sample Efficient than Softmax Gating in Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Nguyen",
      "Nhat Ho",
      "Alessandro Rinaldo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d65c4ce22241138c1784ff753d4c746c-Abstract-Conference.html": {
    "title": "Principled Probabilistic Imaging using Diffusion Models as Plug-and-Play Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihui Wu",
      "Yu Sun",
      "Yifan Chen",
      "Bingliang Zhang",
      "Yisong Yue",
      "Katherine Bouman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d68c1d10957c8d21ed9dea209533c5a4-Abstract-Conference.html": {
    "title": "Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingqi Ma",
      "ZHUOFAN ZONG",
      "Guanglu Song",
      "Hongsheng Li",
      "Yu Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d6938c8e88ef62394d2f4f3fd428e036-Abstract-Conference.html": {
    "title": "Point-PRC: A Prompt Learning Based Regulation Framework for Generalizable Point Cloud Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Sun",
      "Qiuhong Ke",
      "Yongcai Wang",
      "Wang Chen",
      "Kang Yang",
      "Deying Li",
      "Jianfei Cai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d6be2b51b213f4f5994243ccb494d97e-Abstract-Conference.html": {
    "title": "E2ENet: Dynamic Sparse Feature Fusion for Accurate and Efficient 3D Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boqian Wu",
      "Qiao Xiao",
      "Shiwei Liu",
      "Lu Yin",
      "Mykola Pechenizkiy",
      "Decebal Constantin Mocanu",
      "Maurice Keulen",
      "Elena Mocanu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d6be51e667e0b263e89a23294b57f8cf-Abstract-Conference.html": {
    "title": "Multi-Agent Coordination via Multi-Level Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang Ding",
      "Zeyuan Liu",
      "Zhirui Fang",
      "Kefan Su",
      "Liwen Zhu",
      "Zongqing Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d6d0e41e0b1ed38c76d13c9e417a8f1f-Abstract-Conference.html": {
    "title": "MambaLRP: Explaining Selective State Space Sequence Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farnoush Rezaei Jafari",
      "Grégoire Montavon",
      "Klaus-Robert Müller",
      "Oliver Eberle"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d6df31b1be98e04be48af8bedb95b499-Abstract-Conference.html": {
    "title": "Knowledge Circuits in Pretrained Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhi Yao",
      "Ningyu Zhang",
      "Zekun Xi",
      "Mengru Wang",
      "Ziwen Xu",
      "Shumin Deng",
      "Huajun Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d6f034bb216b472fc7d32ec7aff20342-Abstract-Conference.html": {
    "title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaifeng Lyu",
      "Haoyu Zhao",
      "Xinran Gu",
      "Dingli Yu",
      "Anirudh Goyal",
      "Sanjeev Arora"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d6f094ba0f5ce1720466342f78031bdb-Abstract-Conference.html": {
    "title": "Unveiling the Tapestry of Consistency in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Zhang",
      "Fei xiao",
      "Tao Huang",
      "Chun-Kai Fan",
      "Hongyuan Dong",
      "Jiawen Li",
      "Jiacong Wang",
      "Kuan Cheng",
      "Shanghang Zhang",
      "Haoyuan Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d6fdc3002dda4c3cad3d595ac6fa5352-Abstract-Conference.html": {
    "title": "TPC: Test-time Procrustes Calibration for Diffusion-based Human Image Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunjae Yoon",
      "Gwanhyeong Koo",
      "Younghwan Lee",
      "Chang D. Yoo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d705dd6e77decdc399162d6d5b92f6e8-Abstract-Conference.html": {
    "title": "Multi-model Ensemble Conformal Prediction in Dynamic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erfan Hajihashemi",
      "Yanning Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d712c8625fd97424c9744019b28dca21-Abstract-Conference.html": {
    "title": "Improving Generalization and Convergence by Enhancing Implicit Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingze Wang",
      "Jinbo Wang",
      "Haotian He",
      "Zilin Wang",
      "Guanhua Huang",
      "Feiyu Xiong",
      "Zhiyu li",
      "Weinan E",
      "Lei Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d7346ff79699b5bba26f8af89e700a95-Abstract-Conference.html": {
    "title": "Inductive biases of multi-task learning and finetuning: multiple regimes of feature reuse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Lippl",
      "Jack Lindsey"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d738aefead8500f5aed667f0a7ca7b7c-Abstract-Conference.html": {
    "title": "Estimating Heterogeneous Treatment Effects by Combining Weak Instruments and Observational Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miruna Oprescu",
      "Nathan Kallus"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d74f9efa1d8ca30b31d65cef8de7c2bf-Abstract-Conference.html": {
    "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ethan Shen",
      "Alan Fan",
      "Sarah Pratt",
      "Jae Sung Park",
      "Matthew Wallingford",
      "Sham Kakade",
      "Ari Holtzman",
      "Ranjay Krishna",
      "Ali Farhadi",
      "Aditya Kusupati"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d751402fe54c65e766fc958d78930803-Abstract-Conference.html": {
    "title": "Derivatives of Stochastic Gradient Descent in parametric optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Franck Iutzeler",
      "Edouard Pauwels",
      "Samuel Vaiter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d75660d6eb0ce31360c768fef85301dd-Abstract-Conference.html": {
    "title": "Hallo3D: Multi-Modal Hallucination Detection and Mitigation for Consistent 3D Content Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongbo Wang",
      "Jie Cao",
      "Jin Liu",
      "Xiaoqiang Zhou",
      "Huaibo Huang",
      "Ran He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d76c56082390d960499e14ccc157e6a1-Abstract-Conference.html": {
    "title": "Revisiting K-mer Profile for Effective and Scalable Genome Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdulkadir Celikkanat",
      "Andres Masegosa",
      "Thomas Nielsen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d77a7b289361abff82bdd2fb537ae152-Abstract-Conference.html": {
    "title": "Multi-turn Reinforcement Learning with Preference Human Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lior Shani",
      "Aviv Rosenberg",
      "Asaf Cassel",
      "Oran Lang",
      "Daniele Calandriello",
      "Avital Zipori",
      "Hila Noga",
      "Orgad Keller",
      "Bilal Piot",
      "Idan Szpektor",
      "Avinatan Hassidim",
      "Yossi Matias",
      "Remi Munos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d78ca21959823cf8aa8e26d359cf1105-Abstract-Conference.html": {
    "title": "Wasserstein Gradient Boosting: A Framework for Distribution-Valued Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takuo Matsubara"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d78d68cae595fabadd187b583ee8708e-Abstract-Conference.html": {
    "title": "Mixture of Experts Meets Prompt-Based Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Le",
      "An Nguyen The",
      "Huy Nguyen",
      "Trang Nguyen",
      "Trang Pham",
      "Linh Ngo",
      "Nhat Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d78ece6613953f46501b958b7bb4582f-Abstract-Conference.html": {
    "title": "LoD-Loc: Aerial Visual Localization using LoD 3D Map with Neural Wireframe Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juelin Zhu",
      "Shen Yan",
      "Long Wang",
      "zhang shengYue",
      "Yu Liu",
      "Maojun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d796312291ced0c8911d9b26dfe7fb84-Abstract-Conference.html": {
    "title": "STL: Still Tricky Logic (for System Validation, Even When Showing Your Work)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isabelle Hurley",
      "Rohan Paleja",
      "Ashley Suh",
      "Jaime D Pena",
      "Ho Chit Siu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d79792543133425ff79513c147dc8881-Abstract-Conference.html": {
    "title": "Algorithmic Collective Action in Recommender Systems: Promoting Songs by Reordering Playlists",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joachim Baumann",
      "Celestine Mendler-Dünner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d79c1390baa2e4835586b094d82e5ffb-Abstract-Conference.html": {
    "title": "Autoregressive Policy Optimization for Constrained Allocation Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Winkel",
      "Niklas Strauß",
      "Maximilian Bernhard",
      "Zongyue Li",
      "Thomas Seidl",
      "Matthias Schubert"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d7aa002885ccbe68cf6880da583761b2-Abstract-Conference.html": {
    "title": "ElasTST: Towards Robust Varied-Horizon Forecasting with Elastic Time-Series Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawen Zhang",
      "Shun Zheng",
      "Xumeng Wen",
      "Xiaofang Zhou",
      "Jiang Bian",
      "Jia Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d7ae0d47fe6a8dfeb6a149be03ea89ce-Abstract-Conference.html": {
    "title": "Optimal Transport-based Labor-free Text Prompt Modeling for Sketch Re-identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Li",
      "Tingting Ren",
      "Jie Wen",
      "Jinxing Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d7ae378b9dfa7ff7bed35105fab03838-Abstract-Conference.html": {
    "title": "Unlocking the Potential of Global Human Expertise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elliot Meyerson",
      "Olivier Francon",
      "Darren Sargent",
      "Babak Hodjat",
      "Risto Miikkulainen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d7af02c8a8e26608199c087f50a21d37-Abstract-Conference.html": {
    "title": "Physically Compatible 3D Object Modeling from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghao Guo",
      "Bohan Wang",
      "Pingchuan Ma",
      "Tianyuan Zhang",
      "Crystal Owens",
      "Chuang Gan",
      "Josh Tenenbaum",
      "Kaiming He",
      "Wojciech Matusik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d7b351608d824a4680344a02b180a947-Abstract-Conference.html": {
    "title": "Fetch and Forge: Efficient Dataset Condensation for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ding Qi",
      "Jian Li",
      "Jinlong Peng",
      "Bo Zhao",
      "Shuguang Dou",
      "Jialin Li",
      "Jiangning Zhang",
      "Yabiao Wang",
      "Chengjie Wang",
      "Cairong Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d7cb9db5ade2db7814fbd01ee59f4c7b-Abstract-Conference.html": {
    "title": "Multiple Physics Pretraining for Spatiotemporal Surrogate Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael McCabe",
      "Bruno Régaldo-Saint Blancard",
      "Liam Parker",
      "Ruben Ohana",
      "Miles Cranmer",
      "Alberto Bietti",
      "Michael Eickenberg",
      "Siavash Golkar",
      "Geraud Krawezik",
      "Francois Lanusse",
      "Mariel Pettee",
      "Tiberiu Tesileanu",
      "Kyunghyun Cho",
      "Shirley Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d7ce06e9293c3d8e6cb3f80b4157f875-Abstract-Conference.html": {
    "title": "Streaming Long Video Understanding with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Qian",
      "Xiaoyi Dong",
      "Pan Zhang",
      "Yuhang Zang",
      "Shuangrui Ding",
      "Dahua Lin",
      "Jiaqi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d7e7ab160ebcece4f80d1315f957a5ce-Abstract-Conference.html": {
    "title": "Geometry Cloak: Preventing TGS-based 3D Reconstruction from Copyrighted Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Song",
      "Ziyuan Luo",
      "Ka Chun Cheung",
      "Simon See",
      "Renjie Wan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d7ed243b13831bdd468f35039936bcef-Abstract-Conference.html": {
    "title": "Tackling Uncertain Correspondences for Multi-Modal Entity Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyi Chen",
      "Ying Sun",
      "Shengzhe Zhang",
      "Yuyang Ye",
      "Wei Wu",
      "Hui Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d80f533473c2dc8dd6984ceffc3ed3fd-Abstract-Conference.html": {
    "title": "Diversity-Driven Synthesis: Enhancing Dataset Distillation through Directed Weight Adjustment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JIAWEI DU",
      "xin zhang",
      "Juncheng Hu",
      "Wenxin Huang",
      "Joey Tianyi Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d81cb1f4dc6e13aeb45553f80b3d6837-Abstract-Conference.html": {
    "title": "MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongshen Zeng",
      "Yinhong Liu",
      "Yingjia Wan",
      "Jingyao Li",
      "Pengguang Chen",
      "Jianbo Dai",
      "Yuxuan Yao",
      "Rongwu Xu",
      "Zehan Qi",
      "Wanru Zhao",
      "Linling Shen",
      "Jianqiao Lu",
      "Haochen Tan",
      "Yukang Chen",
      "Hao Zhang",
      "Zhan Shi",
      "Bailin Wang",
      "Zhijiang Guo",
      "Jiaya Jia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d8398f4da88975e2a9c62ecaa5ba267b-Abstract-Conference.html": {
    "title": "Learning diverse causally emergent representations from time series data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David McSharry",
      "Christos Kaplanis",
      "Fernando Rosas",
      "Pedro A.M Mediano"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d83d04f40961442fa31dd2552debd0e9-Abstract-Conference.html": {
    "title": "How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingwu Chen",
      "Lei Zhao",
      "Difan Zou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d83fd70a31c64e020844ec80705ba87f-Abstract-Conference.html": {
    "title": "Diffusion Policy Attacker: Crafting Adversarial Attacks for Diffusion-based Policies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yipu Chen",
      "Haotian Xue",
      "Yongxin Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html": {
    "title": "InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaojun Xiao",
      "Pengle Zhang",
      "Xu Han",
      "Guangxuan Xiao",
      "Yankai Lin",
      "Zhengyan Zhang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d848cb2c84f0bba7f1f73cf232734c40-Abstract-Conference.html": {
    "title": "ALPINE: Unveiling The Planning Capability of Autoregressive Learning in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siwei Wang",
      "Yifei Shen",
      "Shi Feng",
      "Haoran Sun",
      "Shang-Hua Teng",
      "Wei Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d85030334fadbd55043c911076caf0ae-Abstract-Conference.html": {
    "title": "Latent Paraphrasing: Perturbation on Layers Improves Knowledge Injection in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minki Kang",
      "Sung Ju Hwang",
      "Gibbeum Lee",
      "Jaewoong Cho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d850b7e0cdc7f1c0820c6ad85405ae94-Abstract-Conference.html": {
    "title": "Articulate your NeRF: Unsupervised articulated object modeling via conditional view synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianning Deng",
      "Kartic Subr",
      "Hakan Bilen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d8577a93072ac583c1618746d757e411-Abstract-Conference.html": {
    "title": "Fearless Stochasticity in Expectation Propagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan So",
      "Richard Turner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d8684e49752e06ac5e4b554b60ad212a-Abstract-Conference.html": {
    "title": "Most Influential Subset Selection: Challenges, Promises, and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzheng Hu",
      "Pingbang Hu",
      "Han Zhao",
      "Jiaqi Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d885c74aa0e00cc07a35346aa7988e34-Abstract-Conference.html": {
    "title": "Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nazar Buzun",
      "Maksim Bobrin",
      "Dmitry V. Dylov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d8a6eb79f8ccaacbe7198a5caf3a0323-Abstract-Conference.html": {
    "title": "Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichong Huang",
      "Xiaocheng Feng",
      "Baohang Li",
      "Yang Xiang",
      "Hui Wang",
      "Ting Liu",
      "Bing Qin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d8ca28a32c05cd3b9b0940e43720f31b-Abstract-Conference.html": {
    "title": "OPUS: Occupancy Prediction Using a Sparse Set",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JiaBao Wang",
      "Zhaojiang Liu",
      "Qiang Meng",
      "Liujiang Yan",
      "Ke Wang",
      "JIE YANG",
      "Wei Liu",
      "Qibin Hou",
      "Ming-Ming Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d8e80772c27beff4ae1676fb147bbf26-Abstract-Conference.html": {
    "title": "Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Behrouz",
      "Michele Santacatterina",
      "Ramin Zabih"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d8efbb5dd415974eb095c3f06bff1f48-Abstract-Conference.html": {
    "title": "AutoGuide: Automated Generation and Selection of Context-Aware Guidelines for Large Language Model Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Fu",
      "Dong-Ki Kim",
      "Jaekyeom Kim",
      "Sungryull Sohn",
      "Lajanugen Logeswaran",
      "Kyunghoon Bae",
      "Honglak Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d8f17478f594f0e22df4bb0260406c09-Abstract-Conference.html": {
    "title": "Aligning Diffusion Behaviors with Q-functions for Efficient Continuous Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huayu Chen",
      "Kaiwen Zheng",
      "Hang Su",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d8f5f134febb4bd74d8f79e338de382c-Abstract-Conference.html": {
    "title": "Label Delay in Online Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Botos Csaba",
      "Wenxuan Zhang",
      "Matthias Müller",
      "Ser Nam Lim",
      "Philip Torr",
      "Adel Bibi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d91ffbe9c126765755ff52d36b715683-Abstract-Conference.html": {
    "title": "Conjugate Bayesian Two-step Change Point Detection for Hawkes Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyue Zhang",
      "Xiaoling LU",
      "Feng Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d9251dc20346dffe9b6db86dcc6f8cc9-Abstract-Conference.html": {
    "title": "Hybrid Reinforcement Learning Breaks Sample Size Barriers In Linear MDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Tan",
      "Wei Fan",
      "Yuting Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d9253fba38ed8a140f86fa22d89344ec-Abstract-Conference.html": {
    "title": "Tangent Space Causal Inference: Leveraging Vector Fields for Causal Discovery in Dynamical Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kurt Butler",
      "Daniel Waxman",
      "Petar Djuric"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d94e778e4cfabddec0cb6d38ab57accd-Abstract-Conference.html": {
    "title": "Learning from Offline Foundation Features with Tensor Augmentations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emir Konuk",
      "Christos Matsoukas",
      "Moein Sorkhei",
      "Phitchapha Lertsiravarameth",
      "Kevin Smith"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d951f73c521d069fefbb73396df01424-Abstract-Conference.html": {
    "title": "Can large language models explore in-context?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Krishnamurthy",
      "Keegan Harris",
      "Dylan J Foster",
      "Cyril Zhang",
      "Aleksandrs Slivkins"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d95cb79a3421e6d9b6c9a9008c4d07c5-Abstract-Conference.html": {
    "title": "NoiseGPT: Label Noise Detection and Rectification through Probability Curvature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Wang",
      "Zhuo Huang",
      "Zhiwei Lin",
      "Tongliang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d96fcc07d623a9eba68616629911143a-Abstract-Conference.html": {
    "title": "Test-Time Adaptation Induces Stronger Accuracy and Agreement-on-the-Line",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eungyeup Kim",
      "Mingjie Sun",
      "Christina Baek",
      "Aditi Raghunathan",
      "J. Zico Kolter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d978cd64d598bbaf6a2fff1d6864d601-Abstract-Conference.html": {
    "title": "On the Expressivity and Sample Complexity of Node-Individualized Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paolo Pellizzoni",
      "Till Hendrik Schulz",
      "Dexiong Chen",
      "Karsten Borgwardt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d99e8e80a6c41e148db686918dd7eab3-Abstract-Conference.html": {
    "title": "Conditional Controllable Image Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bing Cao",
      "Xingxin Xu",
      "Pengfei Zhu",
      "Qilong Wang",
      "Qinghua Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/d9c7c8bd6ad4cebb7d006e5109e0b682-Abstract-Conference.html": {
    "title": "Neural P$^3$M: A Long-Range Interaction Modeling Enhancer for Geometric GNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusong Wang",
      "Chaoran Cheng",
      "Shaoning Li",
      "Yuxuan Ren",
      "Bin Shao",
      "Ge Liu",
      "Pheng-Ann Heng",
      "Nanning Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/da07cfa60cc883c5ee94ba899383bb6d-Abstract-Conference.html": {
    "title": "Understanding and Improving Adversarial Collaborative Filtering for Robust Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaike Zhang",
      "Qi Cao",
      "Yunfan Wu",
      "Fei Sun",
      "Huawei Shen",
      "Xueqi Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/da221e9ad41b0cd22dc79d4675bb1f9f-Abstract-Conference.html": {
    "title": "GL-NeRF: Gauss-Laguerre Quadrature Enables Training-Free NeRF Acceleration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Silong Yong",
      "Yaqi Xie",
      "Simon Stepputtis",
      "Katia Sycara"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/da30215ee52c1daaaaddada8137cfd0b-Abstract-Conference.html": {
    "title": "Scalable Bayesian Optimization via Focalized Sparse Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunyue Wei",
      "Vincent Zhuang",
      "Saraswati Soedarmadji",
      "Yanan Sui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/da36187b68fb72c3fe1c0eaec638221c-Abstract-Conference.html": {
    "title": "ZeroMark: Towards Dataset Ownership Verification without Disclosing Watermark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfeng Guo",
      "Yiming Li",
      "Ruibo Chen",
      "Yihan Wu",
      "chenxi liu",
      "Heng Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/da5498f88193ff61f0daea1940b819da-Abstract-Conference.html": {
    "title": "Meaningful Learning: Enhancing Abstract Reasoning in Large Language Models via Generic Fact Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Xiong",
      "Xiao Ding",
      "Ting Liu",
      "Bing Qin",
      "Dongliang Xu",
      "Qing Yang",
      "Hongtao Liu",
      "Yixin Cao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/da733d44e4be3902d952d6c1ffcb7db6-Abstract-Conference.html": {
    "title": "EGSST: Event-based Graph Spatiotemporal Sensitive Transformer for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Wu",
      "Hang Sheng",
      "Hui Feng",
      "Bo Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/da75d2bbf862b86f10241d0887613b41-Abstract-Conference.html": {
    "title": "Learning the Latent Causal Structure for Modeling Label Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yexiong Lin",
      "Yu Yao",
      "Tongliang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/da76884a4e003ad0de97804ec4578e5b-Abstract-Conference.html": {
    "title": "Unleashing Region Understanding in Intermediate Layers for MLLM-based Referring Expression Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaoyuan Liang",
      "Zhuojun Cai",
      "Jian Xu",
      "Guanbo Huang",
      "Yiran Wang",
      "Xiao Liang",
      "Jiahao Liu",
      "Ziran Li",
      "Jingang Wang",
      "Shao-Lun Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/da84e39ae51fd26bb5110d9659c06e13-Abstract-Conference.html": {
    "title": "Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dylan J Foster",
      "Adam Block",
      "Dipendra Misra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/da8b52ece0bbae748f8d9c16d2328bfa-Abstract-Conference.html": {
    "title": "Dimension-free Private Mean Estimation for Anisotropic Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuval Dagan",
      "Michael I. Jordan",
      "Xuelin Yang",
      "Lydia Zakynthinou",
      "Nikita Zhivotovskiy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/da9e48b452127d5a84cd70a3c9c83a78-Abstract-Conference.html": {
    "title": "DreamSteerer: Enhancing Source Image Conditioned Editability using Personalized Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyang Yu",
      "Zhaoyuan Yang",
      "Jing Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dad28e90cd2c8caedf362d49c4d99e70-Abstract-Conference.html": {
    "title": "Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raffaele Paolino",
      "Sohir Maskey",
      "Pascal Welke",
      "Gitta Kutyniok"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dae8afc6b990aa0b3b5efaa096fbd7fa-Abstract-Conference.html": {
    "title": "Newton Informed Neural Operator for Solving Nonlinear Partial Differential Equations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenrui Hao",
      "Xinliang Liu",
      "Yahong Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/daef77101ba5711084a57442c8cf2709-Abstract-Conference.html": {
    "title": "Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangxin Zhou",
      "Dongyu Xue",
      "Ruizhe Chen",
      "Zaixiang Zheng",
      "Liang Wang",
      "Quanquan Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/db015b65da0343e504c250a76b8b6791-Abstract-Conference.html": {
    "title": "Towards a Scalable Reference-Free Evaluation of Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Azim Ospanov",
      "Jingwei Zhang",
      "Mohammad Jalali",
      "Xuenan Cao",
      "Andrej Bogdanov",
      "Farzan Farnia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/db09e35a316862e37786179af78605a1-Abstract-Conference.html": {
    "title": "EMVP: Embracing Visual Foundation Model for Visual Place Recognition with Centroid-Free Probing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qibo Qiu",
      "Shun Zhang",
      "Haiming Gao",
      "Honghui Yang",
      "Haochao Ying",
      "Wenxiao Wang",
      "Xiaofei He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/db0ee27cb50dd9087b133f6e7d28a90e-Abstract-Conference.html": {
    "title": "Parameter Disparities Dissection for Backdoor Defense in Heterogeneous Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenke Huang",
      "Mang Ye",
      "Zekun Shi",
      "Guancheng Wan",
      "He Li",
      "Bo Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/db1d5c63576587fc1d40d33a75190c71-Abstract-Conference.html": {
    "title": "PSL: Rethinking and Improving Softmax Loss from Pairwise Perspective for Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiqin Yang",
      "Jiawei Chen",
      "Xin Xin",
      "Sheng Zhou",
      "Binbin Hu",
      "Yan Feng",
      "Chun Chen",
      "Can Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/db2808c312de68420ef20f055b726549-Abstract-Conference.html": {
    "title": "An Autoencoder-Like Nonnegative Matrix Co-Factorization for Improved Student Cognitive Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenbao Yu",
      "Yinghui Pan",
      "Yifeng Zeng",
      "Prashant Doshi",
      "Guoquan Liu",
      "Kim-Leng Poh",
      "Mingwei Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/db36f4d603cc9e3a2a5e10b93e6428f2-Abstract-Conference.html": {
    "title": "PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanxu Meng",
      "Zhaohui Wang",
      "Muhan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/db48d94a42706019262ed8304fa658c5-Abstract-Conference.html": {
    "title": "Provably Efficient Interactive-Grounded Learning with Personalized Reward",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxiao Zhang",
      "Yuheng Zhang",
      "Haipeng Luo",
      "Paul Mineiro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/db5ca61dbc08cf5143c05ad2d1b0b2ca-Abstract-Conference.html": {
    "title": "ANT: Adaptive Noise Schedule for Time Series Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghan Lee",
      "Kibok Lee",
      "Taeyoung Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/db7a81128155d6f14970c12d0b5e7a4c-Abstract-Conference.html": {
    "title": "Suitable is the Best: Task-Oriented Knowledge Fusion in Vulnerability Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingjing Wang",
      "Minhuan Huang",
      "yuanping nie",
      "Xiang Li",
      "Qianjin Du",
      "Wei Kong",
      "Huan Deng",
      "Xiaohui Kuang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/db93ccb6cf392f352570dd5af0a223d3-Abstract-Conference.html": {
    "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Yu",
      "Wei Ping",
      "Zihan Liu",
      "Boxin Wang",
      "Jiaxuan You",
      "Chao Zhang",
      "Mohammad Shoeybi",
      "Bryan Catanzaro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/db988b089d8d97d0f159c15ed0be6a71-Abstract-Conference.html": {
    "title": "Convolutional Differentiable Logic Gate Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Petersen",
      "Hilde Kuehne",
      "Christian Borgelt",
      "Julian Welzel",
      "Stefano Ermon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dbae915128892556134f1c5375855590-Abstract-Conference.html": {
    "title": "Discrete Modeling via Boundary Conditional Diffusion Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Gu",
      "Xiaocheng Feng",
      "Lei Huang",
      "Yingsheng Wu",
      "Zekun Zhou",
      "Weihong Zhong",
      "kun Zhu",
      "Bing Qin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dbb5180957513805ebeea787b8c66ac9-Abstract-Conference.html": {
    "title": "SampDetox: Black-box Backdoor Defense via Perturbation-based Sample Detoxification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanxin Yang",
      "Chentao Jia",
      "DengKe Yan",
      "Ming Hu",
      "Tianlin Li",
      "Xiaofei Xie",
      "Xian Wei",
      "Mingsong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dbd07478c4aac41c0ce411e12f2e5a28-Abstract-Conference.html": {
    "title": "RashomonGB: Analyzing the Rashomon Effect and Mitigating Predictive Multiplicity in Gradient Boosting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hsiang Hsu",
      "Ivan Brugere",
      "Shubham Sharma",
      "Freddy Lecue",
      "Richard Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dbdea7859f1d2fc10f2c9e79b8f5ae54-Abstract-Conference.html": {
    "title": "Federated Natural Policy Gradient and Actor Critic Methods for Multi-task Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Yang",
      "Shicong Cen",
      "Yuting Wei",
      "Yuxin Chen",
      "Yuejie Chi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dbe2cfe4767f3255160b73a36ae3162e-Abstract-Conference.html": {
    "title": "Extending Video Masked Autoencoders to 128 frames",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nitesh Bharadwaj Gundavarapu",
      "Luke Friedman",
      "Raghav Goyal",
      "Chaitra Hegde",
      "Eirikur Agustsson",
      "Sagar Waghmare",
      "Mikhail Sirotenko",
      "Ming-Hsuan Yang",
      "Tobias Weyand",
      "Boqing Gong",
      "Leonid Sigal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dbe81b08f7dc4dd8b43bc62dedfd9662-Abstract-Conference.html": {
    "title": "Robust Contrastive Multi-view Clustering against Dual Noisy Correspondence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiming Guo",
      "Mouxing Yang",
      "Yijie Lin",
      "Xi Peng",
      "Peng Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dbeb7e621d4a554069a6a775da0f7273-Abstract-Conference.html": {
    "title": "Toward Semantic Gaze Target Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samy Tafasca",
      "Anshul Gupta",
      "Victor Bros",
      "Jean-marc Odobez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dbfb7b1443583fc7ab87e8b1b4f48c9c-Abstract-Conference.html": {
    "title": "Conformalized Time Series with Semantic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baiting Chen",
      "Zhimei Ren",
      "Lu Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dc06d4d2792265fb5454a6092bfd5c6a-Abstract-Conference.html": {
    "title": "CogVLM: Visual Expert for Pretrained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihan Wang",
      "Qingsong Lv",
      "Wenmeng Yu",
      "Wenyi Hong",
      "Ji Qi",
      "Yan Wang",
      "Junhui Ji",
      "Zhuoyi Yang",
      "Lei Zhao",
      "Song XiXuan",
      "Jiazheng Xu",
      "Keqin Chen",
      "Bin Xu",
      "Juanzi Li",
      "Yuxiao Dong",
      "Ming Ding",
      "Jie Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dc2ccde7ee43e5719e08c68e848bd65a-Abstract-Conference.html": {
    "title": "MeLLoC: Lossless Compression with High-order Mechanism Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyue Luo",
      "Jin Cheng",
      "Yu Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dc300c4d75b94b211b149ae4bcbbf2d2-Abstract-Conference.html": {
    "title": "Unlock the Intermittent Control Ability of Model Free Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiashun Liu",
      "Jianye Hao",
      "Xiaotian Hao",
      "Yi Ma",
      "YAN ZHENG",
      "Yujing Hu",
      "Tangjie Lv"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dc36c213f20300d1381520b0ce0c7788-Abstract-Conference.html": {
    "title": "Neuc-MDS: Non-Euclidean Multidimensional Scaling Through Bilinear Forms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyuan Deng",
      "Jie Gao",
      "Kevin Lu",
      "Feng Luo",
      "Hongbin Sun",
      "Cheng Xin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dc3a0fd5ef0f78ee5934919230d6664f-Abstract-Conference.html": {
    "title": "No Free Lunch Theorem and Black-Box Complexity Analysis for Adversarial Optimisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Per Kristian Lehre",
      "Shishen Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dc4db2ff2c1aefce3b594f821ea82fe6-Abstract-Conference.html": {
    "title": "The Group Robustness is in the Details: Revisiting Finetuning under Spurious Correlations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tyler LaBonte",
      "John Hill",
      "Xinchen Zhang",
      "Vidya Muthukumar",
      "Abhishek Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dc4f891373d19087d1ddda33e81e00e4-Abstract-Conference.html": {
    "title": "Learning Identifiable Factorized Causal Representations of Cellular Responses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyi Mao",
      "Romain Lopez",
      "Kai Liu",
      "Jan-Christian Huetter",
      "David Richmond",
      "Panayiotis Benos",
      "Lin Qiu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dc6319dde4fb182b22fb902da9418566-Abstract-Conference.html": {
    "title": "SimVG: A Simple Framework for Visual Grounding with Decoupled Multi-modal Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Dai",
      "Lingfeng Yang",
      "Yihao Xu",
      "Zhenhua Feng",
      "Wankou Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dc6748383752138af7f00b3185a0a404-Abstract-Conference.html": {
    "title": "Utilizing Image Transforms and Diffusion Models for Generative Modeling of Short and Long Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilan Naiman",
      "Nimrod Berman",
      "Itai Pemper",
      "Idan Arbiv",
      "Gal Fadlon",
      "Omri Azencot"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dc709714c52b35f2f34aca2a92b06bc8-Abstract-Conference.html": {
    "title": "Collaboration! Towards Robust Neural Methods for Routing Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianan Zhou",
      "Yaoxin Wu",
      "Zhiguang Cao",
      "Wen Song",
      "Jie Zhang",
      "Zhiqi Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dc72529d604962a86b7730806b6113fa-Abstract-Conference.html": {
    "title": "Subsurface Scattering for Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan-Niklas Dihlmann",
      "Arjun Majumdar",
      "Andreas Engelhardt",
      "Raphael Braun",
      "Hendrik PA Lensch"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dc89a0709f213fd0ac4b1172719b2c38-Abstract-Conference.html": {
    "title": "A Simple Framework for Generalization in Visual RL under Dynamic Scene Perturbations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonil Song",
      "Hyesong Choi",
      "Kwanghoon Sohn",
      "Dongbo Min"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dc9544b26ad3579477e567588db18cfc-Abstract-Conference.html": {
    "title": "Spherical Frustum Sparse Convolution Network for LiDAR Point Cloud Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zheng",
      "Guangming Wang",
      "Jiuming Liu",
      "Marc Pollefeys",
      "Hesheng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dc970c91c0a82c6e4cb3c4af7bff5388-Abstract-Conference.html": {
    "title": "Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wu Shuang",
      "Youtian Lin",
      "Yifei Zeng",
      "Feihu Zhang",
      "Jingxi Xu",
      "Philip Torr",
      "Xun Cao",
      "Yao Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dc9e095f668044e7a0909a4ea3926beb-Abstract-Conference.html": {
    "title": "Optimal Multi-Fidelity Best-Arm Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Poiani",
      "Rémy Degenne",
      "Emilie Kaufmann",
      "Alberto Maria Metelli",
      "Marcello Restelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dcbc5a5705565452057a8df872595d5c-Abstract-Conference.html": {
    "title": "Adaptive Experimentation When You Can't Experiment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Zhao",
      "Kwang-Sung Jun",
      "Tanner Fiez",
      "Lalit Jain"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dcc0ac74ac8b95dc1939804acce0317d-Abstract-Conference.html": {
    "title": "Accelerating Non-Maximum Suppression: A Graph Theory Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "King-Siong Si",
      "Lu Sun",
      "Weizhan Zhang",
      "Tieliang Gong",
      "Jiahao Wang",
      "Jiang Liu",
      "Hao Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dcc337bb2a4d25afefd9ab800721debb-Abstract-Conference.html": {
    "title": "Targeted Sequential Indirect Experiment Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elisabeth Ailer",
      "Niclas Dern",
      "Jason S Hartford",
      "Niki Kilbertus"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dccbeb7a8df3065c4646928985edf435-Abstract-Conference.html": {
    "title": "On Learning Multi-Modal Forgery Representation for Diffusion Generated Video Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiufeng Song",
      "Xiao Guo",
      "Jiache Zhang",
      "Qirui Li",
      "LEI BAI",
      "Xiaoming Liu",
      "Guangtao Zhai",
      "Xiaohong Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dcd297696d0bb304ba426b3c5a679c37-Abstract-Conference.html": {
    "title": "Robot Policy Learning with Temporal Optimal Transport Reward",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Fu",
      "Haichao Zhang",
      "Di Wu",
      "Wei Xu",
      "Benoit Boulet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dcebcd32dfabf7c917692c8a9855a351-Abstract-Conference.html": {
    "title": "EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Zhao",
      "Hangjie Yuan",
      "Yujie Wei",
      "Shiwei Zhang",
      "Yuchao Gu",
      "Lingmin Ran",
      "Xiang Wang",
      "Jay Zhangjie Wu",
      "David Junhao Zhang",
      "Yingya Zhang",
      "Mike Zheng Shou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dcf51b2eef1af32e594177b326f8cff9-Abstract-Conference.html": {
    "title": "A Non-parametric Direct Learning Approach to Heterogeneous Treatment Effect Estimation under Unmeasured Confounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhai Zhang",
      "Xingye Qiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dcf88cbc8d01ce7309b83d0ebaeb9d29-Abstract-Conference.html": {
    "title": "AutoTimes: Autoregressive Time Series Forecasters via Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yong Liu",
      "Guo Qin",
      "Xiangdong Huang",
      "Jianmin Wang",
      "Mingsheng Long"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dcfb4b195fe1ec750ef13312974b620e-Abstract-Conference.html": {
    "title": "UNIT: Unifying Image and Text Recognition in One Vision Encoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zhu",
      "Zhou Yanpeng",
      "Chunwei Wang",
      "Yang Cao",
      "Jianhua Han",
      "Lu Hou",
      "Hang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dd0ef9308bd10c964bd14c0000438460-Abstract-Conference.html": {
    "title": "Direct Preference-Based Evolutionary Multi-Objective Optimization with Dueling Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Huang",
      "Shengbo Wang",
      "Ke Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dd1fef536655685898a6602bfbf16857-Abstract-Conference.html": {
    "title": "Zipfian Whitening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sho Yokoi",
      "Han Bao",
      "Hiroto Kurita",
      "Hidetoshi Shimodaira"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dd2eb5250696753ea37141bbd89bb569-Abstract-Conference.html": {
    "title": "Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekun Shi",
      "Zheyuan Hu",
      "Min Lin",
      "Kenji Kawaguchi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dd2eb673b257e6827660abf3475f131d-Abstract-Conference.html": {
    "title": "Synergistic Dual Spatial-aware Generation of Image-to-text and Text-to-image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhao",
      "Hao Fei",
      "Xiangtai Li",
      "Libo Qin",
      "Jiayi Ji",
      "Hongyuan Zhu",
      "Meishan Zhang",
      "Min Zhang",
      "Jianguo Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dd3065a00d9b93e3d4d17faa907100bb-Abstract-Conference.html": {
    "title": "Improving Neural Network Surface Processing with Principal Curvatures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josquin Harrison",
      "James Benn",
      "Maxime Sermesant"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dd37fdb24a4e1cfa3ed5c247217a7394-Abstract-Conference.html": {
    "title": "Evaluating alignment between humans and neural network representations in image-based learning tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Can Demircan",
      "Tankred Saanum",
      "Leonardo Pettini",
      "Marcel Binz",
      "Blazej Baczkowski",
      "Christian Doeller",
      "Mona Garvert",
      "Eric Schulz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dd51dbce305433cd60910dc5b0147be4-Abstract-Conference.html": {
    "title": "LP-3DGS: Learning to Prune 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoliang Zhang",
      "Tianchen Song",
      "Yongjae Lee",
      "Li Yang",
      "Cheng Peng",
      "Rama Chellappa",
      "Deliang Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dd540e1c8d26687d56d296e64d35949f-Abstract-Conference.html": {
    "title": "Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuomas Kynkäänniemi",
      "Miika Aittala",
      "Tero Karras",
      "Samuli Laine",
      "Timo Aila",
      "Jaakko Lehtinen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dd59fad18638714e6c447a3b7b9c4160-Abstract-Conference.html": {
    "title": "A Sober Look at the Robustness of CLIPs to Spurious Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qizhou Wang",
      "Yong Lin",
      "Yongqiang Chen",
      "Ludwig Schmidt",
      "Bo Han",
      "Tong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dd65d612d2ddafd54ef5eceb92f1a754-Abstract-Conference.html": {
    "title": "Non-Asymptotic Uncertainty Quantification in High-Dimensional Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederik Hoppe",
      "Claudio Mayrink Verdun",
      "Hannah Laus",
      "Felix Krahmer",
      "Holger Rauhut"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dd6a47bc0aad6f34aa5e77706d90cdc4-Abstract-Conference.html": {
    "title": "DiffuserLite: Towards Real-time Diffusion Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zibin Dong",
      "Jianye Hao",
      "Yifu Yuan",
      "Fei Ni",
      "Yitian Wang",
      "Pengyi Li",
      "YAN ZHENG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dd73f39426a03131c38c8d943153d44b-Abstract-Conference.html": {
    "title": "GLinSAT: The General Linear Satisfiability Neural Network Layer By Accelerated Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongtai Zeng",
      "Chao Yang",
      "Yanzhen Zhou",
      "Cheng Yang",
      "Qinglai Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dd745a0c4f91fe91866fe6788be9cc28-Abstract-Conference.html": {
    "title": "Randomized Sparse Matrix Compression for Large-Scale Constrained Optimization in Cancer Radiotherapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shima Adeli",
      "Mojtaba Tefagh",
      "Gourav Jhanwar",
      "Masoud Zarepisheh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dd850be1e74582b68fa82078f3fc6d4d-Abstract-Conference.html": {
    "title": "On the Optimal Time Complexities in Decentralized Stochastic Asynchronous Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Tyurin",
      "Peter Richtarik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dd8e7dae18cecd7c9137840161e1bf62-Abstract-Conference.html": {
    "title": "Prompt Optimization with EASE? Efficient Ordering-aware Automated Selection of Exemplars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoxuan Wu",
      "Xiaoqiang Lin",
      "Zhongxiang Dai",
      "Wenyang Hu",
      "Yao Shu",
      "See-Kiong Ng",
      "Patrick Jaillet",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dda5cac5272a9bcd4bc73d90bc725ef1-Abstract-Conference.html": {
    "title": "EMR-Merging: Tuning-Free High-Performance Model Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyu Huang",
      "Peng Ye",
      "Tao Chen",
      "Tong He",
      "Xiangyu Yue",
      "Wanli Ouyang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ddb0a18cc21b98fffb4b69c43f9b56f5-Abstract-Conference.html": {
    "title": "On the Convergence of Loss and Uncertainty-based Active Learning Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Haimovich",
      "Dima Karamshuk",
      "Fridolin Linder",
      "Niek Tax",
      "Milan Vojnovic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dde040998d82553cf7f689e8ae173d5a-Abstract-Conference.html": {
    "title": "Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Lyu",
      "Beitao Chen",
      "Lianli Gao",
      "Hengtao Shen",
      "Jingkuan Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ddfb58b61e7213dfb9e06c695475b2bd-Abstract-Conference.html": {
    "title": "DeepLag: Discovering Deep Lagrangian Dynamics for Intuitive Fluid Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qilong Ma",
      "Haixu Wu",
      "Lanxiang Xing",
      "Shangchen Miao",
      "Mingsheng Long"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/de076d0485c1fba8326500a860fe9274-Abstract-Conference.html": {
    "title": "Towards Neuron Attributions in Multi-Modal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfeng Fang",
      "Zac Bi",
      "Ruipeng Wang",
      "Houcheng Jiang",
      "Yuan Gao",
      "Kun Wang",
      "An Zhang",
      "Jie Shi",
      "Xiang Wang",
      "Tat-Seng Chua"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/de0da9c42ee713f2ceaeed7bc40c522d-Abstract-Conference.html": {
    "title": "IPM-LSTM: A Learning-Based Interior Point Method for Solving Nonlinear Programs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Gao",
      "Jinxin Xiong",
      "Akang Wang",
      "qihong duan",
      "Jiang Xue",
      "Qingjiang Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/de0e668df3fe63ec89e5a7e68f3d350f-Abstract-Conference.html": {
    "title": "Cross-Device Collaborative Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guohao Chen",
      "Shuaicheng Niu",
      "Deyu Chen",
      "Shuhai Zhang",
      "Changsheng Li",
      "Yuanqing Li",
      "Mingkui Tan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/de0f2a9943b7bd060e5c10c2fb2654f3-Abstract-Conference.html": {
    "title": "Procedure-Aware Surgical Video-language Pretraining with Hierarchical Knowledge Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Yuan",
      "vinkle srivastav",
      "Nassir Navab",
      "Nicolas Padoy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/de18052eb17cce56554a9637dd5aadba-Abstract-Conference.html": {
    "title": "Decomposed Prompt Decision Transformer for Efficient Unseen Task Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongling Zheng",
      "Li Shen",
      "Yong Luo",
      "Tongliang Liu",
      "Jialie Shen",
      "Dacheng Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/de7858e3e7f9f0f7b2c7bfdc86f6d928-Abstract-Conference.html": {
    "title": "DiffuLT: Diffusion for Long-tail Recognition Without External Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Shao",
      "Ke Zhu",
      "Hanxiao Zhang",
      "Jianxin Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/de85d3cff8512f72fd50b862979f1731-Abstract-Conference.html": {
    "title": "Prospective Learning: Learning for a Dynamic Future",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashwin De Silva",
      "Rahul Ramesh",
      "Rubing Yang",
      "Siyu Yu",
      "Joshua T Vogelstein",
      "Pratik Chaudhari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/deb0e85779f2f003b10528de72b1ebf1-Abstract-Conference.html": {
    "title": "MetaCURL: Non-stationary Concave Utility Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bianca Marin Moreno",
      "Margaux Brégère",
      "Pierre Gaillard",
      "Nadia Oudjane"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/decd42d78c42cea59c95c7c3d40d5e0f-Abstract-Conference.html": {
    "title": "UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yansong Ning",
      "Hao Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ded49449e9239a47057c33ef8d42c26e-Abstract-Conference.html": {
    "title": "MotionCraft: Physics-Based Zero-Shot Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonio Montanaro",
      "Luca Savant Aira",
      "Emanuele Aiello",
      "Diego Valsesia",
      "Enrico Magli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/deddcfbf08f57489b0088b71a00db640-Abstract-Conference.html": {
    "title": "Not Just Object, But State: Compositional Incremental Learning without Forgetting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanyi Zhang",
      "Binglin Qiu",
      "Qi Jia",
      "Yu Liu",
      "Ran He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/deeb4d6bdb5860fd7faf321dd5486d25-Abstract-Conference.html": {
    "title": "What Factors Affect Multi-Modal In-Context Learning? An In-Depth Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Libo Qin",
      "Qiguang Chen",
      "Hao Fei",
      "Zhi Chen",
      "Min Li",
      "Wanxiang Che"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df027cf11469e746ef94d583f9f5537f-Abstract-Conference.html": {
    "title": "Multi-Object 3D Grounding with Dynamic Modules and Language-Informed Spatial Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haomeng Zhang",
      "Chiao-An Yang",
      "Raymond A. Yeh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df04a35d907e894d59d4eab1f92bc87b-Abstract-Conference.html": {
    "title": "Continuously Learning, Adapting, and Improving: A Dual-Process Approach to Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianbiao Mei",
      "Yukai Ma",
      "Xuemeng Yang",
      "Licheng Wen",
      "Xinyu Cai",
      "Xin Li",
      "Daocheng Fu",
      "Bo Zhang",
      "Pinlong Cai",
      "Min Dou",
      "Botian Shi",
      "Liang He",
      "Yong Liu",
      "Yu Qiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df0e4ffa41062ca002f1f68c73269802-Abstract-Conference.html": {
    "title": "Stochastic Optimal Control and Estimation with Multiplicative and Internal Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Damiani",
      "Akiyuki Anzai",
      "Jan Drugowitsch",
      "Gregory DeAngelis",
      "Ruben Moreno Bote"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df15617fd6d6f78fcd485401d0598761-Abstract-Conference.html": {
    "title": "Relational Verification Leaps Forward with RABBit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tarun Suresh",
      "Debangshu Banerjee",
      "Gagandeep Singh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df20e06b04dfc499a1a698796abb0856-Abstract-Conference.html": {
    "title": "DiffLight: A Partial Rewards Conditioned Diffusion Model for Traffic Signal Control with Missing Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyang Chen",
      "Yang Jiang",
      "Shengnan Guo",
      "Xiaowei Mao",
      "Youfang Lin",
      "Huaiyu Wan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df22a19686a558e74f038e6277a51f68-Abstract-Conference.html": {
    "title": "Latent Plan Transformer for Trajectory Abstraction: Planning as Latent Space Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deqian Kong",
      "Dehong Xu",
      "Minglu Zhao",
      "Bo Pang",
      "Jianwen Xie",
      "Andrew Lizarraga",
      "Yuhao Huang",
      "Sirui Xie",
      "Ying Nian Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df29d63af05cb91d705cf06ba5945b9d-Abstract-Conference.html": {
    "title": "Persistent Test-time Adaptation in Recurring Testing Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trung Hieu Hoang",
      "MinhDuc Vo",
      "Minh Do"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df2d51e1d3e899241c5c4c779c1d509f-Abstract-Conference.html": {
    "title": "GraphTrail: Translating GNN Predictions into Human-Interpretable Logical Rules",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Burouj Armgaan",
      "Manthan Dalmia",
      "Sourav Medya",
      "Sayan Ranu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df2d62b96a4003203450cf89cd338bb7-Abstract-Conference.html": {
    "title": "Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Guan",
      "Xiangyu Kong",
      "Fangwei Zhong",
      "Yizhou Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df334022279996b07e0870a629c18857-Abstract-Conference.html": {
    "title": "Deep Learning Through A Telescoping Lens: A Simple Model Provides Empirical Insights On Grokking, Gradient Boosting & Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alan Jeffares",
      "Alicia Curth",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df3e24cc1755e332fab642871bc70a77-Abstract-Conference.html": {
    "title": "Towards Understanding Extrapolation: a Causal Lens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingjing Kong",
      "Guangyi Chen",
      "Petar Stojanov",
      "Haoxuan Li",
      "Eric P. Xing",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df40e4147b55744ab782b09ccde054db-Abstract-Conference.html": {
    "title": "Subject-driven Text-to-Image Generation via Preference-based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanting Miao",
      "William Loh",
      "Suraj Kothawade",
      "Pascal Poupart",
      "Abdullah Rashwan",
      "Yeqing Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df54302388bbc145aacaa1a54a4a5933-Abstract-Conference.html": {
    "title": "Rethinking Optimal Transport in Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arip Asadulaev",
      "Rostislav Korst",
      "Aleksandr Korotin",
      "Vage Egiazarian",
      "Andrey Filchenkov",
      "Evgeny Burnaev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df559d92fba6dc86dec1a2be59ebfce3-Abstract-Conference.html": {
    "title": "Preference Learning of Latent Decision Utilities with a Human-like Model of Preferential Choice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastiaan De Peuter",
      "Shibei Zhu",
      "Yujia Guo",
      "Andrew Howes",
      "Samuel Kaski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df5a8051be8bf4eaaabceb67c6d48332-Abstract-Conference.html": {
    "title": "No-regret Learning in Harmonic Games: Extrapolation in the Face of Conflicting Interests",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Legacci",
      "Panayotis Mertikopoulos",
      "Christos Papadimitriou",
      "Georgios Piliouras",
      "Bary Pradelski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df658fe5097f65485ad80b06e6cb30dd-Abstract-Conference.html": {
    "title": "Stochastic Extragradient with Flip-Flop Shuffling & Anchoring: Provable Improvements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiseok Chae",
      "Chulhee Yun",
      "Donghwan Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df77cca99e2785eafb5b2416d688ae24-Abstract-Conference.html": {
    "title": "Precise asymptotics of reweighted least-squares algorithms for linear diagonal networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiraag Kaushik",
      "Justin Romberg",
      "Vidya Muthukumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/df8b8c12afcaf65639aa686ececbf00c-Abstract-Conference.html": {
    "title": "Grid4D: 4D Decomposed Hash Encoding for High-Fidelity Dynamic Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Xu",
      "Zexin Fan",
      "Jian Yang",
      "Jin Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dfa1106ea7065899b13f2be9da04efb4-Abstract-Conference.html": {
    "title": "Reconstructing the Image Stitching Pipeline: Integrating Fusion and Rectangling into a Unified Inpainting Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Xie",
      "Weidong Zhao",
      "XianhuiLiu",
      "Jian Zhao",
      "Ning Jia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dfaa29ed28dfa175bcc5e2a54aa199f8-Abstract-Conference.html": {
    "title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanke Zhou",
      "Rong Tao",
      "Jianing Zhu",
      "Yiwen Luo",
      "Zengmao Wang",
      "Bo Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dfc24bd3ec5d74960e104268bbb52849-Abstract-Conference.html": {
    "title": "Metric Space Magnitude for Evaluating the Diversity of Latent Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katharina Limbeck",
      "Rayna Andreeva",
      "Rik Sarkar",
      "Bastian Rieck"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dfcf77638847c79bcf1a1449db85218d-Abstract-Conference.html": {
    "title": "Particle Semi-Implicit Variational Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jen Ning Lim",
      "Adam Johansen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/dff528ce3e1390c88f10bbf5e722a241-Abstract-Conference.html": {
    "title": "Rethinking the Capacity of Graph Neural Networks for Branching Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziang Chen",
      "Jialin Liu",
      "Xiaohan Chen",
      "Wang",
      "Wotao Yin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e00573ccc366b57d0d66497c2e21919f-Abstract-Conference.html": {
    "title": "Higher-Rank Irreducible Cartesian Tensors for Equivariant Message Passing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viktor Zaverkin",
      "Francesco Alesiani",
      "Takashi Maruyama",
      "Federico Errica",
      "Henrik Christiansen",
      "Makoto Takamoto",
      "Nicolas Weber",
      "Mathias Niepert"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e01519b47118e2f51aa643151350c905-Abstract-Conference.html": {
    "title": "LLM Dataset Inference: Did you train on my dataset?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratyush Maini",
      "Hengrui Jia",
      "Nicolas Papernot",
      "Adam Dziedzic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e04101138a3c94544760c1dbdf2c7a2d-Abstract-Conference.html": {
    "title": "Robust Reinforcement Learning from Corrupted Human Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Bukharin",
      "Ilgee Hong",
      "Haoming Jiang",
      "Zichong Li",
      "Qingru Zhang",
      "Zixuan Zhang",
      "Tuo Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e06cdb8742f51cf4bca2ffbf32e9a433-Abstract-Conference.html": {
    "title": "RobIR: Robust Inverse Rendering for High-Illumination Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Yang",
      "Chenyanzhen",
      "Xinyu Gao",
      "YazhenYuan",
      "Wu Yu",
      "Xiaowei Zhou",
      "Xiaogang Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e06f8704aca42f1ed8619165892e1195-Abstract-Conference.html": {
    "title": "Crafting Interpretable Embeddings for Language Neuroscience by Asking LLMs Questions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinamra Benara",
      "Chandan Singh",
      "John Morris",
      "Richard Antonello",
      "Ion Stoica",
      "Alexander Huth",
      "Jianfeng Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e08e1a60c006ac3f0c9f953626b0f0c8-Abstract-Conference.html": {
    "title": "Learning-Augmented Priority Queues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyad Benomar",
      "Christian Coester"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e099c1c9699814af0be873a175361713-Abstract-Conference.html": {
    "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Meng",
      "Mengzhou Xia",
      "Danqi Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e0b6f389739496e363a89155c9448a8a-Abstract-Conference.html": {
    "title": "The Power of Extrapolation in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanmin Li",
      "Kirill Acharya",
      "Peter Richtarik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e0c9b65fb3e41aaa86576df3ec33ad2e-Abstract-Conference.html": {
    "title": "Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxiang Li",
      "Siliang Zeng",
      "Hoi-To Wai",
      "Chenliang Li",
      "Alfredo Garcia",
      "Mingyi Hong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e0e25d425450b6fc8e34380de71b3aee-Abstract-Conference.html": {
    "title": "Moving Off-the-Grid: Scene-Grounded Video Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sjoerd van Steenkiste",
      "Daniel Zoran",
      "Yi Yang",
      "Yulia Rubanova",
      "Rishabh Kabra",
      "Carl Doersch",
      "Dilara Gokay",
      "joseph heyward",
      "Etienne Pot",
      "Klaus Greff",
      "Drew Hudson",
      "Thomas Keck",
      "Joao Carreira",
      "Alexey Dosovitskiy",
      "Mehdi S. M. Sajjadi",
      "Thomas Kipf"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e0e956681b04ac126679e8c7dd706b2e-Abstract-Conference.html": {
    "title": "Accuracy is Not All You Need",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhinav Dutta",
      "Sanjeev Krishnan",
      "Nipun Kwatra",
      "Ramachandran Ramjee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e0ebc19bde100bcd340a291a8c8c4859-Abstract-Conference.html": {
    "title": "LoCo: Learning 3D Location-Consistent Image Features with a Memory-Efficient Ranking Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Kloepfer",
      "João F. Henriques",
      "Dylan Campbell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e0f393e7980a24fd12fa6f15adfa25fb-Abstract-Conference.html": {
    "title": "Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lirui Wang",
      "Xinlei Chen",
      "Jialiang Zhao",
      "Kaiming He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e105d1cabb6ee2495595e2baf25493b5-Abstract-Conference.html": {
    "title": "FairWire: Fair Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oyku Kose",
      "Yanning Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e10a6a906ef323efaf708f76cf3c1d1e-Abstract-Conference.html": {
    "title": "ACFun: Abstract-Concrete Fusion Facial Stylization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiapeng Ji",
      "Kun Wei",
      "Ziqi Zhang",
      "Cheng Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e128479c79f38a9a03eada7c04aa67d8-Abstract-Conference.html": {
    "title": "Learning Optimal Tax Design in Nonatomic Congestion Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiwen Cui",
      "Maryam Fazel",
      "Simon S Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e142fd2b70f10db2543c64bca1417de8-Abstract-Conference.html": {
    "title": "Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "WANG JIAWEI",
      "Renhe Jiang",
      "Chuang Yang",
      "Zengqing Wu",
      "makoto onizuka",
      "Ryosuke Shibasaki",
      "Noboru Koshizuka",
      "Chuan Xiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e15ef893e137cd40e6c7313a04307437-Abstract-Conference.html": {
    "title": "Distributional Successor Features Enable Zero-Shot Policy Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuning Zhu",
      "Xinqi Wang",
      "Tyler Han",
      "Simon S Du",
      "Abhishek Gupta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e17afd5784c442d178744533c16d3c96-Abstract-Conference.html": {
    "title": "A Unified Confidence Sequence for Generalized Linear Models, with Applications to Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junghyun Lee",
      "Se-Young Yun",
      "Kwang-Sung Jun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e17fe6fe9990fffb637b42c98c005515-Abstract-Conference.html": {
    "title": "Transductive Active Learning: Theory and Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Hübotter",
      "Bhavya",
      "Lenart Treven",
      "Yarden As",
      "Andreas Krause"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e1b248453bca182b6138b8c14a75340d-Abstract-Conference.html": {
    "title": "On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Zhang",
      "Nan Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e1b619a9e241606a23eb21767f16cf81-Abstract-Conference.html": {
    "title": "CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingkun Zhang",
      "Keping Bi",
      "Wei Chen",
      "Quanrun Chen",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e1cadf5f02cc524b59c208728c73f91c-Abstract-Conference.html": {
    "title": "A Method for Evaluating Hyperparameter Sensitivity in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Adkins",
      "Michael Bowling",
      "Adam White"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e1cd1db6fcd4a6df8618259f5b9cca0f-Abstract-Conference.html": {
    "title": "Almost Surely Asymptotically Constant Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sam Adam-Day",
      "Michael Benedikt",
      "Ismail Ceylan",
      "Ben Finkelshtein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e1d40e928125c515099a3fdb75f06a50-Abstract-Conference.html": {
    "title": "Identifying Latent State-Transition Processes for Individualized Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuewen Sun",
      "Biwei Huang",
      "Yu Yao",
      "Donghuo Zeng",
      "Xinshuai Dong",
      "Songyao Jin",
      "Boyang Sun",
      "Roberto Legaspi",
      "Kazushi Ikeda",
      "Peter Spirtes",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e1f418450107c4a0ddc16d008d131573-Abstract-Conference.html": {
    "title": "SpaceByte: Towards Deleting Tokenization from Large Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Slagle"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e1fcd183ab33714a8464e4e9a20ac710-Abstract-Conference.html": {
    "title": "Open-Vocabulary Object Detection via Language Hierarchy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxing Huang",
      "Jingyi Zhang",
      "Kai Jiang",
      "Shijian Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e209210eae282e23e305df49fbb2769c-Abstract-Conference.html": {
    "title": "Tolerant Algorithms for Learning with Arbitrary Covariate Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Surbhi Goel",
      "Abhishek Shetty",
      "Konstantinos Stavropoulos",
      "Arsen Vasilyan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e21955c93dede886af1d0d362c756757-Abstract-Conference.html": {
    "title": "Confidence Regulation Neurons in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro Stolfo",
      "Ben Wu",
      "Wes Gurnee",
      "Yonatan Belinkov",
      "Xingyi Song",
      "Mrinmaya Sachan",
      "Neel Nanda"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e23133d34964a0a09f6d076fc4b922a4-Abstract-Conference.html": {
    "title": "Invariant Tokenization of Crystalline Materials for Language Model Enabled Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keqiang Yan",
      "Xiner Li",
      "Hongyi Ling",
      "Kenna Ashen",
      "Carl Edwards",
      "Raymundo Arroyave",
      "Marinka Zitnik",
      "Heng Ji",
      "Xiaofeng Qian",
      "Xiaoning Qian",
      "Shuiwang Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e2393d306d44fdec33a8100e31c3dc23-Abstract-Conference.html": {
    "title": "Adaptive Passive-Aggressive Framework for Online Regression with Side Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runhao Shi",
      "Jiaxi Ying",
      "Daniel Palomar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e23c30848bc106b74b8a6993e80fb177-Abstract-Conference.html": {
    "title": "Assembly Fuzzy Representation on Hypergraph for Open-Set 3D Object Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Xu",
      "Yifan Feng",
      "Jun Zhang",
      "Jun-Hai Yong",
      "Yue Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e25198b6a75f74277ee3a2bd4165d9ef-Abstract-Conference.html": {
    "title": "Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kailu Wu",
      "Fangfu Liu",
      "Zhihan Cai",
      "Runjie Yan",
      "Hanyang Wang",
      "Yating Hu",
      "Yueqi Duan",
      "Kaisheng Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e262fc23ec7275230ee77c55d0cc9555-Abstract-Conference.html": {
    "title": "Hierarchical Uncertainty Exploration via Feedforward Posterior Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elias Nehme",
      "Rotem Mulayoff",
      "Tomer Michaeli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e27017440f428159701b9c1682715aba-Abstract-Conference.html": {
    "title": "Efficient $\\Phi$-Regret Minimization with Low-Degree Swap Deviations in Extensive-Form Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Zhang",
      "Ioannis Anagnostides",
      "Gabriele Farina",
      "Tuomas Sandholm"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e28e19d00b23fe0265f433fa05a96b06-Abstract-Conference.html": {
    "title": "fMRI predictors based on language models of increasing complexity recover brain left lateralization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laurent Bonnasse-Gahot",
      "Christophe Pallier"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e2ba11d128034f0c544693374c59e49a-Abstract-Conference.html": {
    "title": "ResAD: A Simple Framework for Class Generalizable Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xincheng Yao",
      "Zixin Chen",
      "Chao Gao",
      "Guangtao Zhai",
      "Chongyang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e2c054ffc0467962d3fd3b2f17df910c-Abstract-Conference.html": {
    "title": "Textual Training for the Hassle-Free Removal of Unwanted Visual Data: Case Studies on OOD and Hateful Image Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saehyung Lee",
      "Jisoo Mok",
      "Sangha Park",
      "Yongho Shin",
      "Dahuin Jung",
      "Sungroh Yoon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e2d28c3ec49945e1922943070ffd21e8-Abstract-Conference.html": {
    "title": "Neural Pfaffians: Solving Many Many-Electron Schrödinger Equations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Gao",
      "Stephan Günnemann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e2d89f6d3d7707709afa2e01f5952f95-Abstract-Conference.html": {
    "title": "Dynamic Rescaling for Training GNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nimrah Mustafa",
      "Rebekka Burkholz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e304d374c85e385eb217ed4a025b6b63-Abstract-Conference.html": {
    "title": "Return of Unconditional Generation: A Self-supervised Representation Generation Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhong Li",
      "Dina Katabi",
      "Kaiming He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e304e04a6f455dd82f8a85a0a3679493-Abstract-Conference.html": {
    "title": "Instance-adaptive Zero-shot Chain-of-Thought Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaosong Yuan",
      "Chen Shen",
      "Shaotian Yan",
      "Xiaofeng Zhang",
      "Liang Xie",
      "Wenxiao Wang",
      "Renchu Guan",
      "Ying Wang",
      "Jieping Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e31bdea0a93741c2157eea705dd219eb-Abstract-Conference.html": {
    "title": "ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Eyring",
      "Shyamgopal Karthik",
      "Karsten Roth",
      "Alexey Dosovitskiy",
      "Zeynep Akata"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e3490ed7541c826d9156c322c910e41b-Abstract-Conference.html": {
    "title": "Federated Online Prediction from Experts with Differential Privacy: Separations and Regret Speed-ups",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengyu Gao",
      "Ruiquan Huang",
      "Jing Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e34d214dfb5b80212ac1b565aa41af55-Abstract-Conference.html": {
    "title": "Reconstruct and Match: Out-of-Distribution Robustness via Topological Homogeneity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoqi Chen",
      "Luyao Tang",
      "Hui Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e34d908241aef40440e61d2a27715424-Abstract-Conference.html": {
    "title": "VeXKD: The Versatile Integration of Cross-Modal Fusion and Knowledge Distillation for 3D Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JI Yuzhe",
      "Yijie CHEN",
      "Liuqing Yang",
      "Ding Rui",
      "Meng Yang",
      "Xinhu Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e356ed5f27885c79c7cb597bb1107c94-Abstract-Conference.html": {
    "title": "From Text to Trajectory: Exploring Complex Constraint Representation and Decomposition in Safe Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pusen Dong",
      "Tianchen Zhu",
      "yue qiu",
      "Haoyi Zhou",
      "Jianxin Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e3642e94ac68254419b7cdeb5e4a46f7-Abstract-Conference.html": {
    "title": "User-Creator Feature Polarization in Recommender Systems with Dual Influence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Lin",
      "Kun Jin",
      "Andrew Estornell",
      "Xiaoying Zhang",
      "Yiling Chen",
      "Yang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e372bfe389adbf742a759396f0170356-Abstract-Conference.html": {
    "title": "Privacy without Noisy Gradients: Slicing Mechanism for Generative Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kristjan Greenewald",
      "Yuancheng Yu",
      "Hao Wang",
      "Kai Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e379877d7880bb1f80c82a9f1c58e6e8-Abstract-Conference.html": {
    "title": "Universality in Transfer Learning for Linear Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Ghane",
      "Danil Akhtiamov",
      "Babak Hassibi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e3abc125ecacb71786cefb9f67b08c5d-Abstract-Conference.html": {
    "title": "Learning rigid-body simulators over implicit shapes for large-scale scenes and vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulia Rubanova",
      "Tatiana Lopez-Guevara",
      "Kelsey Allen",
      "Will Whitney",
      "Kimberly L Stachenfeld",
      "Tobias Pfaff"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e3b1291b7529063172d927588d2b03a1-Abstract-Conference.html": {
    "title": "Convergence of $\\text{log}(1/\\epsilon)$ for Gradient-Based Algorithms in Zero-Sum Games without the Condition Number: A Smoothed Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ioannis Anagnostides",
      "Tuomas Sandholm"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e3b53f89136b1bc69a5714ea465f01b6-Abstract-Conference.html": {
    "title": "Animate3D: Animating Any 3D Model with Multi-view Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanqin Jiang",
      "Chaohui Yu",
      "Chenjie Cao",
      "Fan Wang",
      "Weiming Hu",
      "Jin Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e3ed7183233afa8e5485ff8f6c3f18b1-Abstract-Conference.html": {
    "title": "Skinned Motion Retargeting with Dense Geometric Interaction Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Ye",
      "Jia-Wei Liu",
      "Jia Jia",
      "Shikun Sun",
      "Mike Zheng Shou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e3fe7b34ba4f378df39cb12a97193f41-Abstract-Conference.html": {
    "title": "Fair Allocation in Dynamic Mechanism Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alireza Fallah",
      "Michael I. Jordan",
      "Annie Ulichney"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e40d5118ee8f837729fa877add71c38f-Abstract-Conference.html": {
    "title": "Who's asking? User personas and the mechanics of latent misalignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asma Ghandeharioun",
      "Ann Yuan",
      "Marius Guerard",
      "Emily Reif",
      "Michael Lepori",
      "Lucas Dixon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e4165c96702bac5f4962b70f3cf2f136-Abstract-Conference.html": {
    "title": "Instruction-Guided Visual Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinliang Zheng",
      "Jianxiong Li",
      "Sijie Cheng",
      "Yinan Zheng",
      "Jiaming Li",
      "Jihao Liu",
      "Yu Liu",
      "Jingjing Liu",
      "Xianyuan Zhan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e41efb03e20ca3c231940a3c6917ef6f-Abstract-Conference.html": {
    "title": "SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei Zhou",
      "Jay Pujara",
      "Xiang Ren",
      "Xinyun Chen",
      "Heng-Tze Cheng",
      "Quoc V Le",
      "Ed Chi",
      "Denny Zhou",
      "Swaroop Mishra",
      "Huaixiu (Steven) Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e4343147340c9d65f4c780451eb066f9-Abstract-Conference.html": {
    "title": "Bandits with Abstention under Expert Advice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen Pasteris",
      "Alberto Rumi",
      "Maximilian Thiessen",
      "Shota Saito",
      "Atsushi Miyauchi",
      "Fabio Vitale",
      "Mark Herbster"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e444859b2a22df6b56af9381ad1e9480-Abstract-Conference.html": {
    "title": "Improved Algorithms for Contextual Dynamic Pricing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matilde Tullii",
      "Solenne Gaucher",
      "Nadav Merlis",
      "Vianney Perchet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e4595e77b045971a654edc5e3cd31989-Abstract-Conference.html": {
    "title": "AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Guan",
      "Wei Wu",
      "zujie wen",
      "Peng Xu",
      "Hongning Wang",
      "Minlie Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e45c8d054739d31676619e7e11327f68-Abstract-Conference.html": {
    "title": "Slight Corruption in Pre-training Data Makes Better Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Chen",
      "Yujin Han",
      "Diganta Misra",
      "Xiang Li",
      "Kai Hu",
      "Difan Zou",
      "Masashi Sugiyama",
      "Jindong Wang",
      "Bhiksha Raj"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e45caa3d5273d105b8d045e748636957-Abstract-Conference.html": {
    "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rafael Rafailov",
      "Yaswanth Chittepu",
      "Ryan Park",
      "Harshit Sushil Sikchi",
      "Joey Hejna",
      "Brad Knox",
      "Chelsea Finn",
      "Scott Niekum"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e4630f7c0660d944c132455c124e7d90-Abstract-Conference.html": {
    "title": "Adversarial Representation Engineering: A General Model Editing Framework for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihao Zhang",
      "Zeming Wei",
      "Jun Sun",
      "Meng Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e46984e056185b21ddb1e7973c365f14-Abstract-Conference.html": {
    "title": "Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaomeng Hu",
      "Pin-Yu Chen",
      "Tsung-Yi Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e46c303cfd3b93bd32ecf6070318efc3-Abstract-Conference.html": {
    "title": "Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gen Li",
      "Yuling Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e46fc33e80e9fa2febcdb058fba4beca-Abstract-Conference.html": {
    "title": "HuRef: HUman-REadable Fingerprint for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyi Zeng",
      "Lizheng Wang",
      "Yuncong Hu",
      "Yi Xu",
      "Chenghu Zhou",
      "Xinbing Wang",
      "Yu Yu",
      "Zhouhan Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e47470631adb188a30f14c9738fe157b-Abstract-Conference.html": {
    "title": "Last-Iterate Global Convergence of Policy Gradients for Constrained Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro Montenegro",
      "Marco Mussi",
      "Matteo Papini",
      "Alberto Maria Metelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e4748b6b6ca49f04b6a8cfce1d5f9a70-Abstract-Conference.html": {
    "title": "The Prevalence of Neural Collapse in Neural Multivariate Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "George Andriopoulos",
      "Zixuan Dong",
      "Li Guo",
      "Zifan Zhao",
      "Keith Ross"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e481829e70a46db98c0c2eb46ff91bac-Abstract-Conference.html": {
    "title": "Toward Dynamic Non-Line-of-Sight Imaging with Mamba Enforced Temporal Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Li",
      "Yi Sun",
      "Shida Sun",
      "Juntian Ye",
      "Yueyi Zhang",
      "Feihu Xu",
      "Zhiwei Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e4a0d8aef3567f742b0794844d9b5847-Abstract-Conference.html": {
    "title": "Quantifying the Gain in Weak-to-Strong Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moses Charikar",
      "Chirag Pabbaraju",
      "Kirankumar Shiragur"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e4be7e9867ef163563f4a5e90cec478f-Abstract-Conference.html": {
    "title": "Self-playing Adversarial Language Game Enhances LLM Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyu Cheng",
      "Tianhao Hu",
      "Han Xu",
      "Zhisong Zhang",
      "Yong Dai",
      "Lei Han",
      "nan du",
      "Xiaolong Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e4c61f578ff07830f5c37378dd3ecb0d-Abstract-Conference.html": {
    "title": "Gorilla: Large Language Model Connected with Massive APIs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shishir G Patil",
      "Tianjun Zhang",
      "Xin Wang",
      "Joseph E Gonzalez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e4cd50120b6d7e8daff1749d6bbaa889-Abstract-Conference.html": {
    "title": "Coherence-free Entrywise Estimation of Eigenvectors in Low-rank Signal-plus-noise Matrix Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Yan",
      "Keith Levin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e4cdb4090e04816422afcbb08d4badcf-Abstract-Conference.html": {
    "title": "KALM: Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing-Cheng Pang",
      "Si-Hang Yang",
      "Kaiyuan Li",
      "Jiaji Zhang",
      "Xiong-Hui Chen",
      "Nan Tang",
      "Yang Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e51ec47238fc3e82c269965c5533f9ef-Abstract-Conference.html": {
    "title": "Hierarchical Programmatic Option Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-An Lin",
      "Chen-Tao Lee",
      "Chih-Han Yang",
      "Guan-Ting Liu",
      "Shao-Hua Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e52e4de8689a9955b6d3ff421d019387-Abstract-Conference.html": {
    "title": "IPO: Interpretable Prompt Optimization for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingjun Du",
      "Wenfang Sun",
      "Cees Snoek"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e5419147e53eba322cf12aff266a66f2-Abstract-Conference.html": {
    "title": "Towards Principled Graph Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luis Müller",
      "Daniel Kusuma",
      "Blai Bonet",
      "Christopher Morris"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e55653539c9a9aa096a1fc8ca77ff413-Abstract-Conference.html": {
    "title": "Entrywise error bounds for low-rank approximations of kernel matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Modell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e574db41163e700545ff4114f96b3d7a-Abstract-Conference.html": {
    "title": "AUCSeg: AUC-oriented Pixel-level Long-tail Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyu Han",
      "Qianqian Xu",
      "Zhiyong Yang",
      "Shilong Bao",
      "Peisong Wen",
      "Yangbangyan Jiang",
      "Qingming Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e58026e2b2929108e1bd24cbfa1c8e4b-Abstract-Conference.html": {
    "title": "Consistency Models for Scalable and Fast Simulation-Based Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marvin Schmitt",
      "Valentin Pratz",
      "Ullrich Köthe",
      "Paul-Christian Bürkner",
      "Stefan Radev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e589022774244df75b97eae18bb3628d-Abstract-Conference.html": {
    "title": "General bounds on the quality of Bayesian coresets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trevor Campbell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e59783120660eb4e847550d4584e74d6-Abstract-Conference.html": {
    "title": "RLE: A Unified Perspective of Data Augmentation for Cross-Spectral Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Tan",
      "Yukang Zhang",
      "Keke Han",
      "Pingyang Dai",
      "Yan Zhang",
      "Yongjian Wu",
      "Rongrong Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e5b4633454cb2174779d294ccda02318-Abstract-Conference.html": {
    "title": "4-bit Shampoo for Memory-Efficient Network Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sike Wang",
      "Pan Zhou",
      "Jia Li",
      "Hua Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e5ba3d6d93213db6b1d1931c6517fe1a-Abstract-Conference.html": {
    "title": "State Space Models on Temporal Graphs: A First-Principles Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jintang Li",
      "Ruofan Wu",
      "Xinzhou Jin",
      "Boqun Ma",
      "Liang Chen",
      "Zibin Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e5d5344481c495709eaa6397bf51fb89-Abstract-Conference.html": {
    "title": "DAT: Improving Adversarial Robustness via Generative Amplitude Mix-up in Frequency Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengpeng Li",
      "Kemou Li",
      "Haiwei Wu",
      "Jinyu Tian",
      "Jiantao Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6114e62fdb36d6d91ff43334e763a0e-Abstract-Conference.html": {
    "title": "Protein-Nucleic Acid Complex Modeling with Frame Averaging Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tinglin Huang",
      "Zhenqiao Song",
      "Rex Ying",
      "Wengong Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e618724ac897c6cf3fbfb273f8695d67-Abstract-Conference.html": {
    "title": "Demystify Mamba in Vision: A Linear Attention Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongchen Han",
      "Ziyi Wang",
      "Zhuofan Xia",
      "Yizeng Han",
      "Yifan Pu",
      "Chunjiang Ge",
      "Jun Song",
      "Shiji Song",
      "Bo Zheng",
      "Gao Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6209a02394269f14c29e049f4e05c42-Abstract-Conference.html": {
    "title": "FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihong Yin",
      "Vladimir Yugay",
      "Yue Li",
      "Sezer Karaoglu",
      "Theo Gevers"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6231c5f46598cfd09ff1970524e0436-Abstract-Conference.html": {
    "title": "Theoretical Foundations of Deep Selective State-Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicola Muca Cirone",
      "Antonio Orvieto",
      "Benjamin Walker",
      "Cristopher Salvi",
      "Terry Lyons"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e63309e532688c722177f81e99f94f32-Abstract-Conference.html": {
    "title": "Learning Group Actions on Latent Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinzhu Jin",
      "Aman Shrivastava",
      "Tom Fletcher"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e635a25e49e73adc51f76aef462ff2f8-Abstract-Conference.html": {
    "title": "AverNet: All-in-one Video Restoration for Time-varying Unknown Degradations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyu Zhao",
      "Lei Tian",
      "Xinyan Xiao",
      "Peng Hu",
      "Yuanbiao Gou",
      "Xi Peng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e656933e4e9626f5ada42349bcadffb8-Abstract-Conference.html": {
    "title": "Do Finetti: On Causal Effects for Exchangeable Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Guo",
      "Chi Zhang",
      "Karthika Mohan",
      "Ferenc Huszar",
      "Bernhard Schölkopf"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6617714485265b9380a5315bf3ba98f-Abstract-Conference.html": {
    "title": "NeuralSteiner: Learning Steiner Tree for Overflow-avoiding Global Routing in Chip Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "RUIZHI LIU",
      "ZhishengZeng",
      "Shizhe Ding",
      "Jingyan Sui",
      "Xingquan Li",
      "Dongbo Bu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e66309ead63bc1410d2df261a28f602d-Abstract-Conference.html": {
    "title": "Contextual Bilevel Reinforcement Learning for Incentive Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinzenz Thoma",
      "Barna Pásztor",
      "Andreas Krause",
      "Giorgia Ramponi",
      "Yifan Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e664650506f1cf2b4696df892147c06e-Abstract-Conference.html": {
    "title": "Automating Data Annotation under Strategic Human Agents: Risks and Potential Solutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Xie",
      "Xueru Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6825a8d9ff48e0cfe013608fec3ddec-Abstract-Conference.html": {
    "title": "Understanding the Role of Equivariance in Self-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Wang",
      "Kaiwen Hu",
      "Sharut Gupta",
      "Ziyu Ye",
      "Yisen Wang",
      "Stefanie Jegelka"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e68274fc4f158dbcbd4dddc672f7ee9c-Abstract-Conference.html": {
    "title": "Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masatoshi Uehara",
      "Yulai Zhao",
      "Ehsan Hajiramezanali",
      "Gabriele Scalia",
      "Gokcen Eraslan",
      "Avantika Lal",
      "Sergey Levine",
      "Tommaso Biancalani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e68af7d8a44bc1964f6be4de464e38f9-Abstract-Conference.html": {
    "title": "DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oryan Yehezkel",
      "Alon Zolfi",
      "Amit Baras",
      "Yuval Elovici",
      "Asaf Shabtai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e68c3a624d19154b28951e8690834607-Abstract-Conference.html": {
    "title": "Sample Selection via Contrastive Fragmentation for Noisy Label Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Dongjoo Kim",
      "Sangwoo Moon",
      "Jihwan Moon",
      "Dongyeon Woo",
      "Gunhee Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6a11b618402617342f38f5b49430937-Abstract-Conference.html": {
    "title": "Probablistic Emulation of a Global Climate Model with Spherical DYffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Salva Rühling Cachay",
      "Brian Henn",
      "Oliver Watt-Meyer",
      "Christopher S. Bretherton",
      "Rose Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6b57a990462df5afa58d64ce2709db9-Abstract-Conference.html": {
    "title": "Smoothie: Label Free Language Model Routing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neel Guha",
      "Mayee Chen",
      "Trevor Chow",
      "Ishan Khare",
      "Christopher Ré"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6c185ba1ac2880d6f53aae3cfa80469-Abstract-Conference.html": {
    "title": "Model LEGO: Creating Models Like Disassembling and Assembling Building Blocks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacong Hu",
      "Jing Gao",
      "Jingwen Ye",
      "Yang Gao",
      "Xingen Wang",
      "Zunlei Feng",
      "Mingli Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6c2e85db1f1039177c4495ccd399ac4-Abstract-Conference.html": {
    "title": "Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning-Hsu (Albert) Wang",
      "Yu-Lun Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6c65eb9b56719c1aa45ff73874de317-Abstract-Conference.html": {
    "title": "3D Equivariant Pose Regression via Direct Wigner-D Harmonics Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongmin Lee",
      "Minsu Cho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6c967a0e4168df9614a81ed4195ba78-Abstract-Conference.html": {
    "title": "Identifiable Shared Component Analysis of Unpaired Multimodal Mixtures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subash Timilsina",
      "Sagar Shrestha",
      "Xiao Fu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6d1d6195f6f3e32a930643e0ef46332-Abstract-Conference.html": {
    "title": "Towards Diverse Device Heterogeneous Federated Learning via Task Arithmetic Knowledge Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahdi Morafah",
      "Vyacheslav Kungurtsev",
      "Hojin Chang",
      "Chen Chen",
      "Bill Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6d8684a40187a417461a239238137dd-Abstract-Conference.html": {
    "title": "MoMu-Diffusion: On Learning Long-Term Motion-Music Synchronization and Correspondence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuming You",
      "Minghui Fang",
      "Li Tang",
      "Rongjie Huang",
      "Yongqi Wang",
      "Zhou Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6da278cdd692077b7e4a99d55573d9c-Abstract-Conference.html": {
    "title": "Active Sequential Posterior Estimation for Sample-Efficient Simulation-Based Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sam Griesemer",
      "Defu Cao",
      "Zijun Cui",
      "Carolina Osorio",
      "Yan Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6df4efa20adf8ef9acb80e94072a429-Abstract-Conference.html": {
    "title": "Parseval Regularization for Continual Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wesley Chung",
      "Lynn Cherif",
      "Doina Precup",
      "David Meger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6e706454d72c18582b9c1ff70b11f7d-Abstract-Conference.html": {
    "title": "HC-GAE: The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Bai",
      "Zhuo Xu",
      "Lixin Cui",
      "Ming Li",
      "Yue Wang",
      "Edwin R. Hancock"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6f32e64b9c27d153b46c94f0fe22b56-Abstract-Conference.html": {
    "title": "Continuous Temporal Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekun CAI",
      "Guangji Bai",
      "Renhe Jiang",
      "Xuan Song",
      "Liang Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e6f8759254d86ea9c197d30b92b313ca-Abstract-Conference.html": {
    "title": "Analysis of Corrected Graph Convolutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert J. Wang",
      "Aseem Baranwal",
      "Kimon Fountoulakis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e70ffb3f05096e62b5077d8e1b62e668-Abstract-Conference.html": {
    "title": "UV-free Texture Generation with Denoising and Geodesic Heat Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simone Foti",
      "Stefanos Zafeiriou",
      "Tolga Birdal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e7349e785900b93d8b4971a3f2c1cefe-Abstract-Conference.html": {
    "title": "A Theoretical Perspective for Speculative Decoding Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Yin",
      "Minshuo Chen",
      "Kaixuan Huang",
      "Mengdi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e7384de302bef52319b5067c3115bbfb-Abstract-Conference.html": {
    "title": "Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongqi Wang",
      "Wenxiang Guo",
      "Rongjie Huang",
      "Jiawei Huang",
      "Zehan Wang",
      "Fuming You",
      "Ruiqi Li",
      "Zhou Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e73ad1f690542144ce354637bb913c35-Abstract-Conference.html": {
    "title": "Event-3DGS: Event-based 3D Reconstruction Using 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiqian Han",
      "Jianing Li",
      "Henglu Wei",
      "Xiangyang Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e73f4935d2ef5e23997de852e8a52661-Abstract-Conference.html": {
    "title": "Where's Waldo: Diffusion Features For Personalized Segmentation and Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dvir Samuel",
      "Rami Ben-Ari",
      "Matan Levy",
      "Nir Darshan",
      "Gal Chechik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e742e93b9f709f522986f53eeebaeef7-Abstract-Conference.html": {
    "title": "Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ya-Wei Eileen Lin",
      "Ronen Talmon",
      "Ron Levie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e78399fc43dbb2d87b7e1e6906ce5baf-Abstract-Conference.html": {
    "title": "CondTSF: One-line Plugin of Dataset Condensation for Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianrong Ding",
      "Zhanyu Liu",
      "Guanjie Zheng",
      "Haiming Jin",
      "Linghe Kong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e7b3dd853382f237128943665bca2ca0-Abstract-Conference.html": {
    "title": "Query-Based Adversarial Prompt Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Hayase",
      "Ema Borevković",
      "Nicholas Carlini",
      "Florian Tramer",
      "Milad Nasr"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e7b3e34d118d1fc6135b0bcbf3254d58-Abstract-Conference.html": {
    "title": "RoME: A Robust Mixed-Effects Bandit Algorithm for Optimizing Mobile Health Interventions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Easton Huch",
      "Jieru Shi",
      "Madeline R Abbott",
      "Jessica Golbus",
      "Alexander Moreno",
      "Walter Dempsey"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e7ba43ea2a7f94d86e69de761e178792-Abstract-Conference.html": {
    "title": "Shared Autonomy with IDA: Interventional Diffusion Assistance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brandon McMahan",
      "Zhenghao (Mark) Peng",
      "Bolei Zhou",
      "Jonathan Kao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e7c3ac813288e4aff606c24e59a8410a-Abstract-Conference.html": {
    "title": "Multi-times Monte Carlo Rendering for Inter-reflection Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tengjie Zhu",
      "Zhuo Chen",
      "Jingnan Gao",
      "Yichao Yan",
      "Xiaokang Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e7c786024ca718f2487712bfe9f51030-Abstract-Conference.html": {
    "title": "GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Wang",
      "Aoxue Li",
      "Zhenguo Li",
      "Xihui Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e7e506bc5a94768243083216fe51d98b-Abstract-Conference.html": {
    "title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhang",
      "Junhui Hou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e7eb8128eb26eafbe901348df1dbacdc-Abstract-Conference.html": {
    "title": "SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Li",
      "Xiang Li",
      "Weijie Li",
      "Qibin Hou",
      "Li Liu",
      "Ming-Ming Cheng",
      "Jian Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e7feb9dbd9a94b6c552fc403fcebf2ef-Abstract-Conference.html": {
    "title": "Stabilizing Zero-Shot Prediction: A Novel Antidote to Forgetting in Continual Vision-Language Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Gao",
      "Xingxing Zhang",
      "Kele Xu",
      "Xinjun Mao",
      "Huaimin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e83b86156555ab9692743f9f8f67adf1-Abstract-Conference.html": {
    "title": "Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact Subproblem Solver for Training Structured Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zih-Syuan Huang",
      "Ching-pei Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e83e27af3a239fd0c309cd9ea7fc5402-Abstract-Conference.html": {
    "title": "A hierarchical decomposition for explaining ML performance discrepancies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harvineet Singh",
      "Fan Xia",
      "Adarsh Subbaswamy",
      "Alexej Gossmann",
      "Jean Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e878c8f38381d0964677fb9536c494ee-Abstract-Conference.html": {
    "title": "DistrictNet: Decision-aware learning for geographical districting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheikh Ahmed",
      "Alexandre Forel",
      "Axel Parmentier",
      "Thibaut Vidal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e88476b0ce9d445037422fe68ca097e4-Abstract-Conference.html": {
    "title": "Private Algorithms for Stochastic Saddle Points and Variational Inequalities: Beyond Euclidean Geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raef Bassily",
      "Cristóbal Guzmán",
      "Michael Menart"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e88870ec82f2469b0ddf32c817920c68-Abstract-Conference.html": {
    "title": "Neural Krylov Iteration for Accelerating Linear System Solving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Luo",
      "Jie Wang",
      "Hong Wang",
      "huanshuo dong",
      "Zijie Geng",
      "Hanzhu Chen",
      "Yufei Kuang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e894de44f7587d5ea723120f4d0b8689-Abstract-Conference.html": {
    "title": "Symmetric Linear Bandits with Hidden Symmetry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phuong Nam Tran",
      "The Anh Ta",
      "Debmalya Mandal",
      "Long Tran-Thanh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e8aad0aaa1309659a7d7e4c21202d9d0-Abstract-Conference.html": {
    "title": "SafeWorld: Geo-Diverse Safety Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Da Yin",
      "Haoyi Qiu",
      "Kung-Hsiang Huang",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e8b782d03c623e3e587fdbcc521dd581-Abstract-Conference.html": {
    "title": "Geometric Analysis of Nonlinear Manifold Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nimita Shinde",
      "Tianjiao Ding",
      "Daniel Robinson",
      "Rene Vidal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e8bd617e7dd0394ceadf37b4a7773179-Abstract-Conference.html": {
    "title": "ET-Flow: Equivariant Flow-Matching for Molecular Conformer Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Majdi Hassan",
      "Nikhil Shenoy",
      "Jungyoon Lee",
      "Hannes Stärk",
      "Stephan Thaler",
      "Dominique Beaini"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e8c20cafe841cba3e31a17488dc9c3f1-Abstract-Conference.html": {
    "title": "Spiking Token Mixer: An event-driven friendly Former structure for spiking neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shikuang Deng",
      "Yuhang Wu",
      "KANGRUI DU",
      "Shi Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e8ccaec7aea6f5740a3a3fdba390e466-Abstract-Conference.html": {
    "title": "Graph Structure Inference with BAM: Neural Dependency Processing via Bilinear Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Froehlich",
      "Heinz Koeppl"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e8da5e7ed327823326360ac3c7d7f833-Abstract-Conference.html": {
    "title": "HiCo: Hierarchical Controllable Diffusion Model for Layout-to-image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bocheng",
      "Yuhang Ma",
      "wuliebucha",
      "Shanyuan Liu",
      "Ao Ma",
      "Xiaoyu Wu",
      "Dawei Leng",
      "Yuhui Yin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e8e30fda5ab87ea93360a36288ac0145-Abstract-Conference.html": {
    "title": "Ctrl-X: Controlling Structure and Appearance for Text-To-Image Generation Without Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuan Heng Lin",
      "Sicheng Mo",
      "Ben Klingher",
      "Fangzhou Mu",
      "Bolei Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e91bf7dfba0477554994c6d64833e9d8-Abstract-Conference.html": {
    "title": "An Image is Worth 32 Tokens for Reconstruction and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Yu",
      "Mark Weber",
      "Xueqing Deng",
      "Xiaohui Shen",
      "Daniel Cremers",
      "Liang-Chieh Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e92b8441b702d142435f243806f9ec5a-Abstract-Conference.html": {
    "title": "Almost Minimax Optimal Best Arm Identification in Piecewise Stationary Linear Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunlong Hou",
      "Vincent Tan",
      "Zixin Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e92cb6f981a2cacb2a710ecaa0d7b141-Abstract-Conference.html": {
    "title": "Frustratingly Easy Test-Time Adaptation of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Farina",
      "Gianni Franchi",
      "Giovanni Iacca",
      "Massimiliano Mancini",
      "Elisa Ricci"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e93ac11b7e96337962fa38b0460ede3b-Abstract-Conference.html": {
    "title": "Autoregressive Image Diffusion: Generation of Image Sequence and Application in MRI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanxiong Luo",
      "Shoujin Huang",
      "Martin Uecker"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e94c57064dd5740c117b453bde8404c9-Abstract-Conference.html": {
    "title": "CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saurav Jha",
      "Dong Gong",
      "Lina Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e955bd39051d4d080826b2ffcf3a7fe2-Abstract-Conference.html": {
    "title": "Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suhan Cui",
      "Prasenjit Mitra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e95da8078ec8389533c802e368da5298-Abstract-Conference.html": {
    "title": "4D Gaussian Splatting in the Wild with Uncertainty-Aware Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mijeong Kim",
      "Jongwoo Lim",
      "Bohyung Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e95eb5206c867be843fbc14bbfe8c10e-Abstract-Conference.html": {
    "title": "Toward Real Ultra Image Segmentation: Leveraging Surrounding Context to Cultivate General Segmentation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Wang",
      "Yutian Lin",
      "Yu Wu",
      "Bo Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e969483cbae5a24df038fb5847f5b8b0-Abstract-Conference.html": {
    "title": "MultiOOD: Scaling Out-of-Distribution Detection for Multiple Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Dong",
      "Yue Zhao",
      "Eleni Chatzi",
      "Olga Fink"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e9721921b799b6ea98d37f9e77f1a7fe-Abstract-Conference.html": {
    "title": "Sequential Harmful Shift Detection Without Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Salim I. Amoukou",
      "Tom Bewley",
      "Saumitra Mishra",
      "Freddy Lecue",
      "Daniele Magazzeni",
      "Manuela M. Veloso"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e9954d4e15d59448d3edd3835ac57c95-Abstract-Conference.html": {
    "title": "Learning Cortico-Muscular Dependence through Orthonormal Decomposition of Density Ratios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihan Ma",
      "Bo Hu",
      "Tianyu Jia",
      "Alexander Clarke",
      "Blanka Zicher",
      "Arnault Caillet",
      "Dario Farina",
      "Jose C Principe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e99ed1162e984a5f08cb57ecde2d2231-Abstract-Conference.html": {
    "title": "REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang-Hsuan Tseng",
      "En-Pei Hu",
      "Cheng-Han Chiang",
      "Yuan Tseng",
      "Hung-yi Lee",
      "Lin-shan Lee",
      "Shao-Hua Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e9b0ae84d6879b30c78cb8537466a4e0-Abstract-Conference.html": {
    "title": "Autobidder's Dilemma: Why More Sophisticated Autobidders Lead to Worse Auction Efficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Deng",
      "Jieming Mao",
      "Vahab Mirrokni",
      "Hanrui Zhang",
      "Song Zuo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e9df36b21ff4ee211a8b71ee8b7e9f57-Abstract-Conference.html": {
    "title": "Foundation Inference Models for Markov Jump Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Berghaus",
      "Kostadin Cvejoski",
      "Patrick Seifner",
      "César Ali Ojeda Marin",
      "Ramsés J. Sánchez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/e9e3e5bdb017bbc887271c6f6de5353f-Abstract-Conference.html": {
    "title": "Controlling Counterfactual Harm in Decision Support Systems Based on Prediction Sets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eleni Straitouri",
      "Suhas Thejaswi",
      "Manuel Rodriguez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ea011cd13bcdf7ca38506c844dccfee8-Abstract-Conference.html": {
    "title": "$\\textit{Bifr\\\"ost}$: 3D-Aware Image Compositing with Language Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingxiao Li",
      "Kaixiong Gong",
      "Wei-Hong Li",
      "xili dai",
      "Tao Chen",
      "Xiaojun Yuan",
      "Xiangyu Yue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ea13534ee239bb3977795b8cc855bacc-Abstract-Conference.html": {
    "title": "GSDF: 3DGS Meets SDF for Improved Neural Rendering and Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mulin Yu",
      "Tao Lu",
      "Linning Xu",
      "Lihan Jiang",
      "Yuanbo Xiangli",
      "Bo Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ea1f5f0878d43ff4fb8bf64ef4a2326c-Abstract-Conference.html": {
    "title": "Sequoia: Scalable and Robust Speculative Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoming Chen",
      "Avner May",
      "Ruslan Svirschevski",
      "Yu-Hsun Huang",
      "Max Ryabinin",
      "Zhihao Jia",
      "Beidi Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ea2e5f8777105309a900d30dc4898095-Abstract-Conference.html": {
    "title": "MAC Advice for facility location mechanism design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zohar Barak",
      "Anupam Gupta",
      "Inbal Talgam-Cohen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ea2f5244b30cefefa66f1813693ca2dc-Abstract-Conference.html": {
    "title": "Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuval Filmus",
      "Steve Hanneke",
      "Idan Mehalel",
      "Shay Moran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ea370419760b421ce12e3082eb2ae1a8-Abstract-Conference.html": {
    "title": "Towards Multi-dimensional Explanation Alignment for Medical Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijie Hu",
      "Songning Lai",
      "Wenshuo Chen",
      "Hongru Xiao",
      "Hongbin Lin",
      "Lu Yu",
      "Jingfeng ZHANG",
      "Di Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ea3f85a33f9ba072058e3df233cf6cca-Abstract-Conference.html": {
    "title": "SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Colombo",
      "Telmo Pessoa Pires",
      "Malik Boudiaf",
      "Rui Melo",
      "Gabriel Hautreux",
      "Etienne Malaboeuf",
      "Johanne Charpentier",
      "Dominic Culver",
      "Michael Desa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ea456e232efb72d261715e33ce25f208-Abstract-Conference.html": {
    "title": "Many-shot Jailbreaking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cem Anil",
      "Esin DURMUS",
      "Nina Panickssery",
      "Mrinank Sharma",
      "Joe Benton",
      "Sandipan Kundu",
      "Joshua Batson",
      "Meg Tong",
      "Jesse Mu",
      "Daniel Ford",
      "Francesco Mosconi",
      "Rajashree Agrawal",
      "Rylan Schaeffer",
      "Naomi Bashkansky",
      "Samuel Svenningsen",
      "Mike Lambert",
      "Ansh Radhakrishnan",
      "Carson Denison",
      "Evan Hubinger",
      "Yuntao Bai",
      "Trenton Bricken",
      "Timothy Maxwell",
      "Nicholas Schiefer",
      "James Sully",
      "Alex Tamkin",
      "Tamera Lanham",
      "Karina Nguyen",
      "Tomek Korbak",
      "Jared Kaplan",
      "Deep Ganguli",
      "Samuel Bowman",
      "Ethan Perez",
      "Roger B Grosse",
      "David K. Duvenaud"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ea48cb23a02ec3b895b0b0124307b0ec-Abstract-Conference.html": {
    "title": "Geometry-aware training of factorized layers in tensor Tucker format",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emanuele Zangrando",
      "Steffen Schotthöfer",
      "Gianluca Ceruti",
      "Jonas Kusch",
      "Francesco Tudisco"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ea57fac4f3bdfbe98591f6f3acd3aae6-Abstract-Conference.html": {
    "title": "Semi-Random Matrix Completion via Flow-Based Adaptive Reweighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Kelner",
      "Jerry Li",
      "Allen Liu",
      "Aaron Sidford",
      "Kevin Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ea5a63f7ddb82e58623693fd1f4933f7-Abstract-Conference.html": {
    "title": "Fast yet Safe: Early-Exiting with Risk Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Metod Jazbec",
      "Alexander Timans",
      "Tin Hadži Veljković",
      "Kaspar Sakmann",
      "Dan Zhang",
      "Christian Andersson Naesseth",
      "Eric Nalisnick"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ea6d17af54f827336fc8fed27ca0319d-Abstract-Conference.html": {
    "title": "Regularized Q-Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han-Dong Lim",
      "Donghwan Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ea8153fc07d892e24057ba09a7d38d0c-Abstract-Conference.html": {
    "title": "A Functional Extension of Semi-Structured Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Rügamer",
      "Bernard Liew",
      "Zainab Altai",
      "Almond Stöcker"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ea8620683340facbd5f754dd169e0980-Abstract-Conference.html": {
    "title": "Safe and Sparse Newton Method for Entropic-Regularized Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Tang",
      "Yixuan Qiu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ea888178abdb6fc233226d12321d754f-Abstract-Conference.html": {
    "title": "$\\beta$-DPO: Direct Preference Optimization with Dynamic $\\beta$",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junkang Wu",
      "Yuexiang Xie",
      "Zhengyi Yang",
      "Jiancan Wu",
      "Jinyang Gao",
      "Bolin Ding",
      "Xiang Wang",
      "Xiangnan He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ead13878cd158f013becb6a559a60364-Abstract-Conference.html": {
    "title": "On-Road Object Importance Estimation: A New Dataset and A Model with Multi-Fold Top-Down Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixiong Nan",
      "Yilong Chen",
      "Tianfei Zhou",
      "Tao Xiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ead542f13a38179d1b55b88610f959a1-Abstract-Conference.html": {
    "title": "Block Sparse Bayesian Learning: A Diversified Scheme",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanhao Zhang",
      "Zhihan Zhu",
      "Yong Xia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ead8e195df586b9e7c10fcd114a6b9d1-Abstract-Conference.html": {
    "title": "A Huber Loss Minimization Approach to Mean Estimation under User-level Differential Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Puning Zhao",
      "Lifeng LAI",
      "Li Shen",
      "Qingming Li",
      "Jiafei Wu",
      "Zhe Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eadb6e5ed8a02ada4affb07dfd62ab5e-Abstract-Conference.html": {
    "title": "How to Continually Adapt Text-to-Image Diffusion Models for Flexible Customization?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahua Dong",
      "Wenqi Liang",
      "Hongliu Li",
      "Duzhen Zhang",
      "Meng Cao",
      "Henghui Ding",
      "Salman H Khan",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eae7fa3e1584f46253c891bcb61846b8-Abstract-Conference.html": {
    "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Li",
      "Aijia Zhang",
      "Junqi Gao",
      "Biqing Qi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eaf956b52bae51fbf387b8be4cc3ce18-Abstract-Conference.html": {
    "title": "Online Adaptation of Language Models with a Memory of Amortized Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihoon Tack",
      "Jaehyung Kim",
      "Eric Mitchell",
      "Jinwoo Shin",
      "Yee Whye Teh",
      "Jonathan Richard Schwarz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb0b13cc515724ab8015bc978fdde0ad-Abstract-Conference.html": {
    "title": "Simple and Effective Masked Diffusion Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subham Sahoo",
      "Marianne Arriola",
      "Yair Schiff",
      "Aaron Gokaslan",
      "Edgar Marroquin",
      "Justin Chiu",
      "Alexander Rush",
      "Volodymyr Kuleshov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb113910e9c3f6242541c1652e30dfd6-Abstract-Conference.html": {
    "title": "AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaorun Chen",
      "Zhen J. Xiang",
      "Chaowei Xiao",
      "Dawn Song",
      "Bo Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb1a323fa10d4102ff13422476a744ff-Abstract-Conference.html": {
    "title": "Information Re-Organization Improves Reasoning in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxia Cheng",
      "Zeqi Tan",
      "Wei Xue",
      "Weiming Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb1fc4b546d293b36622e1925f86f14d-Abstract-Conference.html": {
    "title": "Improved learning rates in multi-unit uniform price auctions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marius Potfer",
      "Dorian Baudry",
      "Hugo Richard",
      "Vianney Perchet",
      "Cheng Wan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb2f4fb51ac3b8dc4aac9cf71b0e7799-Abstract-Conference.html": {
    "title": "Ultrafast classical phylogenetic method beats large protein language models on variant effect prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Prillo",
      "Wilson Wu",
      "Yun Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb3878c1dcbfff9ee95d5d033e5f5942-Abstract-Conference.html": {
    "title": "DiffusionPDE: Generative PDE-Solving under Partial Observation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahe Huang",
      "Guandao Yang",
      "Zichen Wang",
      "Jeong Joon Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb5254c4ee813d05af9c098f2d9c5708-Abstract-Conference.html": {
    "title": "Addressing Hidden Confounding with Heterogeneous Observational Datasets for Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanghao Xiao",
      "Haoxuan Li",
      "Yongqiang Tang",
      "Wensheng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb58e6745340616828a053500220f963-Abstract-Conference.html": {
    "title": "Activation Map Compression through Tensor Decomposition for Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le-Trung Nguyen",
      "Aël Quélennec",
      "Enzo Tartaglione",
      "Samuel Tardieu",
      "Van-Tam Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb5dd4476448c44e55a759a985b3bbec-Abstract-Conference.html": {
    "title": "Probing the Decision Boundaries of In-context Learning in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyan Zhao",
      "Tung Nguyen",
      "Aditya Grover"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb6281d3f2cfe2eb8b1307227d3677b3-Abstract-Conference.html": {
    "title": "B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Zancato",
      "Arjun Seshadri",
      "Yonatan Dukler",
      "Aditya Sharad Golatkar",
      "Yantao Shen",
      "Benjamin Bowman",
      "Matthew Trager",
      "Alessandro Achille",
      "Stefano Soatto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb6855c821fda49de4656c983ce66971-Abstract-Conference.html": {
    "title": "Online Posterior Sampling with a Diffusion Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Branislav Kveton",
      "Boris Oreshkin",
      "Youngsuk Park",
      "Aniket Anand Deshmukh",
      "Rui Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb696eb3c461f631389b4b61a8465794-Abstract-Conference.html": {
    "title": "RTify: Aligning Deep Neural Networks with Human Behavioral Decisions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Ang Cheng",
      "Ivan F Rodriguez Rodriguez",
      "Sixuan Chen",
      "Kohitij Kar",
      "Takeo Watanabe",
      "Thomas Serre"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb7295a8bc613b375726659c2ecd6f14-Abstract-Conference.html": {
    "title": "Transductive Learning is Compact",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Asilis",
      "Siddartha Devic",
      "Shaddin Dughmi",
      "Vatsal Sharan",
      "Shang-Hua Teng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb9120be0dcaac0aec66d2e75deb69dd-Abstract-Conference.html": {
    "title": "Self-Guiding Exploration for Combinatorial Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zangir Iklassov",
      "Yali Du",
      "Farkhad Akimov",
      "Martin Takac"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb91e094b1fb67d650470ce603acf085-Abstract-Conference.html": {
    "title": "Minimizing UCB: a Better Local Search Strategy in Local Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZHEYI FAN",
      "Wenyu Wang",
      "Szu Hui Ng",
      "Qingpei Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eb9b18ccb76a1156af5779ffdca1d91f-Abstract-Conference.html": {
    "title": "From Similarity to Superiority: Channel Clustering for Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialin Chen",
      "Jan Eric Lenssen",
      "Aosong Feng",
      "Weihua Hu",
      "Matthias Fey",
      "Leandros Tassiulas",
      "Jure Leskovec",
      "Rex Ying"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ebc62a3af9342eb4ebc728e5c5bc4cca-Abstract-Conference.html": {
    "title": "Resfusion: Denoising Diffusion Probabilistic Models for Image Restoration Based on Prior Residual Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenning Shi",
      "haoshuai zheng",
      "Chen Xu",
      "Changsheng Dong",
      "Bin Pan",
      "Xie xueshuo",
      "Along He",
      "Tao Li",
      "Huazhu Fu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ebd8a977883fd8d5bccf9575e873ce75-Abstract-Conference.html": {
    "title": "Achieving Constant Regret in Linear Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weitong Zhang",
      "Zhiyuan Fan",
      "Jiafan He",
      "Quanquan Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ebf8764ecf0688cdd9fe1e5a9c525d0d-Abstract-Conference.html": {
    "title": "MoGenTS: Motion Generation based on Spatial-Temporal Joint Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihao Yuan",
      "Yisheng HE",
      "Weichao Shen",
      "Yuan Dong",
      "Xiaodong Gu",
      "Zilong Dong",
      "Liefeng Bo",
      "Qixing Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ec045a5ca2d8cfc528591b4c34296370-Abstract-Conference.html": {
    "title": "Average gradient outer product as a mechanism for deep neural collapse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Beaglehole",
      "Peter Súkeník",
      "Marco Mondelli",
      "Misha Belkin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ec2b1931cbda8e4c1a601ff5ff81c4a6-Abstract-Conference.html": {
    "title": "Spectral Adapter: Fine-Tuning in Spectral Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangzhao Zhang",
      "Mert Pilanci"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ec3c79dc0c2b85532cfd1012a4aaa923-Abstract-Conference.html": {
    "title": "Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mustafa Shukor",
      "Matthieu Cord"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ec4c05921d70202023898568a2a8c002-Abstract-Conference.html": {
    "title": "CONTRAST: Continual Multi-source Adaptation to Dynamic Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sk Miraj Ahmed",
      "Fahim Faisal Niloy",
      "Xiangyu Chang",
      "Dripta S. Raychaudhuri",
      "Samet Oymak",
      "Amit Roy-Chowdhury"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ec52572b9e16b91edff5dc70e2642240-Abstract-Conference.html": {
    "title": "On Causal Discovery in the Presence of Deterministic Relations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Loka Li",
      "Haoyue Dai",
      "Hanin Al Ghothani",
      "Biwei Huang",
      "Jiji Zhang",
      "Shahar Harel",
      "Isaac Bentwich",
      "Guangyi Chen",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ecd92623ac899357312aaa8915853699-Abstract-Conference.html": {
    "title": "Unlocking the Capabilities of Masked Generative Models for Image Synthesis via Self-Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiwan Hur",
      "DongJae Lee",
      "Gyojin Han",
      "Jaehyun Choi",
      "Yunho Jeon",
      "Junmo Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ecda225cb187b40ea8edc1f46b03ffda-Abstract-Conference.html": {
    "title": "AutoMix: Automatically Mixing Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranjal Aggarwal",
      "Aman Madaan",
      "Ankit Anand",
      "Srividya Pranavi Potharaju",
      "Swaroop Mishra",
      "Pei Zhou",
      "Aditya Gupta",
      "Dheeraj Rajagopal",
      "Karthik Kappaganthu",
      "Yiming Yang",
      "Shyam Upadhyay",
      "Manaal Faruqui",
      "Mausam"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ecfa5340fd896e5314bc5e132b5dd5ca-Abstract-Conference.html": {
    "title": "Paralinguistics-Aware Speech-Empowered Large Language Models for Natural Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heeseung Kim",
      "Soonshin Seo",
      "Kyeongseok Jeong",
      "Ohsung Kwon",
      "Soyoon Kim",
      "Jungwhan Kim",
      "Jaehong Lee",
      "Eunwoo Song",
      "Myungwoo Oh",
      "Jung-Woo Ha",
      "Sungroh Yoon",
      "Kang Min Yoo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ecffd829f90b0a4b6aa017b6df15904f-Abstract-Conference.html": {
    "title": "Bayesian Online Natural Gradient (BONG)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matt Jones",
      "Peter Chang",
      "Kevin P. Murphy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ed165f2ff227cf36c7e3ef88957dadd9-Abstract-Conference.html": {
    "title": "GREATS: Online Selection of High-Quality Data for LLM Training in Every Iteration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen (Tianhao) Wang",
      "Tong Wu",
      "Dawn Song",
      "Prateek Mittal",
      "Ruoxi Jia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ed1d3d4c64dc1b95332a8cde3f2a0bdf-Abstract-Conference.html": {
    "title": "CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Li",
      "Xinyao Wang",
      "Sijie Zhu",
      "Chia-Wen Kuo",
      "Lu XU",
      "Fan Chen",
      "Jitesh Jain",
      "Humphrey Shi",
      "Longyin Wen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ed24eafde44aef581b9f605319583b6d-Abstract-Conference.html": {
    "title": "Wormhole Loss for Partial Shape Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amit Bracha",
      "Thomas Dagès",
      "Ron Kimmel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ed2dad593d87ca474a636cba610a29d3-Abstract-Conference.html": {
    "title": "Lumina-Next : Making Lumina-T2X Stronger and Faster with Next-DiT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Zhuo",
      "Ruoyi Du",
      "Han Xiao",
      "Yangguang Li",
      "Dongyang Liu",
      "Rongjie Huang",
      "Wenze Liu",
      "Xiangyang Zhu",
      "Fu-Yun Wang",
      "Zhanyu Ma",
      "Xu Luo",
      "Zehan Wang",
      "Kaipeng Zhang",
      "Lirui Zhao",
      "Si Liu",
      "Xiangyu Yue",
      "Wanli Ouyang",
      "Yu Qiao",
      "Hongsheng Li",
      "Peng Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ed3c686f9cda57e56cc859402c775414-Abstract-Conference.html": {
    "title": "Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikai Wang",
      "Xinzhou Wang",
      "Zilong Chen",
      "Zhengyi Wang",
      "Fuchun Sun",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ed3cd2520148b577039adfade82a5566-Abstract-Conference.html": {
    "title": "BackTime: Backdoor Attacks on Multivariate Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Lin",
      "Zhining Liu",
      "Dongqi Fu",
      "Ruizhong Qiu",
      "Hanghang Tong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ed45d6a03de84cc650cae0655f699356-Abstract-Conference.html": {
    "title": "Enhancing Large Vision Language Models with Self-Training on Image Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihe Deng",
      "Pan Lu",
      "Fan Yin",
      "Ziniu Hu",
      "Sheng Shen",
      "Quanquan Gu",
      "James Y Zou",
      "Kai-Wei Chang",
      "Wei Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ed5854c456e136afa3faa5e41b1f3509-Abstract-Conference.html": {
    "title": "Parallelizing Model-based Reinforcement Learning Over the Sequence Length",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Wang",
      "Yue DENG",
      "Junfeng Long",
      "Yin Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ed67dff7cb96e7e86c4d91c0d5db49bb-Abstract-Conference.html": {
    "title": "FreeLong: Training-Free Long Video Generation with SpectralBlend Temporal Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Lu",
      "Yuanzhi Liang",
      "Linchao Zhu",
      "Yi Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ed8baaf9059a5cee3ffe56cedbf26f69-Abstract-Conference.html": {
    "title": "Inference of Neural Dynamics Using Switching Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongxu Zhang",
      "Shreya Saxena"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ed93b2b5722acc2341d421b8916404a1-Abstract-Conference.html": {
    "title": "Nearly Tight Black-Box Auditing of Differentially Private Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meenatchi Sundaram Muthu Selva Annamalai",
      "Emiliano De Cristofaro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ed99503856b2ba85092e5413add65d86-Abstract-Conference.html": {
    "title": "Beyond the Doors of Perception: Vision Transformers Represent Relations Between Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Lepori",
      "Alexa Tartaglini",
      "Wai Keen Vong",
      "Thomas Serre",
      "Brenden M Lake",
      "Ellie Pavlick"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ed9dcde1eb9c597f68c1d375bbecf3fc-Abstract-Conference.html": {
    "title": "LLMDFA: Analyzing Dataflow in Code with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengpeng Wang",
      "Wuqi Zhang",
      "Zian Su",
      "Xiangzhe Xu",
      "Xiaoheng Xie",
      "Xiangyu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ed9f00cb7dd5fbdc2175d55e2fdf1b05-Abstract-Conference.html": {
    "title": "Unveiling LoRA Intrinsic Ranks via Salience Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjun Ke",
      "Jiahao Wang",
      "Peng Wang",
      "Jiajun Liu",
      "Dong Nie",
      "Guozheng Li",
      "Yining Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eda3ee49bf6e868c35dc741948517891-Abstract-Conference.html": {
    "title": "Rethinking No-reference Image Exposure Assessment from Holism to Pixel: Models, Datasets and Benchmarks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai He",
      "Shuntian Zheng",
      "Anlong Ming",
      "Banyu Wu",
      "Huadong Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/edbeca7811f9365c924c72a8a9bce83a-Abstract-Conference.html": {
    "title": "Compositional 3D-aware Video Generation with LLM Director",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanxin Zhu",
      "Tianyu He",
      "Anni Tang",
      "Junliang Guo",
      "Zhibo Chen",
      "Jiang Bian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/edcbd38b2509b57954d38de6cd9c05b3-Abstract-Conference.html": {
    "title": "Equivariant Blurring Diffusion for Hierarchical Molecular Conformer Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiwoong Park",
      "Yang Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/edcd1aa172dceda2ea9d45a48f25d3e3-Abstract-Conference.html": {
    "title": "PhyloGen: Language Model-Enhanced Phylogenetic Inference via Graph Structure Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ChenRui Duan",
      "Zelin Zang",
      "Siyuan Li",
      "Yongjie Xu",
      "Stan Z. Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ede2d0f5b99f93632098e89c9e77a361-Abstract-Conference.html": {
    "title": "Learning Bregman Divergences with Application to Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed-Hicham LEGHETTAS",
      "Markus Püschel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eded464db02ad5b5138970efe194050a-Abstract-Conference.html": {
    "title": "Sharpness-Aware Minimization Activates the Interactive Teaching's Understanding and Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingwei Xu",
      "Xiaofeng Cao",
      "Ivor W. Tsang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/edfd99d02718585697fda9df97fcf53d-Abstract-Conference.html": {
    "title": "Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuri Fonseca",
      "Caio Peixoto",
      "Yuri Saporito"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee0e45ff4de76cbfdf07015a7839f339-Abstract-Conference.html": {
    "title": "Why Transformers Need Adam: A Hessian Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushun Zhang",
      "Congliang Chen",
      "Tian Ding",
      "Ziniu Li",
      "Ruoyu Sun",
      "Zhiquan Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee1331338cdf0e4d1055304d875548df-Abstract-Conference.html": {
    "title": "Using Noise to Infer Aspects of Simplicity Without Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zachery Boner",
      "Harry Chen",
      "Lesia Semenova",
      "Ronald Parr",
      "Cynthia Rudin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee20461718669c6c9c5da478d46d60d9-Abstract-Conference.html": {
    "title": "Energy-based Hopfield Boosting for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Claus Hofmann",
      "Simon Schmid",
      "Bernhard Lehner",
      "Daniel Klotz",
      "Sepp Hochreiter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee3ce0121939f42098cdefd3ea025bf1-Abstract-Conference.html": {
    "title": "Neglected Hessian component explains mysteries in sharpness regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yann N. Dauphin",
      "Atish Agarwala",
      "Hossein Mobahi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee45939f4403e8abd63a15a29a9c055b-Abstract-Conference.html": {
    "title": "A Label is Worth A Thousand Images in Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Qin",
      "Zhiwei Deng",
      "David Alvarez-Melis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee46288ab2aaf5c6e53aebebe719712c-Abstract-Conference.html": {
    "title": "LLaMo: Large Language Model-based Molecular Graph Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyoung Park",
      "Minseong Bae",
      "Dohwan Ko",
      "Hyunwoo J Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee5884bc4908108d672e1c8e940c6c7b-Abstract-Conference.html": {
    "title": "Transformers as Game Players: Provable In-context Game-playing Capabilities of Pre-trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengshuai Shi",
      "Kun Yang",
      "Jing Yang",
      "Cong Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee5bb72130c332c3d4bf8d231e617506-Abstract-Conference.html": {
    "title": "Hierarchical Object-Aware Dual-Level Contrastive Learning for Domain Generalized Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikun Miao",
      "Meiqing Wu",
      "Siew Kei Lam",
      "Changsheng Li",
      "Thambipillai Srikanthan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee62ab636066cf45a27246acca9545b7-Abstract-Conference.html": {
    "title": "RAW: A Robust and Agile Plug-and-Play Watermark Framework for AI-Generated Images with Provable Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xun Xian",
      "Ganghua Wang",
      "Xuan Bi",
      "Jayanth Srinivasa",
      "Ashish Kundu",
      "Mingyi Hong",
      "Jie Ding"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee657707107aa886da19ef6c4b311975-Abstract-Conference.html": {
    "title": "Long-Tailed Out-of-Distribution Detection via Normalized Outlier Distribution Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjun Miao",
      "Guansong Pang",
      "Jin Zheng",
      "Xiao Bai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee66188f019df7199c4c06320f698fa1-Abstract-Conference.html": {
    "title": "Conformal Prediction for Class-wise Coverage via Augmented Label Rank Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanjie Shi",
      "Subhankar Ghosh",
      "Taha Belkhouja",
      "Jana Doppa",
      "Yan Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee6c4b99b4c0d3d60efd22c1ecdd9891-Abstract-Conference.html": {
    "title": "SDformer: Similarity-driven Discrete Transformer For Time Series Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhicheng Chen",
      "FENG SHIBO",
      "Zhong Zhang",
      "Xi Xiao",
      "Xingyu Gao",
      "Peilin Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee71a4b14ec26710b39ee6be113d7750-Abstract-Conference.html": {
    "title": "Chain of Agents: Large Language Models Collaborating on Long-Context Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusen Zhang",
      "Ruoxi Sun",
      "Yanfei Chen",
      "Tomas Pfister",
      "Rui Zhang",
      "Sercan Arik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee74a6ade401e200985e2421b20bbae4-Abstract-Conference.html": {
    "title": "Scene Graph Generation with Role-Playing Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guikun Chen",
      "Jin Li",
      "Wenguan Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee81a23d6b83ac15fbeb5b7a30934e0b-Abstract-Conference.html": {
    "title": "Parameter-Inverted Image Pyramid Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xizhou Zhu",
      "Xue Yang",
      "Zhaokai Wang",
      "Hao Li",
      "Wenhan Dou",
      "Junqi Ge",
      "Lewei Lu",
      "Yu Qiao",
      "Jifeng Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee8493dfdca7ead9f06c235725f98033-Abstract-Conference.html": {
    "title": "Universal Sample Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Szymon Kobus",
      "Tze-Yang Tung",
      "Deniz Gunduz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ee94bf235482e4c1f689c04c81656dbf-Abstract-Conference.html": {
    "title": "Exogenous Matching: Learning Good Proposals for Tractable Counterfactual Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikang Chen",
      "Dehui du",
      "Lili Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eeab2e00835c71d64458ad1821e05664-Abstract-Conference.html": {
    "title": "Fast Graph Sharpness-Aware Minimization for Enhancing and Accelerating Few-Shot Node Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Luo",
      "Yuhan Chen",
      "Siya Qiu",
      "Yiwei Wang",
      "Chen Zhang",
      "Yan Zhou",
      "Xiaochun Cao",
      "Jing Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eeae43a68515325cad64c0f54b2d0c70-Abstract-Conference.html": {
    "title": "Attention boosted Individualized Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guang Yang",
      "Yuan Cao",
      "Long Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eed57814c16645298db3164829e2e45c-Abstract-Conference.html": {
    "title": "NVRC: Neural Video Representation Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ho Man Kwan",
      "Ge Gao",
      "Fan Zhang",
      "Andrew Gower",
      "David Bull"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ef107bb91dae96368c856b1064370bd0-Abstract-Conference.html": {
    "title": "Molecule Generation with Fragment Retrieval Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seul Lee",
      "Karsten Kreis",
      "Srimukh Veccham",
      "Meng Liu",
      "Danny Reidenbach",
      "Saee Paliwal",
      "Arash Vahdat",
      "Weili Nie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ef283d62b4bce30854a8d4827f331229-Abstract-Conference.html": {
    "title": "A Topology-aware Graph Coarsening Framework for Continual Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxue Han",
      "Zhuo Feng",
      "Yue Ning"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ef52fd1e24634cb8f7003ebbfb3644d9-Abstract-Conference.html": {
    "title": "Scanning Trojaned Models Using Out-of-Distribution Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hossein Mirzaei",
      "Ali Ansari",
      "Bahar Dibaei Nia",
      "Mojtaba Nafez",
      "Moein Madadi",
      "Sepehr Rezaee",
      "Zeinab Sadat Taghavi",
      "Arad Maleki",
      "Kian Shamsaie",
      "Mahdi Hajialilue",
      "Jafar Habibi",
      "Mohammad Sabokrou",
      "Mohammad Hossein Rohban"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ef62614753535977071395fb1f1435be-Abstract-Conference.html": {
    "title": "Model Sensitivity Aware Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyi Wang",
      "Heng Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ef63b00ad8475605b2eaf520747f61d4-Abstract-Conference.html": {
    "title": "DataStealing: Steal Data from Diffusion Models in Federated Learning with Multiple Trojans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Gan",
      "Jiaxu Miao",
      "Yi Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ef6b667990416111e3ebbeeba4a8f283-Abstract-Conference.html": {
    "title": "Nonparametric Evaluation of Noisy ICA Solutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Syamantak Kumar",
      "Derek Bean",
      "peter j bickel",
      "Purnamrita Sarkar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ef72fa6579401ffff9da246a5014f055-Abstract-Conference.html": {
    "title": "EigenVI: score-based variational inference with orthogonal function expansions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diana Cai",
      "Chirag Modi",
      "Charles Margossian",
      "Robert Gower",
      "David M. Blei",
      "Lawrence K. Saul"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ef74413c7bf1d915c3e45c72e19a5d32-Abstract-Conference.html": {
    "title": "Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuaipeng Li",
      "Penghao Zhao",
      "Hailin Zhang",
      "Xingwu Sun",
      "Hao Wu",
      "Dian Jiao",
      "Weiyan Wang",
      "Chengjun Liu",
      "Zheng Fang",
      "Jinbao Xue",
      "Yangyu Tao",
      "Bin CUI",
      "Di Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ef7a500bc90d45e362c82d63a0eff22d-Abstract-Conference.html": {
    "title": "Towards Understanding Evolving Patterns in Sequential Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "QIUHAO Zeng",
      "Long-Kai Huang",
      "Qi CHEN",
      "Charles X. Ling",
      "Boyu Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ef7dd4a0e8622530ce87ae7d2265aa4a-Abstract-Conference.html": {
    "title": "Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mark Rowland",
      "Kevin Li",
      "Remi Munos",
      "Clare Lyle",
      "Yunhao Tang",
      "Will Dabney"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ef8080f246b4b2c7a2b26daee6e6f22a-Abstract-Conference.html": {
    "title": "MVSDet: Multi-View Indoor 3D Object Detection via Efficient Plane Sweeps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yating Xu",
      "Chen Li",
      "Gim Hee Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/efa9e3ed9c411aeb53d7bfe38a19d884-Abstract-Conference.html": {
    "title": "Approximating mutual information of high-dimensional variables using learned representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gokul Gowri",
      "Xiaokang Lun",
      "Allon Klein",
      "Peng Yin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/efaf1c9726648c8ba363a5c927440529-Abstract-Conference.html": {
    "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxin He",
      "Yijun Tian",
      "Yifei Sun",
      "Nitesh Chawla",
      "Thomas Laurent",
      "Yann LeCun",
      "Xavier Bresson",
      "Bryan Hooi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/efd1e27afcb94addd03b9e14c8d9f78f-Abstract-Conference.html": {
    "title": "FOOGD: Federated Collaboration for Both Out-of-distribution Generalization and Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinting Liao",
      "Weiming Liu",
      "Pengyang Zhou",
      "Fengyuan Yu",
      "Jiahe Xu",
      "Jun Wang",
      "Wenjie Wang",
      "Chaochao Chen",
      "Xiaolin Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/efe4e50d492fedc0dfd2959f3320a974-Abstract-Conference.html": {
    "title": "Right this way: Can VLMs Guide Us to See More to Answer Questions?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Liu",
      "Diji Yang",
      "Sijia Zhong",
      "Kalyana Suma Sree Tholeti",
      "Lei Ding",
      "Yi Zhang",
      "Leilani Gilpin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/eff4231d13b71a2859ce90242ed6ad8d-Abstract-Conference.html": {
    "title": "Entropy testing and its application to testing Bayesian networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clément L Canonne",
      "Qiping Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f0262cfa14a20f833fcdae1719b144ec-Abstract-Conference.html": {
    "title": "Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philip Amortila",
      "Dylan J Foster",
      "Nan Jiang",
      "Akshay Krishnamurthy",
      "Zak Mhammedi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f02a816cd50c0b1441601dbde012fa24-Abstract-Conference.html": {
    "title": "CNCA: Toward Customizable and Natural Generation of Adversarial Camouflage for Vehicle Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linye Lyu",
      "Jiawei Zhou",
      "Daojing He",
      "YU LI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f02d7fb7ddd2e6be33b6f3224e5cc44a-Abstract-Conference.html": {
    "title": "Erasing Undesirable Concepts in Diffusion Models with Adversarial Preservation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anh Bui",
      "Tung-Long Vuong",
      "Khanh Doan",
      "Trung Le",
      "Paul Montague",
      "Tamas Abraham",
      "Dinh Phung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f03fc3545c3cfb3aa696ad6d58eed1a7-Abstract-Conference.html": {
    "title": "Hybrid Top-Down Global Causal Discovery with Local Search for Linear and Nonlinear Additive Noise Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sujai Hiremath",
      "Jacqueline Maasch",
      "Mengxiao Gao",
      "Promit Ghosal",
      "Kyra Gan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f074a994e062146561db9cdc63999efa-Abstract-Conference.html": {
    "title": "Causal Context Adjustment Loss for Learned Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghao Han",
      "Shiyin Jiang",
      "Shengxi Li",
      "Xin Deng",
      "Mai Xu",
      "Ce Zhu",
      "Shuhang Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f0b1515be276f6ba82b4f2b25e50bef0-Abstract-Conference.html": {
    "title": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyin Ma",
      "Gongfan Fang",
      "Michael Bi Mi",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f0b42291ddab77dcb2ef8a3488301b62-Abstract-Conference.html": {
    "title": "3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Liu",
      "Chaoyi Zhou",
      "Siyu Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f0c68d99827dc09ed28aa073455efcbe-Abstract-Conference.html": {
    "title": "Classifier-guided Gradient Modulation for Enhanced Multimodal Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirun Guo",
      "Tao Jin",
      "Jingyuan Chen",
      "Zhou Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f0d629a734b56a642701bba7bc8bb3ed-Abstract-Conference.html": {
    "title": "Discrete Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Itai Gat",
      "Tal Remez",
      "Neta Shaul",
      "Felix Kreuk",
      "Ricky T. Q. Chen",
      "Gabriel Synnaeve",
      "Yossi Adi",
      "Yaron Lipman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f0ebc318e2df08360b2df559e81602e5-Abstract-Conference.html": {
    "title": "Large Language Models Play StarCraft II:Benchmarks and A Chain of Summarization Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyu Ma",
      "Qirui Mi",
      "Yongcheng Zeng",
      "Xue Yan",
      "Runji Lin",
      "Yuqiao Wu",
      "Jun Wang",
      "Haifeng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f10ceee5c6979988f334058561cac89f-Abstract-Conference.html": {
    "title": "Ex Uno Pluria: Insights on Ensembling in Low Precision Number Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giung Nam",
      "Juho Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f11394cdd377aab9ff5e2a4e9f27367f-Abstract-Conference.html": {
    "title": "Treatment of Statistical Estimation Problems in Randomized Smoothing for Adversarial Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vaclav Voracek"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f12d4e75bb8c62aba3e88d0586af96d3-Abstract-Conference.html": {
    "title": "EnsIR: An Ensemble Algorithm for Image Restoration via Gaussian Mixture Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangquan Sun",
      "Wenqi Ren",
      "Zikun Liu",
      "Hyunhee Park",
      "Rui Wang",
      "Xiaochun Cao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f13159aecc416659a3c6cef0aecd0d94-Abstract-Conference.html": {
    "title": "Any2Policy: Learning Visuomotor Policy with Any-Modality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Zhu",
      "Zhicai Ou",
      "Feifei Feng",
      "Jian Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f144ab9985c739a5091ec188a2688644-Abstract-Conference.html": {
    "title": "Exact, Tractable Gauss-Newton Optimization in Deep Reversible Architectures Reveal Poor Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Buffelli",
      "Jamie McGowan",
      "Wangkun Xu",
      "Alexandru Cioba",
      "Da-shan Shiu",
      "Guillaume Hennequin",
      "Alberto Bernacchia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f1592b0d4ab737e18bb1899484d28d96-Abstract-Conference.html": {
    "title": "CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junho Kim",
      "Hyunjun Kim",
      "Kim Yeonju",
      "Yong Man Ro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f15cc4c30e8e5116e1e874e4895b74d0-Abstract-Conference.html": {
    "title": "Uniform Last-Iterate Guarantee for Bandits and Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyan Liu",
      "Yunfan Li",
      "Ruosong Wang",
      "Lin Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f162fa05675e3db4a733aafc081653cf-Abstract-Conference.html": {
    "title": "Accelerating Diffusion Models with Parallel Sampling: Inference at Sub-Linear Time Complexity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxuan Chen",
      "Yinuo Ren",
      "Lexing Ying",
      "Grant Rotskoff"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f169ec4d47933ea4896b994af8ff4f17-Abstract-Conference.html": {
    "title": "Enhancing LLM's Cognition via Structurization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Liu",
      "Zhihang Fu",
      "Chao Chen",
      "Wei Zhang",
      "Rongxin Jiang",
      "Fan Zhou",
      "Yaowu Chen",
      "Yue Wu",
      "Jieping Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f18222190deda602622b3ed90d28bd98-Abstract-Conference.html": {
    "title": "Offline Oracle-Efficient Learning for Contextual MDPs via Layerwise Exploration-Exploitation Tradeoff",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Qian",
      "Haichen Hu",
      "David Simchi-Levi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f19e6e04ed32735cb0e52bdfe6282673-Abstract-Conference.html": {
    "title": "Is Multiple Object Tracking a Matter of Specialization?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gianluca Mancusi",
      "Mattia Bernardi",
      "Aniello Panariello",
      "Angelo Porrello",
      "Rita Cucchiara",
      "SIMONE CALDERARA"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f1a92c4df8cd7dc1cab2613fb999d5e7-Abstract-Conference.html": {
    "title": "DiffuPac: Contextual Mimicry in Adversarial Packets Generation via Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdullah Bin Jasni",
      "Akiko Manada",
      "Kohei Watabe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f1aa53cac69ef6980ca4a911ffcf278b-Abstract-Conference.html": {
    "title": "Alleviating Distortion in Image Generation via Multi-Resolution Diffusion Models and Time-Dependent Layer Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihao Liu",
      "Zhanpeng Zeng",
      "Ju He",
      "Qihang Yu",
      "Xiaohui Shen",
      "Liang-Chieh Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f1bb7ac6fb8b63a509e08e7b127d7236-Abstract-Conference.html": {
    "title": "Off-policy estimation with adaptively collected data: the power of online learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghwan Lee",
      "Cong Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f1c131d41c3d5119a8de329140b3384e-Abstract-Conference.html": {
    "title": "FedNE: Surrogate-Assisted Federated Neighbor Embedding for Dimensionality Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziwei Li",
      "Xiaoqi Wang",
      "Hong-You Chen",
      "Han Wei Shen",
      "Wei-Lun (Harry) Chao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f1cf02ce09757f57c3b93c0db83181e0-Abstract-Conference.html": {
    "title": "Exploring the trade-off between deep-learning and explainable models for brain-machine interfaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luis Cubillos",
      "Guy Revach",
      "Matthew Mender",
      "Joseph Costello",
      "Hisham Temmar",
      "Aren Hite",
      "Diksha Anoop Kumar Zutshi",
      "Dylan Wallace",
      "Xiaoyong Ni",
      "Madison Kelberman",
      "Matt Willsey",
      "Ruud Van Sloun",
      "Nir Shlezinger",
      "Parag Patil",
      "Anne Draelos",
      "Cynthia Chestek"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f1d8400fec75f683c4d823f5836a81bb-Abstract-Conference.html": {
    "title": "Detecting Bugs with Substantial Monetary Consequences by LLM and Rule-based Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Zhang",
      "Zhuo Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f1e115bc9e9d700b93ce24f0d4f640ad-Abstract-Conference.html": {
    "title": "SAND: Smooth imputation of sparse and noisy functional data with Transformer networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ju-Sheng Hong",
      "Junwen Yao",
      "Jonas W Mueller",
      "Jane-Ling Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f1e7c90552850afcc2558d78950c519d-Abstract-Conference.html": {
    "title": "Adaptive Randomized Smoothing: Certified Adversarial Robustness for Multi-Step Defences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saiyue Lyu",
      "Shadab Shaikh",
      "Frederick Shpilevskiy",
      "Evan Shelhamer",
      "Mathias Lécuyer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f1f9962f76581ce8bf38d04c6d6c96b1-Abstract-Conference.html": {
    "title": "EDT: An Efficient Diffusion Transformer Framework Inspired by Human-like Sketching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinwang Chen",
      "Ning Liu",
      "Yichen Zhu",
      "Feifei Feng",
      "Jian Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f20cc4ba33ee8ed8734a0456ba00127d-Abstract-Conference.html": {
    "title": "Hamiltonian Monte Carlo on ReLU Neural Networks is Inefficient",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vu C Dinh",
      "Lam S Ho",
      "Cuong V. Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f21a76d688be0553c943a6b6c1d4bb1f-Abstract-Conference.html": {
    "title": "Certified Adversarial Robustness via Randomized $\\alpha$-Smoothing for Regression Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aref Rekavandi",
      "Farhad Farokhi",
      "Olga Ohrimenko",
      "Benjamin I. Rubinstein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f23098fa0cfcdef0e743b134d380eeb9-Abstract-Conference.html": {
    "title": "An Improved Empirical Fisher Approximation for Natural Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaodong Wu",
      "Wenyi Yu",
      "Chao Zhang",
      "Phil Woodland"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f23653913d8390cd4fc1bee8a3238e17-Abstract-Conference.html": {
    "title": "The Benefits of Balance: From Information Projections to Variance Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lang Liu",
      "Ronak Mehta",
      "Soumik Pal",
      "Zaid Harchaoui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f249db9ab5975586f36df46f8958c008-Abstract-Conference.html": {
    "title": "Meta-Controller: Few-Shot Imitation of Unseen Embodiments and Tasks in Continuous Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongwoong Cho",
      "Donggyun Kim",
      "Jinwoo Lee",
      "Seunghoon Hong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f24e8cc1c1c06a689850ee766a7357b2-Abstract-Conference.html": {
    "title": "ChronoEpilogi: Scalable Time Series Selection with Multiple Solutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Etienne Vareille",
      "Michele Linardi",
      "Ioannis Tsamardinos",
      "Vassilis Christophides"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f2543511e5f4d4764857f9ad833a977d-Abstract-Conference.html": {
    "title": "Kernel PCA for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Fang",
      "Qinghua Tao",
      "Kexin Lv",
      "Mingzhen He",
      "Xiaolin Huang",
      "JIE YANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f257fe05388ba5d98b7c6089b1841bd0-Abstract-Conference.html": {
    "title": "Bisimulation Metrics are Optimal Transport Distances, and Can be Computed Efficiently",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergio Calo",
      "Anders Jonsson",
      "Gergely Neu",
      "Ludovic Schwartz",
      "Javier Segovia-Aguas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f25d75fc760aec0a6174f9f5d9da59b8-Abstract-Conference.html": {
    "title": "InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchun Miao",
      "Sen Zhang",
      "Liang Ding",
      "Rong Bao",
      "Lefei Zhang",
      "Dacheng Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f2621bfa8b39cd3c5a361273aa7ec4db-Abstract-Conference.html": {
    "title": "Computational Aspects of Bayesian Persuasion under Approximate Best Response",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunhe Yang",
      "Hanrui Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f269520d5f111719dd82b36667997d49-Abstract-Conference.html": {
    "title": "Learning Where to Edit Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunqiao Yang",
      "Long-Kai Huang",
      "Shengzhuang Chen",
      "Kede Ma",
      "Ying Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f26d4fbaf7dfa115f1d4b3f104e26bce-Abstract-Conference.html": {
    "title": "Semantic Density: Uncertainty Quantification for Large Language Models through Confidence Measurement in Semantic Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Qiu",
      "Risto Miikkulainen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f271a36160097fbdb06a9adeb1605343-Abstract-Conference.html": {
    "title": "Understanding the Differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jerome Sieber",
      "Carmen Amo Alonso",
      "Alexandre Didier",
      "Melanie Zeilinger",
      "Antonio Orvieto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f2733d3b0dde1d74995f35a9cf442d38-Abstract-Conference.html": {
    "title": "Adam on Local Time: Addressing Nonstationarity in RL with Relative Adam Timesteps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Ellis",
      "Matthew T Jackson",
      "Andrei Lupu",
      "Alexander D. Goldie",
      "Mattie Fellows",
      "Shimon Whiteson",
      "Jakob Foerster"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f29369d192b13184b65c6d2515474d78-Abstract-Conference.html": {
    "title": "Understanding Hallucinations in Diffusion Models through Mode Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumukh K Aithal",
      "Pratyush Maini",
      "Zachary Lipton",
      "J. Zico Kolter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f2ae937c93d45def2b0eb246831c8685-Abstract-Conference.html": {
    "title": "Beating Adversarial Low-Rank MDPs with Unknown Transition and Bandit Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haolin Liu",
      "Zak Mhammedi",
      "Chen-Yu Wei",
      "Julian Zimmert"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f2bb120e9a2cb9c2a50921b7f865c421-Abstract-Conference.html": {
    "title": "To Err Like Human: Affective Bias-Inspired Measures for Visual Emotion Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxi Zhao",
      "Jinglei Shi",
      "Liqiang Nie",
      "Jufeng Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f2c5a0d8e445cc359eb446521f438ee3-Abstract-Conference.html": {
    "title": "Drago: Primal-Dual Coupled Variance Reduction for Faster Distributionally Robust Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronak Mehta",
      "Jelena Diakonikolas",
      "Zaid Harchaoui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f30e35a09ac622dec1c121a13dd809d4-Abstract-Conference.html": {
    "title": "Neural Combinatorial Optimization for Robust Routing Problem with Uncertain Travel Times",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei Xiao",
      "Zizhen Zhang",
      "Jinbiao Chen",
      "Jiahai Wang",
      "Zhenzhen Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f33e853ba1f5f038268f9839e37821d5-Abstract-Conference.html": {
    "title": "Attack-Aware Noise Calibration for Differential Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bogdan Kulynych",
      "Juan Gomez",
      "Georgios Kaissis",
      "Flavio Calmon",
      "Carmela Troncoso"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f358b2a880adf34939d2d6f926e54d2a-Abstract-Conference.html": {
    "title": "SeeClear: Semantic Distillation Enhances Pixel Condensation for Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Tang",
      "Yao Zhao",
      "Meiqin Liu",
      "Chao Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f359ff8b0966cfc6cad70aaf251622ac-Abstract-Conference.html": {
    "title": "Optimal Batched Best Arm Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyuan Jin",
      "Yu Yang",
      "Jing Tang",
      "Xiaokui Xiao",
      "Pan Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f36ad694188bb4c4bbbd61e2038e069e-Abstract-Conference.html": {
    "title": "Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Fu",
      "Huandong Wang",
      "Chen Gao",
      "Guanghua Liu",
      "Yong Li",
      "Tao Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f381114cf5aba4e45552869863deaaa7-Abstract-Conference.html": {
    "title": "Metric Flow Matching for Smooth Interpolations on the Data Manifold",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kacper Kapusniak",
      "Peter Potaptchik",
      "Teodora Reu",
      "Leo Zhang",
      "Alexander Tong",
      "Michael Bronstein",
      "Joey Bose",
      "Francesco Di Giovanni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f38a1f38090503d301ee15276232f3f6-Abstract-Conference.html": {
    "title": "Feature-Level Adversarial Attacks and Ranking Disruption for Visible-Infrared Person Re-identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Yang",
      "Huanling Liu",
      "De Cheng",
      "Nannan Wang",
      "Xinbo Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f38cb4cf9a5eaa92b3cfa481832719c6-Abstract-Conference.html": {
    "title": "SpatialRGPT: Grounded Spatial Reasoning in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "An-Chieh Cheng",
      "Hongxu Yin",
      "Yang Fu",
      "Qiushan Guo",
      "Ruihan Yang",
      "Jan Kautz",
      "Xiaolong Wang",
      "Sifei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f39931608cdc52d7d9f8ba7003af9136-Abstract-Conference.html": {
    "title": "Language Model as Visual Explainer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyi Yang",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f3c8bfcc9c2bc13a0f141cb58afc6e5a-Abstract-Conference.html": {
    "title": "Practical Bayesian Algorithm Execution via Posterior Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chu Xin Cheng",
      "Raul Astudillo",
      "Thomas A Desautels",
      "Yisong Yue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f3da4165893c2465fd7e8df453c41ffa-Abstract-Conference.html": {
    "title": "Unveiling the Potential of Robustness in Selecting Conditional Average Treatment Effect Estimators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyan Huang",
      "Cheuk Hang LEUNG",
      "Siyi WANG",
      "YIJUN LI",
      "Qi WU"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f3e644506dad33613919fa85af6665d0-Abstract-Conference.html": {
    "title": "Intrinsic Robustness of Prophet Inequality to Strategic Reward Signaling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Tang",
      "Haifeng Xu",
      "Ruimin Zhang",
      "Derek Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f40b522ef32bbc050d37a2a1ab7ffc2b-Abstract-Conference.html": {
    "title": "Unrolled denoising networks provably learn to perform optimal Bayesian inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aayush Karan",
      "Kulin Shah",
      "Sitan Chen",
      "Yonina Eldar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f445ba15f0f05c26e1d24f908ea78d60-Abstract-Conference.html": {
    "title": "Improving Environment Novelty Quantification for Effective Unsupervised Environment Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jayden Teoh",
      "Wenjun Li",
      "Pradeep Varakantham"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f463d31ed2fdd7b0ec585c041ec1baa8-Abstract-Conference.html": {
    "title": "The Bayesian sampling in a canonical recurrent circuit with a diversity of inhibitory interneurons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eryn Sale",
      "Wenhao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f46b6689a645184b5ff84b4feb3e7bb4-Abstract-Conference.html": {
    "title": "Evaluate then Cooperate: Shapley-based View Cooperation Enhancement for Multi-view Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangdi Wang",
      "Jiaqi Jin",
      "Jingtao Hu",
      "Suyuan Liu",
      "Xihong Yang",
      "Siwei Wang",
      "Xinwang Liu",
      "En Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f4757db82a02eea015670ecca605d5cc-Abstract-Conference.html": {
    "title": "PureGen: Universal Data Purification for Train-Time Poison Defense via Generative Model Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omead Pooladzandi",
      "Sunay Bhat",
      "Jeffrey Jiang",
      "Alexander Branch",
      "Gregory Pottie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f478d99e1e47b9d5e4a3127f33545719-Abstract-Conference.html": {
    "title": "A Local Method for Satisfying Interventional Fairness with Partially Known Causal Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxuan Li",
      "Yue Liu",
      "Zhi Geng",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f48877d7081b83c220ddafb3fcd0a0e3-Abstract-Conference.html": {
    "title": "Learning Generalized Linear Programming Value Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tu Anh-Nguyen",
      "Joey Huchette",
      "Christian Tjandraatmadja"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f49287371916715b9209fa41a275851e-Abstract-Conference.html": {
    "title": "Unveil Benign Overfitting for Transformer in Vision: Training Dynamics, Convergence, and Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarui Jiang",
      "Wei Huang",
      "Miao Zhang",
      "Taiji Suzuki",
      "Liqiang Nie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f4b6ef2a78684dca2fb3f1c09372e041-Abstract-Conference.html": {
    "title": "AirSketch: Generative Motion to Sketch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Xian Grace Lim",
      "Xuanming Cui",
      "Yogesh Rawat",
      "Ser Nam Lim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f4c6bec746b0aeca8c2cd15096f1ad1f-Abstract-Conference.html": {
    "title": "Weak Supervision Performance Evaluation via Partial Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felipe Maia Polo",
      "Subha Maity",
      "Mikhail Yurochkin",
      "Moulinath Banerjee",
      "Yuekai Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f4dbdf190269a839704d9595bce2b11e-Abstract-Conference.html": {
    "title": "Avoiding Undesired Future with Minimal Cost in Non-Stationary Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen-Bo Du",
      "Tian Qin",
      "Tian-Zuo Wang",
      "Zhi-Hua Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f4fba41b554f9aaa013c4062a1c40518-Abstract-Conference.html": {
    "title": "A Concept-Based Explainability Framework for Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jayneel Parekh",
      "Pegah KHAYATAN",
      "Mustafa Shukor",
      "Alasdair Newson",
      "Matthieu Cord"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f4fdb30c290191eacac8993079cb45b6-Abstract-Conference.html": {
    "title": "Learning to Balance Altruism and Self-interest Based on Empathy in Mixed-Motive Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanqi Kong",
      "Yizhe Huang",
      "Song-Chun Zhu",
      "Siyuan Qi",
      "Xue Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f4ff9a0cbd424dc6a30342e356e7659a-Abstract-Conference.html": {
    "title": "UniIF: Unified Molecule Inverse Folding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangyang Gao",
      "Jue Wang",
      "Cheng Tan",
      "Lirong Wu",
      "Yufei Huang",
      "Siyuan Li",
      "Zhirui Ye",
      "Stan Z. Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f50b83e4ca6d57a9b59a2316ba298002-Abstract-Conference.html": {
    "title": "Motif-oriented influence maximization for viral marketing in large-scale social networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyang Zhou",
      "Weiji Cao",
      "Hao Liao",
      "Rui Mao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f5198bc255e1d5f959edd6d1d1a86fab-Abstract-Conference.html": {
    "title": "DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhehao Zhang",
      "Jiaao Chen",
      "Diyi Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f5332c8273d02729730a9c24dec2135e-Abstract-Conference.html": {
    "title": "Noether's Razor: Learning Conserved Quantities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tycho van der Ouderaa",
      "Mark van der Wilk",
      "Pim de Haan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f536d5697b79a9b3b3debbb7a552a7da-Abstract-Conference.html": {
    "title": "Recurrent neural network dynamical systems for biological vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wayne Soo",
      "Aldo Battista",
      "Puria Radmard",
      "Xiao-Jing Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f53a37f820d5be5930415d964f4a0187-Abstract-Conference.html": {
    "title": "Proximal Causal Inference With Text Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Chen",
      "Rohit Bhattacharya",
      "Katherine Keith"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f543648b4332a4d8b9e1b72c6b4e2e26-Abstract-Conference.html": {
    "title": "Quality-Improved and Property-Preserved Polarimetric Imaging via Complementarily Fusing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chu Zhou",
      "Yixing Liu",
      "Chao Xu",
      "Boxin Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f545448535dfde4f9786555403ab7c49-Abstract-Conference.html": {
    "title": "Refusal in Language Models Is Mediated by a Single Direction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andy Arditi",
      "Oscar Obeso",
      "Aaquib Syed",
      "Daniel Paleka",
      "Nina Panickssery",
      "Wes Gurnee",
      "Neel Nanda"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f573c36434796efe066d2f4cf3349e7f-Abstract-Conference.html": {
    "title": "Fine-Tuning is Fine, if Calibrated",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheda Mai",
      "Arpita Chowdhury",
      "Ping Zhang",
      "Cheng-Hao Tu",
      "Hong-You Chen",
      "Vardaan Pahuja",
      "Tanya Berger-Wolf",
      "Song Gao",
      "Charles Stewart",
      "Yu Su",
      "Wei-Lun (Harry) Chao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f57de20ab7bb1540bcac55266ebb5401-Abstract-Conference.html": {
    "title": "UniGAD: Unifying Multi-level Graph Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqing Lin",
      "Jianheng Tang",
      "Chenyi Zi",
      "H. Vicky Zhao",
      "Yuan Yao",
      "Jia Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f5a8b5e5d007e66c929b971c2bc21d76-Abstract-Conference.html": {
    "title": "Exocentric-to-Egocentric Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia-Wei Liu",
      "Weijia Mao",
      "Zhongcong XU",
      "Jussi Keppo",
      "Mike Zheng Shou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f5fcd88d3deb97bb62559208cfa0ab62-Abstract-Conference.html": {
    "title": "X-Ray: A Sequential 3D Representation For Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Hu",
      "Wenhang Ge",
      "Yuyang Zhao",
      "Gim Hee Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f606d45ae7b991988b6eea2af38b7057-Abstract-Conference.html": {
    "title": "KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengcheng Jiang",
      "Lang Cao",
      "Cao (Danica) Xiao",
      "Parminder Bhatia",
      "Jimeng Sun",
      "Jiawei Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f620c653a8f196076f9a2fbc3c9d7efb-Abstract-Conference.html": {
    "title": "Constructing Semantics-Aware Adversarial Examples with a Probabilistic Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andi Zhang",
      "Mingtian Zhang",
      "Damon Wischik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f623b4d99ca41f3f6e2029fdcd231d9e-Abstract-Conference.html": {
    "title": "Data subsampling for Poisson regression with pth-root-link",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Cheng Lie",
      "Alexander Munteanu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f6278e53b11fe607e76fa4c1409032a8-Abstract-Conference.html": {
    "title": "Off-Dynamics Reinforcement Learning via Domain Adaptation and Reward Augmented Imitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Guo",
      "Yixuan Wang",
      "Yuanyuan Shi",
      "Pan Xu",
      "Anqi Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f629a3d821cf01c34db63e38fac03041-Abstract-Conference.html": {
    "title": "Efficient Policy Evaluation Across Multiple Different Experimental Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yonghan Jung",
      "Alexis Bellot"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f631e778fd3c1b871e9e3a94369335e9-Abstract-Conference.html": {
    "title": "Exploring Consistency in Graph Representations: from Graph Kernels to Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuyuan Liu",
      "Yinghao Cai",
      "Qihui Yang",
      "Yujun Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f65d490e9f6b26894919d111306dbbb8-Abstract-Conference.html": {
    "title": "Dynamics of Supervised and Reinforcement Learning in the Non-Linear Perceptron",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Schmid",
      "James M Murray"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f670ef96387d9a5a8a51e2ed80cb148d-Abstract-Conference.html": {
    "title": "Adaptive Visual Scene Understanding: Incremental Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naitik Khandelwal",
      "Xiao Liu",
      "Mengmi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f67449c7ab72f441d3a713b046c6818c-Abstract-Conference.html": {
    "title": "DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Pagliardini",
      "Amirkeivan Mohtashami",
      "François Fleuret",
      "Martin Jaggi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f6adf61977467560f79b95485d1f3a79-Abstract-Conference.html": {
    "title": "Introducing Spectral Attention for Long-Range Dependency in Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bong Gyun Kang",
      "Dongjun Lee",
      "HyunGi Kim",
      "Dohyun Chung",
      "Sungroh Yoon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f6b22ac37beb5da61efd4882082c9ecd-Abstract-Conference.html": {
    "title": "AdaPKC: PeakConv with Adaptive Peak Receptive Field for Radar Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teng Li",
      "Liwen Zhang",
      "Youcheng Zhang",
      "ZijunHu",
      "Pengcheng Pi",
      "Zongqing Lu",
      "Qingmin Liao",
      "Zhe Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f6b35e248a21c71ff1cd47b8919fca83-Abstract-Conference.html": {
    "title": "Prompt-Agnostic Adversarial Perturbation for Customized Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Wan",
      "Yuhang He",
      "Xiang Song",
      "Yihong Gong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f6ec12322f9f9b25d07afcdd25604005-Abstract-Conference.html": {
    "title": "Improving Generalization in Federated Learning with Model-Data Mutual Information Regularization: A Posterior Inference Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhang",
      "Chenglin Li",
      "Nuowen Kan",
      "Ziyang Zheng",
      "Wenrui Dai",
      "Junni Zou",
      "Hongkai Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f6f4b34d255c2c6c2391af975bed0428-Abstract-Conference.html": {
    "title": "Zero-shot Generalizable Incremental Learning for Vision-Language Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieren Deng",
      "Haojian Zhang",
      "Kun Ding",
      "Jianhua Hu",
      "Xingxuan Zhang",
      "Yunkuan Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f72599679c20c6e59878c075706132f1-Abstract-Conference.html": {
    "title": "Get Rid of Isolation: A Continuous Multi-task Spatio-Temporal Learning Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongchao Yi",
      "Zhengyang Zhou",
      "Qihe Huang",
      "Yanjiang Chen",
      "Liheng Yu",
      "Xu Wang",
      "Yang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f72d4fdfd5eb425cd81df9fe6272a533-Abstract-Conference.html": {
    "title": "Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiquan Lu",
      "Xiaotian Liu",
      "Yefan Zhou",
      "Qunli Li",
      "Kurt Keutzer",
      "Michael W. Mahoney",
      "Yujun Yan",
      "Huanrui Yang",
      "Yaoqing Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f7344147dbd1607deac3a7e5f33a23aa-Abstract-Conference.html": {
    "title": "Autonomous Driving with Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui-Jie Zhu",
      "Ziqing Wang",
      "Leilani Gilpin",
      "Jason Eshraghian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f74054328beeb0c21a9b8e99da557f5a-Abstract-Conference.html": {
    "title": "AED: Adaptable Error Detection for Few-shot Imitation Policy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia-Fong Yeh",
      "Kuo-Han Hung",
      "Pang-Chi Lo",
      "Chi Ming Chung",
      "Tsung-Han Wu",
      "Hung-Ting Su",
      "Yi-Ting Chen",
      "Winston Hsu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f751c6f8bfb52c60f43942896fe65904-Abstract-Conference.html": {
    "title": "SPRINQL: Sub-optimal Demonstrations driven Offline Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Hoang",
      "Tien Mai",
      "Pradeep Varakantham"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f75744612447126da06767daecce1a84-Abstract-Conference.html": {
    "title": "Aligning LLM Agents by Learning Latent Preference from User Edits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ge Gao",
      "Alexey Taymanov",
      "Eduardo Salinas",
      "Paul Mineiro",
      "Dipendra Misra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f77012b463892a3f2ac56d3a6086fc5b-Abstract-Conference.html": {
    "title": "Unchosen Experts Can Contribute Too: Unleashing MoE Models' Power by Self-Contrast",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chufan Shi",
      "Cheng Yang",
      "Xinyu Zhu",
      "Jiahao Wang",
      "Taiqiang Wu",
      "Siheng Li",
      "Deng Cai",
      "Yujiu Yang",
      "Yu Meng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f775e2e0e7e12227adecbbf945f43546-Abstract-Conference.html": {
    "title": "Unitary Convolutions for Learning on Graphs and Groups",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bobak Kiani",
      "Lukas Fesser",
      "Melanie Weber"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f782860c2a5d8f675b0066522b8c2cf2-Abstract-Conference.html": {
    "title": "Transfer Learning for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yidong Ouyang",
      "Liyan Xie",
      "Hongyuan Zha",
      "Guang Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f7a7bb369e48f10e85fce85b67d8c516-Abstract-Conference.html": {
    "title": "Reinforcement Learning Guided Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marzi Heidari",
      "Hanping Zhang",
      "Yuhong Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f7ae4fe91d96f50abc2211f09b6a7e49-Abstract-Conference.html": {
    "title": "FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyang Yu",
      "Zhiyuan Yao",
      "Haohang Li",
      "Zhiyang Deng",
      "Yuechen Jiang",
      "Yupeng Cao",
      "Zhi Chen",
      "Jordan Suchow",
      "Zhenyu Cui",
      "Rong Liu",
      "Zhaozhuo Xu",
      "Denghui Zhang",
      "Koduvayur (Suba) Subbalakshmi",
      "GUOJUN XIONG",
      "Yueru He",
      "Jimin Huang",
      "Dong Li",
      "Qianqian Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f7b1906009df59a3a5b186c3b87f89ab-Abstract-Conference.html": {
    "title": "Towards Effective Planning Strategies for Dynamic Opinion Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bharath Muppasani",
      "Protik Nag",
      "Vignesh Narayanan",
      "Biplav Srivastava",
      "Michael Huhns"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f7d3cef7ff579f2f903c8f458e730cae-Abstract-Conference.html": {
    "title": "Object segmentation from common fate: Motion energy processing enables human-like zero-shot generalization to random dot stimuli",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthias Tangemann",
      "Matthias Kümmerer",
      "Matthias Bethge"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f7f47a73d631c0410cbc2748a8015241-Abstract-Conference.html": {
    "title": "Understanding Multi-Granularity for Open-Vocabulary Part Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiho Choi",
      "Seonho Lee",
      "Seungho Lee",
      "Minhyun Lee",
      "Hyunjung Shim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f7faa46b563c2e5343a728c85bace833-Abstract-Conference.html": {
    "title": "Diffusion Imitation from Observation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo-Ruei Huang",
      "Chun-Kai Yang",
      "Chun-Mao Lai",
      "Dai-Jie Wu",
      "Shao-Hua Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f82385b8804009f9a81e1a30f1ff14e3-Abstract-Conference.html": {
    "title": "Generative Modelling of Structurally Constrained Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Madeira",
      "Clement Vignac",
      "Dorina Thanou",
      "Pascal Frossard"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f8238b414fce7d91fedb896253545300-Abstract-Conference.html": {
    "title": "Efficient Discrepancy Testing for Learning with Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gautam Chandrasekaran",
      "Adam Klivans",
      "Vasilis Kontonis",
      "Konstantinos Stavropoulos",
      "Arsen Vasilyan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f84ceafb242f0de36f8c49452fbae6de-Abstract-Conference.html": {
    "title": "EAGLE: Efficient Adaptive Geometry-based Learning in Cross-view Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thanh-Dat Truong",
      "Utsav Prabhu",
      "Dongyi Wang",
      "Bhiksha Raj",
      "Susan Gauch",
      "Jeyamkondan Subbiah",
      "Khoa Luu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f85364507054c257959c2011c28bfc0d-Abstract-Conference.html": {
    "title": "Interpreting the Weight Space of Customized Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amil Dravid",
      "Yossi Gandelsman",
      "Kuan-Chieh Wang",
      "Rameen Abdal",
      "Gordon Wetzstein",
      "Alexei Efros",
      "Kfir Aberman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f88264fcc54775ee1706116e90fe351a-Abstract-Conference.html": {
    "title": "Task-oriented Time Series Imputation Evaluation via Generalized Representers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixian Wang",
      "Linxiao Yang",
      "Liang Sun",
      "Qingsong Wen",
      "Yi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f88cc8930b47a45ec4733123bf3039b9-Abstract-Conference.html": {
    "title": "Linguistic Collapse: Neural Collapse in (Large) Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert Wu",
      "Vardan Papyan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f89221edad5a6a4a54fcf247cb37cd62-Abstract-Conference.html": {
    "title": "Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongwon Jo",
      "Taesu Kim",
      "Yulhwa Kim",
      "jae-joon kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f896c593ae0cce758205646e1325406b-Abstract-Conference.html": {
    "title": "Confusion-Resistant Federated Learning via Diffusion-Based Data Harmonization on Non-IID Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "xiaohong chen",
      "Canran Xiao",
      "Yongmei liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f8a2070082ad05b4deeff4ffb4312a6f-Abstract-Conference.html": {
    "title": "Transformers on Markov data: Constant depth suffices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nived Rajaraman",
      "Marco Bondaschi",
      "Ashok Vardhan Makkuva",
      "Kannan Ramchandran",
      "Michael Gastpar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f8b1ea224de27a460488db5b854f6c53-Abstract-Conference.html": {
    "title": "ProTransformer: Robustify Transformers via Plug-and-Play Paradigm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhichao Hou",
      "Weizhi Gao",
      "Yuchen Shen",
      "Feiyi Wang",
      "Xiaorui Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f8ce25dcb2cb0eb8a24b492bf3e84695-Abstract-Conference.html": {
    "title": "Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taihang Hu",
      "Linxuan Li",
      "Joost van de Weijer",
      "Hongcheng Gao",
      "Fahad Shahbaz Khan",
      "Jian Yang",
      "Ming-Ming Cheng",
      "KAI WANG",
      "Yaxing Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f8de10c9ff056ae3d1eef43ad1762351-Abstract-Conference.html": {
    "title": "Learning Complete Protein Representation by Dynamically Coupling of Sequence and Structure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bozhen Hu",
      "Cheng Tan",
      "Jun Xia",
      "Yue Liu",
      "Lirong Wu",
      "Jiangbin Zheng",
      "Yongjie Xu",
      "Yufei Huang",
      "Stan Z. Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f900437fc473e78e88dc2e021f0fa512-Abstract-Conference.html": {
    "title": "Graph Classification via Reference Distribution Learning: Theory and Practice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixiao Wang",
      "Jicong Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f90338be8ad676ee02cc32d239fc40e7-Abstract-Conference.html": {
    "title": "Diffusion Twigs with Loop Guidance for Conditional Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giangiacomo Mercatali",
      "Yogesh Verma",
      "Andre Freitas",
      "Vikas Garg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f90d2001ec7595d4b8fdfa4db966d154-Abstract-Conference.html": {
    "title": "Partial Transportability for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kasra Jalaldoust",
      "Alexis Bellot",
      "Elias Bareinboim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f930c6e1292a1160189a0734f22b465b-Abstract-Conference.html": {
    "title": "Learning the Infinitesimal Generator of Stochastic Diffusion Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladimir Kostic",
      "Hélène Halconruy",
      "Timothée Devergne",
      "Karim Lounici",
      "Massimiliano Pontil"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f93d03f2ad836c815b7ca60dfbe23bf8-Abstract-Conference.html": {
    "title": "FNP: Fourier Neural Processes for Arbitrary-Resolution Data Assimilation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Chen",
      "Peng Ye",
      "Hao Chen",
      "kang chen",
      "Tao Han",
      "Wanli Ouyang",
      "Tao Chen",
      "LEI BAI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f949c1f490beb42124a267b7476cd353-Abstract-Conference.html": {
    "title": "Computing the Bias of Constant-step Stochastic Approximation with Markovian Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Allmeier",
      "Nicolas Gast"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f94cfd15db3f16ee7789b6b7e91ec476-Abstract-Conference.html": {
    "title": "InversionView: A General-Purpose Method for Reading Information from Neural Activations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinting Huang",
      "Madhur Panwar",
      "Navin Goyal",
      "Michael Hahn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f9668d223e713943634dce9c66e8f2c1-Abstract-Conference.html": {
    "title": "Learning to Reason Iteratively and Parallelly for Complex Visual Reasoning Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shantanu Jaiswal",
      "Debaditya Roy",
      "Basura Fernando",
      "Cheston Tan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f96839fc751b67492e17e70f5c9730e4-Abstract-Conference.html": {
    "title": "Divergences between Language Models and Human Brains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Zhou",
      "Emmy Liu",
      "Graham Neubig",
      "Michael Tarr",
      "Leila Wehbe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f96af360d2a1b1585c3e3a5b82ba4ef7-Abstract-Conference.html": {
    "title": "Going Beyond Heuristics by Imposing Policy Improvement as a Constraint",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi-Chang Lee",
      "Zhang-Wei Hong",
      "Pulkit Agrawal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f9743a7bfee6592e3ff913ffadd8a857-Abstract-Conference.html": {
    "title": "Human Expertise in Algorithmic Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohan Alur",
      "Manish Raghavan",
      "Devavrat Shah"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f99f7b22ad47fa6ce151730cf8d17911-Abstract-Conference.html": {
    "title": "Unsupervised Anomaly Detection in The Presence of Missing Values",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Xiao",
      "Jicong Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f9a50cf037f5ca2f687e3cd70b572c6f-Abstract-Conference.html": {
    "title": "Linear Time Approximation Algorithm for Column Subset Selection with Local Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YuanBin Zou",
      "Ziyun Huang",
      "Jinhui Xu",
      "Jianxin Wang",
      "Qilong Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f9af936fc42039578ace655a82d928aa-Abstract-Conference.html": {
    "title": "Bounds for the smallest eigenvalue of the NTK for arbitrary spherical data of arbitrary dimension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kedar Karhadkar",
      "Michael Murray",
      "Guido F. Montufar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f9c2ab8d429044e0c35bcece2ff6d123-Abstract-Conference.html": {
    "title": "Neural collapse vs. low-rank bias: Is deep neural collapse really optimal?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Súkeník",
      "Christoph H. Lampert",
      "Marco Mondelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f9db8bd38c36391ddc4ccc0d23effdbe-Abstract-Conference.html": {
    "title": "An Analysis of Elo Rating Systems via Markov Chains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sam Olesker-Taylor",
      "Luca Zanetti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f9dc462382fef56d58279e75de2438f3-Abstract-Conference.html": {
    "title": "Fine-grained Analysis of In-context Linear Estimation: Data, Architecture, and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingcong Li",
      "Ankit Rawat",
      "Samet Oymak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f9e72ee379bb781f3005775c870a3871-Abstract-Conference.html": {
    "title": "Enhancing Diversity in Bayesian Deep Learning via Hyperspherical Energy Minimization of CKA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Smerkous",
      "Qinxun Bai",
      "Fuxin Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f9f54762cbb4fe4dbffdd4f792c31221-Abstract-Conference.html": {
    "title": "Truth is Universal: Robust Detection of Lies in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lennart Bürger",
      "Fred A. Hamprecht",
      "Boaz Nadler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/f9f6c8115fac0277c50f506c47f2c781-Abstract-Conference.html": {
    "title": "Theoretical guarantees in KL for Diffusion Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marta Gentiloni Silveri",
      "Alain Durmus",
      "Giovanni Conforti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fa0509f4dab6807e2cb465715bf2d249-Abstract-Conference.html": {
    "title": "Improving the Learning Capability of Small-size Image Restoration Network by Deep Fourier Shifting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "man zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fa080fe0f218871faec1d8ba20e491d5-Abstract-Conference.html": {
    "title": "Thought of Search: Planning with Language Models Through The Lens of Efficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Katz",
      "Harsha Kokel",
      "Kavitha Srinivas",
      "Shirin Sohrabi Araghi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fa31574791443e8e7f38045b98584aa9-Abstract-Conference.html": {
    "title": "TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weichao Zeng",
      "Yan Shu",
      "Zhenhang Li",
      "Dongbao Yang",
      "Yu Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fa54b0edce5eef0bb07654e8ee800cb4-Abstract-Conference.html": {
    "title": "Reflective Multi-Agent Collaboration based on Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohe Bo",
      "Zeyu Zhang",
      "Quanyu Dai",
      "Xueyang Feng",
      "Lei Wang",
      "Rui Li",
      "Xu Chen",
      "Ji-Rong Wen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fa5b423e24b442180bcd4e13ae75a27f-Abstract-Conference.html": {
    "title": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingkang Yang",
      "Jinjie Wei",
      "Dongling Xiao",
      "Shunli Wang",
      "Tong Wu",
      "Gang Li",
      "Mingcheng Li",
      "Shuaibing Wang",
      "Jiawei Chen",
      "Yue Jiang",
      "Qingyao Xu",
      "Ke Li",
      "Peng Zhai",
      "Lihua Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fa69e968b7319fd42524febd41475fb3-Abstract-Conference.html": {
    "title": "Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihan Liu",
      "Miao Lu",
      "Shenao Zhang",
      "Boyi Liu",
      "Hongyi Guo",
      "Yingxiang Yang",
      "Jose Blanchet",
      "Zhaoran Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fa76985f05e0a25c66528308dda33de0-Abstract-Conference.html": {
    "title": "Scalable Constrained Policy Optimization for Safe Multi-agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijun Zhang",
      "Lin Li",
      "Wei Wei",
      "Huizhong Song",
      "Yaodong Yang",
      "Jiye Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fa7f64b45970e6a7f8824781e7e01501-Abstract-Conference.html": {
    "title": "Segment Anything without Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "XuDong Wang",
      "Jingfeng Yang",
      "Trevor Darrell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fa86a9c7b9f341716ccb679d1aeb9afa-Abstract-Conference.html": {
    "title": "No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Pang",
      "Shengyuan Hu",
      "Wenting Zheng",
      "Virginia Smith"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fa948624dfde013671e72c1a7ca4aebc-Abstract-Conference.html": {
    "title": "VISA: Variational Inference with Sequential Sample-Average Approximations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heiko Zimmermann",
      "Christian Andersson Naesseth",
      "Jan-Willem van de Meent"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fa9c7d0f08785e607b7eacac49805d6e-Abstract-Conference.html": {
    "title": "Amortized Eigendecomposition for Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianbo Li",
      "Zekun Shi",
      "Jiaxi Zhao",
      "Min Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fa9cfdb49f7de3dee54007f84c0745b1-Abstract-Conference.html": {
    "title": "AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xixi Hu",
      "Qiang Liu",
      "Xingchao Liu",
      "Bo Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fa9e9b2a5176c6f72e78269087b9fe60-Abstract-Conference.html": {
    "title": "pcaGAN: Improving Posterior-Sampling cGANs via Principal Component Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Bendel",
      "Rizwan Ahmad",
      "Philip Schniter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/faa6276ea12d7afeb3e42b210c86f688-Abstract-Conference.html": {
    "title": "Unified Guidance for Geometry-Conditioned Molecular Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sirine Ayadi",
      "Leon Hetzel",
      "Johanna Sommer",
      "Fabian J. Theis",
      "Stephan Günnemann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/faacb7a4827b4d51e201666b93ab5fa7-Abstract-Conference.html": {
    "title": "Off-Policy Selection for Initiating Human-Centric Experimental Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ge Gao",
      "Xi Yang",
      "Qitong Gao",
      "Song Ju",
      "Miroslav Pajic",
      "Min Chi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/facf3192e99ce60c0ef5ed4067b72f68-Abstract-Conference.html": {
    "title": "GUIDE: Real-Time Human-Shaped Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingyu Zhang",
      "Zhengran Ji",
      "Nicholas Waytowich",
      "Boyuan Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fad7c708dda11f3e72cc1629bb130379-Abstract-Conference.html": {
    "title": "SemFlow: Binding Semantic Segmentation and Image Synthesis via Rectified Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoyang Wang",
      "Xiangtai Li",
      "Lu Qi",
      "Henghui Ding",
      "Yunhai Tong",
      "Ming-Hsuan Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fad8962279154544ed69bb63eb14d677-Abstract-Conference.html": {
    "title": "Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingwen Bu",
      "Jia Zeng",
      "Li Chen",
      "Yanchao Yang",
      "Guyue Zhou",
      "Junchi Yan",
      "Ping Luo",
      "Heming Cui",
      "Yi Ma",
      "Hongyang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fad8e1915f66161581bb127ccf01092e-Abstract-Conference.html": {
    "title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Qu",
      "Aditi Krishnapriyan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fadec8f2e65f181d777507d1df69b92f-Abstract-Conference.html": {
    "title": "Fisher Flow Matching for Generative Modeling over Discrete Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oscar Davis",
      "Samuel Kessler",
      "Mircea Petrache",
      "Ismail Ceylan",
      "Michael Bronstein",
      "Joey Bose"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fae2e63d2ffd3e67d238b5a372febc9b-Abstract-Conference.html": {
    "title": "Novel Object Synthesis via Adaptive Text-Image Harmony",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeren Xiong",
      "Zedong Zhang",
      "Zikun Chen",
      "Shuo Chen",
      "Xiang Li",
      "Gan Sun",
      "Jian Yang",
      "Jun Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fb23cf87a9e04d7677b73c47acd060ef-Abstract-Conference.html": {
    "title": "Scalable and Effective Arithmetic Tree Generation for Adder and Multiplier Designs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Lai",
      "Jinxin Liu",
      "David Z. Pan",
      "Ping Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fb3ad59a84799bfb8d700e56d19c231b-Abstract-Conference.html": {
    "title": "Analysing the Generalisation and Reliability of Steering Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Tan",
      "David Chanin",
      "Aengus Lynch",
      "Brooks Paige",
      "Dimitrios Kanoulas",
      "Adrià Garriga-Alonso",
      "Robert Kirk"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fb42bb10564271d0cf3cc8244ff3e5bb-Abstract-Conference.html": {
    "title": "Adversarially Robust Multi-task Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Austin Watkins",
      "Thanh Nguyen-Tang",
      "Enayat Ullah",
      "Raman Arora"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fb4b2fb2434f7cce5cb5ab50271296ee-Abstract-Conference.html": {
    "title": "AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuze He",
      "Wang Zhao",
      "Shaohui Liu",
      "Yubin Hu",
      "Yushi Bai",
      "Yu-Hui Wen",
      "Yong-jin Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fb6279dab0e9282261e4311301b7f822-Abstract-Conference.html": {
    "title": "Efficiency of the First-Price Auction in the Autobidding World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Deng",
      "Jieming Mao",
      "Vahab Mirrokni",
      "Hanrui Zhang",
      "Song Zuo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fb64a43508e0cfe53ee6179ff31ea900-Abstract-Conference.html": {
    "title": "Search for Efficient Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Shen",
      "Pu Zhao",
      "Yifan Gong",
      "Zhenglun Kong",
      "Zheng Zhan",
      "Yushu Wu",
      "Ming Lin",
      "Chao Wu",
      "Xue Lin",
      "Yanzhi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fb7214d2fdfd84165b08539d59c92e07-Abstract-Conference.html": {
    "title": "Query-Efficient Correlation Clustering with Noisy Oracle",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuko Kuroki",
      "Atsushi Miyauchi",
      "Francesco Bonchi",
      "Wei Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fb82011040977c7712409fbdb5456647-Abstract-Conference.html": {
    "title": "Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushi Hu",
      "Weijia Shi",
      "Xingyu Fu",
      "Dan Roth",
      "Mari Ostendorf",
      "Luke Zettlemoyer",
      "Noah A. Smith",
      "Ranjay Krishna"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fba9133d7fa896ab9414ddd1a6b1ecbf-Abstract-Conference.html": {
    "title": "Enhancing Robustness of Last Layer Two-Stage Fair Model Corrections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Stromberg",
      "Rohan Ayyagari",
      "Sanmi Koyejo",
      "Richard Nock",
      "Lalitha Sankar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fbb07254ef01868967dc891ea3fa6c13-Abstract-Conference.html": {
    "title": "Recurrent neural networks: vanishing and exploding gradients are not the end of the story",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Zucchet",
      "Antonio Orvieto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fbe2b2f74a2ece8070d8fb073717bda6-Abstract-Conference.html": {
    "title": "Boosting Text-to-Video Generative Model with MLLMs Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xun Wu",
      "Shaohan Huang",
      "Guolong Wang",
      "Jing Xiong",
      "Furu Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fbf1fe4819b359f5715c4989b9df3725-Abstract-Conference.html": {
    "title": "Fully Distributed, Flexible Compositional Visual Representations via Soft Tensor Products",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bethia Sun",
      "Maurice Pagnucco",
      "Yang Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fc16089c664195e01139ca7d7432c5bb-Abstract-Conference.html": {
    "title": "Fair GLASSO: Estimating Fair Graphical Models with Unbiased Statistical Behavior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Madeline Navarro",
      "Samuel Rey",
      "Andrei Buciulea",
      "Antonio G. Marques",
      "Santiago Segarra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fc1dce23e0ba01d3b691789cfd4f65c3-Abstract-Conference.html": {
    "title": "The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by Leveraging Second-Order Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diyuan Wu",
      "Ionut-Vlad Modoranu",
      "Mher Safaryan",
      "Denis Kuznedelev",
      "Dan Alistarh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fc69c48e41f7143e2e570f68c0b0951c-Abstract-Conference.html": {
    "title": "Unified Speech Recognition: A Single Model for Auditory, Visual, and Audiovisual Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandros Haliassos",
      "Rodrigo Mira",
      "Honglie Chen",
      "Zoe Landgraf",
      "Stavros Petridis",
      "Maja Pantic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fc8781fb328fb1fd069584a4519a2709-Abstract-Conference.html": {
    "title": "LG-VQ: Language-Guided Codebook Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Guotao",
      "Baoquan Zhang",
      "Yaowei Wang",
      "Yunming Ye",
      "Xutao Li",
      "Wanghuaibin",
      "Luo Chuyao",
      "kolaye",
      "luolinfeng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fc9f83d9925e6885e8f1ae1e17b3c44b-Abstract-Conference.html": {
    "title": "VCR-GauS: View Consistent Depth-Normal Regularizer for Gaussian Surface Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanlin Chen",
      "Fangyin Wei",
      "Chen Li",
      "Tianxin Huang",
      "Yunsong Wang",
      "Gim Hee Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fca83589e85cb061631b7ebc5db5d6bd-Abstract-Conference.html": {
    "title": "HAWK: Learning to Understand Open-World Video Anomalies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Tang",
      "Hao LU",
      "RUIZHENG WU",
      "Xiaogang Xu",
      "Ke Ma",
      "Cheng Fang",
      "Bin Guo",
      "Jiangbo Lu",
      "Qifeng Chen",
      "Yingcong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fcc22e5b7d5d2155d994da22d045f0a6-Abstract-Conference.html": {
    "title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akiyoshi Tomihari",
      "Issei Sato"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fcc3dc27672a12510babe448d665e152-Abstract-Conference.html": {
    "title": "Nearly Optimal Approximation of Matrix Functions by the Lanczos Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noah Amsel",
      "Tyler Chen",
      "Anne Greenbaum",
      "Cameron Musco",
      "Christopher Musco"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fcd812a51b8f8d05cfea22e3c9c4b369-Abstract-Conference.html": {
    "title": "OneRef: Unified One-tower Expression Grounding and Segmentation with Mask Referring Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linhui Xiao",
      "Xiaoshan Yang",
      "Fang Peng",
      "Yaowei Wang",
      "Changsheng Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fce1243cf0efe675d670745d902d7b4c-Abstract-Conference.html": {
    "title": "A Swiss Army Knife for Heterogeneous Federated Learning: Flexible Coupling via Trace Norm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianchi Liao",
      "Lele Fu",
      "Jialong Chen",
      "Zhen Wang",
      "Zibin Zheng",
      "Chuan Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fce2d8a485746f76aac7b5650db2679d-Abstract-Conference.html": {
    "title": "Probing Social Bias in Labor Market Text Generation by ChatGPT: A Masked Language Model Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Ding",
      "Yang Hu",
      "Nicole Denier",
      "Enze Shi",
      "Junxi Zhang",
      "Qirui Hu",
      "Karen Hughes",
      "Linglong Kong",
      "Bei Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fce44e39545c20c2595cb84bdb862136-Abstract-Conference.html": {
    "title": "Safety through feedback in Constrained RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashank Reddy Chirra",
      "Pradeep Varakantham",
      "Praveen Paruchuri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fceae9d27e68e183d71aaa88030f52ee-Abstract-Conference.html": {
    "title": "Regularized Conditional Diffusion Model for Multi-Task Preference Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Yu",
      "Chenjia Bai",
      "Haoran He",
      "Changhong Wang",
      "Xuelong Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fd0705710bf01b88a60a3d479ea341d9-Abstract-Conference.html": {
    "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akide Liu",
      "Jing Liu",
      "Zizheng Pan",
      "Yefei He",
      "Reza Haffari",
      "Bohan Zhuang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fd23a1f3bc89e042d70960b466dc20e8-Abstract-Conference.html": {
    "title": "Neural Localizer Fields for Continuous 3D Human Pose and Shape Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "István Sárándi",
      "Gerard Pons-Moll"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fd3c52db29af5d483b154f9abf02de19-Abstract-Conference.html": {
    "title": "Learning Partitions from Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Buchholz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fd512d61c862e3614686f26a0e09de71-Abstract-Conference.html": {
    "title": "Speaking Your Language: Spatial Relationships in Interpretable Emergent Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olaf Lipinski",
      "Adam Sobey",
      "Federico Cerutti",
      "Timothy Norman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fd881d3b625437354d4421818f81058f-Abstract-Conference.html": {
    "title": "LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwen Fan",
      "Kevin Wang",
      "Kairun Wen",
      "Zehao Zhu",
      "Dejia Xu",
      "Zhangyang &quot;Atlas&quot; Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fd9c2c5d018264c28791cda8bd60c8ce-Abstract-Conference.html": {
    "title": "Rapid Plug-in Defenders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Wu",
      "yujian li",
      "Jian Lou",
      "Xiaoyu Zhang",
      "Handing Wang",
      "Jing Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fda447b299f186d22b2de3ec23d75dcf-Abstract-Conference.html": {
    "title": "Quantitative Convergences of Lie Group Momentum Optimizers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingkai Kong",
      "Molei Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fdb0c77c157d066942f060ae193395c1-Abstract-Conference.html": {
    "title": "ZOPP: A Framework of Zero-shot Offboard Panoptic Perception for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao MA",
      "Hongbin Zhou",
      "Qiusheng Huang",
      "Xuemeng Yang",
      "Jianfei Guo",
      "Bo Zhang",
      "Min Dou",
      "Yu Qiao",
      "Botian Shi",
      "Hongsheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fdb11be1acf5e3724737dd585e590146-Abstract-Conference.html": {
    "title": "An Offline Adaptation Framework for Constrained Multi-Objective Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Lin",
      "Zongkai Liu",
      "Danying Mo",
      "Chao Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fdf14882bd645b5a37c912bbc16928c6-Abstract-Conference.html": {
    "title": "On Tractable $\\Phi$-Equilibria in Non-Concave Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Cai",
      "Constantinos Daskalakis",
      "Haipeng Luo",
      "Chen-Yu Wei",
      "Weiqiang Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fdf3c2bd19867fc59cdc49d4e569f6d2-Abstract-Conference.html": {
    "title": "Achievable Fairness on Your Data With Utility Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Faaiz Taufiq",
      "Jean-Francois Ton",
      "Yang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fdff3c4130c24c40c88aa41eb52d2a27-Abstract-Conference.html": {
    "title": "A New Neural Kernel Regime: The Inductive Bias of Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julia Nakhleh",
      "Joseph Shenouda",
      "Robert Nowak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fe03053bd2cf5b5c56de1e463bc53e1a-Abstract-Conference.html": {
    "title": "MKGL: Mastery of a Three-Word Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingbing Guo",
      "Zhongpu Bo",
      "Zhuo Chen",
      "Yichi Zhang",
      "Jiaoyan Chen",
      "Lan Yarong",
      "Mengshu Sun",
      "Zhiqiang Zhang",
      "Yangyifei Luo",
      "Qian Li",
      "Qiang Zhang",
      "Wen Zhang",
      "Huajun Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fe066022bab2a6c6a3c57032a1623c70-Abstract-Conference.html": {
    "title": "MatFormer: Nested Transformer for Elastic Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Devvrit",
      "Sneha Kudugunta",
      "Aditya Kusupati",
      "Tim Dettmers",
      "Kaifeng Chen",
      "Inderjit Dhillon",
      "Yulia Tsvetkov",
      "Hannaneh Hajishirzi",
      "Sham Kakade",
      "Ali Farhadi",
      "Prateek Jain"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fe224a60b878e79d5b3d79d7f113f76b-Abstract-Conference.html": {
    "title": "Reinforcement Learning Policy as Macro Regulator Rather than Macro Placer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Xue",
      "Ruo-Tong Chen",
      "Xi Lin",
      "Yunqi Shi",
      "Shixiong Kai",
      "Siyuan Xu",
      "Chao Qian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fe248e22b241ae5a9adf11493c8c12bc-Abstract-Conference.html": {
    "title": "UniTS: A Unified Multi-Task Time Series Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanghua Gao",
      "Teddy Koker",
      "Owen Queen",
      "Tom Hartvigsen",
      "Theodoros Tsiligkaridis",
      "Marinka Zitnik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fe489a28a54583ee802b8e2955c024c2-Abstract-Conference.html": {
    "title": "Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johannes Treutlein",
      "Dami Choi",
      "Jan Betley",
      "Samuel Marks",
      "Cem Anil",
      "Roger B Grosse",
      "Owain Evans"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fe61e76998bbe3db53a6a48fa58207e9-Abstract-Conference.html": {
    "title": "Penalty-based Methods for Simple Bilevel Optimization under Hölderian Error Bounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyu Chen",
      "Xu Shi",
      "Rujun Jiang",
      "Jiulin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fe692980c5d9732cf153ce27947653a7-Abstract-Conference.html": {
    "title": "KptLLM: Unveiling the Power of Large Language Model for Keypoint Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Yang",
      "Wang ZENG",
      "Sheng Jin",
      "Lumin Xu",
      "Wentao Liu",
      "Chen Qian",
      "Ruimao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fe6bfafa8734bf34da716f3747d0fdba-Abstract-Conference.html": {
    "title": "Recurrent Complex-Weighted Autoencoders for Unsupervised Object Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anand Gopalakrishnan",
      "Aleksandar Stanić",
      "Jürgen Schmidhuber",
      "Michael Mozer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fe7f375ef01e43f17d2c32b28a176577-Abstract-Conference.html": {
    "title": "A Unifying Normative Framework of Decision Confidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amelia Johnson",
      "Michael Buice",
      "Koosha Khalvati"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fe99ca7c293fe083c8fe7e85dce2f814-Abstract-Conference.html": {
    "title": "Hyperbolic Embeddings of Supervised Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richard Nock",
      "Ehsan Amid",
      "Frank Nielsen",
      "Alexander Soen",
      "Manfred Warmuth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fec946957ce1af51a61e8f2d851ac98f-Abstract-Conference.html": {
    "title": "High-probability complexity bounds for stochastic non-convex minimax optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yassine Laguel",
      "Yasa Syed",
      "Necdet Serhat Aybat",
      "Mert Gurbuzbalaban"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fed12564dc3dd40cfdc1c359a2e6884e-Abstract-Conference.html": {
    "title": "Instance-Specific Asymmetric Sensitivity in Differential Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Durfee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fed6d142d12b2f8c031615cc8fd50893-Abstract-Conference.html": {
    "title": "End-to-End Video Semantic Segmentation in Adverse Weather using Fusion Blocks and Temporal-Spatial Teacher-Student Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Yang",
      "YAN WENDING",
      "Michael Bi Mi",
      "Yuan Yuan",
      "Robby Tan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ff1f4141fa2a2d5d5aca6762cfbe6b57-Abstract-Conference.html": {
    "title": "AV-Cloud: Spatial Audio Rendering Through Audio-Visual Cloud Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingfei Chen",
      "Eli Shlizerman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ff54269c8c51ae379822f5f655b0f9a7-Abstract-Conference.html": {
    "title": "A Best-of-both-worlds Algorithm for Bandits with Delayed Feedback with Robustness to Excessive Delays",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeed Masoudian",
      "Julian Zimmert",
      "Yevgeny Seldin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ff56c8dc4c61017eab9771481b7fda0d-Abstract-Conference.html": {
    "title": "Propensity Score Alignment of Unpaired Multimodal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johnny Xi",
      "Jana Osea",
      "Zuheng Xu",
      "Jason S Hartford"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ff7373914a96956f2a7cacbdf3b0b8d8-Abstract-Conference.html": {
    "title": "Neural Gaffer: Relighting Any Object via Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haian Jin",
      "Yuan Li",
      "Fujun Luan",
      "Yuanbo Xiangli",
      "Sai Bi",
      "Kai Zhang",
      "Zexiang Xu",
      "Jin Sun",
      "Noah Snavely"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ff7bf6014f7826da531aa50f4538ee19-Abstract-Conference.html": {
    "title": "Improving Temporal Link Prediction via Temporal Walk Matrix Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaodong Lu",
      "Leilei Sun",
      "Tongyu Zhu",
      "Weifeng Lv"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ff80e644be415af4bcd7e4b4efb2152f-Abstract-Conference.html": {
    "title": "CLUES: Collaborative Private-domain High-quality Data Selection for LLMs via Training Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanru Zhao",
      "Hongxiang Fan",
      "Shell Xu Hu",
      "Wangchunshu Zhou",
      "Nicholas Lane"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ff887781480973bd3cb6026feb378d1e-Abstract-Conference.html": {
    "title": "BAKU: An Efficient Transformer for Multi-Task Policy Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddhant Haldar",
      "Zhuoran Peng",
      "Lerrel Pinto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ff9783ec29688387d44779d67d06ef66-Abstract-Conference.html": {
    "title": "ImOV3D: Learning Open Vocabulary Point Clouds 3D Object Detection from Only 2D Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timing Yang",
      "Yuanliang Ju",
      "Li Yi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ff997469ac66cf893c4183efeb22212a-Abstract-Conference.html": {
    "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lujun Li",
      "Peijie Dong",
      "Zhenheng Tang",
      "Xiang Liu",
      "Qiang Wang",
      "Wenhan Luo",
      "Wei Xue",
      "Qifeng Liu",
      "Xiaowen Chu",
      "Yike Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ff9c70659c39cdd801dd5f5a1201c29e-Abstract-Conference.html": {
    "title": "A Compositional Atlas for Algebraic Circuits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjie Wang",
      "Denis Mauá",
      "Guy Van den Broeck",
      "YooJung Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ffa5ed75f75f1975162d8c7e4dad7301-Abstract-Conference.html": {
    "title": "Efficient and Private Marginal Reconstruction with Local Non-Negativity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brett Mullins",
      "Miguel Fuentes",
      "Yingtai Xiao",
      "Daniel Kifer",
      "Cameron Musco",
      "Daniel R. Sheldon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ffbfff81fb78e4bb558273b91e12e318-Abstract-Conference.html": {
    "title": "DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anthony Liang",
      "Guy Tennenholtz",
      "Chih-wei Hsu",
      "Yinlam Chow",
      "Erdem Bıyık",
      "Craig Boutilier"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ffd86e56e6403d63dd6face033060e5a-Abstract-Conference.html": {
    "title": "First-Order Methods for Linearly Constrained Bilevel Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guy Kornowski",
      "Swati Padmanabhan",
      "Kai Wang",
      "Zhe Zhang",
      "Suvrit Sra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ffe78e2b3c80439e6dfd3f7f38cfa888-Abstract-Conference.html": {
    "title": "Neural Embeddings Rank: Aligning 3D latent dynamics with movements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenggang Chen",
      "Zhiyu Yang",
      "Xiaoqin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/ffee3090eac0aae698b2d77ac5642c2c-Abstract-Conference.html": {
    "title": "Optimal Hypothesis Selection in (Almost) Linear Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maryam Aliakbarpour",
      "Mark Bun",
      "Adam Smith"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2024/hash/fffe5a7804c40465ef2432386850c2c7-Abstract-Conference.html": {
    "title": "Iterative Methods via Locally Evolving Set Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baojian Zhou",
      "Yifan Sun",
      "Reza Babanezhad Harikandeh",
      "Xingzhi Guo",
      "Deqing Yang",
      "Yanghua Xiao"
    ]
  }
}