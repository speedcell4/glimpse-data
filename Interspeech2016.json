{
  "https://www.isca-speech.org/archive/interspeech_2016/makhoul16_interspeech.html": {
    "title": "A 50-Year Retrospective on Speech and Language Processing",
    "volume": "main",
    "abstract": "This talk is a retrospective of speech and language processing as witnessed by the speaker during the last 50 years. From exploratory scientific beginnings that emphasized the discovery of how speech is produced and perceived by humans to today's plethora of applications using our technology, our field has witnessed explosive growth. The talk will review the historical development of our community and some of the key technical ideas that have shaped our field. Some of the ideas were influenced by developments in other fields, while some of the developments in our field have been instrumental in key advances in other fields, such as optical character recognition and machine translation. Important developments include the source-filter model, digital signal processing, linear prediction, vector quantization, deep neural networks, and statistical modeling methods, especially hidden Markov models (HMMs), with primary applications to speech analysis, synthesis, coding, and recognition. The talk will be sprinkled with lessons learned in the importance of various factors in performing our research, and will be peppered with interesting tidbits about key moments in the development of our technology. The talk will end with a brief prospective peek at the next 50 years",
    "checked": true,
    "id": "5af3c7edf0a4353bb9890b0d830c295ef808d907",
    "semantic_title": "a 50-year retrospective on speech and language processing",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/medennikov16_interspeech.html": {
    "title": "Improving English Conversational Telephone Speech Recognition",
    "volume": "main",
    "abstract": "The goal of this work is to build a state-of-the-art English conversational telephone speech recognition system. We investigated several techniques to improve acoustic modeling, namely speaker-dependent bottleneck features, deep Bidirectional Long Short-Term Memory (BLSTM) recurrent neural networks, data augmentation and score fusion of DNN and BLSTM models. Training set consisted of the 300 hour Switchboard English speech corpus. We also examined the hypothesis rescoring using language models based on recurrent neural networks. The resulting system achieves a word error rate of 7.8% on the Switchboard part of the HUB5 2000 evaluation set which is the competitive result",
    "checked": true,
    "id": "a824aa5a86293c5f5f80bef2b77e8d676da38110",
    "semantic_title": "improving english conversational telephone speech recognition",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2016/saon16_interspeech.html": {
    "title": "The IBM 2016 English Conversational Telephone Speech Recognition System",
    "volume": "main",
    "abstract": "We describe a collection of acoustic and language modeling techniques that lowered the word error rate of our English conversational telephone LVCSR system to a record 6.6% on the Switchboard subset of the Hub5 2000 evaluation testset. On the acoustic side, we use a score fusion of three strong models: recurrent nets with maxout activations, very deep convolutional nets with 3×3 kernels, and bidirectional long short-term memory nets which operate on FMLLR and i-vector features. On the language modeling side, we use an updated model \"M\" and hierarchical neural network LMs",
    "checked": true,
    "id": "05d2700846c0323f79c1344aca5333994c7c03a5",
    "semantic_title": "the ibm 2016 english conversational telephone speech recognition system",
    "citation_count": 104
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lu16_interspeech.html": {
    "title": "Small-Footprint Deep Neural Networks with Highway Connections for Speech Recognition",
    "volume": "main",
    "abstract": "For speech recognition, deep neural networks (DNNs) have significantly improved the recognition accuracy in most of benchmark datasets and application domains. However, compared to the conventional Gaussian mixture models, DNN-based acoustic models usually have much larger number of model parameters, making it challenging for their applications in resource constrained platforms, e.g., mobile devices. In this paper, we study the application of the recently proposed highway network to train small-footprint DNNs, which are thinner and deeper, and have significantly smaller number of model parameters compared to conventional DNNs. We investigated this approach on the AMI meeting speech transcription corpus which has around 80 hours of audio data. The highway neural networks constantly outperformed their plain DNN counterparts, and the number of model parameters can be reduced significantly without sacrificing the recognition accuracy",
    "checked": true,
    "id": "de0681f80ea57abc0e877b09e33fbb6ef313f392",
    "semantic_title": "small-footprint deep neural networks with highway connections for speech recognition",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yu16_interspeech.html": {
    "title": "Deep Convolutional Neural Networks with Layer-Wise Context Expansion and Attention",
    "volume": "main",
    "abstract": "In this paper, we propose a deep convolutional neural network (CNN) with layer-wise context expansion and location-based attention, for large vocabulary speech recognition. In our model each higher layer uses information from broader contexts, along both the time and frequency dimensions, than its immediate lower layer. We show that both the layer-wise context expansion and the location-based attention can be implemented using the element-wise matrix product and the convolution operation. For this reason, contrary to other CNNs, no pooling operation is used in our model. Experiments on the 309hr Switchboard task and the 375hr short message dictation task indicates that our model outperforms both the DNN and LSTM significantly",
    "checked": true,
    "id": "716e60cbbdacf01b3148e91a555358a96308b770",
    "semantic_title": "deep convolutional neural networks with layer-wise context expansion and attention",
    "citation_count": 73
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pundak16_interspeech.html": {
    "title": "Lower Frame Rate Neural Network Acoustic Models",
    "volume": "main",
    "abstract": "Recently neural network acoustic models trained with Connectionist Temporal Classification (CTC) were proposed as an alternative approach to conventional cross-entropy trained neural network acoustic models which output frame-level decisions every 10ms [1]. As opposed to conventional models, CTC learns an alignment jointly with the acoustic model, and outputs a blank symbol in addition to the regular acoustic state units. This allows the CTC model to run with a lower frame rate, outputting decisions every 30ms rather than 10ms as in conventional models, thus improving overall system speed. In this work, we explore how conventional models behave with lower frame rates. On a large vocabulary Voice Search task, we will show that with conventional models, we can slow the frame rate to 40ms while improving WER by 3% relative over a CTC-based model",
    "checked": true,
    "id": "f79925410329ab4e3045243f4a652dd03afd4cc8",
    "semantic_title": "lower frame rate neural network acoustic models",
    "citation_count": 138
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kurata16_interspeech.html": {
    "title": "Improved Neural Network Initialization by Grouping Context-Dependent Targets for Acoustic Modeling",
    "volume": "main",
    "abstract": "Neural Network (NN) Acoustic Models (AMs) are usually trained using context-dependent Hidden Markov Model (CD-HMM) states as independent targets. For example, the CD-HMM states of A-b-2 (second variant of beginning state of A) and A-m-1 (first variant of middle state of A) both correspond to the phone A, and A-b-1 and A-b-2 both correspond to the Context-independent HMM (CI-HMM) state A-b, but this relationship is not explicitly modeled. We propose a method that treats some neurons in the final hidden layer just below the output layer as dedicated neurons for phones or CI-HMM states by initializing connections between the dedicated neurons and the corresponding CD-HMM outputs with stronger weights than to other outputs. We obtained 6.5% and 3.6% relative error reductions with a DNN AM and a CNN AM, respectively, on a 50-hour English broadcast news task and 4.6% reduction with a CNN AM on a 500-hour Japanese task, in all cases after Hessian-free sequence training. Our proposed method only changes the NN parameter initialization and requires no additional computation in NN training or speech recognition run-time",
    "checked": true,
    "id": "d62fe0ab3a023ecc3f5e1c198c02812b10c568e7",
    "semantic_title": "improved neural network initialization by grouping context-dependent targets for acoustic modeling",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16_interspeech.html": {
    "title": "Automatic Scoring of Monologue Video Interviews Using Multimodal Cues",
    "volume": "main",
    "abstract": "Job interviews are an important tool for employee selection. When making hiring decisions, a variety of information from interviewees, such as previous work experience, skills, and their verbal and nonverbal communication, are jointly considered. In recent years, Social Signal Processing (SSP), an emerging research area on enabling computers to sense and understand human social signals, is being used develop systems for the coaching and evaluation of job interview performance. However this research area is still in its infancy and lacks essential resources (e.g., adequate corpora). In this paper, we report on our efforts to create an automatic interview rating system for monologue-style video interviews, which have been widely used in today's job hiring market. We created the first multimodal corpus for such video interviews. Additionally, we conducted manual rating on the interviewee's personality and performance during 12 structured interview questions measuring different types of job-related skills. Finally, focusing on predicting overall interview performance, we explored a set of verbal and nonverbal features and several machine learning models. We found that using both verbal and nonverbal features provides more accurate predictions. Our initial results suggest that it is feasible to continue working in this newly formed area",
    "checked": true,
    "id": "039499f66eda6c33d47d4036601656ee98b88570",
    "semantic_title": "automatic scoring of monologue video interviews using multimodal cues",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chong16_interspeech.html": {
    "title": "The Sound of Disgust: How Facial Expression May Influence Speech Production",
    "volume": "main",
    "abstract": "In speech articulation, mouth/lip shapes determine properties of the front part of the vocal tract, and so alter vowel formant frequencies. Mouth and lip shapes also determine facial emotional expressions, e.g., disgust is typically expressed with a distinctive lip and mouth configuration (i.e., closed mouth, pulled back lip corners). This overlap of speech and emotion gestures suggests that expressive speech will have different vowel formant frequencies from neutral speech. This study tested this hypothesis by comparing vowels produced in neutral versus disgust expressions. We used our database of five female native Cantonese talkers each uttering 50 CHINT sentences in both a neutral tone of voice and in disgust to examine five vowels ([ɐ], [εː], [iː], [ɔː], [ᴜː]). Mean fundamental frequency (F0) and the first two formants (F1 and F2) were calculated and analysed using mixed effects logistic regression. The results showed that the disgust vowels showed a significant reduction in either or both formant values (depending on vowel type) compared to neutral. We discuss the results in terms of how vowel synthesis could be used to alter the recognition of the sound of disgust",
    "checked": true,
    "id": "0336c778b6ae1669f23cc451020764af182dde8a",
    "semantic_title": "the sound of disgust: how facial expression may influence speech production",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yang16_interspeech.html": {
    "title": "Analyzing Temporal Dynamics of Dyadic Synchrony in Affective Interactions",
    "volume": "main",
    "abstract": "Human communication is a dynamical and interactive process that naturally induces an active flow of interpersonal coordination, and synchrony, along various behavioral dimensions. Assessing and characterizing the temporal dynamics of synchrony during an interaction is essential for fully understanding the human communication mechanisms. In this work, we focus on uncovering the temporal variability patterns of synchrony in visual gesture and vocal behavior in affectively rich interactions. We propose a statistical scheme to robustly quantify the turn-wise interpersonal synchrony. The analysis of the synchrony dynamics measure relies heavily on functional data analysis techniques. Our analysis results reveal that: 1) the dynamical patterns of interpersonal synchrony differ depending on the global emotions of an interaction dyad; 2) there generally exists a tight dynamical emotion-synchrony coupling over the interaction. These observations corroborate that interpersonal behavioral synchrony is a critical manifestation of the underlying affective processes, shedding light toward improved affective interaction modeling and automatic emotion recognition",
    "checked": true,
    "id": "c412cc824dd67d8ce39ee9493b51449be63cbcaf",
    "semantic_title": "analyzing temporal dynamics of dyadic synchrony in affective interactions",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ganesh16_interspeech.html": {
    "title": "Audiovisual Speech Scene Analysis in the Context of Competing Sources",
    "volume": "main",
    "abstract": "Audiovisual fusion in speech perception is generally conceived as a process independent from scene analysis, which is supposed to occur separately in the auditory and visual domain. On the contrary, we have been proposing in the last years that scene analysis such as what takes place in the cocktail party effect was an audiovisual process. We review here a series of experiments illustrating how audiovisual speech scene analysis occurs in the context of competing sources. Indeed, we show that a short contextual audiovisual stimulus made of competing auditory and visual sources modifies the perception of a following McGurk target. We interpret this in terms of binding, unbinding and rebinding processes, and we show how these processes depend on audiovisual correlations in time, attentional processes and differences between junior and senior participants",
    "checked": true,
    "id": "f30cc62b661b764425762ca0f4add273619ec884",
    "semantic_title": "audiovisual speech scene analysis in the context of competing sources",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sadoughi16_interspeech.html": {
    "title": "Head Motion Generation with Synthetic Speech: A Data Driven Approach",
    "volume": "main",
    "abstract": "To have believable head movements for conversational agents (CAs), the natural coupling between speech and head movements needs to be preserved, even when the CA uses synthetic speech. To incorporate the relation between speech head movements, studies have learned these couplings from real recordings, where speech is used to derive head movements. However, relying on recorded speech for every sentence that a virtual agent utters constrains the versatility and scalability of the interface, so most practical solutions for CAs use text to speech. While we can generate head motion using rule-based models, the head movements may become repetitive, spanning only a limited range of behaviors. This paper proposes strategies to leverage speech-driven models for head motion generation for cases relying on synthetic speech. The straightforward approach is to drive the speech-based models using synthetic speech, which creates mismatch between the test and train conditions. Instead, we propose to create a parallel corpus of synthetic speech aligned with natural recordings for which we have motion capture recordings. We use this parallel corpus to either retrain or adapt the speech-based models with synthetic speech. Objective and subjective metrics show significant improvements of the proposed approaches over the case with mismatched condition",
    "checked": true,
    "id": "c63a2a9adf3d0837cabe1f4221306de1d4512c22",
    "semantic_title": "head motion generation with synthetic speech: a data driven approach",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kim16_interspeech.html": {
    "title": "The Consistency and Stability of Acoustic and Visual Cues for Different Prosodic Attitudes",
    "volume": "main",
    "abstract": "Recently it has been argued that speakers use conventionalized forms to express different prosodic attitudes [1]. We examined this by looking at across speaker consistency in the expression of auditory and visual (head and face motion) prosodic attitudes produced on multiple different occasions. Specifically, we examined acoustic and motion profiles of a female and a male speaker expressing six different prosodic attitudes for four within-session repetitions across four different sessions. We used the same acoustic features as [1] and visual prosody was assessed by examining patterns of speaker's mouth, eyebrow and head movements. There was considerable variation in how prosody was realized across speakers, with the productions of one speaker more discriminable than the other. Within-session variation for both the acoustic and movement data was smaller than across-session variation, suggesting that short-term memory plays a role in consistency. The expression of some attitudes was less variable than others and better discrimination was found with the acoustic compared to the visual data, although certain visual features (e.g., eyebrow brow motion) provided better discrimination than others",
    "checked": true,
    "id": "d1b10a10626d6edb0d8a48874c9d6ae053ffcb3b",
    "semantic_title": "the consistency and stability of acoustic and visual cues for different prosodic attitudes",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kim16b_interspeech.html": {
    "title": "Introduction to Poster Presentation of Part II",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5f14e9a4dce354bb25ba2322c03de89ede6ce94c",
    "semantic_title": "introduction to poster presentation of part ii",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/vogel16_interspeech.html": {
    "title": "The Unit of Speech Encoding: The Case of Romanian",
    "volume": "main",
    "abstract": "The number of units in an utterance determines how much time speakers require to physically plan and begin their production [1]–[2]. Previous research proposed that the crucial units are prosodic i.e., Phonological Words (PWs), not syntactic or morphological [3]. Experiments on Dutch using a prepared speech paradigm claimed to support this view [4]–[5]; however, compounds did not conform to predictions and required the introduction of a different way of counting units. Since two PWs in compounds patterned with one PW, with or without clitics, rather than a phrase containing two PWs, a recursive PW' was invoked. Similar results emerged using the same methodology with compounds in Italian [6], and it was thus proposed that the relevant unit for speech encoding is not the PW, but rather the Composite Group (CompG), a constituent of the Prosodic Hierarchy between the PW and Phonological Phrase that comprises both compounds and clitic constructions [7]. We further investigate the relevant unit for speech encoding using the same methodology in Romanian. Similar findings support the CompG as the speech planning unit since, again, compounds with two PWs pattern with single words and clitic constructions, not Phonological Phrases which also contain two PWs",
    "checked": true,
    "id": "18c19a47e82b8abc8901276b40948f4227bc4303",
    "semantic_title": "the unit of speech encoding: the case of romanian",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jugler16_interspeech.html": {
    "title": "The Perceptual Effect of L1 Prosody Transplantation on L2 Speech: The Case of French Accented German",
    "volume": "main",
    "abstract": "Research has shown that language learners are not only challenged by segmental differences between their native language (L1) and the second language (L2). They also have problems with the correct production of suprasegmental structures, like phone/syllable duration and the realization of pitch. These difficulties often lead to a perceptible foreign accent. This study investigates the influence of prosody transplantation on foreign accent ratings. Syllable duration and pitch contour were transferred from utterances of a male and female German native speaker to utterances of ten French native speakers speaking German. Acoustic measurements show that French learners spoke with a significantly lower speaking rate. As expected, results of a perception experiment judging the accentedness of 1) German native utterances, 2) unmanipulated and 3) manipulated utterances of French learners of German suggest that the transplantation of the prosodic features syllable duration and pitch leads to a decrease in accentedness rating. These findings confirm results found in similar studies investigating prosody transplantation with different L1 and L2 and provide a beneficial technique for (computer-assisted) pronunciation training",
    "checked": true,
    "id": "deeaba4d6a0fd99a6a661eaa9236d090da416f64",
    "semantic_title": "the perceptual effect of l1 prosody transplantation on l2 speech: the case of french accented german",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ling16_interspeech.html": {
    "title": "Organizing Syllables into Sandhi Domains — Evidence from F0 and Duration Patterns in Shanghai Chinese",
    "volume": "main",
    "abstract": "In this study we investigated grouping-related F0 patterns in Shanghai Chinese by examining the effect of syllable position in a sandhi domain while controlling for tone, number of syllables in a domain, and focus condition. Results showed that F0 alignment had the most consistent grouping-related patterns, and syllable duration was positively related to F0 movement. Focus and word length both increased F0 peak and F0 excursion, but they had opposite influence on F0 slope, which indicated that focus and word length had different mechanisms in affecting F0 implementation, as focus increased articulation strength while word length influenced speaker's pre-planning",
    "checked": false,
    "id": "61cc99c9ea92f4b0f21f3b3d7a4f4a69985aa0ea",
    "semantic_title": "organizing syllables into sandhi domains - evidence from f0 and duration patterns in shanghai chinese",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ryant16_interspeech.html": {
    "title": "Automatic Analysis of Phonetic Speech Style Dimensions",
    "volume": "main",
    "abstract": "We apply automated analysis methods to create a multidimensional characterization of the prosodic characteristics of a large variety of speech datasets, with the goal of developing a general framework for comparing prosodic styles. Our datasets span styles including conversation, fluent reading, extemporized narratives, political speech, and advertisements; we compare several different languages including English, Spanish, and Chinese; and the features we extract are based on the joint distributions of F0 and amplitude values and sequences, speech and silence segment durations, syllable durations, and modulation spectra. Rather than focus on the acoustic correlates of a small number of discrete and mutually exclusive categories, we aim to characterize the space in which diverse speech styles live",
    "checked": true,
    "id": "d25ec1e7f56e261e46a48e7449ace96fd47f6348",
    "semantic_title": "automatic analysis of phonetic speech style dimensions",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/athanasopoulou16_interspeech.html": {
    "title": "The Acoustic Manifestation of Prominence in Stressless Languages",
    "volume": "main",
    "abstract": "Languages frequently express focus by enhancing various acoustic attributes of an utterance, but it is widely accepted that the main enhancement appears on stressed syllables. In languages without lexical stress, the question arises as to how focus is acoustically manifested. We thus examine the acoustic properties associated with prominence in three stressless languages, Indonesian, Korean and Vietnamese, comparing real three-syllable words in non-focused and focused contexts. Despite other prosodic differences, our findings confirm that none of the languages exhibits stress in the absence of focus, and under focus, no syllable shows consistent enhancement that could be indirectly interpreted as a manifestation of focus. Instead, a combination of boundary phenomena consistent with the right edge of a major prosodic constituent (Intonational Phrase) appears in each language: increased duration on the final syllable and in Indonesian and Korean, a decrease in F0. Since these properties are also found in languages with stress, we suggest that boundary phenomena signaling a major prosodic constituent break are used universally to indicate focus, regardless of a language's word-prosody; stress languages may use the same boundary properties, but these are most likely to be combined with enhancement of the stressed syllable of a word",
    "checked": true,
    "id": "e81f59d9168d031c028599bb36971696b3486673",
    "semantic_title": "the acoustic manifestation of prominence in stressless languages",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lai16_interspeech.html": {
    "title": "The Rhythmic Constraint on Prosodic Boundaries in Mandarin Chinese Based on Corpora of Silent Reading and Speech Perception",
    "volume": "main",
    "abstract": "This study investigated the interaction between rhythmic and syntactic constraints on prosodic phrases in Mandarin Chinese. A set of 4000 sentences was annotated twice, once based on silent reading by 130 students assigned 500 sentences each, and a second time by speech perception based on a recording by one professional speaker. In both types of annotation, the general pattern of phrasing was consistent, with short \"rhythmic phrases\" behaving differently from longer \"intonational phrases\". The probability of a rhythmic-phrase boundary between two words increased with the total length of those two words, and was also influenced by the nature of the syntactic boundary between them. The resulting rhythmic phrases were mainly 2–5 syllables long, independent of the length of the sentence. In contrast, the length of intonational phrases was not stable, and was heavily affected by sentence length. Intonational-phrase boundaries were also found to be affected by higher-level syntactic features, such as the depth of syntactic tree and the number of IP nodes. However, these syntactic influences on intonational phrases were weakened in long sentences (>20 syllable) and also in short sentences (<10 syllable), where the length effect played the main role",
    "checked": true,
    "id": "d38f2d331a4dbe57804463aa1f70a88fedc75bd6",
    "semantic_title": "the rhythmic constraint on prosodic boundaries in mandarin chinese based on corpora of silent reading and speech perception",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tsai16_interspeech.html": {
    "title": "Toward Development and Evaluation of Pain Level-Rating Scale for Emergency Triage based on Vocal Characteristics and Facial Expressions",
    "volume": "main",
    "abstract": "In order to allocate the healthcare resource, triage classification system plays an important role in assessing the severity of illness of the boarding patient at emergency department. The self-report pain intensity numerical-rating scale (NRS) is one of the major modifiers of the current triage system based on the Taiwan Triage and Acuity Scale (TTAS). The validity and reliability of self-report scheme for pain level assessment is a major concern. In this study, we model the observed expressive behaviors, i.e., facial expressions and vocal characteristics, directly from audio-video recordings in order to measure pain level for patients during triage. This work demonstrates a feasible model, which achieves an accuracy of 72.3% and 51.6% in a binary and ternary pain intensity classification. Moreover, the study result reveals a significant association of current model and analgesic prescription/patient disposition after adjusted for patient-report NRS and triage vital signs",
    "checked": true,
    "id": "ccd58c136ae8bed88889a93ca518f343ee3892c6",
    "semantic_title": "toward development and evaluation of pain level-rating scale for emergency triage based on vocal characteristics and facial expressions",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lee16_interspeech.html": {
    "title": "Predicting Severity of Voice Disorder from DNN-HMM Acoustic Posteriors",
    "volume": "main",
    "abstract": "Acoustical analysis of speech is considered a favorable and promising approach to objective assessment of voice disorders. Previous research emphasized on the extraction and classification of voice quality features from sustained vowel sounds. In this paper, an investigation on voice assessment using continuous speech utterances of Cantonese is presented. A DNN-HMM based speech recognition system is trained with speech data of unimpaired voice. The recognition accuracy for pathological utterances is found to decrease significantly with the disorder severity increasing. Average acoustic posterior probabilities are computed for individual phones from the speech recognition output lattices and the DNN soft-max layer. The phone posteriors obtained for continuous speech from the mild, moderate and severe categories are highly distinctive and thus useful to the determination of voice disorder severity. A subset of Cantonese phonemes are identified to be suitable and reliable for voice assessment with continuous speech",
    "checked": true,
    "id": "502d3f8c02fa77609a040d5e421f6c6f8f7e92a0",
    "semantic_title": "predicting severity of voice disorder from dnn-hmm acoustic posteriors",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sluis16_interspeech.html": {
    "title": "Long-Term Stability of Tracheoesophageal Voices",
    "volume": "main",
    "abstract": "Long-term voice outcomes of 13 tracheoesophageal speakers are assessed using speech samples that were recorded with at least 7 years in between. Intelligibility and voice quality are perceptually evaluated by 10 experienced speech and language pathologists. In addition, automatic speech evaluations are performed with tools from Ghent University. No significant group effect was found for changes in voice quality and intelligibility. The recordings showed a wide interspeaker variability. It is concluded that intelligibility and voice quality of tracheoesophageal voice is mostly stable over a period of 7 to 18 years",
    "checked": true,
    "id": "b8c06ac9a5dedaf314734fed5c315761624c0a32",
    "semantic_title": "long-term stability of tracheoesophageal voices",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gosztolya16_interspeech.html": {
    "title": "Detecting Mild Cognitive Impairment from Spontaneous Speech by Correlation-Based Phonetic Feature Selection",
    "volume": "main",
    "abstract": "Mild Cognitive Impairment (MCI), sometimes regarded as a prodromal stage of Alzheimer's disease, is a mental disorder that is difficult to diagnose. Recent studies reported that MCI causes slight changes in the speech of the patient. Our previous studies showed that MCI can be efficiently classified by machine learning methods such as Support-Vector Machines and Random Forest, using features describing the amount of pause in the spontaneous speech of the subject. Furthermore, as hesitation is the most important indicator of MCI, we took special care when handling filled pauses, which usually correspond to hesitation. In contrast to our previous studies which employed manually constructed feature sets, we now employ (automatic) correlation-based feature selection methods to find the relevant feature subset for MCI classification. By analyzing the selected feature subsets we also show that features related to filled pauses are useful for MCI detection from speech samples",
    "checked": true,
    "id": "dd7a14b0d6d814e7351261e184eb4d273a5d21c7",
    "semantic_title": "detecting mild cognitive impairment from spontaneous speech by correlation-based phonetic feature selection",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gong16_interspeech.html": {
    "title": "Towards an Automated Screening Tool for Developmental Speech and Language Impairments",
    "volume": "main",
    "abstract": "Approximately 60% of children with speech and language impairments do not receive the intervention they need because their impairment was missed by parents and professionals who lack specialized training. Diagnoses of these disorders require a time-intensive battery of assessments, and these are often only administered after parents, doctors, or teachers show concern An automated test could enable more widespread screening for speech and language impairments. To build classification models to distinguish children with speech or language impairments from typically developing children, we use acoustic features describing speech and pause events in story retell tasks. We developed and evaluated our method using two datasets. The smaller dataset contains many children with severe speech or language impairments and few typically developing children. The larger dataset contains primarily typically developing children. In three out of five classification tasks, even after accounting for age, gender, and dataset differences, our models achieve good discrimination performance (AUC > 0.70)",
    "checked": true,
    "id": "be67bb3a243e6b3b89d650fce1131e760b30718d",
    "semantic_title": "towards an automated screening tool for developmental speech and language impairments",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cm16_interspeech.html": {
    "title": "Spectral Enhancement of Cleft Lip and Palate Speech",
    "volume": "main",
    "abstract": "The quality of cleft lip and palate (CLP) speech is affected due to hyper-nasality and mis-articulation. Surgery and speech therapy are required to correct the structural and functional defects of CLP, which will result in an enhanced speech signal. The quality of the enhanced speech is perceptually evaluated by speech-language pathologists and results are highly biased. In this work, a signal processing based two stage speech enhancement method is proposed to get the perceptual benchmark to compare the signal after the surgery / therapy. In the first stage, CLP speech is enhanced by suppressing the nasal formant and in the second stage, spectral peak-valley enhancement is carried out to reduce the hyper-nasality associated with the CLP speech. The evaluation results show that the perceptual quality of CLP speech signal is improved after enhancement in both stages. Further, the improvement in the quality of the enhanced signal is compared with the speech signal after palatal prosthesis / surgery. The perceptual evaluation results show that the enhanced speech signals are better than the speech after prosthesis / surgery",
    "checked": true,
    "id": "5dccd526a6e6ddfaad768e0b80869b531a797e3f",
    "semantic_title": "spectral enhancement of cleft lip and palate speech",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/guan16_interspeech.html": {
    "title": "Assessing Level-Dependent Segmental Contribution to the Intelligibility of Speech Processed by Single-Channel Noise-Suppression Algorithms",
    "volume": "main",
    "abstract": "Most existing single-channel noise-suppression algorithms cannot improve speech intelligibility for normal-hearing listeners; however, the underlying reason for this performance deficit is still unclear. Given that various speech segments contain different perceptual contributions, the present work assesses whether the intelligibility of noisy speech can be improved when selectively suppressing its noise at high-level (vowel-dominated) or middle-level (containing vowel-consonant transitions) segments by existing single-channel noise-suppression algorithms. The speech signal was corrupted by speech-spectrum shaped noise and two-talker babble masker, and its noisy high- or middle-level segments were replaced by their noise-suppressed versions processed by four types of existing single-channel noise-suppression algorithms. Experimental results showed that performing segmental noise-suppression at high- or middle-level led to decreased intelligibility relative to noisy speech. This suggests that the lack of intelligibility improvement by existing noise-suppression algorithms is also present at segmental level, which may account for the deficit traditionally observed at full-sentence level",
    "checked": true,
    "id": "0d4d6f043e185614f8a8ea6426a73da4087d6a24",
    "semantic_title": "assessing level-dependent segmental contribution to the intelligibility of speech processed by single-channel noise-suppression algorithms",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zorila16_interspeech.html": {
    "title": "Effectiveness of Near-End Speech Enhancement Under Equal-Loudness and Equal-Level Constraints",
    "volume": "main",
    "abstract": "Most recently proposed near-end speech enhancement methods have been evaluated with the overall power (RMS) of the speech held constant. While significant intelligibility gains have been reported in various noisy conditions, an equal-RMS constraint may lead to enhancement solutions that increase the loudness of the original speech. Comparable effects might be produced simply by increasing the power of the original speech, which also leads to an increase in loudness. Here we suggest modifying the equal-RMS constraint to one of equal loudness between the original and the modified signals, based on a loudness model for time-varying sounds. Four state-of-the-art speech-in-noise intelligibility enhancement systems were evaluated under the equal-loudness constraint, using intelligibility tests with normal-hearing listeners. Results were compared with those obtained under the equal-RMS constraint. The methods based on spectral shaping and dynamic range compression yielded significant intelligibility gains regardless of the constraint, while for the method without dynamic range compression the intelligibility gain was lower under the equal-loudness than under the equal-RMS constraint",
    "checked": true,
    "id": "f1a5d8e6ae60925b5a8590c59d9ce267f0e92ec5",
    "semantic_title": "effectiveness of near-end speech enhancement under equal-loudness and equal-level constraints",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sharma16_interspeech.html": {
    "title": "Speech Synthesis in Noisy Environment by Enhancing Strength of Excitation and Formant Prominence",
    "volume": "main",
    "abstract": "Text-to-speech (TTS) synthesis systems have grown popularity due to their diverse practical usability. While most of the technologies developed aims to meet requirements in laboratory environment, the practical appliance is not limited to a specific environment. This work aims towards improving intelligibility of synthesized speech to make it deployable in realism. Based on the comparison of Lombard speech and speech produced in quiet, strength of excitation is found to play a crucial role in making speech intelligible in noisy situation. A novel method for enhancement of strength of excitation is proposed which makes the synthesized speech more intelligible in practical scenario. Linear-prediction analysis based formant enhancement method is also employed to further improve the intelligibility. The proposed enhancement framework is applied in synthesized speech and evaluated in presence of different types and levels of noise. Subjective evaluation results show that, the proposed method makes the synthesized speech applicable in practical noisy environment",
    "checked": true,
    "id": "ac6a0febe1aade6bba6ce4a4ac3d3f9ab0f31360",
    "semantic_title": "speech synthesis in noisy environment by enhancing strength of excitation and formant prominence",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16_interspeech.html": {
    "title": "Relative Contributions of Amplitude and Phase to the Intelligibility Advantage of Ideal Binary Masked Sentences",
    "volume": "main",
    "abstract": "Many studies have shown the advantage of using ideal binary masking (IdBM) to improve the intelligibility of speech corrupted by interfering maskers. Given the fact that amplitude and phase are two important acoustic cues for speech perception, the present work further investigated the relative contributions of these two cues to the intelligibility advantage of IdBM-processed sentences. Three types of Mandarin IdBM-processed stimuli (i.e., amplitude-only, phase-only, and amplitude-and-phase) were generated, and played to normal-hearing listeners to recognize. Experiment results showed that amplitude- or phase-only cue could lead to significantly improved intelligibility of IdBM-processed sentences in relative to noise-masked sentences. A masker-dependent amplitude over phase advantage was observed when accounting for their relative contributions to the intelligibility advantage of IdBM-processed sentences. Under steady-state speech-spectrum shaped noise, both amplitude- and phase-only IdBM-processed sentences contained intelligibility information close to that contained in amplitude-and-phase IdBM-processed sentences. In contrast, under competing babble masker, amplitude-only IdBM-processed sentences were more intelligible than phase-only IdBM-processed sentences, and neither could account for the intelligibility advantage of amplitude-and-phase IdBM-processed sentences",
    "checked": true,
    "id": "4bcec492c4857b196645876188e707558da7fc9b",
    "semantic_title": "relative contributions of amplitude and phase to the intelligibility advantage of ideal binary masked sentences",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16_interspeech.html": {
    "title": "Predicting Binaural Speech Intelligibility from Signals Estimated by a Blind Source Separation Algorithm",
    "volume": "main",
    "abstract": "State-of-the-art binaural objective intelligibility measures (OIMs) require individual source signals for making intelligibility predictions, limiting their usability in real-time online operations. This limitation may be addressed by a blind source separation (BSS) process, which is able to extract the underlying sources from a mixture. In this study, a speech source is presented with either a stationary noise masker or a fluctuating noise masker whose azimuth varies in a horizontal plane, at two speech-to-noise ratios (SNRs). Three binaural OIMs are used to predict speech intelligibility from the signals separated by a BSS algorithm. The model predictions are compared with listeners' word identification rate in a perceptual listening experiment. The results suggest that with SNR compensation to the BSS-separated speech signal, the OIMs can maintain their predictive power for individual maskers compared to their performance measured from the direct signals. It also reveals that the errors in SNR between the estimated signals are not the only factors that decrease the predictive accuracy of the OIMs with the separated signals. Artefacts or distortions on the estimated signals caused by the BSS algorithm may also be concerns",
    "checked": true,
    "id": "62915790378f12ddeabdbaa1558ec24b7c5974b3",
    "semantic_title": "predicting binaural speech intelligibility from signals estimated by a blind source separation algorithm",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/petkov16_interspeech.html": {
    "title": "Automated Pause Insertion for Improved Intelligibility Under Reverberation",
    "volume": "main",
    "abstract": "Speech intelligibility in reverberant environments is reduced because of overlap-masking. Signal modification prior to presentation in such listening environments, e.g., with a public announcement system, can be employed to alleviate this problem. Time-scale modifications are particularly effective in reducing the effect of overlap-masking. A method for introducing linguistically-motivated pauses is proposed in this paper. Given the transcription of a sentence, pause strengths are predicted at word boundaries. Pause duration is obtained by combining the pause strength and the time it takes late reverberation to decay to a level where a target signal-to-late-reverberation ratio criterion is satisfied. Considering a moderate reverberation condition and both binary and continuous pause strengths, a formal listening test was performed. The results show that the proposed methodology offers a significant intelligibility improvement over unmodified speech while continuous pause strengths offer an advantage over binary pause strengths",
    "checked": true,
    "id": "fa69948cf187861b7266a84c65c4c9049808b3c5",
    "semantic_title": "automated pause insertion for improved intelligibility under reverberation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/rouas16_interspeech.html": {
    "title": "Automatic Classification of Phonation Modes in Singing Voice: Towards Singing Style Characterisation and Application to Ethnomusicological Recordings",
    "volume": "main",
    "abstract": "This paper describes our work on automatic classification of phonation modes on singing voice. In the first part of the paper, we will briefly review the main characteristics of the different phonation modes. Then, we will describe the isolated vowels databases we used, with emphasis on a new database we recorded specifically for the purpose of this work. The next section will be dedicated to the description of the proposed set of parameters (acoustic and glottal) and the classification framework. The results obtained with only acoustic parameters are close to 80% of correct recognition, which seems sufficient for experimenting with continuous singing. Therefore, we set up two other experiments in order to see if the system may be of any practical use for singing voice characterisation. The first experiment aims at assessing if automatic detection of phonation modes may help classify singing into different styles. This experiment is carried out using a database of one singer singing the same song in 8 styles. The second experiment is carried out on field recordings from ethnomusicologists and concerns the distinction between \"normal\" singing and \"laments\" from a variety of countries",
    "checked": true,
    "id": "44332098caa62d119fb9896f021d53cf5dabc79f",
    "semantic_title": "automatic classification of phonation modes in singing voice: towards singing style characterisation and application to ethnomusicological recordings",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bhavsar16_interspeech.html": {
    "title": "Novel Nonlinear Prediction Based Features for Spoofed Speech Detection",
    "volume": "main",
    "abstract": "Several speech synthesis and voice conversion techniques can easily generate or manipulate speech to deceive the speaker verification (SV) systems. Hence, there is a need to develop spoofing countermeasures to detect the human speech from spoofed speech. System-based features have been known to contribute significantly to this task. In this paper, we extend a recent study of Linear Prediction (LP) and Long-Term Prediction (LTP)-based features to LP and Nonlinear Prediction (NLP)-based features. To evaluate the effectiveness of the proposed countermeasure, we use the corpora provided at the ASVspoof 2015 challenge. A Gaussian Mixture Model (GMM)-based classifier is used and the % Equal Error Rate (EER) is used as a performance measure. On the development set, it is found that LP-LTP and LP-NLP features gave an average EER of 4.78% and 9.18%, respectively. Score-level fusion of LP-LTP (and LP-NLP) with Mel Frequency Cepstral Coefficients (MFCC) gave an EER of 0.8% (and 1.37%), respectively. After score-level fusion of LP-LTP, LP-NLP and MFCC features, the EER is significantly reduced to 0.57%. The LP-LTP and LP-NLP features have found to work well even for Blizzard Challenge 2012 speech database",
    "checked": true,
    "id": "2071191e7dbd0571d236c84138f7694a4aa1cfdb",
    "semantic_title": "novel nonlinear prediction based features for spoofed speech detection",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dumpala16_interspeech.html": {
    "title": "Robust Vowel Landmark Detection Using Epoch-Based Features",
    "volume": "main",
    "abstract": "Automatic detection of vowel landmarks is useful in many applications such as automatic speech recognition (ASR), audio search, syllabification of speech and expressive speech processing. In this paper, acoustic features extracted around epochs are proposed for detection of vowel landmarks in continuous speech. These features are based on zero frequency filtering (ZFF) and single frequency filtering (SFF) analyses of speech. Excitation source based features are extracted using ZFF method and vocal tract system based features are extracted using SFF method. Based on these features, a rule-based algorithm is developed for vowel landmark detection (VLD). Performance of the proposed VLD algorithm is studied on three different databases namely, TIMIT (read), NTIMIT (channel degraded) and Switchboard corpus (conversational speech). Results show that the proposed algorithm performs equally well compared to state-of-the-art techniques on TIMIT and better on NTIMIT and Switchboard corpora. Proposed algorithm also displays consistent performance on TIMIT and NTIMIT datasets for different levels of noise degradations",
    "checked": true,
    "id": "8a750b922321c73398ede0b59d9164681e0b986d",
    "semantic_title": "robust vowel landmark detection using epoch-based features",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/toger16_interspeech.html": {
    "title": "Sensitivity of Quantitative RT-MRI Metrics of Vocal Tract Dynamics to Image Reconstruction Settings",
    "volume": "main",
    "abstract": "Real-time Magnetic Resonance Imaging (RT-MRI) is a powerful method for quantitative analysis of speech. Current state-of-the-art methods use constrained reconstruction to achieve high frame rates and spatial resolution. The reconstruction involves two free parameters that can be retrospectively selected: 1) the temporal resolution and 2) the regularization parameter λ, which balances temporal regularization and fidelity to the collected MRI data. In this work, we study the sensitivity of derived quantitative measures of vocal tract function to these two parameters. Specifically, the cross-distance between the tongue tip and the alveolar ridge was investigated for different temporal resolutions (21, 42, 56 and 83 frames per second) and values of the regularization parameter. Data from one subject is included. The phrase ‘one two three four five' was repeated 8 times at a normal pace. The results show that 1) a high regularization factor leads to lower cross-distance values 2) using a low value for the regularization parameter gives poor reproducibility and 3) a temporal resolution of at least 42 frames per second is desirable to achieve good reproducibility for all utterances in this speech task. The process employed here can be generalized to quantitative imaging of the vocal tract and other body parts",
    "checked": true,
    "id": "5177cd78ff774f4542804cb79b233c2b2d4bfe1f",
    "semantic_title": "sensitivity of quantitative rt-mri metrics of vocal tract dynamics to image reconstruction settings",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cernak16_interspeech.html": {
    "title": "Sound Pattern Matching for Automatic Prosodic Event Detection",
    "volume": "main",
    "abstract": "Prosody in speech is manifested by variations of loudness, exaggeration of pitch, and specific phonetic variations of prosodic segments. For example, in the stressed and unstressed syllables, there are differences in place or manner of articulation, vowels in unstressed syllables may have a more central articulation, and vowel reduction may occur when a vowel changes from a stressed to an unstressed position In this paper, we characterize the sound patterns using phonological posteriors to capture the phonetic variations in a concise manner. The phonological posteriors quantify the posterior probabilities of the phonological classes given the input speech acoustics, and they are obtained using the deep neural network (DNN) computational method. Built on the assumption that there are unique sound patterns in different prosodic segments, we devise a sound pattern matching (SPM) method based on 1-nearest neighbour classifier. In this work, we focus on automatic detection of prosodic stress placed on words, called also emphasized words. We evaluate the SPM method on English and French data with emphasized words. The word emphasis detection works very well also on cross-lingual tests, that is using a French classifier on English data, and vice versa",
    "checked": true,
    "id": "c3d5fb06030195c940f537482bca728f0285cd83",
    "semantic_title": "sound pattern matching for automatic prosodic event detection",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shahin16_interspeech.html": {
    "title": "Automatic Classification of Lexical Stress in English and Arabic Languages Using Deep Learning",
    "volume": "main",
    "abstract": "Prosodic features are important for the intelligibility and proficiency of stress-timed languages such as English and Arabic. Producing the appropriate lexical stress is challenging for second language (L2) learners, in particular, those whose first language (L1) is a syllable-timed language such as Spanish, French, etc. In this paper we introduce a method for automatic classification of lexical stress to be integrated into computer-aided pronunciation learning (CAPL) tools for L2 learning. We trained two different deep learning architectures, the deep feedforward neural network (DNN) and the deep convolutional neural network (CNN) using a set of temporal and spectral features related to the intensity, duration, pitch and energies in different frequency bands. The system was applied on both English (kids and adult) and Arabic (adult) speech corpora collected from native speakers. Our method results in error rates of 9%, 7% and 18% when tested on the English children corpus, English adult corpus and Arabic adult corpus respectively",
    "checked": true,
    "id": "a146207998af62d579107570e6882ae761f1e575",
    "semantic_title": "automatic classification of lexical stress in english and arabic languages using deep learning",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16b_interspeech.html": {
    "title": "Development of Mandarin Onset-Rime Detection in Relation to Age and Pinyin Instruction",
    "volume": "main",
    "abstract": "Development of explicit phonological awareness (PA) is thought to be dependent on formal instruction in reading or spelling. However, the development of implicit PA emerges before literacy instruction and interacts with how the phonological representations are constructed within a certain language. The present study systematically investigated the development of implicit PA of Mandarin onset-rime detection in relation to age and Pinyin instruction, involving 70 four- to seven-year-old kindergarten and first-grade children. Results indicated that the overall rate of correct responses in the rime detection task was much higher than that in the onset detection one, with better discrimination ability of larger units. Moreover, the underlying factors facilitating the development of Mandarin onset and rime detection were different, although both correlated positively with Pinyin instruction. On one hand, with age, development of rime detection appeared to develop naturally through spoken language experience before schooling, and was further optimized to the best after Pinyin instruction. On the other hand, the accuracy of onset detection exhibited a drastic improvement, boosting from 66% among preschoolers to 93% among first graders, establishing the primacy of Pinyin instruction responsible for the development of implicit onset awareness in Mandarin",
    "checked": true,
    "id": "7dc4a0ede57955bbae46a7d26a639874675ba520",
    "semantic_title": "development of mandarin onset-rime detection in relation to age and pinyin instruction",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wen16_interspeech.html": {
    "title": "Joint Effect of Dialect and Mandarin on English Vowel Production: A Case Study in Changsha EFL Learners",
    "volume": "main",
    "abstract": "Phonetic acquisition of English as a Foreign Language (EFL) for learners in dialectal areas has been increasingly regarded as an important research area in second language acquisition. However, most existing research has been focused on finding out the transfer effect of dialect on English production from a second language acquisition point of view, but ignores the impact of Mandarin. The present research aims to investigate the joint effect of dialect and Mandarin on Changsha EFL learners' vowel production through acoustic analysis, from both spectral and temporal perspectives. We will further explain the results with the Speech Learning Model (SLM). Three corner vowels, i.e., /a/ /i/ /u/, are studied, and the results show that: English vowels /i/ and /a/ produced by Changsha learners are significantly different from those of American speakers; specifically, /i/ is more affected by Mandarin, and /a/ is more affected by Changsha dialect, which can be explained by SLM. While /u/ produced by Changsha learners is similar to that of American speakers. Besides, Changsha learners produce shorter vowels in duration, due to dialect and Mandarin's transfer effect, but can still make tense-lax contrasts in /i-ɪ/ and /u-ʊ/ pairs",
    "checked": true,
    "id": "4b90d8cc1fa42694aa570c05be1cebbde362031f",
    "semantic_title": "joint effect of dialect and mandarin on english vowel production: a case study in changsha efl learners",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/katayama16_interspeech.html": {
    "title": "Effects of L1 Phonotactic Constraints on L2 Word Segmentation Strategies",
    "volume": "main",
    "abstract": "In the present study, it was examined whether phonotactic constraints of the first language affect speech processing by Japanese learners of English and whether L2 proficiency influences it. Seventeen native English speakers (ES), 18 Japanese speakers with high proficiency of English (JH), and 20 Japanese speakers with relatively low English proficiency (JL) took part in a monitoring task. Two types of target words (CVC/CV, e.g., team/tea) were embedded in bisyllabic non-words (e.g., teamfesh) and given to the participants with other non-words in the lists. The three groups were instructed to respond as soon as they spot targets, and response times and error rates were analyzed. The results showed that all of the groups segmented the CVC target words significantly faster and more accurately than the CV targets. L1 phonotactic constraints did not hinder L2 speech processing, and a word segmentation strategy was not language-specific in the case of Japanese English learners",
    "checked": true,
    "id": "717c0760e89d22773a2f8d5934762c2bd25757ff",
    "semantic_title": "effects of l1 phonotactic constraints on l2 word segmentation strategies",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wottawa16_interspeech.html": {
    "title": "Putting German [ʃ] and [ç] in Two Different Boxes: Native German vs L2 German of French Learners",
    "volume": "main",
    "abstract": "French L2 Learners of German (FG) often replace the palatal fricative /ç/ absent in French with the post alveolar fricative /ʃ/. In our study we investigate which cues can be used to distinguish whether FG speakers produce [ʃ] or [ç] in words with the final syllables /ɪʃ/ or /ɪç/. In literature of German as an L2, to our knowledge, this contrast has not yet been studied. In this perspective, we first compared native German (GG) productions of [/ʃ/] and [ç] to the FG speaker productions. Comparisons concerned the F2 of the preceding vowel, the F2 transition between the preceding vowel and the fricative, the center of gravity and intensity of the fricatives in high and low frequencies. To decide which cues are effectively choices to separate [ʃ] and [ç], the Weka interface in R (RWeka) was used. Results show that for German native speech, the F2 of the preceding vowel and the F2 transition are valid cues to distinguish between [ʃ] and [ç]. For FG speakers these cues are not valid. To distinguish between [ʃ] and [ç] in FG speakers, the intensity of high and low frequencies as well as the center of gravity of the fricatives help to decide whether [ʃ] and [ç] was produced. In German native speech, cues furnished only by the fricative itself can as well be used to distinguish between [ʃ] and [ç]",
    "checked": true,
    "id": "38a9a0f41d07196078d8e254c831f56d7e2be8fb",
    "semantic_title": "putting german [ʃ] and [ç] in two different boxes: native german vs l2 german of french learners",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/luo16_interspeech.html": {
    "title": "Naturalness Judgement of L2 English Through Dubbing Practice",
    "volume": "main",
    "abstract": "This Study investigates how different prosodic features affect native speakers' perception of L2 English spoken by Chinese students through dubbing, or re-voicing practice on video clips. Learning oral foreign language through dubbing on movie or animation clips has become very popular in China. In this practice, learners try to reproduce utterances as closely as possible to the original speech by closely matching lip movements on the clips. The L2 utterances before and after substantial dubbing practices were recorded and categorized according to different prosodic error patterns. Objective acoustic features were extracted and analyzed with naturalness scores based on perceptual experiment. Experimental results show that stress and timing play key roles in native speakers' perception of naturalness. With the practice of dubbing, prosodic features, especially timing, can be considerably improved and thus the naturalness of the reproduced utterances increases",
    "checked": true,
    "id": "7ba65f269c772f8a7b7f0cd0e07e2bfe988a5e69",
    "semantic_title": "naturalness judgement of l2 english through dubbing practice",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shinohara16_interspeech.html": {
    "title": "Audiovisual Training Effects for Japanese Children Learning English /r/-/l/",
    "volume": "main",
    "abstract": "In this study, the effects of audiovisual training were examined for Japanese children learning the English /r/-/l/ contrast. After 10 audiovisual training sessions, participants' improvement in English /r/-/l/ identification in audiovisual, visual-only and audio-only conditions was assessed. The results demonstrated that Japanese children significantly improved in their English /r/-/l/ identification accuracy in all three conditions. Although there was no significant modality effect on identification accuracy at pre test, the participants improved their identification accuracy in the audiovisual condition significantly more than in the audio-only condition. The improvement in the audiovisual condition was not significantly different from that in the visual-only condition. These results suggest that Japanese children can improve their identification accuracy of the English /r/-/l/ contrasts using each of visual and auditory modalities, and they appear to improve their lip-reading skills as much as audiovisual identification. Nonetheless, due to the ceiling effect in their improvement, it is unclear whether Japanese children improved their integrated processing of visual and auditory information",
    "checked": true,
    "id": "c3fdf554872fba26b7336b946e41d3ba815da7a8",
    "semantic_title": "audiovisual training effects for japanese children learning english /r/-/l/",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/harper16_interspeech.html": {
    "title": "L2 Acquisition and Production of the English Rhotic Pharyngeal Gesture",
    "volume": "main",
    "abstract": "This study is an investigation of L2 speakers' production of the pharyngeal gesture in the English /ɹ/. Real-time MRI recordings from one L1 French/L2 English and one L1 Greek/L2 English speaker were analyzed and compared with recordings from a native English speaker to examine whether the gestural composition of the rhotic consonant(s) in a speaker's L1, particularly the presence and location of a pharyngeal gesture, influences their production of English /ɹ/. While the L1 French speaker produced the expected high pharyngeal constriction in their production of the French rhotic, he did not appear to consistently produce an English-like low pharyngeal constriction in his production of English /ɹ/. Similarly, the native Greek speaker did not consistently produce a pharyngeal constriction of any kind in either his L1 rhotic (as expected) or in English /ɹ/. These results suggest that the acquisition and production of the pharyngeal gesture in the English rhotic approximant is particularly difficult for learners whose L1 rhotics lack an identical constriction, potentially due to a general difficulty of acquiring pharyngeal gestures that are not in the L1, the similarity of the acoustic consequences of the different components of a rhotic, or L1 transfer into the L2",
    "checked": true,
    "id": "77a1c77de540aa9deb3d301c23595eb448304ff2",
    "semantic_title": "l2 acquisition and production of the english rhotic pharyngeal gesture",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hennequin16_interspeech.html": {
    "title": "Auditory-Visual Perception of VCVs Produced by People with Down Syndrome: Preliminary Results",
    "volume": "main",
    "abstract": "Down Syndrome (DS) is a genetic disease involving a number of anatomical, physiological and cognitive impairments. More particularly it affects speech production abilities. This results in reduced intelligibility which has however only been evaluated auditorily. Yet, many studies have demonstrated that adding vision to audition helps perception of speech produced by people without impairments especially when it is degraded as is the case in noise. The present study aims at examining whether the visual information improves intelligibility of people with DS. 24 participants without DS were presented with VCV sequences (vowel-consonant-vowel) produced by four adults (2 with DS and 2 without DS). These stimuli were presented in noise in three modalities: auditory, auditory-visual and visual. The results confirm a reduced auditory intelligibility of speakers with DS. They also show that, for the speakers involved in this study, visual intelligibility is equivalent to that of speakers without DS and compensates for the auditory intelligibility loss. An analysis of the perceptual errors shows that most of them involve confusions between consonants. These results put forward the crucial role of multimodality in the improvement of the intelligibility of people with DS",
    "checked": true,
    "id": "596a506bf8ffa769ca8fe18061e508c8deeef927",
    "semantic_title": "auditory-visual perception of vcvs produced by people with down syndrome: preliminary results",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ylmaz16_interspeech.html": {
    "title": "Combining Non-Pathological Data of Different Language Varieties to Improve DNN-HMM Performance on Pathological Speech",
    "volume": "main",
    "abstract": "Research on automatic speech recognition (ASR) of pathological speech is particularly hindered by scarce in-domain data resources. Collecting representative pathological speech data is difficult due to the large variability caused by the nature and severity of the disorders, and the rigorous ethical and medical permission requirements. This task becomes even more challenging for languages which have fewer resources, fewer speakers and fewer patients than English, such as the mid-sized language Dutch. In this paper, we investigate the impact of combining speech data from different varieties of the Dutch language for training deep neural network (DNN)-based acoustic models. Flemish is chosen as the target variety for testing the acoustic models, since a Flemish database of pathological speech, the COPAS database, is available. We use non-pathological speech data from the northern Dutch and Flemish varieties and perform speaker-independent recognition using the DNN-HMM system trained on the combined data. The results show that this system provides improved recognition of pathological Flemish speech compared to a baseline system trained only on Flemish data. These findings open up new opportunities for developing useful ASR-based pathological speech applications for languages that are smaller in size and less resourced than English",
    "checked": true,
    "id": "2f47779bd5083583841c6642031efe11f14037f5",
    "semantic_title": "combining non-pathological data of different language varieties to improve dnn-hmm performance on pathological speech",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/laaridh16_interspeech.html": {
    "title": "Evaluation of a Phone-Based Anomaly Detection Approach for Dysarthric Speech",
    "volume": "main",
    "abstract": "Perceptual evaluation is still the most common method in clinical practice for the diagnosing and the following of the condition progression of people with speech disorders. Many automatic approaches were proposed to provide objective tools to deal with speech disorders and help professionals in the severity evaluation of speech impairments. This paper investigates an automatic phone-based anomaly detection approach implying an automatic text-constrained phone alignment. Here, anomalies are related to speech segments, for which an unexpected acoustic pattern is observed, compared with a normal speech production. This objective tool is applied to French dysarthric speech recordings produced by patients suffering from four different pathologies. The behavior of the anomaly detection approach is studied according to the precision of the automatic phone alignment. Faced with the difficulties of having a gold standard reference, especially for the phone-based anomaly annotation, this behavior is observed on both annotated and non-annotated corpora. As expected, alignment errors (large shifts compared with a manual segmentation) lead to a large amount of anomalies automatically detected. However, about 50% of correctly detected anomalies are not related to alignment errors. This behavior shows that the automatic approach is able to catch irregular acoustic patterns of phones",
    "checked": true,
    "id": "b72a350509857093b900b5b29d2638d04b905f6c",
    "semantic_title": "evaluation of a phone-based anomaly detection approach for dysarthric speech",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bhat16_interspeech.html": {
    "title": "Recognition of Dysarthric Speech Using Voice Parameters for Speaker Adaptation and Multi-Taper Spectral Estimation",
    "volume": "main",
    "abstract": "Dysarthria is a motor speech disorder resulting from impairment in muscles responsible for speech production, often characterized by slurred or slow speech resulting in low intelligibility. With speech based applications such as voice biometrics and personal assistants gaining popularity, automatic recognition of dysarthric speech becomes imperative as a step towards including people with dysarthria into mainstream. In this paper we examine the applicability of voice parameters that are traditionally used for pathological voice classification such as jitter, shimmer, F0 and Noise Harmonic Ratio (NHR) contour in addition to Mel Frequency Cepstral Coefficients (MFCC) for dysarthric speech recognition. Additionally, we show that multi-taper spectral estimation for computing MFCC improves the unseen dysarthric speech recognition. A Deep neural network (DNN) - hidden Markov model (HMM) recognition system fared better than a Gaussian Mixture Model (GMM) - HMM based system for dysarthric speech recognition. We propose a method to optimally use incremental dysarthric data to improve dysarthric speech recognition for an ASR with DNN-HMM. All evaluations were done on Universal Access Speech Corpus",
    "checked": true,
    "id": "000e78f18b73e6e0bffe143d7921f83509197b82",
    "semantic_title": "recognition of dysarthric speech using voice parameters for speaker adaptation and multi-taper spectral estimation",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16c_interspeech.html": {
    "title": "Impaired Categorical Perception of Mandarin Tones and its Relationship to Language Ability in Autism Spectrum Disorders",
    "volume": "main",
    "abstract": "While enhanced pitch processing appears to be characteristic of many individuals with autism spectrum disorders (ASD), it remains unclear whether enhancement in pitch perception applies to those who speak a tone language. Using a classic paradigm of categorical perception (CP), the present study investigated the perception of Mandarin tones in six- to eight-year-old children with ASD, and compared it with age-matched typically developing children. In stark contrast to controls, the child participants with ASD exhibited a much wider boundary width (i.e., more gentle slope), and showed no improved discrimination for pairs straddling the boundary, indicating impaired CP of Mandarin tones. Moreover, identification skills of different tone categories were positively correlated with language ability among children with ASD. These findings revealed aberrant tone processing in Mandarin-speaking individuals with ASD, especially in those with significant language impairment. Our results are in support of the notion of impaired change detection for the linguistic elements of speech in children with ASD",
    "checked": true,
    "id": "0ea58ae5443a7acfadac60bdfb4b4bc038c23954",
    "semantic_title": "impaired categorical perception of mandarin tones and its relationship to language ability in autism spectrum disorders",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nagle16_interspeech.html": {
    "title": "Perceived Naturalness of Electrolaryngeal Speech Produced Using sEMG-Controlled vs. Manual Pitch Modulation",
    "volume": "main",
    "abstract": "Producing speech with natural prosodic patterns is an ongoing challenge for users of electrolaryngeal (EL) speech. This study describes speech produced using a method currently in development, wherein a prosodic pattern is derived from skin surface electromyographical (sEMG) signals recorded from under the chin (submental surface) Eight laryngectomees who currently use a TruTone EL as their primary or backup mode of speech provided samples of EL speech in two modes: conventional thumb-pressure pitch-modulated control (represented by the TruTone EL; Griffin Laboratories, CA, U.S.A.) and sEMG-based pitch-modulated control (EMG-EL). Ratings of perceived naturalness were obtained from ten listeners unfamiliar with EL speech Listener ratings indicated that five speakers produced equally natural speech using both devices, and three produced significantly more natural speech using the EMG-EL than the TruTone EL. Mean fundamental frequency (f0) was similar within speakers for both modes; however, mean f0 range and standard deviation were significantly larger for the EMG-EL than for the TruTone EL, despite both devices having similar potential f0 range. This study showed that the EMG-EL provides an intuitive means of controlling f0-based prosodic patterns that are more natural-sounding than push-button control for some EL users",
    "checked": true,
    "id": "b61c0638e7c8d4d4eed490f51b927e1d757e30b6",
    "semantic_title": "perceived naturalness of electrolaryngeal speech produced using semg-controlled vs. manual pitch modulation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/najnin16_interspeech.html": {
    "title": "Identifying Hearing Loss from Learned Speech Kernels",
    "volume": "main",
    "abstract": "Does a hearing-impaired individual's speech reflect his hearing loss? To investigate this question, we recorded at least four hours of speech data from each of 29 adult individuals, both male and female, belonging to four classes: 3 normal, and 26 severely-to-profoundly hearing impaired with high, medium or low speech intelligibility. Acoustic kernels were learned for each individual by capturing the distribution of his speech data points represented as 20 ms duration windows. These kernels were evaluated using a set of neurophysiological metrics, namely, distribution of characteristic frequencies, equal loudness contour, bandwidth and Q value of tuning curve. It turns out that, for our cohort, a feature vector can be constructed out of four properties of these metrics that would accurately classify hearing-impaired individuals with low intelligible speech from normal ones using a linear classifier. However, the overlap in the feature space between normal and hearing-impaired individuals increases as the speech becomes more intelligible. We conclude that a hearing-impaired individual's speech does reflect his hearing loss provided his loss of hearing has considerably affected the intelligibility of his speech",
    "checked": true,
    "id": "b9b9205435751f0784359222fb077a1754b2987f",
    "semantic_title": "identifying hearing loss from learned speech kernels",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/rong16_interspeech.html": {
    "title": "Differential Effects of Velopharyngeal Dysfunction on Speech Intelligibility During Early and Late Stages of Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "The detrimental effects of velopharyngeal dysfunction (VPD) on speech intelligibility in persons with progressive motor speech disorders are poorly understood. In this study, we longitudinally investigated the velopharyngeal and articulatory performance of 142 individuals with varying severities of amyotrophic lateral sclerosis (ALS). Our goal was to determine the mechanisms that underlie the effects of VPD on speech intelligibility during early and late stages of ALS progression. We found that during the early stages of the disease, the effect of VPD on intelligibility was partially mitigated by an increase in articulatory (e.g., lower lip and jaw) movement speed. This apparent articulatory compensation eventually became unavailable during the late stages of disease progression, which led to rapid declines of speech intelligibility. The transition across the early and late stages was characterized by the slowing of the composite movement of lower lip and jaw below 138 mm/s, which indicated the onset of precipitous speech decline and thus, may provide important timing information for helping clinicians to plan interventions",
    "checked": true,
    "id": "876b12677c96c2fc36c56c82d8bfaf4be0fff4d7",
    "semantic_title": "differential effects of velopharyngeal dysfunction on speech intelligibility during early and late stages of amyotrophic lateral sclerosis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/delvaux16_interspeech.html": {
    "title": "The Production of Intervocalic Glides in Non Dysarthric Parkinsonian Speech",
    "volume": "main",
    "abstract": "In the context of a research project aiming at investigating the relationships between speech disorders, quality of life and social participation in Parkinson's Disease (PD), we report here on an acoustic study of glides and steady vowels by non dysarthric parkinsonian and control speakers. Our specific aim is to explore the dynamics of supra-laryngeal articulators in PD. Results suggest that non dysarthric Parkinsonian speakers maintain an accurate production of glides in VC[glide]V pseudo-words at the expense of articulatory undershoot in the surrounding vowels, and some asymmetry between the V1-to-glide and glide-to-V2 articulatory movements. We discuss how these results both support and challenge the accuracy-tempo trade-off hypothesis (Ackermann and Ziegler, 1991)",
    "checked": true,
    "id": "2df58b4cca0072d7d8b1d587984501b174b1e7b4",
    "semantic_title": "the production of intervocalic glides in non dysarthric parkinsonian speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/feng16_interspeech.html": {
    "title": "Auditory Processing Impairments Under Background Noise in Children with Non-Syndromic Cleft Lip and/or Palate",
    "volume": "main",
    "abstract": "Cleft lip and/or palate (CL/P) disorders are commonly occurring congenital malformations and hearing impairment is a very common co-morbidity. Most previous research has only focused on middle ear disorders and related auditory consequences in this group. Studies of higher level auditory status and central auditory processing abilities of this group have been unsystematic. The present study was conducted in order to objectively investigate the central auditory abilities in children with non-syndromic cleft lip and/or palate (NSCLP). A structured behavioral central auditory test battery was conducted in a group of children with NSCLP and their age/sex matched normal peers. The following behavioral central auditory tasks were undertaken, including hearing in noise test (HINT), dichotic digits test (DDT), and gaps in noise test (GIN). Results showed that there were no significant group differences in DDT test, indicating that the binaural separation and integration abilities could be normal in children with NSCLP. However, the cleft group performed significantly poorer than their normal peers for each ear in HINT test under noise condition and GIN test, suggesting that the children with NSCLP could have impaired monaural low redundancy auditory processing ability, and at risk of temporal resolution disability",
    "checked": true,
    "id": "55b54ce888ded10b4e5b6e70869fffd81fbb58be",
    "semantic_title": "auditory processing impairments under background noise in children with non-syndromic cleft lip and/or palate",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhu16_interspeech.html": {
    "title": "Modulation Spectral Features for Predicting Vocal Emotion Recognition by Simulated Cochlear Implants",
    "volume": "main",
    "abstract": "It has been reported that vocal emotion recognition is challenging for cochlear implant (CI) listeners due to the limited spectral cues with CI devices. As the mechanism of CI, modulation information is provided as a primarily cue. Previous studies have revealed that the modulation components of speech are important for speech intelligibility. However, it is unclear whether modulation information can contribute to vocal emotion recognition. We investigated the relationship between human perception of vocal emotion and the modulation spectral features of emotional speech. For human perception, we carried out a vocal-emotion recognition experiment using noise-vocoder simulations with normal-hearing listeners to predict the response from CI listeners. For modulation spectral features, we used auditory-inspired processing (auditory filterbank, temporal envelope extraction, modulation filterbank) to obtain the modulation spectrogram of emotional speech signals. Ten types of modulation spectral feature were then extracted from the modulation spectrogram. As a result, modulation spectral centroid, modulation spectral kurtosis, and modulation spectral tilt exhibited similar trends with the results of human perception. This suggests that these modulation spectral features may be important cues for voice emotion recognition with noise-vocoded speech",
    "checked": true,
    "id": "bca1cf47d0729d824a7d9454189021f6077f7dce",
    "semantic_title": "modulation spectral features for predicting vocal emotion recognition by simulated cochlear implants",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ochi16_interspeech.html": {
    "title": "Automatic Discrimination of Soft Voice Onset Using Acoustic Features of Breathy Voicing",
    "volume": "main",
    "abstract": "Soft onset vocalization is used in certain speech therapies. However, it is not easy to practice it at home because the acoustical evaluation itself needs training. It would be helpful for speech patients to get objective feedback during training. In this paper, new parameters for identifying soft onset with high accuracy are described. One of the parameters measures an aspect of the soft voice onset, in which the vocal folds start to oscillate periodically before coming in contact with each other at the beginning of vocalization. Combined with an onset time exceeding a threshold, the proposed parameters gave about 99% accuracy in identifying soft onset vocalization",
    "checked": true,
    "id": "0c1f0e68c2677c830c7008c6b3c44a04716e7f7b",
    "semantic_title": "automatic discrimination of soft voice onset using acoustic features of breathy voicing",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shao16_interspeech.html": {
    "title": "Effect of Noise on Lexical Tone Perception in Cantonese-Speaking Amusics",
    "volume": "main",
    "abstract": "Congenital amusia is a neurogenetic disorder affecting musical pitch processing. It also affects lexical tone perception. It is well documented that noisy conditions impact speech perception in second language learners and cochlear implant users. However, it is yet unclear whether and how noise affects lexical tone perception in the amusics. This paper examined the effect of multi-talker babble noise [1] on lexical tone identification and discrimination in 14 Cantonese-speaking amusics and 14 controls at three levels of signal-to-noise ratio (SNR). Results reveal that the amusics were less accurate in the identification of tones compared to controls in all SNR conditions. They also showed degraded performance in the discrimination, but less severe than in the identification. These results confirmed that amusia influences lexical tone processing. But the amusics were not influenced more by noise than the controls in either identification or discrimination. This indicates that the deficits of amusia may not be due to the lack of native-like language processing mechanisms or are mechanical in nature, as in the case of second language learners and cochlear implant users. Instead, the amusics may be impaired in the linguistic processing of native tones, showing impaired tone perception already under the clear condition",
    "checked": true,
    "id": "42fc7c3464815fb7bc67af5851d576956ee6b3d8",
    "semantic_title": "effect of noise on lexical tone perception in cantonese-speaking amusics",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2016/takashima16_interspeech.html": {
    "title": "Audio-Visual Speech Recognition Using Bimodal-Trained Bottleneck Features for a Person with Severe Hearing Loss",
    "volume": "main",
    "abstract": "In this paper, we propose an audio-visual speech recognition system for a person with an articulation disorder resulting from severe hearing loss. In the case of a person with this type of articulation disorder, the speech style is quite different from those of people without hearing loss that a speaker-independent acoustic model for unimpaired persons is hardly useful for recognizing it. The audio-visual speech recognition system we present in this paper is for a person with severe hearing loss in noisy environments. Although feature integration is an important factor in multimodal speech recognition, it is difficult to integrate efficiently because those features are different intrinsically. We propose a novel visual feature extraction approach that connects the lip image to audio features efficiently, and the use of convolutive bottleneck networks (CBNs) increases robustness with respect to speech fluctuations caused by hearing loss. The effectiveness of this approach was confirmed through word-recognition experiments in noisy environments, where the CBN-based feature extraction method outperformed the conventional methods",
    "checked": true,
    "id": "563ed3af254a5ef4549a34155718df772ba7b8b6",
    "semantic_title": "audio-visual speech recognition using bimodal-trained bottleneck features for a person with severe hearing loss",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gu16_interspeech.html": {
    "title": "Perception of Tone in Whispered Mandarin Sentences: The Case for Singapore Mandarin",
    "volume": "main",
    "abstract": "Whispering is commonly used when one needs to speak softly (for instance, in a library). Whispered speech mainly differs from neutral speech in that voicing, and thus its acoustic correlate F0, is absent. It is well known that in tonal languages such as Mandarin, tone identity is primarily conveyed by the F0 contour. Previous works also suggest that secondary correlates are both consistent and sufficient to convey Mandarin tone in whisper. However, these results are focused on Standard Mandarin spoken in Mainland China and have only been obtained via small-scale experiments using citation-form speech. To investigate whether these results will carry over to continuous sentences in other variations of Mandarin, we present a study that is the first of its nature to explore native Singapore Mandarin. Unlike related works, our large-scale perceptual experiment thoroughly investigates lexical tones in whispered and neutral Mandarin by involving more diverse speech data, greater number of listeners and use syllables excised from continuous speech to better simulate natural speech conditions. Our findings differ significantly from earlier works in terms of the recognition patterns observed. We present further in-depth analysis on how various phonetic characteristics (vowel contexts, place and manner of articulation) affect whispered tone perception",
    "checked": true,
    "id": "242edfa04172453d09f5f407d9c8d6ef1ab29d39",
    "semantic_title": "perception of tone in whispered mandarin sentences: the case for singapore mandarin",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xie16_interspeech.html": {
    "title": "A KL Divergence and DNN-Based Approach to Voice Conversion without Parallel Training Sentences",
    "volume": "main",
    "abstract": "We extend our recently proposed approach to cross-lingual TTS training to voice conversion, without using parallel training sentences. It employs Speaker Independent, Deep Neural Net (SI-DNN) ASR to equalize the difference between source and target speakers and Kullback-Leibler Divergence (KLD) to convert spectral parameters probabilistically in the phonetic space via ASR senone posterior probabilities of the two speakers. With or without knowing the transcriptions of the target speaker's training speech, the approach can be either supervised or unsupervised. In a supervised mode, where adequate training data of the target speaker with transcriptions is used to train a GMM-HMM TTS of the target speaker, each frame of the source speakers input data is mapped to the closest senone in thus trained TTS. The mapping is done via the posterior probabilities computed by SI-DNN ASR and the minimum KLD matching. In a unsupervised mode, all training data of the target speaker is first grouped into phonetic clusters where KLD is used as the sole distortion measure. Once the phonetic clusters are trained, each frame of the source speakers input is then mapped to the mean of the closest phonetic cluster. The final converted speech is generated with the max probability trajectory generation algorithm. Both objective and subjective evaluations show the proposed approach can achieve higher speaker similarity and better spectral distortions, when comparing with the baseline system based upon our sequential error minimization trained DNN algorithm",
    "checked": true,
    "id": "a57737009573d417871ccbb7229c0ecc15c7091b",
    "semantic_title": "a kl divergence and dnn-based approach to voice conversion without parallel training sentences",
    "citation_count": 72
  },
  "https://www.isca-speech.org/archive/interspeech_2016/aihara16_interspeech.html": {
    "title": "Parallel Dictionary Learning for Voice Conversion Using Discriminative Graph-Embedded Non-Negative Matrix Factorization",
    "volume": "main",
    "abstract": "This paper proposes a discriminative learning method for Non-negative Matrix Factorization (NMF)-based Voice Conversion (VC). NMF-based VC has been researched because of the natural-sounding voice it produces compared with conventional Gaussian Mixture Model (GMM)-based VC. In conventional NMF-based VC, parallel exemplars are used as the dictionary; therefore, dictionary learning is not adopted. In order to enhance the conversion quality of NMF-based VC, we propose Discriminative Graph-embedded Non-negative Matrix Factorization (DGNMF). Parallel dictionaries of the source and target speakers are discriminatively estimated by using DGNMF based on the phoneme labels of the training data. Experimental results show that our proposed method can not only improve the conversion quality but also reduce the computational times",
    "checked": true,
    "id": "b068ba19ea3bd474af8271f1c5841bf7613a229b",
    "semantic_title": "parallel dictionary learning for voice conversion using discriminative graph-embedded non-negative matrix factorization",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gu16b_interspeech.html": {
    "title": "Speech Bandwidth Extension Using Bottleneck Features and Deep Recurrent Neural Networks",
    "volume": "main",
    "abstract": "This paper presents a novel method for speech bandwidth extension (BWE) using deep structured neural networks. In order to utilize linguistic information during the prediction of high-frequency spectral components, the bottleneck (BN) features derived from a deep neural network (DNN)-based state classifier for narrowband speech are employed as auxiliary input. Furthermore, recurrent neural networks (RNNs) incorporating long short-term memory (LSTM) cells are adopted to model the complex mapping relationship between the feature sequences describing low-frequency and high-frequency spectra. Experimental results show that the BWE method proposed in this paper can achieve better performance than the conventional method based on Gaussian mixture models (GMMs) and the state-of-the-art approach based on DNNs in both objective and subjective tests",
    "checked": true,
    "id": "b49803adbc7c7c45fa3fc54091652156dc1ac3a4",
    "semantic_title": "speech bandwidth extension using bottleneck features and deep recurrent neural networks",
    "citation_count": 34
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yang16b_interspeech.html": {
    "title": "Voice Conversion Based on Matrix Variate Gaussian Mixture Model Using Multiple Frame Features",
    "volume": "main",
    "abstract": "This paper presents a novel voice conversion method based on matrix variate Gaussian mixture model (MV-GMM) using features of multiple frames. In voice conversion studies, approaches based on Gaussian mixture models (GMM) are still widely utilized because of their flexibility and easiness in handling. They treat the joint probability density function (PDF) of feature vectors from source and target speakers as that of joint vectors of the two vectors. Addition of dynamic features to the feature vectors in GMM-based approaches achieves certain performance improvements because the correlation between multiple frames is taken into account. Recently, a voice conversion framework based on MV-GMM, in which the joint PDF is modeled in a matrix variate space, has been proposed and it is able to precisely model both the characteristics of the feature spaces and the relation between the source and target speakers. In this paper, in order to additionally model the correlation between multiple frames in the framework more consistently, MV-GMM is constructed in a matrix variate space containing the features of neighboring frames. Experimental results show that an certain performance improvement in both objective and subjective evaluations is observed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hosaka16_interspeech.html": {
    "title": "Voice Conversion Based on Trajectory Model Training of Neural Networks Considering Global Variance",
    "volume": "main",
    "abstract": "This paper proposes a new training method of deep neural networks (DNNs) for statistical voice conversion. DNNs are now being used as conversion models that represent mapping from source features to target features in statistical voice conversion. However, there are two major problems to be solved in conventional DNN-based voice conversion: 1) the inconsistency between the training and synthesis criteria, and 2) the over-smoothing of the generated parameter trajectories. In this paper, we introduce a parameter trajectory generation process considering the global variance (GV) into the training of DNNs for voice conversion. A consistent framework using the same criterion for both training and synthesis provides better conversion accuracy in the original static feature domain, and the over-smoothing can be avoided by optimizing the DNN parameters on the basis of the trajectory likelihood considering the GV. Experimental results show that the proposed method outperforms the DNN-based method in term of both speech quality and speaker similarity",
    "checked": true,
    "id": "9dfddf37381e3dfab38f3bb0ef86916367e2d128",
    "semantic_title": "voice conversion based on trajectory model training of neural networks considering global variance",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/aryal16_interspeech.html": {
    "title": "Comparing Articulatory and Acoustic Strategies for Reducing Non-Native Accents",
    "volume": "main",
    "abstract": "This article presents an experimental comparison of two types of techniques, articulatory and acoustic, for transforming non-native speech to sound more native-like. Articulatory techniques use articulators from a native speaker to drive an articulatory synthesizer of the non-native speaker. These methods have a good theoretical justification, but articulatory measurements (e.g., via electromagnetic articulography) are difficult to obtain. In contrast, acoustic methods use techniques from the voice conversion literature to build a mapping between the two acoustic spaces, making them more attractive for practical applications (e.g., language learning). We compare two representative implementations of these approaches, both based on statistical parametric speech synthesis. Through a series of perceptual listening tests, we evaluate the two approaches in terms of accent reduction, speech intelligibility and speaker quality. Our results show that the acoustic method is more effective than the articulatory method in reducing perceptual ratings of non-native accents, and also produces synthesis of higher intelligibility while preserving voice quality",
    "checked": true,
    "id": "b470cf03a5cd098cf1880a60685dcc85744cf213",
    "semantic_title": "comparing articulatory and acoustic strategies for reducing non-native accents",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sarfjoo16_interspeech.html": {
    "title": "Cross-Lingual Speaker Adaptation for Statistical Speech Synthesis Using Limited Data",
    "volume": "main",
    "abstract": "Cross-lingual speaker adaptation with limited adaptation data has many applications such as use in speech-to-speech translation systems. Here, we focus on cross-lingual adaptation for statistical speech synthesis (SSS) systems using limited adaptation data. To that end, we propose two techniques exploiting a bilingual Turkish-English speech database that we collected. In one approach, speaker-specific state-mapping is proposed for cross-lingual adaptation which performed significantly better than the baseline state-mapping algorithm in adapting the excitation parameter both in objective and subjective tests. In the second approach, eigenvoice adaptation is done in the input language which is then used to estimate the eigenvoice weights in the output language using weighted linear regression. The second approach performed significantly better than the baseline system in adapting the spectral envelope parameters both in objective and subjective tests",
    "checked": true,
    "id": "6c80c6b9101fbb153de2956bca38fcb821d11dc4",
    "semantic_title": "cross-lingual speaker adaptation for statistical speech synthesis using limited data",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sun16_interspeech.html": {
    "title": "Personalized, Cross-Lingual TTS Using Phonetic Posteriorgrams",
    "volume": "main",
    "abstract": "We present a novel approach that enables a target speaker (e.g. monolingual Chinese speaker) to speak a new language (e.g. English) based on arbitrary textual input. Our system includes a trained English speaker-independent automatic speech recognition (SI-ASR) engine using TIMIT. Given the target speaker's speech in a non-target language, we generate Phonetic PosteriorGrams (PPGs) with the SI-ASR and then train a Deep Bidirectional Long Short-Term Memory based Recurrent Neural Networks (DBLSTM) to model the relationships between the PPGs and the acoustic signal. Synthesis involves input of arbitrary text to a general TTS engine (trained on any non-target speaker), the output of which is indexed by SI-ASR as PPGs. These are used by the DBLSTM to synthesize the target language in the target speaker's voice. A main advantage of this approach has very low training data requirement of the target speaker which can be in any language, as compared with a reference approach of training a special TTS engine using many recordings from the target speaker only in the target language. For a given target speaker, our proposed approach trained on 100 Mandarin (i.e. non-target language) utterances achieves comparable performance (in MOS and ABX test) of English synthetic speech as an HTS system trained on 1,000 English utterances",
    "checked": true,
    "id": "2ab50b5e722d61bcd2b379870758b70e5e2f21ec",
    "semantic_title": "personalized, cross-lingual tts using phonetic posteriorgrams",
    "citation_count": 53
  },
  "https://www.isca-speech.org/archive/interspeech_2016/prakash16_interspeech.html": {
    "title": "Acoustic Analysis of Syllables Across Indian Languages",
    "volume": "main",
    "abstract": "Indian languages are broadly classified as Indo-Aryan or Dravidian. The basic set of phones is more or less the same, varying mostly in the phonotactics across languages. There has also been borrowing of sounds and words across languages over time due to intermixing of cultures. Since syllables are fundamental units of speech production and Indian languages are characterised by syllable-timed rhythm, acoustic analysis of syllables has been carried out In this paper, instances of common and most frequent syllables in continuous speech have been studied across six Indian languages, from both Indo-Aryan and Dravidian language groups. The distributions of acoustic features have been compared across these languages. This kind of analysis is useful for developing speech technologies in a multilingual scenario. Owing to similarities in the languages, text-to-speech (TTS) synthesisers have been developed by segmenting speech data at the phone level using hidden Markov models (HMM) from other languages as initial models. Degradation mean opinion scores and word error rates indicate that the quality of synthesised speech is comparable to that of TTSes developed by segmenting the data using language-specific HMMs",
    "checked": true,
    "id": "4219e4f0dfa4b1e22820c49e9920fc2e9f418b62",
    "semantic_title": "acoustic analysis of syllables across indian languages",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16_interspeech.html": {
    "title": "Objective Evaluation Methods for Chinese Text-To-Speech Systems",
    "volume": "main",
    "abstract": "To objectively evaluate the performance of text-to-speech (TTS) systems, many studies have been conducted in the straightforward way to compare synthesized speech and natural speech with the alignment. However, in most situations, there is no natural speech can be used. In this paper, we focus on machine learning approaches for the TTS evaluation. We exploit a subspace decomposition method to separate different components in speech, which generates distinctive acoustic features automatically. Furthermore, a pairwise based Support Vector Machine (SVM) model is used to evaluate TTS systems. With the original prosodic acoustic features and Support Vector Regression model, we obtain a ranking relevance of 0.7709. Meanwhile, with the proposed oblique matrix projection method and pairwise SVM model, we achieve a much better result of 0.9115",
    "checked": true,
    "id": "338b340fa3bd3b246005bbe22dcdac7ca9b612a3",
    "semantic_title": "objective evaluation methods for chinese text-to-speech systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ijima16_interspeech.html": {
    "title": "Objective Evaluation Using Association Between Dimensions Within Spectral Features for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "This paper presents a novel objective evaluation technique for statistical parametric speech synthesis. One of its novel features is that it focuses on the association between dimensions within the spectral features. We first use a maximal information coefficient to analyze the relationship between subjective scores and associations of spectral features obtained from natural and various types of synthesized speech. The analysis results indicate that the scores improve as the association becomes weaker. We then describe the proposed objective evaluation technique, which uses a voice conversion method to detect the associations within spectral features. We perform subjective and objective experiments to investigate the relationship between subjective scores and objective scores. The proposed objective scores are compared to the mel-cepstral distortion. The results indicate that our objective scores achieve dramatically higher correlation to subjective scores than the mel-cepstral distortion",
    "checked": true,
    "id": "9ddf61cbc19635fdc7ca3e148cb172a5bf4c6605",
    "semantic_title": "objective evaluation using association between dimensions within spectral features for statistical parametric speech synthesis",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yoshimura16_interspeech.html": {
    "title": "A Hierarchical Predictor of Synthetic Speech Naturalness Using Neural Networks",
    "volume": "main",
    "abstract": "A problem when developing and tuning speech synthesis systems is that there is no well-established method of automatically rating the quality of the synthetic speech. This research attempts to obtain a new automated measure which is trained on the result of large-scale subjective evaluations employing many human listeners, i.e., the Blizzard Challenge. To exploit the data, we experiment with linear regression, feed-forward and convolutional neural network models, and combinations of them to regress from synthetic speech to the perceptual scores obtained from listeners. The biggest improvements were seen when combining stimulus- and system-level predictions",
    "checked": true,
    "id": "64338a3cd75118ba945b9c341b66c92867600731",
    "semantic_title": "a hierarchical predictor of synthetic speech naturalness using neural networks",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2016/podsiado16_interspeech.html": {
    "title": "Text-to-Speech for Individuals with Vision Loss: A User Study",
    "volume": "main",
    "abstract": "Individuals with vision loss use text-to-speech (TTS) for most of their interaction with devices, and rely on the quality of synthetic voices to a much larger extent than any other user group. A significant amount of local synthesis requests for Google TTS comes from TalkBack, the Android screenreader, making it our top client and making the visually-impaired users the heaviest consumers of the technology. Despite this, very little attention has been devoted to optimizing TTS voices for this user group and the feedback on TTS voices from the blind has been traditionally less-favourable. We present the findings from a TTS user experience study conducted by Google with visually-impaired screen reader users. The study comprised 14 focus groups and evaluated a total of 95 candidate voices with 90 participants across 3 countries. The study uncovered the distinctive usage patterns of this user group, which point to different TTS requirements and voice preferences from those of sighted users",
    "checked": true,
    "id": "4b005fccf4f0ece0a53bbeaf920a3d00cd5e1b42",
    "semantic_title": "text-to-speech for individuals with vision loss: a user study",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/valentinibotinhao16_interspeech.html": {
    "title": "Speech Enhancement for a Noise-Robust Text-to-Speech Synthesis System Using Deep Recurrent Neural Networks",
    "volume": "main",
    "abstract": "Quality of text-to-speech voices built from noisy recordings is diminished. In order to improve it we propose the use of a recurrent neural network to enhance acoustic parameters prior to training. We trained a deep recurrent neural network using a parallel database of noisy and clean acoustics parameters as input and output of the network. The database consisted of multiple speakers and diverse noise conditions. We investigated using text-derived features as an additional input of the network. We processed a noisy database of two other speakers using this network and used its output to train an HMM acoustic text-to-synthesis model for each voice. Listening experiment results showed that the voice built with enhanced parameters was ranked significantly higher than the ones trained with noisy speech and speech that has been enhanced using a conventional enhancement system. The text-derived features improved results only for the female voice, where it was ranked as highly as a voice trained with clean speech",
    "checked": true,
    "id": "710a3b77d317d18d3876d0f187de73a0d9deaba4",
    "semantic_title": "speech enhancement for a noise-robust text-to-speech synthesis system using deep recurrent neural networks",
    "citation_count": 81
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cooper16_interspeech.html": {
    "title": "Data Selection and Adaptation for Naturalness in HMM-Based Speech Synthesis",
    "volume": "main",
    "abstract": "We describe experiments in building HMM text-to-speech voices on professional broadcast news data from multiple speakers. We build on earlier work comparing techniques for selecting utterances from the corpus and voice adaptation to produce the most natural-sounding voices. While our ultimate goal is to develop intelligible and natural-sounding synthetic voices in low-resource languages rapidly and without the expense of collecting and annotating data specifically for text-to-speech, we focus on English initially, in order to develop and evaluate our methods. We evaluate our approaches using crowdsourced listening tests for naturalness. We have found that removing utterances that are outliers with respect to hyper-articulation, as well as combining the selection of hypo-articulated utterances and low mean f0 utterances, produce the most natural-sounding voices",
    "checked": true,
    "id": "094c75373f23f77a123d303de0b96c27f75b5cf2",
    "semantic_title": "data selection and adaptation for naturalness in hmm-based speech synthesis",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tao16_interspeech.html": {
    "title": "A Portable Automatic PA-TA-KA Syllable Detection System to Derive Biomarkers for Neurological Disorders",
    "volume": "main",
    "abstract": "Neurological disorders disrupt brain functions, affecting the life of many individuals. Conventional neurological disorder diagnosis methods require inconvenient and expensive devices. Several studies have identified speech biomarkers that are informative of neurological disorders, so speech-based interfaces can provide effective, convenient and affordable prescreening tools for diagnosis. We have investigated stand-alone automatic speech-based assessment tools for portable devices. Our current data collection protocol includes seven brief tests for which we have developed specialized automatic speech recognition (ASR) systems. The most challenging task from an ASR perspective is a popular diadochokinetic test consisting of fast repetitions of \"PA-TA-KA\", where subjects tend to alter, replace, insert or skip syllables. This paper presents our efforts to build a speech-based application specific for this task, where the computation is fast, efficient, and accurate on a portable device, not in the cloud. The tool recognizes the target syllables, providing phonetic alignment. This information is crucial to reliably estimate biomarkers such as the number of repetitions, insertions, mispronunciations, and temporal prosodic structure of the repetitions. We train and evaluate the application for two neurological disorders: traumatic brain injuries (TBIs) and Parkinson's disease. The results show low syllable error rates and high boundary detection, across populations",
    "checked": true,
    "id": "a54a3cc478e68f366c9eb66f137a843c36e3093a",
    "semantic_title": "a portable automatic pa-ta-ka syllable detection system to derive biomarkers for neurological disorders",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ghahabi16_interspeech.html": {
    "title": "Deep Neural Networks for i-Vector Language Identification of Short Utterances in Cars",
    "volume": "main",
    "abstract": "This paper is focused on the application of the Language Identification (LID) technology for intelligent vehicles. We cope with short sentences or words spoken in moving cars in four languages: English, Spanish, German, and Finnish. As the response time of the LID system is crucial for user acceptance in this particular task, speech signals of different durations with total average of 3.8s are analyzed. In this paper, the authors propose the use of Deep Neural Networks (DNN) to model effectively the i-vector space of languages. Both raw i-vectors and session variability compensated i-vectors are evaluated as input vectors to DNNs. The performance of the proposed DNN architecture is compared with both conventional GMM-UBM and i-vector/LDA systems considering the effect of durations of signals. It is shown that the signals with durations between 2 and 3s meet the requirements of this application, i.e., high accuracy and fast decision, in which the proposed DNN architecture outperforms GMM-UBM and i-vector/LDA systems by 37% and 28%, respectively",
    "checked": true,
    "id": "d93783c488df0b72806aa1bf7b377ffabae2116d",
    "semantic_title": "deep neural networks for i-vector language identification of short utterances in cars",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/woubie16_interspeech.html": {
    "title": "Improving i-Vector and PLDA Based Speaker Clustering with Long-Term Features",
    "volume": "main",
    "abstract": "i-vector modeling techniques have been successfully used for speaker clustering task recently. In this work, we propose the extraction of i-vectors from short- and long-term speech features, and the fusion of their PLDA scores within the frame of speaker diarization. Two sets of i-vectors are first extracted from short-term spectral and long-term voice-quality, prosodic and glottal to noise excitation ratio (GNE) features. Then, the PLDA scores of these two i-vectors are fused for speaker clustering task. Experiments have been carried out on single and multiple site scenario test sets of Augmented Multi-party Interaction (AMI) corpus. Experimental results show that i-vector based PLDA speaker clustering technique provides a significant diarization error rate (DER) improvement than GMM based BIC clustering technique",
    "checked": true,
    "id": "2cad573c7399a44a5e14774983a634ec50883c74",
    "semantic_title": "improving i-vector and plda based speaker clustering with long-term features",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lawson16_interspeech.html": {
    "title": "Open Language Interface for Voice Exploitation (OLIVE)",
    "volume": "main",
    "abstract": "We propose to demonstrate the Open Language Interface for Voice Exploitation (OLIVE) speech-processing system, which SRI International developed under the DARPA Robust Automatic Transcription of Speech (RATS) program. The technology underlying OLIVE was designed to achieve robustness to high levels of noise and distortion for speech activity detection (SAD), speaker identification (SID), language and dialect identification (LID), and keyword spotting (KWS). Our demonstration will show OLIVE performing those four tasks. We will also demonstrate SRI's speaker recognition capability live on a mobile phone for visitors to interact with",
    "checked": true,
    "id": "aeefeadb54943a9c0185de5d9f98e17b2e325844",
    "semantic_title": "open language interface for voice exploitation (olive)",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/smidl16_interspeech.html": {
    "title": "A Multimodal Dialogue System for Air Traffic Control Trainees Based on Discrete-Event Simulation",
    "volume": "main",
    "abstract": "In this paper we present a multimodal dialogue system designed as a learning tool for air traffic control officer trainees (ATCO). It was developed using our discrete-event simulation dialogue management framework with cloud-based speech recognition and text-to-speech systems. Our system mimics pilots in an air traffic communication, allowing the ATCOs to practice a control of a virtual airspace using spoken commands from air traffic control English phraseology",
    "checked": true,
    "id": "ad55d685da50ff6b71b4db11260cceb6a05bd3f1",
    "semantic_title": "a multimodal dialogue system for air traffic control trainees based on discrete-event simulation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gauthier16_interspeech.html": {
    "title": "Lig-Aikuma: A Mobile App to Collect Parallel Speech for Under-Resourced Language Studies",
    "volume": "main",
    "abstract": "This paper reports on our ongoing efforts to collect speech data in under-resourced or endangered languages of Africa. Data collection is carried out using an improved version of the Android application ( Aikuma) developed by Steven Bird and colleagues [1]. Features were added to the app in order to facilitate the collection of parallel speech data in line with the requirements of the French-German ANR/DFG BULB (Breaking the Unwritten Language Barrier) project. The resulting app, called Lig-Aikuma, runs on various mobile phones and tablets and proposes a range of different speech collection modes (recording, respeaking, translation and elicitation). It was used for field data collections in Congo-Brazzaville resulting in a total of over 80 hours of speech",
    "checked": true,
    "id": "2fa369378dbee983fd851510f8936c5b465680d3",
    "semantic_title": "lig-aikuma: a mobile app to collect parallel speech for under-resourced language studies",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gruber16_interspeech.html": {
    "title": "ARET — Automatic Reading of Educational Texts for Visually Impaired Students",
    "volume": "main",
    "abstract": "This paper deals with a presentation of an application which was developed to help in education of visually impaired pupils at a secondary school, i.e. at the pupils' age of 12 to 14 years. The web-based application integrates speech and language technologies to make the education easier in several areas, e.g. in mathematics, physics, chemistry or languages (Czech, English, German). TTS system is used for automatic reading of educational texts and it makes use of a special preprocessing of the texts, namely any formulas which may occur therein. The application is used by both teachers to create and manage the teaching material and pupils to view and listen to the prepared material. The application is currently being used by one special school for visually impaired pupils in daily lessons",
    "checked": false,
    "id": "8581568b2a2d2e80650ad2564292c21e2c3a0f42",
    "semantic_title": "aret - automatic reading of educational texts for visually impaired students",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lu16b_interspeech.html": {
    "title": "Segmental Recurrent Neural Networks for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "We study the segmental recurrent neural network for end-to-end acoustic modelling. This model connects the segmental conditional random field (CRF) with a recurrent neural network (RNN) used for feature extraction. Compared to most previous CRF-based acoustic models, it does not rely on an external system to provide features or segmentation boundaries. Instead, this model marginalises out all the possible segmentations, and features are extracted from the RNN trained together with the segmental CRF. Essentially, this model is self-contained and can be trained end-to-end. In this paper, we discuss practical training and decoding issues as well as the method to speed up the training in the context of speech recognition. We performed experiments on the TIMIT dataset. We achieved 17.3% phone error rate (PER) from the first-pass decoding — the best reported result using CRFs, despite the fact that we only used a zeroth-order CRF and without using any language model",
    "checked": true,
    "id": "aedffcebea081138a0f2bf2454f872700237fbf6",
    "semantic_title": "segmental recurrent neural networks for end-to-end speech recognition",
    "citation_count": 84
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nussbaumthom16_interspeech.html": {
    "title": "Acoustic Modeling Using Bidirectional Gated Recurrent Convolutional Units",
    "volume": "main",
    "abstract": "Convolutional and bidirectional recurrent neural networks have achieved considerable performance gains as acoustic models in automatic speech recognition in recent years. Latest architectures unify long short-term memory, gated recurrent unit and convolutional neural networks by stacking these different neural network types on each other, and providing short and long-term features to different depth levels of the network For the first time, we propose a unified layer for acoustic modeling which is simultaneously recurrent and convolutional, and which operates only on short-term features. Our unified model introduces a bidirectional gated recurrent unit that uses convolutional operations for the gating units. We analyze the performance behavior of the proposed layer, compare and combine it with bidirectional gated recurrent units, deep neural networks and frequency-domain convolutional neural networks on a 50 hour English broadcast news task. The analysis indicates that the proposed layer in combination with stacked bidirectional gated recurrent units outperforms other architectures",
    "checked": true,
    "id": "cd6cf1a39321c1ece4ee5f49e4ff24fbe1e9dc56",
    "semantic_title": "acoustic modeling using bidirectional gated recurrent convolutional units",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hsu16_interspeech.html": {
    "title": "Exploiting Depth and Highway Connections in Convolutional Recurrent Deep Neural Networks for Speech Recognition",
    "volume": "main",
    "abstract": "Deep neural network models have achieved considerable success in a wide range of fields. Several architectures have been proposed to alleviate the vanishing gradient problem, and hence enable training of very deep networks. In the speech recognition area, convolutional neural networks, recurrent neural networks, and fully connected deep neural networks have been shown to be complimentary in their modeling capabilities. Combining all three components, called CLDNN, yields the best performance to date. In this paper, we extend the CLDNN model by introducing a highway connection between LSTM layers, which enables direct information flow from cells of lower layers to cells of upper layers. With this design, we are able to better exploit the advantages of a deeper structure. Experiments on the GALE Chinese Broadcast Conversation/News Speech dataset indicate that our model outperforms all previous models and achieves a new benchmark, which is 22.41% character error rate on the dataset",
    "checked": true,
    "id": "45d2db8d57d9f716d73b75d29231195110ccbeb0",
    "semantic_title": "exploiting depth and highway connections in convolutional recurrent deep neural networks for speech recognition",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wu16_interspeech.html": {
    "title": "Stimulated Deep Neural Network for Speech Recognition",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) and deep learning approaches yield state-of-the-art performance in a range of tasks, including speech recognition. However, the parameters of the network are hard to analyze, making network regularization and robust adaptation challenging. Stimulated training has recently been proposed to address this problem by encouraging the node activation outputs in regions of the network to be related. This kind of information aids visualization of the network, but also has the potential to improve regularization and adaptation. This paper investigates stimulated training of DNNs for both of these options. These schemes take advantage of the smoothness constraints that stimulated training offers. The approaches are evaluated on two large vocabulary speech recognition tasks: a U.S. English broadcast news (BN) task and a Javanese conversational telephone speech task from the IARPA Babel program. Stimulated DNN training acquires consistent performance gains on both tasks over unstimulated baselines. On the BN task, the proposed smoothing approach is also applied to rapid adaptation, again outperforming the standard adaptation scheme",
    "checked": true,
    "id": "6226f19d41ff0a09f2e2cdb4c38d0fa98f0f2a0d",
    "semantic_title": "stimulated deep neural network for speech recognition",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2016/badino16_interspeech.html": {
    "title": "Phonetic Context Embeddings for DNN-HMM Phone Recognition",
    "volume": "main",
    "abstract": "This paper proposes an approach, named phonetic context embedding, to model phonetic context effects for deep neural network - hidden Markov model (DNN-HMM) phone recognition. Phonetic context embeddings can be regarded as continuous and distributed vector representations of context-dependent phonetic units (e.g., triphones). In this work they are computed using neural networks. First, all phone labels are mapped into vectors of binary distinctive features (DFs, e.g., nasal/not-nasal). Then for each speech frame the corresponding DF vector is concatenated with DF vectors of previous and next frames and fed into a neural network that is trained to estimate the acoustic coefficients (e.g., MFCCs) of that frame. The values of the first hidden layer represent the embedding of the input DF vectors. Finally, the resulting embeddings are used as secondary task targets in a multi-task learning (MTL) setting when training the DNN that computes phone state posteriors. The approach allows to easily encode a much larger context than alternative MTL-based approaches. Results on TIMIT with a fully connected DNN shows phone error rate (PER) reductions from 22.4% to 21.0% and from 21.3% to 19.8% on the test core and the validation set respectively and lower PER than an alternative strong MTL approach",
    "checked": true,
    "id": "54dfb6d31119e7cda3d6b26f9fe9e848d099692c",
    "semantic_title": "phonetic context embeddings for dnn-hmm phone recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16b_interspeech.html": {
    "title": "Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Convolutional Neural Networks (CNNs) are effective models for reducing spectral variations and modeling spectral correlations in acoustic features for automatic speech recognition (ASR). Hybrid speech recognition systems incorporating CNNs with Hidden Markov Models/Gaussian Mixture Models (HMMs/GMMs) have achieved the state-of-the-art in various benchmarks. Meanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural Networks (RNNs), which is proposed for labeling unsegmented sequences, makes it feasible to train an ‘end-to-end' speech recognition system instead of hybrid settings. However, RNNs are computationally expensive and sometimes difficult to train. In this paper, inspired by the advantages of both CNNs and the CTC approach, we propose an end-to-end speech framework for sequence labeling, by combining hierarchical CNNs with CTC directly without recurrent connections. By evaluating the approach on the TIMIT phoneme recognition task, we show that the proposed model is not only computationally efficient, but also competitive with the existing baseline systems. Moreover, we argue that CNNs have the capability to model temporal correlations with appropriate context information",
    "checked": true,
    "id": "e0b207e96351671453aa8bf05b7225c8a340a0b2",
    "semantic_title": "towards end-to-end speech recognition with deep convolutional neural networks",
    "citation_count": 341
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16b_interspeech.html": {
    "title": "Joint Speaker and Lexical Modeling for Short-Term Characterization of Speaker",
    "volume": "main",
    "abstract": "For speech utterances of very short duration, speaker characterization has shown strong dependency on the lexical content. In this context, speaker verification is always performed by analyzing and matching speaker pronunciation of individual words, syllables, or phones. In this paper, we advocate the use of hidden Markov model (HMM) for joint modeling of speaker characteristic and lexical content. We then develop a scoring model that scores only the speaker part rather than the joint speaker-lexical component leading to a better speaker verification performance. Experiments were conducted on the text-prompted task of RSR2015 and the RedDots datasets. In the RSR2015, the prompted texts are limited to random sequences of digits. The RedDots dataset dictates an unconstrained scenario where the prompted texts are free-text sentences. Both RSR2015 and RedDots datasets are publicly available",
    "checked": true,
    "id": "f3cf024d3164065ed13638e26029d5d0c310e026",
    "semantic_title": "joint speaker and lexical modeling for short-term characterization of speaker",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2016/alam16_interspeech.html": {
    "title": "Tandem Features for Text-Dependent Speaker Verification on the RedDots Corpus",
    "volume": "main",
    "abstract": "We use tandem features and a fusion of four systems for text-dependent speaker verification on the RedDots corpus. In the tandem system, a senone-discriminant neural network provides a low-dimensional bottleneck feature at each frame which are concatenated with a standard Mel-frequency cepstral coefficients (MFCC) feature representation. The concatenated features are propagated to a conventional GMM/UBM speaker recognition framework. In order to capture complementary information to the MFCC, we also use linear frequency cepstral coefficients and wavelet-based cepstral coefficients features for score level fusion. We report results on the part 1 and part 4 (text-dependent) tasks of RedDots corpus. Both the tandem feature-based system and fused system provided significant improvements over the baseline GMM/UBM system in terms of equal error rates (EER) and detection cost functions (DCFs) as defined in the 2008 and 2010 NIST speaker recognition evaluations. On the part 1 task (impostor correct condition) the fused system reduced the EER from 2.63% to 2.28% for male trials and from 7.01% to 3.48% for female trials. On the part4 task (impostor correct condition) the fused system helped to reduce the EER from 2.49% to 1.96% and from 5.9% to 3.22% for male and female trials respectively",
    "checked": true,
    "id": "8122f33803a4f4a2d907d2255370e919a1702c43",
    "semantic_title": "tandem features for text-dependent speaker verification on the reddots corpus",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sarkar16_interspeech.html": {
    "title": "Text Dependent Speaker Verification Using Un-Supervised HMM-UBM and Temporal GMM-UBM",
    "volume": "main",
    "abstract": "In this paper, we investigate the Hidden Markov Model (HMM) and the temporal Gaussian Mixture Model (GMM) systems based on the Universal Background Model (UBM) concept to capture temporal information of speech for Text Dependent (TD) Speaker Verification (SV). In TD-SV, target speakers are constrained to use only predefined fixed sentence/s during both the enrollment and the test process. The temporal information is therefore important in the sense of utterance verification, i.e. whether the test utterance has the same sequence of textual content as the utterance used during the target enrollment. However, the temporal information is not considered in the classical GMM-UBM based TD-SV system. Moreover, no transcription knowledge of the speech is required in the HMM-UBM and temporal GMM-UBM based systems. We also study the fusion of the HMM-UBM, the temporal GMM-UBM and the classical GMM-UBM systems in SV. We show that the HMM-UBM system yields better performance than the other systems in most cases. Further, fusion of the systems improve the overall speaker verification performance. The results are shown in the different tasks of the RedDots challenge 2016 database",
    "checked": true,
    "id": "12f8cbde4db33e2da7d185552e3a75f49ebb5c70",
    "semantic_title": "text dependent speaker verification using un-supervised hmm-ubm and temporal gmm-ubm",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kinnunen16_interspeech.html": {
    "title": "Utterance Verification for Text-Dependent Speaker Recognition: A Comparative Assessment Using the RedDots Corpus",
    "volume": "main",
    "abstract": "Text-dependent automatic speaker verification naturally calls for the simultaneous verification of speaker identity and spoken content. These two tasks can be achieved with automatic speaker verification (ASV) and utterance verification (UV) technologies. While both have been addressed previously in the literature, a treatment of simultaneous speaker and utterance verification with a modern, standard database is so far lacking. This is despite the burgeoning demand for voice biometrics in a plethora of practical security applications. With the goal of improving overall verification performance, this paper reports different strategies for simultaneous ASV and UV in the context of short-duration, text-dependent speaker verification. Experiments performed on the recently released RedDots corpus are reported for three different ASV systems and four different UV systems. Results show that the combination of utterance verification with automatic speaker verification is (almost) universally beneficial with significant performance improvements being observed",
    "checked": true,
    "id": "51d78ea852ca3017fd547d0aa715bc79b94082c1",
    "semantic_title": "utterance verification for text-dependent speaker recognition: a comparative assessment using the reddots corpus",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ma16_interspeech.html": {
    "title": "Parallel Speaker and Content Modelling for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "Text-dependent short duration speaker verification involves two challenges. The primary challenge of interest is the verification of the speaker's identity, and often a secondary challenge of interest is the verification of the lexical content of the pass-phrase. In this paper, we propose the use of two systems to handle these two tasks in parallel with one sub-system modelling speaker identity based on the assumption that lexical content is known and the other sub-system modelling lexical content in a speaker dependent manner. The text-dependent speaker verification sub-system is based on hidden Markov models and the lexical content verification system is based on models of speech segments that use a distinct Gaussian mixture model for each segment. Furthermore, a mixture selection method based on KL divergence was applied to refine the lexical content sub-system by making the models more discriminative. Experiments on part 1 of the RedDots database showed that the proposed combination of two sub-systems outperformed the baseline system by 39.8%, 51.1% and 37.3% in terms of the ‘imposter_correct', ‘target_wrong' and ‘imposter_wrong' metrics respectively",
    "checked": true,
    "id": "fcb5af7bbb24e3bea6564e57e3a2cceca6af5481",
    "semantic_title": "parallel speaker and content modelling for text-dependent speaker verification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zeinali16_interspeech.html": {
    "title": "i-Vector/HMM Based Text-Dependent Speaker Verification System for RedDots Challenge",
    "volume": "main",
    "abstract": "Recently, a new data collection was initiated within the RedDots project in order to evaluate text-dependent and text-prompted speaker recognition technology on data from a wider speaker population and with more realistic noise, channel and phonetic variability. This paper analyses our systems built for RedDots challenge — the effort to collect and compare the initial results on this new evaluation data set obtained at different sites. We use our recently introduced HMM based i-vector approach, where, instead of the traditional GMM, a set of phone specific HMMs is used to collect the sufficient statistics for i-vector extraction. Our systems are trained in a completely phrase-independent way on the data from RSR2015 and Libri speech databases. We compare systems making use of standard cepstral features and their combination with neural network based bottle-neck features. The best results are obtained with a score-level fusion of such systems",
    "checked": true,
    "id": "63a33cbe736e8fe19e5a5ab3f3a5aded514a27f3",
    "semantic_title": "i-vector/hmm based text-dependent speaker verification system for reddots challenge",
    "citation_count": 35
  },
  "https://www.isca-speech.org/archive/interspeech_2016/das16_interspeech.html": {
    "title": "Exploring Session Variability and Template Aging in Speaker Verification for Fixed Phrase Short Utterances",
    "volume": "main",
    "abstract": "This work highlights the impact of session variability and template aging on speaker verification (SV) using fixed phrase short utterances from the RedDots database. These have been collected over a period of one year and contain a large number of sessions per speaker. Session variation has been found to have a direct influence on SV performance and its significance is even greater for the case of fixed phrase short utterances as a very small amount of speech data is involved for speaker modeling as well as testing. Similarly for a practical deployable SV system when there is large session variation involved over a period of time, the template aging of the speakers may effect the SV performance. This work attempts to address some issues related to session variability and template aging of speakers which are found for data having large session variability, that if considered can be utilized for improving the performance of an SV system",
    "checked": true,
    "id": "fea617ec380d1f3de8cd4b907e04b8bed50ee993",
    "semantic_title": "exploring session variability and template aging in speaker verification for fixed phrase short utterances",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/uchida16_interspeech.html": {
    "title": "Prediction of the Articulatory Movements of Unseen Phonemes of a Speaker Using the Speech Structure of Another Speaker",
    "volume": "main",
    "abstract": "In this paper, we propose a method to predict the articulatory movements of phonemes that are difficult for a speaker to pronounce correctly because those phonemes are not seen in the native language of that speaker. When one wants to predict the articulatory movements of those unseen phonemes, since he/she has difficulty to generate those sounds, the conventional acoustic-to-articulatory mapping cannot be applied as it is. Here, we propose a solution by using the speech structure of another reference speaker who can pronounce the unseen phonemes. Speech structure is a kind of speech feature that represents only the linguistic information by suppressing the non-linguistic information, e.g. speaker identity, of an input utterance. In the proposed method, by using the speech structure of those unseen phonemes and other phonemes as constraint, the articulatory movements of the unseen phonemes are searched for in the articulatory space of the original speaker. Experiments using English short vowels show that the averaged prediction error was 1.02 mm",
    "checked": true,
    "id": "29d6a6d5ef49600074733e1c03eddc1b3241742a",
    "semantic_title": "prediction of the articulatory movements of unseen phonemes of a speaker using the speech structure of another speaker",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sivaraman16_interspeech.html": {
    "title": "Vocal Tract Length Normalization for Speaker Independent Acoustic-to-Articulatory Speech Inversion",
    "volume": "main",
    "abstract": "Speech inversion is a well-known ill-posed problem and addition of speaker differences typically makes it even harder. This paper investigates a vocal tract length normalization (VTLN) technique to transform the acoustic space of different speakers to a target speaker space such that speaker specific details are minimized. The speaker normalized features are then used to train a feed-forward neural network based acoustic-to-articulatory speech inversion system. The acoustic features are parameterized as time-contextualized mel-frequency cepstral coefficients and the articulatory features are represented by six tract-variable (TV) trajectories. Experiments are performed with ten speakers from the U. Wisc. X-ray microbeam database. Speaker dependent speech inversion systems are trained for each speaker as baselines to compare the performance of the speaker independent approach. For each target speaker, data from the remaining nine speakers are transformed using the proposed approach and the transformed features are used to train a speech inversion system. The performances of the individual systems are compared using the correlation between the estimated and the actual TVs on the target speaker's test set. Results show that the proposed speaker normalization approach provides a 7% absolute improvement in correlation as compared to the system where speaker normalization was not performed",
    "checked": true,
    "id": "ee7e37ac12293f8e574852e0052e9beef22bdd30",
    "semantic_title": "vocal tract length normalization for speaker independent acoustic-to-articulatory speech inversion",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lammert16_interspeech.html": {
    "title": "Investigation of Speed-Accuracy Tradeoffs in Speech Production Using Real-Time Magnetic Resonance Imaging",
    "volume": "main",
    "abstract": "Motor actions in speech production are both rapid and highly dexterous, even though speed and accuracy are often thought to conflict. Fitts' law has served as a rigorous formulation of the fundamental speed-accuracy tradeoff in other domains of human motor action, but has not been directly examined with respect to speech production. This paper examines Fitts' law in speech articulation kinematics by analyzing USC-TIMIT, a large database of real-time magnetic resonance imaging data of speech production. This paper also addresses methodological challenges in applying Fitts-style analysis, including the definition and operational measurement of key variables in real-time MRI data. Results suggest high variability in the task demands associated with targeted articulatory kinematics, as well as a clear tradeoff between speed and accuracy for certain types of speech production actions. Consonant targets, and particularly those following vowels, show the strongest evidence of this tradeoff, with correlations as high as 0.71 between movement time and difficulty. Other speech actions seem to challenge Fitts' law. Results are discussed with respect to limitations of Fitts' law in the context of speech production, as well as future improvements and applications",
    "checked": true,
    "id": "74950053ff1d9866daef96e4465c69f79d4481c2",
    "semantic_title": "investigation of speed-accuracy tradeoffs in speech production using real-time magnetic resonance imaging",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sorensen16_interspeech.html": {
    "title": "Characterizing Vocal Tract Dynamics Across Speakers Using Real-Time MRI",
    "volume": "main",
    "abstract": "Real-time magnetic resonance imaging (rtMRI) provides information about the dynamic shaping of the vocal tract during speech production and valuable data for creating and testing models of speech production. In this paper, we use rtMRI videos to develop a dynamical system in the framework of Task Dynamics which controls vocal tract constrictions and induces deformation of the air-tissue boundary. This is the first task dynamical system explicitly derived from speech kinematic data. Simulation identifies differences in articulatory strategy across speakers (n = 18), specifically in the relative contribution of articulators to vocal tract constrictions",
    "checked": true,
    "id": "4bf7727f00fec6d3c2f0fc5ed354eea2868afff2",
    "semantic_title": "characterizing vocal tract dynamics across speakers using real-time mri",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2016/labrunie16_interspeech.html": {
    "title": "Tracking Contours of Orofacial Articulators from Real-Time MRI of Speech",
    "volume": "main",
    "abstract": "We introduce a method for predicting midsagittal contours of orofacial articulators from real-time MRI data. A corpus of about 26 minutes of speech has been recorded of a French speaker at a rate of 55 images / s using highly undersampled radial gradient-echo MRI with image reconstruction by nonlinear inversion. The contours of each articulator have been manually traced for a set of about 60 images selected — by hierarchical clustering — to optimally represent the diversity of the speaker articulations. The data serve to build articulator-specific Principal Component Analysis (PCA) models of contours and associated image intensities, as well as multilinear regression (MLR) models that predict contour parameters from image parameters. The contours obtained by MLR are then refined, using the local information about pixel intensity profiles along the contours' normals, by means of modified Active Shape Models (ASM) trained on the same data. The method reaches RMS of predicted points to reference contour distances between 0.54 and 0.93 mm, depending on articulators. The processing of the corpus demonstrated the efficiency of the procedure, despite the possibility of further improvements. This work opens new perspectives for studying articulatory motion in speech",
    "checked": true,
    "id": "e7b98ef98003be34c8cae36b751d649c3619b31e",
    "semantic_title": "tracking contours of orofacial articulators from real-time mri of speech",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lingala16_interspeech.html": {
    "title": "State-of-the-Art MRI Protocol for Comprehensive Assessment of Vocal Tract Structure and Function",
    "volume": "main",
    "abstract": "Magnetic Resonance Imaging (MRI) provides a safe and flexible means to study the vocal tract, and is increasingly used in speech production research. This work details a state-of-the-art MRI protocol for comprehensive assessment of vocal tract structure and function, and presents results from representative speakers. The system incorporates (a) custom upper airway coils that are maximally sensitive to vocal tract tissues, (b) graphical user interface for 2D real-time MRI that provides on-the-fly reconstruction for interactive localization, and correction of imaging artifacts, (c) off-line constrained reconstruction for generating high spatio-temporal resolution dynamic images at (83 frames per sec, 2.4 mm ), (d) 3D static imaging of sounds sustained for 7 sec with full vocal tract coverage and isotropic resolution (resolution: 1.25 mm ), (e) T2-weighted high-resolution, high-contrast depiction of soft-tissue boundaries of the full vocal tract (axial, coronal, sagittal sweeps with resolution: 0.58 × 0.58 × 3 mm ), and (f) simultaneous audio recording with off-line noise cancellation and temporal alignment of audio with 2D real-time MRI. A stimuli set was designed to capture efficiently salient, static and dynamic, articulatory and morphological aspects of speech production in 90-minute data acquisition sessions",
    "checked": true,
    "id": "3186c634cd16e7c147da9f94a7f7f043af57788a",
    "semantic_title": "state-of-the-art mri protocol for comprehensive assessment of vocal tract structure and function",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xia16_interspeech.html": {
    "title": "DBN-ivector Framework for Acoustic Emotion Recognition",
    "volume": "main",
    "abstract": "Deep learning and i-vectors have been successfully used in speech and speaker recognition recently. In this work we propose a framework based on deep belief network (DBN) and i-vector space modeling for acoustic emotion recognition. We use two types of labels for frame level DBN training. The first one is the vector of posterior probabilities calculated from the GMM universal background model (UBM). The second one is the predicted label based on the GMMs. The DBN is trained to minimize errors for both types. After DBN training, we use the vector of posterior probabilities estimated by DBN to replace the UBM for i-vector extraction. Finally the extracted i-vectors are used in backend classifiers for emotion recognition. Our experiments on the USC IEMOCAP data show the effectiveness of our proposed DBN-ivector framework. In particular, with decision level combination, our proposed system yields significant improvement on both unweighted and weighted accuracy",
    "checked": true,
    "id": "87f3d1d5dafa1e33cf959a83967c1129c9989ac1",
    "semantic_title": "dbn-ivector framework for acoustic emotion recognition",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2016/stasak16_interspeech.html": {
    "title": "An Investigation of Emotional Speech in Depression Classification",
    "volume": "main",
    "abstract": "Assessing depression via speech characteristics is a growing area of interest in quantitative mental health research with a view to a clinical mental health assessment tool. As a mood disorder, depression induces changes in response to emotional stimuli, which motivates this investigation into the relationship between emotion and depression affected speech. This paper investigates how emotional information expressed in speech (i.e. arousal, valence, dominance) contributes to the classification of minimally depressed and moderately-severely depressed individuals. Experiments based on a subset of the AVEC 2014 database show that manual emotion ratings alone are discriminative of depression and combining rating-based emotion features with acoustic features improves classification between mild and severe depression. Emotion-based data selection is also shown to provide improvements in depression classification and a range of threshold methods are explored. Finally, the experiments presented demonstrate that automatically predicted emotion ratings can be incorporated into a fully automatic depression classification to produce a 5% accuracy improvement over an acoustic-only baseline system",
    "checked": true,
    "id": "c6da2b77303d7a7fc13e927210d631573924c8f2",
    "semantic_title": "an investigation of emotional speech in depression classification",
    "citation_count": 36
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lotfian16_interspeech.html": {
    "title": "Retrieving Categorical Emotions Using a Probabilistic Framework to Define Preference Learning Samples",
    "volume": "main",
    "abstract": "Preference learning is an appealing approach for affective recognition. Instead of predicting the underlying emotional class of a sample, this framework relies on pairwise comparisons to rank-order the testing data according to an emotional dimension. This framework is relevant not only for continuous attributes such as arousal or valence, but also for categorical classes (e.g., is this sample happier than the other?). A preference learning system for categorical classes can have applications in several domains including retrieving emotional behaviors conveying a target emotion, and defining the emotional intensity associated with a given class. One important challenge to build such a system is to define relative labels defining the preference between training samples. Instead of building these labels from scratch, we propose a probabilistic framework that creates relative labels from existing categorical annotations. The approach considers individual assessments instead of consensus labels, creating a metrics that is sensitive to the underlying ambiguity of emotional classes. The proposed metric quantifies the likelihood that a sample belong to a target emotion. We build happy, angry and sad rank-classifiers using this metric. We evaluate the approach over cross-corpus experiments, showing improved performance over binary classifiers and rank-based classifiers trained with consensus labels",
    "checked": true,
    "id": "97a6ae7100a0bb416ae1e705d72b117ecf4d637b",
    "semantic_title": "retrieving categorical emotions using a probabilistic framework to define preference learning samples",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schmitt16_interspeech.html": {
    "title": "At the Border of Acoustics and Linguistics: Bag-of-Audio-Words for the Recognition of Emotions in Speech",
    "volume": "main",
    "abstract": "Recognition of natural emotion in speech is a challenging task. Different methods have been proposed to tackle this complex task, such as acoustic feature brute-forcing or even end-to-end learning. Recently, bag-of-audio-words (BoAW) representations of acoustic low-level descriptors (LLDs) have been employed successfully in the domain of acoustic event classification and other audio recognition tasks. In this approach, feature vectors of acoustic LLDs are quantised according to a learnt codebook of audio words. Then, a histogram of the occurring ‘words' is built. Despite their massive potential, BoAW have not been thoroughly studied in emotion recognition. Here, we propose a method using BoAW created only of mel-frequency cepstral coefficients (MFCCs). Support vector regression is then used to predict emotion continuously in time and value, such as in the dimensions arousal and valence. We compare this approach with the computation of functionals based on the MFCCs and perform extensive evaluations on the RECOLA database, which features spontaneous and natural emotions. Results show that, BoAW representation of MFCCs does not only perform significantly better than functionals, but also outperforms by far most of recently published deep learning approaches, including convolutional and recurrent networks",
    "checked": true,
    "id": "7ebf51a3bff0834a33e3313bd51c0c7d7ac50fc2",
    "semantic_title": "at the border of acoustics and linguistics: bag-of-audio-words for the recognition of emotions in speech",
    "citation_count": 127
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chorianopoulou16_interspeech.html": {
    "title": "Speech Emotion Recognition Using Affective Saliency",
    "volume": "main",
    "abstract": "We investigate an affective saliency approach for speech emotion recognition of spoken dialogue utterances that estimates the amount of emotional information over time. The proposed saliency approach uses a regression model that combines features extracted from the acoustic signal and the posteriors of a segment-level classifier to obtain frame or segment-level ratings. The affective saliency model is trained using a minimum classification error (MCE) criterion that learns the weights by optimizing an objective loss function related to the classification error rate of the emotion recognition system. Affective saliency scores are then used to weight the contribution of frame-level posteriors and/or features to the speech emotion classification decision. The algorithm is evaluated for the task of anger detection on four call-center datasets for two languages, Greek and English, with good results",
    "checked": true,
    "id": "db779bc16a620a6fd747217272e1161b822c21ec",
    "semantic_title": "speech emotion recognition using affective saliency",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gupta16_interspeech.html": {
    "title": "Laughter Valence Prediction in Motivational Interviewing Based on Lexical and Acoustic Cues",
    "volume": "main",
    "abstract": "Motivational Interviewing (MI) is a goal oriented psychotherapy counseling that aims to instill positive change in a client through discussion. Since the discourse is in the form of semi-structured natural conversation, it often involves a variety of non-verbal social and affective behaviors such as laughter. Laughter carries information related to affect, mood and personality and can offer a window into the mental state of a person. In this work, we conduct an analytical study on predicting the valence of laughters (positive, neutral or negative) based on lexical and acoustic cues, within the context of MI. We hypothesize that the valence of laughter can be predicted using a window of past and future context around the laughter and, design models to incorporate context, from both text and audio. Through these experiments we validate the relation of the two modalities to perceived laughter valence. Based on the outputs of the prediction experiment, we perform a follow up analysis of the results including: (i) identification of the optimal past and future context in the audio and lexical channels, (ii) investigation of the differences in the prediction patterns for the counselor and the client and, (iii) analysis of feature patterns across the two modalities",
    "checked": true,
    "id": "7fb7eaf674d27fd390ae44e99967656d48cea477",
    "semantic_title": "laughter valence prediction in motivational interviewing based on lexical and acoustic cues",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wodarczak16_interspeech.html": {
    "title": "Respiratory Belts and Whistles: A Preliminary Study of Breathing Acoustics for Turn-Taking",
    "volume": "main",
    "abstract": "This paper presents first results on using acoustic intensity of inhalations as a cue to speech initiation in spontaneous multiparty conversations. We demonstrate that inhalation intensity significantly differentiates between cycles coinciding with no speech activity, shorter (< 1 s) and longer stretches of speech. While the model fit is relatively weak, it is comparable to the fit of a model using kinematic features collected with Respiratory Inductance Plethysmography. We also show that incorporating both kinematic and acoustic features further improves the model. Given the ease of capturing breath acoustics, we consider the results to be a promising first step towards studying communicative functions of respiratory sounds. We discuss possible extensions to the data collection procedure with a view to improving predictive power of the model",
    "checked": true,
    "id": "7f422042053b90cc2b176c508fc562e6fec45093",
    "semantic_title": "respiratory belts and whistles: a preliminary study of breathing acoustics for turn-taking",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kaland16_interspeech.html": {
    "title": "/r/ as Language Marker in Bilingual Speech Production and Perception",
    "volume": "main",
    "abstract": "Across languages of the world /r/ is known for its variability. Recent literature incorporates sociolinguistic factors, such as bilingualism, in order to explain /r/ variation. The current study investigates to what extent /r/ is a marker of a bilingual's dominant language. Specifically, the effects of several sociolinguistic and phonotactic factors on the production and perception of /r/ are investigated, such as the bilingual speaker's linguistic background, the language spoken as well as syllable position and place of articulation. To this end a reading task is carried out with bilingual speakers from South Tyrol (Italy). The major languages spoken in this region are Tyrolean (German dialect) and Italian. The recorded reading data is subsequently used in a perception experiment to investigate whether South Tyrolean listeners can identify the dominant language of the speaker on the basis of the presence of /r/ and the /r/ variant. Results show that listeners can identify the dominant language of the bilingual speakers on the basis of /r/. Specifically, the more Italian dominant the sociolinguistic background of the speaker, the more /r/ is produced frontally and the more that speaker is perceived as Italian dominant",
    "checked": true,
    "id": "be0986cf33a3639b81cd6923de0e0c0ef9932291",
    "semantic_title": "/r/ as language marker in bilingual speech production and perception",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/putzer16_interspeech.html": {
    "title": "Evaluation of Phonatory Behavior of German and French Speakers in Native and Non-Native Speech",
    "volume": "main",
    "abstract": "Phonatory behavior of German speakers (GS) and French speakers (FS) in native (L1) and non-native (L2) speech was instrumentally examined. Vowel productions of the two groups were analyzed using a parametrization of phonatory behavior and phonatory quality properties in the acoustic signal. The behavior of GS is characterized by more strained adduction of the vocal folds whereas FS show more incomplete glottal closure. Furthermore, GS change their phonatory behavior in the foreign language (=French) by adapting phonatory strategies of FS, whereas FS do not show this tendency. In addition, German beginners (BEG) and partly German advanced learners (ADV) are already orientated on production characteristics of the L2. French BEG however retain their phonatory behavior in L2 (=German) by showing less vocal fold adduction in comparison to their L1. French ADV show the opposite behavior. Finally, ADV of the two speaker groups generally show more strained behavior in L2 productions than BEG. The results provide evidence that GS and FS apply different laryngeal phonatory settings and that they altered their settings in L2 differently. Perceptual evaluation of voice quality of the speech material and a correlation analysis between acoustic and perceptual results are suggested for future research",
    "checked": true,
    "id": "827ea31001d7aaa1e3e1e40d312e7471aceb1e98",
    "semantic_title": "evaluation of phonatory behavior of german and french speakers in native and non-native speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/strombergsson16_interspeech.html": {
    "title": "Today's Most Frequently Used F0 Estimation Methods, and Their Accuracy in Estimating Male and Female Pitch in Clean Speech",
    "volume": "main",
    "abstract": "Variation in fundamental frequency (F ) constitutes a valuable source of information for researches across many disciplines, with a shared interest in speech. Different methods for estimating F vary in estimation accuracy and accessibility, and there is yet no gold standard. Through a bibliometric survey, this study examines what methods were the most frequently used in the speech scientific community during the years 2010–2016. Secondly, the most used methods are evaluated against a ground truth reference, with a specific focus on their accuracy in estimating F in male and female speakers, respectively The results show that Praat is the dominant method by far, followed by STRAIGHT, RAPT and YIN. This pattern holds across a range of different research areas, although within Acoustics and Engineering, Praat's dominance is less pronounced. In the evaluation including Praat, RAPT and YIN — with their default and gender-adapted settings — Praat also proved to be the most accurate. The finding that adapting Praat's pitch range settings by gender leads to further improvements should encourage researchers to do this routinely",
    "checked": true,
    "id": "ff040316f44eab5c0497cec280bfb1fd0e7c0e85",
    "semantic_title": "today's most frequently used f0 estimation methods, and their accuracy in estimating male and female pitch in clean speech",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2016/he16_interspeech.html": {
    "title": "A Praat-Based Algorithm to Extract the Amplitude Envelope and Temporal Fine Structure Using the Hilbert Transform",
    "volume": "main",
    "abstract": "A speech signal can be viewed as a high frequency carrier signal containing the temporal fine structure (TFS) that is modulated by a low frequency envelope (ENV). A widely used method to decompose a speech signal into the TFS and ENV is the Hilbert transform. Although this method has been available for about one century and is widely applied in various kinds of speech processing tasks (e.g. speech chimeras), there are only very few speech processing packages that contain readily available functions for the Hilbert transform, and there is very little textbook type literature tailored for speech scientists to explain the processes behind the transform. With this paper we provide the code for carrying out the Hilbert operation to obtain the TFS and ENV in the widely used speech processing software Praat, and explain the basics of the procedure. To verify our code, we compare the Hilbert transform in Praat with a widely applied function for the same purpose in Matlab (\"hilbert(…)\"). We can confirm that both methods arrive at identical outputs",
    "checked": true,
    "id": "6d87fe97a55c142ba2a7aa7aeb8cb800e5f73725",
    "semantic_title": "a praat-based algorithm to extract the amplitude envelope and temporal fine structure using the hilbert transform",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2016/enzinger16_interspeech.html": {
    "title": "Likelihood Ratio Calculation in Acoustic-Phonetic Forensic Voice Comparison: Comparison of Three Statistical Modelling Approaches",
    "volume": "main",
    "abstract": "This study compares three statistical models used to calculate likelihood ratios in acoustic-phonetic forensic-voice-comparison systems: Multivariate kernel density, principal component analysis kernel density, and a multivariate normal model. The data were coefficient values obtained from discrete cosine transforms fitted to human-supervised formant-trajectory measurements of tokens of /iau/ from a database of recordings of 60 female speakers of Chinese. Tests were conducted using high-quality recordings as nominal suspect samples and mobile-to-landline transmitted recordings as nominal offender samples. Performance was assessed before and after fusion with a baseline automatic mel frequency cepstral coefficient Gaussian mixture model universal background model system. In addition, Monte Carlo simulations were used to compare the output of the statistical models to true likelihood-ratio values calculated on the basis of the distribution specified for a simulated population",
    "checked": true,
    "id": "10ec780fe1a9a04502dd392416a441ebacf02ee1",
    "semantic_title": "likelihood ratio calculation in acoustic-phonetic forensic voice comparison: comparison of three statistical modelling approaches",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/qi16_interspeech.html": {
    "title": "A Sparse Spherical Harmonic-Based Model in Subbands for Head-Related Transfer Functions",
    "volume": "main",
    "abstract": "Several functional models for head-related transfer function (HRTF) have been proposed based on spherical harmonic (SH) orthogonal functions, which yield an encouraging performance level in terms of log-spectral distortion (LSD). However, since the properties of subbands are quite different and highly subject-dependent, the degree of SH expansion should be adapted to the subband and the subject, which is quite challenging. In this paper, a sparse spherical harmonic-based model termed SSHM is proposed in order to achieve an intelligent frequency truncation. Different from SH-based model (SHM) which assigns the degree for each subband, SSHM constrains the number of SH coefficients by using an l penalty, and automatically preserves the significant coefficients in each subband. As a result, SSHM requires less coefficients at the same SD level than other truncation methods to reconstruct HRTFs . Furthermore, when used for interpolation, SSHM gives a better fitting precision since it naturally reduces the influence of the fluctuation caused by the movement of the subject and the processing error. The experiments show that even using about 40% less coefficients, SSHM has a slightly lower LSD than SHM. Therefore, SSHM can achieve a better tradeoff between efficiency and accuracy",
    "checked": true,
    "id": "0eff2296c52e2751b6ddd029737e37c1700b7e83",
    "semantic_title": "a sparse spherical harmonic-based model in subbands for head-related transfer functions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/isik16_interspeech.html": {
    "title": "Single-Channel Multi-Speaker Separation Using Deep Clustering",
    "volume": "main",
    "abstract": "Deep clustering is a recently introduced deep learning architecture that uses discriminatively trained embeddings as the basis for clustering. It was recently applied to spectrogram segmentation, resulting in impressive results on speaker-independent multi-speaker separation. In this paper we extend the baseline system with an end-to-end signal approximation objective that greatly improves performance on a challenging speech separation. We first significantly improve upon the baseline system performance by incorporating better regularization, larger temporal context, and a deeper architecture, culminating in an overall improvement in signal to distortion ratio (SDR) of 10.3 dB compared to the baseline of 6.0 dB for two-speaker separation, as well as a 7.1 dB SDR improvement for three-speaker separation. We then extend the model to incorporate an enhancement layer to refine the signal estimates, and perform end-to-end training through both the clustering and enhancement stages to maximize signal fidelity. We evaluate the results using automatic speech recognition. The new signal approximation objective, combined with end-to-end training, produces unprecedented performance, reducing the word error rate (WER) from 89.1% down to 30.8%. This represents a major advancement towards solving the cocktail party problem",
    "checked": true,
    "id": "ab94fae3d49cd7016a47020469dc257d8090f5bb",
    "semantic_title": "single-channel multi-speaker separation using deep clustering",
    "citation_count": 376
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16_interspeech.html": {
    "title": "Jointly Optimizing Activation Coefficients of Convolutive NMF Using DNN for Speech Separation",
    "volume": "main",
    "abstract": "Convolutive non-negative matrix factorization (CNMF) and deep neural networks (DNN) are two efficient methods for monaural speech separation. Conventional DNN focuses on building the non-linear relationship between mixture and target speech. However, it ignores the prominent structure of the target speech. Conventional CNMF model concentrates on capturing prominent harmonic structures and temporal continuities of speech but it ignores the non-linear relationship between the mixture and target. Taking these two aspects into consideration at the same time may result in better performance. In this paper, we propose a joint optimization of DNN models with an extra CNMF layer for speech separation task. We also utilize an extra masking layer on the proposed model to constrain the speech reconstruction. Moreover, a discriminative training criterion is proposed to further enhance the performance of the separation. Experimental results show that the proposed model has significant improvement in PESQ, SAR, SIR and SDR compared with conventional methods",
    "checked": true,
    "id": "8b2c9e86c87fe5d33e207e3e8c178f10ce520a28",
    "semantic_title": "jointly optimizing activation coefficients of convolutive nmf using dnn for speech separation",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/delfarah16_interspeech.html": {
    "title": "A Feature Study for Masking-Based Reverberant Speech Separation",
    "volume": "main",
    "abstract": "Monaural speech separation in reverberant conditions is very challenging. In masking-based separation, features extracted from speech mixtures are employed to predict a time-frequency mask. Robust feature extraction is crucial for the performance of supervised speech separation in adverse acoustic environments. Using objective speech intelligibility as the metric, we investigate a wide variety of monaural features in low signal-to-noise ratios and moderate to high reverberation. Deep neural networks are employed as the learning machine in our feature investigation. We find considerable performance gain using a contextual window in reverberant speech processing, likely due to temporal structure of reverberation. In addition, we systematically evaluate feature combinations. In unmatched noise and reverberation conditions, the resulting feature set from this study substantially outperforms previously employed sets for speech separation in anechoic conditions",
    "checked": true,
    "id": "a683c8a4fa3909aaf03a6c7e5c34469f8cb4a188",
    "semantic_title": "a feature study for masking-based reverberant speech separation",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hsu16b_interspeech.html": {
    "title": "Discriminative Layered Nonnegative Matrix Factorization for Speech Separation",
    "volume": "main",
    "abstract": "This paper proposes a discriminative layered nonnegative matrix factorization (DL-NMF) for monaural speech separation. The standard NMF conducts the parts-based representation using a single-layer of bases which was recently upgraded to the layered NMF (L-NMF) where a tree of bases was estimated for multi-level or multi-aspect decomposition of a complex mixed signal. In this study, we develop the DL-NMF by extending the generative bases in L-NMF to the discriminative bases which are estimated according to a discriminative criterion. The discriminative criterion is conducted by optimizing the recovery of the mixed spectra from the separated spectra and minimizing the reconstruction errors between separated spectra and original source spectra. The experiments on single-channel speech separation show the superiority of DL-NMF to NMF and L-NMF in terms of the SDR, SIR and SAR measures",
    "checked": true,
    "id": "0cd5a69f5662e9b987c16794931e4a91c8d90586",
    "semantic_title": "discriminative layered nonnegative matrix factorization for speech separation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gang16_interspeech.html": {
    "title": "On Discriminative Framework for Single Channel Audio Source Separation",
    "volume": "main",
    "abstract": "Single channel source separation (SCSS) algorithms that utilise discriminative source models perform better in comparison to those that are trained independently. However, all the aspects of training discriminative models have not been addressed in the literature. For instance, the choice of dimensions of source models (number of columns of NMF, Dictionary etc) not only influences the fidelity of a given source but also impacts the interference introduced in it. Therefore choosing a right dimension parameter for every source model is crucial for an effective separation. In fact, the similarity between the constituent sources can be different for different mixtures and thus, dimensions should also be chosen specific to the sources in the concerned mixture. Further, separation of a given constituent from a mixture, assuming remaining to be interferers, offers more freedom for the particular constituent and hence provide better separation. In this paper, we propose a generic discriminative learning framework where we separate one source at a time and embed our dimension search algorithm in the training of discriminative source models. We apply our framework on the NMF based SCSS algorithms and demonstrate a performance improvement in separation for both speech-speech and speech-music mixture",
    "checked": true,
    "id": "433341aa0c00452da994661ac18dd28569db7bed",
    "semantic_title": "on discriminative framework for single channel audio source separation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jin16_interspeech.html": {
    "title": "Generating Natural Video Descriptions via Multimodal Processing",
    "volume": "main",
    "abstract": "Generating natural language descriptions of visual content is an intriguing task which has wide applications such as assisting blind people. The recent advances in image captioning stimulate further study of this task in more depth including generating natural descriptions for videos. Most works of video description generation focus on visual information in the video. However, audio provides rich information for describing video contents as well. In this paper, we propose to generate video descriptions in natural sentences via multimodal processing, which refers to using both audio and visual cues via unified deep neural networks with both convolutional and recurrent structure. Experimental results on the Microsoft Research Video Description (MSVD) corpus prove that fusing audio information greatly improves the video description performance. We also investigate the impact of image amount vs caption amount on the image caption performance and see the trend that when limited amount of training is available, number of various captions is more important than number of various images. This will guide us to investigate in the future how to improve the video description system via increasing amount of training data",
    "checked": true,
    "id": "2abae43b4a7fd85473bd6c906a0fcfc403968e87",
    "semantic_title": "generating natural video descriptions via multimodal processing",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2016/heckmann16_interspeech.html": {
    "title": "Feature-Level Decision Fusion for Audio-Visual Word Prominence Detection",
    "volume": "main",
    "abstract": "Common fusion techniques in audio-visual speech processing operate on the modality level. I.e. they either combine the features extracted from the two modalities directly or derive a decision for each modality separately and then combine the modalities on the decision level. We investigate the audio-visual processing of linguistic prosody, more precisely the extraction of word prominence. In this context the different features for each modality can be assumed to be only partially dependent. Hence we propose to train a classifier for each of these features, acoustic and visual modality, and then combine them on a decision level. We compare this approach with conventional fusion methods, i.e. feature fusion and decision fusion on the modality level. Our results show that the feature-level decision fusion clearly outperforms the other approaches, in particular when we also additionally integrate the features resulting from the feature fusion. Compared to a detection based only on the full audio stream we obtain relative improvements from the audio-visual detection of 19% for clean audio and up to 50% for noisy audio",
    "checked": true,
    "id": "fd6b332fa960c3acdcc6ee194855608661970799",
    "semantic_title": "feature-level decision fusion for audio-visual word prominence detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ouni16_interspeech.html": {
    "title": "Acoustic and Visual Analysis of Expressive Speech: A Case Study of French Acted Speech",
    "volume": "main",
    "abstract": "Within the framework of developing an expressive audiovisual speech synthesis, an acoustic and visual analysis of expressive acted speech is proposed in this paper. Our purpose is to identify the main characteristics of audiovisual expressions that need to be integrated during synthesis to provide believable emotions to the virtual 3D talking head. We conducted a case study of a semi-professional actor who uttered a set of sentences for 6 different emotions in addition to neutral speech. We have recorded concurrently audio and motion capture data. The acoustic and the visual data have been analyzed. The main finding is that although some expressions are not well identified, some expressions were well characterized and tied in both acoustic and visual space",
    "checked": true,
    "id": "6079eb2dcdce230b2acf67290169094eaa580c6c",
    "semantic_title": "acoustic and visual analysis of expressive speech: a case study of french acted speech",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/barbulescu16_interspeech.html": {
    "title": "Characterization of Audiovisual Dramatic Attitudes",
    "volume": "main",
    "abstract": "In this work we explore the capability of audiovisual parameters (such as fundamental frequency, rhythm, head motion or facial expressions) to discriminate among different dramatic attitudes. We extract the audiovisual parameters from an acted corpus of attitudes and structure them as frame, syllable, and sentence-level features. Using Linear Discriminant Analysis classifiers, we show that sentence-level features present a higher discriminating rate among the attitudes. We also compare the classification results with the perceptual evaluation tests, showing that F0 is correlated to the perceptual results for all attitudes, while other features, such as head motion, contribute differently, depending both on the attitude and the speaker",
    "checked": true,
    "id": "5338ad86b1b4de74cc9dca5c287814ae34e37bc9",
    "semantic_title": "characterization of audiovisual dramatic attitudes",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/huang16_interspeech.html": {
    "title": "Conversational Engagement Recognition Using Auditory and Visual Cues",
    "volume": "main",
    "abstract": "Automatic prediction of engagement in human-human and human-machine dyadic and multiparty interaction scenarios could greatly aid in evaluation of the success of communication. A corpus of eight face-to-face dyadic casual conversations was recorded and used as the basis for an engagement study, which examined the effectiveness of several methods of engagement level recognition. A convolutional neural network based analysis was seen to be the most effective",
    "checked": true,
    "id": "c2d439dd891553659e1805ea8d98be5a9be7732a",
    "semantic_title": "conversational engagement recognition using auditory and visual cues",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chaspari16_interspeech.html": {
    "title": "An Acoustic Analysis of Child-Child and Child-Robot Interactions for Understanding Engagement during Speech-Controlled Computer Games",
    "volume": "main",
    "abstract": "Engagement is an essential factor towards successful game design and effective human-computer interaction. We analyze the prosodic patterns of child-child and child-robot pairs playing a language-based computer game. Acoustic features include speech loudness and fundamental frequency. We use a linear mixed-effects model to capture the coordination of acoustic patterns between interactors as well as its relation to annotated engagement levels. Our results indicate that the considered acoustic features are related to engagement levels for both the child-child and child-robot interaction. They further suggest significant association of the prosodic patterns during the child-child scenario, which is moderated by the co-occurring engagement. This acoustic coordination is not present in the child-robot interaction, since the robot's behavior was not automatically adjusted to the child. These findings are discussed in relation to automatic robot adaptation and provide a foundation for promoting engagement and enhancing rapport during the considered game-based interactions",
    "checked": true,
    "id": "bc246f90ca78fca4866106de3913ee6d46981036",
    "semantic_title": "an acoustic analysis of child-child and child-robot interactions for understanding engagement during speech-controlled computer games",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kasisopa16_interspeech.html": {
    "title": "Auditory-Visual Lexical Tone Perception in Thai Elderly Listeners with and without Hearing Impairment",
    "volume": "main",
    "abstract": "Lexical tone perception was investigated in elderly Thais with Normal Hearing (NH), or Hearing Impairment (HI), the latter with and without Hearing Aids. Auditory-visual (AV), auditory-only (AO), and visual-only (VO) discrimination of Thai tones was investigated. Both groups performed poorly in VO. In AV and AO, the NH performed better than the HI group, and Hearing Aids facilitated tone discrimination. There was slightly more visual augmentation (AV>AO) for the HI group, but not the NH group. The Falling-Rising (FR) pair of tones was easiest to discriminate for both groups and there was a similar ranking of relative discriminability of all 10 tone contrasts for the HI group with and without hearing aids, but this differed from the ranking in the NH group. These results show that the Hearing Impaired elderly with and without hearing aids can, and do, use visual speech information to augment tone perception, but do so in a similar, not a significantly more enhanced manner than the Normal Hearing elderly. Thus hearing loss in the Thai elderly does not result in greater use of visual information for discrimination of lexical tone; rather, all Thai elderly use visual information to augment their auditory perception of tone",
    "checked": true,
    "id": "8928061883cab4e352c71eca759187d62a9dddfd",
    "semantic_title": "auditory-visual lexical tone perception in thai elderly listeners with and without hearing impairment",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/khaki16_interspeech.html": {
    "title": "Use of Agreement/Disagreement Classification in Dyadic Interactions for Continuous Emotion Recognition",
    "volume": "main",
    "abstract": "Natural and affective handshakes of two participants define the course of dyadic interaction. Affective states of the participants are expected to be correlated with the nature or type of the dyadic interaction. In this study, we investigate relationship between affective attributes and nature of dyadic interaction. In this investigation we use the JESTKOD database, which consists of speech and full-body motion capture data recordings for dyadic interactions under agreement and disagreement scenarios. The dataset also has affective annotations in activation, valence and dominance (AVD) attributes. We pose the continuous affect recognition problem under agreement and disagreement scenarios of dyadic interactions. We define a statistical mapping using the support vector regression (SVR) from speech and motion modalities to affective attributes with and without the dyadic interaction type (DIT) information. We observe an improvement in estimation of the valence attribute when the DIT is available. Furthermore this improvement sustains even we estimate the DIT from the speech and motion modalities of the dyadic interaction",
    "checked": true,
    "id": "e7ad909b554506b3658c19def0bc691af28b2d07",
    "semantic_title": "use of agreement/disagreement classification in dyadic interactions for continuous emotion recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schadler16_interspeech.html": {
    "title": "Microscopic Multilingual Matrix Test Predictions Using an ASR-Based Speech Recognition Model",
    "volume": "main",
    "abstract": "In an attempt to predict the outcomes of matrix sentence tests in different languages and various noise conditions for native listeners, the simulation framework for auditory discrimination experiments (FADE) and the extended Speech Intelligibility Index (eSII) is employed. FADE uses an automatic speech recognition system to simulate recognition experiments and reports the highest achievable performance as the outcome, which showed good predictions for the German matrix test in noise. The eSII is based on the short-time analysis of weighted signal-to-noise ratios in different frequency bands. In contrast to many other approaches, including the eSII, FADE uses no empirical reference. In this work, the FADE approach is evaluated for predictions of the German, Polish, Russian, and Spanish matrix test in stationary and fluctuating noise conditions. The FADE-based predictions yield a high correlation (Pearsons R = 0.94) with the empirical data and a root-mean-square (RMS) prediction error of 1.9 dB outperforming the eSII-based predictions (R = 0.78, RMS = 4.2 dB). FADE can also predict the data of subgroups with only stationary or only fluctuating noises, while the eSII cannot. The FADE-based predictions seem to generalize over different languages and noise conditions",
    "checked": true,
    "id": "b410233111e76648c4a917fdde69230e02b1e8af",
    "semantic_title": "microscopic multilingual matrix test predictions using an asr-based speech recognition model",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2016/exter16_interspeech.html": {
    "title": "DNN-Based Automatic Speech Recognition as a Model for Human Phoneme Perception",
    "volume": "main",
    "abstract": "In this paper, we test the applicability of state-of-the-art automatic speech recognition (ASR) to predict phoneme confusions in human listeners. Phoneme-specific response rates are obtained from ASR based on deep neural networks (DNNs) and from listening tests with six normal-hearing subjects. The measure for model quality is the correlation of phoneme recognition accuracies obtained in ASR and in human speech recognition (HSR). Various feature representations are used as input to the DNNs to explore their relation to overall ASR performance and model prediction power. Standard filterbank output and perceptual linear prediction (PLP) features result in best predictions, with correlation coefficients reaching r = 0.9",
    "checked": true,
    "id": "2211cee65df1be7fcdd4c69a908f9369f00657b2",
    "semantic_title": "dnn-based automatic speech recognition as a model for human phoneme perception",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2016/toth16_interspeech.html": {
    "title": "Undoing Misperceptions: A Microscopic Analysis of Consistent Confusions Through Signal Modifications",
    "volume": "main",
    "abstract": "Consistent confusions — word misperceptions reported in an open set task with a high agreement across listeners — can be especially valuable in understanding the detailed processes underlying speech perception. The current study investigates the origin of a set of consistent confusions collected in a variety of masking conditions, by applying signal-level modifications to the stimuli eliciting the confusion, and subsequently reevaluating listeners' percepts. Modifications were selected to provide release from either the energetic or the informational component of the maskers and involved manipulations of signal-to-noise ratio, fundamental frequency, and resynthesis of the noise-mixture in glimpsed regions of the target speech. Increasing signal-to-noise ratio and glimpse resynthesis showed the expected release from energetic and informational masking respectively. However, manipulations targeting informational masking release, including fundamental frequency modification, affected a surprisingly high number of confusions stemming from energetic maskers. The degree of fundamental frequency shift did not have a significant effect on the response patterns observed. Around 30% of confusions can be explained solely based on the information contained within the target glimpses surviving energetic masking, while for the rest of the cases additional factors, such as recruitment of information from the masker, appear to be involved",
    "checked": true,
    "id": "7c193f57b8b29f2a8d1fa27f9a6939433569c299",
    "semantic_title": "undoing misperceptions: a microscopic analysis of consistent confusions through signal modifications",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/karbasi16_interspeech.html": {
    "title": "Blind Non-Intrusive Speech Intelligibility Prediction Using Twin-HMMs",
    "volume": "main",
    "abstract": "Automatic prediction of speech intelligibility is highly desirable in the speech research community, since listening tests are time-consuming and can not be used online. Most of the available objective speech intelligibility measures are intrusive methods, as they require a clean reference signal in addition to the corresponding noisy/processed signal at hand. In order to overcome the problem of predicting the speech intelligibility in the absence of the clean reference signal, we have proposed in [1] to employ a recognition/synthesis framework called twin hidden Markov model (THMM) for synthesizing the clean features, required inside an intrusive intelligibility prediction method. The new framework can predict the speech intelligibility equally well as well-known intrusive methods like the short-time objective intelligibility (STOI). The original THMM, however, requires the correct transcription for synthesizing the clean reference features, which is not always available. In this paper, we go one step further and investigate the use of the recognized transcription instead of the oracle transcription for obtaining a more widely applicable speech intelligibility prediction. We show that the output of the newly-proposed blind approach is highly correlated with the human speech recognition results, collected via crowdsourcing in different noise conditions",
    "checked": true,
    "id": "9d439e0238e369b1b777cfb47e87a45f57bf088f",
    "semantic_title": "blind non-intrusive speech intelligibility prediction using twin-hmms",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/toth16b_interspeech.html": {
    "title": "Misperceptions Arising from Speech-in-Babble Interactions",
    "volume": "main",
    "abstract": "The deterioration of speech intelligibility in the presence of other sound sources has been explained in terms of both energetic masking, which renders parts of the speech signal inaudible, and informational masking, in which audible components of the masker interfere with speech identification. The current study focuses on the role of a specific form of informational masking in which audible glimpses of both target and masker combine to produce an incorrect listener percept. We examine a corpus of word misperceptions in Spanish which occur when target words are combined with a babble masker. Glimpses originating in both the target and the masker are force-aligned to the reported misperceived word in order to identify the most likely acoustic evidential basis for the confusion. In this way, the degree of involvement of both target and masker can be quantified. In nearly all cases, the best explanation for the misperception involves recruiting evidence from the babble masker (type I error), and in more than 80% of the tokens some of the audible target evidence is ignored (type II error). These findings suggest misallocation of acoustic-phonetic material plays a significant role in the generation of speech-in-babble confusions",
    "checked": true,
    "id": "804cb0251f9ce98ecb353343f727b62446f7950b",
    "semantic_title": "misperceptions arising from speech-in-babble interactions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/eichenauer16_interspeech.html": {
    "title": "Introducing Temporal Rate Coding for Speech in Cochlear Implants: A Microscopic Evaluation in Humans and Models",
    "volume": "main",
    "abstract": "Standard cochlea implant (CI) speech coding strategies transmit formant information only via the place of the stimulated electrode. In acoustic hearing, however, formant frequencies are additionally coded via the temporal rate of auditory nerve firing. This study presents a novel CI coding strategy (\"Formant Locking (FL)-strategy\") that varies stimulation rates in relation to extracted fundamental and formant frequencies. Simulated auditory nerve activity resulting from stimulation with the FL-strategy shows that the FL-strategy triggers spike rates that are related to the formant frequencies similar as in normal hearing, and greatly different than in a standard CI strategy. Vowel recognition in seven CI users via direct stimulation of their electrode array shows that the FL-strategy results in significantly increased scores of the vowels /u/ and /i/ compared to a standard CI strategy. However, at the same time, a decrease in scores for /o/ and /e/ occurred. A microscopic speech intelligibility model involving an automatic speech recognizer reveals good agreement between modeled and predicted confusion matrices for the FL-strategy. This suggests that microscopic models can be used to test CI strategies in the development phase, and gives indications which cues might be used by the listeners for speech recognition",
    "checked": true,
    "id": "a1dc945f2bda9bcd9b0e400b790f9770031fb0c9",
    "semantic_title": "introducing temporal rate coding for speech in cochlear implants: a microscopic evaluation in humans and models",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lecumberri16_interspeech.html": {
    "title": "Language Effects in Noise-Induced Word Misperceptions",
    "volume": "main",
    "abstract": "Speech misperceptions provide a window into the processes underlying spoken language comprehension. One approach shown to catalyse robust misperceptions is to embed words in noise. However, the use of masking noise makes it difficult to measure the relative contributions of low-level auditory processing and higher-level factors which involve the deployment of linguistic experience. The current study addresses this confound by comparing noise-induced misperceptions in two languages, Spanish and English, which display marked phonological differences in properties such as consonant-vowel ratio, rhythm and syllable structure. An analysis of over 5000 word-level misperceptions generated using a common experimental framework in the two languages reveals some striking similarities: the proportion of confusions generated by three distinct types of masker are almost identical for the two languages, as are the proportions of phonemic and syllabic insertions, deletions and substitutions. The biggest difference is seen for babble noise, which tends to induce relatively complex confusions in English and simpler confusions in Spanish. We speculate that the inflectional morphology of Spanish lends itself to more easily recruit single elements from a babble masker into valid word hypotheses",
    "checked": true,
    "id": "e48944bf6a1c609be0222c9a01fd86e16771f59c",
    "semantic_title": "language effects in noise-induced word misperceptions",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/varnet16_interspeech.html": {
    "title": "Speech Reductions Cause a De-Weighting of Secondary Acoustic Cues",
    "volume": "main",
    "abstract": "The ability of the auditory system to change the perceptual weighting of acoustic cues when faced with degraded speech has long been evidenced. However, the exact changes that occur remain mostly unknown. Here, we proposed to use the Auditory Classification Image (ACI) methodology to reveal the acoustic cues used in natural speech comprehension and in reduced (i.e. noise-vocoded or re-synthesized) speech comprehension. The results show that in the latter case the auditory system updates its listening strategy by de-weighting secondary acoustic cues. Indeed, these are often weaker and thus more easily erased in adverse listening conditions. Furthermore our data suggests that this de-weighting does not directly depend on the actual reliability of the cues, but rather on the expected change in informativeness",
    "checked": true,
    "id": "7b010463107e77c0ca6026021201741c32c0d3d4",
    "semantic_title": "speech reductions cause a de-weighting of secondary acoustic cues",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fontan16_interspeech.html": {
    "title": "Using Phonologically Weighted Levenshtein Distances for the Prediction of Microscopic Intelligibility",
    "volume": "main",
    "abstract": "This article presents a new method for analyzing Automatic Speech Recognition (ASR) results at the phonological feature level. To this end the Levenshtein distance algorithm is refined in order to take into account the distinctive features opposing substituted phonemes. This method allows to survey features additions or deletions, providing microscopic qualitative information as a complement to word recognition scores. To explore the relevance of the qualitative data gathered by this method, a study is conducted on a speech corpus simulating presbycusis effects on speech perception at eight severity stages. Consonantic features additions and deletions in ASR outputs are analyzed and put in relation with intelligibility data collected in 30 human subjects. ASR results show monotonic trends in most consonantic features along the degradation conditions, which appear to be consistent with the misperceptions that could be observed in human subjects",
    "checked": true,
    "id": "8e31bae45564d6e03490e3f8fb923a51b34394cd",
    "semantic_title": "using phonologically weighted levenshtein distances for the prediction of microscopic intelligibility",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2016/matsui16_interspeech.html": {
    "title": "The Impact of Manner of Articulation on the Intelligibility of Voicing Contrast in Noise: Cross-Linguistic Implications",
    "volume": "main",
    "abstract": "The current study addresses the impact of manner of articulation on the intelligibility of voicing contrast in noise from a cross-linguistic perspective. Previous noise-masking studies have suggested that the impact of manner of articulation on the intelligibility of voicing contrast in noise is apparently different in Russian and English. In order to further assess the source of such a cross-linguistic inconsistency, the current study examines how Russian voicing contrast is perceived by English listeners. Native listeners of English performed a forced-choice identification task with Russian voiced and voiceless stimuli in quiet and noisy conditions. The results showed that the voicing contrast in stops were more confused than that in fricatives for English listeners, showing a pattern similar to Russian listeners. The results suggest that the source of the cross-linguistic difference identified in previous studies comes from the difference in the acoustic properties of the stimuli, reflecting the difference in phonetic implementation of voicing contrasts in each language. The results in turn suggest that perceptual cue weighting strategies for perceiving voicing contrast in different manners of articulation is similar among Russian and English listeners",
    "checked": true,
    "id": "0512001b1a715875c026c8474c4c30b69309e322",
    "semantic_title": "the impact of manner of articulation on the intelligibility of voicing contrast in noise: cross-linguistic implications",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mandel16_interspeech.html": {
    "title": "Directly Comparing the Listening Strategies of Humans and Machines",
    "volume": "main",
    "abstract": "In a given noisy environment, human listeners can more accurately identify spoken words than automatic speech recognizers. It is not clear, however, what information the humans are able to utilize in doing so that the machines are not. This paper uses a recently introduced technique to directly characterize the information used by humans and machines on the same task. The task was a forced choice between eight sentences spoken by a single talker from the small-vocabulary GRID corpus that were selected to be maximally confusable with one another. These sentences were mixed with \"bubble\" noise, which is designed to reveal randomly selected time-frequency glimpses of the sentence. Responses to these noisy mixtures allowed the identification of time-frequency regions that were important for each listener to recognize each sentence, i.e., regions that were frequently audible when a sentence was correctly identified and inaudible when it was not. In comparing these regions across human and machine listeners, we found that dips in noise allowed the humans to recognize words based on informative speech cues. In contrast, the baseline CHiME-2-GRID recognizer correctly identified sentences only when the time-frequency profile of the noisy mixture matched that of the underlying speech",
    "checked": true,
    "id": "73a9ecf0ba89d277e9c1664d4da8f4105bc7ea85",
    "semantic_title": "directly comparing the listening strategies of humans and machines",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/rondeau16_interspeech.html": {
    "title": "LSTM-Based NeuroCRFs for Named Entity Recognition",
    "volume": "main",
    "abstract": "Although NeuroCRF, an augmented Conditional Random Fields (CRF) model whose feature function is parameterized as a Feed-Forward Neural Network (FF NN) on word embeddings, has soundly outperformed traditional linear-chain CRF on many sequence labeling tasks, it is held back by the fact that FF NNs have a fixed input length and therefore cannot take advantage of the full input sentence. We propose to address this issue by replacing the FF NN with a Long Short-Term Memory (LSTM) NN, which can summarize an input of arbitrary length into a fixed dimension representation. The resulting model obtains F =89.28 on WikiNER dataset, a significant improvement over the NeuroCRF baseline's F =87.58, which is already a highly competitive result",
    "checked": true,
    "id": "ff02c951e5ca708b409993dc5b9b8ea712b69b07",
    "semantic_title": "lstm-based neurocrfs for named entity recognition",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16b_interspeech.html": {
    "title": "Exploring Word Mover's Distance and Semantic-Aware Embedding Techniques for Extractive Broadcast News Summarization",
    "volume": "main",
    "abstract": "Extractive summarization is a process that manages to select the most salient sentences from a document (or a set of documents) and subsequently assemble them to form an informative summary, facilitating users to browse and assimilate the main theme of the document efficiently. Our work in this paper continues this general line of research and its main contributions are two-fold. First, we explore to leverage the recently proposed word mover's distance (WMD) metric, in conjunction with semantic-aware continuous space representations of words, to authentically capture finer-grained sentence-to-document and/or sentence-to-sentence semantic relatedness for effective use in the summarization process. Second, we investigate to combine our proposed approach with several state-of-the-art summarization methods, which originally adopted the conventional term-overlap or bag-of-words (BOW) approaches for similarity calculation. A series of experiments conducted on a typical broadcast news summarization task seem to suggest the performance merits of our proposed approach, in comparison to the mainstream methods",
    "checked": true,
    "id": "5ba21683c165a19d6719173a3d4fcdbb0566d7fb",
    "semantic_title": "exploring word mover's distance and semantic-aware embedding techniques for extractive broadcast news summarization",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sheikh16_interspeech.html": {
    "title": "Improved Neural Bag-of-Words Model to Retrieve Out-of-Vocabulary Words in Speech Recognition",
    "volume": "main",
    "abstract": "Many Proper Names (PNs) are Out-Of-Vocabulary (OOV) words for speech recognition systems used to process diachronic audio data. To enable recovery of the PNs missed by the system, relevant OOV PNs can be retrieved by exploiting the semantic context of the spoken content. In this paper, we explore the Neural Bag-of-Words (NBOW) model, proposed previously for text classification, to retrieve relevant OOV PNs. We propose a Neural Bag-of-Weighted-Words (NBOW2) model in which the input embedding layer is augmented with a context anchor layer. This layer learns to assign importance to input words and has the ability to capture (task specific) key-words in a NBOW model. With experiments on French broadcast news videos we show that the NBOW and NBOW2 models outperform earlier methods based on raw embeddings from LDA and Skip-gram. Combining NBOW with NBOW2 gives faster convergence during training",
    "checked": true,
    "id": "0289897736a87e906f93b7e2fa5fe3fe1f3d3cb3",
    "semantic_title": "improved neural bag-of-words model to retrieve out-of-vocabulary words in speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/trione16_interspeech.html": {
    "title": "Beyond Utterance Extraction: Summary Recombination for Speech Summarization",
    "volume": "main",
    "abstract": "This paper describes a template filling approach for creating conversation summaries. The templates are generated from generalized summary fragments from a training corpus. Necessary pieces of information for filling them are extracted automatically from the conversation transcripts given linguistic features, and drive the fragment selection process. The approach obtains ROUGE-2 scores of 0.08471 on the RATP-DECODA corpus, which represents a significant improvement over extractive baselines and hand-written templates",
    "checked": true,
    "id": "4d9451a6892690cf1e3b3b3a34fb1595428aa396",
    "semantic_title": "beyond utterance extraction: summary recombination for speech summarization",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16c_interspeech.html": {
    "title": "Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling",
    "volume": "main",
    "abstract": "Attention-based encoder-decoder neural network models have recently shown promising results in machine translation and speech recognition. In this work, we propose an attention-based neural network model for joint intent detection and slot filling, both of which are critical steps for many speech understanding and dialog systems. Unlike in machine translation and speech recognition, alignment is explicit in slot filling. We explore different strategies in incorporating this alignment information to the encoder-decoder framework. Learning from the attention mechanism in encoder-decoder model, we further propose introducing attention to the alignment-based RNN models. Such attentions provide additional information to the intent classification and slot label prediction. Our independent task models achieve state-of-the-art intent detection error rate and slot filling F1 score on the benchmark ATIS task. Our joint training model further obtains 0.56% absolute (23.8% relative) error reduction on intent detection and 0.23% absolute gain on slot filling over the independent task models",
    "checked": true,
    "id": "2167f9ffd36af6c723d3527eab60c731e13d3a90",
    "semantic_title": "attention-based recurrent neural network models for joint intent detection and slot filling",
    "citation_count": 585
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jaech16_interspeech.html": {
    "title": "Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding",
    "volume": "main",
    "abstract": "The goal of this paper is to use multi-task learning to efficiently scale slot filling models for natural language understanding to handle multiple target tasks or domains. The key to scalability is reducing the amount of training data needed to learn a model for a new task. The proposed multi-task model delivers better performance with less data by leveraging patterns that it learns from the other tasks. The approach supports an open vocabulary, which allows the models to generalize to unseen words, which is particularly important when very little training data is used. A newly collected crowd-sourced data set, covering four different domains, is used to demonstrate the effectiveness of the domain adaptation and open vocabulary techniques",
    "checked": true,
    "id": "cf8a8d9f3c7466c9ada36420ebb510a742b34308",
    "semantic_title": "domain adaptation of recurrent neural networks for natural language understanding",
    "citation_count": 72
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ladhak16_interspeech.html": {
    "title": "LatticeRnn: Recurrent Neural Networks Over Lattices",
    "volume": "main",
    "abstract": "We present a new model called LatticeRnn, which generalizes recurrent neural networks (RNNs) to process weighted lattices as input, instead of sequences. A LatticeRnn can encode the complete structure of a lattice into a dense representation, which makes it suitable to a variety of problems, including rescoring, classifying, parsing, or translating lattices using deep neural networks (DNNs). In this paper, we use LatticeRnns for a classification task: each lattice represents the output from an automatic speech recognition (ASR) component of a spoken language understanding (SLU) system, and we classify the intent of the spoken utterance based on the lattice embedding computed by a LatticeRnn. We show that making decisions based on the full ASR output lattice, as opposed to 1-best or n-best hypotheses, makes SLU systems more robust to ASR errors. Our experiments yield improvements of 13% over a baseline RNN system trained on transcriptions and 10% over an n-best list rescoring system for intent classification",
    "checked": true,
    "id": "3d82efb6a2613853df4e811bdc2158c1cbb7875c",
    "semantic_title": "latticernn: recurrent neural networks over lattices",
    "citation_count": 63
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kesiraju16_interspeech.html": {
    "title": "Learning Document Representations Using Subspace Multinomial Model",
    "volume": "main",
    "abstract": "Subspace multinomial model (SMM) is a log-linear model and can be used for learning low dimensional continuous representation for discrete data. SMM and its variants have been used for speaker verification based on prosodic features and phonotactic language recognition. In this paper, we propose a new variant of SMM that introduces sparsity and call the resulting model as ℓ SMM. We show that ℓ SMM can be used for learning document representations that are helpful in topic identification or classification and clustering tasks. Our experiments in document classification show that SMM achieves comparable results to models such as latent Dirichlet allocation and sparse topical coding, while having a useful property that the resulting document vectors are Gaussian distributed",
    "checked": true,
    "id": "dec6984e611eb7538f7526fe43b17ce429c0951c",
    "semantic_title": "learning document representations using subspace multinomial model",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhao16_interspeech.html": {
    "title": "Attention-Based Convolutional Neural Networks for Sentence Classification",
    "volume": "main",
    "abstract": "Sentence classification is one of the foundational tasks in spoken language understanding (SLU) and natural language processing (NLP). In this paper we propose a novel convolutional neural network (CNN) with attention mechanism to improve the performance of sentence classification. In traditional CNN, it is not easy to encode long term contextual information and correlation between non-consecutive words effectively. In contrast, our attention-based CNN is able to capture these kinds of information for each word without any external features. We conducted experiments on various public and in-house datasets. The experimental results demonstrate that our proposed model significantly outperforms the traditional CNN model and achieves competitive performance with the ones that exploit rich syntactic features",
    "checked": true,
    "id": "fe888bd5f170d0e4c0a6eca0a508b4681bcfdb95",
    "semantic_title": "attention-based convolutional neural networks for sentence classification",
    "citation_count": 61
  },
  "https://www.isca-speech.org/archive/interspeech_2016/morchid16_interspeech.html": {
    "title": "Spoken Language Understanding in a Latent Topic-Based Subspace",
    "volume": "main",
    "abstract": "Performance of spoken language understanding applications declines when spoken documents are automatically transcribed in noisy conditions due to high Word Error Rates (WER). To improve the robustness to transcription errors, recent solutions propose to map these automatic transcriptions in a latent space. These studies have proposed to compare classical topic-based representations such as Latent Dirichlet Allocation (LDA), supervised LDA and author-topic (AT) models. An original compact representation, called c-vector, has recently been introduced to walk around the tricky choice of the number of latent topics in these topic-based representations. Moreover, c-vectors allow to increase the robustness of document classification with respect to transcription errors by compacting different LDA representations of a same speech document in a reduced space and then compensate most of the noise of the document representation. The main drawback of this method is the number of sub-tasks needed to build the c-vector space. This paper proposes to both improve this compact representation (c-vector) of spoken documents and to reduce the number of needed sub-tasks, using an original framework in a robust low dimensional space of features from a set of AT models called \"Latent Topic-based Subspace\" (LTS). In comparison to LDA, the AT model considers not only the dialogue content (words), but also the class related to the document. Experiments are conducted on the DECODA corpus containing speech conversations from the call-center of the RATP Paris transportation company. Results show that the original LTS representation outperforms the best previous compact representation (c-vector), with a substantial gain of more than 2.5% in terms of correctly labeled conversations",
    "checked": true,
    "id": "f248666cdeab5a16ebfa8902ad6b240992fd34a9",
    "semantic_title": "spoken language understanding in a latent topic-based subspace",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hakkanitur16_interspeech.html": {
    "title": "Multi-Domain Joint Semantic Frame Parsing Using Bi-Directional RNN-LSTM",
    "volume": "main",
    "abstract": "Sequence-to-sequence deep learning has recently emerged as a new paradigm in supervised learning for spoken language understanding. However, most of the previous studies explored this framework for building single domain models for each task, such as slot filling or domain classification, comparing deep learning based approaches with conventional ones like conditional random fields. This paper proposes a holistic multi-domain, multi-task (i.e. slot filling, domain and intent detection) modeling approach to estimate complete semantic frames for all user utterances addressed to a conversational system, demonstrating the distinctive power of deep learning methods, namely bi-directional recurrent neural network (RNN) with long-short term memory (LSTM) cells (RNN-LSTM) to handle such complexity. The contributions of the presented work are three-fold: (i) we propose an RNN-LSTM architecture for joint modeling of slot filling, intent determination, and domain classification; (ii) we build a joint multi-domain model enabling multi-task deep learning where the data from each domain reinforces each other; (iii) we investigate alternative architectures for modeling lexical context in spoken language understanding. In addition to the simplicity of the single model framework, experimental results show the power of such an approach on Microsoft Cortana real user data over alternative methods based on single domain/task deep learning",
    "checked": true,
    "id": "9b82c6e78ceaa5e540862849defc818f7c8a47df",
    "semantic_title": "multi-domain joint semantic frame parsing using bi-directional rnn-lstm",
    "citation_count": 409
  },
  "https://www.isca-speech.org/archive/interspeech_2016/janod16_interspeech.html": {
    "title": "Deep Stacked Autoencoders for Spoken Language Understanding",
    "volume": "main",
    "abstract": "The automatic transcription process of spoken document results in several word errors, especially when very noisy conditions are encountered. Document representations based on neural embedding frameworks have recently shown significant improvements in different Spoken and Natural Language Understanding tasks such as denoising and filtering. Nonetheless, these methods mainly need clean representations, failing to properly remove noise contained in noisy representations. This paper proposes to study the impact of residual noise contained into automatic transcripts of spoken dialogues in highly abstract spaces from deep neural networks. The paper makes the assumption that the noise learned from \"clean\" manual transcripts of spoken documents moves down dramatically the performance of theme identification systems in noisy conditions. The proposed deep neural network takes, as input and output, highly imperfect transcripts from spoken dialogues to improve the robustness of the document representation in a noisy environment. Results obtained on the DECODA theme classification task of dialogues reach an accuracy of 82% with a significant gain of about 5%",
    "checked": true,
    "id": "70189b6be960084a28279ca5c3d5a81d5736b4cb",
    "semantic_title": "deep stacked autoencoders for spoken language understanding",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kurata16b_interspeech.html": {
    "title": "Labeled Data Generation with Encoder-Decoder LSTM for Semantic Slot Filling",
    "volume": "main",
    "abstract": "To train a model for semantic slot filling, manually labeled data in which each word is annotated with a semantic slot label is necessary while manually preparing such data is costly. Starting from a small amount of manually labeled data, we propose a method to generate the labeled data with using the encoder-decoder LSTM. We first train the encoder-decoder LSTM that accepts and generates the same manually labeled data. Then, to generate a wide variety of labeled data, we add perturbations to the vector that encodes the manually labeled data and generate labeled data with the decoder LSTM based on the perturbated encoded vector. We also try to enhance the encoder-decoder LSTM to generate the word sequences and their label sequences separately to obtain new pairs of words and their labels. Through the experiments with the standard ATIS slot filling task, by using the generated data, we obtained improvement in slot filling accuracy over the strong baseline with the NN-based slot filling model",
    "checked": true,
    "id": "7ffe83d7dd3a474e15ccc2aef412009f100a5802",
    "semantic_title": "labeled data generation with encoder-decoder lstm for semantic slot filling",
    "citation_count": 42
  },
  "https://www.isca-speech.org/archive/interspeech_2016/stehwien16_interspeech.html": {
    "title": "Exploring the Correlation of Pitch Accents and Semantic Slots for Spoken Language Understanding",
    "volume": "main",
    "abstract": "We investigate the correlation between pitch accents and semantic slots in human-machine speech. Using an automatic pitch accent detector on the ATIS corpus, we find that most words labelled with semantic slots also carry a pitch accent. Most of the pitch accented words that are not associated with a semantic label are still meaningful, pointing towards the speaker's intention. Our findings show that prosody constitutes a relevant and useful resource for spoken language understanding, especially considering the fact that our pitch accent detector does not require any kind of manual transcriptions during testing time",
    "checked": true,
    "id": "d21ee832e173affb43b31e536dd05036daee2641",
    "semantic_title": "exploring the correlation of pitch accents and semantic slots for spoken language understanding",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tang16_interspeech.html": {
    "title": "Analysis on Gated Recurrent Unit Based Question Detection Approach",
    "volume": "main",
    "abstract": "Recent studies have shown various kinds of recurrent neural networks (RNNs) are becoming powerful sequence models in speech related applications. Our previous work in detecting questions of Mandarin speech presents that gated recurrent unit (GRU) based RNN can achieve significantly better results. In this paper, we try to open the black box to find the correlations between inner architecture of GRU and phonetic features of question sentences. We find that both update gate and reset gate in GRU blocks react when people begin to pronounce a word. According to the reactions, experiments are conducted to show the behavior of GRU based question detection approach on three important factors, including keywords or special structure of questions, final particles and interrogative intonation. We also observe that update gate and reset gate don't collaborate well on our dataset. Based on the asynchronous acts of update gate and reset gate in GRU, we adapt the structure of GRU block to our dataset and get further performance improvement in question detection task",
    "checked": true,
    "id": "9ea01d987bf991f40bf22392578e0dd88428e8ec",
    "semantic_title": "analysis on gated recurrent unit based question detection approach",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/oishi16_interspeech.html": {
    "title": "Combining State-Level Spotting and Posterior-Based Acoustic Match for Improved Query-by-Example Spoken Term Detection",
    "volume": "main",
    "abstract": "In spoken term detection (STD) systems, automatic speech recognition (ASR) frontend is often employed for its reasonable accuracy and efficiency. However, out-of-vocabulary (OOV) problem at ASR stage has a great impact on the STD performance for spoken query. In this paper, we propose combining feature-based acoustic match which is often employed in the STD systems for low resource languages, along with the other ASR-derived features. First, automatic transcripts for spoken document and spoken query are decomposed into corresponding acoustic model state sequences and used for spotting plausible speech segments. Second, DTW-based acoustic match between the query and candidate segment is performed using the posterior features derived from a monophone-state DNN. Finally, an integrated score is obtained by a logistic regression model, which is trained with a large spoken document and automatically generated spoken queries as development data. The experimental results on NTCIR-12 SpokenQuery&Doc-2 task showed that the proposed method significantly outperforms the baseline systems which use the subword-level or state-level spotting alone. Also, our universal scoring model trained with a separate set of development data could achieve the best STD performance, and showed the effectiveness of additional ASR-derived features regarding the confidence measure and query length",
    "checked": true,
    "id": "bbdf56cc40160cd1d5ff22ed974ebe288d54dc3f",
    "semantic_title": "combining state-level spotting and posterior-based acoustic match for improved query-by-example spoken term detection",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lv16_interspeech.html": {
    "title": "A Novel Discriminative Score Calibration Method for Keyword Search",
    "volume": "main",
    "abstract": "The performance of keyword search systems depends heavily on the quality of confidence scores. In this work, a novel discriminative score calibration method has been proposed. By training an MLP classifier employing the word posterior probability and several novel normalized scores, we can obtain a relative improvement of 4.67% for the actual term-weighted value (ATWV) metric on the OpenKWS15 development test dataset. In addition, a LSTM-CTC based keyword verification method has been proposed to supply extra acoustic information. After the information is added, a further improvement of 7.05% over the baseline can be observed",
    "checked": true,
    "id": "2b22f19fca23e238cf54136077e9dcb6c0715290",
    "semantic_title": "a novel discriminative score calibration method for keyword search",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/proenca16_interspeech.html": {
    "title": "Segmented Dynamic Time Warping for Spoken Query-by-Example Search",
    "volume": "main",
    "abstract": "This paper describes a low-resource approach to a Query-by-Example task, where spoken queries must be matched in a large dataset of spoken documents sometimes in complex or non-exact ways. Our approach tackles these complex match cases by using Dynamic Time Warping to obtain alternative paths that account for reordering of words, small extra content and small lexical variations. We also report certain advances on calibration and fusion of sub-systems that improve overall results, such as manipulating the score distribution per query and using an average posteriorgram distance matrix as an extra sub-system. Results are evaluated on the MediaEval task of Query-by-Example Search on Speech (QUESST). For this task, the language of the audio being searched is almost irrelevant, approaching the use case scenario to a language of very low resources. For that, we use as features the posterior probabilities obtained from five phonetic recognizers trained with five different languages",
    "checked": true,
    "id": "55c081a2ee9b0ff41bf1ce8e62dda408894095c3",
    "semantic_title": "segmented dynamic time warping for spoken query-by-example search",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lee16b_interspeech.html": {
    "title": "Generating Complementary Acoustic Model Spaces in DNN-Based Sequence-to-Frame DTW Scheme for Out-of-Vocabulary Spoken Term Detection",
    "volume": "main",
    "abstract": "This paper proposes a sequence-to-frame dynamic time warping (DTW) combination approach to improve out-of-vocabulary (OOV) spoken term detection (STD) performance gain. The goal of this paper is twofold: first, we propose a method that directly adopts the posterior probability of deep neural network (DNN) and Gaussian mixture model (GMM) as the similarity distance for sequence-to-frame DTW. Second, we investigate combinations of diverse schemes in GMM and DNN, with different subword units and acoustic models, estimate the complementarity in terms of performance gap and correlation of the combined systems, and discuss the performance gain of the combined systems. The results of evaluations conducted of the combined systems on an out-of-vocabulary spoken term detection task show that the performance gain of DNN-based systems is better than that of GMM-based systems. However, the performance gain obtained by combining DNN- and GMM-based systems is insignificant, even though DNN and GMM are highly heterogeneous. This is because the performance gap between DNN-based systems and GMM-based systems is quite large. On the other hand, score fusion of two heterogeneous subword units, triphone and sub-phonetic segments, in DNN-based systems provides significantly improved performance",
    "checked": true,
    "id": "766bd768cf0869554a5674c2d6000f3450e2b1d3",
    "semantic_title": "generating complementary acoustic model spaces in dnn-based sequence-to-frame dtw scheme for out-of-vocabulary spoken term detection",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/panchapagesan16_interspeech.html": {
    "title": "Multi-Task Learning and Weighted Cross-Entropy for DNN-Based Keyword Spotting",
    "volume": "main",
    "abstract": "We propose improved Deep Neural Network (DNN) training loss functions for more accurate single keyword spotting on resource-constrained embedded devices. The loss function modifications consist of a combination of multi-task training and weighted cross entropy. In the multi-task architecture, the keyword DNN acoustic model is trained with two tasks in parallel — the main task of predicting the keyword-specific phone states, and an auxiliary task of predicting LVCSR senones. We show that multi-task learning leads to comparable accuracy over a previously proposed transfer learning approach where the keyword DNN training is initialized by an LVCSR DNN of the same input and hidden layer sizes. The combination of LVCSR-initialization and Multi-task training gives improved keyword detection accuracy compared to either technique alone. We also propose modifying the loss function to give a higher weight on input frames corresponding to keyword phone targets, with a motivation to balance the keyword and background training data. We show that weighted cross-entropy results in additional accuracy improvements. Finally, we show that the combination of 3 techniques — LVCSR-initialization, multi-task training and weighted cross-entropy gives the best results, with significantly lower False Alarm Rate than the LVCSR-initialization technique alone, across a wide range of Miss Rates",
    "checked": true,
    "id": "c2d6c0613b7efedd507d44d8f6edd80b814ae8af",
    "semantic_title": "multi-task learning and weighted cross-entropy for dnn-based keyword spotting",
    "citation_count": 136
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chung16_interspeech.html": {
    "title": "Audio Word2Vec: Unsupervised Learning of Audio Segment Representations Using Sequence-to-Sequence Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/meng16_interspeech.html": {
    "title": "Non-Uniform Boosted MCE Training of Deep Neural Networks for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gorin16_interspeech.html": {
    "title": "Language Model Data Augmentation for Keyword Spotting in Low-Resourced Training Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/verwimp16_interspeech.html": {
    "title": "STON: Efficient Subtitling in Dutch Using State-of-the-Art Tools",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/stanislav16_interspeech.html": {
    "title": "An Automatic Training Tool for Air Traffic Control Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/karhila16_interspeech.html": {
    "title": "Digitala: An Augmented Test and Review Process Prototype for High-Stakes Spoken Foreign Language Examination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/damnati16_interspeech.html": {
    "title": "Exploring Collections of Multimedia Archives Through Innovative Interfaces in the Context of Digital Humanities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yuan16_interspeech.html": {
    "title": "Learning Neural Network Representations Using Cross-Lingual Bottleneck Features with Word-Pair Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16d_interspeech.html": {
    "title": "Novel Front-End Features Based on Neural Graph Embeddings for DNN-HMM and LSTM-CTC Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/abraham16_interspeech.html": {
    "title": "Articulatory Feature Extraction Using CTC to Build Articulatory Classifiers Without Forced Frame Alignments for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nagamine16_interspeech.html": {
    "title": "On the Role of Nonlinear Transformations in Deep Neural Network Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/variani16_interspeech.html": {
    "title": "Complex Linear Projection (CLP): A Discriminative Approach to Joint Feature Extraction and Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sainath16_interspeech.html": {
    "title": "Modeling Time-Frequency Patterns with LSTM vs. Convolutional Architectures for LVCSR Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mclaren16_interspeech.html": {
    "title": "The Speakers in the Wild (SITW) Speaker Recognition Database",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mclaren16b_interspeech.html": {
    "title": "The 2016 Speakers in the Wild Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/novotny16_interspeech.html": {
    "title": "Analysis of Speaker Recognition Systems in Realistic Scenarios of the SITW 2016 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kudashev16_interspeech.html": {
    "title": "A Speaker Recognition System for the SITW Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ghaemmaghami16_interspeech.html": {
    "title": "Speakers In The Wild (SITW): The QUT Speaker Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/khosravani16_interspeech.html": {
    "title": "AUT System for SITW Speaker Recognition Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kheder16_interspeech.html": {
    "title": "LIA System for the SITW Speaker Recognition Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16e_interspeech.html": {
    "title": "Investigating Various Diarization Algorithms for Speaker in the Wild (SITW) Speaker Recognition Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/scharenborg16_interspeech.html": {
    "title": "Does the Importance of Word-Initial and Word-Final Information Differ in Native versus Non-Native Spoken-Word Recognition?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/scharenborg16b_interspeech.html": {
    "title": "The Effect of Sentence Accent on Non-Native Speech Perception in Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cooke16_interspeech.html": {
    "title": "The Effects of Modified Speech Styles on Intelligibility for Non-Native Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16c_interspeech.html": {
    "title": "The Influence of Language Experience on the Categorical Perception of Vowels: Evidence from Mandarin and Korean",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/massaro16_interspeech.html": {
    "title": "Multiple Influences on Vocabulary Acquisition: Parental Input Dominates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gong16b_interspeech.html": {
    "title": "Can Intensive Exposure to Foreign Language Sounds Affect the Perception of Native Sounds?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bassiou16_interspeech.html": {
    "title": "Privacy-Preserving Speech Analytics for Automatic Assessment of Student Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nasir16_interspeech.html": {
    "title": "Complexity in Prosody: A Nonlinear Dynamical Systems Approach for Dyadic Conversations; Behavior and Outcomes in Couples Therapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tseng16_interspeech.html": {
    "title": "Couples Behavior Modeling and Annotation Using Low-Resource LSTM Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gallardo16_interspeech.html": {
    "title": "Speech Likability and Personality-Based Social Relations: A Round-Robin Analysis over Communication Channels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xiao16_interspeech.html": {
    "title": "Behavioral Coding of Therapist Language in Addiction Counseling Using Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dang16_interspeech.html": {
    "title": "Factor Analysis Based Speaker Normalisation for Continuous Emotion Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ram16_interspeech.html": {
    "title": "Subspace Detection of DNN Posterior Probabilities via Sparse Representation for Query by Example Spoken Term Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16d_interspeech.html": {
    "title": "Unsupervised Bottleneck Features for Low-Resource Query-by-Example Spoken Term Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/torbati16_interspeech.html": {
    "title": "A Nonparametric Bayesian Approach for Spoken Term Detection by Example Query",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pham16_interspeech.html": {
    "title": "Rescoring Hypothesized Detections of Out-of-Vocabulary Keywords Using Subword Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhuang16_interspeech.html": {
    "title": "Unrestricted Vocabulary Keyword Spotting Using LSTM-CTC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wu16b_interspeech.html": {
    "title": "Interactive Spoken Content Retrieval by Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/godoy16_interspeech.html": {
    "title": "Relating Estimated Cyclic Spectral Peak Frequency to Measured Epilarynx Length Using Magnetic Resonance Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tobing16_interspeech.html": {
    "title": "Acoustic-to-Articulatory Inversion Mapping Based on Latent Trajectory Gaussian Mixture Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dissen16_interspeech.html": {
    "title": "Formant Estimation and Tracking Using Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/vaz16_interspeech.html": {
    "title": "Convex Hull Convolutive Non-Negative Matrix Factorization for Uncovering Temporal Patterns in Multivariate Time-Series Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/juvela16_interspeech.html": {
    "title": "Majorisation-Minimisation Based Optimisation of the Composite Autoregressive System with Application to Glottal Inverse Filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16c_interspeech.html": {
    "title": "F0 Contour Analysis Based on Empirical Mode Decomposition for DNN Acoustic Modeling in Mandarin Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hu16_interspeech.html": {
    "title": "Vowels and Diphthongs in Cangnan Southern Min Chinese Dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hu16b_interspeech.html": {
    "title": "Diphthongization of Nuclear Vowels and the Emergence of a Tetraphthong in Hetang Cantonese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cernak16b_interspeech.html": {
    "title": "PhonVoc: A Phonetic and Phonological Vocoding Toolkit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xia16b_interspeech.html": {
    "title": "Vowels and Diphthongs in the Taiyuan Jin Chinese Dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/turco16_interspeech.html": {
    "title": "The Effects of Prosody on French V-to-V Coarticulation: A Corpus-Based Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/galata16_interspeech.html": {
    "title": "An Acoustic Analysis of /r/ in Tyrolean",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chang16_interspeech.html": {
    "title": "Hyperarticulated Production of Korean Glides by Age Group",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pan16_interspeech.html": {
    "title": "Coda Stop and Taiwan Min Checked Tone Sound Changes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fenwick16_interspeech.html": {
    "title": "The Influence of Modality and Speaking Style on the Assimilation Type and Categorization Consistency of Non-Native Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zellers16_interspeech.html": {
    "title": "Prosodic Convergence with Spoken Stimuli in Laboratory Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/themistocleous16_interspeech.html": {
    "title": "Effects of Stress on Fricatives: Evidence from Standard Modern Greek",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sun16b_interspeech.html": {
    "title": "Analysis of Chinese Syllable Durations in Running Speech of Japanese L2 Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lai16b_interspeech.html": {
    "title": "Automatic Paragraph Segmentation with Lexical and Prosodic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/airaksinen16_interspeech.html": {
    "title": "Automatic Glottal Inverse Filtering with Non-Negative Matrix Factorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/park16_interspeech.html": {
    "title": "Speaker Identity and Voice Quality: Modeling Human Responses and Automatic Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kalita16_interspeech.html": {
    "title": "Analysis of Glottal Stop in Assam Sora Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/garellek16_interspeech.html": {
    "title": "Acoustic Differences Between English /t/ Glottalization and Phrasal Creak",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/eriksson16_interspeech.html": {
    "title": "The Acoustics of Lexical Stress in Italian as a Function of Stress Level and Speaking Style",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schweitzer16_interspeech.html": {
    "title": "Cross-Gender and Cross-Dialect Tone Recognition for Vietnamese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/vijayan16_interspeech.html": {
    "title": "Prosody Modification Using Allpass Residual of Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kakouros16_interspeech.html": {
    "title": "Analyzing the Contribution of Top-Down Lexical and Bottom-Up Acoustic Cues in the Detection of Sentence Prominence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kallay16_interspeech.html": {
    "title": "A Longitudinal Study of Children's Intonation in Narrative Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/blaylock16_interspeech.html": {
    "title": "Velum Control for Oral Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/son16_interspeech.html": {
    "title": "F0 Development in Acquiring Korean Stop Distinction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cohen16_interspeech.html": {
    "title": "Phonetic Reduction Can Lead to Lengthening, and Enhancement Can Lead to Shortening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/arai16_interspeech.html": {
    "title": "Mechanical Production of [b], [m] and [w] Using Controlled Labial and Velopharyngeal Gestures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fang16_interspeech.html": {
    "title": "An Improved 3D Geometric Tongue Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tiainen16_interspeech.html": {
    "title": "Congruency Effect Between Articulation and Grasping in Native English Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/najnin16b_interspeech.html": {
    "title": "Emergence of Vocal Developmental Sequences in a Predictive Coding Model of Speech Acquisition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/meyer16_interspeech.html": {
    "title": "Categorization of Natural Spanish Whistled Vowels by Naïve Spanish Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/voigt16_interspeech.html": {
    "title": "Between- and Within-Speaker Effects of Bilingualism on F0 Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/graham16_interspeech.html": {
    "title": "Vowel Characteristics in the Assessment of L2 English Pronunciation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/geneid16_interspeech.html": {
    "title": "Kulning (Swedish Cattle Calls): Acoustic, EGG, Stroboscopic and High-Speed Video Analyses of an Unusual Singing Style",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hejna16_interspeech.html": {
    "title": "Glottal Squeaks in VC Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/takahashi16_interspeech.html": {
    "title": "Automatic Pronunciation Generation by Utilizing a Semi-Supervised Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16f_interspeech.html": {
    "title": "Personalized Natural Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/asri16_interspeech.html": {
    "title": "A Sequence-to-Sequence Model for User Simulation in Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/georgiladakis16_interspeech.html": {
    "title": "Root Cause Analysis of Miscommunication Hotspots in Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/khan16_interspeech.html": {
    "title": "Making Personal Digital Assistants Aware of What They Do Not Know",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/levitan16_interspeech.html": {
    "title": "Implementing Acoustic-Prosodic Entrainment in a Conversational Avatar",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/silvervarg16_interspeech.html": {
    "title": "Perceived Usability and Cognitive Demand of Secondary Tasks in Spoken Versus Visual-Manual Automotive Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fung16_interspeech.html": {
    "title": "Zara: An Empathetic Interactive Virtual Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tejedorgarcia16_interspeech.html": {
    "title": "Measuring Pronunciation Improvement in Users of CAPT Tool TipTopTalk!",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kawahara16_interspeech.html": {
    "title": "SparkNG: Interactive MATLAB Tools for Introduction to Speech Production, Perception and Processing Fundamentals and Application of the Aliasing-Free L-F Model Component",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/marchi16_interspeech.html": {
    "title": "Real-Time Tracking of Speakers' Emotions, States, and Traits on Mobile Platforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mirghafori16_interspeech.html": {
    "title": "Mindfulness Special Event",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chang16b_interspeech.html": {
    "title": "The Human Speech Cortex",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bonastre16_interspeech.html": {
    "title": "Speaker Comparison for Forensic and Investigative Applications II",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bone16_interspeech.html": {
    "title": "Acoustic-Prosodic and Turn-Taking Features in Interactions with Children with Neurodevelopmental Disorders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hemmerling16_interspeech.html": {
    "title": "Automatic Detection of Parkinson's Disease Based on Modulated Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16d_interspeech.html": {
    "title": "Towards Automatic Detection of Amyotrophic Lateral Sclerosis from Speech Acoustic and Articulatory Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ciccarelli16_interspeech.html": {
    "title": "Neurophysiological Vocal Source Modeling for Biomarkers of Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/horwitzmartin16_interspeech.html": {
    "title": "Relation of Automatically Extracted Formant Trajectories with Intelligibility Loss and Speaking Rate Decline in Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ringeval16_interspeech.html": {
    "title": "Automatic Analysis of Typical and Atypical Encoding of Spontaneous Emotion in the Voice of Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/khorram16_interspeech.html": {
    "title": "Recognition of Depression in Bipolar Disorder: Leveraging Cohort and Person-Specific Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mirheidari16_interspeech.html": {
    "title": "Diagnosing People with Dementia Using Automatic Conversation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chan16_interspeech.html": {
    "title": "SERAPHIM: A Wavetable Synthesis System with 3D Lip Animation for Real-Time Speech and Singing Applications on Mobile Platforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bonada16_interspeech.html": {
    "title": "Expressive Singing Synthesis Based on Unit Selection for the Singing Synthesis Challenge 2016",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/perrotin16_interspeech.html": {
    "title": "Vocal Effort Modification for Singing Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/blanco16_interspeech.html": {
    "title": "Bertsokantari: a TTS Based Singing Synthesis System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/feugere16_interspeech.html": {
    "title": "Evaluation of Singing Synthesis: Methodology and Case Study with Concatenative and Performative Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ardaillon16_interspeech.html": {
    "title": "Expressive Control of Singing Voice Synthesis Using Musical Contexts and a Parametric F0 Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cotescu16_interspeech.html": {
    "title": "Optimal Unit Stitching in a Unit Selection Singing Synthesis System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hilton16_interspeech.html": {
    "title": "The Perception of Overlapping Speech: Effects of Speaker Prosody and Listener Attitudes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gravano16_interspeech.html": {
    "title": "Who Do You Think Will Speak Next? Perception of Turn-Taking Cues in Slovak and Argentine Spanish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/perez16_interspeech.html": {
    "title": "Disentrainment may be a Positive Thing: A Novel Measure of Unsigned Acoustic-Prosodic Synchrony, and its Relation to Speaker Engagement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wodarczak16b_interspeech.html": {
    "title": "Respiratory Turn-Taking Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/rennie16_interspeech.html": {
    "title": "The Discourse Marker \"so\" in Turn-Taking and Turn-Releasing Behavior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sherrziarko16_interspeech.html": {
    "title": "Acoustic Properties of Formality in Conversational Japanese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pellegrini16_interspeech.html": {
    "title": "Inferring Phonemic Classes from CNN Activation Maps Using Clustering Techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zeghidour16_interspeech.html": {
    "title": "Joint Learning of Speaker and Phonetic Similarities with Siamese Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mitra16_interspeech.html": {
    "title": "Unsupervised Learning of Acoustic Units Using Autoencoders and Kohonen Nets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhu16b_interspeech.html": {
    "title": "Learning Multiscale Features Directly from Waveforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/heck16_interspeech.html": {
    "title": "Supervised Learning of Acoustic Models in a Zero Resource Setting to Improve DPGMM Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xu16_interspeech.html": {
    "title": "Semi-Supervised and Cross-Lingual Knowledge Transfer Learnings for DNN Hybrid Acoustic Models Under Low-Resource Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/asami16_interspeech.html": {
    "title": "Recurrent Out-of-Vocabulary Word Detection Using Distribution of Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kanda16_interspeech.html": {
    "title": "Investigation of Semi-Supervised Acoustic Model Training Based on the Committee of Heterogeneous Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ghannay16_interspeech.html": {
    "title": "Acoustic Word Embeddings for ASR Error Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/horndasch16_interspeech.html": {
    "title": "Combining Semantic Word Classes and Sub-Word Unit Speech Recognition for Robust OOV Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xie16b_interspeech.html": {
    "title": "Web Data Selection Based on Word Embedding for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/alshareef16_interspeech.html": {
    "title": "Colloquialising Modern Standard Arabic Text for Improved Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kuang16_interspeech.html": {
    "title": "Pitch-Range Perception: The Dynamic Interaction Between Voice Quality and Fundamental Frequency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16e_interspeech.html": {
    "title": "Comparing the Contributions of Amplitude and Phase to Speech Intelligibility in a Vocoder-Based Speech Synthesis Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16f_interspeech.html": {
    "title": "Modeling Noise Influence to Speech Intelligibility Non-Intrusively by Reduced Speech Dynamic Range",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pinter16_interspeech.html": {
    "title": "Do GMM Phoneme Classifiers Perceive Synthetic Sibilants as Humans Do?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/frye16_interspeech.html": {
    "title": "Neural Responses to Speech-Specific Modulations Derived from a Spectro-Temporal Filter Bank",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mulder16_interspeech.html": {
    "title": "Comparing Different Methods for Analyzing ERP Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/eklund16_interspeech.html": {
    "title": "Supplementary Motor Area Activation in Disfluency Perception: An fMRI Study of Listener Neural Responses to Spontaneously Produced Unfilled and Filled Pauses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fogerty16_interspeech.html": {
    "title": "Vowel Fundamental and Formant Frequency Contributions to English and Mandarin Sentence Intelligibility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/huang16b_interspeech.html": {
    "title": "Attention Assisted Discovery of Sub-Utterance Structure in Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16b_interspeech.html": {
    "title": "Combining CNN and BLSTM to Extract Textual and Acoustic Features for Recognizing Stances in Mandarin Ideological Debate Competition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/trouvain16_interspeech.html": {
    "title": "Inter-Speech Clicks in an Interspeech Keynote",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/grzybowska16_interspeech.html": {
    "title": "Speaker Age Classification and Regression Using i-Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16c_interspeech.html": {
    "title": "Sparsely Connected and Disjointly Trained Deep Neural Networks for Low Resource Behavioral Annotation: Acoustic Classification in Couples' Therapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/an16_interspeech.html": {
    "title": "Automatically Classifying Self-Rated Personality Scores from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lehman16_interspeech.html": {
    "title": "Estimation of Children's Physical Characteristics from Their Voices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/akira16_interspeech.html": {
    "title": "Talking to a System and Talking to a Human: A Study from a Speech-to-Speech, Machine Translation Mediated Map Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gupta16b_interspeech.html": {
    "title": "Predicting Affective Dimensions Based on Self Assessed Depression Severity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/huang16c_interspeech.html": {
    "title": "Enhancement of Automatic Oral Presentation Assessment System Using Latent N-Grams Word Representation and Part-of-Speech Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dumpala16b_interspeech.html": {
    "title": "Use of Vowels in Discriminating Speech-Laugh from Laughter and Neutral Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kawabata16_interspeech.html": {
    "title": "A Convex Model for Linguistic Influence in Group Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gibson16_interspeech.html": {
    "title": "A Deep Learning Approach to Modeling Empathy in Addiction Counseling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/huang16d_interspeech.html": {
    "title": "Unipolar Depression vs. Bipolar Disorder: An Elicitation-Based Approach to Short-Term Detection of Mood Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/masmoudi16_interspeech.html": {
    "title": "Conditional Random Fields for the Tunisian Dialect Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/saychum16_interspeech.html": {
    "title": "Efficient Thai Grapheme-to-Phoneme Conversion Using CRF-Based Joint Sequence Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jaumardhakoun16_interspeech.html": {
    "title": "An Articulatory-Based Singing Voice Synthesis Using Tongue and Lips Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16d_interspeech.html": {
    "title": "Phoneme Embedding and its Application to Speech Driven Talking Avatar Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16e_interspeech.html": {
    "title": "Expressive Speech Driven Talking Avatar Synthesis with DBLSTM Using Limited Amount of Emotional Bimodal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/taylor16_interspeech.html": {
    "title": "Audio-to-Visual Speech Conversion Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nakashika16_interspeech.html": {
    "title": "Generative Acoustic-Phonemic-Speaker Model Based on Three-Way Restricted Boltzmann Machine",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/toutios16_interspeech.html": {
    "title": "Articulatory Synthesis Based on Real-Time Magnetic Resonance Imaging Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xie16c_interspeech.html": {
    "title": "Deep Neural Network Based Acoustic-to-Articulatory Inversion Using Phone Sequence Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16g_interspeech.html": {
    "title": "Articulatory-to-Acoustic Conversion with Cascaded Prediction of Spectral and Excitation Features Using Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liberatore16_interspeech.html": {
    "title": "Generating Gestural Scores from Acoustics Through a Sparse Anchor-Based Representation of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/guennec16_interspeech.html": {
    "title": "On the Suitability of Vocalic Sandwiches in a Corpus-Based TTS Engine",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/moungsri16_interspeech.html": {
    "title": "Unsupervised Stress Information Labeling Using Gaussian Process Latent Variable Model for Statistical Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ni16_interspeech.html": {
    "title": "Using Zero-Frequency Resonator to Extract Multilingual Intonation Structure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yu16b_interspeech.html": {
    "title": "A DNN-HMM Approach to Story Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/goldman16_interspeech.html": {
    "title": "The SIWIS Database: A Multilingual Speech Database with Acted Emphasis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ylmaz16b_interspeech.html": {
    "title": "Open Source Speech and Language Resources for Frisian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kathol16_interspeech.html": {
    "title": "The SRI CLEO Speaker-State Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16g_interspeech.html": {
    "title": "SingaKids-Mandarin: Speech Corpus of Singaporean Children Speaking Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/richey16_interspeech.html": {
    "title": "The SRI Speech-Based Collaborative Learning Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ramakrishna16_interspeech.html": {
    "title": "An Expectation Maximization Approach to Joint Modeling of Multidimensional Ratings Derived from Multiple Annotators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/matousek16_interspeech.html": {
    "title": "Voting Detector: A Combination of Anomaly Detectors to Reveal Annotation Errors in TTS Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/corralesastorgano16_interspeech.html": {
    "title": "The Magic Stone: A Video Game to Improve Communication Skills of People with Intellectual Disabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kelly16_interspeech.html": {
    "title": "Identifying Perceptually Similar Voices with a Speaker Recognition System Using Auto-Phonetic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/james16_interspeech.html": {
    "title": "A Real-Time Framework for Visual Feedback of Articulatory Data Using Statistical Shape Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/marin16_interspeech.html": {
    "title": "Flexible, Rapid Authoring of Goal-Orientated, Multi-Turn Dialogues Using the Task Completion Platform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/delcroix16_interspeech.html": {
    "title": "Context Adaptive Neural Network for Rapid Adaptation of Deep CNN Based Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lim16_interspeech.html": {
    "title": "Transfer Learning with Bottleneck Feature Networks for Whispered Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nagamine16b_interspeech.html": {
    "title": "Adaptation of Neural Networks Constrained by Prior Statistics of Node Co-Activations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/suzuki16_interspeech.html": {
    "title": "Domain Adaptation of CNN Based Acoustic Models Under Limited Resource Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/samarakoon16_interspeech.html": {
    "title": "Subspace LHUC for Fast Adaptation of Deep Neural Network Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fainberg16_interspeech.html": {
    "title": "Improving Children's Speech Recognition Through Out-of-Domain Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/metze16_interspeech.html": {
    "title": "Virtual Machines and Containers as a Platform for Experimentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/green16_interspeech.html": {
    "title": "CloudCAST — Remote Speech Technology for Speech Professionals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hain16_interspeech.html": {
    "title": "webASR 2 — Improved Cloud Based Speech Technology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/plummer16_interspeech.html": {
    "title": "Sharing Speech Synthesis Software for Research and Education Within Low-Tech and Low-Resource Communities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sprouse16_interspeech.html": {
    "title": "The Berkeley Phonetics Machine",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bates16_interspeech.html": {
    "title": "Experiences with Shared Resources for Research and Education in Speech and Language Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/toda16_interspeech.html": {
    "title": "The Voice Conversion Challenge 2016",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wester16_interspeech.html": {
    "title": "Analysis of the Voice Conversion Challenge 2016 Evaluation Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16h_interspeech.html": {
    "title": "The USTC System for Voice Conversion Challenge 2016: Neural Network Based Approaches for Spectrum, Aperiodicity and F0 Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mohammadi16_interspeech.html": {
    "title": "A Voice Conversion Mapping Function Based on a Stacked Joint-Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wu16c_interspeech.html": {
    "title": "Locally Linear Embedding for Exemplar-Based Spectral Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/villavicencio16_interspeech.html": {
    "title": "Applying Spectral Normalisation and Efficient Envelope Estimation and Statistical Transformation for the Voice Conversion Challenge 2016",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/erro16_interspeech.html": {
    "title": "ML Parameter Generation with a Reformulated MGE Training Criterion — Participation in the Voice Conversion Challenge 2016",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kobayashi16_interspeech.html": {
    "title": "The NU-NAIST Voice Conversion System for the Voice Conversion Challenge 2016",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/landerportnoy16_interspeech.html": {
    "title": "Release from Energetic Masking Caused by Repeated Patterns of Glimpsing Windows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gibbs16_interspeech.html": {
    "title": "Glimpsing Predictions for Natural and Vocoded Sentence Intelligibility During Modulation Masking: Effect of the Glimpse Cutoff Criterion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xu16b_interspeech.html": {
    "title": "Temporal Envelopes in Sine-Wave Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16h_interspeech.html": {
    "title": "Understanding Periodically Interrupted Mandarin Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16i_interspeech.html": {
    "title": "Factors Affecting the Intelligibility of Sine-Wave Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hodoshima16_interspeech.html": {
    "title": "Effects of Urgent Speech and Preceding Sounds on Speech Intelligibility in Noisy and Reverberant Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sahidullah16_interspeech.html": {
    "title": "Integrated Spoofing Countermeasures and Automatic Speaker Verification: An Evaluation on ASVspoof 2015",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/korshunov16_interspeech.html": {
    "title": "Cross-Database Evaluation of Audio-Based Spoofing Detection Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sriskandaraja16_interspeech.html": {
    "title": "Investigation of Sub-Band Discriminative Information Between Spoofed and Genuine Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tian16_interspeech.html": {
    "title": "An Investigation of Spoofing Speech Detection Under Additive Noise and Reverberant Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sahidullah16b_interspeech.html": {
    "title": "Robust Speaker Recognition with Combined Use of Acoustic and Throat Microphone Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/meng16b_interspeech.html": {
    "title": "Statistical Modeling of Speaker's Voice with Temporal Co-Location for Active Voice Authentication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fischer16_interspeech.html": {
    "title": "Joint Enhancement and Coding of Speech by Incorporating Wiener Filtering in a CELP Codec",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16i_interspeech.html": {
    "title": "Multi-Channel Linear Prediction Based on Binaural Coherence for Speech Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/blass16_interspeech.html": {
    "title": "Single-Channel Speech Enhancement Using Double Spectrum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/drude16_interspeech.html": {
    "title": "On the Appropriateness of Complex-Valued Neural Networks for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zeiler16_interspeech.html": {
    "title": "Introducing the Turbo-Twin-HMM for Audio-Visual Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/spille16_interspeech.html": {
    "title": "Assessing Speech Quality in Speech-Aware Hearing Aids Based on Phoneme Posteriorgrams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gowda16_interspeech.html": {
    "title": "Time-Varying Quasi-Closed-Phase Weighted Linear Prediction Analysis of Speech for Accurate Formant Detection and Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lim16b_interspeech.html": {
    "title": "Improved Depiction of Tissue Boundaries in Vocal Tract Real-Time MRI Using Automatic Off-Resonance Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/blaauw16_interspeech.html": {
    "title": "Modeling and Transforming Speech Using Variational Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/seelamantula16_interspeech.html": {
    "title": "Phase-Encoded Speech Spectrograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/birkholz16_interspeech.html": {
    "title": "Towards Minimally Invasive Velar State Detection in Normal and Silent Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16d_interspeech.html": {
    "title": "RNN-BLSTM Based Multi-Pitch Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/morise16_interspeech.html": {
    "title": "TUSK: A Framework for Overviewing the Performance of F0 Estimators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/rengaswamy16_interspeech.html": {
    "title": "A Robust Non-Parametric and Filtering Based Approach for Glottal Closure Instant Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/saeidi16_interspeech.html": {
    "title": "Analysis of Face Mask Effect on Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/singer16_interspeech.html": {
    "title": "Data Selection for Within-Class Covariance Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ferras16_interspeech.html": {
    "title": "Inter-Task System Fusion for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lei16_interspeech.html": {
    "title": "Mahalanobis Metric Scoring Learned from Weighted Pairwise Constraints in I-Vector Speaker Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/soni16_interspeech.html": {
    "title": "Novel Subband Autoencoder Features for Detection of Spoofed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mclaren16c_interspeech.html": {
    "title": "On the Issue of Calibration in DNN-Based Speaker Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kheder16b_interspeech.html": {
    "title": "Probabilistic Approach Using Joint Long and Short Session i-Vectors Modeling to Deal with Short Utterances for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kanagasundaram16_interspeech.html": {
    "title": "Short Utterance Variance Modelling and Utterance Partitioning for PLDA Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/thomsen16_interspeech.html": {
    "title": "Speaker-Dependent Dictionary-Based Speech Enhancement for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yu16c_interspeech.html": {
    "title": "Text-Available Speaker Recognition System for Forensic Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hong16_interspeech.html": {
    "title": "Transfer Learning for Speaker Verification on Short Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ma16b_interspeech.html": {
    "title": "Twin Model G-PLDA for Duration Mismatch Compensation in Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16e_interspeech.html": {
    "title": "Universal Background Sparse Coding and Multilayer Bootstrap Network for Speaker Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tian16b_interspeech.html": {
    "title": "Improving Deep Neural Networks Based Speaker Verification Using Unlabeled Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kanda16b_interspeech.html": {
    "title": "Maximum a posteriori Based Decoding for CTC Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/asaei16_interspeech.html": {
    "title": "Phonetic and Phonological Posterior Search Space Hashing Exploiting Class-Specific Sparsity Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tucker16_interspeech.html": {
    "title": "Model Compression Applied to Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/martinez16_interspeech.html": {
    "title": "Why do ASR Systems Despite Neural Nets Still Depend on Robust Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/he16b_interspeech.html": {
    "title": "An Adaptive Multi-Band System for Low Power Voice Command Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/price16_interspeech.html": {
    "title": "Memory-Efficient Modeling and Search Techniques for Hardware ASR Decoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yang16c_interspeech.html": {
    "title": "Log-Linear System Combination Using Structured Support Vector Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tang16b_interspeech.html": {
    "title": "Efficient Segmental Cascades for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xu16c_interspeech.html": {
    "title": "A WFST Framework for Single-Pass Multi-Stream Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hartmann16_interspeech.html": {
    "title": "Comparison of Multiple System Combination Techniques for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/obara16_interspeech.html": {
    "title": "Rescoring by Combination of Posteriorgram Score and Subword-Matching Score for Use in Query-by-Example",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16j_interspeech.html": {
    "title": "Phone Synchronous Decoding with CTC Lattice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sahu16_interspeech.html": {
    "title": "Speech Features for Depression Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ariasvergara16_interspeech.html": {
    "title": "Parkinson's Disease Progression Assessment from Speech Using GMM-UBM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/weiner16_interspeech.html": {
    "title": "Speech-Based Detection of Alzheimer's Disease in Conversational German",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/alghowinem16_interspeech.html": {
    "title": "Cross-Cultural Depression Recognition from Vocal Biomarkers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhou16_interspeech.html": {
    "title": "Speech Recognition in Alzheimer's Disease and in its Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pokorny16_interspeech.html": {
    "title": "Does She Speak RTT? Towards an Earlier Identification of Rett Syndrome Through Intelligent Pre-Linguistic Vocalisation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pettorino16_interspeech.html": {
    "title": "Speech Rhythm in Parkinson's Disease: A Study on Italian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/anguera16_interspeech.html": {
    "title": "English Language Speech Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/guo16_interspeech.html": {
    "title": "Remeeting — Deep Insights to Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chan16b_interspeech.html": {
    "title": "SERAPHIM Live! — Singing Synthesis for the Performer, the Composer, and the 3D Game Developer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/malfrere16_interspeech.html": {
    "title": "My-Own-Voice: A Web Service That Allows You to Create a Text-to-Speech Voice From Your Own Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fernald16_interspeech.html": {
    "title": "Talking with Kids Really Matters: Early Language Experience Shapes Later Life Chances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sainath16b_interspeech.html": {
    "title": "Reducing the Computational Complexity of Multimicrophone Acoustic Models with Integrated Feature Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16f_interspeech.html": {
    "title": "Neural Network Adaptive Beamforming for Robust Multichannel Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/erdogan16_interspeech.html": {
    "title": "Improved MVDR Beamforming Using Single-Channel Mask Prediction Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/guerrero16_interspeech.html": {
    "title": "Channel Selection for Distant Speech Recognition Exploiting Cepstral Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mandel16b_interspeech.html": {
    "title": "Multichannel Spatial Clustering for Robust Far-Field Automatic Speech Recognition in Mismatched Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/peddinti16_interspeech.html": {
    "title": "Far-Field ASR Without Parallel Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schuller16_interspeech.html": {
    "title": "The INTERSPEECH 2016 Computational Paralinguistics Challenge: Deception, Sincerity & Native Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schuller16b_interspeech.html": {
    "title": "The Deception Sub-Challenge: The Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/levitan16b_interspeech.html": {
    "title": "Combining Acoustic-Prosodic, Lexical, and Phonotactic Features for Automatic Deception Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/amiriparian16_interspeech.html": {
    "title": "Is Deception Emotional? An Emotion-Driven Predictive Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/montacie16_interspeech.html": {
    "title": "Prosodic Cues and Answer Type Detection for the Deception Sub-Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schuller16c_interspeech.html": {
    "title": "The Sincerity Sub-Challenge: The Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/booth16_interspeech.html": {
    "title": "Automatic Estimation of Perceived Sincerity from Spoken Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gosztolya16b_interspeech.html": {
    "title": "Estimating the Sincerity of Apologies in Speech by DNN Rank Learning and Prosodic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lee16c_interspeech.html": {
    "title": "Minimization of Regression and Ranking Losses with Shallow Neural Networks on Automatic Sincerity Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/herms16_interspeech.html": {
    "title": "Prediction of Deception and Sincerity from Speech Using Automatic Phone Recognition-Based Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16f_interspeech.html": {
    "title": "Sincerity and Deception in Speech: Two Sides of the Same Coin? A Transfer- and Multi-Task Learning Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kaya16_interspeech.html": {
    "title": "Fusing Acoustic Feature Representations for Computational Paralinguistics Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/harte16_interspeech.html": {
    "title": "Introduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/harte16b_interspeech.html": {
    "title": "Poster Overview Presentations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/harte16c_interspeech.html": {
    "title": "Discussion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/harte16d_interspeech.html": {
    "title": "Closing Remarks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/barlier16_interspeech.html": {
    "title": "A Stochastic Model for Computer-Aided Human-Human Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lejeune16_interspeech.html": {
    "title": "Highlighting Psychological Features for Predicting Child Interjections During Story Telling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sun16c_interspeech.html": {
    "title": "Hybrid Dialogue State Tracking for Real World Human-to-Human Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fotedar16_interspeech.html": {
    "title": "Automatic Recognition of Social Roles Using Long Term Role Transitions in Small Group Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/eecke16_interspeech.html": {
    "title": "On the Influence of Gender on Interruptions in Multiparty Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/beaver16_interspeech.html": {
    "title": "Detection of User Escalation in Human-Computer Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/barnaud16_interspeech.html": {
    "title": "Assessing Idiosyncrasies in a Bayesian Model of Speech Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wolters16_interspeech.html": {
    "title": "Prosodic and Linguistic Analysis of Semantic Fluency Data: A Window into Speech Production and Cognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/katz16_interspeech.html": {
    "title": "Sensorimotor Response to Visual Imagery of Tongue Displacement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/caudrelier16_interspeech.html": {
    "title": "Does Auditory-Motor Learning of Speech Transfer from the CV Syllable to the CVCV Word?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schweitzer16b_interspeech.html": {
    "title": "Exemplar Dynamics in Phonetic Convergence of Speech Rate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tuomainen16_interspeech.html": {
    "title": "Articulation Rate in Adverse Listening Conditions in Younger and Older Adults",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/olcoz16_interspeech.html": {
    "title": "Error Correction in Lightly Supervised Alignment of Broadcast Subtitles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/doulaty16_interspeech.html": {
    "title": "Automatic Genre and Show Identification of Broadcast Media",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chao16_interspeech.html": {
    "title": "Speaker-Targeted Audio-Visual Models for Speech Recognition in Cocktail-Party Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/aides16_interspeech.html": {
    "title": "Text-Dependent Audiovisual Synchrony Detection for Spoofing Detection in Mobile Person Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tao16b_interspeech.html": {
    "title": "Improving Boundary Estimation in Audiovisual Speech Activity Detection Using Bayesian Information Criterion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gergen16_interspeech.html": {
    "title": "Dynamic Stream Weighting for Turbo-Decoding-Based Audiovisual ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kruspe16_interspeech.html": {
    "title": "Retrieval of Textual Song Lyrics from Sung Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yuan16b_interspeech.html": {
    "title": "Phoneme, Phone Boundary, and Tone in Automatic Scoring of Mandarin Proficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16k_interspeech.html": {
    "title": "Tone Classification in Mandarin Chinese Using Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pannala16_interspeech.html": {
    "title": "Robust Estimation of Fundamental Frequency Using Single Frequency Filtering Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/daido16_interspeech.html": {
    "title": "A Fast and Accurate Fundamental Frequency Estimator Using Recursive Moving Average Filters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/verma16_interspeech.html": {
    "title": "Frequency Estimation from Waveforms Using Multi-Layered Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sturim16_interspeech.html": {
    "title": "Speaker Linking and Applications Using Non-Parametric Hashing Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lan16_interspeech.html": {
    "title": "Iterative PLDA Adaptation for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dubey16_interspeech.html": {
    "title": "A Speaker Diarization System for Studying Peer-Led Team Learning Groups",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/milner16_interspeech.html": {
    "title": "DNN-Based Speaker Clustering for Speaker Diarisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lapidot16_interspeech.html": {
    "title": "On the Importance of Efficient Transition Modeling for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sell16_interspeech.html": {
    "title": "Priors for Speaker Counting and Diarization with AHC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dawalatabad16_interspeech.html": {
    "title": "Two-Pass IB Based Speaker Diarization System Using Meeting-Specific ANN Based Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/oo16_interspeech.html": {
    "title": "DNN-Based Amplitude and Phase Feature Enhancement for Noise Robust Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/scherhag16_interspeech.html": {
    "title": "Unit-Selection Attack Detection Based on Unfiltered Frequency-Domain Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/monteserin16_interspeech.html": {
    "title": "Investigating the Impact of Dialect Prestige on Lexical Decision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/guo16b_interspeech.html": {
    "title": "Speaker Verification Using Short Utterances with DNN-Based Estimation of Subglottal Acoustic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/su16_interspeech.html": {
    "title": "Factor Analysis Based Speaker Verification Using ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zegers16_interspeech.html": {
    "title": "Joint Sound Source Separation and Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kumar16_interspeech.html": {
    "title": "Robust Multichannel Gender Classification from Speech in Movie Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gonzalvo16_interspeech.html": {
    "title": "Recent Advances in Google Real-Time HMM-Driven Unit Selection Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16e_interspeech.html": {
    "title": "First Step Towards End-to-End Parametric TTS Synthesis: Generating Spectral Parameters with Neural Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wen16b_interspeech.html": {
    "title": "The Parameterized Phoneme Identity Feature as a Continuous Real-Valued Vector for Neural Network Based Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/song16_interspeech.html": {
    "title": "Improved Time-Frequency Trajectory Excitation Vocoder for DNN-Based Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ohtani16_interspeech.html": {
    "title": "Voice Quality Control Using Perceptual Expressions for Statistical Parametric Speech Synthesis Based on Cluster Adaptive Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/espic16_interspeech.html": {
    "title": "Waveform Generation Based on Signal Reshaping for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhao16b_interspeech.html": {
    "title": "Speaker Representations for Speaker Adaptation in Multiple Speakers' BLSTM-RNN-Based Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zen16_interspeech.html": {
    "title": "Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hojo16_interspeech.html": {
    "title": "An Investigation of DNN-Based Speech Synthesis Using Speaker Codes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/juvela16b_interspeech.html": {
    "title": "Using Text and Acoustic Features in Predicting Glottal Excitation Waveforms for Parametric Speech Synthesis with Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tachibana16_interspeech.html": {
    "title": "Model Integration for HMM- and DNN-Based Speech Synthesis Using Product-of-Experts Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/potard16_interspeech.html": {
    "title": "Idlak Tangle: An Open Source Kaldi Based Parametric Speech Synthesiser Based on DNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lazaridis16_interspeech.html": {
    "title": "Probabilistic Amplitude Demodulation Features in Speech Synthesis for Improving Prosody",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chiang16_interspeech.html": {
    "title": "On Smoothing and Enhancing Dynamics of Pitch Contours Represented by Discrete Orthogonal Polynomials for Prosody Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/vadapalli16_interspeech.html": {
    "title": "An Investigation of Recurrent Neural Network Architectures Using Word Embeddings for Phrase Break Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16j_interspeech.html": {
    "title": "Model-Based Parametric Prosody Synthesis with Deep Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/drugman16_interspeech.html": {
    "title": "Active and Semi-Supervised Learning in ASR: Benefits on the Acoustic and Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kuznetsov16_interspeech.html": {
    "title": "Learning N-Gram Language Models from Uncertain Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/oguz16_interspeech.html": {
    "title": "Entropy Based Pruning for Non-Negative Matrix Based Language Models with Contextual Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gangireddy16_interspeech.html": {
    "title": "Unsupervised Adaptation of Recurrent Neural Network Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/halpern16_interspeech.html": {
    "title": "Contextual Prediction Models for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/deena16_interspeech.html": {
    "title": "Combining Feature and Model-Based Adaptation of RNNLMs for Multi-Genre Broadcast Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/brady16_interspeech.html": {
    "title": "A Low Cost Desktop Robot and Tele-Presence Device for Interactive Speech Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/stone16_interspeech.html": {
    "title": "Silent-Speech Command Word Recognition Using Electro-Optical Stomatography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/stanislav16b_interspeech.html": {
    "title": "An Engine for Online Video Search in Large Archives of the Holocaust Testimonies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zmolikova16_interspeech.html": {
    "title": "Data Selection by Sequence Summarizing Neural Network in Mismatch Condition Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kundu16_interspeech.html": {
    "title": "Incorporating a Generative Front-End Layer to Deep Neural Network for Noise Robust Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/markov16_interspeech.html": {
    "title": "Robust Speech Recognition Using Generalized Distillation Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shinohara16b_interspeech.html": {
    "title": "Adversarial Multi-Task Learning of Deep Neural Networks for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/poblete16_interspeech.html": {
    "title": "The Use of Locally Normalized Cepstral Coefficients (LNCC) to Improve Speaker Recognition Accuracy in Highly Reverberant Rooms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hartmann16b_interspeech.html": {
    "title": "Two-Stage Data Augmentation for Low-Resourced Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schuller16d_interspeech.html": {
    "title": "The Native Language Sub-Challenge: The Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/rajpal16_interspeech.html": {
    "title": "Native Language Identification Using Spectral and Source-Based Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jiao16_interspeech.html": {
    "title": "Accent Identification by Combining Deep Neural Networks and Recurrent Neural Networks Trained on Long and Short Term Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/keren16_interspeech.html": {
    "title": "Convolutional Neural Networks with Data Augmentation for Classifying Speakers' Native Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/senoussaoui16_interspeech.html": {
    "title": "Native Language Detection Using the I-Vector Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/huckvale16_interspeech.html": {
    "title": "Within-Speaker Features for Native Language Recognition in the Interspeech 2016 Computational Paralinguistics Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shivakumar16_interspeech.html": {
    "title": "Multimodal Fusion of Multirate Acoustic, Prosodic, and Lexical Speaker Characteristics for Native Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/abad16_interspeech.html": {
    "title": "Exploiting Phone Log-Likelihood Ratio Features for the Detection of the Native Language of Non-Native English Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gosztolya16c_interspeech.html": {
    "title": "Determining Native Language and Deception Using Phonetic Features and Classifier Combination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schuller16e_interspeech.html": {
    "title": "The INTERSPEECH 2016 Computational Paralinguistics Challenge: A Summary of Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schuller16f_interspeech.html": {
    "title": "Discussion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tabain16_interspeech.html": {
    "title": "A Preliminary Ultrasound Study of Nasal and Lateral Coronals in Arrernte",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/toutios16b_interspeech.html": {
    "title": "Illustrating the Production of the International Phonetic Alphabet Sounds Using Fast Real-Time Magnetic Resonance Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/renwick16_interspeech.html": {
    "title": "Marginal Contrast Among Romanian Vowels: Evidence from ASR and Functional Load",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fan16_interspeech.html": {
    "title": "Effects of Subglottal-Coupling and Interdental-Space on Formant Trajectories During Front-to-Back Vowel Transitions in Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/monteserin16b_interspeech.html": {
    "title": "Perceptual Lateralization of Coda Rhotic Production in Puerto Rican Spanish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yi16_interspeech.html": {
    "title": "Interaction Between Lexical Tone and Intonation: An EMA Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ming16_interspeech.html": {
    "title": "Deep Bidirectional LSTM Modeling of Timbre and Prosody for Emotional Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/thangthai16_interspeech.html": {
    "title": "Visual Speech Synthesis Using Dynamic Visemes, Contextual Features and DNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ronanki16_interspeech.html": {
    "title": "A Template-Based Approach for Speech Synthesis Intonation Generation Using LSTMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16g_interspeech.html": {
    "title": "Multi-Language Multi-Speaker Acoustic Modeling for LSTM-RNN Based Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/airaksinen16b_interspeech.html": {
    "title": "GlottDNN — A Full-Band Glottal Vocoder for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nishimura16_interspeech.html": {
    "title": "Singing Voice Synthesis Based on Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/backstrom16_interspeech.html": {
    "title": "Blind Recovery of Perceptual Models in Distributed Speech and Audio Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tang16c_interspeech.html": {
    "title": "Glimpse-Based Metrics for Predicting Speech Intelligibility in Additive Noise Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/koster16_interspeech.html": {
    "title": "Analyzing the Relation Between Overall Quality and the Quality of Individual Phases in a Telephone Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jokinen16_interspeech.html": {
    "title": "Intelligibility Enhancement at the Receiving End of the Speech Transmission System — Effects of Far-End Noise Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ganzeboom16_interspeech.html": {
    "title": "Intelligibility of Disordered Speech: Global and Detailed Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/koutsogiannaki16_interspeech.html": {
    "title": "Modulation Enhancement of Temporal Envelopes for Increasing Speech Intelligibility in Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/niehues16_interspeech.html": {
    "title": "Dynamic Transcription for Low-Latency Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/adams16_interspeech.html": {
    "title": "Learning a Translation Model from Word Lattices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zayats16_interspeech.html": {
    "title": "Disfluency Detection Using a Bidirectional LSTM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/che16_interspeech.html": {
    "title": "Sentence Boundary Detection Based on Parallel Lexical and Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/do16_interspeech.html": {
    "title": "Transferring Emphasis in Speech Translation Using Hard-Attentional Neural Network Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/le16_interspeech.html": {
    "title": "Better Evaluation of ASR in Speech Translation Context Using Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/korse16_interspeech.html": {
    "title": "Entropy Coding of Spectral Envelopes for Speech and Audio Coding Using Distribution Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/villette16_interspeech.html": {
    "title": "An Objective Evaluation Methodology for Blind Bandwidth Extension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ramo16_interspeech.html": {
    "title": "EVS Channel Aware Mode Robustness to Frame Erasures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pirhosseinloo16_interspeech.html": {
    "title": "An Interaural Magnification Algorithm for Enhancement of Naturally-Occurring Level Differences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kayser16_interspeech.html": {
    "title": "Probabilistic Spatial Filter Estimation for Signal Enhancement in Multi-Channel Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ji16_interspeech.html": {
    "title": "Improved a priori SAP Estimator in Complex Noisy Environment for Dual Channel Microphone System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cheong16_interspeech.html": {
    "title": "A Spectral Modulation Sensitivity Weighted Pre-Emphasis Filter for Active Noise Control System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sreeram16_interspeech.html": {
    "title": "Semi-Coupled Dictionary Based Automatic Bandwidth Extension Approach for Enhancing Children's ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bonada16b_interspeech.html": {
    "title": "Bird Song Synthesis Based on Hidden Markov Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kaewtip16_interspeech.html": {
    "title": "Noise-Robust Hidden Markov Models for Limited Training Data for Within-Species Bird Phrase Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wisler16_interspeech.html": {
    "title": "A Framework for Automated Marmoset Vocalization Detection and Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/aihara16b_interspeech.html": {
    "title": "Call Alternation Between Specific Pairs of Male Frogs Revealed by a Sound-Imaging Method in Their Natural Habitat",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/guyot16_interspeech.html": {
    "title": "Sinusoidal Modelling for Ecoacoustics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/stowell16_interspeech.html": {
    "title": "Individual Identity in Songbirds: Signal Representations and Metric Learning for Locating the Information in Complex Corvid Calls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jancovic16_interspeech.html": {
    "title": "Recognition of Multiple Bird Species Based on Penalised Maximum Likelihood and HMM-Based Modelling of Individual Vocalisation Elements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/maina16_interspeech.html": {
    "title": "Cost Effective Acoustic Monitoring of Bird Species",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kohlsdorf16_interspeech.html": {
    "title": "Feature Learning and Automatic Segmentation for Dolphin Communication Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/suzuki16b_interspeech.html": {
    "title": "Localizing Bird Songs Using an Open Source Robot Audition System with a Microphone Array",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kurth16_interspeech.html": {
    "title": "Robust Detection of Multiple Bioacoustic Events with Repetitive Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/moore16_interspeech.html": {
    "title": "A Real-Time Parametric General-Purpose Mammalian Vocal Synthesiser",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/oreilly16_interspeech.html": {
    "title": "YIN-Bird: Improved Pitch Tracking for Bird Vocalisations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hsu16c_interspeech.html": {
    "title": "Mispronunciation Detection Leveraging Maximum Performance Criterion Training of Acoustic Models and Decision Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/heeman16_interspeech.html": {
    "title": "Using Clinician Annotations to Improve Automatic Speech Recognition of Stuttered Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xie16d_interspeech.html": {
    "title": "Deep Neural Networks for Voice Quality Assessment Based on the GRBAS Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ward16_interspeech.html": {
    "title": "Automated Screening of Speech Development Issues in Children by Identifying Phonological Error Patterns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lin16_interspeech.html": {
    "title": "Automatic Pronunciation Evaluation of Non-Native Mandarin Tone by Using Multi-Level Confidence Measures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kim16c_interspeech.html": {
    "title": "Dysarthric Speech Recognition Using Kullback-Leibler Divergence-Based Hidden Markov Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/warlaumont16_interspeech.html": {
    "title": "Detection of Total Syllables and Canonical Syllables in Infant Vocalizations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/le16b_interspeech.html": {
    "title": "Improving Automatic Recognition of Aphasic Speech with AphasiaBank",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/laborde16_interspeech.html": {
    "title": "Pronunciation Assessment of Japanese Learners of French with GOP Scores and Phonetic Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/robertson16_interspeech.html": {
    "title": "Pronunciation Error Detection for New Language Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ding16_interspeech.html": {
    "title": "L2 English Rhythm in Read Speech by Chinese Students",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16h_interspeech.html": {
    "title": "Improving the Probabilistic Framework for Representing Dialogue Systems with User Response Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/song16b_interspeech.html": {
    "title": "Dialogue Session Segmentation by Embedding-Enhanced TextTiling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16i_interspeech.html": {
    "title": "Target-Based State and Tracking Algorithm for Spoken Dialogue System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shen16_interspeech.html": {
    "title": "Neural Attention Models for Sequence Classification: Analysis and Application to Key Term Extraction and Dialogue Act Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kumar16b_interspeech.html": {
    "title": "Objective Language Feature Analysis in Children with Neurodevelopmental Disorders During Autism Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/casanueva16_interspeech.html": {
    "title": "Improving Generalisation to New Speakers in Spoken Dialogue State Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tseng16b_interspeech.html": {
    "title": "Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening Comprehension Test by Machine",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ravuri16_interspeech.html": {
    "title": "How Neural Network Depth Compensates for HMM Conditional Independence Assumptions in DNN-HMM Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/palaz16_interspeech.html": {
    "title": "Jointly Learning to Locate and Classify Words Using Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/alvarez16_interspeech.html": {
    "title": "On the Efficient Representation and Execution of Deep Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/povey16_interspeech.html": {
    "title": "Purely Sequence-Trained Neural Networks for ASR Based on Lattice-Free MMI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ratajczak16_interspeech.html": {
    "title": "Virtual Adversarial Training Applied to Neural Higher-Order Factors for Phone Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wong16_interspeech.html": {
    "title": "Sequence Student-Teacher Training of Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hansen16_interspeech.html": {
    "title": "Robustness in Speech, Speaker, and Language Recognition: \"You've Got to Know Your Limitations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jokinen16b_interspeech.html": {
    "title": "The Use of Read versus Conversational Lombard Speech in Spectral Tilt Modeling for Intelligibility Enhancement in Near-End Noise Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sturim16b_interspeech.html": {
    "title": "Corpora for the Evaluation of Robust Speaker Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bertin16_interspeech.html": {
    "title": "A French Corpus for Distant-Microphone Speech Processing in Real Homes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ravanelli16_interspeech.html": {
    "title": "Realistic Multi-Microphone Data Simulation for Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gamper16_interspeech.html": {
    "title": "Synthesis of Device-Independent Noise Corpora for Realistic ASR Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/richardson16_interspeech.html": {
    "title": "Speaker Recognition Using Real vs Synthetic Parallel Data for DNN Channel Compensation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ribas16_interspeech.html": {
    "title": "Discussion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bosch16_interspeech.html": {
    "title": "Combining Data-Oriented and Process-Oriented Approaches to Modeling Reaction Time Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mcauliffe16_interspeech.html": {
    "title": "Do Listeners Learn Better from Natural Speech?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/drozdova16_interspeech.html": {
    "title": "Processing and Adaptation to Ambiguous Sounds during the Course of Perceptual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hintz16_interspeech.html": {
    "title": "The Effect of Background Noise on the Activation of Phonological and Semantic Information During Spoken-Word Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kang16_interspeech.html": {
    "title": "Relationships Between Functional Load and Auditory Confusability Under Different Speech Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kanwal16_interspeech.html": {
    "title": "The Role of Pitch in Punjabi Word Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tahon16_interspeech.html": {
    "title": "Improving TTS with Corpus-Specific Pronunciation Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mousa16_interspeech.html": {
    "title": "Deep Bidirectional Long Short-Term Memory Recurrent Neural Networks for Grapheme-to-Phoneme Conversion Utilizing Complex Many-to-Many Alignments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/esch16_interspeech.html": {
    "title": "Predicting Pronunciations with Syllabification and Stress with Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pouget16_interspeech.html": {
    "title": "Adaptive Latency for Part-of-Speech Tagging in Incremental Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dall16_interspeech.html": {
    "title": "Redefining the Linguistic Context Feature Set for HMM and DNN TTS Through Position and Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16f_interspeech.html": {
    "title": "Enhance the Word Vector with Prosodic Information for the Recurrent Neural Network Based TTS System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jeon16_interspeech.html": {
    "title": "Local Sparsity Based Online Dictionary Learning for Environment-Adaptive Speech Enhancement with Nonnegative Matrix Factorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/papadopoulos16_interspeech.html": {
    "title": "Noise Aware and Combined Noise Models for Speech Denoising in Unknown Noise Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mirsamadi16_interspeech.html": {
    "title": "Causal Speech Enhancement Combining Data-Driven Learning and Suppression Rule Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/brutti16_interspeech.html": {
    "title": "A Phase-Based Time-Frequency Masking for Multi-Channel Speech Enhancement in Domestic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/petkov16b_interspeech.html": {
    "title": "Generalizing Steady State Suppression for Enhanced Intelligibility Under Reverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yamamoto16_interspeech.html": {
    "title": "Speech Intelligibility Prediction Based on the Envelope Power Spectrum Model with the Dynamic Compressive Gammachirp Auditory Filterbank",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kawahara16b_interspeech.html": {
    "title": "Prediction and Generation of Backchannel Form for Attentive Listening Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lunsford16_interspeech.html": {
    "title": "Measuring Turn-Taking Offsets in Human-Human Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/meshorer16_interspeech.html": {
    "title": "Using Past Speaker Behavior to Better Predict Turn Transitions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bailly16_interspeech.html": {
    "title": "Quantitative Analysis of Backchannels Uttered by an Interviewer During Neuropsychological Tests",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chowdhury16_interspeech.html": {
    "title": "Predicting User Satisfaction from Turn-Taking in Spoken Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/oertel16_interspeech.html": {
    "title": "Towards Building an Attentive Artificial Listener: On the Perception of Attentiveness in Feedback Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gwon16_interspeech.html": {
    "title": "Language Recognition via Sparse Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fernando16_interspeech.html": {
    "title": "A Feature Normalisation Technique for PLLR Based Language Identification Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kv16_interspeech.html": {
    "title": "An Investigation of Deep Neural Network Architectures for Language Recognition in Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ali16_interspeech.html": {
    "title": "Automatic Dialect Detection in Arabic Broadcast Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ng16_interspeech.html": {
    "title": "Combining Weak Tokenisers for Phonotactic Language Recognition in a Resource-Constrained Setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/geng16_interspeech.html": {
    "title": "End-to-End Language Identification Using Attention-Based Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sagha16_interspeech.html": {
    "title": "Enhancing Multilingual Recognition of Emotion in Speech by Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mun16_interspeech.html": {
    "title": "Deep Neural Network Bottleneck Features for Acoustic Event Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/origlia16_interspeech.html": {
    "title": "Combining Energy and Cross-Entropy Analysis for Nuclear Segments Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/maas16_interspeech.html": {
    "title": "Anchored Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nandwana16_interspeech.html": {
    "title": "Towards Smart-Cars That Can Listen: Abnormal Acoustic Event Detection on the Road",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/girish16_interspeech.html": {
    "title": "Hierarchical Classification of Speaker and Background Noise and Estimation of SNR Using Sparse Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16g_interspeech.html": {
    "title": "Robust Sound Event Detection in Continuous Audio Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/takahashi16b_interspeech.html": {
    "title": "Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/meier16_interspeech.html": {
    "title": "Artificial Neural Network-Based Feature Combination for Spatial Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kinnunen16b_interspeech.html": {
    "title": "HAPPY Team Entry to NIST OpenSAD Challenge: A Fusion of Short-Term Unsupervised and Segment i-Vector Based Speech Activity Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pokorny16b_interspeech.html": {
    "title": "Manual versus Automated: The Challenging Routine of Infant Vocalisation Segmentation in Home Videos to Study Neuro(mal)development",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ferrer16_interspeech.html": {
    "title": "Minimizing Annotation Effort for Adaptation of Speech-Activity Detection Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/moore16b_interspeech.html": {
    "title": "Progress and Prospects for Spoken Language Technology: What Ordinary People Think",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/moore16c_interspeech.html": {
    "title": "Progress and Prospects for Spoken Language Technology: Results from Four Sexennial Surveys",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/radadia16_interspeech.html": {
    "title": "On Employing a Highly Mismatched Crowd for Speech Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hsiao16_interspeech.html": {
    "title": "Sage: The New BBN Speech Processing Platform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lee16d_interspeech.html": {
    "title": "DNN-Based Feature Enhancement Using Joint Training Framework for Robust Multichannel Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wand16_interspeech.html": {
    "title": "Deep Neural Network Frontend for Continuous EMG-Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/abraham16b_interspeech.html": {
    "title": "Overcoming Data Sparsity in Acoustic Modeling of Low-Resource Language by Borrowing Data and Model Parameters from High-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ragni16_interspeech.html": {
    "title": "Multi-Language Neural Network Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tilk16_interspeech.html": {
    "title": "Bidirectional Recurrent Neural Network with Attention Mechanism for Punctuation Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/enarvi16_interspeech.html": {
    "title": "TheanoLM — An Extensible Toolkit for Neural Network Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lanchantin16_interspeech.html": {
    "title": "Selection of Multi-Genre Broadcast Data for the Training of Automatic Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gaur16_interspeech.html": {
    "title": "Manipulating Word Lattices to Incorporate Human Corrections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fischer16b_interspeech.html": {
    "title": "Context-Aware Restaurant Recommendation for Natural Language Queries: A Formative User Study in the Automotive Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pancoast16_interspeech.html": {
    "title": "Teaming Up: Making the Most of Diverse Representations for a Novel Personalized Speech Retrieval Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mitra16b_interspeech.html": {
    "title": "Automatic Speech Transcription for Low-Resource Languages — The Case of Yoloxóchitl Mixtec (Mexico)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/asadi16_interspeech.html": {
    "title": "Real-Time Presentation Tracking Using Semantic Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wilkinson16_interspeech.html": {
    "title": "Deriving Phonetic Transcriptions and Discovering Word Segmentations for Speech-to-Speech Translation in Low-Resource Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tsujioka16_interspeech.html": {
    "title": "Unsupervised Joint Estimation of Grapheme-to-Phoneme Conversion Systems and Acoustic Model Adaptation for Non-Native Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bruguier16_interspeech.html": {
    "title": "Learning Personalized Pronunciations for Contact Name Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ge16_interspeech.html": {
    "title": "Generation and Pruning of Pronunciation Variants to Improve ASR Accuracy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pylkkonen16_interspeech.html": {
    "title": "Optimizing Speech Recognition Evaluation Using Stratified Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jurafsky16_interspeech.html": {
    "title": "Ketchup, Interdisciplinarity, and the Spread of Innovation in Speech and Language Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/scheffer16_interspeech.html": {
    "title": "Speech Ventures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tong16_interspeech.html": {
    "title": "Context Aware Mispronunciation Detection for Mandarin Pronunciation Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tao16c_interspeech.html": {
    "title": "DNN Online with iVectors Acoustic Modeling and Doc2Vec Distributed Representations for Improving Automated Speech Scoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/qian16_interspeech.html": {
    "title": "Self-Adaptive DNN for Improving Spoken Language Proficiency Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16j_interspeech.html": {
    "title": "Detecting Mispronunciations of L2 Learners and Providing Corrective Feedback Using Knowledge-Guided and Data-Driven Decision Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16g_interspeech.html": {
    "title": "Phoneme Set Design Considering Integrated Acoustic and Linguistic Features of Second Language Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/rasipuram16_interspeech.html": {
    "title": "HMM-Based Non-Native Accent Assessment Using Posterior Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shi16_interspeech.html": {
    "title": "Automatic Assessment and Error Detection of Shadowing Speech: Case of English Spoken by Japanese Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hejna16b_interspeech.html": {
    "title": "Multiplicity of the Acoustic Correlates of the Fortis-Lenis Contrast: Plosives in Aberystwyth English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/adi16_interspeech.html": {
    "title": "Automatic Measurement of Voice Onset Time and Prevoicing Using Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ghosh16_interspeech.html": {
    "title": "L1-L2 Interference: The Case of Final Devoicing of French Voiced Fricatives in Final Position by German Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yanushevskaya16_interspeech.html": {
    "title": "Perceptual Salience of Voice Source Parameters in Signaling Focal Prominence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/borsky16_interspeech.html": {
    "title": "Classification of Voice Modality Using Electroglottogram Waveforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/maekawa16_interspeech.html": {
    "title": "Voice-Quality Difference Between the Vowels in Filled Pauses and Ordinary Lexical Items",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16l_interspeech.html": {
    "title": "Generation of Emotion Control Vector Using MDS-Based Space Transformation for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jauk16_interspeech.html": {
    "title": "Direct Expressive Voice Training Based on Semantic Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ribeiro16_interspeech.html": {
    "title": "Syllable-Level Representations of Suprasegmental Features for DNN-Based Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/braunschweiler16_interspeech.html": {
    "title": "Pause Prediction from Text for Speech Synthesis with User-Definable Pause Insertion Likelihood Threshold",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/do16b_interspeech.html": {
    "title": "A Hybrid System for Continuous Word-Level Emphasis Modeling Based on HMM State Clustering and Adaptive Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zheng16_interspeech.html": {
    "title": "Improving Prosodic Boundaries Prediction for Mandarin Speech Synthesis by Using Enhanced Embedding Feature and Model Fusion Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhao16c_interspeech.html": {
    "title": "Results of The 2015 NIST Language Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lee16e_interspeech.html": {
    "title": "The 2015 NIST Language Recognition Evaluation: The Shared View of I2R, Fantastic4 and SingaMS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lu16c_interspeech.html": {
    "title": "Pair-Wise Distance Metric Learning of Neural Network Model for Spoken Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/travadi16_interspeech.html": {
    "title": "Non-Iterative Parameter Estimation for Total Variability Model Using Randomized Singular Value Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/garciaromero16_interspeech.html": {
    "title": "Stacked Long-Term TDNN for Spoken Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gelly16_interspeech.html": {
    "title": "A Divide-and-Conquer Approach for Language Identification Based on Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hori16_interspeech.html": {
    "title": "Context-Sensitive and Role-Dependent Spoken Language Understanding Using Bidirectional and Attention LSTMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/vukotic16_interspeech.html": {
    "title": "A Step Beyond Local Observations with a Dialog Aware Bidirectional GRU Network for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16m_interspeech.html": {
    "title": "End-to-End Memory Networks with Knowledge Carryover for Multi-Turn Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/vu16_interspeech.html": {
    "title": "Sequential Convolutional Neural Networks for Slot Filling in Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/celikyilmaz16_interspeech.html": {
    "title": "A New Pre-Training Method for Training Deep Learning Models with Application to Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tafforeau16_interspeech.html": {
    "title": "Joint Syntactic and Semantic Analysis with a Multitask Deep Learning Framework for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16k_interspeech.html": {
    "title": "Exploiting Hidden-Layer Responses of Deep Neural Networks for Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/irtza16_interspeech.html": {
    "title": "Out of Set Language Modelling in Hierarchical Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/masumura16_interspeech.html": {
    "title": "Language Identification Based on Generative Modeling of Posteriorgram Sequences Extracted from Frame-by-Frame DNNs and LSTM-RNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/geng16b_interspeech.html": {
    "title": "Gating Recurrent Enhanced Memory Neural Networks on Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pesan16_interspeech.html": {
    "title": "Sequence Summarizing Neural Networks for Spoken Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kapolowicz16_interspeech.html": {
    "title": "The Role of Spectral Resolution in Foreign-Accented Speech Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/he16c_interspeech.html": {
    "title": "THU-EE System Description for NIST LRE 2015",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jokinen16c_interspeech.html": {
    "title": "Variation in Spoken North Sami Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16h_interspeech.html": {
    "title": "Improved Music Genre Classification with Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/m16_interspeech.html": {
    "title": "Enhanced Harmonic Content and Vocal Note Based Predominant Melody Extraction from Vocal Polyphonic Music Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16n_interspeech.html": {
    "title": "Long Short-Term Memory for Speaker Generalization in Supervised Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kruspe16b_interspeech.html": {
    "title": "Phonotactic Language Identification for Singing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bentsen16_interspeech.html": {
    "title": "Comparing the Influence of Spectro-Temporal Integration in Computational Speech Segregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wood16_interspeech.html": {
    "title": "Blind Speech Separation with GCC-NMF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/montazeri16_interspeech.html": {
    "title": "Effects of Cochlear Hearing Loss on the Benefits of Ideal Binary Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/grais16_interspeech.html": {
    "title": "Combining Mask Estimates for Single Channel Audio Source Separation Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/riday16_interspeech.html": {
    "title": "Monaural Source Separation Using a Random Forest Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16l_interspeech.html": {
    "title": "Adaptive Group Sparsity for Non-Negative Matrix Factorization with Application to Unsupervised Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/guo16c_interspeech.html": {
    "title": "A Robust Dual-Microphone Speech Source Localization Algorithm for Reverberant Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ma16c_interspeech.html": {
    "title": "Speech Localisation in a Multitalker Mixture by Humans and Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sundar16_interspeech.html": {
    "title": "Reverberation-Robust One-Bit TDOA Based Moving Source Localization for Automatic Camera Steering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ochi16b_interspeech.html": {
    "title": "Multi-Talker Speech Recognition Based on Blind Source Separation with ad hoc Microphone Array Using Smartphones and Cloud Storage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fahringer16_interspeech.html": {
    "title": "Phase-Aware Signal Processing for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sailor16_interspeech.html": {
    "title": "Unsupervised Deep Auditory Model Using Stack of Convolutional RBMs for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/weber16_interspeech.html": {
    "title": "Interpretation of Low Dimensional Neural Network Bottleneck Features in Terms of Human Perception and Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16i_interspeech.html": {
    "title": "Compact Feedforward Sequential Memory Networks for Large Vocabulary Continuous Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tang16d_interspeech.html": {
    "title": "Future Context Attention for Unidirectional LSTM Based Acoustic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chien16_interspeech.html": {
    "title": "Hybrid Accelerated Optimization for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chan16c_interspeech.html": {
    "title": "On Online Attention-Based Speech Recognition and Joint Mandarin Character-Pinyin Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gosztolya16d_interspeech.html": {
    "title": "GMM-Free Flat Start Sequence-Discriminative DNN Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/miao16_interspeech.html": {
    "title": "Open-Domain Audio-Visual Speech Recognition: A Deep Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhao16d_interspeech.html": {
    "title": "Multidimensional Residual Learning Based on Recurrent Neural Networks for Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zeyer16_interspeech.html": {
    "title": "Towards Online-Recognition with Deep Bidirectional LSTM Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sercu16_interspeech.html": {
    "title": "Advances in Very Deep Convolutional Neural Networks for LVCSR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ghahremani16_interspeech.html": {
    "title": "Acoustic Modelling from the Signal Domain Using CNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chebotar16_interspeech.html": {
    "title": "Distilling Knowledge from Ensembles of Neural Networks for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16h_interspeech.html": {
    "title": "Triphone State-Tying via Deep Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/luyet16_interspeech.html": {
    "title": "Low-Rank Representation of Nearest Neighbor Posterior Probabilities to Enhance DNN Based Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zheng16b_interspeech.html": {
    "title": "Improving Large Vocabulary Accented Mandarin Speech Recognition with Attribute-Based I-Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shahnawazuddin16_interspeech.html": {
    "title": "Pitch-Adaptive Front-End Features for Robust Children's ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/delagua16_interspeech.html": {
    "title": "ASR Confidence Estimation with Speaker-Adapted Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dharo16_interspeech.html": {
    "title": "Automatic Correction of ASR Outputs by Using Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mallidi16_interspeech.html": {
    "title": "A Framework for Practical Multistream ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/joy16_interspeech.html": {
    "title": "DNNs for Unsupervised Extraction of Pseudo FMLLR Features Without Explicit Adaptation Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/samarakoon16b_interspeech.html": {
    "title": "Multi-Attribute Factorized Hidden Layer Adaptation for DNN Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/goo16_interspeech.html": {
    "title": "Speaker Normalization Through Feature Shifting of Linearly Transformed i-Vector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/diab16_interspeech.html": {
    "title": "Computational Approaches to Linguistic Code Switching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/arisoy16_interspeech.html": {
    "title": "Compositional Neural Network Language Models for Agglutinative Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/damavandi16_interspeech.html": {
    "title": "NN-Grams: Unifying Neural Network and n-Gram Language Models for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/haidar16_interspeech.html": {
    "title": "Recurrent Neural Network Language Model with Incremental Updated Context Information Generated Using Bag-of-Words Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/oualil16_interspeech.html": {
    "title": "Sequential Recurrent Neural Networks for Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/levit16_interspeech.html": {
    "title": "Word-Phrase-Entity Recurrent Neural Networks for Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/irie16_interspeech.html": {
    "title": "LSTM, GRU, Highway and a Bit of Attention: An Empirical Overview for Language Modeling in Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/das16b_interspeech.html": {
    "title": "Automatic Speech Recognition Using Probabilistic Transcriptions in Swahili, Amharic, and Dinka",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gauthier16b_interspeech.html": {
    "title": "Speed Perturbation and Vowel Duration Modeling for ASR in Hausa and Wolof Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/heerden16_interspeech.html": {
    "title": "Improving the Lwazi ASR Baseline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/godard16_interspeech.html": {
    "title": "Preliminary Experiments on Unsupervised Word Discovery in Mboshi",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/vetter16_interspeech.html": {
    "title": "Unsupervised Phoneme Segmentation of Previously Unseen Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/manenti16_interspeech.html": {
    "title": "CNN-Based Phone Segmentation Experiments in a Less-Represented Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schlunz16_interspeech.html": {
    "title": "Part-of-Speech Tagging and Chunking in Text-to-Speech Synthesis for South African Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/westhuizen16_interspeech.html": {
    "title": "The Effect of Postlexical Deletion on Automatic Speech Recognition in Fast Spontaneously Spoken Zulu",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ramanarayanan16_interspeech.html": {
    "title": "A New Model of Speech Motor Control Based on Task Dynamics and State Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dabbaghchian16_interspeech.html": {
    "title": "Using a Biomechanical Model and Articulatory Data for the Numerical Production of Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wei16_interspeech.html": {
    "title": "A New Model for Acoustic Wave Propagation and Scattering in the Vocal Tract",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/szabados16_interspeech.html": {
    "title": "Uncontrolled Manifolds in Vowel Production: Assessment with a Biomechanical Model of the Tongue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yoshinaga16_interspeech.html": {
    "title": "Experimental Validation of Sound Generated from Flow in Simplified Vocal Tract Model of Sibilant /s/",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/patri16_interspeech.html": {
    "title": "Bayesian Modeling in Speech Motor Control: A Principled Structure for the Integration of Various Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16j_interspeech.html": {
    "title": "Facing Realism in Spontaneous Emotion Recognition from Speech: Feature Enhancement by Autoencoder with LSTM Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/parthasarathy16_interspeech.html": {
    "title": "Defining Emotionally Salient Regions Using Qualitative Agreement Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ghosh16b_interspeech.html": {
    "title": "Representation Learning for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16m_interspeech.html": {
    "title": "Multilingual Speech Emotion Recognition System Based on a Three-Layer Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kalinli16_interspeech.html": {
    "title": "Analysis of Multi-Lingual Emotion Recognition Using Auditory Attention Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fayek16_interspeech.html": {
    "title": "On the Correlation and Transferability of Features Between Automatic Speech Recognition and Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/valenti16_interspeech.html": {
    "title": "On the Influence of Text Content on Pass-Phrase Strength for Short-Duration Text-Dependent Automatic Speaker Authentication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/todisco16_interspeech.html": {
    "title": "Articulation Rate Filtering of CQCC Features for Automatic Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sadjadi16_interspeech.html": {
    "title": "The IBM Speaker Recognition System: Recent Advances and Error Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kheder16c_interspeech.html": {
    "title": "Probabilistic Approach Using Joint Clean and Noisy i-Vectors Modeling for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bahmaninezhad16_interspeech.html": {
    "title": "Generalized Discriminant Analysis (GDA) for Improved i-Vector Based Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/qian16b_interspeech.html": {
    "title": "Noise and Metadata Sensitive Bottleneck Features for Improving Speaker Recognition with Non-Native Speech Input",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/phan16_interspeech.html": {
    "title": "Robust Audio Event Recognition with 1-Max Pooling Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/karamanolakis16_interspeech.html": {
    "title": "Audio-Based Distributional Representations of Meaning Using a Fusion of Feature Encodings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fujita16_interspeech.html": {
    "title": "Robust DNN-Based VAD Augmented with Phone Entropy Based Rejection of Background Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zazo16_interspeech.html": {
    "title": "Feature Learning with Raw-Waveform CLDNNs for Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/graciarena16_interspeech.html": {
    "title": "The SRI System for the NIST OpenSAD 2015 Speech Activity Detection Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/karakos16_interspeech.html": {
    "title": "Model Adaptation and Active Learning in the BBN Speech Activity Detection System for the DARPA RATS Program",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mitra16c_interspeech.html": {
    "title": "Fusion Strategies for Robust Speech Recognition and Keyword Spotting for Channel- and Noise-Degraded Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sawada16_interspeech.html": {
    "title": "Recurrent Neural Network-Based Phoneme Sequence Estimation Using Multiple ASR Systems' Outputs for Spoken Term Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kane16_interspeech.html": {
    "title": "Enhancing Data-Driven Phone Confusions Using Restricted Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ni16b_interspeech.html": {
    "title": "Rapid Update of Multilingual Deep Neural Network for Low-Resource Keyword Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/leung16_interspeech.html": {
    "title": "Toward High-Performance Language-Independent Query-by-Example Spoken Term Detection for MediaEval 2015: Post-Evaluation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/soni16b_interspeech.html": {
    "title": "Novel Subband Autoencoder Features for Non-Intrusive Quality Assessment of Noise Suppressed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gao16_interspeech.html": {
    "title": "SNR-Based Progressive Learning of Deep Neural Network for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sadasivan16_interspeech.html": {
    "title": "A Novel Risk-Estimation-Theoretic Framework for Speech Enhancement in Nonstationary and Non-Gaussian Noise Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/samui16_interspeech.html": {
    "title": "Two-Stage Temporal Processing for Single-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pm16_interspeech.html": {
    "title": "A Class-Specific Speech Enhancement for Phoneme Recognition: A Dictionary Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ogawa16_interspeech.html": {
    "title": "Robust Example Search Using Bottleneck Features for Example-Based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kumar16c_interspeech.html": {
    "title": "Speech Enhancement in Multiple-Noise Conditions Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shivakumar16b_interspeech.html": {
    "title": "Perception Optimized Deep Denoising AutoEncoders for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kato16_interspeech.html": {
    "title": "HMM-Based Speech Enhancement Using Sub-Word Models and Noise Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16n_interspeech.html": {
    "title": "Semi-Supervised Joint Enhancement of Spectral and Cepstral Sequences of Noisy Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chinaev16_interspeech.html": {
    "title": "A priori SNR Estimation Using a Generalized Decision Directed Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16i_interspeech.html": {
    "title": "A DNN-HMM Approach to Non-Negative Matrix Factorization Based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fu16_interspeech.html": {
    "title": "SNR-Aware Convolutional Neural Network Modeling for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16o_interspeech.html": {
    "title": "An Iterative Phase Recovery Framework with Phase Mask for Spectral Mapping with an Application to Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16k_interspeech.html": {
    "title": "A Novel Research to Artificial Bandwidth Extension Based on Deep BLSTM Recurrent Neural Networks and Exemplar-Based Sparse Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mitra16d_interspeech.html": {
    "title": "Coping with Unseen Data Conditions: Investigating Neural Net Architectures, Robust Features, and Information Fusion for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tomashenko16_interspeech.html": {
    "title": "On the Use of Gaussian Mixture Model Framework to Improve Speaker Adaptation of Deep Neural Network Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bosch16b_interspeech.html": {
    "title": "Analytical Assessment of Dual-Stream Merging for Noise-Robust ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/loweimi16_interspeech.html": {
    "title": "Use of Generalised Nonlinearity in Vector Taylor Series Noise Compensation for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mimura16_interspeech.html": {
    "title": "Joint Optimization of Denoising Autoencoder and DNN Acoustic Model Based on Multi-Target Learning for Noisy Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/higuchi16_interspeech.html": {
    "title": "Optimization of Speech Enhancement Front-End with Speech Recognition-Level Criterion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tran16_interspeech.html": {
    "title": "Factorized Linear Input Network for Acoustic Model Adaptation in Noisy Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fujita16b_interspeech.html": {
    "title": "Data Augmentation Using Multi-Input Multi-Output Source Separation for Deep Neural Network Based Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/prasad16_interspeech.html": {
    "title": "Microphone Distance Adaptation Using Cluster Adaptive Training for Robust Far Field Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dimitriadis16_interspeech.html": {
    "title": "An Investigation on the Use of i-Vectors for Robust ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16l_interspeech.html": {
    "title": "The Sheffield Wargame Corpus — Day Two and Day Three",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kim16d_interspeech.html": {
    "title": "Recurrent Models for Auditory Attention in Multi-Microphone Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lee16f_interspeech.html": {
    "title": "Semi-Supervised Speaker Adaptation for In-Vehicle Speech Recognition with Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/huang16e_interspeech.html": {
    "title": "Semi-Supervised Training in Deep Learning Acoustic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/thomas16_interspeech.html": {
    "title": "Multilingual Data Selection for Low Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/das16c_interspeech.html": {
    "title": "An Investigation on Training Deep Neural Networks Using Probabilistic Transcriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/do16c_interspeech.html": {
    "title": "Analysis of Mismatched Transcriptions Generated by Humans and Machines for Under-Resourced Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nouza16_interspeech.html": {
    "title": "ASR for South Slavic Languages Developed in Almost Automated Way",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/razavi16_interspeech.html": {
    "title": "Improving Under-Resourced Language ASR Through Latent Subword Unit Space Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/muller16_interspeech.html": {
    "title": "Language Adaptive DNNs for Improved Low Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/alumae16_interspeech.html": {
    "title": "Improved Multilingual Training of Stacked Neural Network Acoustic Models for Low Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}