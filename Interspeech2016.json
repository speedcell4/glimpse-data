{
  "https://www.isca-speech.org/archive/interspeech_2016/makhoul16_interspeech.html": {
    "title": "A 50-Year Retrospective on Speech and Language Processing",
    "volume": "main",
    "abstract": "This talk is a retrospective of speech and language processing as witnessed by the speaker during the last 50 years. From exploratory scientific beginnings that emphasized the discovery of how speech is produced and perceived by humans to today's plethora of applications using our technology, our field has witnessed explosive growth. The talk will review the historical development of our community and some of the key technical ideas that have shaped our field. Some of the ideas were influenced by developments in other fields, while some of the developments in our field have been instrumental in key advances in other fields, such as optical character recognition and machine translation. Important developments include the source-filter model, digital signal processing, linear prediction, vector quantization, deep neural networks, and statistical modeling methods, especially hidden Markov models (HMMs), with primary applications to speech analysis, synthesis, coding, and recognition. The talk will be sprinkled with lessons learned in the importance of various factors in performing our research, and will be peppered with interesting tidbits about key moments in the development of our technology. The talk will end with a brief prospective peek at the next 50 years",
    "checked": true,
    "id": "5af3c7edf0a4353bb9890b0d830c295ef808d907",
    "semantic_title": "a 50-year retrospective on speech and language processing",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/medennikov16_interspeech.html": {
    "title": "Improving English Conversational Telephone Speech Recognition",
    "volume": "main",
    "abstract": "The goal of this work is to build a state-of-the-art English conversational telephone speech recognition system. We investigated several techniques to improve acoustic modeling, namely speaker-dependent bottleneck features, deep Bidirectional Long Short-Term Memory (BLSTM) recurrent neural networks, data augmentation and score fusion of DNN and BLSTM models. Training set consisted of the 300 hour Switchboard English speech corpus. We also examined the hypothesis rescoring using language models based on recurrent neural networks. The resulting system achieves a word error rate of 7.8% on the Switchboard part of the HUB5 2000 evaluation set which is the competitive result",
    "checked": true,
    "id": "a824aa5a86293c5f5f80bef2b77e8d676da38110",
    "semantic_title": "improving english conversational telephone speech recognition",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2016/saon16_interspeech.html": {
    "title": "The IBM 2016 English Conversational Telephone Speech Recognition System",
    "volume": "main",
    "abstract": "We describe a collection of acoustic and language modeling techniques that lowered the word error rate of our English conversational telephone LVCSR system to a record 6.6% on the Switchboard subset of the Hub5 2000 evaluation testset. On the acoustic side, we use a score fusion of three strong models: recurrent nets with maxout activations, very deep convolutional nets with 3×3 kernels, and bidirectional long short-term memory nets which operate on FMLLR and i-vector features. On the language modeling side, we use an updated model \"M\" and hierarchical neural network LMs",
    "checked": true,
    "id": "05d2700846c0323f79c1344aca5333994c7c03a5",
    "semantic_title": "the ibm 2016 english conversational telephone speech recognition system",
    "citation_count": 104
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lu16_interspeech.html": {
    "title": "Small-Footprint Deep Neural Networks with Highway Connections for Speech Recognition",
    "volume": "main",
    "abstract": "For speech recognition, deep neural networks (DNNs) have significantly improved the recognition accuracy in most of benchmark datasets and application domains. However, compared to the conventional Gaussian mixture models, DNN-based acoustic models usually have much larger number of model parameters, making it challenging for their applications in resource constrained platforms, e.g., mobile devices. In this paper, we study the application of the recently proposed highway network to train small-footprint DNNs, which are thinner and deeper, and have significantly smaller number of model parameters compared to conventional DNNs. We investigated this approach on the AMI meeting speech transcription corpus which has around 80 hours of audio data. The highway neural networks constantly outperformed their plain DNN counterparts, and the number of model parameters can be reduced significantly without sacrificing the recognition accuracy",
    "checked": true,
    "id": "de0681f80ea57abc0e877b09e33fbb6ef313f392",
    "semantic_title": "small-footprint deep neural networks with highway connections for speech recognition",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yu16_interspeech.html": {
    "title": "Deep Convolutional Neural Networks with Layer-Wise Context Expansion and Attention",
    "volume": "main",
    "abstract": "In this paper, we propose a deep convolutional neural network (CNN) with layer-wise context expansion and location-based attention, for large vocabulary speech recognition. In our model each higher layer uses information from broader contexts, along both the time and frequency dimensions, than its immediate lower layer. We show that both the layer-wise context expansion and the location-based attention can be implemented using the element-wise matrix product and the convolution operation. For this reason, contrary to other CNNs, no pooling operation is used in our model. Experiments on the 309hr Switchboard task and the 375hr short message dictation task indicates that our model outperforms both the DNN and LSTM significantly",
    "checked": true,
    "id": "716e60cbbdacf01b3148e91a555358a96308b770",
    "semantic_title": "deep convolutional neural networks with layer-wise context expansion and attention",
    "citation_count": 73
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pundak16_interspeech.html": {
    "title": "Lower Frame Rate Neural Network Acoustic Models",
    "volume": "main",
    "abstract": "Recently neural network acoustic models trained with Connectionist Temporal Classification (CTC) were proposed as an alternative approach to conventional cross-entropy trained neural network acoustic models which output frame-level decisions every 10ms [1]. As opposed to conventional models, CTC learns an alignment jointly with the acoustic model, and outputs a blank symbol in addition to the regular acoustic state units. This allows the CTC model to run with a lower frame rate, outputting decisions every 30ms rather than 10ms as in conventional models, thus improving overall system speed. In this work, we explore how conventional models behave with lower frame rates. On a large vocabulary Voice Search task, we will show that with conventional models, we can slow the frame rate to 40ms while improving WER by 3% relative over a CTC-based model",
    "checked": true,
    "id": "f79925410329ab4e3045243f4a652dd03afd4cc8",
    "semantic_title": "lower frame rate neural network acoustic models",
    "citation_count": 138
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kurata16_interspeech.html": {
    "title": "Improved Neural Network Initialization by Grouping Context-Dependent Targets for Acoustic Modeling",
    "volume": "main",
    "abstract": "Neural Network (NN) Acoustic Models (AMs) are usually trained using context-dependent Hidden Markov Model (CD-HMM) states as independent targets. For example, the CD-HMM states of A-b-2 (second variant of beginning state of A) and A-m-1 (first variant of middle state of A) both correspond to the phone A, and A-b-1 and A-b-2 both correspond to the Context-independent HMM (CI-HMM) state A-b, but this relationship is not explicitly modeled. We propose a method that treats some neurons in the final hidden layer just below the output layer as dedicated neurons for phones or CI-HMM states by initializing connections between the dedicated neurons and the corresponding CD-HMM outputs with stronger weights than to other outputs. We obtained 6.5% and 3.6% relative error reductions with a DNN AM and a CNN AM, respectively, on a 50-hour English broadcast news task and 4.6% reduction with a CNN AM on a 500-hour Japanese task, in all cases after Hessian-free sequence training. Our proposed method only changes the NN parameter initialization and requires no additional computation in NN training or speech recognition run-time",
    "checked": true,
    "id": "d62fe0ab3a023ecc3f5e1c198c02812b10c568e7",
    "semantic_title": "improved neural network initialization by grouping context-dependent targets for acoustic modeling",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16_interspeech.html": {
    "title": "Automatic Scoring of Monologue Video Interviews Using Multimodal Cues",
    "volume": "main",
    "abstract": "Job interviews are an important tool for employee selection. When making hiring decisions, a variety of information from interviewees, such as previous work experience, skills, and their verbal and nonverbal communication, are jointly considered. In recent years, Social Signal Processing (SSP), an emerging research area on enabling computers to sense and understand human social signals, is being used develop systems for the coaching and evaluation of job interview performance. However this research area is still in its infancy and lacks essential resources (e.g., adequate corpora). In this paper, we report on our efforts to create an automatic interview rating system for monologue-style video interviews, which have been widely used in today's job hiring market. We created the first multimodal corpus for such video interviews. Additionally, we conducted manual rating on the interviewee's personality and performance during 12 structured interview questions measuring different types of job-related skills. Finally, focusing on predicting overall interview performance, we explored a set of verbal and nonverbal features and several machine learning models. We found that using both verbal and nonverbal features provides more accurate predictions. Our initial results suggest that it is feasible to continue working in this newly formed area",
    "checked": true,
    "id": "039499f66eda6c33d47d4036601656ee98b88570",
    "semantic_title": "automatic scoring of monologue video interviews using multimodal cues",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chong16_interspeech.html": {
    "title": "The Sound of Disgust: How Facial Expression May Influence Speech Production",
    "volume": "main",
    "abstract": "In speech articulation, mouth/lip shapes determine properties of the front part of the vocal tract, and so alter vowel formant frequencies. Mouth and lip shapes also determine facial emotional expressions, e.g., disgust is typically expressed with a distinctive lip and mouth configuration (i.e., closed mouth, pulled back lip corners). This overlap of speech and emotion gestures suggests that expressive speech will have different vowel formant frequencies from neutral speech. This study tested this hypothesis by comparing vowels produced in neutral versus disgust expressions. We used our database of five female native Cantonese talkers each uttering 50 CHINT sentences in both a neutral tone of voice and in disgust to examine five vowels ([ɐ], [εː], [iː], [ɔː], [ᴜː]). Mean fundamental frequency (F0) and the first two formants (F1 and F2) were calculated and analysed using mixed effects logistic regression. The results showed that the disgust vowels showed a significant reduction in either or both formant values (depending on vowel type) compared to neutral. We discuss the results in terms of how vowel synthesis could be used to alter the recognition of the sound of disgust",
    "checked": true,
    "id": "0336c778b6ae1669f23cc451020764af182dde8a",
    "semantic_title": "the sound of disgust: how facial expression may influence speech production",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yang16_interspeech.html": {
    "title": "Analyzing Temporal Dynamics of Dyadic Synchrony in Affective Interactions",
    "volume": "main",
    "abstract": "Human communication is a dynamical and interactive process that naturally induces an active flow of interpersonal coordination, and synchrony, along various behavioral dimensions. Assessing and characterizing the temporal dynamics of synchrony during an interaction is essential for fully understanding the human communication mechanisms. In this work, we focus on uncovering the temporal variability patterns of synchrony in visual gesture and vocal behavior in affectively rich interactions. We propose a statistical scheme to robustly quantify the turn-wise interpersonal synchrony. The analysis of the synchrony dynamics measure relies heavily on functional data analysis techniques. Our analysis results reveal that: 1) the dynamical patterns of interpersonal synchrony differ depending on the global emotions of an interaction dyad; 2) there generally exists a tight dynamical emotion-synchrony coupling over the interaction. These observations corroborate that interpersonal behavioral synchrony is a critical manifestation of the underlying affective processes, shedding light toward improved affective interaction modeling and automatic emotion recognition",
    "checked": true,
    "id": "c412cc824dd67d8ce39ee9493b51449be63cbcaf",
    "semantic_title": "analyzing temporal dynamics of dyadic synchrony in affective interactions",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ganesh16_interspeech.html": {
    "title": "Audiovisual Speech Scene Analysis in the Context of Competing Sources",
    "volume": "main",
    "abstract": "Audiovisual fusion in speech perception is generally conceived as a process independent from scene analysis, which is supposed to occur separately in the auditory and visual domain. On the contrary, we have been proposing in the last years that scene analysis such as what takes place in the cocktail party effect was an audiovisual process. We review here a series of experiments illustrating how audiovisual speech scene analysis occurs in the context of competing sources. Indeed, we show that a short contextual audiovisual stimulus made of competing auditory and visual sources modifies the perception of a following McGurk target. We interpret this in terms of binding, unbinding and rebinding processes, and we show how these processes depend on audiovisual correlations in time, attentional processes and differences between junior and senior participants",
    "checked": true,
    "id": "f30cc62b661b764425762ca0f4add273619ec884",
    "semantic_title": "audiovisual speech scene analysis in the context of competing sources",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sadoughi16_interspeech.html": {
    "title": "Head Motion Generation with Synthetic Speech: A Data Driven Approach",
    "volume": "main",
    "abstract": "To have believable head movements for conversational agents (CAs), the natural coupling between speech and head movements needs to be preserved, even when the CA uses synthetic speech. To incorporate the relation between speech head movements, studies have learned these couplings from real recordings, where speech is used to derive head movements. However, relying on recorded speech for every sentence that a virtual agent utters constrains the versatility and scalability of the interface, so most practical solutions for CAs use text to speech. While we can generate head motion using rule-based models, the head movements may become repetitive, spanning only a limited range of behaviors. This paper proposes strategies to leverage speech-driven models for head motion generation for cases relying on synthetic speech. The straightforward approach is to drive the speech-based models using synthetic speech, which creates mismatch between the test and train conditions. Instead, we propose to create a parallel corpus of synthetic speech aligned with natural recordings for which we have motion capture recordings. We use this parallel corpus to either retrain or adapt the speech-based models with synthetic speech. Objective and subjective metrics show significant improvements of the proposed approaches over the case with mismatched condition",
    "checked": true,
    "id": "c63a2a9adf3d0837cabe1f4221306de1d4512c22",
    "semantic_title": "head motion generation with synthetic speech: a data driven approach",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kim16_interspeech.html": {
    "title": "The Consistency and Stability of Acoustic and Visual Cues for Different Prosodic Attitudes",
    "volume": "main",
    "abstract": "Recently it has been argued that speakers use conventionalized forms to express different prosodic attitudes [1]. We examined this by looking at across speaker consistency in the expression of auditory and visual (head and face motion) prosodic attitudes produced on multiple different occasions. Specifically, we examined acoustic and motion profiles of a female and a male speaker expressing six different prosodic attitudes for four within-session repetitions across four different sessions. We used the same acoustic features as [1] and visual prosody was assessed by examining patterns of speaker's mouth, eyebrow and head movements. There was considerable variation in how prosody was realized across speakers, with the productions of one speaker more discriminable than the other. Within-session variation for both the acoustic and movement data was smaller than across-session variation, suggesting that short-term memory plays a role in consistency. The expression of some attitudes was less variable than others and better discrimination was found with the acoustic compared to the visual data, although certain visual features (e.g., eyebrow brow motion) provided better discrimination than others",
    "checked": true,
    "id": "d1b10a10626d6edb0d8a48874c9d6ae053ffcb3b",
    "semantic_title": "the consistency and stability of acoustic and visual cues for different prosodic attitudes",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kim16b_interspeech.html": {
    "title": "Introduction to Poster Presentation of Part II",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5f14e9a4dce354bb25ba2322c03de89ede6ce94c",
    "semantic_title": "introduction to poster presentation of part ii",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/vogel16_interspeech.html": {
    "title": "The Unit of Speech Encoding: The Case of Romanian",
    "volume": "main",
    "abstract": "The number of units in an utterance determines how much time speakers require to physically plan and begin their production [1]–[2]. Previous research proposed that the crucial units are prosodic i.e., Phonological Words (PWs), not syntactic or morphological [3]. Experiments on Dutch using a prepared speech paradigm claimed to support this view [4]–[5]; however, compounds did not conform to predictions and required the introduction of a different way of counting units. Since two PWs in compounds patterned with one PW, with or without clitics, rather than a phrase containing two PWs, a recursive PW' was invoked. Similar results emerged using the same methodology with compounds in Italian [6], and it was thus proposed that the relevant unit for speech encoding is not the PW, but rather the Composite Group (CompG), a constituent of the Prosodic Hierarchy between the PW and Phonological Phrase that comprises both compounds and clitic constructions [7]. We further investigate the relevant unit for speech encoding using the same methodology in Romanian. Similar findings support the CompG as the speech planning unit since, again, compounds with two PWs pattern with single words and clitic constructions, not Phonological Phrases which also contain two PWs",
    "checked": true,
    "id": "18c19a47e82b8abc8901276b40948f4227bc4303",
    "semantic_title": "the unit of speech encoding: the case of romanian",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jugler16_interspeech.html": {
    "title": "The Perceptual Effect of L1 Prosody Transplantation on L2 Speech: The Case of French Accented German",
    "volume": "main",
    "abstract": "Research has shown that language learners are not only challenged by segmental differences between their native language (L1) and the second language (L2). They also have problems with the correct production of suprasegmental structures, like phone/syllable duration and the realization of pitch. These difficulties often lead to a perceptible foreign accent. This study investigates the influence of prosody transplantation on foreign accent ratings. Syllable duration and pitch contour were transferred from utterances of a male and female German native speaker to utterances of ten French native speakers speaking German. Acoustic measurements show that French learners spoke with a significantly lower speaking rate. As expected, results of a perception experiment judging the accentedness of 1) German native utterances, 2) unmanipulated and 3) manipulated utterances of French learners of German suggest that the transplantation of the prosodic features syllable duration and pitch leads to a decrease in accentedness rating. These findings confirm results found in similar studies investigating prosody transplantation with different L1 and L2 and provide a beneficial technique for (computer-assisted) pronunciation training",
    "checked": true,
    "id": "deeaba4d6a0fd99a6a661eaa9236d090da416f64",
    "semantic_title": "the perceptual effect of l1 prosody transplantation on l2 speech: the case of french accented german",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ling16_interspeech.html": {
    "title": "Organizing Syllables into Sandhi Domains — Evidence from F0 and Duration Patterns in Shanghai Chinese",
    "volume": "main",
    "abstract": "In this study we investigated grouping-related F0 patterns in Shanghai Chinese by examining the effect of syllable position in a sandhi domain while controlling for tone, number of syllables in a domain, and focus condition. Results showed that F0 alignment had the most consistent grouping-related patterns, and syllable duration was positively related to F0 movement. Focus and word length both increased F0 peak and F0 excursion, but they had opposite influence on F0 slope, which indicated that focus and word length had different mechanisms in affecting F0 implementation, as focus increased articulation strength while word length influenced speaker's pre-planning",
    "checked": false,
    "id": "61cc99c9ea92f4b0f21f3b3d7a4f4a69985aa0ea",
    "semantic_title": "organizing syllables into sandhi domains - evidence from f0 and duration patterns in shanghai chinese",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ryant16_interspeech.html": {
    "title": "Automatic Analysis of Phonetic Speech Style Dimensions",
    "volume": "main",
    "abstract": "We apply automated analysis methods to create a multidimensional characterization of the prosodic characteristics of a large variety of speech datasets, with the goal of developing a general framework for comparing prosodic styles. Our datasets span styles including conversation, fluent reading, extemporized narratives, political speech, and advertisements; we compare several different languages including English, Spanish, and Chinese; and the features we extract are based on the joint distributions of F0 and amplitude values and sequences, speech and silence segment durations, syllable durations, and modulation spectra. Rather than focus on the acoustic correlates of a small number of discrete and mutually exclusive categories, we aim to characterize the space in which diverse speech styles live",
    "checked": true,
    "id": "d25ec1e7f56e261e46a48e7449ace96fd47f6348",
    "semantic_title": "automatic analysis of phonetic speech style dimensions",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/athanasopoulou16_interspeech.html": {
    "title": "The Acoustic Manifestation of Prominence in Stressless Languages",
    "volume": "main",
    "abstract": "Languages frequently express focus by enhancing various acoustic attributes of an utterance, but it is widely accepted that the main enhancement appears on stressed syllables. In languages without lexical stress, the question arises as to how focus is acoustically manifested. We thus examine the acoustic properties associated with prominence in three stressless languages, Indonesian, Korean and Vietnamese, comparing real three-syllable words in non-focused and focused contexts. Despite other prosodic differences, our findings confirm that none of the languages exhibits stress in the absence of focus, and under focus, no syllable shows consistent enhancement that could be indirectly interpreted as a manifestation of focus. Instead, a combination of boundary phenomena consistent with the right edge of a major prosodic constituent (Intonational Phrase) appears in each language: increased duration on the final syllable and in Indonesian and Korean, a decrease in F0. Since these properties are also found in languages with stress, we suggest that boundary phenomena signaling a major prosodic constituent break are used universally to indicate focus, regardless of a language's word-prosody; stress languages may use the same boundary properties, but these are most likely to be combined with enhancement of the stressed syllable of a word",
    "checked": true,
    "id": "e81f59d9168d031c028599bb36971696b3486673",
    "semantic_title": "the acoustic manifestation of prominence in stressless languages",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lai16_interspeech.html": {
    "title": "The Rhythmic Constraint on Prosodic Boundaries in Mandarin Chinese Based on Corpora of Silent Reading and Speech Perception",
    "volume": "main",
    "abstract": "This study investigated the interaction between rhythmic and syntactic constraints on prosodic phrases in Mandarin Chinese. A set of 4000 sentences was annotated twice, once based on silent reading by 130 students assigned 500 sentences each, and a second time by speech perception based on a recording by one professional speaker. In both types of annotation, the general pattern of phrasing was consistent, with short \"rhythmic phrases\" behaving differently from longer \"intonational phrases\". The probability of a rhythmic-phrase boundary between two words increased with the total length of those two words, and was also influenced by the nature of the syntactic boundary between them. The resulting rhythmic phrases were mainly 2–5 syllables long, independent of the length of the sentence. In contrast, the length of intonational phrases was not stable, and was heavily affected by sentence length. Intonational-phrase boundaries were also found to be affected by higher-level syntactic features, such as the depth of syntactic tree and the number of IP nodes. However, these syntactic influences on intonational phrases were weakened in long sentences (>20 syllable) and also in short sentences (<10 syllable), where the length effect played the main role",
    "checked": true,
    "id": "d38f2d331a4dbe57804463aa1f70a88fedc75bd6",
    "semantic_title": "the rhythmic constraint on prosodic boundaries in mandarin chinese based on corpora of silent reading and speech perception",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tsai16_interspeech.html": {
    "title": "Toward Development and Evaluation of Pain Level-Rating Scale for Emergency Triage based on Vocal Characteristics and Facial Expressions",
    "volume": "main",
    "abstract": "In order to allocate the healthcare resource, triage classification system plays an important role in assessing the severity of illness of the boarding patient at emergency department. The self-report pain intensity numerical-rating scale (NRS) is one of the major modifiers of the current triage system based on the Taiwan Triage and Acuity Scale (TTAS). The validity and reliability of self-report scheme for pain level assessment is a major concern. In this study, we model the observed expressive behaviors, i.e., facial expressions and vocal characteristics, directly from audio-video recordings in order to measure pain level for patients during triage. This work demonstrates a feasible model, which achieves an accuracy of 72.3% and 51.6% in a binary and ternary pain intensity classification. Moreover, the study result reveals a significant association of current model and analgesic prescription/patient disposition after adjusted for patient-report NRS and triage vital signs",
    "checked": true,
    "id": "ccd58c136ae8bed88889a93ca518f343ee3892c6",
    "semantic_title": "toward development and evaluation of pain level-rating scale for emergency triage based on vocal characteristics and facial expressions",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lee16_interspeech.html": {
    "title": "Predicting Severity of Voice Disorder from DNN-HMM Acoustic Posteriors",
    "volume": "main",
    "abstract": "Acoustical analysis of speech is considered a favorable and promising approach to objective assessment of voice disorders. Previous research emphasized on the extraction and classification of voice quality features from sustained vowel sounds. In this paper, an investigation on voice assessment using continuous speech utterances of Cantonese is presented. A DNN-HMM based speech recognition system is trained with speech data of unimpaired voice. The recognition accuracy for pathological utterances is found to decrease significantly with the disorder severity increasing. Average acoustic posterior probabilities are computed for individual phones from the speech recognition output lattices and the DNN soft-max layer. The phone posteriors obtained for continuous speech from the mild, moderate and severe categories are highly distinctive and thus useful to the determination of voice disorder severity. A subset of Cantonese phonemes are identified to be suitable and reliable for voice assessment with continuous speech",
    "checked": true,
    "id": "502d3f8c02fa77609a040d5e421f6c6f8f7e92a0",
    "semantic_title": "predicting severity of voice disorder from dnn-hmm acoustic posteriors",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sluis16_interspeech.html": {
    "title": "Long-Term Stability of Tracheoesophageal Voices",
    "volume": "main",
    "abstract": "Long-term voice outcomes of 13 tracheoesophageal speakers are assessed using speech samples that were recorded with at least 7 years in between. Intelligibility and voice quality are perceptually evaluated by 10 experienced speech and language pathologists. In addition, automatic speech evaluations are performed with tools from Ghent University. No significant group effect was found for changes in voice quality and intelligibility. The recordings showed a wide interspeaker variability. It is concluded that intelligibility and voice quality of tracheoesophageal voice is mostly stable over a period of 7 to 18 years",
    "checked": true,
    "id": "b8c06ac9a5dedaf314734fed5c315761624c0a32",
    "semantic_title": "long-term stability of tracheoesophageal voices",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gosztolya16_interspeech.html": {
    "title": "Detecting Mild Cognitive Impairment from Spontaneous Speech by Correlation-Based Phonetic Feature Selection",
    "volume": "main",
    "abstract": "Mild Cognitive Impairment (MCI), sometimes regarded as a prodromal stage of Alzheimer's disease, is a mental disorder that is difficult to diagnose. Recent studies reported that MCI causes slight changes in the speech of the patient. Our previous studies showed that MCI can be efficiently classified by machine learning methods such as Support-Vector Machines and Random Forest, using features describing the amount of pause in the spontaneous speech of the subject. Furthermore, as hesitation is the most important indicator of MCI, we took special care when handling filled pauses, which usually correspond to hesitation. In contrast to our previous studies which employed manually constructed feature sets, we now employ (automatic) correlation-based feature selection methods to find the relevant feature subset for MCI classification. By analyzing the selected feature subsets we also show that features related to filled pauses are useful for MCI detection from speech samples",
    "checked": true,
    "id": "dd7a14b0d6d814e7351261e184eb4d273a5d21c7",
    "semantic_title": "detecting mild cognitive impairment from spontaneous speech by correlation-based phonetic feature selection",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gong16_interspeech.html": {
    "title": "Towards an Automated Screening Tool for Developmental Speech and Language Impairments",
    "volume": "main",
    "abstract": "Approximately 60% of children with speech and language impairments do not receive the intervention they need because their impairment was missed by parents and professionals who lack specialized training. Diagnoses of these disorders require a time-intensive battery of assessments, and these are often only administered after parents, doctors, or teachers show concern An automated test could enable more widespread screening for speech and language impairments. To build classification models to distinguish children with speech or language impairments from typically developing children, we use acoustic features describing speech and pause events in story retell tasks. We developed and evaluated our method using two datasets. The smaller dataset contains many children with severe speech or language impairments and few typically developing children. The larger dataset contains primarily typically developing children. In three out of five classification tasks, even after accounting for age, gender, and dataset differences, our models achieve good discrimination performance (AUC > 0.70)",
    "checked": true,
    "id": "be67bb3a243e6b3b89d650fce1131e760b30718d",
    "semantic_title": "towards an automated screening tool for developmental speech and language impairments",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cm16_interspeech.html": {
    "title": "Spectral Enhancement of Cleft Lip and Palate Speech",
    "volume": "main",
    "abstract": "The quality of cleft lip and palate (CLP) speech is affected due to hyper-nasality and mis-articulation. Surgery and speech therapy are required to correct the structural and functional defects of CLP, which will result in an enhanced speech signal. The quality of the enhanced speech is perceptually evaluated by speech-language pathologists and results are highly biased. In this work, a signal processing based two stage speech enhancement method is proposed to get the perceptual benchmark to compare the signal after the surgery / therapy. In the first stage, CLP speech is enhanced by suppressing the nasal formant and in the second stage, spectral peak-valley enhancement is carried out to reduce the hyper-nasality associated with the CLP speech. The evaluation results show that the perceptual quality of CLP speech signal is improved after enhancement in both stages. Further, the improvement in the quality of the enhanced signal is compared with the speech signal after palatal prosthesis / surgery. The perceptual evaluation results show that the enhanced speech signals are better than the speech after prosthesis / surgery",
    "checked": true,
    "id": "5dccd526a6e6ddfaad768e0b80869b531a797e3f",
    "semantic_title": "spectral enhancement of cleft lip and palate speech",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/guan16_interspeech.html": {
    "title": "Assessing Level-Dependent Segmental Contribution to the Intelligibility of Speech Processed by Single-Channel Noise-Suppression Algorithms",
    "volume": "main",
    "abstract": "Most existing single-channel noise-suppression algorithms cannot improve speech intelligibility for normal-hearing listeners; however, the underlying reason for this performance deficit is still unclear. Given that various speech segments contain different perceptual contributions, the present work assesses whether the intelligibility of noisy speech can be improved when selectively suppressing its noise at high-level (vowel-dominated) or middle-level (containing vowel-consonant transitions) segments by existing single-channel noise-suppression algorithms. The speech signal was corrupted by speech-spectrum shaped noise and two-talker babble masker, and its noisy high- or middle-level segments were replaced by their noise-suppressed versions processed by four types of existing single-channel noise-suppression algorithms. Experimental results showed that performing segmental noise-suppression at high- or middle-level led to decreased intelligibility relative to noisy speech. This suggests that the lack of intelligibility improvement by existing noise-suppression algorithms is also present at segmental level, which may account for the deficit traditionally observed at full-sentence level",
    "checked": true,
    "id": "0d4d6f043e185614f8a8ea6426a73da4087d6a24",
    "semantic_title": "assessing level-dependent segmental contribution to the intelligibility of speech processed by single-channel noise-suppression algorithms",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zorila16_interspeech.html": {
    "title": "Effectiveness of Near-End Speech Enhancement Under Equal-Loudness and Equal-Level Constraints",
    "volume": "main",
    "abstract": "Most recently proposed near-end speech enhancement methods have been evaluated with the overall power (RMS) of the speech held constant. While significant intelligibility gains have been reported in various noisy conditions, an equal-RMS constraint may lead to enhancement solutions that increase the loudness of the original speech. Comparable effects might be produced simply by increasing the power of the original speech, which also leads to an increase in loudness. Here we suggest modifying the equal-RMS constraint to one of equal loudness between the original and the modified signals, based on a loudness model for time-varying sounds. Four state-of-the-art speech-in-noise intelligibility enhancement systems were evaluated under the equal-loudness constraint, using intelligibility tests with normal-hearing listeners. Results were compared with those obtained under the equal-RMS constraint. The methods based on spectral shaping and dynamic range compression yielded significant intelligibility gains regardless of the constraint, while for the method without dynamic range compression the intelligibility gain was lower under the equal-loudness than under the equal-RMS constraint",
    "checked": true,
    "id": "f1a5d8e6ae60925b5a8590c59d9ce267f0e92ec5",
    "semantic_title": "effectiveness of near-end speech enhancement under equal-loudness and equal-level constraints",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sharma16_interspeech.html": {
    "title": "Speech Synthesis in Noisy Environment by Enhancing Strength of Excitation and Formant Prominence",
    "volume": "main",
    "abstract": "Text-to-speech (TTS) synthesis systems have grown popularity due to their diverse practical usability. While most of the technologies developed aims to meet requirements in laboratory environment, the practical appliance is not limited to a specific environment. This work aims towards improving intelligibility of synthesized speech to make it deployable in realism. Based on the comparison of Lombard speech and speech produced in quiet, strength of excitation is found to play a crucial role in making speech intelligible in noisy situation. A novel method for enhancement of strength of excitation is proposed which makes the synthesized speech more intelligible in practical scenario. Linear-prediction analysis based formant enhancement method is also employed to further improve the intelligibility. The proposed enhancement framework is applied in synthesized speech and evaluated in presence of different types and levels of noise. Subjective evaluation results show that, the proposed method makes the synthesized speech applicable in practical noisy environment",
    "checked": true,
    "id": "ac6a0febe1aade6bba6ce4a4ac3d3f9ab0f31360",
    "semantic_title": "speech synthesis in noisy environment by enhancing strength of excitation and formant prominence",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16_interspeech.html": {
    "title": "Relative Contributions of Amplitude and Phase to the Intelligibility Advantage of Ideal Binary Masked Sentences",
    "volume": "main",
    "abstract": "Many studies have shown the advantage of using ideal binary masking (IdBM) to improve the intelligibility of speech corrupted by interfering maskers. Given the fact that amplitude and phase are two important acoustic cues for speech perception, the present work further investigated the relative contributions of these two cues to the intelligibility advantage of IdBM-processed sentences. Three types of Mandarin IdBM-processed stimuli (i.e., amplitude-only, phase-only, and amplitude-and-phase) were generated, and played to normal-hearing listeners to recognize. Experiment results showed that amplitude- or phase-only cue could lead to significantly improved intelligibility of IdBM-processed sentences in relative to noise-masked sentences. A masker-dependent amplitude over phase advantage was observed when accounting for their relative contributions to the intelligibility advantage of IdBM-processed sentences. Under steady-state speech-spectrum shaped noise, both amplitude- and phase-only IdBM-processed sentences contained intelligibility information close to that contained in amplitude-and-phase IdBM-processed sentences. In contrast, under competing babble masker, amplitude-only IdBM-processed sentences were more intelligible than phase-only IdBM-processed sentences, and neither could account for the intelligibility advantage of amplitude-and-phase IdBM-processed sentences",
    "checked": true,
    "id": "4bcec492c4857b196645876188e707558da7fc9b",
    "semantic_title": "relative contributions of amplitude and phase to the intelligibility advantage of ideal binary masked sentences",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16_interspeech.html": {
    "title": "Predicting Binaural Speech Intelligibility from Signals Estimated by a Blind Source Separation Algorithm",
    "volume": "main",
    "abstract": "State-of-the-art binaural objective intelligibility measures (OIMs) require individual source signals for making intelligibility predictions, limiting their usability in real-time online operations. This limitation may be addressed by a blind source separation (BSS) process, which is able to extract the underlying sources from a mixture. In this study, a speech source is presented with either a stationary noise masker or a fluctuating noise masker whose azimuth varies in a horizontal plane, at two speech-to-noise ratios (SNRs). Three binaural OIMs are used to predict speech intelligibility from the signals separated by a BSS algorithm. The model predictions are compared with listeners' word identification rate in a perceptual listening experiment. The results suggest that with SNR compensation to the BSS-separated speech signal, the OIMs can maintain their predictive power for individual maskers compared to their performance measured from the direct signals. It also reveals that the errors in SNR between the estimated signals are not the only factors that decrease the predictive accuracy of the OIMs with the separated signals. Artefacts or distortions on the estimated signals caused by the BSS algorithm may also be concerns",
    "checked": true,
    "id": "62915790378f12ddeabdbaa1558ec24b7c5974b3",
    "semantic_title": "predicting binaural speech intelligibility from signals estimated by a blind source separation algorithm",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/petkov16_interspeech.html": {
    "title": "Automated Pause Insertion for Improved Intelligibility Under Reverberation",
    "volume": "main",
    "abstract": "Speech intelligibility in reverberant environments is reduced because of overlap-masking. Signal modification prior to presentation in such listening environments, e.g., with a public announcement system, can be employed to alleviate this problem. Time-scale modifications are particularly effective in reducing the effect of overlap-masking. A method for introducing linguistically-motivated pauses is proposed in this paper. Given the transcription of a sentence, pause strengths are predicted at word boundaries. Pause duration is obtained by combining the pause strength and the time it takes late reverberation to decay to a level where a target signal-to-late-reverberation ratio criterion is satisfied. Considering a moderate reverberation condition and both binary and continuous pause strengths, a formal listening test was performed. The results show that the proposed methodology offers a significant intelligibility improvement over unmodified speech while continuous pause strengths offer an advantage over binary pause strengths",
    "checked": true,
    "id": "fa69948cf187861b7266a84c65c4c9049808b3c5",
    "semantic_title": "automated pause insertion for improved intelligibility under reverberation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/rouas16_interspeech.html": {
    "title": "Automatic Classification of Phonation Modes in Singing Voice: Towards Singing Style Characterisation and Application to Ethnomusicological Recordings",
    "volume": "main",
    "abstract": "This paper describes our work on automatic classification of phonation modes on singing voice. In the first part of the paper, we will briefly review the main characteristics of the different phonation modes. Then, we will describe the isolated vowels databases we used, with emphasis on a new database we recorded specifically for the purpose of this work. The next section will be dedicated to the description of the proposed set of parameters (acoustic and glottal) and the classification framework. The results obtained with only acoustic parameters are close to 80% of correct recognition, which seems sufficient for experimenting with continuous singing. Therefore, we set up two other experiments in order to see if the system may be of any practical use for singing voice characterisation. The first experiment aims at assessing if automatic detection of phonation modes may help classify singing into different styles. This experiment is carried out using a database of one singer singing the same song in 8 styles. The second experiment is carried out on field recordings from ethnomusicologists and concerns the distinction between \"normal\" singing and \"laments\" from a variety of countries",
    "checked": true,
    "id": "44332098caa62d119fb9896f021d53cf5dabc79f",
    "semantic_title": "automatic classification of phonation modes in singing voice: towards singing style characterisation and application to ethnomusicological recordings",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bhavsar16_interspeech.html": {
    "title": "Novel Nonlinear Prediction Based Features for Spoofed Speech Detection",
    "volume": "main",
    "abstract": "Several speech synthesis and voice conversion techniques can easily generate or manipulate speech to deceive the speaker verification (SV) systems. Hence, there is a need to develop spoofing countermeasures to detect the human speech from spoofed speech. System-based features have been known to contribute significantly to this task. In this paper, we extend a recent study of Linear Prediction (LP) and Long-Term Prediction (LTP)-based features to LP and Nonlinear Prediction (NLP)-based features. To evaluate the effectiveness of the proposed countermeasure, we use the corpora provided at the ASVspoof 2015 challenge. A Gaussian Mixture Model (GMM)-based classifier is used and the % Equal Error Rate (EER) is used as a performance measure. On the development set, it is found that LP-LTP and LP-NLP features gave an average EER of 4.78% and 9.18%, respectively. Score-level fusion of LP-LTP (and LP-NLP) with Mel Frequency Cepstral Coefficients (MFCC) gave an EER of 0.8% (and 1.37%), respectively. After score-level fusion of LP-LTP, LP-NLP and MFCC features, the EER is significantly reduced to 0.57%. The LP-LTP and LP-NLP features have found to work well even for Blizzard Challenge 2012 speech database",
    "checked": true,
    "id": "2071191e7dbd0571d236c84138f7694a4aa1cfdb",
    "semantic_title": "novel nonlinear prediction based features for spoofed speech detection",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dumpala16_interspeech.html": {
    "title": "Robust Vowel Landmark Detection Using Epoch-Based Features",
    "volume": "main",
    "abstract": "Automatic detection of vowel landmarks is useful in many applications such as automatic speech recognition (ASR), audio search, syllabification of speech and expressive speech processing. In this paper, acoustic features extracted around epochs are proposed for detection of vowel landmarks in continuous speech. These features are based on zero frequency filtering (ZFF) and single frequency filtering (SFF) analyses of speech. Excitation source based features are extracted using ZFF method and vocal tract system based features are extracted using SFF method. Based on these features, a rule-based algorithm is developed for vowel landmark detection (VLD). Performance of the proposed VLD algorithm is studied on three different databases namely, TIMIT (read), NTIMIT (channel degraded) and Switchboard corpus (conversational speech). Results show that the proposed algorithm performs equally well compared to state-of-the-art techniques on TIMIT and better on NTIMIT and Switchboard corpora. Proposed algorithm also displays consistent performance on TIMIT and NTIMIT datasets for different levels of noise degradations",
    "checked": true,
    "id": "8a750b922321c73398ede0b59d9164681e0b986d",
    "semantic_title": "robust vowel landmark detection using epoch-based features",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/toger16_interspeech.html": {
    "title": "Sensitivity of Quantitative RT-MRI Metrics of Vocal Tract Dynamics to Image Reconstruction Settings",
    "volume": "main",
    "abstract": "Real-time Magnetic Resonance Imaging (RT-MRI) is a powerful method for quantitative analysis of speech. Current state-of-the-art methods use constrained reconstruction to achieve high frame rates and spatial resolution. The reconstruction involves two free parameters that can be retrospectively selected: 1) the temporal resolution and 2) the regularization parameter λ, which balances temporal regularization and fidelity to the collected MRI data. In this work, we study the sensitivity of derived quantitative measures of vocal tract function to these two parameters. Specifically, the cross-distance between the tongue tip and the alveolar ridge was investigated for different temporal resolutions (21, 42, 56 and 83 frames per second) and values of the regularization parameter. Data from one subject is included. The phrase ‘one two three four five' was repeated 8 times at a normal pace. The results show that 1) a high regularization factor leads to lower cross-distance values 2) using a low value for the regularization parameter gives poor reproducibility and 3) a temporal resolution of at least 42 frames per second is desirable to achieve good reproducibility for all utterances in this speech task. The process employed here can be generalized to quantitative imaging of the vocal tract and other body parts",
    "checked": true,
    "id": "5177cd78ff774f4542804cb79b233c2b2d4bfe1f",
    "semantic_title": "sensitivity of quantitative rt-mri metrics of vocal tract dynamics to image reconstruction settings",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cernak16_interspeech.html": {
    "title": "Sound Pattern Matching for Automatic Prosodic Event Detection",
    "volume": "main",
    "abstract": "Prosody in speech is manifested by variations of loudness, exaggeration of pitch, and specific phonetic variations of prosodic segments. For example, in the stressed and unstressed syllables, there are differences in place or manner of articulation, vowels in unstressed syllables may have a more central articulation, and vowel reduction may occur when a vowel changes from a stressed to an unstressed position In this paper, we characterize the sound patterns using phonological posteriors to capture the phonetic variations in a concise manner. The phonological posteriors quantify the posterior probabilities of the phonological classes given the input speech acoustics, and they are obtained using the deep neural network (DNN) computational method. Built on the assumption that there are unique sound patterns in different prosodic segments, we devise a sound pattern matching (SPM) method based on 1-nearest neighbour classifier. In this work, we focus on automatic detection of prosodic stress placed on words, called also emphasized words. We evaluate the SPM method on English and French data with emphasized words. The word emphasis detection works very well also on cross-lingual tests, that is using a French classifier on English data, and vice versa",
    "checked": true,
    "id": "c3d5fb06030195c940f537482bca728f0285cd83",
    "semantic_title": "sound pattern matching for automatic prosodic event detection",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shahin16_interspeech.html": {
    "title": "Automatic Classification of Lexical Stress in English and Arabic Languages Using Deep Learning",
    "volume": "main",
    "abstract": "Prosodic features are important for the intelligibility and proficiency of stress-timed languages such as English and Arabic. Producing the appropriate lexical stress is challenging for second language (L2) learners, in particular, those whose first language (L1) is a syllable-timed language such as Spanish, French, etc. In this paper we introduce a method for automatic classification of lexical stress to be integrated into computer-aided pronunciation learning (CAPL) tools for L2 learning. We trained two different deep learning architectures, the deep feedforward neural network (DNN) and the deep convolutional neural network (CNN) using a set of temporal and spectral features related to the intensity, duration, pitch and energies in different frequency bands. The system was applied on both English (kids and adult) and Arabic (adult) speech corpora collected from native speakers. Our method results in error rates of 9%, 7% and 18% when tested on the English children corpus, English adult corpus and Arabic adult corpus respectively",
    "checked": true,
    "id": "a146207998af62d579107570e6882ae761f1e575",
    "semantic_title": "automatic classification of lexical stress in english and arabic languages using deep learning",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16b_interspeech.html": {
    "title": "Development of Mandarin Onset-Rime Detection in Relation to Age and Pinyin Instruction",
    "volume": "main",
    "abstract": "Development of explicit phonological awareness (PA) is thought to be dependent on formal instruction in reading or spelling. However, the development of implicit PA emerges before literacy instruction and interacts with how the phonological representations are constructed within a certain language. The present study systematically investigated the development of implicit PA of Mandarin onset-rime detection in relation to age and Pinyin instruction, involving 70 four- to seven-year-old kindergarten and first-grade children. Results indicated that the overall rate of correct responses in the rime detection task was much higher than that in the onset detection one, with better discrimination ability of larger units. Moreover, the underlying factors facilitating the development of Mandarin onset and rime detection were different, although both correlated positively with Pinyin instruction. On one hand, with age, development of rime detection appeared to develop naturally through spoken language experience before schooling, and was further optimized to the best after Pinyin instruction. On the other hand, the accuracy of onset detection exhibited a drastic improvement, boosting from 66% among preschoolers to 93% among first graders, establishing the primacy of Pinyin instruction responsible for the development of implicit onset awareness in Mandarin",
    "checked": true,
    "id": "7dc4a0ede57955bbae46a7d26a639874675ba520",
    "semantic_title": "development of mandarin onset-rime detection in relation to age and pinyin instruction",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wen16_interspeech.html": {
    "title": "Joint Effect of Dialect and Mandarin on English Vowel Production: A Case Study in Changsha EFL Learners",
    "volume": "main",
    "abstract": "Phonetic acquisition of English as a Foreign Language (EFL) for learners in dialectal areas has been increasingly regarded as an important research area in second language acquisition. However, most existing research has been focused on finding out the transfer effect of dialect on English production from a second language acquisition point of view, but ignores the impact of Mandarin. The present research aims to investigate the joint effect of dialect and Mandarin on Changsha EFL learners' vowel production through acoustic analysis, from both spectral and temporal perspectives. We will further explain the results with the Speech Learning Model (SLM). Three corner vowels, i.e., /a/ /i/ /u/, are studied, and the results show that: English vowels /i/ and /a/ produced by Changsha learners are significantly different from those of American speakers; specifically, /i/ is more affected by Mandarin, and /a/ is more affected by Changsha dialect, which can be explained by SLM. While /u/ produced by Changsha learners is similar to that of American speakers. Besides, Changsha learners produce shorter vowels in duration, due to dialect and Mandarin's transfer effect, but can still make tense-lax contrasts in /i-ɪ/ and /u-ʊ/ pairs",
    "checked": true,
    "id": "4b90d8cc1fa42694aa570c05be1cebbde362031f",
    "semantic_title": "joint effect of dialect and mandarin on english vowel production: a case study in changsha efl learners",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/katayama16_interspeech.html": {
    "title": "Effects of L1 Phonotactic Constraints on L2 Word Segmentation Strategies",
    "volume": "main",
    "abstract": "In the present study, it was examined whether phonotactic constraints of the first language affect speech processing by Japanese learners of English and whether L2 proficiency influences it. Seventeen native English speakers (ES), 18 Japanese speakers with high proficiency of English (JH), and 20 Japanese speakers with relatively low English proficiency (JL) took part in a monitoring task. Two types of target words (CVC/CV, e.g., team/tea) were embedded in bisyllabic non-words (e.g., teamfesh) and given to the participants with other non-words in the lists. The three groups were instructed to respond as soon as they spot targets, and response times and error rates were analyzed. The results showed that all of the groups segmented the CVC target words significantly faster and more accurately than the CV targets. L1 phonotactic constraints did not hinder L2 speech processing, and a word segmentation strategy was not language-specific in the case of Japanese English learners",
    "checked": true,
    "id": "717c0760e89d22773a2f8d5934762c2bd25757ff",
    "semantic_title": "effects of l1 phonotactic constraints on l2 word segmentation strategies",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wottawa16_interspeech.html": {
    "title": "Putting German [ʃ] and [ç] in Two Different Boxes: Native German vs L2 German of French Learners",
    "volume": "main",
    "abstract": "French L2 Learners of German (FG) often replace the palatal fricative /ç/ absent in French with the post alveolar fricative /ʃ/. In our study we investigate which cues can be used to distinguish whether FG speakers produce [ʃ] or [ç] in words with the final syllables /ɪʃ/ or /ɪç/. In literature of German as an L2, to our knowledge, this contrast has not yet been studied. In this perspective, we first compared native German (GG) productions of [/ʃ/] and [ç] to the FG speaker productions. Comparisons concerned the F2 of the preceding vowel, the F2 transition between the preceding vowel and the fricative, the center of gravity and intensity of the fricatives in high and low frequencies. To decide which cues are effectively choices to separate [ʃ] and [ç], the Weka interface in R (RWeka) was used. Results show that for German native speech, the F2 of the preceding vowel and the F2 transition are valid cues to distinguish between [ʃ] and [ç]. For FG speakers these cues are not valid. To distinguish between [ʃ] and [ç] in FG speakers, the intensity of high and low frequencies as well as the center of gravity of the fricatives help to decide whether [ʃ] and [ç] was produced. In German native speech, cues furnished only by the fricative itself can as well be used to distinguish between [ʃ] and [ç]",
    "checked": true,
    "id": "38a9a0f41d07196078d8e254c831f56d7e2be8fb",
    "semantic_title": "putting german [ʃ] and [ç] in two different boxes: native german vs l2 german of french learners",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/luo16_interspeech.html": {
    "title": "Naturalness Judgement of L2 English Through Dubbing Practice",
    "volume": "main",
    "abstract": "This Study investigates how different prosodic features affect native speakers' perception of L2 English spoken by Chinese students through dubbing, or re-voicing practice on video clips. Learning oral foreign language through dubbing on movie or animation clips has become very popular in China. In this practice, learners try to reproduce utterances as closely as possible to the original speech by closely matching lip movements on the clips. The L2 utterances before and after substantial dubbing practices were recorded and categorized according to different prosodic error patterns. Objective acoustic features were extracted and analyzed with naturalness scores based on perceptual experiment. Experimental results show that stress and timing play key roles in native speakers' perception of naturalness. With the practice of dubbing, prosodic features, especially timing, can be considerably improved and thus the naturalness of the reproduced utterances increases",
    "checked": true,
    "id": "7ba65f269c772f8a7b7f0cd0e07e2bfe988a5e69",
    "semantic_title": "naturalness judgement of l2 english through dubbing practice",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shinohara16_interspeech.html": {
    "title": "Audiovisual Training Effects for Japanese Children Learning English /r/-/l/",
    "volume": "main",
    "abstract": "In this study, the effects of audiovisual training were examined for Japanese children learning the English /r/-/l/ contrast. After 10 audiovisual training sessions, participants' improvement in English /r/-/l/ identification in audiovisual, visual-only and audio-only conditions was assessed. The results demonstrated that Japanese children significantly improved in their English /r/-/l/ identification accuracy in all three conditions. Although there was no significant modality effect on identification accuracy at pre test, the participants improved their identification accuracy in the audiovisual condition significantly more than in the audio-only condition. The improvement in the audiovisual condition was not significantly different from that in the visual-only condition. These results suggest that Japanese children can improve their identification accuracy of the English /r/-/l/ contrasts using each of visual and auditory modalities, and they appear to improve their lip-reading skills as much as audiovisual identification. Nonetheless, due to the ceiling effect in their improvement, it is unclear whether Japanese children improved their integrated processing of visual and auditory information",
    "checked": true,
    "id": "c3fdf554872fba26b7336b946e41d3ba815da7a8",
    "semantic_title": "audiovisual training effects for japanese children learning english /r/-/l/",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/harper16_interspeech.html": {
    "title": "L2 Acquisition and Production of the English Rhotic Pharyngeal Gesture",
    "volume": "main",
    "abstract": "This study is an investigation of L2 speakers' production of the pharyngeal gesture in the English /ɹ/. Real-time MRI recordings from one L1 French/L2 English and one L1 Greek/L2 English speaker were analyzed and compared with recordings from a native English speaker to examine whether the gestural composition of the rhotic consonant(s) in a speaker's L1, particularly the presence and location of a pharyngeal gesture, influences their production of English /ɹ/. While the L1 French speaker produced the expected high pharyngeal constriction in their production of the French rhotic, he did not appear to consistently produce an English-like low pharyngeal constriction in his production of English /ɹ/. Similarly, the native Greek speaker did not consistently produce a pharyngeal constriction of any kind in either his L1 rhotic (as expected) or in English /ɹ/. These results suggest that the acquisition and production of the pharyngeal gesture in the English rhotic approximant is particularly difficult for learners whose L1 rhotics lack an identical constriction, potentially due to a general difficulty of acquiring pharyngeal gestures that are not in the L1, the similarity of the acoustic consequences of the different components of a rhotic, or L1 transfer into the L2",
    "checked": true,
    "id": "77a1c77de540aa9deb3d301c23595eb448304ff2",
    "semantic_title": "l2 acquisition and production of the english rhotic pharyngeal gesture",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hennequin16_interspeech.html": {
    "title": "Auditory-Visual Perception of VCVs Produced by People with Down Syndrome: Preliminary Results",
    "volume": "main",
    "abstract": "Down Syndrome (DS) is a genetic disease involving a number of anatomical, physiological and cognitive impairments. More particularly it affects speech production abilities. This results in reduced intelligibility which has however only been evaluated auditorily. Yet, many studies have demonstrated that adding vision to audition helps perception of speech produced by people without impairments especially when it is degraded as is the case in noise. The present study aims at examining whether the visual information improves intelligibility of people with DS. 24 participants without DS were presented with VCV sequences (vowel-consonant-vowel) produced by four adults (2 with DS and 2 without DS). These stimuli were presented in noise in three modalities: auditory, auditory-visual and visual. The results confirm a reduced auditory intelligibility of speakers with DS. They also show that, for the speakers involved in this study, visual intelligibility is equivalent to that of speakers without DS and compensates for the auditory intelligibility loss. An analysis of the perceptual errors shows that most of them involve confusions between consonants. These results put forward the crucial role of multimodality in the improvement of the intelligibility of people with DS",
    "checked": true,
    "id": "596a506bf8ffa769ca8fe18061e508c8deeef927",
    "semantic_title": "auditory-visual perception of vcvs produced by people with down syndrome: preliminary results",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ylmaz16_interspeech.html": {
    "title": "Combining Non-Pathological Data of Different Language Varieties to Improve DNN-HMM Performance on Pathological Speech",
    "volume": "main",
    "abstract": "Research on automatic speech recognition (ASR) of pathological speech is particularly hindered by scarce in-domain data resources. Collecting representative pathological speech data is difficult due to the large variability caused by the nature and severity of the disorders, and the rigorous ethical and medical permission requirements. This task becomes even more challenging for languages which have fewer resources, fewer speakers and fewer patients than English, such as the mid-sized language Dutch. In this paper, we investigate the impact of combining speech data from different varieties of the Dutch language for training deep neural network (DNN)-based acoustic models. Flemish is chosen as the target variety for testing the acoustic models, since a Flemish database of pathological speech, the COPAS database, is available. We use non-pathological speech data from the northern Dutch and Flemish varieties and perform speaker-independent recognition using the DNN-HMM system trained on the combined data. The results show that this system provides improved recognition of pathological Flemish speech compared to a baseline system trained only on Flemish data. These findings open up new opportunities for developing useful ASR-based pathological speech applications for languages that are smaller in size and less resourced than English",
    "checked": true,
    "id": "2f47779bd5083583841c6642031efe11f14037f5",
    "semantic_title": "combining non-pathological data of different language varieties to improve dnn-hmm performance on pathological speech",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/laaridh16_interspeech.html": {
    "title": "Evaluation of a Phone-Based Anomaly Detection Approach for Dysarthric Speech",
    "volume": "main",
    "abstract": "Perceptual evaluation is still the most common method in clinical practice for the diagnosing and the following of the condition progression of people with speech disorders. Many automatic approaches were proposed to provide objective tools to deal with speech disorders and help professionals in the severity evaluation of speech impairments. This paper investigates an automatic phone-based anomaly detection approach implying an automatic text-constrained phone alignment. Here, anomalies are related to speech segments, for which an unexpected acoustic pattern is observed, compared with a normal speech production. This objective tool is applied to French dysarthric speech recordings produced by patients suffering from four different pathologies. The behavior of the anomaly detection approach is studied according to the precision of the automatic phone alignment. Faced with the difficulties of having a gold standard reference, especially for the phone-based anomaly annotation, this behavior is observed on both annotated and non-annotated corpora. As expected, alignment errors (large shifts compared with a manual segmentation) lead to a large amount of anomalies automatically detected. However, about 50% of correctly detected anomalies are not related to alignment errors. This behavior shows that the automatic approach is able to catch irregular acoustic patterns of phones",
    "checked": true,
    "id": "b72a350509857093b900b5b29d2638d04b905f6c",
    "semantic_title": "evaluation of a phone-based anomaly detection approach for dysarthric speech",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bhat16_interspeech.html": {
    "title": "Recognition of Dysarthric Speech Using Voice Parameters for Speaker Adaptation and Multi-Taper Spectral Estimation",
    "volume": "main",
    "abstract": "Dysarthria is a motor speech disorder resulting from impairment in muscles responsible for speech production, often characterized by slurred or slow speech resulting in low intelligibility. With speech based applications such as voice biometrics and personal assistants gaining popularity, automatic recognition of dysarthric speech becomes imperative as a step towards including people with dysarthria into mainstream. In this paper we examine the applicability of voice parameters that are traditionally used for pathological voice classification such as jitter, shimmer, F0 and Noise Harmonic Ratio (NHR) contour in addition to Mel Frequency Cepstral Coefficients (MFCC) for dysarthric speech recognition. Additionally, we show that multi-taper spectral estimation for computing MFCC improves the unseen dysarthric speech recognition. A Deep neural network (DNN) - hidden Markov model (HMM) recognition system fared better than a Gaussian Mixture Model (GMM) - HMM based system for dysarthric speech recognition. We propose a method to optimally use incremental dysarthric data to improve dysarthric speech recognition for an ASR with DNN-HMM. All evaluations were done on Universal Access Speech Corpus",
    "checked": true,
    "id": "000e78f18b73e6e0bffe143d7921f83509197b82",
    "semantic_title": "recognition of dysarthric speech using voice parameters for speaker adaptation and multi-taper spectral estimation",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16c_interspeech.html": {
    "title": "Impaired Categorical Perception of Mandarin Tones and its Relationship to Language Ability in Autism Spectrum Disorders",
    "volume": "main",
    "abstract": "While enhanced pitch processing appears to be characteristic of many individuals with autism spectrum disorders (ASD), it remains unclear whether enhancement in pitch perception applies to those who speak a tone language. Using a classic paradigm of categorical perception (CP), the present study investigated the perception of Mandarin tones in six- to eight-year-old children with ASD, and compared it with age-matched typically developing children. In stark contrast to controls, the child participants with ASD exhibited a much wider boundary width (i.e., more gentle slope), and showed no improved discrimination for pairs straddling the boundary, indicating impaired CP of Mandarin tones. Moreover, identification skills of different tone categories were positively correlated with language ability among children with ASD. These findings revealed aberrant tone processing in Mandarin-speaking individuals with ASD, especially in those with significant language impairment. Our results are in support of the notion of impaired change detection for the linguistic elements of speech in children with ASD",
    "checked": true,
    "id": "0ea58ae5443a7acfadac60bdfb4b4bc038c23954",
    "semantic_title": "impaired categorical perception of mandarin tones and its relationship to language ability in autism spectrum disorders",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nagle16_interspeech.html": {
    "title": "Perceived Naturalness of Electrolaryngeal Speech Produced Using sEMG-Controlled vs. Manual Pitch Modulation",
    "volume": "main",
    "abstract": "Producing speech with natural prosodic patterns is an ongoing challenge for users of electrolaryngeal (EL) speech. This study describes speech produced using a method currently in development, wherein a prosodic pattern is derived from skin surface electromyographical (sEMG) signals recorded from under the chin (submental surface) Eight laryngectomees who currently use a TruTone EL as their primary or backup mode of speech provided samples of EL speech in two modes: conventional thumb-pressure pitch-modulated control (represented by the TruTone EL; Griffin Laboratories, CA, U.S.A.) and sEMG-based pitch-modulated control (EMG-EL). Ratings of perceived naturalness were obtained from ten listeners unfamiliar with EL speech Listener ratings indicated that five speakers produced equally natural speech using both devices, and three produced significantly more natural speech using the EMG-EL than the TruTone EL. Mean fundamental frequency (f0) was similar within speakers for both modes; however, mean f0 range and standard deviation were significantly larger for the EMG-EL than for the TruTone EL, despite both devices having similar potential f0 range. This study showed that the EMG-EL provides an intuitive means of controlling f0-based prosodic patterns that are more natural-sounding than push-button control for some EL users",
    "checked": true,
    "id": "b61c0638e7c8d4d4eed490f51b927e1d757e30b6",
    "semantic_title": "perceived naturalness of electrolaryngeal speech produced using semg-controlled vs. manual pitch modulation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/najnin16_interspeech.html": {
    "title": "Identifying Hearing Loss from Learned Speech Kernels",
    "volume": "main",
    "abstract": "Does a hearing-impaired individual's speech reflect his hearing loss? To investigate this question, we recorded at least four hours of speech data from each of 29 adult individuals, both male and female, belonging to four classes: 3 normal, and 26 severely-to-profoundly hearing impaired with high, medium or low speech intelligibility. Acoustic kernels were learned for each individual by capturing the distribution of his speech data points represented as 20 ms duration windows. These kernels were evaluated using a set of neurophysiological metrics, namely, distribution of characteristic frequencies, equal loudness contour, bandwidth and Q value of tuning curve. It turns out that, for our cohort, a feature vector can be constructed out of four properties of these metrics that would accurately classify hearing-impaired individuals with low intelligible speech from normal ones using a linear classifier. However, the overlap in the feature space between normal and hearing-impaired individuals increases as the speech becomes more intelligible. We conclude that a hearing-impaired individual's speech does reflect his hearing loss provided his loss of hearing has considerably affected the intelligibility of his speech",
    "checked": true,
    "id": "b9b9205435751f0784359222fb077a1754b2987f",
    "semantic_title": "identifying hearing loss from learned speech kernels",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/rong16_interspeech.html": {
    "title": "Differential Effects of Velopharyngeal Dysfunction on Speech Intelligibility During Early and Late Stages of Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "The detrimental effects of velopharyngeal dysfunction (VPD) on speech intelligibility in persons with progressive motor speech disorders are poorly understood. In this study, we longitudinally investigated the velopharyngeal and articulatory performance of 142 individuals with varying severities of amyotrophic lateral sclerosis (ALS). Our goal was to determine the mechanisms that underlie the effects of VPD on speech intelligibility during early and late stages of ALS progression. We found that during the early stages of the disease, the effect of VPD on intelligibility was partially mitigated by an increase in articulatory (e.g., lower lip and jaw) movement speed. This apparent articulatory compensation eventually became unavailable during the late stages of disease progression, which led to rapid declines of speech intelligibility. The transition across the early and late stages was characterized by the slowing of the composite movement of lower lip and jaw below 138 mm/s, which indicated the onset of precipitous speech decline and thus, may provide important timing information for helping clinicians to plan interventions",
    "checked": true,
    "id": "876b12677c96c2fc36c56c82d8bfaf4be0fff4d7",
    "semantic_title": "differential effects of velopharyngeal dysfunction on speech intelligibility during early and late stages of amyotrophic lateral sclerosis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/delvaux16_interspeech.html": {
    "title": "The Production of Intervocalic Glides in Non Dysarthric Parkinsonian Speech",
    "volume": "main",
    "abstract": "In the context of a research project aiming at investigating the relationships between speech disorders, quality of life and social participation in Parkinson's Disease (PD), we report here on an acoustic study of glides and steady vowels by non dysarthric parkinsonian and control speakers. Our specific aim is to explore the dynamics of supra-laryngeal articulators in PD. Results suggest that non dysarthric Parkinsonian speakers maintain an accurate production of glides in VC[glide]V pseudo-words at the expense of articulatory undershoot in the surrounding vowels, and some asymmetry between the V1-to-glide and glide-to-V2 articulatory movements. We discuss how these results both support and challenge the accuracy-tempo trade-off hypothesis (Ackermann and Ziegler, 1991)",
    "checked": true,
    "id": "2df58b4cca0072d7d8b1d587984501b174b1e7b4",
    "semantic_title": "the production of intervocalic glides in non dysarthric parkinsonian speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/feng16_interspeech.html": {
    "title": "Auditory Processing Impairments Under Background Noise in Children with Non-Syndromic Cleft Lip and/or Palate",
    "volume": "main",
    "abstract": "Cleft lip and/or palate (CL/P) disorders are commonly occurring congenital malformations and hearing impairment is a very common co-morbidity. Most previous research has only focused on middle ear disorders and related auditory consequences in this group. Studies of higher level auditory status and central auditory processing abilities of this group have been unsystematic. The present study was conducted in order to objectively investigate the central auditory abilities in children with non-syndromic cleft lip and/or palate (NSCLP). A structured behavioral central auditory test battery was conducted in a group of children with NSCLP and their age/sex matched normal peers. The following behavioral central auditory tasks were undertaken, including hearing in noise test (HINT), dichotic digits test (DDT), and gaps in noise test (GIN). Results showed that there were no significant group differences in DDT test, indicating that the binaural separation and integration abilities could be normal in children with NSCLP. However, the cleft group performed significantly poorer than their normal peers for each ear in HINT test under noise condition and GIN test, suggesting that the children with NSCLP could have impaired monaural low redundancy auditory processing ability, and at risk of temporal resolution disability",
    "checked": true,
    "id": "55b54ce888ded10b4e5b6e70869fffd81fbb58be",
    "semantic_title": "auditory processing impairments under background noise in children with non-syndromic cleft lip and/or palate",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhu16_interspeech.html": {
    "title": "Modulation Spectral Features for Predicting Vocal Emotion Recognition by Simulated Cochlear Implants",
    "volume": "main",
    "abstract": "It has been reported that vocal emotion recognition is challenging for cochlear implant (CI) listeners due to the limited spectral cues with CI devices. As the mechanism of CI, modulation information is provided as a primarily cue. Previous studies have revealed that the modulation components of speech are important for speech intelligibility. However, it is unclear whether modulation information can contribute to vocal emotion recognition. We investigated the relationship between human perception of vocal emotion and the modulation spectral features of emotional speech. For human perception, we carried out a vocal-emotion recognition experiment using noise-vocoder simulations with normal-hearing listeners to predict the response from CI listeners. For modulation spectral features, we used auditory-inspired processing (auditory filterbank, temporal envelope extraction, modulation filterbank) to obtain the modulation spectrogram of emotional speech signals. Ten types of modulation spectral feature were then extracted from the modulation spectrogram. As a result, modulation spectral centroid, modulation spectral kurtosis, and modulation spectral tilt exhibited similar trends with the results of human perception. This suggests that these modulation spectral features may be important cues for voice emotion recognition with noise-vocoded speech",
    "checked": true,
    "id": "bca1cf47d0729d824a7d9454189021f6077f7dce",
    "semantic_title": "modulation spectral features for predicting vocal emotion recognition by simulated cochlear implants",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ochi16_interspeech.html": {
    "title": "Automatic Discrimination of Soft Voice Onset Using Acoustic Features of Breathy Voicing",
    "volume": "main",
    "abstract": "Soft onset vocalization is used in certain speech therapies. However, it is not easy to practice it at home because the acoustical evaluation itself needs training. It would be helpful for speech patients to get objective feedback during training. In this paper, new parameters for identifying soft onset with high accuracy are described. One of the parameters measures an aspect of the soft voice onset, in which the vocal folds start to oscillate periodically before coming in contact with each other at the beginning of vocalization. Combined with an onset time exceeding a threshold, the proposed parameters gave about 99% accuracy in identifying soft onset vocalization",
    "checked": true,
    "id": "0c1f0e68c2677c830c7008c6b3c44a04716e7f7b",
    "semantic_title": "automatic discrimination of soft voice onset using acoustic features of breathy voicing",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shao16_interspeech.html": {
    "title": "Effect of Noise on Lexical Tone Perception in Cantonese-Speaking Amusics",
    "volume": "main",
    "abstract": "Congenital amusia is a neurogenetic disorder affecting musical pitch processing. It also affects lexical tone perception. It is well documented that noisy conditions impact speech perception in second language learners and cochlear implant users. However, it is yet unclear whether and how noise affects lexical tone perception in the amusics. This paper examined the effect of multi-talker babble noise [1] on lexical tone identification and discrimination in 14 Cantonese-speaking amusics and 14 controls at three levels of signal-to-noise ratio (SNR). Results reveal that the amusics were less accurate in the identification of tones compared to controls in all SNR conditions. They also showed degraded performance in the discrimination, but less severe than in the identification. These results confirmed that amusia influences lexical tone processing. But the amusics were not influenced more by noise than the controls in either identification or discrimination. This indicates that the deficits of amusia may not be due to the lack of native-like language processing mechanisms or are mechanical in nature, as in the case of second language learners and cochlear implant users. Instead, the amusics may be impaired in the linguistic processing of native tones, showing impaired tone perception already under the clear condition",
    "checked": true,
    "id": "42fc7c3464815fb7bc67af5851d576956ee6b3d8",
    "semantic_title": "effect of noise on lexical tone perception in cantonese-speaking amusics",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2016/takashima16_interspeech.html": {
    "title": "Audio-Visual Speech Recognition Using Bimodal-Trained Bottleneck Features for a Person with Severe Hearing Loss",
    "volume": "main",
    "abstract": "In this paper, we propose an audio-visual speech recognition system for a person with an articulation disorder resulting from severe hearing loss. In the case of a person with this type of articulation disorder, the speech style is quite different from those of people without hearing loss that a speaker-independent acoustic model for unimpaired persons is hardly useful for recognizing it. The audio-visual speech recognition system we present in this paper is for a person with severe hearing loss in noisy environments. Although feature integration is an important factor in multimodal speech recognition, it is difficult to integrate efficiently because those features are different intrinsically. We propose a novel visual feature extraction approach that connects the lip image to audio features efficiently, and the use of convolutive bottleneck networks (CBNs) increases robustness with respect to speech fluctuations caused by hearing loss. The effectiveness of this approach was confirmed through word-recognition experiments in noisy environments, where the CBN-based feature extraction method outperformed the conventional methods",
    "checked": true,
    "id": "563ed3af254a5ef4549a34155718df772ba7b8b6",
    "semantic_title": "audio-visual speech recognition using bimodal-trained bottleneck features for a person with severe hearing loss",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gu16_interspeech.html": {
    "title": "Perception of Tone in Whispered Mandarin Sentences: The Case for Singapore Mandarin",
    "volume": "main",
    "abstract": "Whispering is commonly used when one needs to speak softly (for instance, in a library). Whispered speech mainly differs from neutral speech in that voicing, and thus its acoustic correlate F0, is absent. It is well known that in tonal languages such as Mandarin, tone identity is primarily conveyed by the F0 contour. Previous works also suggest that secondary correlates are both consistent and sufficient to convey Mandarin tone in whisper. However, these results are focused on Standard Mandarin spoken in Mainland China and have only been obtained via small-scale experiments using citation-form speech. To investigate whether these results will carry over to continuous sentences in other variations of Mandarin, we present a study that is the first of its nature to explore native Singapore Mandarin. Unlike related works, our large-scale perceptual experiment thoroughly investigates lexical tones in whispered and neutral Mandarin by involving more diverse speech data, greater number of listeners and use syllables excised from continuous speech to better simulate natural speech conditions. Our findings differ significantly from earlier works in terms of the recognition patterns observed. We present further in-depth analysis on how various phonetic characteristics (vowel contexts, place and manner of articulation) affect whispered tone perception",
    "checked": true,
    "id": "242edfa04172453d09f5f407d9c8d6ef1ab29d39",
    "semantic_title": "perception of tone in whispered mandarin sentences: the case for singapore mandarin",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xie16_interspeech.html": {
    "title": "A KL Divergence and DNN-Based Approach to Voice Conversion without Parallel Training Sentences",
    "volume": "main",
    "abstract": "We extend our recently proposed approach to cross-lingual TTS training to voice conversion, without using parallel training sentences. It employs Speaker Independent, Deep Neural Net (SI-DNN) ASR to equalize the difference between source and target speakers and Kullback-Leibler Divergence (KLD) to convert spectral parameters probabilistically in the phonetic space via ASR senone posterior probabilities of the two speakers. With or without knowing the transcriptions of the target speaker's training speech, the approach can be either supervised or unsupervised. In a supervised mode, where adequate training data of the target speaker with transcriptions is used to train a GMM-HMM TTS of the target speaker, each frame of the source speakers input data is mapped to the closest senone in thus trained TTS. The mapping is done via the posterior probabilities computed by SI-DNN ASR and the minimum KLD matching. In a unsupervised mode, all training data of the target speaker is first grouped into phonetic clusters where KLD is used as the sole distortion measure. Once the phonetic clusters are trained, each frame of the source speakers input is then mapped to the mean of the closest phonetic cluster. The final converted speech is generated with the max probability trajectory generation algorithm. Both objective and subjective evaluations show the proposed approach can achieve higher speaker similarity and better spectral distortions, when comparing with the baseline system based upon our sequential error minimization trained DNN algorithm",
    "checked": true,
    "id": "a57737009573d417871ccbb7229c0ecc15c7091b",
    "semantic_title": "a kl divergence and dnn-based approach to voice conversion without parallel training sentences",
    "citation_count": 72
  },
  "https://www.isca-speech.org/archive/interspeech_2016/aihara16_interspeech.html": {
    "title": "Parallel Dictionary Learning for Voice Conversion Using Discriminative Graph-Embedded Non-Negative Matrix Factorization",
    "volume": "main",
    "abstract": "This paper proposes a discriminative learning method for Non-negative Matrix Factorization (NMF)-based Voice Conversion (VC). NMF-based VC has been researched because of the natural-sounding voice it produces compared with conventional Gaussian Mixture Model (GMM)-based VC. In conventional NMF-based VC, parallel exemplars are used as the dictionary; therefore, dictionary learning is not adopted. In order to enhance the conversion quality of NMF-based VC, we propose Discriminative Graph-embedded Non-negative Matrix Factorization (DGNMF). Parallel dictionaries of the source and target speakers are discriminatively estimated by using DGNMF based on the phoneme labels of the training data. Experimental results show that our proposed method can not only improve the conversion quality but also reduce the computational times",
    "checked": true,
    "id": "b068ba19ea3bd474af8271f1c5841bf7613a229b",
    "semantic_title": "parallel dictionary learning for voice conversion using discriminative graph-embedded non-negative matrix factorization",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gu16b_interspeech.html": {
    "title": "Speech Bandwidth Extension Using Bottleneck Features and Deep Recurrent Neural Networks",
    "volume": "main",
    "abstract": "This paper presents a novel method for speech bandwidth extension (BWE) using deep structured neural networks. In order to utilize linguistic information during the prediction of high-frequency spectral components, the bottleneck (BN) features derived from a deep neural network (DNN)-based state classifier for narrowband speech are employed as auxiliary input. Furthermore, recurrent neural networks (RNNs) incorporating long short-term memory (LSTM) cells are adopted to model the complex mapping relationship between the feature sequences describing low-frequency and high-frequency spectra. Experimental results show that the BWE method proposed in this paper can achieve better performance than the conventional method based on Gaussian mixture models (GMMs) and the state-of-the-art approach based on DNNs in both objective and subjective tests",
    "checked": true,
    "id": "b49803adbc7c7c45fa3fc54091652156dc1ac3a4",
    "semantic_title": "speech bandwidth extension using bottleneck features and deep recurrent neural networks",
    "citation_count": 34
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yang16b_interspeech.html": {
    "title": "Voice Conversion Based on Matrix Variate Gaussian Mixture Model Using Multiple Frame Features",
    "volume": "main",
    "abstract": "This paper presents a novel voice conversion method based on matrix variate Gaussian mixture model (MV-GMM) using features of multiple frames. In voice conversion studies, approaches based on Gaussian mixture models (GMM) are still widely utilized because of their flexibility and easiness in handling. They treat the joint probability density function (PDF) of feature vectors from source and target speakers as that of joint vectors of the two vectors. Addition of dynamic features to the feature vectors in GMM-based approaches achieves certain performance improvements because the correlation between multiple frames is taken into account. Recently, a voice conversion framework based on MV-GMM, in which the joint PDF is modeled in a matrix variate space, has been proposed and it is able to precisely model both the characteristics of the feature spaces and the relation between the source and target speakers. In this paper, in order to additionally model the correlation between multiple frames in the framework more consistently, MV-GMM is constructed in a matrix variate space containing the features of neighboring frames. Experimental results show that an certain performance improvement in both objective and subjective evaluations is observed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hosaka16_interspeech.html": {
    "title": "Voice Conversion Based on Trajectory Model Training of Neural Networks Considering Global Variance",
    "volume": "main",
    "abstract": "This paper proposes a new training method of deep neural networks (DNNs) for statistical voice conversion. DNNs are now being used as conversion models that represent mapping from source features to target features in statistical voice conversion. However, there are two major problems to be solved in conventional DNN-based voice conversion: 1) the inconsistency between the training and synthesis criteria, and 2) the over-smoothing of the generated parameter trajectories. In this paper, we introduce a parameter trajectory generation process considering the global variance (GV) into the training of DNNs for voice conversion. A consistent framework using the same criterion for both training and synthesis provides better conversion accuracy in the original static feature domain, and the over-smoothing can be avoided by optimizing the DNN parameters on the basis of the trajectory likelihood considering the GV. Experimental results show that the proposed method outperforms the DNN-based method in term of both speech quality and speaker similarity",
    "checked": true,
    "id": "9dfddf37381e3dfab38f3bb0ef86916367e2d128",
    "semantic_title": "voice conversion based on trajectory model training of neural networks considering global variance",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/aryal16_interspeech.html": {
    "title": "Comparing Articulatory and Acoustic Strategies for Reducing Non-Native Accents",
    "volume": "main",
    "abstract": "This article presents an experimental comparison of two types of techniques, articulatory and acoustic, for transforming non-native speech to sound more native-like. Articulatory techniques use articulators from a native speaker to drive an articulatory synthesizer of the non-native speaker. These methods have a good theoretical justification, but articulatory measurements (e.g., via electromagnetic articulography) are difficult to obtain. In contrast, acoustic methods use techniques from the voice conversion literature to build a mapping between the two acoustic spaces, making them more attractive for practical applications (e.g., language learning). We compare two representative implementations of these approaches, both based on statistical parametric speech synthesis. Through a series of perceptual listening tests, we evaluate the two approaches in terms of accent reduction, speech intelligibility and speaker quality. Our results show that the acoustic method is more effective than the articulatory method in reducing perceptual ratings of non-native accents, and also produces synthesis of higher intelligibility while preserving voice quality",
    "checked": true,
    "id": "b470cf03a5cd098cf1880a60685dcc85744cf213",
    "semantic_title": "comparing articulatory and acoustic strategies for reducing non-native accents",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sarfjoo16_interspeech.html": {
    "title": "Cross-Lingual Speaker Adaptation for Statistical Speech Synthesis Using Limited Data",
    "volume": "main",
    "abstract": "Cross-lingual speaker adaptation with limited adaptation data has many applications such as use in speech-to-speech translation systems. Here, we focus on cross-lingual adaptation for statistical speech synthesis (SSS) systems using limited adaptation data. To that end, we propose two techniques exploiting a bilingual Turkish-English speech database that we collected. In one approach, speaker-specific state-mapping is proposed for cross-lingual adaptation which performed significantly better than the baseline state-mapping algorithm in adapting the excitation parameter both in objective and subjective tests. In the second approach, eigenvoice adaptation is done in the input language which is then used to estimate the eigenvoice weights in the output language using weighted linear regression. The second approach performed significantly better than the baseline system in adapting the spectral envelope parameters both in objective and subjective tests",
    "checked": true,
    "id": "6c80c6b9101fbb153de2956bca38fcb821d11dc4",
    "semantic_title": "cross-lingual speaker adaptation for statistical speech synthesis using limited data",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sun16_interspeech.html": {
    "title": "Personalized, Cross-Lingual TTS Using Phonetic Posteriorgrams",
    "volume": "main",
    "abstract": "We present a novel approach that enables a target speaker (e.g. monolingual Chinese speaker) to speak a new language (e.g. English) based on arbitrary textual input. Our system includes a trained English speaker-independent automatic speech recognition (SI-ASR) engine using TIMIT. Given the target speaker's speech in a non-target language, we generate Phonetic PosteriorGrams (PPGs) with the SI-ASR and then train a Deep Bidirectional Long Short-Term Memory based Recurrent Neural Networks (DBLSTM) to model the relationships between the PPGs and the acoustic signal. Synthesis involves input of arbitrary text to a general TTS engine (trained on any non-target speaker), the output of which is indexed by SI-ASR as PPGs. These are used by the DBLSTM to synthesize the target language in the target speaker's voice. A main advantage of this approach has very low training data requirement of the target speaker which can be in any language, as compared with a reference approach of training a special TTS engine using many recordings from the target speaker only in the target language. For a given target speaker, our proposed approach trained on 100 Mandarin (i.e. non-target language) utterances achieves comparable performance (in MOS and ABX test) of English synthetic speech as an HTS system trained on 1,000 English utterances",
    "checked": true,
    "id": "2ab50b5e722d61bcd2b379870758b70e5e2f21ec",
    "semantic_title": "personalized, cross-lingual tts using phonetic posteriorgrams",
    "citation_count": 53
  },
  "https://www.isca-speech.org/archive/interspeech_2016/prakash16_interspeech.html": {
    "title": "Acoustic Analysis of Syllables Across Indian Languages",
    "volume": "main",
    "abstract": "Indian languages are broadly classified as Indo-Aryan or Dravidian. The basic set of phones is more or less the same, varying mostly in the phonotactics across languages. There has also been borrowing of sounds and words across languages over time due to intermixing of cultures. Since syllables are fundamental units of speech production and Indian languages are characterised by syllable-timed rhythm, acoustic analysis of syllables has been carried out In this paper, instances of common and most frequent syllables in continuous speech have been studied across six Indian languages, from both Indo-Aryan and Dravidian language groups. The distributions of acoustic features have been compared across these languages. This kind of analysis is useful for developing speech technologies in a multilingual scenario. Owing to similarities in the languages, text-to-speech (TTS) synthesisers have been developed by segmenting speech data at the phone level using hidden Markov models (HMM) from other languages as initial models. Degradation mean opinion scores and word error rates indicate that the quality of synthesised speech is comparable to that of TTSes developed by segmenting the data using language-specific HMMs",
    "checked": true,
    "id": "4219e4f0dfa4b1e22820c49e9920fc2e9f418b62",
    "semantic_title": "acoustic analysis of syllables across indian languages",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16_interspeech.html": {
    "title": "Objective Evaluation Methods for Chinese Text-To-Speech Systems",
    "volume": "main",
    "abstract": "To objectively evaluate the performance of text-to-speech (TTS) systems, many studies have been conducted in the straightforward way to compare synthesized speech and natural speech with the alignment. However, in most situations, there is no natural speech can be used. In this paper, we focus on machine learning approaches for the TTS evaluation. We exploit a subspace decomposition method to separate different components in speech, which generates distinctive acoustic features automatically. Furthermore, a pairwise based Support Vector Machine (SVM) model is used to evaluate TTS systems. With the original prosodic acoustic features and Support Vector Regression model, we obtain a ranking relevance of 0.7709. Meanwhile, with the proposed oblique matrix projection method and pairwise SVM model, we achieve a much better result of 0.9115",
    "checked": true,
    "id": "338b340fa3bd3b246005bbe22dcdac7ca9b612a3",
    "semantic_title": "objective evaluation methods for chinese text-to-speech systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ijima16_interspeech.html": {
    "title": "Objective Evaluation Using Association Between Dimensions Within Spectral Features for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "This paper presents a novel objective evaluation technique for statistical parametric speech synthesis. One of its novel features is that it focuses on the association between dimensions within the spectral features. We first use a maximal information coefficient to analyze the relationship between subjective scores and associations of spectral features obtained from natural and various types of synthesized speech. The analysis results indicate that the scores improve as the association becomes weaker. We then describe the proposed objective evaluation technique, which uses a voice conversion method to detect the associations within spectral features. We perform subjective and objective experiments to investigate the relationship between subjective scores and objective scores. The proposed objective scores are compared to the mel-cepstral distortion. The results indicate that our objective scores achieve dramatically higher correlation to subjective scores than the mel-cepstral distortion",
    "checked": true,
    "id": "9ddf61cbc19635fdc7ca3e148cb172a5bf4c6605",
    "semantic_title": "objective evaluation using association between dimensions within spectral features for statistical parametric speech synthesis",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yoshimura16_interspeech.html": {
    "title": "A Hierarchical Predictor of Synthetic Speech Naturalness Using Neural Networks",
    "volume": "main",
    "abstract": "A problem when developing and tuning speech synthesis systems is that there is no well-established method of automatically rating the quality of the synthetic speech. This research attempts to obtain a new automated measure which is trained on the result of large-scale subjective evaluations employing many human listeners, i.e., the Blizzard Challenge. To exploit the data, we experiment with linear regression, feed-forward and convolutional neural network models, and combinations of them to regress from synthetic speech to the perceptual scores obtained from listeners. The biggest improvements were seen when combining stimulus- and system-level predictions",
    "checked": true,
    "id": "64338a3cd75118ba945b9c341b66c92867600731",
    "semantic_title": "a hierarchical predictor of synthetic speech naturalness using neural networks",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2016/podsiado16_interspeech.html": {
    "title": "Text-to-Speech for Individuals with Vision Loss: A User Study",
    "volume": "main",
    "abstract": "Individuals with vision loss use text-to-speech (TTS) for most of their interaction with devices, and rely on the quality of synthetic voices to a much larger extent than any other user group. A significant amount of local synthesis requests for Google TTS comes from TalkBack, the Android screenreader, making it our top client and making the visually-impaired users the heaviest consumers of the technology. Despite this, very little attention has been devoted to optimizing TTS voices for this user group and the feedback on TTS voices from the blind has been traditionally less-favourable. We present the findings from a TTS user experience study conducted by Google with visually-impaired screen reader users. The study comprised 14 focus groups and evaluated a total of 95 candidate voices with 90 participants across 3 countries. The study uncovered the distinctive usage patterns of this user group, which point to different TTS requirements and voice preferences from those of sighted users",
    "checked": true,
    "id": "4b005fccf4f0ece0a53bbeaf920a3d00cd5e1b42",
    "semantic_title": "text-to-speech for individuals with vision loss: a user study",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/valentinibotinhao16_interspeech.html": {
    "title": "Speech Enhancement for a Noise-Robust Text-to-Speech Synthesis System Using Deep Recurrent Neural Networks",
    "volume": "main",
    "abstract": "Quality of text-to-speech voices built from noisy recordings is diminished. In order to improve it we propose the use of a recurrent neural network to enhance acoustic parameters prior to training. We trained a deep recurrent neural network using a parallel database of noisy and clean acoustics parameters as input and output of the network. The database consisted of multiple speakers and diverse noise conditions. We investigated using text-derived features as an additional input of the network. We processed a noisy database of two other speakers using this network and used its output to train an HMM acoustic text-to-synthesis model for each voice. Listening experiment results showed that the voice built with enhanced parameters was ranked significantly higher than the ones trained with noisy speech and speech that has been enhanced using a conventional enhancement system. The text-derived features improved results only for the female voice, where it was ranked as highly as a voice trained with clean speech",
    "checked": true,
    "id": "710a3b77d317d18d3876d0f187de73a0d9deaba4",
    "semantic_title": "speech enhancement for a noise-robust text-to-speech synthesis system using deep recurrent neural networks",
    "citation_count": 81
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cooper16_interspeech.html": {
    "title": "Data Selection and Adaptation for Naturalness in HMM-Based Speech Synthesis",
    "volume": "main",
    "abstract": "We describe experiments in building HMM text-to-speech voices on professional broadcast news data from multiple speakers. We build on earlier work comparing techniques for selecting utterances from the corpus and voice adaptation to produce the most natural-sounding voices. While our ultimate goal is to develop intelligible and natural-sounding synthetic voices in low-resource languages rapidly and without the expense of collecting and annotating data specifically for text-to-speech, we focus on English initially, in order to develop and evaluate our methods. We evaluate our approaches using crowdsourced listening tests for naturalness. We have found that removing utterances that are outliers with respect to hyper-articulation, as well as combining the selection of hypo-articulated utterances and low mean f0 utterances, produce the most natural-sounding voices",
    "checked": true,
    "id": "094c75373f23f77a123d303de0b96c27f75b5cf2",
    "semantic_title": "data selection and adaptation for naturalness in hmm-based speech synthesis",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tao16_interspeech.html": {
    "title": "A Portable Automatic PA-TA-KA Syllable Detection System to Derive Biomarkers for Neurological Disorders",
    "volume": "main",
    "abstract": "Neurological disorders disrupt brain functions, affecting the life of many individuals. Conventional neurological disorder diagnosis methods require inconvenient and expensive devices. Several studies have identified speech biomarkers that are informative of neurological disorders, so speech-based interfaces can provide effective, convenient and affordable prescreening tools for diagnosis. We have investigated stand-alone automatic speech-based assessment tools for portable devices. Our current data collection protocol includes seven brief tests for which we have developed specialized automatic speech recognition (ASR) systems. The most challenging task from an ASR perspective is a popular diadochokinetic test consisting of fast repetitions of \"PA-TA-KA\", where subjects tend to alter, replace, insert or skip syllables. This paper presents our efforts to build a speech-based application specific for this task, where the computation is fast, efficient, and accurate on a portable device, not in the cloud. The tool recognizes the target syllables, providing phonetic alignment. This information is crucial to reliably estimate biomarkers such as the number of repetitions, insertions, mispronunciations, and temporal prosodic structure of the repetitions. We train and evaluate the application for two neurological disorders: traumatic brain injuries (TBIs) and Parkinson's disease. The results show low syllable error rates and high boundary detection, across populations",
    "checked": true,
    "id": "a54a3cc478e68f366c9eb66f137a843c36e3093a",
    "semantic_title": "a portable automatic pa-ta-ka syllable detection system to derive biomarkers for neurological disorders",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ghahabi16_interspeech.html": {
    "title": "Deep Neural Networks for i-Vector Language Identification of Short Utterances in Cars",
    "volume": "main",
    "abstract": "This paper is focused on the application of the Language Identification (LID) technology for intelligent vehicles. We cope with short sentences or words spoken in moving cars in four languages: English, Spanish, German, and Finnish. As the response time of the LID system is crucial for user acceptance in this particular task, speech signals of different durations with total average of 3.8s are analyzed. In this paper, the authors propose the use of Deep Neural Networks (DNN) to model effectively the i-vector space of languages. Both raw i-vectors and session variability compensated i-vectors are evaluated as input vectors to DNNs. The performance of the proposed DNN architecture is compared with both conventional GMM-UBM and i-vector/LDA systems considering the effect of durations of signals. It is shown that the signals with durations between 2 and 3s meet the requirements of this application, i.e., high accuracy and fast decision, in which the proposed DNN architecture outperforms GMM-UBM and i-vector/LDA systems by 37% and 28%, respectively",
    "checked": true,
    "id": "d93783c488df0b72806aa1bf7b377ffabae2116d",
    "semantic_title": "deep neural networks for i-vector language identification of short utterances in cars",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/woubie16_interspeech.html": {
    "title": "Improving i-Vector and PLDA Based Speaker Clustering with Long-Term Features",
    "volume": "main",
    "abstract": "i-vector modeling techniques have been successfully used for speaker clustering task recently. In this work, we propose the extraction of i-vectors from short- and long-term speech features, and the fusion of their PLDA scores within the frame of speaker diarization. Two sets of i-vectors are first extracted from short-term spectral and long-term voice-quality, prosodic and glottal to noise excitation ratio (GNE) features. Then, the PLDA scores of these two i-vectors are fused for speaker clustering task. Experiments have been carried out on single and multiple site scenario test sets of Augmented Multi-party Interaction (AMI) corpus. Experimental results show that i-vector based PLDA speaker clustering technique provides a significant diarization error rate (DER) improvement than GMM based BIC clustering technique",
    "checked": true,
    "id": "2cad573c7399a44a5e14774983a634ec50883c74",
    "semantic_title": "improving i-vector and plda based speaker clustering with long-term features",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lawson16_interspeech.html": {
    "title": "Open Language Interface for Voice Exploitation (OLIVE)",
    "volume": "main",
    "abstract": "We propose to demonstrate the Open Language Interface for Voice Exploitation (OLIVE) speech-processing system, which SRI International developed under the DARPA Robust Automatic Transcription of Speech (RATS) program. The technology underlying OLIVE was designed to achieve robustness to high levels of noise and distortion for speech activity detection (SAD), speaker identification (SID), language and dialect identification (LID), and keyword spotting (KWS). Our demonstration will show OLIVE performing those four tasks. We will also demonstrate SRI's speaker recognition capability live on a mobile phone for visitors to interact with",
    "checked": true,
    "id": "aeefeadb54943a9c0185de5d9f98e17b2e325844",
    "semantic_title": "open language interface for voice exploitation (olive)",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/smidl16_interspeech.html": {
    "title": "A Multimodal Dialogue System for Air Traffic Control Trainees Based on Discrete-Event Simulation",
    "volume": "main",
    "abstract": "In this paper we present a multimodal dialogue system designed as a learning tool for air traffic control officer trainees (ATCO). It was developed using our discrete-event simulation dialogue management framework with cloud-based speech recognition and text-to-speech systems. Our system mimics pilots in an air traffic communication, allowing the ATCOs to practice a control of a virtual airspace using spoken commands from air traffic control English phraseology",
    "checked": true,
    "id": "ad55d685da50ff6b71b4db11260cceb6a05bd3f1",
    "semantic_title": "a multimodal dialogue system for air traffic control trainees based on discrete-event simulation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gauthier16_interspeech.html": {
    "title": "Lig-Aikuma: A Mobile App to Collect Parallel Speech for Under-Resourced Language Studies",
    "volume": "main",
    "abstract": "This paper reports on our ongoing efforts to collect speech data in under-resourced or endangered languages of Africa. Data collection is carried out using an improved version of the Android application ( Aikuma) developed by Steven Bird and colleagues [1]. Features were added to the app in order to facilitate the collection of parallel speech data in line with the requirements of the French-German ANR/DFG BULB (Breaking the Unwritten Language Barrier) project. The resulting app, called Lig-Aikuma, runs on various mobile phones and tablets and proposes a range of different speech collection modes (recording, respeaking, translation and elicitation). It was used for field data collections in Congo-Brazzaville resulting in a total of over 80 hours of speech",
    "checked": true,
    "id": "2fa369378dbee983fd851510f8936c5b465680d3",
    "semantic_title": "lig-aikuma: a mobile app to collect parallel speech for under-resourced language studies",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gruber16_interspeech.html": {
    "title": "ARET — Automatic Reading of Educational Texts for Visually Impaired Students",
    "volume": "main",
    "abstract": "This paper deals with a presentation of an application which was developed to help in education of visually impaired pupils at a secondary school, i.e. at the pupils' age of 12 to 14 years. The web-based application integrates speech and language technologies to make the education easier in several areas, e.g. in mathematics, physics, chemistry or languages (Czech, English, German). TTS system is used for automatic reading of educational texts and it makes use of a special preprocessing of the texts, namely any formulas which may occur therein. The application is used by both teachers to create and manage the teaching material and pupils to view and listen to the prepared material. The application is currently being used by one special school for visually impaired pupils in daily lessons",
    "checked": false,
    "id": "8581568b2a2d2e80650ad2564292c21e2c3a0f42",
    "semantic_title": "aret - automatic reading of educational texts for visually impaired students",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lu16b_interspeech.html": {
    "title": "Segmental Recurrent Neural Networks for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "We study the segmental recurrent neural network for end-to-end acoustic modelling. This model connects the segmental conditional random field (CRF) with a recurrent neural network (RNN) used for feature extraction. Compared to most previous CRF-based acoustic models, it does not rely on an external system to provide features or segmentation boundaries. Instead, this model marginalises out all the possible segmentations, and features are extracted from the RNN trained together with the segmental CRF. Essentially, this model is self-contained and can be trained end-to-end. In this paper, we discuss practical training and decoding issues as well as the method to speed up the training in the context of speech recognition. We performed experiments on the TIMIT dataset. We achieved 17.3% phone error rate (PER) from the first-pass decoding — the best reported result using CRFs, despite the fact that we only used a zeroth-order CRF and without using any language model",
    "checked": true,
    "id": "aedffcebea081138a0f2bf2454f872700237fbf6",
    "semantic_title": "segmental recurrent neural networks for end-to-end speech recognition",
    "citation_count": 84
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nussbaumthom16_interspeech.html": {
    "title": "Acoustic Modeling Using Bidirectional Gated Recurrent Convolutional Units",
    "volume": "main",
    "abstract": "Convolutional and bidirectional recurrent neural networks have achieved considerable performance gains as acoustic models in automatic speech recognition in recent years. Latest architectures unify long short-term memory, gated recurrent unit and convolutional neural networks by stacking these different neural network types on each other, and providing short and long-term features to different depth levels of the network For the first time, we propose a unified layer for acoustic modeling which is simultaneously recurrent and convolutional, and which operates only on short-term features. Our unified model introduces a bidirectional gated recurrent unit that uses convolutional operations for the gating units. We analyze the performance behavior of the proposed layer, compare and combine it with bidirectional gated recurrent units, deep neural networks and frequency-domain convolutional neural networks on a 50 hour English broadcast news task. The analysis indicates that the proposed layer in combination with stacked bidirectional gated recurrent units outperforms other architectures",
    "checked": true,
    "id": "cd6cf1a39321c1ece4ee5f49e4ff24fbe1e9dc56",
    "semantic_title": "acoustic modeling using bidirectional gated recurrent convolutional units",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hsu16_interspeech.html": {
    "title": "Exploiting Depth and Highway Connections in Convolutional Recurrent Deep Neural Networks for Speech Recognition",
    "volume": "main",
    "abstract": "Deep neural network models have achieved considerable success in a wide range of fields. Several architectures have been proposed to alleviate the vanishing gradient problem, and hence enable training of very deep networks. In the speech recognition area, convolutional neural networks, recurrent neural networks, and fully connected deep neural networks have been shown to be complimentary in their modeling capabilities. Combining all three components, called CLDNN, yields the best performance to date. In this paper, we extend the CLDNN model by introducing a highway connection between LSTM layers, which enables direct information flow from cells of lower layers to cells of upper layers. With this design, we are able to better exploit the advantages of a deeper structure. Experiments on the GALE Chinese Broadcast Conversation/News Speech dataset indicate that our model outperforms all previous models and achieves a new benchmark, which is 22.41% character error rate on the dataset",
    "checked": true,
    "id": "45d2db8d57d9f716d73b75d29231195110ccbeb0",
    "semantic_title": "exploiting depth and highway connections in convolutional recurrent deep neural networks for speech recognition",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wu16_interspeech.html": {
    "title": "Stimulated Deep Neural Network for Speech Recognition",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) and deep learning approaches yield state-of-the-art performance in a range of tasks, including speech recognition. However, the parameters of the network are hard to analyze, making network regularization and robust adaptation challenging. Stimulated training has recently been proposed to address this problem by encouraging the node activation outputs in regions of the network to be related. This kind of information aids visualization of the network, but also has the potential to improve regularization and adaptation. This paper investigates stimulated training of DNNs for both of these options. These schemes take advantage of the smoothness constraints that stimulated training offers. The approaches are evaluated on two large vocabulary speech recognition tasks: a U.S. English broadcast news (BN) task and a Javanese conversational telephone speech task from the IARPA Babel program. Stimulated DNN training acquires consistent performance gains on both tasks over unstimulated baselines. On the BN task, the proposed smoothing approach is also applied to rapid adaptation, again outperforming the standard adaptation scheme",
    "checked": true,
    "id": "6226f19d41ff0a09f2e2cdb4c38d0fa98f0f2a0d",
    "semantic_title": "stimulated deep neural network for speech recognition",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2016/badino16_interspeech.html": {
    "title": "Phonetic Context Embeddings for DNN-HMM Phone Recognition",
    "volume": "main",
    "abstract": "This paper proposes an approach, named phonetic context embedding, to model phonetic context effects for deep neural network - hidden Markov model (DNN-HMM) phone recognition. Phonetic context embeddings can be regarded as continuous and distributed vector representations of context-dependent phonetic units (e.g., triphones). In this work they are computed using neural networks. First, all phone labels are mapped into vectors of binary distinctive features (DFs, e.g., nasal/not-nasal). Then for each speech frame the corresponding DF vector is concatenated with DF vectors of previous and next frames and fed into a neural network that is trained to estimate the acoustic coefficients (e.g., MFCCs) of that frame. The values of the first hidden layer represent the embedding of the input DF vectors. Finally, the resulting embeddings are used as secondary task targets in a multi-task learning (MTL) setting when training the DNN that computes phone state posteriors. The approach allows to easily encode a much larger context than alternative MTL-based approaches. Results on TIMIT with a fully connected DNN shows phone error rate (PER) reductions from 22.4% to 21.0% and from 21.3% to 19.8% on the test core and the validation set respectively and lower PER than an alternative strong MTL approach",
    "checked": true,
    "id": "54dfb6d31119e7cda3d6b26f9fe9e848d099692c",
    "semantic_title": "phonetic context embeddings for dnn-hmm phone recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16b_interspeech.html": {
    "title": "Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Convolutional Neural Networks (CNNs) are effective models for reducing spectral variations and modeling spectral correlations in acoustic features for automatic speech recognition (ASR). Hybrid speech recognition systems incorporating CNNs with Hidden Markov Models/Gaussian Mixture Models (HMMs/GMMs) have achieved the state-of-the-art in various benchmarks. Meanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural Networks (RNNs), which is proposed for labeling unsegmented sequences, makes it feasible to train an ‘end-to-end' speech recognition system instead of hybrid settings. However, RNNs are computationally expensive and sometimes difficult to train. In this paper, inspired by the advantages of both CNNs and the CTC approach, we propose an end-to-end speech framework for sequence labeling, by combining hierarchical CNNs with CTC directly without recurrent connections. By evaluating the approach on the TIMIT phoneme recognition task, we show that the proposed model is not only computationally efficient, but also competitive with the existing baseline systems. Moreover, we argue that CNNs have the capability to model temporal correlations with appropriate context information",
    "checked": true,
    "id": "e0b207e96351671453aa8bf05b7225c8a340a0b2",
    "semantic_title": "towards end-to-end speech recognition with deep convolutional neural networks",
    "citation_count": 341
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16b_interspeech.html": {
    "title": "Joint Speaker and Lexical Modeling for Short-Term Characterization of Speaker",
    "volume": "main",
    "abstract": "For speech utterances of very short duration, speaker characterization has shown strong dependency on the lexical content. In this context, speaker verification is always performed by analyzing and matching speaker pronunciation of individual words, syllables, or phones. In this paper, we advocate the use of hidden Markov model (HMM) for joint modeling of speaker characteristic and lexical content. We then develop a scoring model that scores only the speaker part rather than the joint speaker-lexical component leading to a better speaker verification performance. Experiments were conducted on the text-prompted task of RSR2015 and the RedDots datasets. In the RSR2015, the prompted texts are limited to random sequences of digits. The RedDots dataset dictates an unconstrained scenario where the prompted texts are free-text sentences. Both RSR2015 and RedDots datasets are publicly available",
    "checked": true,
    "id": "f3cf024d3164065ed13638e26029d5d0c310e026",
    "semantic_title": "joint speaker and lexical modeling for short-term characterization of speaker",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2016/alam16_interspeech.html": {
    "title": "Tandem Features for Text-Dependent Speaker Verification on the RedDots Corpus",
    "volume": "main",
    "abstract": "We use tandem features and a fusion of four systems for text-dependent speaker verification on the RedDots corpus. In the tandem system, a senone-discriminant neural network provides a low-dimensional bottleneck feature at each frame which are concatenated with a standard Mel-frequency cepstral coefficients (MFCC) feature representation. The concatenated features are propagated to a conventional GMM/UBM speaker recognition framework. In order to capture complementary information to the MFCC, we also use linear frequency cepstral coefficients and wavelet-based cepstral coefficients features for score level fusion. We report results on the part 1 and part 4 (text-dependent) tasks of RedDots corpus. Both the tandem feature-based system and fused system provided significant improvements over the baseline GMM/UBM system in terms of equal error rates (EER) and detection cost functions (DCFs) as defined in the 2008 and 2010 NIST speaker recognition evaluations. On the part 1 task (impostor correct condition) the fused system reduced the EER from 2.63% to 2.28% for male trials and from 7.01% to 3.48% for female trials. On the part4 task (impostor correct condition) the fused system helped to reduce the EER from 2.49% to 1.96% and from 5.9% to 3.22% for male and female trials respectively",
    "checked": true,
    "id": "8122f33803a4f4a2d907d2255370e919a1702c43",
    "semantic_title": "tandem features for text-dependent speaker verification on the reddots corpus",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sarkar16_interspeech.html": {
    "title": "Text Dependent Speaker Verification Using Un-Supervised HMM-UBM and Temporal GMM-UBM",
    "volume": "main",
    "abstract": "In this paper, we investigate the Hidden Markov Model (HMM) and the temporal Gaussian Mixture Model (GMM) systems based on the Universal Background Model (UBM) concept to capture temporal information of speech for Text Dependent (TD) Speaker Verification (SV). In TD-SV, target speakers are constrained to use only predefined fixed sentence/s during both the enrollment and the test process. The temporal information is therefore important in the sense of utterance verification, i.e. whether the test utterance has the same sequence of textual content as the utterance used during the target enrollment. However, the temporal information is not considered in the classical GMM-UBM based TD-SV system. Moreover, no transcription knowledge of the speech is required in the HMM-UBM and temporal GMM-UBM based systems. We also study the fusion of the HMM-UBM, the temporal GMM-UBM and the classical GMM-UBM systems in SV. We show that the HMM-UBM system yields better performance than the other systems in most cases. Further, fusion of the systems improve the overall speaker verification performance. The results are shown in the different tasks of the RedDots challenge 2016 database",
    "checked": true,
    "id": "12f8cbde4db33e2da7d185552e3a75f49ebb5c70",
    "semantic_title": "text dependent speaker verification using un-supervised hmm-ubm and temporal gmm-ubm",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kinnunen16_interspeech.html": {
    "title": "Utterance Verification for Text-Dependent Speaker Recognition: A Comparative Assessment Using the RedDots Corpus",
    "volume": "main",
    "abstract": "Text-dependent automatic speaker verification naturally calls for the simultaneous verification of speaker identity and spoken content. These two tasks can be achieved with automatic speaker verification (ASV) and utterance verification (UV) technologies. While both have been addressed previously in the literature, a treatment of simultaneous speaker and utterance verification with a modern, standard database is so far lacking. This is despite the burgeoning demand for voice biometrics in a plethora of practical security applications. With the goal of improving overall verification performance, this paper reports different strategies for simultaneous ASV and UV in the context of short-duration, text-dependent speaker verification. Experiments performed on the recently released RedDots corpus are reported for three different ASV systems and four different UV systems. Results show that the combination of utterance verification with automatic speaker verification is (almost) universally beneficial with significant performance improvements being observed",
    "checked": true,
    "id": "51d78ea852ca3017fd547d0aa715bc79b94082c1",
    "semantic_title": "utterance verification for text-dependent speaker recognition: a comparative assessment using the reddots corpus",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ma16_interspeech.html": {
    "title": "Parallel Speaker and Content Modelling for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "Text-dependent short duration speaker verification involves two challenges. The primary challenge of interest is the verification of the speaker's identity, and often a secondary challenge of interest is the verification of the lexical content of the pass-phrase. In this paper, we propose the use of two systems to handle these two tasks in parallel with one sub-system modelling speaker identity based on the assumption that lexical content is known and the other sub-system modelling lexical content in a speaker dependent manner. The text-dependent speaker verification sub-system is based on hidden Markov models and the lexical content verification system is based on models of speech segments that use a distinct Gaussian mixture model for each segment. Furthermore, a mixture selection method based on KL divergence was applied to refine the lexical content sub-system by making the models more discriminative. Experiments on part 1 of the RedDots database showed that the proposed combination of two sub-systems outperformed the baseline system by 39.8%, 51.1% and 37.3% in terms of the ‘imposter_correct', ‘target_wrong' and ‘imposter_wrong' metrics respectively",
    "checked": true,
    "id": "fcb5af7bbb24e3bea6564e57e3a2cceca6af5481",
    "semantic_title": "parallel speaker and content modelling for text-dependent speaker verification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zeinali16_interspeech.html": {
    "title": "i-Vector/HMM Based Text-Dependent Speaker Verification System for RedDots Challenge",
    "volume": "main",
    "abstract": "Recently, a new data collection was initiated within the RedDots project in order to evaluate text-dependent and text-prompted speaker recognition technology on data from a wider speaker population and with more realistic noise, channel and phonetic variability. This paper analyses our systems built for RedDots challenge — the effort to collect and compare the initial results on this new evaluation data set obtained at different sites. We use our recently introduced HMM based i-vector approach, where, instead of the traditional GMM, a set of phone specific HMMs is used to collect the sufficient statistics for i-vector extraction. Our systems are trained in a completely phrase-independent way on the data from RSR2015 and Libri speech databases. We compare systems making use of standard cepstral features and their combination with neural network based bottle-neck features. The best results are obtained with a score-level fusion of such systems",
    "checked": true,
    "id": "63a33cbe736e8fe19e5a5ab3f3a5aded514a27f3",
    "semantic_title": "i-vector/hmm based text-dependent speaker verification system for reddots challenge",
    "citation_count": 35
  },
  "https://www.isca-speech.org/archive/interspeech_2016/das16_interspeech.html": {
    "title": "Exploring Session Variability and Template Aging in Speaker Verification for Fixed Phrase Short Utterances",
    "volume": "main",
    "abstract": "This work highlights the impact of session variability and template aging on speaker verification (SV) using fixed phrase short utterances from the RedDots database. These have been collected over a period of one year and contain a large number of sessions per speaker. Session variation has been found to have a direct influence on SV performance and its significance is even greater for the case of fixed phrase short utterances as a very small amount of speech data is involved for speaker modeling as well as testing. Similarly for a practical deployable SV system when there is large session variation involved over a period of time, the template aging of the speakers may effect the SV performance. This work attempts to address some issues related to session variability and template aging of speakers which are found for data having large session variability, that if considered can be utilized for improving the performance of an SV system",
    "checked": true,
    "id": "fea617ec380d1f3de8cd4b907e04b8bed50ee993",
    "semantic_title": "exploring session variability and template aging in speaker verification for fixed phrase short utterances",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/uchida16_interspeech.html": {
    "title": "Prediction of the Articulatory Movements of Unseen Phonemes of a Speaker Using the Speech Structure of Another Speaker",
    "volume": "main",
    "abstract": "In this paper, we propose a method to predict the articulatory movements of phonemes that are difficult for a speaker to pronounce correctly because those phonemes are not seen in the native language of that speaker. When one wants to predict the articulatory movements of those unseen phonemes, since he/she has difficulty to generate those sounds, the conventional acoustic-to-articulatory mapping cannot be applied as it is. Here, we propose a solution by using the speech structure of another reference speaker who can pronounce the unseen phonemes. Speech structure is a kind of speech feature that represents only the linguistic information by suppressing the non-linguistic information, e.g. speaker identity, of an input utterance. In the proposed method, by using the speech structure of those unseen phonemes and other phonemes as constraint, the articulatory movements of the unseen phonemes are searched for in the articulatory space of the original speaker. Experiments using English short vowels show that the averaged prediction error was 1.02 mm",
    "checked": true,
    "id": "29d6a6d5ef49600074733e1c03eddc1b3241742a",
    "semantic_title": "prediction of the articulatory movements of unseen phonemes of a speaker using the speech structure of another speaker",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sivaraman16_interspeech.html": {
    "title": "Vocal Tract Length Normalization for Speaker Independent Acoustic-to-Articulatory Speech Inversion",
    "volume": "main",
    "abstract": "Speech inversion is a well-known ill-posed problem and addition of speaker differences typically makes it even harder. This paper investigates a vocal tract length normalization (VTLN) technique to transform the acoustic space of different speakers to a target speaker space such that speaker specific details are minimized. The speaker normalized features are then used to train a feed-forward neural network based acoustic-to-articulatory speech inversion system. The acoustic features are parameterized as time-contextualized mel-frequency cepstral coefficients and the articulatory features are represented by six tract-variable (TV) trajectories. Experiments are performed with ten speakers from the U. Wisc. X-ray microbeam database. Speaker dependent speech inversion systems are trained for each speaker as baselines to compare the performance of the speaker independent approach. For each target speaker, data from the remaining nine speakers are transformed using the proposed approach and the transformed features are used to train a speech inversion system. The performances of the individual systems are compared using the correlation between the estimated and the actual TVs on the target speaker's test set. Results show that the proposed speaker normalization approach provides a 7% absolute improvement in correlation as compared to the system where speaker normalization was not performed",
    "checked": true,
    "id": "ee7e37ac12293f8e574852e0052e9beef22bdd30",
    "semantic_title": "vocal tract length normalization for speaker independent acoustic-to-articulatory speech inversion",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lammert16_interspeech.html": {
    "title": "Investigation of Speed-Accuracy Tradeoffs in Speech Production Using Real-Time Magnetic Resonance Imaging",
    "volume": "main",
    "abstract": "Motor actions in speech production are both rapid and highly dexterous, even though speed and accuracy are often thought to conflict. Fitts' law has served as a rigorous formulation of the fundamental speed-accuracy tradeoff in other domains of human motor action, but has not been directly examined with respect to speech production. This paper examines Fitts' law in speech articulation kinematics by analyzing USC-TIMIT, a large database of real-time magnetic resonance imaging data of speech production. This paper also addresses methodological challenges in applying Fitts-style analysis, including the definition and operational measurement of key variables in real-time MRI data. Results suggest high variability in the task demands associated with targeted articulatory kinematics, as well as a clear tradeoff between speed and accuracy for certain types of speech production actions. Consonant targets, and particularly those following vowels, show the strongest evidence of this tradeoff, with correlations as high as 0.71 between movement time and difficulty. Other speech actions seem to challenge Fitts' law. Results are discussed with respect to limitations of Fitts' law in the context of speech production, as well as future improvements and applications",
    "checked": true,
    "id": "74950053ff1d9866daef96e4465c69f79d4481c2",
    "semantic_title": "investigation of speed-accuracy tradeoffs in speech production using real-time magnetic resonance imaging",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sorensen16_interspeech.html": {
    "title": "Characterizing Vocal Tract Dynamics Across Speakers Using Real-Time MRI",
    "volume": "main",
    "abstract": "Real-time magnetic resonance imaging (rtMRI) provides information about the dynamic shaping of the vocal tract during speech production and valuable data for creating and testing models of speech production. In this paper, we use rtMRI videos to develop a dynamical system in the framework of Task Dynamics which controls vocal tract constrictions and induces deformation of the air-tissue boundary. This is the first task dynamical system explicitly derived from speech kinematic data. Simulation identifies differences in articulatory strategy across speakers (n = 18), specifically in the relative contribution of articulators to vocal tract constrictions",
    "checked": true,
    "id": "4bf7727f00fec6d3c2f0fc5ed354eea2868afff2",
    "semantic_title": "characterizing vocal tract dynamics across speakers using real-time mri",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2016/labrunie16_interspeech.html": {
    "title": "Tracking Contours of Orofacial Articulators from Real-Time MRI of Speech",
    "volume": "main",
    "abstract": "We introduce a method for predicting midsagittal contours of orofacial articulators from real-time MRI data. A corpus of about 26 minutes of speech has been recorded of a French speaker at a rate of 55 images / s using highly undersampled radial gradient-echo MRI with image reconstruction by nonlinear inversion. The contours of each articulator have been manually traced for a set of about 60 images selected — by hierarchical clustering — to optimally represent the diversity of the speaker articulations. The data serve to build articulator-specific Principal Component Analysis (PCA) models of contours and associated image intensities, as well as multilinear regression (MLR) models that predict contour parameters from image parameters. The contours obtained by MLR are then refined, using the local information about pixel intensity profiles along the contours' normals, by means of modified Active Shape Models (ASM) trained on the same data. The method reaches RMS of predicted points to reference contour distances between 0.54 and 0.93 mm, depending on articulators. The processing of the corpus demonstrated the efficiency of the procedure, despite the possibility of further improvements. This work opens new perspectives for studying articulatory motion in speech",
    "checked": true,
    "id": "e7b98ef98003be34c8cae36b751d649c3619b31e",
    "semantic_title": "tracking contours of orofacial articulators from real-time mri of speech",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lingala16_interspeech.html": {
    "title": "State-of-the-Art MRI Protocol for Comprehensive Assessment of Vocal Tract Structure and Function",
    "volume": "main",
    "abstract": "Magnetic Resonance Imaging (MRI) provides a safe and flexible means to study the vocal tract, and is increasingly used in speech production research. This work details a state-of-the-art MRI protocol for comprehensive assessment of vocal tract structure and function, and presents results from representative speakers. The system incorporates (a) custom upper airway coils that are maximally sensitive to vocal tract tissues, (b) graphical user interface for 2D real-time MRI that provides on-the-fly reconstruction for interactive localization, and correction of imaging artifacts, (c) off-line constrained reconstruction for generating high spatio-temporal resolution dynamic images at (83 frames per sec, 2.4 mm ), (d) 3D static imaging of sounds sustained for 7 sec with full vocal tract coverage and isotropic resolution (resolution: 1.25 mm ), (e) T2-weighted high-resolution, high-contrast depiction of soft-tissue boundaries of the full vocal tract (axial, coronal, sagittal sweeps with resolution: 0.58 × 0.58 × 3 mm ), and (f) simultaneous audio recording with off-line noise cancellation and temporal alignment of audio with 2D real-time MRI. A stimuli set was designed to capture efficiently salient, static and dynamic, articulatory and morphological aspects of speech production in 90-minute data acquisition sessions",
    "checked": true,
    "id": "3186c634cd16e7c147da9f94a7f7f043af57788a",
    "semantic_title": "state-of-the-art mri protocol for comprehensive assessment of vocal tract structure and function",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xia16_interspeech.html": {
    "title": "DBN-ivector Framework for Acoustic Emotion Recognition",
    "volume": "main",
    "abstract": "Deep learning and i-vectors have been successfully used in speech and speaker recognition recently. In this work we propose a framework based on deep belief network (DBN) and i-vector space modeling for acoustic emotion recognition. We use two types of labels for frame level DBN training. The first one is the vector of posterior probabilities calculated from the GMM universal background model (UBM). The second one is the predicted label based on the GMMs. The DBN is trained to minimize errors for both types. After DBN training, we use the vector of posterior probabilities estimated by DBN to replace the UBM for i-vector extraction. Finally the extracted i-vectors are used in backend classifiers for emotion recognition. Our experiments on the USC IEMOCAP data show the effectiveness of our proposed DBN-ivector framework. In particular, with decision level combination, our proposed system yields significant improvement on both unweighted and weighted accuracy",
    "checked": true,
    "id": "87f3d1d5dafa1e33cf959a83967c1129c9989ac1",
    "semantic_title": "dbn-ivector framework for acoustic emotion recognition",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2016/stasak16_interspeech.html": {
    "title": "An Investigation of Emotional Speech in Depression Classification",
    "volume": "main",
    "abstract": "Assessing depression via speech characteristics is a growing area of interest in quantitative mental health research with a view to a clinical mental health assessment tool. As a mood disorder, depression induces changes in response to emotional stimuli, which motivates this investigation into the relationship between emotion and depression affected speech. This paper investigates how emotional information expressed in speech (i.e. arousal, valence, dominance) contributes to the classification of minimally depressed and moderately-severely depressed individuals. Experiments based on a subset of the AVEC 2014 database show that manual emotion ratings alone are discriminative of depression and combining rating-based emotion features with acoustic features improves classification between mild and severe depression. Emotion-based data selection is also shown to provide improvements in depression classification and a range of threshold methods are explored. Finally, the experiments presented demonstrate that automatically predicted emotion ratings can be incorporated into a fully automatic depression classification to produce a 5% accuracy improvement over an acoustic-only baseline system",
    "checked": true,
    "id": "c6da2b77303d7a7fc13e927210d631573924c8f2",
    "semantic_title": "an investigation of emotional speech in depression classification",
    "citation_count": 36
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lotfian16_interspeech.html": {
    "title": "Retrieving Categorical Emotions Using a Probabilistic Framework to Define Preference Learning Samples",
    "volume": "main",
    "abstract": "Preference learning is an appealing approach for affective recognition. Instead of predicting the underlying emotional class of a sample, this framework relies on pairwise comparisons to rank-order the testing data according to an emotional dimension. This framework is relevant not only for continuous attributes such as arousal or valence, but also for categorical classes (e.g., is this sample happier than the other?). A preference learning system for categorical classes can have applications in several domains including retrieving emotional behaviors conveying a target emotion, and defining the emotional intensity associated with a given class. One important challenge to build such a system is to define relative labels defining the preference between training samples. Instead of building these labels from scratch, we propose a probabilistic framework that creates relative labels from existing categorical annotations. The approach considers individual assessments instead of consensus labels, creating a metrics that is sensitive to the underlying ambiguity of emotional classes. The proposed metric quantifies the likelihood that a sample belong to a target emotion. We build happy, angry and sad rank-classifiers using this metric. We evaluate the approach over cross-corpus experiments, showing improved performance over binary classifiers and rank-based classifiers trained with consensus labels",
    "checked": true,
    "id": "97a6ae7100a0bb416ae1e705d72b117ecf4d637b",
    "semantic_title": "retrieving categorical emotions using a probabilistic framework to define preference learning samples",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schmitt16_interspeech.html": {
    "title": "At the Border of Acoustics and Linguistics: Bag-of-Audio-Words for the Recognition of Emotions in Speech",
    "volume": "main",
    "abstract": "Recognition of natural emotion in speech is a challenging task. Different methods have been proposed to tackle this complex task, such as acoustic feature brute-forcing or even end-to-end learning. Recently, bag-of-audio-words (BoAW) representations of acoustic low-level descriptors (LLDs) have been employed successfully in the domain of acoustic event classification and other audio recognition tasks. In this approach, feature vectors of acoustic LLDs are quantised according to a learnt codebook of audio words. Then, a histogram of the occurring ‘words' is built. Despite their massive potential, BoAW have not been thoroughly studied in emotion recognition. Here, we propose a method using BoAW created only of mel-frequency cepstral coefficients (MFCCs). Support vector regression is then used to predict emotion continuously in time and value, such as in the dimensions arousal and valence. We compare this approach with the computation of functionals based on the MFCCs and perform extensive evaluations on the RECOLA database, which features spontaneous and natural emotions. Results show that, BoAW representation of MFCCs does not only perform significantly better than functionals, but also outperforms by far most of recently published deep learning approaches, including convolutional and recurrent networks",
    "checked": true,
    "id": "7ebf51a3bff0834a33e3313bd51c0c7d7ac50fc2",
    "semantic_title": "at the border of acoustics and linguistics: bag-of-audio-words for the recognition of emotions in speech",
    "citation_count": 127
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chorianopoulou16_interspeech.html": {
    "title": "Speech Emotion Recognition Using Affective Saliency",
    "volume": "main",
    "abstract": "We investigate an affective saliency approach for speech emotion recognition of spoken dialogue utterances that estimates the amount of emotional information over time. The proposed saliency approach uses a regression model that combines features extracted from the acoustic signal and the posteriors of a segment-level classifier to obtain frame or segment-level ratings. The affective saliency model is trained using a minimum classification error (MCE) criterion that learns the weights by optimizing an objective loss function related to the classification error rate of the emotion recognition system. Affective saliency scores are then used to weight the contribution of frame-level posteriors and/or features to the speech emotion classification decision. The algorithm is evaluated for the task of anger detection on four call-center datasets for two languages, Greek and English, with good results",
    "checked": true,
    "id": "db779bc16a620a6fd747217272e1161b822c21ec",
    "semantic_title": "speech emotion recognition using affective saliency",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gupta16_interspeech.html": {
    "title": "Laughter Valence Prediction in Motivational Interviewing Based on Lexical and Acoustic Cues",
    "volume": "main",
    "abstract": "Motivational Interviewing (MI) is a goal oriented psychotherapy counseling that aims to instill positive change in a client through discussion. Since the discourse is in the form of semi-structured natural conversation, it often involves a variety of non-verbal social and affective behaviors such as laughter. Laughter carries information related to affect, mood and personality and can offer a window into the mental state of a person. In this work, we conduct an analytical study on predicting the valence of laughters (positive, neutral or negative) based on lexical and acoustic cues, within the context of MI. We hypothesize that the valence of laughter can be predicted using a window of past and future context around the laughter and, design models to incorporate context, from both text and audio. Through these experiments we validate the relation of the two modalities to perceived laughter valence. Based on the outputs of the prediction experiment, we perform a follow up analysis of the results including: (i) identification of the optimal past and future context in the audio and lexical channels, (ii) investigation of the differences in the prediction patterns for the counselor and the client and, (iii) analysis of feature patterns across the two modalities",
    "checked": true,
    "id": "7fb7eaf674d27fd390ae44e99967656d48cea477",
    "semantic_title": "laughter valence prediction in motivational interviewing based on lexical and acoustic cues",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wodarczak16_interspeech.html": {
    "title": "Respiratory Belts and Whistles: A Preliminary Study of Breathing Acoustics for Turn-Taking",
    "volume": "main",
    "abstract": "This paper presents first results on using acoustic intensity of inhalations as a cue to speech initiation in spontaneous multiparty conversations. We demonstrate that inhalation intensity significantly differentiates between cycles coinciding with no speech activity, shorter (< 1 s) and longer stretches of speech. While the model fit is relatively weak, it is comparable to the fit of a model using kinematic features collected with Respiratory Inductance Plethysmography. We also show that incorporating both kinematic and acoustic features further improves the model. Given the ease of capturing breath acoustics, we consider the results to be a promising first step towards studying communicative functions of respiratory sounds. We discuss possible extensions to the data collection procedure with a view to improving predictive power of the model",
    "checked": true,
    "id": "7f422042053b90cc2b176c508fc562e6fec45093",
    "semantic_title": "respiratory belts and whistles: a preliminary study of breathing acoustics for turn-taking",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kaland16_interspeech.html": {
    "title": "/r/ as Language Marker in Bilingual Speech Production and Perception",
    "volume": "main",
    "abstract": "Across languages of the world /r/ is known for its variability. Recent literature incorporates sociolinguistic factors, such as bilingualism, in order to explain /r/ variation. The current study investigates to what extent /r/ is a marker of a bilingual's dominant language. Specifically, the effects of several sociolinguistic and phonotactic factors on the production and perception of /r/ are investigated, such as the bilingual speaker's linguistic background, the language spoken as well as syllable position and place of articulation. To this end a reading task is carried out with bilingual speakers from South Tyrol (Italy). The major languages spoken in this region are Tyrolean (German dialect) and Italian. The recorded reading data is subsequently used in a perception experiment to investigate whether South Tyrolean listeners can identify the dominant language of the speaker on the basis of the presence of /r/ and the /r/ variant. Results show that listeners can identify the dominant language of the bilingual speakers on the basis of /r/. Specifically, the more Italian dominant the sociolinguistic background of the speaker, the more /r/ is produced frontally and the more that speaker is perceived as Italian dominant",
    "checked": true,
    "id": "be0986cf33a3639b81cd6923de0e0c0ef9932291",
    "semantic_title": "/r/ as language marker in bilingual speech production and perception",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/putzer16_interspeech.html": {
    "title": "Evaluation of Phonatory Behavior of German and French Speakers in Native and Non-Native Speech",
    "volume": "main",
    "abstract": "Phonatory behavior of German speakers (GS) and French speakers (FS) in native (L1) and non-native (L2) speech was instrumentally examined. Vowel productions of the two groups were analyzed using a parametrization of phonatory behavior and phonatory quality properties in the acoustic signal. The behavior of GS is characterized by more strained adduction of the vocal folds whereas FS show more incomplete glottal closure. Furthermore, GS change their phonatory behavior in the foreign language (=French) by adapting phonatory strategies of FS, whereas FS do not show this tendency. In addition, German beginners (BEG) and partly German advanced learners (ADV) are already orientated on production characteristics of the L2. French BEG however retain their phonatory behavior in L2 (=German) by showing less vocal fold adduction in comparison to their L1. French ADV show the opposite behavior. Finally, ADV of the two speaker groups generally show more strained behavior in L2 productions than BEG. The results provide evidence that GS and FS apply different laryngeal phonatory settings and that they altered their settings in L2 differently. Perceptual evaluation of voice quality of the speech material and a correlation analysis between acoustic and perceptual results are suggested for future research",
    "checked": true,
    "id": "827ea31001d7aaa1e3e1e40d312e7471aceb1e98",
    "semantic_title": "evaluation of phonatory behavior of german and french speakers in native and non-native speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/strombergsson16_interspeech.html": {
    "title": "Today's Most Frequently Used F0 Estimation Methods, and Their Accuracy in Estimating Male and Female Pitch in Clean Speech",
    "volume": "main",
    "abstract": "Variation in fundamental frequency (F ) constitutes a valuable source of information for researches across many disciplines, with a shared interest in speech. Different methods for estimating F vary in estimation accuracy and accessibility, and there is yet no gold standard. Through a bibliometric survey, this study examines what methods were the most frequently used in the speech scientific community during the years 2010–2016. Secondly, the most used methods are evaluated against a ground truth reference, with a specific focus on their accuracy in estimating F in male and female speakers, respectively The results show that Praat is the dominant method by far, followed by STRAIGHT, RAPT and YIN. This pattern holds across a range of different research areas, although within Acoustics and Engineering, Praat's dominance is less pronounced. In the evaluation including Praat, RAPT and YIN — with their default and gender-adapted settings — Praat also proved to be the most accurate. The finding that adapting Praat's pitch range settings by gender leads to further improvements should encourage researchers to do this routinely",
    "checked": true,
    "id": "ff040316f44eab5c0497cec280bfb1fd0e7c0e85",
    "semantic_title": "today's most frequently used f0 estimation methods, and their accuracy in estimating male and female pitch in clean speech",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2016/he16_interspeech.html": {
    "title": "A Praat-Based Algorithm to Extract the Amplitude Envelope and Temporal Fine Structure Using the Hilbert Transform",
    "volume": "main",
    "abstract": "A speech signal can be viewed as a high frequency carrier signal containing the temporal fine structure (TFS) that is modulated by a low frequency envelope (ENV). A widely used method to decompose a speech signal into the TFS and ENV is the Hilbert transform. Although this method has been available for about one century and is widely applied in various kinds of speech processing tasks (e.g. speech chimeras), there are only very few speech processing packages that contain readily available functions for the Hilbert transform, and there is very little textbook type literature tailored for speech scientists to explain the processes behind the transform. With this paper we provide the code for carrying out the Hilbert operation to obtain the TFS and ENV in the widely used speech processing software Praat, and explain the basics of the procedure. To verify our code, we compare the Hilbert transform in Praat with a widely applied function for the same purpose in Matlab (\"hilbert(…)\"). We can confirm that both methods arrive at identical outputs",
    "checked": true,
    "id": "6d87fe97a55c142ba2a7aa7aeb8cb800e5f73725",
    "semantic_title": "a praat-based algorithm to extract the amplitude envelope and temporal fine structure using the hilbert transform",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2016/enzinger16_interspeech.html": {
    "title": "Likelihood Ratio Calculation in Acoustic-Phonetic Forensic Voice Comparison: Comparison of Three Statistical Modelling Approaches",
    "volume": "main",
    "abstract": "This study compares three statistical models used to calculate likelihood ratios in acoustic-phonetic forensic-voice-comparison systems: Multivariate kernel density, principal component analysis kernel density, and a multivariate normal model. The data were coefficient values obtained from discrete cosine transforms fitted to human-supervised formant-trajectory measurements of tokens of /iau/ from a database of recordings of 60 female speakers of Chinese. Tests were conducted using high-quality recordings as nominal suspect samples and mobile-to-landline transmitted recordings as nominal offender samples. Performance was assessed before and after fusion with a baseline automatic mel frequency cepstral coefficient Gaussian mixture model universal background model system. In addition, Monte Carlo simulations were used to compare the output of the statistical models to true likelihood-ratio values calculated on the basis of the distribution specified for a simulated population",
    "checked": true,
    "id": "10ec780fe1a9a04502dd392416a441ebacf02ee1",
    "semantic_title": "likelihood ratio calculation in acoustic-phonetic forensic voice comparison: comparison of three statistical modelling approaches",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/qi16_interspeech.html": {
    "title": "A Sparse Spherical Harmonic-Based Model in Subbands for Head-Related Transfer Functions",
    "volume": "main",
    "abstract": "Several functional models for head-related transfer function (HRTF) have been proposed based on spherical harmonic (SH) orthogonal functions, which yield an encouraging performance level in terms of log-spectral distortion (LSD). However, since the properties of subbands are quite different and highly subject-dependent, the degree of SH expansion should be adapted to the subband and the subject, which is quite challenging. In this paper, a sparse spherical harmonic-based model termed SSHM is proposed in order to achieve an intelligent frequency truncation. Different from SH-based model (SHM) which assigns the degree for each subband, SSHM constrains the number of SH coefficients by using an l penalty, and automatically preserves the significant coefficients in each subband. As a result, SSHM requires less coefficients at the same SD level than other truncation methods to reconstruct HRTFs . Furthermore, when used for interpolation, SSHM gives a better fitting precision since it naturally reduces the influence of the fluctuation caused by the movement of the subject and the processing error. The experiments show that even using about 40% less coefficients, SSHM has a slightly lower LSD than SHM. Therefore, SSHM can achieve a better tradeoff between efficiency and accuracy",
    "checked": true,
    "id": "0eff2296c52e2751b6ddd029737e37c1700b7e83",
    "semantic_title": "a sparse spherical harmonic-based model in subbands for head-related transfer functions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/isik16_interspeech.html": {
    "title": "Single-Channel Multi-Speaker Separation Using Deep Clustering",
    "volume": "main",
    "abstract": "Deep clustering is a recently introduced deep learning architecture that uses discriminatively trained embeddings as the basis for clustering. It was recently applied to spectrogram segmentation, resulting in impressive results on speaker-independent multi-speaker separation. In this paper we extend the baseline system with an end-to-end signal approximation objective that greatly improves performance on a challenging speech separation. We first significantly improve upon the baseline system performance by incorporating better regularization, larger temporal context, and a deeper architecture, culminating in an overall improvement in signal to distortion ratio (SDR) of 10.3 dB compared to the baseline of 6.0 dB for two-speaker separation, as well as a 7.1 dB SDR improvement for three-speaker separation. We then extend the model to incorporate an enhancement layer to refine the signal estimates, and perform end-to-end training through both the clustering and enhancement stages to maximize signal fidelity. We evaluate the results using automatic speech recognition. The new signal approximation objective, combined with end-to-end training, produces unprecedented performance, reducing the word error rate (WER) from 89.1% down to 30.8%. This represents a major advancement towards solving the cocktail party problem",
    "checked": true,
    "id": "ab94fae3d49cd7016a47020469dc257d8090f5bb",
    "semantic_title": "single-channel multi-speaker separation using deep clustering",
    "citation_count": 376
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16_interspeech.html": {
    "title": "Jointly Optimizing Activation Coefficients of Convolutive NMF Using DNN for Speech Separation",
    "volume": "main",
    "abstract": "Convolutive non-negative matrix factorization (CNMF) and deep neural networks (DNN) are two efficient methods for monaural speech separation. Conventional DNN focuses on building the non-linear relationship between mixture and target speech. However, it ignores the prominent structure of the target speech. Conventional CNMF model concentrates on capturing prominent harmonic structures and temporal continuities of speech but it ignores the non-linear relationship between the mixture and target. Taking these two aspects into consideration at the same time may result in better performance. In this paper, we propose a joint optimization of DNN models with an extra CNMF layer for speech separation task. We also utilize an extra masking layer on the proposed model to constrain the speech reconstruction. Moreover, a discriminative training criterion is proposed to further enhance the performance of the separation. Experimental results show that the proposed model has significant improvement in PESQ, SAR, SIR and SDR compared with conventional methods",
    "checked": true,
    "id": "8b2c9e86c87fe5d33e207e3e8c178f10ce520a28",
    "semantic_title": "jointly optimizing activation coefficients of convolutive nmf using dnn for speech separation",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/delfarah16_interspeech.html": {
    "title": "A Feature Study for Masking-Based Reverberant Speech Separation",
    "volume": "main",
    "abstract": "Monaural speech separation in reverberant conditions is very challenging. In masking-based separation, features extracted from speech mixtures are employed to predict a time-frequency mask. Robust feature extraction is crucial for the performance of supervised speech separation in adverse acoustic environments. Using objective speech intelligibility as the metric, we investigate a wide variety of monaural features in low signal-to-noise ratios and moderate to high reverberation. Deep neural networks are employed as the learning machine in our feature investigation. We find considerable performance gain using a contextual window in reverberant speech processing, likely due to temporal structure of reverberation. In addition, we systematically evaluate feature combinations. In unmatched noise and reverberation conditions, the resulting feature set from this study substantially outperforms previously employed sets for speech separation in anechoic conditions",
    "checked": true,
    "id": "a683c8a4fa3909aaf03a6c7e5c34469f8cb4a188",
    "semantic_title": "a feature study for masking-based reverberant speech separation",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hsu16b_interspeech.html": {
    "title": "Discriminative Layered Nonnegative Matrix Factorization for Speech Separation",
    "volume": "main",
    "abstract": "This paper proposes a discriminative layered nonnegative matrix factorization (DL-NMF) for monaural speech separation. The standard NMF conducts the parts-based representation using a single-layer of bases which was recently upgraded to the layered NMF (L-NMF) where a tree of bases was estimated for multi-level or multi-aspect decomposition of a complex mixed signal. In this study, we develop the DL-NMF by extending the generative bases in L-NMF to the discriminative bases which are estimated according to a discriminative criterion. The discriminative criterion is conducted by optimizing the recovery of the mixed spectra from the separated spectra and minimizing the reconstruction errors between separated spectra and original source spectra. The experiments on single-channel speech separation show the superiority of DL-NMF to NMF and L-NMF in terms of the SDR, SIR and SAR measures",
    "checked": true,
    "id": "0cd5a69f5662e9b987c16794931e4a91c8d90586",
    "semantic_title": "discriminative layered nonnegative matrix factorization for speech separation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gang16_interspeech.html": {
    "title": "On Discriminative Framework for Single Channel Audio Source Separation",
    "volume": "main",
    "abstract": "Single channel source separation (SCSS) algorithms that utilise discriminative source models perform better in comparison to those that are trained independently. However, all the aspects of training discriminative models have not been addressed in the literature. For instance, the choice of dimensions of source models (number of columns of NMF, Dictionary etc) not only influences the fidelity of a given source but also impacts the interference introduced in it. Therefore choosing a right dimension parameter for every source model is crucial for an effective separation. In fact, the similarity between the constituent sources can be different for different mixtures and thus, dimensions should also be chosen specific to the sources in the concerned mixture. Further, separation of a given constituent from a mixture, assuming remaining to be interferers, offers more freedom for the particular constituent and hence provide better separation. In this paper, we propose a generic discriminative learning framework where we separate one source at a time and embed our dimension search algorithm in the training of discriminative source models. We apply our framework on the NMF based SCSS algorithms and demonstrate a performance improvement in separation for both speech-speech and speech-music mixture",
    "checked": true,
    "id": "433341aa0c00452da994661ac18dd28569db7bed",
    "semantic_title": "on discriminative framework for single channel audio source separation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jin16_interspeech.html": {
    "title": "Generating Natural Video Descriptions via Multimodal Processing",
    "volume": "main",
    "abstract": "Generating natural language descriptions of visual content is an intriguing task which has wide applications such as assisting blind people. The recent advances in image captioning stimulate further study of this task in more depth including generating natural descriptions for videos. Most works of video description generation focus on visual information in the video. However, audio provides rich information for describing video contents as well. In this paper, we propose to generate video descriptions in natural sentences via multimodal processing, which refers to using both audio and visual cues via unified deep neural networks with both convolutional and recurrent structure. Experimental results on the Microsoft Research Video Description (MSVD) corpus prove that fusing audio information greatly improves the video description performance. We also investigate the impact of image amount vs caption amount on the image caption performance and see the trend that when limited amount of training is available, number of various captions is more important than number of various images. This will guide us to investigate in the future how to improve the video description system via increasing amount of training data",
    "checked": true,
    "id": "2abae43b4a7fd85473bd6c906a0fcfc403968e87",
    "semantic_title": "generating natural video descriptions via multimodal processing",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2016/heckmann16_interspeech.html": {
    "title": "Feature-Level Decision Fusion for Audio-Visual Word Prominence Detection",
    "volume": "main",
    "abstract": "Common fusion techniques in audio-visual speech processing operate on the modality level. I.e. they either combine the features extracted from the two modalities directly or derive a decision for each modality separately and then combine the modalities on the decision level. We investigate the audio-visual processing of linguistic prosody, more precisely the extraction of word prominence. In this context the different features for each modality can be assumed to be only partially dependent. Hence we propose to train a classifier for each of these features, acoustic and visual modality, and then combine them on a decision level. We compare this approach with conventional fusion methods, i.e. feature fusion and decision fusion on the modality level. Our results show that the feature-level decision fusion clearly outperforms the other approaches, in particular when we also additionally integrate the features resulting from the feature fusion. Compared to a detection based only on the full audio stream we obtain relative improvements from the audio-visual detection of 19% for clean audio and up to 50% for noisy audio",
    "checked": true,
    "id": "fd6b332fa960c3acdcc6ee194855608661970799",
    "semantic_title": "feature-level decision fusion for audio-visual word prominence detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ouni16_interspeech.html": {
    "title": "Acoustic and Visual Analysis of Expressive Speech: A Case Study of French Acted Speech",
    "volume": "main",
    "abstract": "Within the framework of developing an expressive audiovisual speech synthesis, an acoustic and visual analysis of expressive acted speech is proposed in this paper. Our purpose is to identify the main characteristics of audiovisual expressions that need to be integrated during synthesis to provide believable emotions to the virtual 3D talking head. We conducted a case study of a semi-professional actor who uttered a set of sentences for 6 different emotions in addition to neutral speech. We have recorded concurrently audio and motion capture data. The acoustic and the visual data have been analyzed. The main finding is that although some expressions are not well identified, some expressions were well characterized and tied in both acoustic and visual space",
    "checked": true,
    "id": "6079eb2dcdce230b2acf67290169094eaa580c6c",
    "semantic_title": "acoustic and visual analysis of expressive speech: a case study of french acted speech",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/barbulescu16_interspeech.html": {
    "title": "Characterization of Audiovisual Dramatic Attitudes",
    "volume": "main",
    "abstract": "In this work we explore the capability of audiovisual parameters (such as fundamental frequency, rhythm, head motion or facial expressions) to discriminate among different dramatic attitudes. We extract the audiovisual parameters from an acted corpus of attitudes and structure them as frame, syllable, and sentence-level features. Using Linear Discriminant Analysis classifiers, we show that sentence-level features present a higher discriminating rate among the attitudes. We also compare the classification results with the perceptual evaluation tests, showing that F0 is correlated to the perceptual results for all attitudes, while other features, such as head motion, contribute differently, depending both on the attitude and the speaker",
    "checked": true,
    "id": "5338ad86b1b4de74cc9dca5c287814ae34e37bc9",
    "semantic_title": "characterization of audiovisual dramatic attitudes",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/huang16_interspeech.html": {
    "title": "Conversational Engagement Recognition Using Auditory and Visual Cues",
    "volume": "main",
    "abstract": "Automatic prediction of engagement in human-human and human-machine dyadic and multiparty interaction scenarios could greatly aid in evaluation of the success of communication. A corpus of eight face-to-face dyadic casual conversations was recorded and used as the basis for an engagement study, which examined the effectiveness of several methods of engagement level recognition. A convolutional neural network based analysis was seen to be the most effective",
    "checked": true,
    "id": "c2d439dd891553659e1805ea8d98be5a9be7732a",
    "semantic_title": "conversational engagement recognition using auditory and visual cues",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chaspari16_interspeech.html": {
    "title": "An Acoustic Analysis of Child-Child and Child-Robot Interactions for Understanding Engagement during Speech-Controlled Computer Games",
    "volume": "main",
    "abstract": "Engagement is an essential factor towards successful game design and effective human-computer interaction. We analyze the prosodic patterns of child-child and child-robot pairs playing a language-based computer game. Acoustic features include speech loudness and fundamental frequency. We use a linear mixed-effects model to capture the coordination of acoustic patterns between interactors as well as its relation to annotated engagement levels. Our results indicate that the considered acoustic features are related to engagement levels for both the child-child and child-robot interaction. They further suggest significant association of the prosodic patterns during the child-child scenario, which is moderated by the co-occurring engagement. This acoustic coordination is not present in the child-robot interaction, since the robot's behavior was not automatically adjusted to the child. These findings are discussed in relation to automatic robot adaptation and provide a foundation for promoting engagement and enhancing rapport during the considered game-based interactions",
    "checked": true,
    "id": "bc246f90ca78fca4866106de3913ee6d46981036",
    "semantic_title": "an acoustic analysis of child-child and child-robot interactions for understanding engagement during speech-controlled computer games",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kasisopa16_interspeech.html": {
    "title": "Auditory-Visual Lexical Tone Perception in Thai Elderly Listeners with and without Hearing Impairment",
    "volume": "main",
    "abstract": "Lexical tone perception was investigated in elderly Thais with Normal Hearing (NH), or Hearing Impairment (HI), the latter with and without Hearing Aids. Auditory-visual (AV), auditory-only (AO), and visual-only (VO) discrimination of Thai tones was investigated. Both groups performed poorly in VO. In AV and AO, the NH performed better than the HI group, and Hearing Aids facilitated tone discrimination. There was slightly more visual augmentation (AV>AO) for the HI group, but not the NH group. The Falling-Rising (FR) pair of tones was easiest to discriminate for both groups and there was a similar ranking of relative discriminability of all 10 tone contrasts for the HI group with and without hearing aids, but this differed from the ranking in the NH group. These results show that the Hearing Impaired elderly with and without hearing aids can, and do, use visual speech information to augment tone perception, but do so in a similar, not a significantly more enhanced manner than the Normal Hearing elderly. Thus hearing loss in the Thai elderly does not result in greater use of visual information for discrimination of lexical tone; rather, all Thai elderly use visual information to augment their auditory perception of tone",
    "checked": true,
    "id": "8928061883cab4e352c71eca759187d62a9dddfd",
    "semantic_title": "auditory-visual lexical tone perception in thai elderly listeners with and without hearing impairment",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/khaki16_interspeech.html": {
    "title": "Use of Agreement/Disagreement Classification in Dyadic Interactions for Continuous Emotion Recognition",
    "volume": "main",
    "abstract": "Natural and affective handshakes of two participants define the course of dyadic interaction. Affective states of the participants are expected to be correlated with the nature or type of the dyadic interaction. In this study, we investigate relationship between affective attributes and nature of dyadic interaction. In this investigation we use the JESTKOD database, which consists of speech and full-body motion capture data recordings for dyadic interactions under agreement and disagreement scenarios. The dataset also has affective annotations in activation, valence and dominance (AVD) attributes. We pose the continuous affect recognition problem under agreement and disagreement scenarios of dyadic interactions. We define a statistical mapping using the support vector regression (SVR) from speech and motion modalities to affective attributes with and without the dyadic interaction type (DIT) information. We observe an improvement in estimation of the valence attribute when the DIT is available. Furthermore this improvement sustains even we estimate the DIT from the speech and motion modalities of the dyadic interaction",
    "checked": true,
    "id": "e7ad909b554506b3658c19def0bc691af28b2d07",
    "semantic_title": "use of agreement/disagreement classification in dyadic interactions for continuous emotion recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schadler16_interspeech.html": {
    "title": "Microscopic Multilingual Matrix Test Predictions Using an ASR-Based Speech Recognition Model",
    "volume": "main",
    "abstract": "In an attempt to predict the outcomes of matrix sentence tests in different languages and various noise conditions for native listeners, the simulation framework for auditory discrimination experiments (FADE) and the extended Speech Intelligibility Index (eSII) is employed. FADE uses an automatic speech recognition system to simulate recognition experiments and reports the highest achievable performance as the outcome, which showed good predictions for the German matrix test in noise. The eSII is based on the short-time analysis of weighted signal-to-noise ratios in different frequency bands. In contrast to many other approaches, including the eSII, FADE uses no empirical reference. In this work, the FADE approach is evaluated for predictions of the German, Polish, Russian, and Spanish matrix test in stationary and fluctuating noise conditions. The FADE-based predictions yield a high correlation (Pearsons R = 0.94) with the empirical data and a root-mean-square (RMS) prediction error of 1.9 dB outperforming the eSII-based predictions (R = 0.78, RMS = 4.2 dB). FADE can also predict the data of subgroups with only stationary or only fluctuating noises, while the eSII cannot. The FADE-based predictions seem to generalize over different languages and noise conditions",
    "checked": true,
    "id": "b410233111e76648c4a917fdde69230e02b1e8af",
    "semantic_title": "microscopic multilingual matrix test predictions using an asr-based speech recognition model",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2016/exter16_interspeech.html": {
    "title": "DNN-Based Automatic Speech Recognition as a Model for Human Phoneme Perception",
    "volume": "main",
    "abstract": "In this paper, we test the applicability of state-of-the-art automatic speech recognition (ASR) to predict phoneme confusions in human listeners. Phoneme-specific response rates are obtained from ASR based on deep neural networks (DNNs) and from listening tests with six normal-hearing subjects. The measure for model quality is the correlation of phoneme recognition accuracies obtained in ASR and in human speech recognition (HSR). Various feature representations are used as input to the DNNs to explore their relation to overall ASR performance and model prediction power. Standard filterbank output and perceptual linear prediction (PLP) features result in best predictions, with correlation coefficients reaching r = 0.9",
    "checked": true,
    "id": "2211cee65df1be7fcdd4c69a908f9369f00657b2",
    "semantic_title": "dnn-based automatic speech recognition as a model for human phoneme perception",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2016/toth16_interspeech.html": {
    "title": "Undoing Misperceptions: A Microscopic Analysis of Consistent Confusions Through Signal Modifications",
    "volume": "main",
    "abstract": "Consistent confusions — word misperceptions reported in an open set task with a high agreement across listeners — can be especially valuable in understanding the detailed processes underlying speech perception. The current study investigates the origin of a set of consistent confusions collected in a variety of masking conditions, by applying signal-level modifications to the stimuli eliciting the confusion, and subsequently reevaluating listeners' percepts. Modifications were selected to provide release from either the energetic or the informational component of the maskers and involved manipulations of signal-to-noise ratio, fundamental frequency, and resynthesis of the noise-mixture in glimpsed regions of the target speech. Increasing signal-to-noise ratio and glimpse resynthesis showed the expected release from energetic and informational masking respectively. However, manipulations targeting informational masking release, including fundamental frequency modification, affected a surprisingly high number of confusions stemming from energetic maskers. The degree of fundamental frequency shift did not have a significant effect on the response patterns observed. Around 30% of confusions can be explained solely based on the information contained within the target glimpses surviving energetic masking, while for the rest of the cases additional factors, such as recruitment of information from the masker, appear to be involved",
    "checked": true,
    "id": "7c193f57b8b29f2a8d1fa27f9a6939433569c299",
    "semantic_title": "undoing misperceptions: a microscopic analysis of consistent confusions through signal modifications",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/karbasi16_interspeech.html": {
    "title": "Blind Non-Intrusive Speech Intelligibility Prediction Using Twin-HMMs",
    "volume": "main",
    "abstract": "Automatic prediction of speech intelligibility is highly desirable in the speech research community, since listening tests are time-consuming and can not be used online. Most of the available objective speech intelligibility measures are intrusive methods, as they require a clean reference signal in addition to the corresponding noisy/processed signal at hand. In order to overcome the problem of predicting the speech intelligibility in the absence of the clean reference signal, we have proposed in [1] to employ a recognition/synthesis framework called twin hidden Markov model (THMM) for synthesizing the clean features, required inside an intrusive intelligibility prediction method. The new framework can predict the speech intelligibility equally well as well-known intrusive methods like the short-time objective intelligibility (STOI). The original THMM, however, requires the correct transcription for synthesizing the clean reference features, which is not always available. In this paper, we go one step further and investigate the use of the recognized transcription instead of the oracle transcription for obtaining a more widely applicable speech intelligibility prediction. We show that the output of the newly-proposed blind approach is highly correlated with the human speech recognition results, collected via crowdsourcing in different noise conditions",
    "checked": true,
    "id": "9d439e0238e369b1b777cfb47e87a45f57bf088f",
    "semantic_title": "blind non-intrusive speech intelligibility prediction using twin-hmms",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/toth16b_interspeech.html": {
    "title": "Misperceptions Arising from Speech-in-Babble Interactions",
    "volume": "main",
    "abstract": "The deterioration of speech intelligibility in the presence of other sound sources has been explained in terms of both energetic masking, which renders parts of the speech signal inaudible, and informational masking, in which audible components of the masker interfere with speech identification. The current study focuses on the role of a specific form of informational masking in which audible glimpses of both target and masker combine to produce an incorrect listener percept. We examine a corpus of word misperceptions in Spanish which occur when target words are combined with a babble masker. Glimpses originating in both the target and the masker are force-aligned to the reported misperceived word in order to identify the most likely acoustic evidential basis for the confusion. In this way, the degree of involvement of both target and masker can be quantified. In nearly all cases, the best explanation for the misperception involves recruiting evidence from the babble masker (type I error), and in more than 80% of the tokens some of the audible target evidence is ignored (type II error). These findings suggest misallocation of acoustic-phonetic material plays a significant role in the generation of speech-in-babble confusions",
    "checked": true,
    "id": "804cb0251f9ce98ecb353343f727b62446f7950b",
    "semantic_title": "misperceptions arising from speech-in-babble interactions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/eichenauer16_interspeech.html": {
    "title": "Introducing Temporal Rate Coding for Speech in Cochlear Implants: A Microscopic Evaluation in Humans and Models",
    "volume": "main",
    "abstract": "Standard cochlea implant (CI) speech coding strategies transmit formant information only via the place of the stimulated electrode. In acoustic hearing, however, formant frequencies are additionally coded via the temporal rate of auditory nerve firing. This study presents a novel CI coding strategy (\"Formant Locking (FL)-strategy\") that varies stimulation rates in relation to extracted fundamental and formant frequencies. Simulated auditory nerve activity resulting from stimulation with the FL-strategy shows that the FL-strategy triggers spike rates that are related to the formant frequencies similar as in normal hearing, and greatly different than in a standard CI strategy. Vowel recognition in seven CI users via direct stimulation of their electrode array shows that the FL-strategy results in significantly increased scores of the vowels /u/ and /i/ compared to a standard CI strategy. However, at the same time, a decrease in scores for /o/ and /e/ occurred. A microscopic speech intelligibility model involving an automatic speech recognizer reveals good agreement between modeled and predicted confusion matrices for the FL-strategy. This suggests that microscopic models can be used to test CI strategies in the development phase, and gives indications which cues might be used by the listeners for speech recognition",
    "checked": true,
    "id": "a1dc945f2bda9bcd9b0e400b790f9770031fb0c9",
    "semantic_title": "introducing temporal rate coding for speech in cochlear implants: a microscopic evaluation in humans and models",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lecumberri16_interspeech.html": {
    "title": "Language Effects in Noise-Induced Word Misperceptions",
    "volume": "main",
    "abstract": "Speech misperceptions provide a window into the processes underlying spoken language comprehension. One approach shown to catalyse robust misperceptions is to embed words in noise. However, the use of masking noise makes it difficult to measure the relative contributions of low-level auditory processing and higher-level factors which involve the deployment of linguistic experience. The current study addresses this confound by comparing noise-induced misperceptions in two languages, Spanish and English, which display marked phonological differences in properties such as consonant-vowel ratio, rhythm and syllable structure. An analysis of over 5000 word-level misperceptions generated using a common experimental framework in the two languages reveals some striking similarities: the proportion of confusions generated by three distinct types of masker are almost identical for the two languages, as are the proportions of phonemic and syllabic insertions, deletions and substitutions. The biggest difference is seen for babble noise, which tends to induce relatively complex confusions in English and simpler confusions in Spanish. We speculate that the inflectional morphology of Spanish lends itself to more easily recruit single elements from a babble masker into valid word hypotheses",
    "checked": true,
    "id": "e48944bf6a1c609be0222c9a01fd86e16771f59c",
    "semantic_title": "language effects in noise-induced word misperceptions",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/varnet16_interspeech.html": {
    "title": "Speech Reductions Cause a De-Weighting of Secondary Acoustic Cues",
    "volume": "main",
    "abstract": "The ability of the auditory system to change the perceptual weighting of acoustic cues when faced with degraded speech has long been evidenced. However, the exact changes that occur remain mostly unknown. Here, we proposed to use the Auditory Classification Image (ACI) methodology to reveal the acoustic cues used in natural speech comprehension and in reduced (i.e. noise-vocoded or re-synthesized) speech comprehension. The results show that in the latter case the auditory system updates its listening strategy by de-weighting secondary acoustic cues. Indeed, these are often weaker and thus more easily erased in adverse listening conditions. Furthermore our data suggests that this de-weighting does not directly depend on the actual reliability of the cues, but rather on the expected change in informativeness",
    "checked": true,
    "id": "7b010463107e77c0ca6026021201741c32c0d3d4",
    "semantic_title": "speech reductions cause a de-weighting of secondary acoustic cues",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fontan16_interspeech.html": {
    "title": "Using Phonologically Weighted Levenshtein Distances for the Prediction of Microscopic Intelligibility",
    "volume": "main",
    "abstract": "This article presents a new method for analyzing Automatic Speech Recognition (ASR) results at the phonological feature level. To this end the Levenshtein distance algorithm is refined in order to take into account the distinctive features opposing substituted phonemes. This method allows to survey features additions or deletions, providing microscopic qualitative information as a complement to word recognition scores. To explore the relevance of the qualitative data gathered by this method, a study is conducted on a speech corpus simulating presbycusis effects on speech perception at eight severity stages. Consonantic features additions and deletions in ASR outputs are analyzed and put in relation with intelligibility data collected in 30 human subjects. ASR results show monotonic trends in most consonantic features along the degradation conditions, which appear to be consistent with the misperceptions that could be observed in human subjects",
    "checked": true,
    "id": "8e31bae45564d6e03490e3f8fb923a51b34394cd",
    "semantic_title": "using phonologically weighted levenshtein distances for the prediction of microscopic intelligibility",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2016/matsui16_interspeech.html": {
    "title": "The Impact of Manner of Articulation on the Intelligibility of Voicing Contrast in Noise: Cross-Linguistic Implications",
    "volume": "main",
    "abstract": "The current study addresses the impact of manner of articulation on the intelligibility of voicing contrast in noise from a cross-linguistic perspective. Previous noise-masking studies have suggested that the impact of manner of articulation on the intelligibility of voicing contrast in noise is apparently different in Russian and English. In order to further assess the source of such a cross-linguistic inconsistency, the current study examines how Russian voicing contrast is perceived by English listeners. Native listeners of English performed a forced-choice identification task with Russian voiced and voiceless stimuli in quiet and noisy conditions. The results showed that the voicing contrast in stops were more confused than that in fricatives for English listeners, showing a pattern similar to Russian listeners. The results suggest that the source of the cross-linguistic difference identified in previous studies comes from the difference in the acoustic properties of the stimuli, reflecting the difference in phonetic implementation of voicing contrasts in each language. The results in turn suggest that perceptual cue weighting strategies for perceiving voicing contrast in different manners of articulation is similar among Russian and English listeners",
    "checked": true,
    "id": "0512001b1a715875c026c8474c4c30b69309e322",
    "semantic_title": "the impact of manner of articulation on the intelligibility of voicing contrast in noise: cross-linguistic implications",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mandel16_interspeech.html": {
    "title": "Directly Comparing the Listening Strategies of Humans and Machines",
    "volume": "main",
    "abstract": "In a given noisy environment, human listeners can more accurately identify spoken words than automatic speech recognizers. It is not clear, however, what information the humans are able to utilize in doing so that the machines are not. This paper uses a recently introduced technique to directly characterize the information used by humans and machines on the same task. The task was a forced choice between eight sentences spoken by a single talker from the small-vocabulary GRID corpus that were selected to be maximally confusable with one another. These sentences were mixed with \"bubble\" noise, which is designed to reveal randomly selected time-frequency glimpses of the sentence. Responses to these noisy mixtures allowed the identification of time-frequency regions that were important for each listener to recognize each sentence, i.e., regions that were frequently audible when a sentence was correctly identified and inaudible when it was not. In comparing these regions across human and machine listeners, we found that dips in noise allowed the humans to recognize words based on informative speech cues. In contrast, the baseline CHiME-2-GRID recognizer correctly identified sentences only when the time-frequency profile of the noisy mixture matched that of the underlying speech",
    "checked": true,
    "id": "73a9ecf0ba89d277e9c1664d4da8f4105bc7ea85",
    "semantic_title": "directly comparing the listening strategies of humans and machines",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/rondeau16_interspeech.html": {
    "title": "LSTM-Based NeuroCRFs for Named Entity Recognition",
    "volume": "main",
    "abstract": "Although NeuroCRF, an augmented Conditional Random Fields (CRF) model whose feature function is parameterized as a Feed-Forward Neural Network (FF NN) on word embeddings, has soundly outperformed traditional linear-chain CRF on many sequence labeling tasks, it is held back by the fact that FF NNs have a fixed input length and therefore cannot take advantage of the full input sentence. We propose to address this issue by replacing the FF NN with a Long Short-Term Memory (LSTM) NN, which can summarize an input of arbitrary length into a fixed dimension representation. The resulting model obtains F =89.28 on WikiNER dataset, a significant improvement over the NeuroCRF baseline's F =87.58, which is already a highly competitive result",
    "checked": true,
    "id": "ff02c951e5ca708b409993dc5b9b8ea712b69b07",
    "semantic_title": "lstm-based neurocrfs for named entity recognition",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16b_interspeech.html": {
    "title": "Exploring Word Mover's Distance and Semantic-Aware Embedding Techniques for Extractive Broadcast News Summarization",
    "volume": "main",
    "abstract": "Extractive summarization is a process that manages to select the most salient sentences from a document (or a set of documents) and subsequently assemble them to form an informative summary, facilitating users to browse and assimilate the main theme of the document efficiently. Our work in this paper continues this general line of research and its main contributions are two-fold. First, we explore to leverage the recently proposed word mover's distance (WMD) metric, in conjunction with semantic-aware continuous space representations of words, to authentically capture finer-grained sentence-to-document and/or sentence-to-sentence semantic relatedness for effective use in the summarization process. Second, we investigate to combine our proposed approach with several state-of-the-art summarization methods, which originally adopted the conventional term-overlap or bag-of-words (BOW) approaches for similarity calculation. A series of experiments conducted on a typical broadcast news summarization task seem to suggest the performance merits of our proposed approach, in comparison to the mainstream methods",
    "checked": true,
    "id": "5ba21683c165a19d6719173a3d4fcdbb0566d7fb",
    "semantic_title": "exploring word mover's distance and semantic-aware embedding techniques for extractive broadcast news summarization",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sheikh16_interspeech.html": {
    "title": "Improved Neural Bag-of-Words Model to Retrieve Out-of-Vocabulary Words in Speech Recognition",
    "volume": "main",
    "abstract": "Many Proper Names (PNs) are Out-Of-Vocabulary (OOV) words for speech recognition systems used to process diachronic audio data. To enable recovery of the PNs missed by the system, relevant OOV PNs can be retrieved by exploiting the semantic context of the spoken content. In this paper, we explore the Neural Bag-of-Words (NBOW) model, proposed previously for text classification, to retrieve relevant OOV PNs. We propose a Neural Bag-of-Weighted-Words (NBOW2) model in which the input embedding layer is augmented with a context anchor layer. This layer learns to assign importance to input words and has the ability to capture (task specific) key-words in a NBOW model. With experiments on French broadcast news videos we show that the NBOW and NBOW2 models outperform earlier methods based on raw embeddings from LDA and Skip-gram. Combining NBOW with NBOW2 gives faster convergence during training",
    "checked": true,
    "id": "0289897736a87e906f93b7e2fa5fe3fe1f3d3cb3",
    "semantic_title": "improved neural bag-of-words model to retrieve out-of-vocabulary words in speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/trione16_interspeech.html": {
    "title": "Beyond Utterance Extraction: Summary Recombination for Speech Summarization",
    "volume": "main",
    "abstract": "This paper describes a template filling approach for creating conversation summaries. The templates are generated from generalized summary fragments from a training corpus. Necessary pieces of information for filling them are extracted automatically from the conversation transcripts given linguistic features, and drive the fragment selection process. The approach obtains ROUGE-2 scores of 0.08471 on the RATP-DECODA corpus, which represents a significant improvement over extractive baselines and hand-written templates",
    "checked": true,
    "id": "4d9451a6892690cf1e3b3b3a34fb1595428aa396",
    "semantic_title": "beyond utterance extraction: summary recombination for speech summarization",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16c_interspeech.html": {
    "title": "Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling",
    "volume": "main",
    "abstract": "Attention-based encoder-decoder neural network models have recently shown promising results in machine translation and speech recognition. In this work, we propose an attention-based neural network model for joint intent detection and slot filling, both of which are critical steps for many speech understanding and dialog systems. Unlike in machine translation and speech recognition, alignment is explicit in slot filling. We explore different strategies in incorporating this alignment information to the encoder-decoder framework. Learning from the attention mechanism in encoder-decoder model, we further propose introducing attention to the alignment-based RNN models. Such attentions provide additional information to the intent classification and slot label prediction. Our independent task models achieve state-of-the-art intent detection error rate and slot filling F1 score on the benchmark ATIS task. Our joint training model further obtains 0.56% absolute (23.8% relative) error reduction on intent detection and 0.23% absolute gain on slot filling over the independent task models",
    "checked": true,
    "id": "2167f9ffd36af6c723d3527eab60c731e13d3a90",
    "semantic_title": "attention-based recurrent neural network models for joint intent detection and slot filling",
    "citation_count": 585
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jaech16_interspeech.html": {
    "title": "Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding",
    "volume": "main",
    "abstract": "The goal of this paper is to use multi-task learning to efficiently scale slot filling models for natural language understanding to handle multiple target tasks or domains. The key to scalability is reducing the amount of training data needed to learn a model for a new task. The proposed multi-task model delivers better performance with less data by leveraging patterns that it learns from the other tasks. The approach supports an open vocabulary, which allows the models to generalize to unseen words, which is particularly important when very little training data is used. A newly collected crowd-sourced data set, covering four different domains, is used to demonstrate the effectiveness of the domain adaptation and open vocabulary techniques",
    "checked": true,
    "id": "cf8a8d9f3c7466c9ada36420ebb510a742b34308",
    "semantic_title": "domain adaptation of recurrent neural networks for natural language understanding",
    "citation_count": 72
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ladhak16_interspeech.html": {
    "title": "LatticeRnn: Recurrent Neural Networks Over Lattices",
    "volume": "main",
    "abstract": "We present a new model called LatticeRnn, which generalizes recurrent neural networks (RNNs) to process weighted lattices as input, instead of sequences. A LatticeRnn can encode the complete structure of a lattice into a dense representation, which makes it suitable to a variety of problems, including rescoring, classifying, parsing, or translating lattices using deep neural networks (DNNs). In this paper, we use LatticeRnns for a classification task: each lattice represents the output from an automatic speech recognition (ASR) component of a spoken language understanding (SLU) system, and we classify the intent of the spoken utterance based on the lattice embedding computed by a LatticeRnn. We show that making decisions based on the full ASR output lattice, as opposed to 1-best or n-best hypotheses, makes SLU systems more robust to ASR errors. Our experiments yield improvements of 13% over a baseline RNN system trained on transcriptions and 10% over an n-best list rescoring system for intent classification",
    "checked": true,
    "id": "3d82efb6a2613853df4e811bdc2158c1cbb7875c",
    "semantic_title": "latticernn: recurrent neural networks over lattices",
    "citation_count": 63
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kesiraju16_interspeech.html": {
    "title": "Learning Document Representations Using Subspace Multinomial Model",
    "volume": "main",
    "abstract": "Subspace multinomial model (SMM) is a log-linear model and can be used for learning low dimensional continuous representation for discrete data. SMM and its variants have been used for speaker verification based on prosodic features and phonotactic language recognition. In this paper, we propose a new variant of SMM that introduces sparsity and call the resulting model as ℓ SMM. We show that ℓ SMM can be used for learning document representations that are helpful in topic identification or classification and clustering tasks. Our experiments in document classification show that SMM achieves comparable results to models such as latent Dirichlet allocation and sparse topical coding, while having a useful property that the resulting document vectors are Gaussian distributed",
    "checked": true,
    "id": "dec6984e611eb7538f7526fe43b17ce429c0951c",
    "semantic_title": "learning document representations using subspace multinomial model",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhao16_interspeech.html": {
    "title": "Attention-Based Convolutional Neural Networks for Sentence Classification",
    "volume": "main",
    "abstract": "Sentence classification is one of the foundational tasks in spoken language understanding (SLU) and natural language processing (NLP). In this paper we propose a novel convolutional neural network (CNN) with attention mechanism to improve the performance of sentence classification. In traditional CNN, it is not easy to encode long term contextual information and correlation between non-consecutive words effectively. In contrast, our attention-based CNN is able to capture these kinds of information for each word without any external features. We conducted experiments on various public and in-house datasets. The experimental results demonstrate that our proposed model significantly outperforms the traditional CNN model and achieves competitive performance with the ones that exploit rich syntactic features",
    "checked": true,
    "id": "fe888bd5f170d0e4c0a6eca0a508b4681bcfdb95",
    "semantic_title": "attention-based convolutional neural networks for sentence classification",
    "citation_count": 61
  },
  "https://www.isca-speech.org/archive/interspeech_2016/morchid16_interspeech.html": {
    "title": "Spoken Language Understanding in a Latent Topic-Based Subspace",
    "volume": "main",
    "abstract": "Performance of spoken language understanding applications declines when spoken documents are automatically transcribed in noisy conditions due to high Word Error Rates (WER). To improve the robustness to transcription errors, recent solutions propose to map these automatic transcriptions in a latent space. These studies have proposed to compare classical topic-based representations such as Latent Dirichlet Allocation (LDA), supervised LDA and author-topic (AT) models. An original compact representation, called c-vector, has recently been introduced to walk around the tricky choice of the number of latent topics in these topic-based representations. Moreover, c-vectors allow to increase the robustness of document classification with respect to transcription errors by compacting different LDA representations of a same speech document in a reduced space and then compensate most of the noise of the document representation. The main drawback of this method is the number of sub-tasks needed to build the c-vector space. This paper proposes to both improve this compact representation (c-vector) of spoken documents and to reduce the number of needed sub-tasks, using an original framework in a robust low dimensional space of features from a set of AT models called \"Latent Topic-based Subspace\" (LTS). In comparison to LDA, the AT model considers not only the dialogue content (words), but also the class related to the document. Experiments are conducted on the DECODA corpus containing speech conversations from the call-center of the RATP Paris transportation company. Results show that the original LTS representation outperforms the best previous compact representation (c-vector), with a substantial gain of more than 2.5% in terms of correctly labeled conversations",
    "checked": true,
    "id": "f248666cdeab5a16ebfa8902ad6b240992fd34a9",
    "semantic_title": "spoken language understanding in a latent topic-based subspace",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hakkanitur16_interspeech.html": {
    "title": "Multi-Domain Joint Semantic Frame Parsing Using Bi-Directional RNN-LSTM",
    "volume": "main",
    "abstract": "Sequence-to-sequence deep learning has recently emerged as a new paradigm in supervised learning for spoken language understanding. However, most of the previous studies explored this framework for building single domain models for each task, such as slot filling or domain classification, comparing deep learning based approaches with conventional ones like conditional random fields. This paper proposes a holistic multi-domain, multi-task (i.e. slot filling, domain and intent detection) modeling approach to estimate complete semantic frames for all user utterances addressed to a conversational system, demonstrating the distinctive power of deep learning methods, namely bi-directional recurrent neural network (RNN) with long-short term memory (LSTM) cells (RNN-LSTM) to handle such complexity. The contributions of the presented work are three-fold: (i) we propose an RNN-LSTM architecture for joint modeling of slot filling, intent determination, and domain classification; (ii) we build a joint multi-domain model enabling multi-task deep learning where the data from each domain reinforces each other; (iii) we investigate alternative architectures for modeling lexical context in spoken language understanding. In addition to the simplicity of the single model framework, experimental results show the power of such an approach on Microsoft Cortana real user data over alternative methods based on single domain/task deep learning",
    "checked": true,
    "id": "9b82c6e78ceaa5e540862849defc818f7c8a47df",
    "semantic_title": "multi-domain joint semantic frame parsing using bi-directional rnn-lstm",
    "citation_count": 409
  },
  "https://www.isca-speech.org/archive/interspeech_2016/janod16_interspeech.html": {
    "title": "Deep Stacked Autoencoders for Spoken Language Understanding",
    "volume": "main",
    "abstract": "The automatic transcription process of spoken document results in several word errors, especially when very noisy conditions are encountered. Document representations based on neural embedding frameworks have recently shown significant improvements in different Spoken and Natural Language Understanding tasks such as denoising and filtering. Nonetheless, these methods mainly need clean representations, failing to properly remove noise contained in noisy representations. This paper proposes to study the impact of residual noise contained into automatic transcripts of spoken dialogues in highly abstract spaces from deep neural networks. The paper makes the assumption that the noise learned from \"clean\" manual transcripts of spoken documents moves down dramatically the performance of theme identification systems in noisy conditions. The proposed deep neural network takes, as input and output, highly imperfect transcripts from spoken dialogues to improve the robustness of the document representation in a noisy environment. Results obtained on the DECODA theme classification task of dialogues reach an accuracy of 82% with a significant gain of about 5%",
    "checked": true,
    "id": "70189b6be960084a28279ca5c3d5a81d5736b4cb",
    "semantic_title": "deep stacked autoencoders for spoken language understanding",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kurata16b_interspeech.html": {
    "title": "Labeled Data Generation with Encoder-Decoder LSTM for Semantic Slot Filling",
    "volume": "main",
    "abstract": "To train a model for semantic slot filling, manually labeled data in which each word is annotated with a semantic slot label is necessary while manually preparing such data is costly. Starting from a small amount of manually labeled data, we propose a method to generate the labeled data with using the encoder-decoder LSTM. We first train the encoder-decoder LSTM that accepts and generates the same manually labeled data. Then, to generate a wide variety of labeled data, we add perturbations to the vector that encodes the manually labeled data and generate labeled data with the decoder LSTM based on the perturbated encoded vector. We also try to enhance the encoder-decoder LSTM to generate the word sequences and their label sequences separately to obtain new pairs of words and their labels. Through the experiments with the standard ATIS slot filling task, by using the generated data, we obtained improvement in slot filling accuracy over the strong baseline with the NN-based slot filling model",
    "checked": true,
    "id": "7ffe83d7dd3a474e15ccc2aef412009f100a5802",
    "semantic_title": "labeled data generation with encoder-decoder lstm for semantic slot filling",
    "citation_count": 42
  },
  "https://www.isca-speech.org/archive/interspeech_2016/stehwien16_interspeech.html": {
    "title": "Exploring the Correlation of Pitch Accents and Semantic Slots for Spoken Language Understanding",
    "volume": "main",
    "abstract": "We investigate the correlation between pitch accents and semantic slots in human-machine speech. Using an automatic pitch accent detector on the ATIS corpus, we find that most words labelled with semantic slots also carry a pitch accent. Most of the pitch accented words that are not associated with a semantic label are still meaningful, pointing towards the speaker's intention. Our findings show that prosody constitutes a relevant and useful resource for spoken language understanding, especially considering the fact that our pitch accent detector does not require any kind of manual transcriptions during testing time",
    "checked": true,
    "id": "d21ee832e173affb43b31e536dd05036daee2641",
    "semantic_title": "exploring the correlation of pitch accents and semantic slots for spoken language understanding",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tang16_interspeech.html": {
    "title": "Analysis on Gated Recurrent Unit Based Question Detection Approach",
    "volume": "main",
    "abstract": "Recent studies have shown various kinds of recurrent neural networks (RNNs) are becoming powerful sequence models in speech related applications. Our previous work in detecting questions of Mandarin speech presents that gated recurrent unit (GRU) based RNN can achieve significantly better results. In this paper, we try to open the black box to find the correlations between inner architecture of GRU and phonetic features of question sentences. We find that both update gate and reset gate in GRU blocks react when people begin to pronounce a word. According to the reactions, experiments are conducted to show the behavior of GRU based question detection approach on three important factors, including keywords or special structure of questions, final particles and interrogative intonation. We also observe that update gate and reset gate don't collaborate well on our dataset. Based on the asynchronous acts of update gate and reset gate in GRU, we adapt the structure of GRU block to our dataset and get further performance improvement in question detection task",
    "checked": true,
    "id": "9ea01d987bf991f40bf22392578e0dd88428e8ec",
    "semantic_title": "analysis on gated recurrent unit based question detection approach",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/oishi16_interspeech.html": {
    "title": "Combining State-Level Spotting and Posterior-Based Acoustic Match for Improved Query-by-Example Spoken Term Detection",
    "volume": "main",
    "abstract": "In spoken term detection (STD) systems, automatic speech recognition (ASR) frontend is often employed for its reasonable accuracy and efficiency. However, out-of-vocabulary (OOV) problem at ASR stage has a great impact on the STD performance for spoken query. In this paper, we propose combining feature-based acoustic match which is often employed in the STD systems for low resource languages, along with the other ASR-derived features. First, automatic transcripts for spoken document and spoken query are decomposed into corresponding acoustic model state sequences and used for spotting plausible speech segments. Second, DTW-based acoustic match between the query and candidate segment is performed using the posterior features derived from a monophone-state DNN. Finally, an integrated score is obtained by a logistic regression model, which is trained with a large spoken document and automatically generated spoken queries as development data. The experimental results on NTCIR-12 SpokenQuery&Doc-2 task showed that the proposed method significantly outperforms the baseline systems which use the subword-level or state-level spotting alone. Also, our universal scoring model trained with a separate set of development data could achieve the best STD performance, and showed the effectiveness of additional ASR-derived features regarding the confidence measure and query length",
    "checked": true,
    "id": "bbdf56cc40160cd1d5ff22ed974ebe288d54dc3f",
    "semantic_title": "combining state-level spotting and posterior-based acoustic match for improved query-by-example spoken term detection",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lv16_interspeech.html": {
    "title": "A Novel Discriminative Score Calibration Method for Keyword Search",
    "volume": "main",
    "abstract": "The performance of keyword search systems depends heavily on the quality of confidence scores. In this work, a novel discriminative score calibration method has been proposed. By training an MLP classifier employing the word posterior probability and several novel normalized scores, we can obtain a relative improvement of 4.67% for the actual term-weighted value (ATWV) metric on the OpenKWS15 development test dataset. In addition, a LSTM-CTC based keyword verification method has been proposed to supply extra acoustic information. After the information is added, a further improvement of 7.05% over the baseline can be observed",
    "checked": true,
    "id": "2b22f19fca23e238cf54136077e9dcb6c0715290",
    "semantic_title": "a novel discriminative score calibration method for keyword search",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/proenca16_interspeech.html": {
    "title": "Segmented Dynamic Time Warping for Spoken Query-by-Example Search",
    "volume": "main",
    "abstract": "This paper describes a low-resource approach to a Query-by-Example task, where spoken queries must be matched in a large dataset of spoken documents sometimes in complex or non-exact ways. Our approach tackles these complex match cases by using Dynamic Time Warping to obtain alternative paths that account for reordering of words, small extra content and small lexical variations. We also report certain advances on calibration and fusion of sub-systems that improve overall results, such as manipulating the score distribution per query and using an average posteriorgram distance matrix as an extra sub-system. Results are evaluated on the MediaEval task of Query-by-Example Search on Speech (QUESST). For this task, the language of the audio being searched is almost irrelevant, approaching the use case scenario to a language of very low resources. For that, we use as features the posterior probabilities obtained from five phonetic recognizers trained with five different languages",
    "checked": true,
    "id": "55c081a2ee9b0ff41bf1ce8e62dda408894095c3",
    "semantic_title": "segmented dynamic time warping for spoken query-by-example search",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lee16b_interspeech.html": {
    "title": "Generating Complementary Acoustic Model Spaces in DNN-Based Sequence-to-Frame DTW Scheme for Out-of-Vocabulary Spoken Term Detection",
    "volume": "main",
    "abstract": "This paper proposes a sequence-to-frame dynamic time warping (DTW) combination approach to improve out-of-vocabulary (OOV) spoken term detection (STD) performance gain. The goal of this paper is twofold: first, we propose a method that directly adopts the posterior probability of deep neural network (DNN) and Gaussian mixture model (GMM) as the similarity distance for sequence-to-frame DTW. Second, we investigate combinations of diverse schemes in GMM and DNN, with different subword units and acoustic models, estimate the complementarity in terms of performance gap and correlation of the combined systems, and discuss the performance gain of the combined systems. The results of evaluations conducted of the combined systems on an out-of-vocabulary spoken term detection task show that the performance gain of DNN-based systems is better than that of GMM-based systems. However, the performance gain obtained by combining DNN- and GMM-based systems is insignificant, even though DNN and GMM are highly heterogeneous. This is because the performance gap between DNN-based systems and GMM-based systems is quite large. On the other hand, score fusion of two heterogeneous subword units, triphone and sub-phonetic segments, in DNN-based systems provides significantly improved performance",
    "checked": true,
    "id": "766bd768cf0869554a5674c2d6000f3450e2b1d3",
    "semantic_title": "generating complementary acoustic model spaces in dnn-based sequence-to-frame dtw scheme for out-of-vocabulary spoken term detection",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/panchapagesan16_interspeech.html": {
    "title": "Multi-Task Learning and Weighted Cross-Entropy for DNN-Based Keyword Spotting",
    "volume": "main",
    "abstract": "We propose improved Deep Neural Network (DNN) training loss functions for more accurate single keyword spotting on resource-constrained embedded devices. The loss function modifications consist of a combination of multi-task training and weighted cross entropy. In the multi-task architecture, the keyword DNN acoustic model is trained with two tasks in parallel — the main task of predicting the keyword-specific phone states, and an auxiliary task of predicting LVCSR senones. We show that multi-task learning leads to comparable accuracy over a previously proposed transfer learning approach where the keyword DNN training is initialized by an LVCSR DNN of the same input and hidden layer sizes. The combination of LVCSR-initialization and Multi-task training gives improved keyword detection accuracy compared to either technique alone. We also propose modifying the loss function to give a higher weight on input frames corresponding to keyword phone targets, with a motivation to balance the keyword and background training data. We show that weighted cross-entropy results in additional accuracy improvements. Finally, we show that the combination of 3 techniques — LVCSR-initialization, multi-task training and weighted cross-entropy gives the best results, with significantly lower False Alarm Rate than the LVCSR-initialization technique alone, across a wide range of Miss Rates",
    "checked": true,
    "id": "c2d6c0613b7efedd507d44d8f6edd80b814ae8af",
    "semantic_title": "multi-task learning and weighted cross-entropy for dnn-based keyword spotting",
    "citation_count": 136
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chung16_interspeech.html": {
    "title": "Audio Word2Vec: Unsupervised Learning of Audio Segment Representations Using Sequence-to-Sequence Autoencoder",
    "volume": "main",
    "abstract": "The vector representations of fixed dimensionality for words (in text) offered by Word2Vec have been shown to be very useful in many application scenarios, in particular due to the semantic information they carry. This paper proposes a parallel version, the Audio Word2Vec. It offers the vector representations of fixed dimensionality for variable-length audio segments. These vector representations are shown to describe the sequential phonetic structures of the audio segments to a good degree, with very attractive real world applications such as query-by-example Spoken Term Detection (STD). In this STD application, the proposed approach significantly outperformed the conventional Dynamic Time Warping (DTW) based approaches at significantly lower computation requirements. We propose unsupervised learning of Audio Word2Vec from audio data without human annotation using Sequence-to-sequence Autoencoder (SA). SA consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN (encoder) maps the input audio sequence into a vector representation of fixed dimensionality, and the second RNN (decoder) maps the representation back to the input audio sequence. The two RNNs are jointly trained by minimizing the reconstruction error. Denoising Sequence-to-sequence Autoencoder (DSA) is further proposed offering more robust learning",
    "checked": true,
    "id": "a5c7e4f9600952996cc3dfeca58ce2176bee6360",
    "semantic_title": "audio word2vec: unsupervised learning of audio segment representations using sequence-to-sequence autoencoder",
    "citation_count": 174
  },
  "https://www.isca-speech.org/archive/interspeech_2016/meng16_interspeech.html": {
    "title": "Non-Uniform Boosted MCE Training of Deep Neural Networks for Keyword Spotting",
    "volume": "main",
    "abstract": "Keyword spotting can be formulated as a non-uniform error automatic speech recognition (ASR) problem. It has been demonstrated [1] that this new formulation with the non-uniform MCE training technique can lead to improved system performance in keyword spotting applications. In this paper, we demonstrate that deep neural networks (DNNs) can be successfully trained on the non-uniform minimum classification error (MCE) criterion which weighs the errors on keywords much more significantly than those on non-keywords in an ASR task. The integration with a DNN-HMM system enables modeling of multi-frame distributions, which conventional systems find difficult to accomplish. To further improve the performance, more confusable data is generated by boosting the likelihood of the sentences that have more errors. The keyword spotting system is implemented within a weighted finite state transducer (WFST) framework and the DNN is optimized using standard backpropagation and stochastic gradient decent. We evaluate the performance of the proposed framework on a large vocabulary spontaneous conversational telephone speech dataset (Switchboard-1 Release 2). The proposed approach achieves an absolute figure of merit improvement of 3.65% over the baseline system",
    "checked": true,
    "id": "7ecf64cc52bdaa8f8426d02241c21c4ac31b2af4",
    "semantic_title": "non-uniform boosted mce training of deep neural networks for keyword spotting",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gorin16_interspeech.html": {
    "title": "Language Model Data Augmentation for Keyword Spotting in Low-Resourced Training Conditions",
    "volume": "main",
    "abstract": "This research extends our earlier work on using machine translation (MT) and word-based recurrent neural networks to augment language model training data for keyword search in conversational Cantonese speech. MT-based data augmentation is applied to two language pairs: English-Lithuanian and English-Amharic. Using filtered N-best MT hypotheses for language modeling is found to perform better than just using the 1-best translation. Target language texts collected from the Web and filtered to select conversational-like data are used in several manners. In addition to using Web data for training the language model of the speech recognizer, we further investigate using this data to improve the language model and phrase table of the MT system to get better translations of the English data. Finally, generating text data with a character-based recurrent neural network is investigated. This approach allows new word forms to be produced, providing a way to reduce the out-of-vocabulary rate and thereby improve keyword spotting performance. We study how these different methods of language model data augmentation impact speech-to-text and keyword spotting performance for the Lithuanian and Amharic languages. The best results are obtained by combining all of the explored methods",
    "checked": true,
    "id": "c54d5e8c8ef72302d1e6262742a674833fec3bf8",
    "semantic_title": "language model data augmentation for keyword spotting in low-resourced training conditions",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2016/verwimp16_interspeech.html": {
    "title": "STON: Efficient Subtitling in Dutch Using State-of-the-Art Tools",
    "volume": "main",
    "abstract": "We present a modular video subtitling platform that integrates speech/non-speech segmentation, speaker diarisation, language identification, Dutch speech recognition with state-of-the-art acoustic models and language models optimised for efficient subtitling, appropriate pre- and postprocessing of the data and alignment of the final result with the video fragment. Moreover, the system is able to learn from subtitles that are newly created. The platform is developed for the Flemish national broadcaster VRT in the context of the project STON, and enables the easy upload of a new fragment and inspection of both the timings and results of each step in the subtitling process",
    "checked": true,
    "id": "cd6efeffb29ed8b406d9713609f6520c821f4a93",
    "semantic_title": "ston: efficient subtitling in dutch using state-of-the-art tools",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/stanislav16_interspeech.html": {
    "title": "An Automatic Training Tool for Air Traffic Control Training",
    "volume": "main",
    "abstract": "In this paper we presents an automatic training tool (ATT) for air traffic control officer (ATCO) trainees. It was developed using our cloud-based speech recognition and text-to-speech systems and allows dynamically generate the content. Our system significantly expands the available training materials, allowing ATCOs practice the basics of communication and phraseology. Furthermore, the automatic training tool is designed generally to be used for teaching in various areas, from specialized skills to a simple general knowledge",
    "checked": true,
    "id": "e17a44c89fc3b2e852a7ad7f720cc45a2dd3de6e",
    "semantic_title": "an automatic training tool for air traffic control training",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/karhila16_interspeech.html": {
    "title": "Digitala: An Augmented Test and Review Process Prototype for High-Stakes Spoken Foreign Language Examination",
    "volume": "main",
    "abstract": "This paper introduces the first prototype for a computerised examination procedure of spoken foreign languages in Finland, intended for national scale upper secondary school final examinations. Speech technology and profiling of reviewers are used to minimise the otherwise massive reviewing effort",
    "checked": true,
    "id": "926a7a7460ae1fb2d5c37b4eadd4126f5054b31d",
    "semantic_title": "digitala: an augmented test and review process prototype for high-stakes spoken foreign language examination",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/damnati16_interspeech.html": {
    "title": "Exploring Collections of Multimedia Archives Through Innovative Interfaces in the Context of Digital Humanities",
    "volume": "main",
    "abstract": "STIK is a platform that gathers Speech, Texts and Images of Knowledge. It allows browsing and navigating through collections of multimedia, facilitating access to archives in the domain of Knowledge resources. STIK includes a back-end with a specific automatic metadata extraction pipeline, a front-end with innovative interfaces for navigating within a document and a specific implementation of a search engine with dedicated key-word search functionality. It gathers multimedia contents from Canal-U, a French institution that exploits audiovisual archives produced by Higher Education and Research, with various formats and various academic disciplines. STIK is a contribution to the emerging domain of Digital Humanities",
    "checked": true,
    "id": "aa5733e7d18f84b9230cfe0482250b2b9ead5899",
    "semantic_title": "exploring collections of multimedia archives through innovative interfaces in the context of digital humanities",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yuan16_interspeech.html": {
    "title": "Learning Neural Network Representations Using Cross-Lingual Bottleneck Features with Word-Pair Information",
    "volume": "main",
    "abstract": "We assume that only word pairs identified by human are available in a low-resource target language. The word pairs are parameterized by a bottleneck feature (BNF) extractor that is trained using transcribed data in a high-resource language. The cross-lingual BNFs of the word pairs are used for training another neural network to generate a new feature representation in the target language. Pairwise learning of frame-level and word-level feature representations are investigated. Our proposed feature representations were evaluated in a word discrimination task on the Switchboard telephone speech corpus. Our learned features could bring 27.5% relative improvement over the previously best reported result on the task",
    "checked": true,
    "id": "4dd36b91997f949b4b69cd0a9c2c8b13e144e2ac",
    "semantic_title": "learning neural network representations using cross-lingual bottleneck features with word-pair information",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16d_interspeech.html": {
    "title": "Novel Front-End Features Based on Neural Graph Embeddings for DNN-HMM and LSTM-CTC Acoustic Modeling",
    "volume": "main",
    "abstract": "In this paper we investigate neural graph embeddings as front-end features for various deep neural network (DNN) architectures for speech recognition. Neural graph embedding features are produced by an autoencoder that maps graph structures defined over speech samples to a continuous vector space. The resulting feature representation is then used to augment the standard acoustic features at the input level of a DNN classifier. We compare two different neural graph embedding methods, one based on a local neighborhood graph encoding, and another based on a global similarity graph encoding. They are evaluated in DNN-HMM-based and LSTM-CTC-based ASR systems on a 110-hour Switchboard conversational speech recognition task. Significant improvements in word error rates are achieved by both methods in the DNN-HMM system, and by global graph embeddings in the LSTM-CTC system",
    "checked": true,
    "id": "1495ae96e0464d652001367c5cae8529bb718fd4",
    "semantic_title": "novel front-end features based on neural graph embeddings for dnn-hmm and lstm-ctc acoustic modeling",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/abraham16_interspeech.html": {
    "title": "Articulatory Feature Extraction Using CTC to Build Articulatory Classifiers Without Forced Frame Alignments for Speech Recognition",
    "volume": "main",
    "abstract": "Articulatory features provide robustness to speaker and environment variability by incorporating speech production knowledge. Pseudo articulatory features are a way of extracting articulatory features using articulatory classifiers trained from speech data. One of the major problems faced in building articulatory classifiers is the requirement of speech data aligned in terms of articulatory feature values at frame level. Manually aligning data at frame level is a tedious task and alignments obtained from the phone alignments using phone-to-articulatory feature mapping are prone to errors. In this paper, a technique using connectionist temporal classification (CTC) criterion to train an articulatory classifier using bidirectional long short-term memory (BLSTM) recurrent neural network (RNN) is proposed. The CTC criterion eliminates the need for forced frame level alignments. Articulatory classifiers were also built using different neural network architectures like deep neural networks (DNN), convolutional neural network (CNN) and BLSTM with frame level alignments and were compared to the proposed approach of using CTC. Among the different architectures, articulatory features extracted using articulatory classifiers built with BLSTM gave better recognition performance. Further, the proposed approach of BLSTM with CTC gave the best overall performance on both SVitchboard (6 hours) and Switchboard 33 hours data set",
    "checked": true,
    "id": "09030d3d68326c0a86c65ca6c5e60a99fdd756fd",
    "semantic_title": "articulatory feature extraction using ctc to build articulatory classifiers without forced frame alignments for speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nagamine16_interspeech.html": {
    "title": "On the Role of Nonlinear Transformations in Deep Neural Network Acoustic Models",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) are widely utilized for acoustic modeling in speech recognition systems. Through training, DNNs used for phoneme recognition nonlinearly transform the time-frequency representation of a speech signal into a sequence of invariant phonemic categories. However, little is known about how this nonlinear mapping is performed and what its implications are for the classification of individual phones and phonemic categories. In this paper, we analyze a sigmoid DNN trained for a phoneme recognition task and characterized several aspects of the nonlinear transformations that occur in hidden layers. We show that the function learned by deeper hidden layers becomes increasingly nonlinear, and that network selectively warps the feature space so as to increase the discriminability of acoustically similar phones, aiding in their classification. We also demonstrate that the nonlinear transformation of the feature space in deeper layers is more dedicated to the phone instances that are more difficult to discriminate, while the more separable phones are dealt with in the superficial layers of the network. This study describes how successive nonlinear transformations are applied to the feature space non-uniformly when a deep neural network model learns categorical boundaries, which may partly explain their superior performance in pattern classification applications",
    "checked": true,
    "id": "75b59511132b408f857dd92f4788eb9fe5cdebd0",
    "semantic_title": "on the role of nonlinear transformations in deep neural network acoustic models",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2016/variani16_interspeech.html": {
    "title": "Complex Linear Projection (CLP): A Discriminative Approach to Joint Feature Extraction and Acoustic Modeling",
    "volume": "main",
    "abstract": "State-of-the-art automatic speech recognition (ASR) systems typically rely on pre-processed features. This paper studies the time-frequency duality in ASR feature extraction methods and proposes extending the standard acoustic model with a complex-valued linear projection layer to learn and optimize features that minimize standard cost functions such as cross-entropy. The proposed Complex Linear Projection (CLP) features achieve superior performance compared to pre-processed Log Mel features",
    "checked": true,
    "id": "fa28f1d96146a18138ab21aaf94640ec185ff33d",
    "semantic_title": "complex linear projection (clp): a discriminative approach to joint feature extraction and acoustic modeling",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sainath16_interspeech.html": {
    "title": "Modeling Time-Frequency Patterns with LSTM vs. Convolutional Architectures for LVCSR Tasks",
    "volume": "main",
    "abstract": "Various neural network architectures have been proposed in the literature to model 2D correlations in the input signal, including convolutional layers, frequency LSTMs and 2D LSTMs such as time-frequency LSTMs, grid LSTMs and ReNet LSTMs. It has been argued that frequency LSTMs can model translational variations similar to CNNs, and 2D LSTMs can model even more variations [1], but no proper comparison has been done for speech tasks. While convolutional layers have been a popular technique in speech tasks, this paper compares convolutional and LSTM architectures to model time-frequency patterns as the first layer in an LDNN [2] architecture. This comparison is particularly interesting when the convolutional layer degrades performance, such as in noisy conditions or when the learned filterbank is not constant-Q [3]. We find that grid-LDNNs offer the best performance of all techniques, and provide between a 1–4% relative improvement over an LDNN and CLDNN on 3 different large vocabulary Voice Search tasks",
    "checked": true,
    "id": "4ce8da2f3de109f13fc7033dd37e81da01a9cbcb",
    "semantic_title": "modeling time-frequency patterns with lstm vs. convolutional architectures for lvcsr tasks",
    "citation_count": 65
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mclaren16_interspeech.html": {
    "title": "The Speakers in the Wild (SITW) Speaker Recognition Database",
    "volume": "main",
    "abstract": "The Speakers in the Wild (SITW) speaker recognition database contains hand-annotated speech samples from open-source media for the purpose of benchmarking text-independent speaker recognition technology on single and multi-speaker audio acquired across unconstrained or \"wild\" conditions. The database consists of recordings of 299 speakers, with an average of eight different sessions per person. Unlike existing databases for speaker recognition, this data was not collected under controlled conditions and thus contains real noise, reverberation, intra-speaker variability and compression artifacts. These factors are often convolved in the real world, as the SITW data shows, and they make SITW a challenging database for single- and multi-speaker recognition",
    "checked": true,
    "id": "3fe358a66359ee2660ec0d13e727eb8f3f0007c2",
    "semantic_title": "the speakers in the wild (sitw) speaker recognition database",
    "citation_count": 231
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mclaren16b_interspeech.html": {
    "title": "The 2016 Speakers in the Wild Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "The newly collected Speakers in the Wild (SITW) database was central to a text-independent speaker recognition challenge held as part of a special session at Interspeech 2016. The SITW database is composed of audio recordings from 299 speakers collected from open source media, with an average of 8 sessions per speaker. The recordings contain unconstrained or \"wild\" acoustic conditions, rarely found in large speaker recognition datasets, and multi-speaker recordings for both speaker enrollment and verification. This article provides details of the SITW speaker recognition challenge and analysis of evaluation results. There were 25 international teams involved in the challenge of which 11 teams participated in an evaluation track. Teams were tasked with applying existing and novel speaker recognition algorithms to the challenges associated with the real world conditions of SITW. We provide an analysis of some of the top performing systems submitted during the evaluation and provide future research directions",
    "checked": true,
    "id": "a0778b2833f0572306b0eb911d4a421a600421e4",
    "semantic_title": "the 2016 speakers in the wild speaker recognition evaluation",
    "citation_count": 51
  },
  "https://www.isca-speech.org/archive/interspeech_2016/novotny16_interspeech.html": {
    "title": "Analysis of Speaker Recognition Systems in Realistic Scenarios of the SITW 2016 Challenge",
    "volume": "main",
    "abstract": "In this paper, we summarize our efforts for the Speakers In The Wild (SITW) challenge, and we present our findings with this new dataset for speaker recognition. Apart from the standard comparison of different SRE systems, we analyze the use of diarization for dealing with audio segments containing multiple speakers, as in part of the newly introduced enrollment and test protocols, diarization is a necessary system component. Our state-of-the-art systems used in this work utilize both cepstral and DNN-based bottleneck features and are based on i-vectors followed by Probabilistic Linear Discriminant Analysis (PLDA) classifier and logistic regression calibration/fusion. We present both narrow-band (8 kHz) and wide-band (16 kHz) systems together with their fusions",
    "checked": true,
    "id": "90d534a83e08dddb05c3adade74d3bb03ea19e6a",
    "semantic_title": "analysis of speaker recognition systems in realistic scenarios of the sitw 2016 challenge",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kudashev16_interspeech.html": {
    "title": "A Speaker Recognition System for the SITW Challenge",
    "volume": "main",
    "abstract": "This paper presents an ITMO university system submitted to the Speakers in the Wild (SITW) Speaker Recognition Challenge. During evaluation track of the SITW challenge we explored conventional universal background model (UBM) Gaussian mixture model (GMM) i-vector systems and recently developed DNN-posteriors based i-vector systems. The systems were investigated under the real-world media channel conditions represented in the challenge. This paper discusses practical issues of the robust i-vector systems training and performs investigation of denoising autoencoder (DAE) based back-end when applied to \"in the wild\" conditions. Our speak-er diarization approach for \"multi-speaker in the file\" conditions is also briefly presented in the paper. Experiments per-formed on the evaluation dataset demonstrate that DNN- based i-vector systems are superior to the UBM-GMM based sys-tems and applying DAE-based back-end helps to improve system performance",
    "checked": true,
    "id": "8fd99a573862548d50dc5368c0be9c9787285933",
    "semantic_title": "a speaker recognition system for the sitw challenge",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ghaemmaghami16_interspeech.html": {
    "title": "Speakers In The Wild (SITW): The QUT Speaker Recognition System",
    "volume": "main",
    "abstract": "This paper presents the QUT speaker recognition system, as a competing system in the Speakers In The Wild (SITW) speaker recognition challenge. Our proposed system achieved an overall ranking of second place, in the main core-core condition evaluations of the SITW challenge. This system uses an i-vector/PLDA approach, with domain adaptation and a deep neural network (DNN) trained to provide feature statistics. The statistics are accumulated by using class posteriors from the DNN, in place of GMM component posteriors in a typical GMM-UBM i-vector/PLDA system. Once the statistics have been collected, the i-vector computation is carried out as in a GMM-UBM based system. We apply domain adaptation to the extracted i-vectors to ensure robustness against dataset variability, PLDA modelling is used to capture speaker and session variability in the i-vector space, and the processed i-vectors are compared using the batch likelihood ratio. The final scores are calibrated to obtain the calibrated likelihood scores, which are then used to carry out speaker recognition and evaluate the performance of the system. Finally, we explore the practical application of our system to the core-multi condition recordings of the SITW data and propose a technique for speaker recognition in recordings with multiple speakers",
    "checked": true,
    "id": "9a9a3550cee7f3445b86146ebc5eff4c09a692a7",
    "semantic_title": "speakers in the wild (sitw): the qut speaker recognition system",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/khosravani16_interspeech.html": {
    "title": "AUT System for SITW Speaker Recognition Challenge",
    "volume": "main",
    "abstract": "This document intends to present AUT speaker recognition system submitted to SITW (Speakers in the Wild) speaker recognition challenge. This challenge aims to provide real world data across a wide range of acoustic and environmental conditions in the context of automatic speaker recognition so as to facilitate the development of new algorithms. The presented system is based on the state-of-the-art i-vector/PLDA and source normalization techniques. The system has been developed on publically available databases and evaluated on the data provided by SITW challenge. Taking advantage of the challenge development data, our experiments indicate that source normalization can help speaker recognition system to better adapt to the evaluation condition. Post evaluation analysis is conducted on the conditions of SITW database",
    "checked": true,
    "id": "348f42f088abeda63810270dc2002fd60433e520",
    "semantic_title": "aut system for sitw speaker recognition challenge",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kheder16_interspeech.html": {
    "title": "LIA System for the SITW Speaker Recognition Challenge",
    "volume": "main",
    "abstract": "This paper presents the speaker verification systems developed in the LIA lab at the University of Avignon for the SITW (Speakers In The Wild) challenge. We present the algorithms used to deal with additive noise, short utterances and propose an improved scoring scheme using a discriminative classifier and integrating the homogeneity of the two compared recordings. Due to the heterogeneity of this database (presence of background noise, reverberation, Lombard effect, etc.), it is hard to analyze the contribution of individual techniques used to deal with each problem. For this reason, a subset of the trials will be studied for each algorithm in order to emphasize its contribution",
    "checked": true,
    "id": "dd5ef5052f05d9b717f923ae023f810469c2f14d",
    "semantic_title": "lia system for the sitw speaker recognition challenge",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16e_interspeech.html": {
    "title": "Investigating Various Diarization Algorithms for Speaker in the Wild (SITW) Speaker Recognition Challenge",
    "volume": "main",
    "abstract": "Collecting training data for real-world text-independent speaker recognition is challenging. In practice, utterances for a specific speaker are often mixed with many other acoustic signals. To guarantee the recognition performance, the segments spoken by target speakers should be precisely picked out. An automatic detection could be developed to reduce the cost of expensive human hand-made annotations. One way to achieve this goal is by using speaker diarization as a pre-processing step in the speaker enrollment phase. To this end, three speaker diarization algorithms based on Bayesian information criterion (BIC), agglomerative information bottleneck (aIB) and i-vector are investigated in this paper. The corresponding impacts on the results of speaker recognition system are also studied. Experiments conducted on Speaker in the Wild (SITW) Speaker Recognition Challenge (SRC) 2016 showed that the utilization of a proper speaker diarization improves the overall performance. Some more efforts are made to combine these methods together as well",
    "checked": true,
    "id": "691e32f42711425ab48fbd7abe1ce887d3dc2a02",
    "semantic_title": "investigating various diarization algorithms for speaker in the wild (sitw) speaker recognition challenge",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2016/scharenborg16_interspeech.html": {
    "title": "Does the Importance of Word-Initial and Word-Final Information Differ in Native versus Non-Native Spoken-Word Recognition?",
    "volume": "main",
    "abstract": "This paper investigates whether the importance and use of word-initial and word-final information in spoken-word recognition is dependent on whether one is listening in a native or a non-native language and on the presence of background noise. Native English and non-native Dutch and Finnish listeners participated in an English word recognition experiment, where either a word's onset or offset was masked by speech-shaped noise with different signal-to-noise ratios. The results showed that for all listener groups the masking of word onset information was more detrimental to spoken-word recognition than the masking of word offset information. The reliance on word-initial information was larger in harder listening conditions for the English but not so for the Dutch and Finnish listeners. Moreover, no significant differences in the use of word-initial and word-final information were found between the two non-native listener groups. Taken together, these results show that the reliance on word-initial information in deteriorating listening conditions seems to be dependent on whether one is listening in one's native or a non-native language rather than on the listener's native language",
    "checked": true,
    "id": "31938320f778903095ce3bd8b081579ff2d69ebe",
    "semantic_title": "does the importance of word-initial and word-final information differ in native versus non-native spoken-word recognition?",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/scharenborg16b_interspeech.html": {
    "title": "The Effect of Sentence Accent on Non-Native Speech Perception in Noise",
    "volume": "main",
    "abstract": "This paper investigates the uptake and use of prosodic information signalling sentence accent during native and non-native speech perception in the presence of background noise. A phoneme monitoring experiment was carried out in which English, Dutch, and Finnish listeners were presented with target phonemes in semantically unpredictable yet meaningful English sentences. Sentences were presented in different levels of speech-shaped noise and, crucially, in two prosodic contexts in which the target-bearing word was either deaccented or accented. Results showed that overall performance was high for both the native and the non-native listeners; however, where native listeners seemed able to partially overcome the problems at the acoustic level in degraded listening conditions by using prosodic information signalling upcoming sentence accent, non-native listeners could not do so to the same extent. These results support the hypothesis that the performance difference between native and non-native listeners in the presence of background noise is, at least partially, caused by a reduced exploitation of contextual information during speech processing by non-native listeners",
    "checked": true,
    "id": "d0e6815640532f4fb17ae4107ba1033aff24ca5a",
    "semantic_title": "the effect of sentence accent on non-native speech perception in noise",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cooke16_interspeech.html": {
    "title": "The Effects of Modified Speech Styles on Intelligibility for Non-Native Listeners",
    "volume": "main",
    "abstract": "Speech output, including modified and synthetic speech, is used increasingly in natural settings where message reception might be affected by noise. Recent evaluations have demonstrated the effect of different speech styles on intelligibility for native listeners, but their impact on listening in a second language is less well-understood. The current study measured the intelligibility of four speech styles in the presence of stationary and fluctuating maskers for a non-native listener cohort, and compared the results with those of native listeners on the same task. Both groups showed a similar pattern of effects, but the scale of intelligibility gains and losses with respect to plain speech was significantly compressed for the non-native group relative to native listeners. In addition, non-native listeners identified speech from the four styles in the absence of noise, revealing that styles shown to be beneficial in noise lost their benefits or were harmful in quiet conditions. This result suggests that while enhanced styles lead to gains by reducing the effect of masking noise, the same styles distort the acoustic-phonetic integrity of the speech signal. More work is needed to develop speech modification approaches that simultaneously preserve speech information and promote unmasking",
    "checked": true,
    "id": "391e8bb6ff731e2419e5cecc0ed1a5fda581a166",
    "semantic_title": "the effects of modified speech styles on intelligibility for non-native listeners",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16c_interspeech.html": {
    "title": "The Influence of Language Experience on the Categorical Perception of Vowels: Evidence from Mandarin and Korean",
    "volume": "main",
    "abstract": "Previous research on categorical perception of speech sounds has demonstrated a strong influence of language experience on the categorical perception of consonants and lexical tones. In order to explore the influence of language experience on vowel perception, the present study investigated the perceptual performance for Mandarin and Korean listeners along a vowel continuum, which spanned three vowel categories /a/, /ɜ/, and /u/. The results showed that both language groups exhibited categorical features in vowel perception, with a sharper categorical boundary of /ɜ/-/u/ than that of /a/-/ɜ/. Moreover, the differences found between the two groups revealed that the Korean listeners' perception tended to be more categorical along the /a/-/ɜ/-/u/ vowel continuum than that of the Mandarin listeners. Furthermore, the Mandarin listeners tended to label stimuli more often as /a/ and less often as /u/ than the Korean counterparts. These perceptual differences between the Mandarin and Korean groups might be attributed to the different acoustic distribution in the F1×F2 vowel space of the two different native languages",
    "checked": true,
    "id": "badfdce9217f6b0e17a38eb1f9aaf1cf1259275b",
    "semantic_title": "the influence of language experience on the categorical perception of vowels: evidence from mandarin and korean",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/massaro16_interspeech.html": {
    "title": "Multiple Influences on Vocabulary Acquisition: Parental Input Dominates",
    "volume": "main",
    "abstract": "How spoken language is acquired has been an active area of inquiry in linguistic, psychological, and speech science. New advances in this controversial field are promising given the recent accumulation of large databases of children's speech understanding and production, as well as various properties of words. This paper explores the contribution of a variety of potential influences on vocabulary acquisition including difficulty of articulation, iconicity, log parental input frequency, lexical category, and imageability. The influence of difficulty of articulation, iconicity ratings, and imagery ratings decreased more or less linearly with increasing age. Lexical category effects were fairly small. Parental input in terms of child directed speech has by far the largest influence. Multiple regressions with these variables give a fairly complete account of spoken vocabulary acquisition. The increasing availability of large databases promises progress in this area of inquiry",
    "checked": true,
    "id": "db7885d471d001aa16596b27d7d2ee93285317d8",
    "semantic_title": "multiple influences on vocabulary acquisition: parental input dominates",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gong16b_interspeech.html": {
    "title": "Can Intensive Exposure to Foreign Language Sounds Affect the Perception of Native Sounds?",
    "volume": "main",
    "abstract": "A possible side-effect of exposure to non-native sounds is a change in the way we perceive native sounds. Previous studies have demonstrated that native speakers' speech production can change as a result of learning a new language, but little work has been carried out to measure the perceptual consequences of exposure. The current study examined how intensive exposure to Spanish intervocalic consonants affected Chinese learners with no prior experience of Spanish. Before, during and after a training period, listeners undertook both an adaptive noise task, which measured the noise level at which listeners could identify native language consonants, and an assimilation task, in which listeners assigned Spanish consonants to Chinese consonant categories. Listeners exhibited a significantly reduced noise tolerance for the Chinese consonants /l/ and /w/ following exposure to Spanish. These two consonants also showed the largest reductions in Spanish to Chinese category assimilations. Taken together, these findings suggest that Chinese listeners modified their native language categories boundaries as a result of exposure to Spanish sounds in order to accommodate them, and that as a consequence their identification performance in noise reduced. Some differences between the two sounds in the time-course of recovery from perceptual adaptation were observed",
    "checked": true,
    "id": "694f70fca26ca86928750092656080dc6511fae8",
    "semantic_title": "can intensive exposure to foreign language sounds affect the perception of native sounds?",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bassiou16_interspeech.html": {
    "title": "Privacy-Preserving Speech Analytics for Automatic Assessment of Student Collaboration",
    "volume": "main",
    "abstract": "This work investigates whether nonlexical information from speech can automatically predict the quality of small-group collaborations. Audio was collected from students as they collaborated in groups of three to solve math problems. Experts in education annotated 30-second time windows by hand for collaboration quality. Speech activity features (computed at the group level) and spectral, temporal and prosodic features (extracted at the speaker level) were explored. After the latter were transformed from the speaker level to the group level, features were fused. Results using support vector machines and random forests show that feature fusion yields best classification performance. The corresponding unweighted average F measure on a 4-class prediction task ranges between 40% and 50%, significantly higher than chance (12%). Speech activity features alone are strong predictors of collaboration quality, achieving an F measure between 35% and 43%. Speaker-based acoustic features alone achieve lower classification performance, but offer value in fusion. These findings illustrate that the approach under study offers promise for future monitoring of group dynamics, and should be attractive for many collaboration activity settings in which privacy is desired",
    "checked": true,
    "id": "73e23bcbc766ed4e748166b9c57eca29a90deade",
    "semantic_title": "privacy-preserving speech analytics for automatic assessment of student collaboration",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nasir16_interspeech.html": {
    "title": "Complexity in Prosody: A Nonlinear Dynamical Systems Approach for Dyadic Conversations; Behavior and Outcomes in Couples Therapy",
    "volume": "main",
    "abstract": "In this paper, we model dyadic human conversational interactions from a nonlinear dynamical systems perspective. We focus on deriving measures of the underlying system complexity using the observed dyadic behavioral signals. Specifically, we analyze different measures of complexity in prosody of speech (pitch and energy) during dyadic conversations of couples with marital conflict. We evaluate the importance of these measures as features by correlating them with different behavioral attributes of the couple codified in terms of behavioral codes. Furthermore, we investigate the relation between the computed complexity and outcomes of couples therapy. The results show that the derived complexity measures are more correlated to session level behavioral codes, and to the marital therapy outcomes, compared to traditional speech prosody features. It shows that nonlinear dynamical analysis of speech acoustic features can be a useful tool for behavioral analysis",
    "checked": true,
    "id": "0ed885517ceb48497d4276649b477c0bc23c43ea",
    "semantic_title": "complexity in prosody: a nonlinear dynamical systems approach for dyadic conversations; behavior and outcomes in couples therapy",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tseng16_interspeech.html": {
    "title": "Couples Behavior Modeling and Annotation Using Low-Resource LSTM Language Models",
    "volume": "main",
    "abstract": "Observational studies on couple interactions are often based on manual annotations of a set of behavior codes. Such annotations are expensive, time-consuming, and often suffer from low inter-annotator agreement. In previous studies it has been shown that the lexical channels contain sufficient information for capturing behavior and predicting the interaction labels, and various automated processes using language models have been proposed. However, current methods are restricted to a small context window due to the difficulty of training language models with limited data as well as the lack of frame-level labels. In this paper we investigate the application of recurrent neural networks for capturing behavior trajectories through larger context windows. We solve the issue of data sparsity and improve robustness by introducing out-of-domain knowledge through pretrained word representations. Finally, we show that our system can accurately estimate true rating values of couples interactions using a fusion of the frame-level behavior trajectories. The ratings predicted by our proposed system achieve inter-annotator agreements comparable to those of trained human annotators Importantly, our system promises robust handling of out of domain data, exploitation of longer context, on-line feedback with continuous labels and easy fusion with other modalities",
    "checked": true,
    "id": "eb781804dbea972c8a29e5590bf74bbf157a9822",
    "semantic_title": "couples behavior modeling and annotation using low-resource lstm language models",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gallardo16_interspeech.html": {
    "title": "Speech Likability and Personality-Based Social Relations: A Round-Robin Analysis over Communication Channels",
    "volume": "main",
    "abstract": "The Social Relations Model is well-known for analyses of interpersonal attraction. As a novelty in this paper, the model is applied to assess different effects on likability ratings from speech only. A group of 30 unacquainted participants is considered in our experiment. Their voices were recorded and transmitted through communication channels, and ratings of speech likability and speaker personality were then collected from the same individuals following a round-robin approach. This setup enabled us to detect the influence of participants' personality and of narrowband and wideband speech on the sources of variance according to the Social Relations Model. An analysis of acoustic correlates of speech likability has also been conducted, which shows differences in the relevance of speech features and in the description of likability ratings depending on the speech bandwidth",
    "checked": true,
    "id": "8a367f507a0540830ca4248578d0620041bca7a2",
    "semantic_title": "speech likability and personality-based social relations: a round-robin analysis over communication channels",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xiao16_interspeech.html": {
    "title": "Behavioral Coding of Therapist Language in Addiction Counseling Using Recurrent Neural Networks",
    "volume": "main",
    "abstract": "Manual annotation of human behaviors with domain specific codes is a primary method of research and treatment fidelity evaluation in psychotherapy. However, manual annotation has a prohibitively high cost and does not scale to coding large amounts of psychotherapy session data. In this paper, we present a case study of modeling therapist language in addiction counseling, and propose an automatic coding approach. The task objective is to code therapist utterances with domain specific codes. We employ Recurrent Neural Networks (RNNs) to predict these behavioral codes based on session transcripts. Experiments show that RNNs outperform the baseline method using Maximum Entropy models. The model with bi-directional Gated Recurrent Units and domain specific word embeddings achieved the highest overall accuracy. We also briefly discuss about client code prediction and comparison to previous work",
    "checked": true,
    "id": "fc1f7582769d777d29f9c07b71fe483204b0fd81",
    "semantic_title": "behavioral coding of therapist language in addiction counseling using recurrent neural networks",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dang16_interspeech.html": {
    "title": "Factor Analysis Based Speaker Normalisation for Continuous Emotion Prediction",
    "volume": "main",
    "abstract": "Speaker variability has been shown to be a significant confounding factor in speech based emotion classification systems and a number of speaker normalisation techniques have been proposed. However, speaker normalisation in systems that predict continuous multidimensional descriptions of emotion such as arousal and valence has not been explored. This paper investigates the effect of speaker variability in such speech based continuous emotion prediction systems and proposes a factor analysis based speaker normalisation technique. The proposed technique operates directly on the feature space and decomposes it into speaker and emotion specific sub-spaces. The proposed technique is validated on both the USC CreativeIT database and the SEMAINE database and leads to improvements of 8.2% and 11.0% (in terms of correlation coefficient) on the two databases respectively when predicting arousal",
    "checked": true,
    "id": "bd169475388fa2b3844de04da94032abc2e71115",
    "semantic_title": "factor analysis based speaker normalisation for continuous emotion prediction",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ram16_interspeech.html": {
    "title": "Subspace Detection of DNN Posterior Probabilities via Sparse Representation for Query by Example Spoken Term Detection",
    "volume": "main",
    "abstract": "We cast the query by example spoken term detection (QbE-STD) problem as subspace detection where query and background subspaces are modeled as union of low-dimensional subspaces. The speech exemplars used for subspace modeling are class-conditional posterior probabilities estimated using deep neural network (DNN). The query and background training exemplars are exploited to model the underlying low-dimensional subspaces through dictionary learning for sparse representation. Given the dictionaries characterizing the query and background subspaces, QbE-STD is performed based on the ratio of the two corresponding sparse representation reconstruction errors. The proposed subspace detection method can be formulated as the generalized likelihood ratio test for composite hypothesis testing. The experimental evaluation demonstrate that the proposed method is able to detect the query given a single example and performs significantly better than a highly competitive QbE-STD baseline system based on dynamic time warping (DTW) for exemplar matching",
    "checked": true,
    "id": "137d0f22f80a766ed37a3adc05ca5aedd8dae6b3",
    "semantic_title": "subspace detection of dnn posterior probabilities via sparse representation for query by example spoken term detection",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16d_interspeech.html": {
    "title": "Unsupervised Bottleneck Features for Low-Resource Query-by-Example Spoken Term Detection",
    "volume": "main",
    "abstract": "We propose a framework which ports Dirichlet Gaussian mixture model (DPGMM) based labels to deep neural network (DNN). The DNN trained using the unsupervised labels is used to extract a low-dimensional unsupervised speech representation, named as unsupervised bottleneck features (uBNFs), which capture considerable information for sound cluster discrimination. We investigate the performance of uBNF in query-by-example spoken term detection (QbE-STD) on the TIMIT English speech corpus. Our uBNF performs comparably with the cross-lingual bottleneck features (BNFs) extracted from a DNN trained using 171 hours of transcribed telephone speech in another language (Mandarin Chinese). With the score fusion of uBNFs and cross-lingual BNFs, we gain about 10% relative improvement in terms of mean average precision (MAP) comparing with the cross-lingual BNFs. We also study the performance of the framework with different input features and different lengths of temporal context",
    "checked": true,
    "id": "b99384f75264f8e63401b83ace2fdf4d61af952d",
    "semantic_title": "unsupervised bottleneck features for low-resource query-by-example spoken term detection",
    "citation_count": 38
  },
  "https://www.isca-speech.org/archive/interspeech_2016/torbati16_interspeech.html": {
    "title": "A Nonparametric Bayesian Approach for Spoken Term Detection by Example Query",
    "volume": "main",
    "abstract": "State of the art speech recognition systems use data-intensive context-dependent phonemes as acoustic units. However, these approaches do not translate well to low resourced languages where large amounts of training data is not available. For such languages, automatic discovery of acoustic units is critical. In this paper, we demonstrate the application of nonparametric Bayesian models to acoustic unit discovery. We show that the discovered units are correlated with phonemes and therefore are linguistically meaningful We also present a spoken term detection (STD) by example query algorithm based on these automatically learned units. We show that our proposed system produces a P@N of 61.2% and an EER of 13.95% on the TIMIT dataset. The improvement in the EER is 5% while P@N is only slightly lower than the best reported system in the literature",
    "checked": true,
    "id": "3a5f11c65e23bcf9a47e8bd0414cb1fafeac65ad",
    "semantic_title": "a nonparametric bayesian approach for spoken term detection by example query",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pham16_interspeech.html": {
    "title": "Rescoring Hypothesized Detections of Out-of-Vocabulary Keywords Using Subword Samples",
    "volume": "main",
    "abstract": "Rescoring hypothesized detections, using keyword's audio samples extracted from training data, is an effective way to improve the performance of a Keyword Search (KWS) system. Unfortunately such rescoring framework cannot be applied directly to Out-of-Vocabulary (OOV) keywords since there is no sample in the training data. To address this limitation, we propose two techniques for OOV keywords in this work. The first technique generates samples for an OOV keyword by concatenating samples of its constituent subwords. The second technique splits hypothesized detections into segments, then estimates the acoustic similarities between detections and subword's samples according to the similarities between segments and these samples. The similarity scores from these two techniques are used to rescore and re-rank the list of detections returned by the automatic speech recognition (ASR) systems. The experiments show that incorporating the proposed similarity scores results in a better separation between the correct and false alarm detections than using the ASR scores alone. Furthermore, experimental results on the NIST OpenKWS15 Evaluation show that rescoring with the proposed similarity scores significantly outperforms the raw ASR scores, and other methods that do not use the similarity scores, in both Maximum Term Weighted Value (MTWV) and Mean Average Precision (MAP) metrics",
    "checked": true,
    "id": "b5034e8ffd185c33ccb8f72bf934fc7b3b94e8d4",
    "semantic_title": "rescoring hypothesized detections of out-of-vocabulary keywords using subword samples",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhuang16_interspeech.html": {
    "title": "Unrestricted Vocabulary Keyword Spotting Using LSTM-CTC",
    "volume": "main",
    "abstract": "Keyword spotting (KWS) aims to detect predefined keywords in continuous speech. Recently, direct deep learning approaches have been used for KWS and achieved great success. However, these approaches mostly assume fixed keyword vocabulary and require significant retraining efforts if new keywords are to be detected. For unrestricted vocabulary, HMM based keyword-filler framework is still the mainstream technique. In this paper, a novel deep learning approach is proposed for unrestricted vocabulary KWS based on Connectionist Temporal Classification (CTC) with Long Short-Term Memory (LSTM). Here, an LSTM is trained to discriminant phones with the CTC criterion. During KWS, an arbitrary keyword can be specified and it is represented by one or more phone sequences. Due to the property of peaky phone posteriors of CTC, the LSTM can produce a phone lattice. Then, a fast substring matching algorithm based on minimum edit distance is used to search the keyword phone sequence on the phone lattice. The approach is highly efficient and vocabulary independent. Experiments showed that the proposed approach can achieve significantly better results compared to a DNN-HMM based keyword-filler decoding system. In addition, the proposed approach is also more efficient than the DNN-HMM KWS baseline",
    "checked": true,
    "id": "bddf8115d6f685d08d7deda0cf225a6859f7b186",
    "semantic_title": "unrestricted vocabulary keyword spotting using lstm-ctc",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wu16b_interspeech.html": {
    "title": "Interactive Spoken Content Retrieval by Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "User-machine interaction is important for spoken content retrieval. For text content retrieval, the user can easily scan through and select on a list of retrieved item. This is impossible for spoken content retrieval, because the retrieved items are difficult to show on screen. Besides, due to the high degree of uncertainty for speech recognition, the retrieval results can be very noisy. One way to counter such difficulties is through user-machine interaction. The machine can take different actions to interact with the user to obtain better retrieval results before showing to the user. The suitable actions depend on the retrieval status, for example requesting for extra information from the user, returning a list of topics for user to select, etc. In our previous work, some hand-crafted states estimated from the present retrieval results are used to determine the proper actions. In this paper, we propose to use Deep-Q-Learning techniques instead to determine the machine actions for interactive spoken content retrieval. Deep-Q-Learning bypasses the need for estimation of the hand-crafted states, and directly determine the best action base on the present retrieval status even without any human knowledge. It is shown to achieve significantly better performance compared with the previous hand-crafted states",
    "checked": true,
    "id": "8049049144472e58698fe32f428e3a2cd4926e1e",
    "semantic_title": "interactive spoken content retrieval by deep reinforcement learning",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2016/godoy16_interspeech.html": {
    "title": "Relating Estimated Cyclic Spectral Peak Frequency to Measured Epilarynx Length Using Magnetic Resonance Imaging",
    "volume": "main",
    "abstract": "The epilarynx plays an important role in speech production, carrying information about the individual speaker and manner of articulation. However, precise acoustic behavior of this lower vocal tract structure is difficult to establish. Focusing on acoustics observable in natural speech, recent spectral processing techniques isolate a unique resonance with characteristics of the epilarynx previously shown via simulation, specifically cyclicity (i.e. energy differences between the closed and open phases of the glottal cycle) in a 3–5kHz region observed across vowels. Using Magnetic Resonance Imaging (MRI), the present work relates this estimated cyclic peak frequency to measured epilarynx length. Assuming a simple quarter wavelength relationship, the cavity length estimated from the cyclic peak frequency is shown to be directly proportional (linear fit slope =1.1) and highly correlated (ρ = 0.85, pval<10 ) to the measured epilarynx length across speakers. Results are discussed, as are implications in speech science and application domains",
    "checked": true,
    "id": "b5313808490154e1a772d65b5b408026cf9b92ef",
    "semantic_title": "relating estimated cyclic spectral peak frequency to measured epilarynx length using magnetic resonance imaging",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tobing16_interspeech.html": {
    "title": "Acoustic-to-Articulatory Inversion Mapping Based on Latent Trajectory Gaussian Mixture Model",
    "volume": "main",
    "abstract": "A maximum likelihood parameter trajectory estimation based on a Gaussian mixture model (GMM) has been successfully implemented for acoustic-to-articulatory inversion mapping. In the conventional method, GMM parameters are optimized by maximizing a likelihood function for joint static and dynamic features of acoustic-articulatory data, and then, the articulatory parameter trajectories are estimated for given the acoustic data by maximizing a likelihood function for only the static features, imposing a constraint between static and dynamic features to consider the inter-frame correlation. Due to the inconsistency of the training and mapping criterion, the trained GMM is not optimum for the mapping process. This inconsistency problem is addressed within a trajectory training framework, but it becomes more difficult to optimize some parameters, e.g., covariance matrices and mixture component sequences. In this paper, we propose an inversion mapping method based on a latent trajectory GMM (LT-GMM) as yet another way to overcome the inconsistency issue. The proposed method makes it possible to use a well-formulated algorithm, such as EM algorithm, to optimize the LT-GMM parameters, which is not feasible in the traditional trajectory training. Experimental results demonstrate that the proposed method yields higher accuracy in the inversion mapping compared to the conventional GMM-based method",
    "checked": true,
    "id": "2b9d6ea505958580a9ff9071eab75470e984b299",
    "semantic_title": "acoustic-to-articulatory inversion mapping based on latent trajectory gaussian mixture model",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dissen16_interspeech.html": {
    "title": "Formant Estimation and Tracking Using Deep Learning",
    "volume": "main",
    "abstract": "Formant frequency estimation and tracking are among the most fundamental problems in speech processing. In the former task the input is a stationary speech segment such as the middle part of a vowel and the goal is to estimate the formant frequencies, whereas in the latter task the input is a series of speech frames and the goal is to track the trajectory of the formant frequencies throughout the signal. Traditionally, formant estimation and tracking is done using ad-hoc signal processing methods. In this paper we propose using machine learning techniques trained on an annotated corpus of read speech for these tasks. Our feature set is composed of LPC-based cepstral coefficients with a range of model orders and pitch-synchronous cepstral coefficients. Two deep network architectures are used as learning algorithms: a deep feed-forward network for the estimation task and a recurrent neural network for the tracking task. The performance of our methods compares favorably with mainstream LPC-based implementations and state-of-the-art tracking algorithms",
    "checked": true,
    "id": "5377e41bc6d124192d95788880f0a186ce0a29c3",
    "semantic_title": "formant estimation and tracking using deep learning",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2016/vaz16_interspeech.html": {
    "title": "Convex Hull Convolutive Non-Negative Matrix Factorization for Uncovering Temporal Patterns in Multivariate Time-Series Data",
    "volume": "main",
    "abstract": "We propose the Convex Hull Convolutive Non-negative Matrix Factorization (CH-CNMF) algorithm to learn temporal patterns in multivariate time-series data. The algorithm factors a data matrix into a basis tensor that contains temporal patterns and an activation matrix that indicates the time instants when the temporal patterns occurred in the data. Importantly, the temporal patterns correspond closely to the observed data and represent a wide range of dynamics. Experiments with synthetic data show that the temporal patterns found by CH-CNMF match the data better and provide more meaningful information than the temporal patterns found by Convolutive Non-negative Matrix Factorization with sparsity constraints (CNMF-SC). Additionally, CH-CNMF applied on vocal tract constriction data yields a wider range of articulatory gestures compared to CNMF-SC. Moreover, we find that the gestures comprising the CH-CNMF basis generalize better to unseen data and capture vocal tract structure and dynamics significantly better than those comprising the CNMF-SC basis",
    "checked": true,
    "id": "7c27c597e11918fcaa66b10546631227bcf0a116",
    "semantic_title": "convex hull convolutive non-negative matrix factorization for uncovering temporal patterns in multivariate time-series data",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2016/juvela16_interspeech.html": {
    "title": "Majorisation-Minimisation Based Optimisation of the Composite Autoregressive System with Application to Glottal Inverse Filtering",
    "volume": "main",
    "abstract": "The composite autoregressive system can be used to estimate a speech source-filter decomposition in a rigorous manner, thus having potential use in glottal inverse filtering. By introducing a suitable prior, spectral tilt can be introduced into the source component estimation to better correspond to human voice production. However, the current expectation-maximisation based composite autoregressive model optimisation leaves room for improvement in terms of speed. Inspired by majorisation-minimisation techniques used for nonnegative matrix factorisation, this work derives new update rules for the model, resulting in faster convergence compared to the original approach. Additionally, we present a new glottal inverse filtering method based on the composite autoregressive system and compare it with inverse filtering methods currently used in glottal excitation modelling for parametric speech synthesis. These initial results show that the proposed method performs comparatively well, sometimes outperforming the reference methods",
    "checked": true,
    "id": "fb36b0a090b79ce86a48a0002275d4711862d60e",
    "semantic_title": "majorisation-minimisation based optimisation of the composite autoregressive system with application to glottal inverse filtering",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16c_interspeech.html": {
    "title": "F0 Contour Analysis Based on Empirical Mode Decomposition for DNN Acoustic Modeling in Mandarin Speech Recognition",
    "volume": "main",
    "abstract": "Tone information provides a strong distinction for many ambiguous characters in Mandarin Chinese. The use of tonal acoustic units and F related tonal features have been shown to be effective at improving the accuracy of Mandarin automatic speech recognition (ASR) systems, as F contains the most prominent tonal information for distinguishing words that are phonemically identical. Both long-term temporal intonations and short-term quick variations coexist in F Using untreated F as an acoustic feature renders the F contour patterns differently from their citation form and downplays the significance of tonal information in ASR. In this paper, we explore the empirical mode decomposition (EMD) on F contours to reconstruct F related tonal features with a view to removing the components that are irrelevant for Mandarin ASR.We investigate both GMM-HMM and DNN-HMM based acoustic modeling with the reconstructed tonal features. In comparison with the baseline systems using typical tonal features, our best system using reconstructed tonal features leads to a 4.5% relative word error rate reduction for the GMM-HMM system and a 3.5% relative word error rate reduction for the DNN-HMM system",
    "checked": true,
    "id": "06cec55bc65a1b73fa8ca04c73a62b393351cae9",
    "semantic_title": "f0 contour analysis based on empirical mode decomposition for dnn acoustic modeling in mandarin speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hu16_interspeech.html": {
    "title": "Vowels and Diphthongs in Cangnan Southern Min Chinese Dialect",
    "volume": "main",
    "abstract": "This paper gives an acoustic phonetic description of the vowels and diphthongs in Cangnan Southern Min Chinese dialect. Vowel formant data from 10 speakers (5 male and 5 female) show that the distribution of Cangnan monophthongs in the acoustic vowel space is of particular typological interest. Diphthong production is examined in terms of temporal organization, spectral property, and dynamic aspects. Results suggest that the production of falling diphthongs tends to be a single articulatory event with a dynamic spectral target, while the production of rising diphthongs and level diphthongs is a sequence of two spectral targets",
    "checked": true,
    "id": "3e15a627fdf92aeeb0e206b5e52e1f6e49ef528d",
    "semantic_title": "vowels and diphthongs in cangnan southern min chinese dialect",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hu16b_interspeech.html": {
    "title": "Diphthongization of Nuclear Vowels and the Emergence of a Tetraphthong in Hetang Cantonese",
    "volume": "main",
    "abstract": "This paper is an acoustic phonetic description of vowels in Hetang Cantonese, and focuses on the diphthongization of nuclear vowels. Different to the representative dialect such as Guangzhou or Hong Kong Cantonese, the Hetang dialect exhibits its unique characteristics regarding the phonetics and phonology of vowels. A noticeable phenomenon is the diphthongization of nuclear vowels. And, a tetraphthong [uɔᵄi] emerges when the nuclear vowel is diphthongized in a triphthong",
    "checked": true,
    "id": "51632ad3f3dab37ccb36d887efd9f81718064259",
    "semantic_title": "diphthongization of nuclear vowels and the emergence of a tetraphthong in hetang cantonese",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cernak16b_interspeech.html": {
    "title": "PhonVoc: A Phonetic and Phonological Vocoding Toolkit",
    "volume": "main",
    "abstract": "We present the PhonVoc toolkit, a cascaded deep neural network (DNN) composed of speech analyser and synthesizer that use a shared phonetic and/or phonological speech representation. The free toolkit is distributed as open-source software under a BSD 3-Clause License, available at https://github.com/idiap/phonvoc with the pre-trained US English analysis and synthesis DNNs, and thus it is ready for immediate use In a broader context, the toolkit implements training and testing of the analysis by synthesis heuristic model. It is thus designed for the wider speech community working in acoustic phonetics, laboratory phonology, and parametric speech coding. The toolkit interprets the phonetic posterior probabilities as a sequential scheme, whereas the phonological posterior-class probabilities are considered as a parallel via K different phonological classes. A case study is presented on a LibriSpeech database and a LibriVox US English native female speaker. The phonetic and phonological vocoding yield comparable performance, improving speech quality by merging the phonetic and phonological speech representation",
    "checked": true,
    "id": "01866ab8cf7fd2e58c494e46959b06000b36ce48",
    "semantic_title": "phonvoc: a phonetic and phonological vocoding toolkit",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xia16b_interspeech.html": {
    "title": "Vowels and Diphthongs in the Taiyuan Jin Chinese Dialect",
    "volume": "main",
    "abstract": "On the basis of an acoustic phonetic analysis of monophthongs and diphthongs, this paper describes vowel phonology in the Taiyuan Jin dialect. The results show that Taiyuan has a comparable but different vowel inventory for C(G)V versus C(G)VN syllables. And the vowel contrast is dramatically reduced in checked syllables. The asymmetry between falling and rising diphthongs suggests a dynamic account of vowels, rather than a sequential taxonomy of vowels into monophthongs and diphthongs. Phonetically, monophthongs are composed of a static spectral target, falling diphthongs are composed of a dynamic spectral target, and rising diphthongs are sequences of two spectral targets. Phonologically, falling diphthongs are grouped with monophthongs, rather than rising diphthongs",
    "checked": true,
    "id": "720a4f4f2f7d09f2daba639a83c61ef2102c64c6",
    "semantic_title": "vowels and diphthongs in the taiyuan jin chinese dialect",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/turco16_interspeech.html": {
    "title": "The Effects of Prosody on French V-to-V Coarticulation: A Corpus-Based Study",
    "volume": "main",
    "abstract": "This study examines whether the degree of vowel-to-vowel coarticulation in French (better known as \"vowel height harmony\", V2-to-V1 henceforth) varies as a function of position in prosodic domain (i.e. IP initial vs. word-medial) and duration of V1 (i.e. short vs. long). Following the literature on the phonetics-prosody interface, segments at stronger edges are more resistant to coarticulatory effects induced by their neighboring vowel. While previous studies have mainly looked at non-local V-to-V coarticulation across prosodic boundaries/domains (e.g.,V#(C)V), here we look at V2-to-V1 coarticulation within an Intonational Phrase according to whether target V1 is in absolute initial position (#V1C(C)V2, e.g., #essaie [esε]/[εsε] – ‘try') or not (word-medial, e.g., épaissit [epεsi]/[epesi] – ‘thikened'). The analyses are based on 33k words presenting possible V1C(C)V2 harmonic contexts, which were extracted from a corpus of French running speech. V2-to-V1 coarticulation is measured as the lowering of the first formant of the target V1 (/e, ε, o, ɔ/) in relation to the height of the V2 trigger /+high/ (i.e. mid-high and high) vs. /-high/ (i.e. mid-low and low). Results show an effect of prosodic position (but no effect of V1 duration) on V2-to-V1 coarticulation: V1 is more resistant to coarticulation when initial in an IP",
    "checked": true,
    "id": "2985c2ab2da60cde0d4b0f5626fe466da0446a08",
    "semantic_title": "the effects of prosody on french v-to-v coarticulation: a corpus-based study",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/galata16_interspeech.html": {
    "title": "An Acoustic Analysis of /r/ in Tyrolean",
    "volume": "main",
    "abstract": "This paper offers a preliminary contribution to the phonetic description and acoustic characterization of /r/ allophony in Tyrolean dialect, an under-researched South Bavarian Dialect spoken in the North of Italy. The analysis of target words containing /r/ in different phonotactic contexts, produced by six Tyrolean female speakers, confirms the high degree of intra-speaker variation in the production of /r/ with a uvular place of articulation. The distributional analysis of the allophones in our sample shows a preference among all the speakers for a fricative manner of articulation followed by approximants and taps and, to a lesser extent, by trills (with a very small amount of vocalized variants). These results are in line with previous research in the South-Tyrolean community. Due to the high variability of rhotic sounds, we further investigate and report on some of their shared acoustic features such as duration across the different phonotactic contexts and Harmonics-to-Noise Ratio for the different allophones attested",
    "checked": true,
    "id": "4fab6af04eacbecb261ea8ebfb2f6d92eab3ce3a",
    "semantic_title": "an acoustic analysis of /r/ in tyrolean",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chang16_interspeech.html": {
    "title": "Hyperarticulated Production of Korean Glides by Age Group",
    "volume": "main",
    "abstract": "This research uses the hyperspace effect (Johnson, Flemming, & Wright, 1993; Lindblom, 1990) of Korean glides to address the issues triggered by the diachronic sound change of some Korean vowels. Specifically, we examine whether there is any difference between Korean ‘wae [wε]' versus ‘oe [we]' by speech style (casual and clear speech) and speakers' age. Twenty adults from Seoul and the Kyunggi area participated: (i) a younger group (21–34 years old) and (ii) an older group (44–71 years old). The first and second formant frequencies (Hz) were measured at two time points: (i) onset of test syllable and (ii) vowel midpoint. The results showed that the transitional trait of glides \"wae [wε]\" and \"oe [we]\" at initial timing of syllable was more enhanced in clear speech than in casual speech, as predicted. However, no phonetic evidence was found for the difference between \"wae [wε]\" and \"oe [we]\" in terms of F1 and F2, even in clear speech. Also, no systematic difference of age group depending on vowel type was found. Therefore, we argue that the diachronic sound merge between \"wae [wε]\" and \"oe [we]\" is now completed even in the Seoul area and for older groups",
    "checked": true,
    "id": "6e2f786e6465568a4b20a8fa492c513a9c37ae93",
    "semantic_title": "hyperarticulated production of korean glides by age group",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pan16_interspeech.html": {
    "title": "Coda Stop and Taiwan Min Checked Tone Sound Changes",
    "volume": "main",
    "abstract": "This acoustical and Electroglottography (EGG) study investigates the effect of coda deletion and co-articulatory phasing on vowels and final coda stops, [p t k ʔ], in Taiwan Min checked tones 3 and 5 syllables. Vowel duration, f0, spectral tilt (H1*-A3*), cepstral peak prominence (CPP) and glottal contact quotient (CQ_H) were analyzed. Compensatory lengthening, f0 lowering and increasing periodic phonation during the production of vowels after coda deletion were observed. During gradual phasing when codas were produced as energy damping, the vowels were found to be shorter in duration and less periodic in voicing than vowels abruptly phased with coda that were produced as full stop closure. However, spectral tilt H1*-A3* was not affected by either coda deletion or co-articulatory phasing. Therefore, these findings suggest that H1*-A3* may play a salient role in checked tone identification, and, as a result, is unaffected by sound change",
    "checked": true,
    "id": "01eb4a4e2fb8b190b8eab9b5b5d16ffd051ea450",
    "semantic_title": "coda stop and taiwan min checked tone sound changes",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fenwick16_interspeech.html": {
    "title": "The Influence of Modality and Speaking Style on the Assimilation Type and Categorization Consistency of Non-Native Speech",
    "volume": "main",
    "abstract": "The Perceptual Assimilation Model [1] proposes that non-native contrast discrimination accuracy can be predicted by perceptual assimilation type. However, assimilation types have been based just on auditory-only (AO) citation speech. Since auditory-visual (AV) and clear speech can benefit non-native speech perception [2, 3], we reasoned that modality and speaking style could influence assimilation. This was tested by presenting English monolinguals Sindhi consonants in a categorization task. Results showed that, across speaking styles, consonants were assimilated the same way in AV and AO. For consonants that were uncategorized in visual-only (VO) conditions: 1) their AO counterpart was more consistently categorized than AV; and 2) citation speech was also more consistently categorized than clear. Interestingly, this set of results was reversed for consonants that were assimilated to the same native category across modalities; participants were able to use the visual articulatory information to make more consistent categorization judgments for AV than AO. This was also the case for speaking style: clear speech was more consistently categorized than citation. Together these results demonstrate that the extent to which AV and clear speech is beneficial for cross-language perception may depend on the similarities between the articulatory characteristics of native and non-native consonants",
    "checked": true,
    "id": "f4ca226805ccced645bb9b191043c29a6534cfb0",
    "semantic_title": "the influence of modality and speaking style on the assimilation type and categorization consistency of non-native speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zellers16_interspeech.html": {
    "title": "Prosodic Convergence with Spoken Stimuli in Laboratory Data",
    "volume": "main",
    "abstract": "Accommodation or convergence between speakers has been shown to occur on a variety of levels of linguistic structure. Phonetic convergence appears to be a very variable phenomenon in conversation, with social roles strongly influencing who accommodates to whom. Since phonetic convergence appears to be strongly under speaker control, it is unclear whether speakers might converge phonetically in a laboratory setting. The current study investigates accommodation of pitch and duration features in data collected in a laboratory setting. While speakers in the study did not converge to spoken stimuli in terms of duration features, they did converge to an extent on pitch features. However, only some information-structure contexts led to convergence, suggesting that even in a laboratory setting, speakers are aware of the discourse implications of their production",
    "checked": true,
    "id": "a0ed0794200ef216248feed95ed24516d07ecc1f",
    "semantic_title": "prosodic convergence with spoken stimuli in laboratory data",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/themistocleous16_interspeech.html": {
    "title": "Effects of Stress on Fricatives: Evidence from Standard Modern Greek",
    "volume": "main",
    "abstract": "This study investigates the effects of stress on the spectral properties of fricative noise in Standard Modern Greek (SMG). Twenty female speakers of SMG participated in the study. Fricatives were produced in stressed and unstressed positions in two vowel place positions: back and front vowels. Acoustic measurements were taken and the temporal and spectral properties of fricatives — using spectral moments — were calculated. Stressed fricatives are produced with increased duration, center of gravity, standard deviation, and normalized intensity. The machine learning and classification algorithm C5.0 has been employed to estimate the contribution of the temporal and spectral parameters for the classification of fricatives. Overall, duration and center of gravity contribute the most to the classification of stressed vs. unstressed fricatives",
    "checked": true,
    "id": "4a691c1e96235cbea56c7f7ee6fd0d2e7b3d307d",
    "semantic_title": "effects of stress on fricatives: evidence from standard modern greek",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sun16b_interspeech.html": {
    "title": "Analysis of Chinese Syllable Durations in Running Speech of Japanese L2 Learners",
    "volume": "main",
    "abstract": "Aiming at better understanding of prosody generation by native Japanese learners of Mandarin as a second language (L2), we analyzed the syllable duration differences between tone types. By comparing the mean syllable durations and the variation of normalized syllable durations across tone types and speakers, significant differences were found between tone types as well as between speakers. Native Chinese speakers generate tone 1 and tone 2 with relatively long durations but smaller variations, contrary to tone 3 and tone 4. Japanese L2 learners generate tone 3 with relatively high variations compared to the other tones, while the mean duration of tone 4 was remarkably different from natives. Compared with native speakers, the variations of both tone 3 and tone 4 are significantly smaller. Furthermore, the neutral tone caused a significant increase of the mean variation across tones for the Japanese L2 learners. The results suggest that native Chinese speakers control syllable durations adaptively with tones, especially for tone 3 and tone 4, in running speech while Japanese L2 learners tend to pronounce them in isolated syllable fashion",
    "checked": true,
    "id": "61067f9ded5b2bb9808c0ee352eeedc9f2c2a5d9",
    "semantic_title": "analysis of chinese syllable durations in running speech of japanese l2 learners",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lai16b_interspeech.html": {
    "title": "Automatic Paragraph Segmentation with Lexical and Prosodic Features",
    "volume": "main",
    "abstract": "As long-form spoken documents become more ubiquitous in everyday life, so does the need for automatic discourse segmentation in spoken language processing tasks. Although previous work has focused on broad topic segmentation, detection of finer-grained discourse units, such as paragraphs, is highly desirable for presenting and analyzing spoken content. To better understand how different aspects of speech cue these subtle discourse transitions, we investigate automatic paragraph segmentation of TED talks. We build lexical and prosodic paragraph segmenters using Support Vector Machines, AdaBoost, and Long Short Term Memory (LSTM) recurrent neural networks. In general, we find that induced cue words and supra-sentential prosodic features outperform features based on topical coherence, syntactic form and complexity. However, our best performance is achieved by combining a wide range of individually weak lexical and prosodic features, with the sequence modelling LSTM generally outperforming the other classifiers by a large margin. Moreover, we find that models that allow lower level interactions between different feature types produce better results than treating lexical and prosodic contributions as separate, independent information sources",
    "checked": true,
    "id": "fd1a124b890682fb8ad9eb512740db736d573594",
    "semantic_title": "automatic paragraph segmentation with lexical and prosodic features",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/airaksinen16_interspeech.html": {
    "title": "Automatic Glottal Inverse Filtering with Non-Negative Matrix Factorization",
    "volume": "main",
    "abstract": "This study presents an automatic glottal inverse filtering (GIF) technique based on separating the effect of the glottal main excitation from the impulse response of the vocal tract. The proposed method is based on a non-negative matrix factorization (NMF) based decomposition of an ultra short-term spectrogram of the analyzed signal. Unlike other state-of-the-art GIF techniques, the proposed method does not require estimation of glottal closure instants The proposed method was objectively evaluated with two test sets of continuous synthetic speech created with a glottal vocoding analysis/synthesis procedure. When compared to a set of reference GIF methods, the proposed NMF technique shows improved estimation accuracy especially for male voices",
    "checked": true,
    "id": "63bef67f7773cbe7773cbaeaf5e5519296141eb9",
    "semantic_title": "automatic glottal inverse filtering with non-negative matrix factorization",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/park16_interspeech.html": {
    "title": "Speaker Identity and Voice Quality: Modeling Human Responses and Automatic Speaker Recognition",
    "volume": "main",
    "abstract": "Despite recent breakthroughs in automatic speaker recognition (ASpR), system performance still degrades when utterances are short and/or when within-speaker variability is large. This study used short test utterances (2–3sec) to investigate the effect of within-speaker variability on state-of-the-art ASpR system performance. A subset of a newly-developed UCLA database is used, which contains multiple speech tasks per speaker. The short utterances combined with a speaking-style mismatch between read sentences and spontaneous affective speech degraded system performance, for 25 female speakers, by 36%. Because humans are more robust to utterance length or within-speaker variability, understanding human perception might benefit ASpR systems. Perception experiments were conducted with recorded read sentences from 3 female speakers, and a model is proposed to predict the perceptual dissimilarity between tokens. Results showed that a set of voice quality features including F0, F1, F2, F3, H1*-H2*, H2*-H4*, H4*-H2k*, H2k*-H5k, and CPP provides information that complements MFCCs. By fusing the feature set with MFCCs, human response prediction RMS error was .12, which represents a 12% relative error reduction compared to using MFCCs alone. In ASpR experiments with short utterances from 50 speakers, the voice quality feature set decreased the error rate by 11% when fused with MFCCs",
    "checked": true,
    "id": "f0758cc76b5fe82d99977994e27cc1540e1bd5a7",
    "semantic_title": "speaker identity and voice quality: modeling human responses and automatic speaker recognition",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kalita16_interspeech.html": {
    "title": "Analysis of Glottal Stop in Assam Sora Language",
    "volume": "main",
    "abstract": "The objective of this work is to characterize the intervocalic glottal stops in Assam Sora. Assam Sora is a low resource language of the South Munda language family. Glottal stops are produced with gestures in the deep laryngeal level; hence, the estimated excitation source signal is used in this study to characterize the source dynamics during the production of Assam Sora glottal stops. From that, temporal domain voice source features, Quasi-Open Quotient (QOQ) and Normalized Amplitude Quotient (NAQ) are extracted along with spectral features such as H1-H2 ratio and Harmonic Richness Factor (HRF). One excitation source feature is extracted from the zero frequency filtered version of the speech signal to characterize the variations within the glottal cycles in glottal stop region. A recently proposed wavelet based voice source feature, Maxima Dispersion Quotient (MDQ) is also used to characterize the abrupt glottal closure during glottal stop production. From the analysis, it is observed that the features are salient enough to uniquely characterize glottal stops from the adjacent vowel sounds and may also be used in continuous speech. A Mann-Whitney U test confirmed the statistical significance of the differences between glottal stops and their adjacent vowels",
    "checked": true,
    "id": "fb504e241df9f62562c1f6f582da4d5f370752c1",
    "semantic_title": "analysis of glottal stop in assam sora language",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/garellek16_interspeech.html": {
    "title": "Acoustic Differences Between English /t/ Glottalization and Phrasal Creak",
    "volume": "main",
    "abstract": "In American English, the presence of creaky voice can derive from distinct linguistic processes, including phrasal creak (prolonged irregular voicing, often at edges of prosodic phrases) and coda /t/ glottalization (when the alveolar closure for syllable-final /t/ is replaced by or produced simultaneously with glottal constriction). Previous work has shown that listeners can differentiate words in phrasal creak from those with /t/ glottalization, which suggests that there are acoustic differences between the creaky voice derived from phrasal creak and /t/ glottalization. In this study, we analyzed vowels preceding syllable-final /t/ in the Buckeye Corpus, which includes audio recordings of spontaneous speech from 40 speakers of American English. Tokens were coded for presence of phrasal creak (prolonged irregular voicing extending beyond the target syllable) and /t/ glottalization (whether the /t/ was produced only with glottal constriction). Eleven spectral measures of voice quality, including both harmonic and noise measures, were extracted automatically and discriminant analyses were performed. The results indicate that the discriminant functions can classify these sources of creaky voice above chance, and that Cepstral Peak Prominence, a measure of harmonics-to-noise ratio, is important for distinguishing phrasal creak from glottalization",
    "checked": true,
    "id": "6d00b3b3e359a1be0d15abc44d707b3f49bbf228",
    "semantic_title": "acoustic differences between english /t/ glottalization and phrasal creak",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2016/eriksson16_interspeech.html": {
    "title": "The Acoustics of Lexical Stress in Italian as a Function of Stress Level and Speaking Style",
    "volume": "main",
    "abstract": "The study is part of a series of studies, describing the acoustics of lexical stress in a way that should be applicable to any language. The present database of recordings includes Brazilian Portuguese, English, Estonian, German, French, Italian and Swedish. The acoustic parameters examined are F -level, F -variation, Duration, and Spectral Emphasis. Values for these parameters, computed for all vowels (a little over 24000 vowels for Italian), are the data upon which the analyses are based. All parameters are examined with respect to their correlation with Stress (primary, secondary, unstressed) and speaking Style (wordlist reading, phrase reading, spontaneous speech) and Sex of the speaker (female, male). For Italian Duration was found to be the dominant factor by a wide margin, in agreement with previous studies. Spectral Emphasis was the second most important factor. Spectral Emphasis has not been studied previously for Italian but intensity, a related parameter, has been shown to correlate with stress. F -level was also significantly correlated but not to the same degree. Speaker Sex turned out as significant in many comparisons. The differences were, however, mainly a function of the degree to which a given parameter was used, not how it was used to signal lexical stress contrasts",
    "checked": true,
    "id": "97c5199c4f01ac48e7c2021f73213abaad207911",
    "semantic_title": "the acoustics of lexical stress in italian as a function of stress level and speaking style",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schweitzer16_interspeech.html": {
    "title": "Cross-Gender and Cross-Dialect Tone Recognition for Vietnamese",
    "volume": "main",
    "abstract": "We investigate tone recognition in Vietnamese across gender and dialects. In addition to well-known parameters such as single fundamental frequency (F0) values and energy features, we explore the impact of harmonicity on recognition accuracy, as well as that of the PaIntE parameters, which quantify the shape of the F0 contour over complete syllables instead of providing more local single values. Using these new features for tone recognition in the GlobalPhone database, we observe significant improvements of approx. 1% in recognition accuracy when adding harmonicity, and of another approx. 4% when adding the PaIntE parameters. Furthermore, we analyze the influence of gender and dialect on recognition accuracy. The results show that it is easier to recognize tones for female than for male speakers, and easier for the Northern dialect than for the Southern dialect. Moreover, we achieve reasonable results testing models across gender, while the performance drops strongly when testing across dialects",
    "checked": true,
    "id": "84bd45dfede7f7aae13dca78cb78c746f325c312",
    "semantic_title": "cross-gender and cross-dialect tone recognition for vietnamese",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/vijayan16_interspeech.html": {
    "title": "Prosody Modification Using Allpass Residual of Speech Signals",
    "volume": "main",
    "abstract": "In this paper, we attempt to signify the role of phase spectrum of speech signals in acquiring an accurate estimate of excitation source for prosody modification. The phase spectrum is parametrically modeled as the response of an allpass (AP) filter, and the filter coefficients are estimated by considering the linear prediction (LP) residual as the output of the AP filter. The resultant residual signal, namely AP residual, exhibits unambiguous peaks corresponding to epochs, which are chosen as pitch markers for prosody modification. This strategy efficiently removes ambiguities associated with pitch marking, required for pitch synchronous overlap-add (PSOLA) method. The prosody modification using AP residual is advantageous than time domain PSOLA (TD-PSOLA) using speech signals, as it offers fewer distortions due to its flat magnitude spectrum. Windowing centered around unambiguous peaks in AP residual is used for segmentation, followed by pitch/duration modification of AP residual by mapping of pitch markers. The modified speech signal is obtained from modified AP residual using synthesis filters. The mean opinion scores are used for performance evaluation of the proposed method, and it is observed that the AP residual-based method delivers equivalent performance as that of LP residual-based method using epochs, and better performance than the linear prediction PSOLA (LP-PSOLA)",
    "checked": true,
    "id": "1d279dfb39a70a4dcc1a1b4806f7d55478edc2dd",
    "semantic_title": "prosody modification using allpass residual of speech signals",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kakouros16_interspeech.html": {
    "title": "Analyzing the Contribution of Top-Down Lexical and Bottom-Up Acoustic Cues in the Detection of Sentence Prominence",
    "volume": "main",
    "abstract": "Recent work has suggested that prominence perception could be driven by the predictability of the acoustic prosodic features of speech. On the other hand, lexical predictability and part of speech information are also known to correlate with prominence. In this paper, we investigate how the bottom-up acoustic and top-down lexical cues contribute to sentence prominence by using both types of features in unsupervised and supervised systems for automatic prominence detection. The study is conducted using a corpus of Dutch continuous speech with manually annotated prominence labels. Our results show that unpredictability of speech patterns is a consistent and important cue for prominence at both the lexical and acoustic levels, and also that lexical predictability and part-of-speech information can be used as efficient features in supervised prominence classifiers",
    "checked": true,
    "id": "10752bedc0a678703ebcc7ca7e8c92600a9f4597",
    "semantic_title": "analyzing the contribution of top-down lexical and bottom-up acoustic cues in the detection of sentence prominence",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kallay16_interspeech.html": {
    "title": "A Longitudinal Study of Children's Intonation in Narrative Speech",
    "volume": "main",
    "abstract": "Adults' narratives are hierarchically structured. This structure is evident in the linguistic and prosodic domains. Children's narratives have a flatter structure. This structure is evident in the linguistic domain, but less is known about the prosodic domain. Here, we report results from a longitudinal study of children's narratives that enhance our understanding of the development of discourse prosody. Spontaneous narratives were obtained from 60 children (aged 5 to 7) over a 3-year period. F0 was tracked to obtain absolute measures of slope steepness and linearity for every utterance of each narrative. These measures are known correlates of syntactic and semantic complexity. Slope direction and inter-utterance continuity in F0 were also calculated. These measures are known correlates of event boundaries in adult discourse. The results indicated systematic developmental changes related to age and year for all measures except slope steepness, consistent with developmental increases in linguistic complexity and the production of more adult-like narratives. The evidence also indicates that developmental change is most pronounced between the ages of 5 and 7 years, and levels out afterwards",
    "checked": true,
    "id": "c61e2ea05e1c413d61ae33bb123c8784e84d96cf",
    "semantic_title": "a longitudinal study of children's intonation in narrative speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/blaylock16_interspeech.html": {
    "title": "Velum Control for Oral Sounds",
    "volume": "main",
    "abstract": "Velum position during speech shows systematic variability within and across speakers, but has a binary phonological contrast (nasal and oral). Velum lowering is often thought to constitute an independent phonological unit, partly because of its robust prosodically-conditioned timing during nasal stops. Velum raising, on the other hand, is usually considered to be a non-phonological consequence of other vocal tract movements. Moreover, velum raising has almost always been observed in the context of nasals, and has rarely been studied in purely oral contexts. This experiment directly contrasts velum movement in oral and nasal contexts. The results show that temporal coordination of velum raising during oral stops resembles the temporal coordination of velum lowering during nasals, suggesting that velum position and movement are controlled for both raising and lowering. The results imply that some revisions to the Articulatory Phonology model may be appropriate, specifically with regards to the treatment of velum raising as an independent phonological unit",
    "checked": true,
    "id": "b31ae2bca2c89373c65bc8cd9c7ff41c0abeeaef",
    "semantic_title": "velum control for oral sounds",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/son16_interspeech.html": {
    "title": "F0 Development in Acquiring Korean Stop Distinction",
    "volume": "main",
    "abstract": "A number of studies have investigated the role of Voice Onset Time (VOT) on acquisition of stop voicing contrast. Korean stop contrasts (lenis, fortis, and aspirated), however, cannot be differentiated only by VOT since they are all pulmonic egressive voiceless stops. For this three-way distinction, another acoustic parameter, fundamental frequency (F0), critically operates. The present study explores how F0 is perceptually acquired and phonetically operates for Korean stop contrast over age. In order to reveal the relationship between F0 developmental patterns and age, a quantitative acoustic model dealt with word-initial stop productions by 58 Korean young children aged 20 months to 47 months. The results showed that phonetic accuracy depends on the perceptual thresholds in F0, and the significant phonetic differentiation with F0 between lenis and aspirated stops was significantly related to age. These findings suggest that acquisition of F0 plays a crucial role in the formation of phonemic categories for lenis and aspirated stops and this process significantly affects articulatory distinction",
    "checked": true,
    "id": "992410ea2bbe1bfd55a5336c73ca30733ebad6c6",
    "semantic_title": "f0 development in acquiring korean stop distinction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cohen16_interspeech.html": {
    "title": "Phonetic Reduction Can Lead to Lengthening, and Enhancement Can Lead to Shortening",
    "volume": "main",
    "abstract": "Contextually probable, high-frequency, or easily accessible words tend to be phonetically reduced, a pattern usually attributed to faster lexical access. In principle, word forms that are frequent in their inflectional paradigms should also enjoy faster lexical access, leading again to phonetic reduction. Yet research has found evidence of both reduction and enhancement on paradigmatically probable inflectional affixes. The current corpus study uses pronunciation data from conversationally produced English verbs and nouns to test the predictions of two accounts. In an exemplar account, paradigmatically probable forms seem enhanced because their denser exemplar clouds resist influence from related word forms on the average production target. A second pressure reduces such forms because they are, after all, more easily accessed. Under this account, paradigmatically probable forms should have longer affixes but shorter stems. An alternative account proposes that paradigmatically probable forms are produced in such a way as to enhance not articulation, but contrasts between related word forms. This account predicts lengthening of suffixed forms, and shortening of unsuffixed forms The results of the corpus study support the second account, suggesting that characterizing pronunciation variation in terms of phonetic reduction and enhancement oversimplifies the relationship between lexical storage, retrieval, and articulation",
    "checked": true,
    "id": "94790cfa290bf443c346bac66e0fe3b1aff3e7a5",
    "semantic_title": "phonetic reduction can lead to lengthening, and enhancement can lead to shortening",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/arai16_interspeech.html": {
    "title": "Mechanical Production of [b], [m] and [w] Using Controlled Labial and Velopharyngeal Gestures",
    "volume": "main",
    "abstract": "As an extension of a series of models we have developed, a mechanical bent vocal-tract model with nasal cavity was proposed for educational and clinical applications, as well as for understanding human speech production. Although our recent studies have focused on flap and approximant sounds, this paper introduced a new model for the consonants [b], [m] and [w]. Because the articulatory gesture of approximants is slow compared to the more rapid movement of plosives, in our [b] and [m] model, the elastic force of a spring is applied to affect the movement of the lower lip block, as was done for flap sounds in our previous studies. The main difference between [b] and [m] is in the velopharyngeal port, which is closed for [b] and open for [m]. In this study, we concluded that 1) a slower manipulation of the lip block is needed for [w], while 2) [b] and [m] require a faster movement, and finally, 3) close-open coordination of the lip and velopharyngeal gestures is important for [m]",
    "checked": true,
    "id": "2a90e725d8846432f2b69febe8092a3a758d27d2",
    "semantic_title": "mechanical production of [b], [m] and [w] using controlled labial and velopharyngeal gestures",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fang16_interspeech.html": {
    "title": "An Improved 3D Geometric Tongue Model",
    "volume": "main",
    "abstract": "This study describes an improved geometric articulatory model based on MRI and CBCT (Cone Beam Computer Tomography) data. The basic idea is to improve the coherence of the vertices of tongue meshes so as to obtain more accurate tongue model. This is conducted in two aspects: i) The representative vertices of tongue surface are depicted in Cartesian coordinate system rather than in a semi-polar gridline coordinate system. ii) tongue surface meshes are modeled with reference to anatomical landmarks. Then, guided PCA is used to extract the control components based on MRI data. The average reconstruction error is less than 1.0 mm. Both qualitative and quantitative evaluation indicates that the proposed method surpasses the conventional semi-polar gridline system based method",
    "checked": true,
    "id": "5a90e171a33f9749288049f61ab3323ae42a2384",
    "semantic_title": "an improved 3d geometric tongue model",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tiainen16_interspeech.html": {
    "title": "Congruency Effect Between Articulation and Grasping in Native English Speakers",
    "volume": "main",
    "abstract": "Previous studies have shown congruency effects between specific speech articulations and manual grasping actions. For example, uttering the syllable [kɑ] facilitates power grip responses in terms of reaction time and response accuracy. A similar association of the syllable [ti] with precision grip has also been observed. As these congruency effects have been to date shown only for Finnish native speakers, this study explored whether the congruency effects generalize to native speakers of another language. The original experiments were therefore replicated with English participants (N=16). Several previous findings were reproduced, namely the association of syllables [kɑ] and [ke] with power grip and of [ti] and [te] with precision grip. However, the association of vowels [ɑ] and [i] with power and precision grip, respectively, previously found for Finnish participants, was not significant for English speakers. This difference could be related to ambiguities of English orthography and pronunciation variations. It is possible that for English speakers seeing a certain written vowel activates several different phonological representations associated with that letter. If the congruency effects are based on interactions between specific phonological representations and grasp actions, this ambiguity might lead to weakening of the effects in the manner demonstrated here",
    "checked": true,
    "id": "8691dca68784c461124ee6d2ddd4ae2b8bd5ba0f",
    "semantic_title": "congruency effect between articulation and grasping in native english speakers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/najnin16b_interspeech.html": {
    "title": "Emergence of Vocal Developmental Sequences in a Predictive Coding Model of Speech Acquisition",
    "volume": "main",
    "abstract": "Learning temporal patterns among primitive speech sequences and being able to control the motor apparatus for effective production of the learned patterns are imperative for speech acquisition in infants. In this paper, we develop a predictive coding model whose objective is to minimize the sensory (auditory) and proprioceptive prediction errors. Temporal patterns are learned by minimizing the former while control is learned by minimizing the latter. The model is learned using a set of synthetically generated syllables, as in other contemporary models. We show that the proposed model outperforms existing ones in learning vocalization classes. It also computes the control/muscle activation which is useful for determining the degree of easiness of vocalization",
    "checked": true,
    "id": "b25cc66e3340fec8413a1b23038a8e644144d5ea",
    "semantic_title": "emergence of vocal developmental sequences in a predictive coding model of speech acquisition",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/meyer16_interspeech.html": {
    "title": "Categorization of Natural Spanish Whistled Vowels by Naïve Spanish Listeners",
    "volume": "main",
    "abstract": "Whistled speech in a non tonal language consists of the natural emulation of vocalic and consonantal qualities in a simple modulated whistled signal. This special speech register represents a natural telecommunication system that enables high levels of sentence intelligibility by trained speakers. It is not directly intelligible to naïve listeners. Yet, it is easily learned by speakers of the language that is being whistled, as attested by current efforts of revitalization of whistled Spanish in the Canary Islands. To understand better the relation between whistled and spoken speech perception, we looked here at how Spanish native speakers knowing nothing about whistled speech categorized four Spanish whistled vowels. The results show that naïve participants were able to categorize these vowels, although not as accurately as a native whistler",
    "checked": true,
    "id": "a51a65d3bf2d885b9ddaa46b505c6a51f8a03351",
    "semantic_title": "categorization of natural spanish whistled vowels by naïve spanish listeners",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/voigt16_interspeech.html": {
    "title": "Between- and Within-Speaker Effects of Bilingualism on F0 Variation",
    "volume": "main",
    "abstract": "To what extent is prosody shaped by cultural and social factors? Existing research has shown that an individual bilingual speaker exhibits differences in framing, ideology, and personality when speaking their two languages. To understand whether these differences extend to prosody we study F0 variation in a corpus of interviews with German-Italian and German-French bilingual speakers. We find two primary effects. First, a between-speaker effect: these two groups of bilinguals make different use of F0 even when they are all speaking German. Second, a within-speaker effect: bilinguals use F0 differently depending on which language they are speaking, differences that are consistent across speakers. These effects are modulated strongly by gender, suggesting that language-specific social positioning may play a central role. These results have important implications for our understanding of bilingualism and cross-cultural linguistic difference in general. Prosody appears to be a moving target rather than a stable feature, as speakers use prosodic variation to position themselves on cultural and social axes like linguistic context and gender",
    "checked": true,
    "id": "eae06fc54be27650f162a24b7c6953baa8ea655f",
    "semantic_title": "between- and within-speaker effects of bilingualism on f0 variation",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/graham16_interspeech.html": {
    "title": "Vowel Characteristics in the Assessment of L2 English Pronunciation",
    "volume": "main",
    "abstract": "There is considerable need to utilise linguistically meaningful measures of second language (L2) proficiency that are based on perceptual cues used by humans to assess pronunciation. Previous research on non-native acquisition of vowel systems suggests a strong link between vowel production accuracy and speech intelligibility. It is well known that the acoustic and perceptual identification of vowels rely on formant frequencies. However, formant analysis may not be viable in large-scale corpus research, given the need for manual correction of tracking errors. Spectral analysis techniques have been shown to be a robust alternative to formant tracking. This paper explores the use of one such technique — the discrete cosine transform (DCT) — for modelling English vowel spectra in the productions of non-native English speakers. Mel-scaled DCT coefficients were calculated over a frequency band of 200–4000 Hz. Results show a statistically significant correlation between coefficients and the proficiency level of speakers, and suggest that this technique holds some promise in automated L2 pronunciation teaching and assessment",
    "checked": true,
    "id": "2319071df0690c6431093dce933aeb13527f78ae",
    "semantic_title": "vowel characteristics in the assessment of l2 english pronunciation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/geneid16_interspeech.html": {
    "title": "Kulning (Swedish Cattle Calls): Acoustic, EGG, Stroboscopic and High-Speed Video Analyses of an Unusual Singing Style",
    "volume": "main",
    "abstract": "The Swedish cattle call singing style ‘kulning' is surprisingly understudied, despite its mythical status in folklore. While some acoustic and physiological aspects have been addressed previously [1,2], a more detailed analysis is still lacking. Previous work [2] showed that sound pressure level (SPL) in kulning tapered off less than in head register as a function of distance, which warrants a study of underlying physiological mechanisms responsible for this. In the present paper, the same singer, singing the same song — in kulning and in head register (\"falsetto\") mode — was recorded indoors. Electroglottographic (EGG), stroboscopic, high-speed endoscopic and audio registrations were made. Analyses examined differences between kulning and head register. Results show somewhat higher SPL in kulning than in head register confirming the previous findings. EGG showed longer relative glottal closed time and higher amplitude of the signal in kulning. This suggests better vocal fold contact in kulning. Flexible nasofiberoscopy and high-speed recordings during kulning showed medial and antero-posterior narrowing of the laryngeal inlet, a clear approximation of the false vocal folds and marked adduction of the vocal folds",
    "checked": true,
    "id": "76bc11be734cd3ef05c0c8ea83592206488b8043",
    "semantic_title": "kulning (swedish cattle calls): acoustic, egg, stroboscopic and high-speed video analyses of an unusual singing style",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hejna16_interspeech.html": {
    "title": "Glottal Squeaks in VC Sequences",
    "volume": "main",
    "abstract": "This paper reports results related to the phenomenon referred to as a \"glottal squeak\" (coined by [1]). At present, nothing is known about the conditioning and the articulation of this feature of speech. Our qualitative acoustic analyses of the conditioning of squeaks (their frequency of occurrence, duration, and f ) found in Aberystwyth English and Manchester English suggest that squeaking may be a result of intrinsically tense vocal fold state associated with thyroarytenoid (TA) muscle recruitment [2] required for epilaryngeal constriction and vocal-ventricular fold contact (VVFC) needed to produce glottalisation [3]. In this interpretation, we hypothesise that squeaks occasionally occur during constriction disengagement: at the point when VVFC suddenly releases but the TAs have not yet fully relaxed. Extralinguistic conditioning identified in this study corroborates findings reported by [1]: females are more prone to squeaking and the phenomenon is individual-dependent",
    "checked": true,
    "id": "256967d8d52ef3ba2fad7f12cf3251aadee63ffa",
    "semantic_title": "glottal squeaks in vc sequences",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/takahashi16_interspeech.html": {
    "title": "Automatic Pronunciation Generation by Utilizing a Semi-Supervised Deep Neural Networks",
    "volume": "main",
    "abstract": "Phonemic or phonetic sub-word units are the most commonly used atomic elements to represent speech signals in modern ASRs. However they are not the optimal choice due to several reasons such as: large amount of effort required to handcraft a pronunciation dictionary, pronunciation variations, human mistakes and under-resourced dialects and languages. Here, we propose a data-driven pronunciation estimation and acoustic modeling method which only takes the orthographic transcription to jointly estimate a set of sub-word units and a reliable dictionary. Experimental results show that the proposed method which is based on semi-supervised training of a deep neural network largely outperforms phoneme based continuous speech recognition on the TIMIT dataset",
    "checked": true,
    "id": "23cb77b6c458a735e2bc5877fa53a9b700169a45",
    "semantic_title": "automatic pronunciation generation by utilizing a semi-supervised deep neural networks",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16f_interspeech.html": {
    "title": "Personalized Natural Language Understanding",
    "volume": "main",
    "abstract": "Natural language understanding (NLU) is one of the critical components of dialog systems. Its aim is to extract semantic meaning from typed text input or the spoken text coming out of the speech recognizer. Traditionally, NLU systems are built in a user-independent fashion, where the system behavior does not adapt to the user. However, personal information can be very useful for language understanding tasks, if it is made available to the system. With personal digital assistant (PDA) systems, many forms of personal data are readily available for the NLU systems to make the models and the system more personal. In this paper, we propose a method to personalize language understanding models by making use of the personal data with privacy respected and protected. We report experiments on two domains for intent classification and slot tagging, where we achieve significant accuracy improvements compared to the baseline models that are trained in a user independent manner",
    "checked": true,
    "id": "5fc215187e313899f9f41085b4db0fd53cd32282",
    "semantic_title": "personalized natural language understanding",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/asri16_interspeech.html": {
    "title": "A Sequence-to-Sequence Model for User Simulation in Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "User simulation is essential for generating enough data to train a statistical spoken dialogue system. Previous models for user simulation suffer from several drawbacks, such as the inability to take dialogue history into account, the need of rigid structure to ensure coherent user behaviour, heavy dependence on a specific domain, the inability to output several user intentions during one dialogue turn, or the requirement of a summarized action space for tractability. This paper introduces a data-driven user simulator based on an encoder-decoder recurrent neural network. The model takes as input a sequence of dialogue contexts and outputs a sequence of dialogue acts corresponding to user intentions. The dialogue contexts include information about the machine acts and the status of the user goal. We show on the Dialogue State Tracking Challenge 2 (DSTC2) dataset that the sequence-to-sequence model outperforms an agenda-based simulator and an n-gram simulator, according to F-score. Furthermore, we show how this model can be used on the original action space and thereby models user behaviour with finer granularity",
    "checked": true,
    "id": "ffebeeffe7264b5be41a8143070b839970c7b97a",
    "semantic_title": "a sequence-to-sequence model for user simulation in spoken dialogue systems",
    "citation_count": 98
  },
  "https://www.isca-speech.org/archive/interspeech_2016/georgiladakis16_interspeech.html": {
    "title": "Root Cause Analysis of Miscommunication Hotspots in Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "A major challenge in Spoken Dialogue Systems (SDS) is the detection of problematic communication (hotspots), as well as the classification of these hotspots into different types (root cause analysis). In this work, we focus on two classes of root cause, namely, erroneous speech recognition vs. other (e.g., dialogue strategy). Specifically, we propose an automatic algorithm for detecting hotspots and classifying root causes in two subsequent steps. Regarding hotspot detection, various lexico-semantic features are used for capturing repetition patterns along with affective features. Lexico-semantic and repetition features are also employed for root cause analysis. Both algorithms are evaluated with respect to the Let's Go dataset (bus information system). In terms of classification unweighted average recall, performance of 80% and 70% is achieved for hotspot detection and root cause analysis, respectively",
    "checked": true,
    "id": "f0ec07a8f1aaf0894c11aa9242c7de898a5beee3",
    "semantic_title": "root cause analysis of miscommunication hotspots in spoken dialogue systems",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/khan16_interspeech.html": {
    "title": "Making Personal Digital Assistants Aware of What They Do Not Know",
    "volume": "main",
    "abstract": "Personal digital assistants (PDAs) are spoken (and typed) dialog systems that are expected to assist users without being constrained to a particular domain. Typically, it is possible to construct deep multi-domain dialog systems focused on a narrow set of head domains. For the long tail (or when the speech recognition is not correct) the PDA does not know what to do. Two common fallback approaches are to either acknowledge its limitation or show web search results. Either approach can severely undermine the user's trust in the PDA's intelligence if invoked at the wrong time. In this paper, we propose features that are helpful in predicting the right fallback response. We then use these features to construct dialog policies such that the PDA is able to correctly decide between invoking web search or acknowledging its limitation. We evaluate these dialog policies on real user logs gathered from a PDA, deployed to millions of users, using both offline (judged) and online (user-click) metrics. We demonstrate that our hybrid dialog policy significantly increases the accuracy of choosing the correct response, measured by analyzing click-rate in logs, and also enhances user satisfaction, measured by human evaluations of the replayed experience",
    "checked": true,
    "id": "fc6c984360e88be9f4b2cde96c9f6b1533b57d0b",
    "semantic_title": "making personal digital assistants aware of what they do not know",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/levitan16_interspeech.html": {
    "title": "Implementing Acoustic-Prosodic Entrainment in a Conversational Avatar",
    "volume": "main",
    "abstract": "Entrainment, aka accommodation or alignment, is the phenomenon by which conversational partners become more similar to each other in behavior. While there has been much work on some behaviors there has been little on entrainment in speech and even less on how Spoken Dialogue Systems (SDS) which entrain to their users' speech can be created. We present an architecture and algorithm for implementing acoustic-prosodic entrainment in SDS and show that speech produced under this algorithm conforms to the feature targets, satisfying the properties of entrainment behavior observed in human-human conversations. We present results of an extrinsic evaluation of this method, comparing whether subjects are more likely to ask advice from a conversational avatar that entrains vs. one that does not, in English, Spanish and Slovak SDS",
    "checked": true,
    "id": "bf1868ecfd8326e2b10524ba94c420cce1d8d9e7",
    "semantic_title": "implementing acoustic-prosodic entrainment in a conversational avatar",
    "citation_count": 49
  },
  "https://www.isca-speech.org/archive/interspeech_2016/silvervarg16_interspeech.html": {
    "title": "Perceived Usability and Cognitive Demand of Secondary Tasks in Spoken Versus Visual-Manual Automotive Interaction",
    "volume": "main",
    "abstract": "We present results from a study of truck drivers' experience of using two different interfaces; spoken interaction and visual-manual interaction, to perform secondary tasks while driving. The instruments used to measure their experience are based on three popular questionnaires, measuring different aspects of usability and cognitive load: SASSI, SUS and DALI. Our results show that the speech interface is preferred both regarding usability and cognitive demand",
    "checked": true,
    "id": "1ea856b2e33d091577fb5cc557d7dfa1886f7583",
    "semantic_title": "perceived usability and cognitive demand of secondary tasks in spoken versus visual-manual automotive interaction",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fung16_interspeech.html": {
    "title": "Zara: An Empathetic Interactive Virtual Agent",
    "volume": "main",
    "abstract": "Zara, or ‘Zara the Supergirl', is a virtual robot that can show empathy while interacting with an user, and at the end of a 5–10 minute conversation, it can give a personality analysis based on the user responses. It can display and share emotions with the aid of its built in sentiment analysis, facial and emotion recognition, and speech module. Being the first of its kind, it has successfully integrated an empathetic system along with the human emotion recognition and sharing, into an augmented human-robot interaction system. Zara was also displayed at the World Economic Forum held at Dalian in September 2015",
    "checked": true,
    "id": "053eab53e12c4c65f97d3fc41bdb8f71bc6334a1",
    "semantic_title": "zara: an empathetic interactive virtual agent",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tejedorgarcia16_interspeech.html": {
    "title": "Measuring Pronunciation Improvement in Users of CAPT Tool TipTopTalk!",
    "volume": "main",
    "abstract": "We present a L2 pronunciation training serious game based on the minimal-pairs technique, incorporating sequences of exposure, discrimination and production, and using text-to-speech and speech recognition systems. We have measured the quality of users' production during a period of time in order to assess improvement after using the application. Substantial improvement is found among users with poorer initial performance levels. The program's gamification resources manage to engage a high percentage of users. A need is felt to include feedback for users in future versions with the purpose of increasing their performance and avoiding the performance drop detected after protracted use of the tool",
    "checked": true,
    "id": "141873643c2bf9f6003f9cf664a349988d18fd56",
    "semantic_title": "measuring pronunciation improvement in users of capt tool tiptoptalk!",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kawahara16_interspeech.html": {
    "title": "SparkNG: Interactive MATLAB Tools for Introduction to Speech Production, Perception and Processing Fundamentals and Application of the Aliasing-Free L-F Model Component",
    "volume": "main",
    "abstract": "This article introduces a set of interactive tools for studying fundamentals of speech production, perception and processing. In addition to this voice production simulator, it consists of interactive time-frequency representation, auditory representation visualizer and a vocal tract shape visualizer for introductory materials. It consists of compiled executables for Windows and Mac environment, which do not require MATLAB license. The MATLAB sources of the tools and their constituent functions are publicly accessible under open source license",
    "checked": true,
    "id": "db7bb38a77405c34c03f75259e202afa589a9d47",
    "semantic_title": "sparkng: interactive matlab tools for introduction to speech production, perception and processing fundamentals and application of the aliasing-free l-f model component",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/marchi16_interspeech.html": {
    "title": "Real-Time Tracking of Speakers' Emotions, States, and Traits on Mobile Platforms",
    "volume": "main",
    "abstract": "We demonstrate audEERING's sensAI technology running natively on low-resource mobile devices applied to emotion analytics and speaker characterisation tasks. A show-case application for the Android platform is provided, where audEERING's highly noise robust voice activity detection based on LSTM-RNN is combined with our core emotion recognition and speaker characterisation engine natively on the mobile device. This eliminates the need for network connectivity and allows to perform robust speaker state and trait recognition efficiently in real-time without network transmission lags. Real-time factors are benchmarked for a popular mobile device to demonstrate the efficiency, and average response times are compared to a server based approach. The output of the emotion analysis is visualized graphically in the arousal and valence space alongside the emotion category and further speaker characteristics",
    "checked": true,
    "id": "89768675417787ed02b86308798fed5fe0c72ff7",
    "semantic_title": "real-time tracking of speakers' emotions, states, and traits on mobile platforms",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mirghafori16_interspeech.html": {
    "title": "Mindfulness Special Event",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "95c44be090a39996262dc5cbd48a4b483fed1179",
    "semantic_title": "mindfulness special event",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chang16b_interspeech.html": {
    "title": "The Human Speech Cortex",
    "volume": "main",
    "abstract": "A unique and defining trait of human behavior is our ability to communicate through speech. The fundamental organizational principles of the neural circuits within speech brain areas are largely unknown. In this talk, I will present new results from our research on the functional organization of the human higher-order auditory cortex, known as Wernicke's area. I will focus on how neural populations in the superior temporal lobe encode acoustic-phonetic representations of speech, and also how they integrate influences of linguistic context to achieve perceptual robustness",
    "checked": true,
    "id": "4c3bcf22570e21bda52c2124b016a542a354ca2c",
    "semantic_title": "the human speech cortex",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bonastre16_interspeech.html": {
    "title": "Speaker Comparison for Forensic and Investigative Applications II",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "48cb77830b9cac19f80202e4befc9c2c79ed5f3d",
    "semantic_title": "speaker comparison for forensic and investigative applications ii",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bone16_interspeech.html": {
    "title": "Acoustic-Prosodic and Turn-Taking Features in Interactions with Children with Neurodevelopmental Disorders",
    "volume": "main",
    "abstract": "Atypical speech prosody is a hallmark feature of autism spectrum disorder (ASD) that presents across the lifespan, but is difficult to reliably characterize qualitatively. Given the great heterogeneity of symptoms in ASD, an acoustic-based objective measure would be vital for clinical assessment and interventions. In this study, we investigate speech features in child-psychologist conversational samples, including: segmental and suprasegmental pitch dynamics, speech rate, coordination of prosodic attributes, and turn-taking. Data consist of 95 children with ASD as well as 81 controls with non-ASD developmental disorders. We demonstrate significant predictive performance using these features as well as interpret feature correlations of both interlocutors. The most robust finding is that segmental and suprasegmental prosodic variability increases for both participants in interactions with children having higher ASD severity. Recommendations for future research towards a fully-automatic quantitative measure of speech prosody in neurodevelopmental disorders are discussed",
    "checked": true,
    "id": "1aac65a96de5a3caffa80272432b5b6c9d898982",
    "semantic_title": "acoustic-prosodic and turn-taking features in interactions with children with neurodevelopmental disorders",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hemmerling16_interspeech.html": {
    "title": "Automatic Detection of Parkinson's Disease Based on Modulated Vowels",
    "volume": "main",
    "abstract": "In this paper we present a novel approach of automatic detection of phonatory and articulatory impairments caused by Parkinson's disease (PD). Modulated (varying between low and high pitch) and sustained vowels are considered and analysed. The fundamental frequency of the phonations and its range are computed using the Hilbert-Huang transformation. Additionally, a set with \"standard\" measures are calculated to model phonatory and articulatory deficits exhibited by Parkinson's patients. Kernel Principal Component Analysis was also applied in order to reduce the dimensionality of the representation space. The automatic discrimination between speakers with PD and healthy controls (HC) is performed using decision trees. According to the results, modulated vowels are suitable to evaluate phonatory and articulatory deficits observed in PD speech",
    "checked": true,
    "id": "b0a00fabc292f2bbfa95257e75a620338f94c40c",
    "semantic_title": "automatic detection of parkinson's disease based on modulated vowels",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16d_interspeech.html": {
    "title": "Towards Automatic Detection of Amyotrophic Lateral Sclerosis from Speech Acoustic and Articulatory Samples",
    "volume": "main",
    "abstract": "Amyotrophic lateral sclerosis (ALS) is a rapid neurodegenerative disease that affects the speech motor functions of patients, thus causes dysarthria. There is no definite marker for the diagnosis of ALS. Currently, the diagnosis of ALS is primarily based on clinical observations of upper and lower motor neuron damage in the absence of other causes, which is time-consuming, of high cost, and often delayed. Timely diagnosis and assessment for ALS are crucial. Automatic detection of ALS from speech samples would advance the diagnosis of ALS. In this paper, we investigated the automatic detection of ALS from short, pre-symptom speech acoustic and articulatory samples using machine learning approaches (support vector machine and deep neural network). A data set of more than 2,500 speech samples collected from eleven patients with ALS and eleven healthy speakers was used. Leave-subjects-out cross validation experimental results indicate the feasibility of the automatic detection of ALS from speech samples. Adding articulatory motion information (from tongue and lips) further improved the detection performance",
    "checked": true,
    "id": "b84f17589cb509deb15b8565551692268ef28a78",
    "semantic_title": "towards automatic detection of amyotrophic lateral sclerosis from speech acoustic and articulatory samples",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ciccarelli16_interspeech.html": {
    "title": "Neurophysiological Vocal Source Modeling for Biomarkers of Disease",
    "volume": "main",
    "abstract": "Speech is potentially a rich source of biomarkers for detecting and monitoring neuropsychological disorders. Current biomarkers typically comprise acoustic descriptors extracted from behavioral measures of source, filter, prosodic and linguistic cues. In contrast, in this paper, we extract vocal features based on a neurocomputational model of speech production, reflecting latent or internal motor control parameters that may be more sensitive to individual variation under neuropsychological disease. These features, which are constrained by neurophysiology, may be resilient to artifacts and provide an articulatory complement to acoustic features. Our features represent a mapping from a low-dimensional acoustics-based feature space to a high-dimensional space that captures the underlying neural process including articulatory commands and auditory and somatosensory feedback errors. In particular, we demonstrate a neurophysiological vocal source model that generates biomarkers of disease by modeling vocal source control. By using the fundamental frequency contour and a biophysical representation of the vocal source, we infer two neuromuscular time series whose coordination provides vocal features that are applied to depression and Parkinson's disease as examples. These vocal source coordination features alone, on a single held vowel, outperform or are comparable to other features sets and reflect a significant compression of the feature space",
    "checked": true,
    "id": "cceaf33987d89fd3bb98fc2fffd32a0a038ca050",
    "semantic_title": "neurophysiological vocal source modeling for biomarkers of disease",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/horwitzmartin16_interspeech.html": {
    "title": "Relation of Automatically Extracted Formant Trajectories with Intelligibility Loss and Speaking Rate Decline in Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "Effective monitoring of bulbar disease progression in persons with amyotrophic lateral sclerosis (ALS) requires rapid, objective, automatic assessment of speech loss. The purpose of this work was to identify acoustic features that aid in predicting intelligibility loss and speaking rate decline in individuals with ALS. Features were derived from statistics of the first (F ) and second (F ) formant frequency trajectories and their first and second derivatives. Motivated by a possible link between components of formant dynamics and specific articulator movements, these features were also computed for low-pass and high-pass filtered formant trajectories. When compared to clinician-rated intelligibility and speaking rate assessments, F features, particularly mean F speed and a novel feature, mean F acceleration, were most strongly correlated with intelligibility and speaking rate, respectively (Spearman correlations > 0.70, p < 0.0001). These features also yielded the best predictions in regression experiments (r > 0.60, p < 0.0001). Comparable results were achieved using low-pass filtered F trajectory features, with higher correlations and lower prediction errors achieved for speaking rate over intelligibility. These findings suggest information can be exploited in specific frequency components of formant trajectories, with implications for automatic monitoring of ALS",
    "checked": true,
    "id": "5b7adfc7dc334ffb4e251f54e3cd046828b176c1",
    "semantic_title": "relation of automatically extracted formant trajectories with intelligibility loss and speaking rate decline in amyotrophic lateral sclerosis",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ringeval16_interspeech.html": {
    "title": "Automatic Analysis of Typical and Atypical Encoding of Spontaneous Emotion in the Voice of Children",
    "volume": "main",
    "abstract": "Children with Autism Spectrum Disorders (ASD) present significant difficulties to understand and express emotions. Systems have thus been proposed to provide objective measurements of acoustic features used by children suffering from ASD to encode emotion in speech. However, only a few studies have exploited such systems to compare different groups of children in their ability to express emotions, and even less have focused on the analysis of spontaneous emotion. In this contribution, we provide insights by extensive evaluations carried out on a new database of spontaneous speech inducing three emotion categories of valence (positive, neutral, and negative). We evaluate the potential of using an automatic recognition system to differentiate groups of children, i.e., pervasive developmental disorders, pervasive developmental disorders not-otherwise specified, specific language impairments, and typically developing, in their abilities to express spontaneous emotion in a common unconstrained task. Results show that all groups of children can be differentiated directly (diagnosis recognition) and indirectly (emotion recognition) by the proposed system",
    "checked": true,
    "id": "deeffc3a31d1fc980d483dc6e5b635a7a58ea5ce",
    "semantic_title": "automatic analysis of typical and atypical encoding of spontaneous emotion in the voice of children",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2016/khorram16_interspeech.html": {
    "title": "Recognition of Depression in Bipolar Disorder: Leveraging Cohort and Person-Specific Knowledge",
    "volume": "main",
    "abstract": "Individuals with bipolar disorder typically exhibit changes in the acoustics of their speech. Mobile health systems seek to model these changes to automatically detect and correctly identify current states in an individual and to ultimately predict impending mood episodes. We have developed a program, PRIORI (Predicting Individual Outcomes for Rapid Intervention), that analyzes acoustics of speech as predictors of mood states from mobile smartphone data. Mood prediction systems generally assume that the symptomatology of an individual can be modeled using patterns common in a cohort population due to limitations in the size of available datasets. However, individuals are unique. This paper explores person-level systems that can be developed from the current PRIORI database of an extensive and longitudinal collection composed of two subsets: a smaller labeled portion and a larger unlabeled portion. The person-level system employs the unlabeled portion to extract i-vectors, which characterize single individuals. The labeled portion is then used to train person-level and population-level supervised classifiers, operating on the i-vectors and on speech rhythm statistics, respectively. The unification of these two approaches results in a significant improvement over the baseline system, demonstrating the importance of a multi-level approach to capturing depression symptomatology",
    "checked": true,
    "id": "08f83a8fccccfb8fb19acc3eab331ded2fa3ae9b",
    "semantic_title": "recognition of depression in bipolar disorder: leveraging cohort and person-specific knowledge",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mirheidari16_interspeech.html": {
    "title": "Diagnosing People with Dementia Using Automatic Conversation Analysis",
    "volume": "main",
    "abstract": "A recent study using Conversation Analysis (CA) has demonstrated that communication problems may be picked up during conversations between patients and neurologists, and that this can be used to differentiate between patients with (progressive neurodegenerative dementia) ND and those with (nonprogressive) functional memory disorders (FMD). This paper presents a novel automatic method for transcribing such conversations and extracting CA-style features. A range of acoustic, syntactic, semantic and visual features were automatically extracted and used to train a set of classifiers. In a proof-of-principle style study, using data recording during real neurologist-patient consultations, we demonstrate that automatically extracting CA-style features gives a classification accuracy of 95%when using verbatim transcripts. Replacing those transcripts with automatic speech recognition transcripts, we obtain a classification accuracy of 79% which improves to 90% when feature selection is applied. This is a first and encouraging step towards replacing inaccurate, potentially stressful cognitive tests with a test based on monitoring conversation capabilities that could be conducted in e.g. the privacy of the patient's own home",
    "checked": true,
    "id": "c0d6386f1b2d3bc37412320aff12ed29b423214f",
    "semantic_title": "diagnosing people with dementia using automatic conversation analysis",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chan16_interspeech.html": {
    "title": "SERAPHIM: A Wavetable Synthesis System with 3D Lip Animation for Real-Time Speech and Singing Applications on Mobile Platforms",
    "volume": "main",
    "abstract": "Singing synthesis is a rising musical art form gaining popularity amongst composers and end-listeners alike. To date, this art form is largely confined to offline boundaries of the music studio, whereas a large part music is about live performances. This calls for a real-time synthesis system readily deployable for onstage applications SERAPHIM is a wavetable synthesis system that is lightweight and deployable on mobile platforms. Apart from conventional offline studio applications, SERAPHIM also supports real-time synthesis applications, enabling live control inputs for on-stage performances. It also provides for easy lip animation control. SERAPHIM will be made available as a toolbox on Unity 3D for easy adoption into game development across multiple platforms. A readily compiled version will also be deployed as a VST studio plugin, directly addressing end users. It currently supports Japanese (singing only) and Mandarin (speech and singing) languages. This paper describes our work on SERAPHIM and discusses its capabilities and applications",
    "checked": true,
    "id": "955071759563b142d2ba401eee872a77b5e8b2b4",
    "semantic_title": "seraphim: a wavetable synthesis system with 3d lip animation for real-time speech and singing applications on mobile platforms",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bonada16_interspeech.html": {
    "title": "Expressive Singing Synthesis Based on Unit Selection for the Singing Synthesis Challenge 2016",
    "volume": "main",
    "abstract": "Sample and statistically based singing synthesizers typically require a large amount of data for automatically generating expressive synthetic performances. In this paper we present a singing synthesizer that using two rather small databases is able to generate expressive synthesis from an input consisting of notes and lyrics. The system is based on unit selection and uses the Wide-Band Harmonic Sinusoidal Model for transforming samples. The first database focuses on expression and consists of less than 2 minutes of free expressive singing using solely vowels. The second one is the timbre database which for the English case consists of roughly 35 minutes of monotonic singing of a set of sentences, one syllable per beat. The synthesis is divided in two steps. First, an expressive vowel singing performance of the target song is generated using the expression database. Next, this performance is used as input control of the synthesis using the timbre database and the target lyrics. A selection of synthetic performances have been submitted to the Interspeech Singing Synthesis Challenge 2016, in which they are compared to other competing systems",
    "checked": true,
    "id": "53a8f569f42098337f28c03d1441c15b490a5899",
    "semantic_title": "expressive singing synthesis based on unit selection for the singing synthesis challenge 2016",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2016/perrotin16_interspeech.html": {
    "title": "Vocal Effort Modification for Singing Synthesis",
    "volume": "main",
    "abstract": "Vocal effort modification of natural speech is an asset to various applications, in particular, for adding flexibility to concatenative voice synthesis systems. Although decreasing vocal effort is not particularly difficult, increasing vocal effort is a challenging issue. It requires the generation of artificial harmonics in the voice spectrum, along with transformation of the spectral envelope. After a raw source-filter decomposition, harmonic enrichment is achieved by 1/ increasing the source signal impulsiveness using time distortion, 2/ mixing the distorted and natural signals' spectra. Two types of spectral envelope transformations are used: spectral morphing and spectral modeling. Spectral morphing is the transplantation of natural spectral envelopes. Spectral modeling focuses on spectral tilt, formant amplitudes and first formant position modifications. The effectiveness of source enrichment, spectrum morphing, and spectrum modeling for vocal effort modification of sung vowels was evaluated with the help of a perceptive experiment. Results showed a significant positive influence of harmonic enrichment on vocal effort perception with both spectral envelope transformations. Spectral envelope morphing and harmonic enrichment applied on soft voices were perceptively close to natural loud voices. Automatic spectral envelope modeling did not match the results of spectral envelope morphing, but it significantly increased the perception of vocal effort",
    "checked": true,
    "id": "9a6d47d1bb915e4a3a03bc7185f611b5d77548f8",
    "semantic_title": "vocal effort modification for singing synthesis",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/blanco16_interspeech.html": {
    "title": "Bertsokantari: a TTS Based Singing Synthesis System",
    "volume": "main",
    "abstract": "This paper describes the implementation of the Aholab entry for the Singing Synthesis Challenge: Fill-in the Gap. Our approach in this work makes use of an HTS based Text-to-Speech (TTS) synthesizer for Basque to generate the singing voice. The prosody related parameters provided by the TTS system for a spoken version of the score are modified to adapt them to the requirements of the music score concerning syllables duration and tone, while the spectral parameters are basically maintained. The paper describes the processing details developed to improve the quality of the output signal: the syllable timing, the generation of the intonation with vibrato and the manipulation of the model states. In this entry, the lyrics have been freely translated into Basque and the rhythm has been adapted to a Basque traditional rhythm",
    "checked": true,
    "id": "961c9b857fe32b5b84f003cc83b8f0c8ee4a8ac3",
    "semantic_title": "bertsokantari: a tts based singing synthesis system",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/feugere16_interspeech.html": {
    "title": "Evaluation of Singing Synthesis: Methodology and Case Study with Concatenative and Performative Systems",
    "volume": "main",
    "abstract": "The special session Singing Synthesis Challenge: Fill-In the Gap aims at comparative evaluation of singing synthesis systems. The task is to synthesize a new couplet for two popular songs. This paper address the methodology needed for quality assessment of singing synthesis systems and reports on a case study using 2 systems with a total of 6 different configurations. The two synthesis systems are: a concatenative Text-to-Chant (TTC) system, including a parametric representation of the melodic curve; a Singing Instrument (SI), allowing for real-time interpretation of utterances made of flat-pitch natural voice or diphone concatenated voice. Absolute Category Rating (ACR) and Paired Comparison (PC) tests are used. Natural and natural-degraded reference conditions are used for calibration of the ACR test. The MOS obtained using ACR shows that the TTC (resp. the SI) ranks below natural voice but above (resp. in between) degraded conditions. Then singing synthesis quality is judged better than auto-tuned or distorted natural voice in some cases. PC results show that: 1/ signal processing is an important quality issue, making the difference between systems; 2/ diphone concatenation degrades the quality compared to flat-pitch natural voice; 3/ Automatic melodic modelling is preferred to gestural control for off-line synthesis",
    "checked": true,
    "id": "3590870074f98df43d674cd69aa007120b7647ed",
    "semantic_title": "evaluation of singing synthesis: methodology and case study with concatenative and performative systems",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ardaillon16_interspeech.html": {
    "title": "Expressive Control of Singing Voice Synthesis Using Musical Contexts and a Parametric F0 Model",
    "volume": "main",
    "abstract": "Expressive singing voice synthesis requires an appropriate control of both prosodic and timbral aspects. While it is desirable to have an intuitive control over the expressive parameters, synthesis systems should be able to produce convincing results directly from a score. As countless interpretations of a same score are possible, the system should also target a particular singing style, which implies to mimic the various strategies used by different singers. Among the control parameters involved, the pitch ( F0) should be modeled in priority. In previous work, a parametric F0 model with intuitive controls has been proposed, but no automatic way to choose the model parameters was given. In the present work, we propose a new approach for modeling singing style, based on parametric templates selection. In this approach, the F0 parameters and phonemes durations are extracted from annotated recordings, along with a rich description of contextual informations, and stored to form a database of parametric templates. This database is then used to build a model of the singing style using decision-trees. At the synthesis stage, appropriate parameters are then selected according to the target contexts. The results produced by this approach have been evaluated by means of a listening test",
    "checked": true,
    "id": "ac073f7a9feb496c90d4691fbf4eff7ab69de5b8",
    "semantic_title": "expressive control of singing voice synthesis using musical contexts and a parametric f0 model",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cotescu16_interspeech.html": {
    "title": "Optimal Unit Stitching in a Unit Selection Singing Synthesis System",
    "volume": "main",
    "abstract": "Unit Selection based speech synthesis systems are currently the best performing, producing natural sounding speech with minimal CPU load. One of the important reasons behind their success is the amount of recordings that are now commonly used in synthesis applications. However, in the case of singing applications, it is quite hard for a database to cover a large phonetic space due to the relative inefficiency of the recording process. Thus, due to the reduced catalogue of units, singing unit selection systems are more likely to produce spectral discontinuity artefacts. Taking advantage of the quasi stable nature of articulation during singing, we propose a novel unit stitching method. The method was implemented into the system that was used for the \"Fill-In the Gap\" Singing Synthesis Challenge",
    "checked": true,
    "id": "8e9137d8acebe03d62637cc968ce79ca73b20c48",
    "semantic_title": "optimal unit stitching in a unit selection singing synthesis system",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hilton16_interspeech.html": {
    "title": "The Perception of Overlapping Speech: Effects of Speaker Prosody and Listener Attitudes",
    "volume": "main",
    "abstract": "Speakers use overlapping speech to achieve a range of interactional moves. Competitive overlaps, or interruptions, challenge an interlocutor's control of the conversational floor, while non-competitive overlaps, like back-channeling and co-constructed discourse, communicate engagement with the conversation and ratify the interlocutor's right to be speaking. Being able to evaluate the intentions behind moments of overlap is critical for interlocutors, as well as researchers seeking to model human-human interaction. Researchers have analyzed the acoustics of overlapping speech in order to understand what determines whether an overlap is heard as competitive or non-competitive. They have overwhelmingly found that prosodic prominence plays an important role; incoming overlaps with higher pitch and intensity are more competitive or interruptive. However, no research has directly tested whether and how listeners use prosodic cues to evaluate moments of overlap. Furthermore, much of the current research on classifying overlapping speech ignores listener variability. The present study uses a perception experiment with 500 participants to test the effects of speaker prosody and listener attitudes on the evaluation of overlapping speech. The results demonstrate that prosodic prominence does significantly affect evaluations of overlapping speech, but it is mediated by the listener's own interactional style and attitudes toward overlapping speech",
    "checked": true,
    "id": "747e10bd241695b8b3e1f08392895e1e42cba056",
    "semantic_title": "the perception of overlapping speech: effects of speaker prosody and listener attitudes",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gravano16_interspeech.html": {
    "title": "Who Do You Think Will Speak Next? Perception of Turn-Taking Cues in Slovak and Argentine Spanish",
    "volume": "main",
    "abstract": "We investigate perceptual cues in human-human dialogue management related to signalling the change of speaker and the interlocutor's wish to backchannel or contribute with propositional content. We are interested primarily in the relevance of prosodic cues in relation to textual ones, and their cross-linguistic validity by comparing unrelated languages Slovak and Argentine Spanish. Results of a perception study indicate that 1) in addition to textual cues, prosodic cues also play a clear role in perceiving how the dialogue will unfold; and 2) there exists a non-empty intersection of temporal and intonational prosodic turn-taking cues in the two languages, despite their belonging to separate families",
    "checked": true,
    "id": "050ba97e776d4695a6b08d0d294b705ce2fa1753",
    "semantic_title": "who do you think will speak next? perception of turn-taking cues in slovak and argentine spanish",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2016/perez16_interspeech.html": {
    "title": "Disentrainment may be a Positive Thing: A Novel Measure of Unsigned Acoustic-Prosodic Synchrony, and its Relation to Speaker Engagement",
    "volume": "main",
    "abstract": "Synchrony is a form of entrainment which consists in a relative coordination between two speakers, who throughout conversation simultaneously vary some properties of their speech. We describe two novel measures of acoustic-prosodic synchrony that are derived from a time-series analysis of the speech signal. Both of these measures reward positive synchrony (entrainment) and, while one penalizes negative synchrony (disentrainment), the other one rewards it. We describe significant correlations between the second measure and a number of positive social characteristics of the conversations, such as degree of speaker engagement, in a corpus of task-oriented dialogues in Standard American English. Since these correlations are not found to be significant for the first measure, our results suggest that disentrainment may sometimes have a positive effect on the development of conversation",
    "checked": true,
    "id": "dbb6775a1e7845330f047784bb3ff3e11dd599cb",
    "semantic_title": "disentrainment may be a positive thing: a novel measure of unsigned acoustic-prosodic synchrony, and its relation to speaker engagement",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wodarczak16b_interspeech.html": {
    "title": "Respiratory Turn-Taking Cues",
    "volume": "main",
    "abstract": "This paper investigates to what extent breathing can be used as a cue to turn-taking behaviour. The paper improves on existing accounts by considering all possible transitions between speaker states (silent, speaking, backchanneling) and by not relying on global speaker models. Instead, all features (including breathing range and resting expiratory level) are estimated in an incremental fashion using the left-hand context. We identify several inhalatory features relevant to turn-management, and assess the fit of models with these features as predictors of turn-taking behaviour",
    "checked": true,
    "id": "5a37ef6a8ccda2fb2d6855d8bf42ceee4a96a1f2",
    "semantic_title": "respiratory turn-taking cues",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2016/rennie16_interspeech.html": {
    "title": "The Discourse Marker \"so\" in Turn-Taking and Turn-Releasing Behavior",
    "volume": "main",
    "abstract": "Although so is a recognized discourse marker, little work has explored its uses in turn-taking, especially when it is not followed by additional speech. In this paper we explore the use of the discourse marker so as it pertains to turn-taking and turn-releasing. Specifically, we compare the duration and intensity of so when used to take a turn, mid-utterance, and when releasing a turn. We found that durations of turn-retaining tokens are generally shorter than turn-releases; we also found that turn-retaining tokens tend to be lower in intensity than the following speech. These trends of turn-taking behavior alongside certain lexical and prosodic features may prove useful for the development of speech-recognition software",
    "checked": true,
    "id": "6d34a72c3edeb88920cd63a2afbfff90146a12b8",
    "semantic_title": "the discourse marker \"so\" in turn-taking and turn-releasing behavior",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sherrziarko16_interspeech.html": {
    "title": "Acoustic Properties of Formality in Conversational Japanese",
    "volume": "main",
    "abstract": "This paper examines potential acoustic cues for level of formality in Japanese conversational speech using speech data gathered outside the laboratory, with the objective of using any significant cues to develop a model to predict level of formality in spoken Japanese. Based on previous work on the phonetic properties of formality in Japanese [1],[2] and other languages [3], and on a pilot study of informal geminate contractions in Japanese (section 2), the study examined the mean f , articulation rate, and f range (the difference between the minimum and maximum f in an utterance) via direct examination of the data and a functional data analysis [4],[5]. Analysis of the speech data shows significant relationships between all three variables and level of formality, and a binary logistic regression indicates that the variables have some potential as predictors of formality independent of lexical cues, although further refinement of any model will be necessary",
    "checked": true,
    "id": "3c4bd208f4c2a5ea705ab93fbba91f4559350829",
    "semantic_title": "acoustic properties of formality in conversational japanese",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pellegrini16_interspeech.html": {
    "title": "Inferring Phonemic Classes from CNN Activation Maps Using Clustering Techniques",
    "volume": "main",
    "abstract": "Today's state-of-art in speech recognition involves deep neural networks (DNN). These last years, a certain research effort has been invested in characterizing the feature representations learned by DNNs. In this paper, we focus on convolutional neural networks (CNN) trained for phoneme recognition in French. We report clustering experiments performed on activation maps extracted from the different layers of a CNN comprised of two convolution and sub-sampling layers followed by three dense layers. Our goal was to get insights into phone separability and phonemic categories inferred by the network, and how they vary according to the successive layers. Two directions were explored with both linear and non-linear clustering techniques. First, we imposed a number of 33 classes equal to the number of context-independent phone models for French, in order to assess the phoneme separability power of the different layers. As expected, we observed that this power increases with the layer depth in the network: from 34% to 74% in F-measure from the first convolution to the last dense layers, when using spectral clustering. Second, optimal numbers of classes were automatically inferred through inter- and intra-cluster measure criteria. We analyze these classes in terms of standard French phonological features",
    "checked": true,
    "id": "189f1fb0b482b161c802d53ab9732bf648e1daf3",
    "semantic_title": "inferring phonemic classes from cnn activation maps using clustering techniques",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zeghidour16_interspeech.html": {
    "title": "Joint Learning of Speaker and Phonetic Similarities with Siamese Networks",
    "volume": "main",
    "abstract": "Recent work has demonstrated, on small datasets, the feasibility of jointly learning specialized speaker and phone embeddings, in a weakly supervised siamese DNN architecture using word and speaker identity as side information. Here, we scale up these architectures to the 360 hours of the Librispeech corpus by implementing a sampling method to efficiently select pairs of words from the dataset and improving the loss function. We also compare the standard siamese networks fed with same (AA) or different (AB) pairs, to a ‘triamese' network fed with AAB triplets. We use ABX discrimination tasks to evaluate the discriminability and invariance properties of the obtained joined embeddings, and compare these results with mono-embeddings architectures. We find that the joined embeddings architectures succeed in effectively disentangling speaker from phoneme information, with around 10% errors for the matching tasks and embeddings (speaker task on speaker embeddings, and phone task on phone embedding) and near chance for the mismatched task. Furthermore, the results carry over in out-of-domain datasets, even beating the best results obtained with similar weakly supervised techniques",
    "checked": true,
    "id": "1133564c4240d9a3af679a2020bf79ab7245c15c",
    "semantic_title": "joint learning of speaker and phonetic similarities with siamese networks",
    "citation_count": 44
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mitra16_interspeech.html": {
    "title": "Unsupervised Learning of Acoustic Units Using Autoencoders and Kohonen Nets",
    "volume": "main",
    "abstract": "Often, prior knowledge of subword units is unavailable for low-resource languages. Instead, a global subword unit description, such as a universal phone set, is typically used in such scenarios. One major bottleneck for existing speech-processing systems is their reliance on transcriptions. Unfortunately, the preponderance of data becoming available everyday is only worsening the problem, as properly transcribing, and hence making this data useful for training speech-processing models, is impossible. This work investigates learning acoustic units in an unsupervised manner from real-world speech data by using a cascade of an autoencoder and a Kohonen net. For this purpose, a deep autoencoder with a bottleneck layer at the center was trained with multiple languages. Once trained, the bottleneck-layer output was used to train a Kohonen net, such that state-level ids can be assigned to the bottleneck outputs. To ascertain how consistent such state-level ids are with respect to the acoustic units, phone-alignment information was used for a part of the data to qualify if indeed a functional relationship existed between the phone ids and the Kohonen state ids and, if yes, whether such relationship can be generalized to data that are not transcribed",
    "checked": true,
    "id": "2a37ad3f13c5914ac2404506e1dbbf26338e6da9",
    "semantic_title": "unsupervised learning of acoustic units using autoencoders and kohonen nets",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhu16b_interspeech.html": {
    "title": "Learning Multiscale Features Directly from Waveforms",
    "volume": "main",
    "abstract": "Deep learning has dramatically improved the performance of speech recognition systems through learning hierarchies of features optimized for the task at hand. However, true end-to-end learning, where features are learned directly from waveforms, has only recently reached the performance of hand-tailored representations based on the Fourier transform. In this paper, we detail an approach to use convolutional filters to push past the inherent tradeoff of temporal and frequency resolution that exists for spectral representations. At increased computational cost, we show that increasing temporal resolution via reduced stride and increasing frequency resolution via additional filters delivers significant performance improvements. Further, we find more efficient representations by simultaneously learning at multiple scales, leading to an overall decrease in word error rate on a difficult internal speech test set by 20.7% relative to networks with the same number of parameters trained on spectrograms",
    "checked": true,
    "id": "479665ab9f081cbabbc8dc698f874156e3a5b22f",
    "semantic_title": "learning multiscale features directly from waveforms",
    "citation_count": 57
  },
  "https://www.isca-speech.org/archive/interspeech_2016/heck16_interspeech.html": {
    "title": "Supervised Learning of Acoustic Models in a Zero Resource Setting to Improve DPGMM Clustering",
    "volume": "main",
    "abstract": "In this work we utilize a supervised acoustic model training pipeline without supervision to improve Dirichlet process Gaussian mixture model (DPGMM) based feature vector clustering. We exploit methods common in supervised acoustic modeling to unsupervisedly learn feature transformations for application to the input data prior to clustering. The idea is to automatically find mappings of feature vectors into sub-spaces that are more robust to channel, context and speaker variability. The need of labels for these techniques makes it difficult to use them in a zero resource setting. To overcome this issue we utilize a first iteration of DPGMM clustering to generate frame based class labels for the target data. The labels serve as basis for learning an acoustic model in the form of hidden Markov models (HMMs) using linear discriminant analysis (LDA), maximum likelihood linear transform (MLLT) and speaker adaptive training (SAT). We show that the learned transformations lead to features that consistently outperform untransformed features on the ABX sound class discriminability task. We also demonstrate that the combination of multiple clustering runs is a suitable method to further enhance sound class discriminability",
    "checked": true,
    "id": "8616fbcc05cbf7019593e0659e0d1d3a1c7a0091",
    "semantic_title": "supervised learning of acoustic models in a zero resource setting to improve dpgmm clustering",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xu16_interspeech.html": {
    "title": "Semi-Supervised and Cross-Lingual Knowledge Transfer Learnings for DNN Hybrid Acoustic Models Under Low-Resource Conditions",
    "volume": "main",
    "abstract": "Semi-supervised and cross-lingual knowledge transfer learnings are two strategies for boosting performance of low-resource speech recognition systems. In this paper, we propose a unified knowledge transfer learning method to deal with these two learning tasks. Such a knowledge transfer learning is realized by fine-tuning of Deep Neural Network (DNN). We demonstrate its effectiveness in both monolingual based semi-supervised learning task and cross-lingual knowledge transfer learning task. We then combine these two learning strategies to obtain further performance improvement",
    "checked": true,
    "id": "9dffcb0314b57e94662d143d1ddb9a1cc1af67c6",
    "semantic_title": "semi-supervised and cross-lingual knowledge transfer learnings for dnn hybrid acoustic models under low-resource conditions",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2016/asami16_interspeech.html": {
    "title": "Recurrent Out-of-Vocabulary Word Detection Using Distribution of Features",
    "volume": "main",
    "abstract": "The repeated use of out-of-vocabulary (OOV) words in a spoken document seriously degrades a speech recognizer's performance. This paper provides a novel method for accurately detecting such recurrent OOV words. Standard OOV word detection methods classify each word segment into in-vocabulary (IV) or OOV. This word-by-word classification tends to be affected by sudden vocal irregularities in spontaneous speech, triggering false alarms. To avoid this sensitivity to the irregularities, our proposal focuses on consistency of the repeated occurrence of OOV words. The proposed method preliminarily detects recurrent segments, segments that contain the same word, in a spoken document by open vocabulary spoken term discovery using a phoneme recognizer. If the recurrent segments are OOV words, features for OOV detection in those segments should exhibit consistency. We capture this consistency by using the mean and variance (distribution) of features (DOF) derived from the recurrent segments, and use the DOF for IV/OOV classification. Experiments illustrate that the proposed method's use of the DOF significantly improves its performance in recurrent OOV word detection",
    "checked": true,
    "id": "814236445e3596d6ef7a831b9095a710a444c10f",
    "semantic_title": "recurrent out-of-vocabulary word detection using distribution of features",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kanda16_interspeech.html": {
    "title": "Investigation of Semi-Supervised Acoustic Model Training Based on the Committee of Heterogeneous Neural Networks",
    "volume": "main",
    "abstract": "This paper investigates the semi-supervised training for deep neural network-based acoustic models (AM). In the conventional self-learning approach, a \"seed-AM\" is first trained by using a small transcribed data set. Then, a large untranscribed data set is decoded by using the seed-AM to create a transcription, which is finally used to train a new AM on the entire data. Our investigation in this paper focuses on the different approach that uses additional complementary AMs to form a committee of label creation for untranscribed data. Especially, we investigate the case of using heterogeneous neural networks as complementary AMs, and the case of intentional exclusion of the primary seed-AM from the committee, both of which could enhance the chance to find more informative training samples for the seed-AM. We investigated those approaches based on Japanese lecture recognition experiments with 50-hours of transcribed data and 190-hours of untranscribed data. In our experiment, the committee-based approach showed significant improvements in the word error rate, and the best method finally recovered 75.2% of the oracle improvement with full manual transcription, while the conventional self-learning approach recovered only 32.7% of the oracle gain",
    "checked": true,
    "id": "d306d9dcfe1fb83343d6866ee11fb2037345cee3",
    "semantic_title": "investigation of semi-supervised acoustic model training based on the committee of heterogeneous neural networks",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ghannay16_interspeech.html": {
    "title": "Acoustic Word Embeddings for ASR Error Detection",
    "volume": "main",
    "abstract": "This paper focuses on error detection in Automatic Speech Recognition (ASR) outputs. A neural network architecture is proposed, which is well suited to handle continuous word representations, like word embeddings. In a previous study, the authors explored the use of linguistic word embeddings, and more particularly their combination. In this new study, the use of acoustic word embeddings is explored. Acoustic word embeddings offer the opportunity of an a priori acoustic representation of words that can be compared, in terms of similarity, to an embedded representation of the audio signal First, we propose an approach to evaluate the intrinsic performances of acoustic word embeddings in comparison to orthographic representations in order to capture discriminative phonetic information. Since French language is targeted in experiments, a particular focus is made on homophone words. Then, the use of acoustic word embeddings is evaluated for ASR error detection. The proposed approach gets a classification error rate of 7.94% while the previous state-of-the-art CRF-based approach gets a CER of 8.56% on the outputs of the ASR system which won the ETAPE evaluation campaign on speech recognition of French broadcast news",
    "checked": true,
    "id": "9fc85af4b7de3dd31677e6d5dcea0b3e748e9dba",
    "semantic_title": "acoustic word embeddings for asr error detection",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2016/horndasch16_interspeech.html": {
    "title": "Combining Semantic Word Classes and Sub-Word Unit Speech Recognition for Robust OOV Detection",
    "volume": "main",
    "abstract": "Out-of-vocabulary words (OOVs) are often the main reason for the failure of tasks like automated voice searches or human-machine dialogs. This is especially true if rare but task-relevant content words, e.g. person or location names, are not in the recognizer's vocabulary. Since applications like spoken dialog systems use the result of the speech recognizer to extract a semantic representation of a user utterance, the detection of OOVs as well as their (semantic) word class can support to manage a dialog successfully. In this paper we suggest to combine two well-known approaches in the context of OOV detection: semantic word classes and OOV models based on sub-word units. With our system, which builds upon the widely used Kaldi speech recognition toolkit, we show on two different data sets that — compared to other methods — such a combination improves OOV detection performance for open word classes at a given false alarm rate. Another result of our approach is a reduction of the word error rate (WER)",
    "checked": true,
    "id": "9f98ad7abdc6ee43350cf7145dcb8d2f2067b572",
    "semantic_title": "combining semantic word classes and sub-word unit speech recognition for robust oov detection",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xie16b_interspeech.html": {
    "title": "Web Data Selection Based on Word Embedding for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "The lack of transcription files will lead to a high out-of-vocabulary (OOV) rate and a weak language model in low-resource speech recognition systems. This paper presents a web data selection method to augment these systems. After mapping all the vocabularies or short sentences to vectors in a low-dimensional space through a word embedding technique, the similarities between the web data and the small pool of training transcriptions are calculated. Then, the web data with high similarity are selected to expand the pronunciation lexicon or language model. Experiments are conducted on the NIST Open KWS15 Swahili VLLP recognition task. Compared with the baseline system, our methods can achieve a 5.23% absolute reduction in word error rate (WER) using the expanded pronunciation lexicon and a 9.54% absolute WER reduction using both the expanded lexicon and language model",
    "checked": true,
    "id": "b14ad225afa1198a6cc7f75ea925fb0a8b2a850f",
    "semantic_title": "web data selection based on word embedding for low-resource speech recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/alshareef16_interspeech.html": {
    "title": "Colloquialising Modern Standard Arabic Text for Improved Speech Recognition",
    "volume": "main",
    "abstract": "Modern standard Arabic (MSA) is the official language of spoken and written Arabic media. Colloquial Arabic (CA) is the set of spoken variants of modern Arabic that exist in the form of regional dialects. CA is used in informal and everyday conversations while MSA is formal communication. An Arabic speaker switches between the two variants according to the situation. Developing an automatic speech recognition system always requires a large collection of transcribed speech or text, and for CA dialects this is an issue. CA has limited textual resources because it exists only as a spoken language, without a standardised written form unlike MSA. This paper focuses on the data sparsity issue in CA textual resources and proposes a strategy to emulate a native speaker in colloquialising MSA to be used in CA language models (LMs) by use of a machine translation (MT) framework. The empirical results in Levantine CA show that using LMs estimated from colloquialised MSA data outperformed MSA LMs with a perplexity reduction up to 68% relative. In addition, interpolating colloquialised MSA LMs with a CA LMs improved speech recognition performance by 4% relative",
    "checked": true,
    "id": "67303eb392ea404bb4d26e7bd68441614e0c6a5a",
    "semantic_title": "colloquialising modern standard arabic text for improved speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kuang16_interspeech.html": {
    "title": "Pitch-Range Perception: The Dynamic Interaction Between Voice Quality and Fundamental Frequency",
    "volume": "main",
    "abstract": "Effective pitch-range normalization is important to uncover intended linguistic pitch targets in continuous speech. Our previous study demonstrated that voice quality plays a role in pitch-range perception: \"tense voice\", implemented as stimuli with spectral balance tilted towards higher frequency, was perceived as higher in pitch. This psychoacoustic effect is consistent with the co-variation between pitch and tense voice in production. However, a spectral balance tilted towards higher frequency is also one of the properties of creaky voice, which is often associated with low pitch in production. Therefore, this raises the possibility that manipulating the f0 range of the stimuli or changing the sex of the speaker of the stimuli can reverse the direction of the shift. This current study replicates the previous experiment with the same forced-choice pitch classification experiment with four spectral conditions, but uses a female voice to create the stimuli. In addition, two f0 ranges are used in the current experiments, which resemble the lower range and the higher range of a female voice. Overall, the results show that spectral balance interacts with f0 range: the presence of voice quality cues affect the perception of pitch range; but, the spectrum with greater energy in the high-frequency range can be interpreted as either creaky or tense depending on the f0 range. This current study enriches our understanding of the interaction between voice quality and pitch",
    "checked": true,
    "id": "e2b53c57fa910fc9841d288b32059b6676e125b2",
    "semantic_title": "pitch-range perception: the dynamic interaction between voice quality and fundamental frequency",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16e_interspeech.html": {
    "title": "Comparing the Contributions of Amplitude and Phase to Speech Intelligibility in a Vocoder-Based Speech Synthesis Model",
    "volume": "main",
    "abstract": "Vocoder-based speech synthesis model has been long used to assess the contribution of acoustic cue for speech recognition. This study compared the perceptual contributions of amplitude and phase by using two types of stimuli, i.e., amplitude- and phase-based vocoded stimuli. The amplitude-based vocoded stimuli were synthesized by preserving amplitude fluctuation cue but discarding phase cue (i.e., setting phase to zero), while the phase-based vocoded stimuli were synthesized by preserving phase cue and discarding amplitude cue (i.e., setting amplitude to unit). Listening experiments with normal-hearing participants showed consistent findings with earlier studies that the intelligibility scores of both amplitude- and phase-based vocoded stimuli increased when using a large number of channels in vocoder-based speech synthesis. In addition, at all tested conditions, the intelligibility scores of amplitude-based vocoded stimuli were significantly larger than those of phase-based vocoded stimuli, suggesting that amplitude might carry more perceptual contribution than phase. This intelligibility advantage of amplitude over phase may be attributed to the difference in the amount of envelope information contained in the two types of vocoded stimuli",
    "checked": true,
    "id": "95ada69cf6d9115313744a4ab0030f88dff1d2b0",
    "semantic_title": "comparing the contributions of amplitude and phase to speech intelligibility in a vocoder-based speech synthesis model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16f_interspeech.html": {
    "title": "Modeling Noise Influence to Speech Intelligibility Non-Intrusively by Reduced Speech Dynamic Range",
    "volume": "main",
    "abstract": "The noise influence to speech signal waveform can be characterized by reduced speech dynamic range (rDR). This motivated the present work to propose an rDR-based intelligibility measure (denoted as rDRm) that could be used to non-intrusively (i.e., do not require clean reference speech signal) predict speech intelligibility in noise and is computed only using the dynamic range extracted from the noise-corrupted speech. The rDRm indices were evaluated with intelligibility scores obtained from normal-hearing listeners presented with sentences corrupted by four types of maskers in a total of 22 conditions. High correlation (r=0.93) was obtained between rDRm values and listeners' sentence recognition scores, and this correlation was comparable to those computed with existing intrusive and non-intrusive intelligibility measures. This suggests that the dynamic range of speech signal may work as a simple but efficient predictor of speech intelligibility in noise, whose computation does not need access to the clean reference speech signal",
    "checked": true,
    "id": "b2d9f915ad0d7fe44ea95aa0c946ec070b0fa197",
    "semantic_title": "modeling noise influence to speech intelligibility non-intrusively by reduced speech dynamic range",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pinter16_interspeech.html": {
    "title": "Do GMM Phoneme Classifiers Perceive Synthetic Sibilants as Humans Do?",
    "volume": "main",
    "abstract": "This study presents a psycholinguistically motivated evaluation method for phoneme classifiers by using non-categorical perceptual data elicited in a Japanese sibilant matching 2AFC task. Probability values of a perceptual [s]-[ʃ] boundary, obtained from 42 speakers over a 7-step synthetic [s]-[ʃ] continuum, were compared to probability estimates of Gaussian mixture models (GMMs) of Japanese [s] and [ʃ]. The GMMs, trained on the Corpus of Spontaneous Japanese, differed in feature vectors (MFCC, PLP, acoustic features), covariance matrix types (full, tied, diagonal, spherical), and numbers of mixtures (1–20). Using ten-fold cross validation, it was found that GMMs trained on MFCC features had the best sibilant classification accuracies (87.4–90.4%), but their correlations with human perceptual data were non-conclusive (0.35–0.98). Acoustic feature-based GMMs with tied covariance matrices had near human-like synthetic stimuli perception (0.957–0.996), but their classification performance was poor (71.3–80.4%). Models trained on perceptual linear prediction (PLP) features were on par with the acoustic feature-based models in terms correlation to the perceptual experiment (0.884–0.995), while losing slightly on classification performance (86.1–88.9%) compared to MFCC models. Across the board correlation tests and mixture-effect models confirmed that GMMs with better sibilant classifying performance produced more human-like probability estimations on the synthetic sibilant continuum",
    "checked": true,
    "id": "3e2fe7ac31a1827e93e1494b8db130480f3098b7",
    "semantic_title": "do gmm phoneme classifiers perceive synthetic sibilants as humans do?",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/frye16_interspeech.html": {
    "title": "Neural Responses to Speech-Specific Modulations Derived from a Spectro-Temporal Filter Bank",
    "volume": "main",
    "abstract": "This paper analyzes the application of methods developed in automatic speech recognition (ASR) to better understand neural activity measured with electrocorticography (ECoG) during the presentation of speech. ECoG data is collected from temporal cortex in two subjects listening to a matrix sentence test. We investigate the relation of ECoG signals and acoustic speech that has been processed with spectro-temporal filters, which have been shown to produce robust and reliable representations for speech applications. The organization of spectro-temporal filters into a filter bank allows for a straight-forward separation into spectral or temporal only, as well as true spectro-temporal components. We find electrodes positioned over the superior temporal gyrus that is associated with the auditory cortex to show significant specific high gamma activity to fine temporal and spectro-temporal patterns present in speech. This indicates that representations developed in machine listening are a suitable tool for the analysis of biosignals",
    "checked": true,
    "id": "9fc8a6904714504d565e45037549bc9ec7aaf094",
    "semantic_title": "neural responses to speech-specific modulations derived from a spectro-temporal filter bank",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mulder16_interspeech.html": {
    "title": "Comparing Different Methods for Analyzing ERP Signals",
    "volume": "main",
    "abstract": "Event-Related Potential (ERP) signals obtained from EEG recordings are widely used for studying cognitive processes in spoken language processing. The computation of ERPs involves averaging over multiple participants and multiple stimuli. Especially with speech stimuli, which also evoke substantial exogenous excitation, even averaging within conditions results in pooling many sources of variance. This raises questions about the statistical processing needed to uncover reliable differences between conditions. In this study we investigate differences between ERPs when participants listened to full and reduced pronunciations of verb forms in Dutch, in isolation and in mid-sentence position. Conventional statistical analysis uncovers some (but not all) differences between full and reduced forms in isolation, but not in mid-sentence position. In this paper, we show that linear mixed models (lmer) and generalized additive models (gam), which are able to account for participant- and stimulus-related variance may uncover more effects than conventional statistical models. However, depending on the complexity of the data, lmer and gam models may not be able to fit the data closely enough to warrant blind interpretation of the summary output. We discuss opportunities and threats of these approaches to analyzing ERP signals",
    "checked": true,
    "id": "f5d02e99f228175c15519a8dfaa043f5f88017bf",
    "semantic_title": "comparing different methods for analyzing erp signals",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/eklund16_interspeech.html": {
    "title": "Supplementary Motor Area Activation in Disfluency Perception: An fMRI Study of Listener Neural Responses to Spontaneously Produced Unfilled and Filled Pauses",
    "volume": "main",
    "abstract": "Spontaneously produced Unfilled Pauses (UPs) and Filled Pauses (FPs) were played to subjects in an fMRI experiment. For both stimuli increased activity was observed in the Primary Auditory Cortex (PAC). However, FPs, but not UPs, elicited modulation in the Supplementary Motor Area (SMA), Brodmann Area 6. Our results provide neurocognitive confirmation of the alleged difference between FPs and other kinds of speech disfluency and could also provide a partial explanation for the previously reported beneficial effect of FPs on reaction times in speech perception. Our results also have potential implications for two of the suggested functions of FPs: the \"floor-holding\" and the \"help-me-out\" hypotheses",
    "checked": true,
    "id": "81a5ac57353779353da0f83b5ecfe734a6e41f7f",
    "semantic_title": "supplementary motor area activation in disfluency perception: an fmri study of listener neural responses to spontaneously produced unfilled and filled pauses",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fogerty16_interspeech.html": {
    "title": "Vowel Fundamental and Formant Frequency Contributions to English and Mandarin Sentence Intelligibility",
    "volume": "main",
    "abstract": "The current study investigated spectral components of vowels that contribute to Mandarin and English sentence intelligibility. Sentences were processed to preserve various amounts of vowel information. Processing parameters ensured similar proportions of speech preserved between the two languages. In the first experiment, speech segments, primarily containing vocalic cues, were processed to flatten fundamental frequency (F0) cues. In the second experiment, sine-wave speech synthesis was used to coarsely code speech to retain only amplitude and frequency variation associated with the first three formants. Results demonstrated remarkable similarity between Mandarin and English sentence intelligibility with flattened F0 sentences. In contrast, the intelligibility of English sentences surpassed that of Mandarin sentences for sine-wave speech. Combined with earlier reports of superior intelligibility of Mandarin sentences with full spectrum vowels, these results highlight significant contributions of Mandarin F0 information, likely related to lexical tone. In contrast, English listeners may rely more on frequency and/or amplitude variation of the formants",
    "checked": true,
    "id": "4839fb829f912c46cf706a9d79df3e51814640de",
    "semantic_title": "vowel fundamental and formant frequency contributions to english and mandarin sentence intelligibility",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/huang16b_interspeech.html": {
    "title": "Attention Assisted Discovery of Sub-Utterance Structure in Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Recently, attention mechanism based deep learning has gained much popularity in speech recognition and natural language processing due to its flexibility at the decoding phase. Through the attention mechanism, the relevant encoding context vectors contribute a majority portion to the construction of the decoding context, while the effect of the irrelevant ones is minimized. Inspired by this idea, a speech emotion recognition system is proposed in this work for an active selection of sub-utterance representations to better compose a discriminative utterance representation. Compared to the baseline of a model based on the uniform attention, i.e. no attention at all, an attention based model improves the weighted accuracy by an absolute of 1.46% (and relative 57.87% to 59.33%) on the emotion classification task. Moreover, the selection distribution leads to a better understanding of the sub-utterance structure in an emotional utterance",
    "checked": true,
    "id": "4bee98ab1db3635660076e067de24d073b3048d2",
    "semantic_title": "attention assisted discovery of sub-utterance structure in speech emotion recognition",
    "citation_count": 81
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16b_interspeech.html": {
    "title": "Combining CNN and BLSTM to Extract Textual and Acoustic Features for Recognizing Stances in Mandarin Ideological Debate Competition",
    "volume": "main",
    "abstract": "Recognizing stances in ideological debates is a relatively new and challenging problem in opinion mining. While previous work mainly focused on text modality, in this paper, we try to recognize stances from both text and acoustic modalities, where how to derive more representative textual and acoustic features still remains the research problem. Inspired by the promising performances of neural network models in natural language understanding and speech processing, we propose a unified framework named C-BLSTM by combining convolutional neural network (CNN) and bidirectional long short-term memory (BLSTM) recurrent neural network (RNN) for feature extraction. In C-BLSTM, CNN is utilized to extract higher-level local features of text (n-grams) and speech (emphasis, intonation), while BLSTM is used to extract bottleneck features for context-sensitive feature compression and target-related feature representation. Maximum entropy model is then used to recognize stances from the bimodal textual acoustic bottleneck features. Experiments on four debate datasets show C-BLSTM outperforms all challenging baseline methods, and specifically, acoustic intonation and emphasis features further improve F1-measure by 6% as compared to textual features only",
    "checked": true,
    "id": "a68ba81dd203ef5b405edf00518d0627c7a1f02c",
    "semantic_title": "combining cnn and blstm to extract textual and acoustic features for recognizing stances in mandarin ideological debate competition",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2016/trouvain16_interspeech.html": {
    "title": "Inter-Speech Clicks in an Interspeech Keynote",
    "volume": "main",
    "abstract": "Clicks are usually described as phoneme realisations in some African languages or as paralinguistic vocalisations, e.g. to signal disapproval or as sound imitation. A more recent discovery is that clicks are, presumably unintentionally, used as discourse markers indexing a new sequence in a conversation or before a word search. In this single-case study, we investigated more than 300 apical clicks of an experienced speaker during a keynote address at an Interspeech conference. The produced clicks occurred only in inter-speech intervals and were often combined with either hesitation particles like \"uhm\" or audible inhalation. Our observations suggest a link between click production and ingressive airflow as well as indicate that clicks are used as hesitation markers. The rather high frequency of clicks in the analysed sections from the 1-hour-talk shows that in larger discourse, the time between articulatory phases consists of more than silence, audible inhalation and typical hesitation particles. The rather large variation in the intensity and duration and particularly the number of bursts of the observed clicks indicates that this prosodic discourse marker seems to be a rather acoustically inconsistent phonetic category",
    "checked": true,
    "id": "dfa50202b2c9e6277c716b6707cc803c938b7c65",
    "semantic_title": "inter-speech clicks in an interspeech keynote",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/grzybowska16_interspeech.html": {
    "title": "Speaker Age Classification and Regression Using i-Vectors",
    "volume": "main",
    "abstract": "In this paper, we examine the use of i-vectors both for age regression as well as for age classification. Although i-vectors have been previously used for age regression task, we extend this approach by applying fusion of i-vectors and acoustic features regression to estimate the speaker age. By our fusion we obtain a relative improvement of 12.6% comparing to solely i-vector system We also use i-vectors for age classification, which to our knowledge is the first attempt to do so. Our best results reach unweighted accuracy 62.9%, which is a relative improvement of 16.7% comparing to the best results obtained in age classification task at Age Sub-Challenge at Interspeech 2010",
    "checked": true,
    "id": "538f3c342f19d498dcbf965685f7433c361fcd84",
    "semantic_title": "speaker age classification and regression using i-vectors",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16c_interspeech.html": {
    "title": "Sparsely Connected and Disjointly Trained Deep Neural Networks for Low Resource Behavioral Annotation: Acoustic Classification in Couples' Therapy",
    "volume": "main",
    "abstract": "Observational studies are based on accurate assessment of human state. A behavior recognition system that models interlocutors' state in real-time can significantly aid the mental health domain. However, behavior recognition from speech remains a challenging task since it is difficult to find generalizable and representative features because of noisy and high-dimensional data, especially when data is limited and annotated coarsely and subjectively. Deep Neural Networks (DNN) have shown promise in a wide range of machine learning tasks, but for Behavioral Signal Processing (BSP) tasks their application has been constrained due to limited quantity of data We propose a Sparsely-Connected and Disjointly-Trained DNN (SD-DNN) framework to deal with limited data. First, we break the acoustic feature set into subsets and train multiple distinct classifiers. Then, the hidden layers of these classifiers become parts of a deeper network that integrates all feature streams. The overall system allows for full connectivity while limiting the number of parameters trained at any time and allows convergence possible with even limited data. We present results on multiple behavior codes in the couples' therapy domain and demonstrate the benefits in behavior classification accuracy. We also show the viability of this system towards live behavior annotations",
    "checked": true,
    "id": "e297000d4e2eff0a741e2e6c1704ccd6da0ea419",
    "semantic_title": "sparsely connected and disjointly trained deep neural networks for low resource behavioral annotation: acoustic classification in couples' therapy",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2016/an16_interspeech.html": {
    "title": "Automatically Classifying Self-Rated Personality Scores from Speech",
    "volume": "main",
    "abstract": "Automatic personality recognition is useful for many computational applications, including recommendation systems, dating websites, and adaptive dialogue systems. There have been numerous successful approaches to classify the \"Big Five\" personality traits from a speaker's utterance, but these have largely relied on judgments of personality obtained from external raters listening to the utterances in isolation. This work instead classifies personality traits based on self-reported personality tests, which are more valid and more difficult to identify. Our approach, which uses lexical and acoustic-prosodic features, yields predictions that are between 6.4% and 19.2% more accurate than chance. This approach predicts Openness-to-Experience and Neuroticism most successfully, with less accurate recognition of Extroversion. We compare the performance of classification and regression techniques, and also explore predicting personality clusters",
    "checked": true,
    "id": "7b8be8496400844863c4106a36b529cd403f3914",
    "semantic_title": "automatically classifying self-rated personality scores from speech",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lehman16_interspeech.html": {
    "title": "Estimation of Children's Physical Characteristics from Their Voices",
    "volume": "main",
    "abstract": "To date, multiple strategies have been proposed for the estimation of speakers' physical parameters such as height, weight, age, gender etc. from their voices. These employ various types of feature measurements in conjunction with different regression and classification mechanisms. While some are quite effective for adults, they are not so for children's voices. This is presumably because in children, the relationship between voice and physical parameters is relatively more complex. The vocal tracts of adults, and the processes that accompany speech production, are fully mature and do not undergo changes within small age differentials. In children, however, these factors change continuously with age, causing variations in style, content, enunciation, rate and quality of their speech. Strategies for the estimation of children's physical parameters from their voice must take this variability into account. In this paper, using different formant-related measurements as exemplary analysis features generated within articulatory-phonetic guidelines, we demonstrate the nonlinear relationships of children's physical parameters to their voice. We also show how such analysis can help us focus on the specific sounds that relate well to each parameter, which can be useful in obtaining more accurate estimates of the physical parameters",
    "checked": true,
    "id": "f6951608b20d81792aa98b7c23263b81a962d23d",
    "semantic_title": "estimation of children's physical characteristics from their voices",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/akira16_interspeech.html": {
    "title": "Talking to a System and Talking to a Human: A Study from a Speech-to-Speech, Machine Translation Mediated Map Task",
    "volume": "main",
    "abstract": "This study focuses on the properties of Human-to-Human (H2H) communication in spontaneous dialogues in two different settings. Direct H2H dialogues are compared to the ones that are mediated by a Speech-to-Speech machine translation system. For the analysis, dialogues from the HCRC Map Task Corpus, for direct H2H conversations, and dialogues from the ILMT-s2s Corpus, for computer mediated conversations, were used. In the conversations speakers take the roles of information giver and follower and all the utterances are labelled as instructions, questions or statement, etc. While direct H2H communication enables speakers also to benefit from non-verbal acts, gestures and facial expressions, machine mediated conversation is more complex for the interlocutors. Due to errors made by speech recognition system, speakers adapt their speaking style and also apply repair strategies in order to accomplish the tasks successfully. Comparing the two corpora showed that in the case of computer mediated communication the utterances of the speakers contained less words than in the case of direct H2H interaction where utterances were longer. Also, different word count was found depending on the role of the speaker as well as on the type of the utterance",
    "checked": false,
    "id": "02c02473e757a7b906e978017badba43418ae672",
    "semantic_title": "talking to a system and oneself: a study from a speech-to-speech, machine translation mediated map task",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gupta16b_interspeech.html": {
    "title": "Predicting Affective Dimensions Based on Self Assessed Depression Severity",
    "volume": "main",
    "abstract": "Depression is a state of severe despondency and affects a person's thoughts and behavior. Depression leads to several psychiatric symptoms such as fatigue, restlessness, insomnia as well as other mood disorders (e.g. anxiety and irritation). These symptoms have a resultant impact on the subject's emotional expression. In this work, we address the problem of predicting the emotional dimensions of valence, arousal and dominance in subjects suffering from variable levels of depression, as quantified by the Beck Depression Inventory-II (BDI-II) index. We investigate the relationship between depression severity and affect, and propose a novel method for incorporating the BDI-II index in affect prediction. We validate our models on two datasets recorded as a part of the AViD (Audio-Visual Depressive language) corpus: Freeform and Northwind. Using the depression severity and a set of audio-visual cues, we obtain an average correlation coefficient of .33/.52 for affective dimension prediction in the Freeform/Northwind datasets, against baseline performances of .24/.48 based on using the audio-visual cues only. Our experiments suggest that the knowledge of depression severity significantly improves the emotion dimension prediction, however the BDI-II score incorporation scheme varies between the two datasets of interest",
    "checked": true,
    "id": "71a3f75eb5528b04729572066ef7f352540a3de0",
    "semantic_title": "predicting affective dimensions based on self assessed depression severity",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/huang16c_interspeech.html": {
    "title": "Enhancement of Automatic Oral Presentation Assessment System Using Latent N-Grams Word Representation and Part-of-Speech Information",
    "volume": "main",
    "abstract": "The development of an automatic oral presentation assessment system is important for the educational researchers to assess and train the communication ability of school leaders. In this work, we aim at enhancing the performance of the existing pre-service school principals' presentation scoring system by including lexical information as an additional modality. We propose to use latent n-grams distributed word representations and weighted counts of part-of-speech tag to derive features from the speech transcripts in the National Academy for Educational Research (NAER) oral presentation database. We carry out two different experiments: Exp I is a binary classification task between high versus low performing speech, and Exp II is a continuous scoring on the entire dataset. In Exp I, the proposed framework achieves a competitive accuracy of 0.79, and in Exp II, by fusing this text-based system to the existing audio-video based system, we obtain a spearman correlation of 0.641 (18.05% relative improvement). The two experiments demonstrate the modeling power of our proposed framework and signify the substantial complementary information in the lexical modality while assessing the quality of an oral presentation",
    "checked": true,
    "id": "a629cf8825ecf2ed2ed1d7c1b730e0df83226a3e",
    "semantic_title": "enhancement of automatic oral presentation assessment system using latent n-grams word representation and part-of-speech information",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dumpala16b_interspeech.html": {
    "title": "Use of Vowels in Discriminating Speech-Laugh from Laughter and Neutral Speech",
    "volume": "main",
    "abstract": "In natural conversations, significant part of laughter co-occurs with speech which is referred to as speech-laugh. Hence, speech-laugh will have characteristics of both laughter and neutral speech. But it is not clearly evident how acoustic properties of neutral speech are influenced by its co-occurring laughter. The objective of this study is to analyze the acoustic variations between vowel regions of laughter, speech-laugh and neutral speech. The features based on excitation source characteristics extracted at epochs are considered in this study. Features extracted in the vowel regions of speech-laugh exhibit deviations from that of laughter and neutral speech. These deviations in feature values are exploited to discriminate speech-laugh from laughter and neutral speech. Two different datasets consisting of conversational speech and meeting recordings are used in this analysis. Experimental results show that the discrimination between the three classes obtained by considering vowel regions is better than that of considering the complete utterance",
    "checked": true,
    "id": "c3ed098849364806ba8b1732655b88c82352acf2",
    "semantic_title": "use of vowels in discriminating speech-laugh from laughter and neutral speech",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kawabata16_interspeech.html": {
    "title": "A Convex Model for Linguistic Influence in Group Conversations",
    "volume": "main",
    "abstract": "Conversational partners can influence each other's speaking patterns. In this paper, we aim to develop a computational model that infers influence levels directly from language samples. We propose a new approach to modeling linguistic influence in conversations based on a well-accepted model of social influence. Very generally, this approach assumes that an individual's language model can be expressed as a convex combination of language models from individuals with whom that person interacts. We propose an optimization criterion to estimate the pairwise influence between conversational partners directly from speech and language data. We evaluate the model on three different corpora: (1) a synthetic corpus where the language influence is experimentally set; (2) a corpus that tracks a child's interaction with her family during the early stages of language development; (3) a corpus of Supreme Court cases analyzing interactions between judges and attorneys",
    "checked": true,
    "id": "167384c713cb343bd5c2bf29a09f98ed83b77487",
    "semantic_title": "a convex model for linguistic influence in group conversations",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gibson16_interspeech.html": {
    "title": "A Deep Learning Approach to Modeling Empathy in Addiction Counseling",
    "volume": "main",
    "abstract": "Motivational interviewing is a goal-oriented psychotherapy, employed in cases such as addiction, that aims to help clients explore and resolve their ambivalence about their problem. In motivational interviewing, it is desirable for the counselor to communicate empathy towards the client to promote better therapy outcomes. In this paper, we propose a deep neural network (DNN) system for predicting counselors' session level empathy ratings from transcripts of the interactions. First, we train a recurrent neural network mapping the text of each speaker turn to a set of task-specific behavioral acts that represent local dynamics of the client-counselor interaction. Subsequently, this network is used to initialize lower layers of a deep network predicting session level counselor empathy rating. We show that this method outperforms training the DNN end-to-end in a single stage and also outperforms a baseline neural network model that attempts to predict empathy ratings directly from text without modeling turn level behavioral dynamics",
    "checked": true,
    "id": "59504cf249eaa4c1664d4342e2a33517094d43f4",
    "semantic_title": "a deep learning approach to modeling empathy in addiction counseling",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2016/huang16d_interspeech.html": {
    "title": "Unipolar Depression vs. Bipolar Disorder: An Elicitation-Based Approach to Short-Term Detection of Mood Disorder",
    "volume": "main",
    "abstract": "Mood disorders include unipolar depression (UD) and bipolar disorder (BD). In this work, an elicitation-based approach to short-term detection of mood disorder based on the elicited speech responses is proposed. First, a long-short term memory (LSTM)-based classifier was constructed to generate the emotion likelihood for each segment in the elicited speech responses. The emotion likelihoods were then clustered into emotion codewords using the K-means algorithm. Latent semantic analysis (LSA) was then adopted to model the latent relationship between the emotion codewords and the elicited responses. The structural relationships among the emotion codewords in the LSA-based matrix were employed to construct a latent affective structure model (LASM) for characterizing each mood. For mood disorder detection, the similarity between the input speech LASM and each of the mood-specific LASMs was estimated. Finally, the mood with its LASM most similar to the input speech LASM is regarded as the detected mood. Experimental results show that the proposed LASM-based method achieved 73.3%, improving the detection accuracy by 13.3% compared to the commonly used SVM-based classifiers",
    "checked": true,
    "id": "504aa4d8de1624b5d34da51abce37d3ead908373",
    "semantic_title": "unipolar depression vs. bipolar disorder: an elicitation-based approach to short-term detection of mood disorder",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/masmoudi16_interspeech.html": {
    "title": "Conditional Random Fields for the Tunisian Dialect Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "Conditional Random Fields (CRFs) represent an effective approach for monotone string-to-string translation tasks. In this work, we apply the CRF model to perform grapheme-to-phoneme (G2P) conversion for the Tunisian Dialect. This choice is motivated by the fact that CRFs give a long term prediction and assume relaxed state independence conditions compared to HMMs [7]. The CRF model needs to be trained on a 1-to-1 alignement between graphemes and phonemes. Alignments are generated using Joint-Multigram Model (JMM) and GIZA++ toolkit. We trained CRF model for each generated alignment. We then compared our models to state-of-the-art G2P systems based on Sequitur G2P and Phonetisaurus toolkit. We also investigate the CRF prediction quality with different training size. Our results show that CRF perform slightly better using JMM alignment and outperform both Sequitur and Phonetisaurus systems with different training size. At the end, our system gets a phone error rate of 14.09%",
    "checked": true,
    "id": "a633896e6665ae1220abed1d05837861a856daf7",
    "semantic_title": "conditional random fields for the tunisian dialect grapheme-to-phoneme conversion",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/saychum16_interspeech.html": {
    "title": "Efficient Thai Grapheme-to-Phoneme Conversion Using CRF-Based Joint Sequence Modeling",
    "volume": "main",
    "abstract": "This paper presents the successful results of applying joint sequence modeling in Thai grapheme-to-phoneme conversion. The proposed method utilizes Conditional Random Fields (CRFs) in two-stage prediction. The first CRF is used for textual syllable segmentation and syllable type prediction. Graphemes and their corresponding phonemes are then aligned using well-designed many-to-many alignment rules and outputs given by the first CRF. The second CRF, modeling the jointly aligned sequences, efficiently predicts phonemes. The proposed method obviously improves the prediction of linking syllables, normally hidden from their textual graphemes. Evaluation results show that the prediction word error rate (WER) of the proposed method reaches 13.66%, which is 11.09% lower than that of the baseline system",
    "checked": true,
    "id": "3756bbd2b5f6c0f2dd6fd2df98bbf20b7f05ef34",
    "semantic_title": "efficient thai grapheme-to-phoneme conversion using crf-based joint sequence modeling",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jaumardhakoun16_interspeech.html": {
    "title": "An Articulatory-Based Singing Voice Synthesis Using Tongue and Lips Imaging",
    "volume": "main",
    "abstract": "Ultrasound imaging of the tongue and videos of lips movements can be used to investigate specific articulation in speech or singing voice. In this study, tongue and lips image sequences recorded during singing performance are used to predict vocal tract properties via Line Spectral Frequencies (LSF). We focused our work on traditional Corsican singing \"Cantu in paghjella\". A multimodal Deep Autoencoder (DAE) extracts salient descriptors directly from tongue and lips images. Afterwards, LSF values are predicted from the most relevant of these features using a multilayer perceptron. A vocal tract model is derived from the predicted LSF, while a glottal flow model is computed from a synchronized electroglottographic recording. Articulatory-based singing voice synthesis is developed using both models. The quality of the prediction and singing voice synthesis using this method outperforms the state of the art method",
    "checked": true,
    "id": "26076e4e4fc80eee51ed9dcda49b3fe7bca7f334",
    "semantic_title": "an articulatory-based singing voice synthesis using tongue and lips imaging",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16d_interspeech.html": {
    "title": "Phoneme Embedding and its Application to Speech Driven Talking Avatar Synthesis",
    "volume": "main",
    "abstract": "Word embedding has made great achievements in many natural language processing tasks. However, the attempt to apply word embedding to the field of speech got few breakthroughs. The reason is that word vectors mainly contain semantic and syntactic information. Such high level features are difficult to be directly incorporated in speech related tasks compared to acoustic or phoneme related features. In this paper, we investigate the method for phoneme embedding to generate phoneme vectors carrying acoustic information for speech related tasks. One-hot representations of phoneme labels are fed into embedding layer to generate phoneme vectors that are then passed through bidirectional long short-term memory (BLSTM) recurrent neural network to predict acoustic features. Weights in embedding layer are updated through backpropagation during training. Analyses indicate that phonemes with similar acoustic pronunciations are close to each other in cosine distance in the generated phoneme vector space, and tend to be in the same category after k-means clustering. We evaluate the phoneme embedding by applying the generated phoneme vector into speech driven talking avatar synthesis. Experimental results indicate that adding phoneme vector as features can achieve 10.2% relative improvement in objective test",
    "checked": true,
    "id": "f4ada5d1439c801c719d47b9bcc81dd970d48b25",
    "semantic_title": "phoneme embedding and its application to speech driven talking avatar synthesis",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16e_interspeech.html": {
    "title": "Expressive Speech Driven Talking Avatar Synthesis with DBLSTM Using Limited Amount of Emotional Bimodal Data",
    "volume": "main",
    "abstract": "One of the essential problems in synthesizing expressive talking avatar is how to model the interactions between emotional facial expressions and lip movements. Traditional methods either simplify such interactions through separately modeling lip movements and facial expressions, or require substantial high quality emotional audio-visual bimodal training data which are usually difficult to collect. This paper proposes several methods to explore different possibilities in capturing the interactions using a large-scale neutral corpus in addition to a small size emotional corpus with limited amount of data. To incorporate contextual influences, deep bidirectional long short-term memory (DBLSTM) recurrent neural network is adopted as the regression model to predict facial features from acoustic features, emotional states as well as contexts. Experimental results indicate that the method by concatenating neutral facial features with emotional acoustic features as the input of DBLSTM model achieves the best performance in both objective and subjective evaluations",
    "checked": true,
    "id": "be5ff8bde77fb369cbed19dcc436da9ff1f0ed1f",
    "semantic_title": "expressive speech driven talking avatar synthesis with dblstm using limited amount of emotional bimodal data",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2016/taylor16_interspeech.html": {
    "title": "Audio-to-Visual Speech Conversion Using Deep Neural Networks",
    "volume": "main",
    "abstract": "We study the problem of mapping from acoustic to visual speech with the goal of generating accurate, perceptually natural speech animation automatically from an audio speech signal. We present a sliding window deep neural network that learns a mapping from a window of acoustic features to a window of visual features from a large audio-visual speech dataset. Overlapping visual predictions are averaged to generate continuous, smoothly varying speech animation. We outperform a baseline HMM inversion approach in both objective and subjective evaluations and perform a thorough analysis of our results",
    "checked": true,
    "id": "1ba384ef80ab9295162833ed2658aac119fe9cc3",
    "semantic_title": "audio-to-visual speech conversion using deep neural networks",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nakashika16_interspeech.html": {
    "title": "Generative Acoustic-Phonemic-Speaker Model Based on Three-Way Restricted Boltzmann Machine",
    "volume": "main",
    "abstract": "In this paper, we argue the way of modeling speech signals based on three-way restricted Boltzmann machine (3WRBM) for separating phonetic-related information and speaker-related information from an observed signal automatically. The proposed model is an energy-based probabilistic model that includes three-way potentials of three variables: acoustic features, latent phonetic features, and speaker-identity features. We train the model so that it automatically captures the undirected relationships among the three variables. Once the model is trained, it can be applied to many tasks in speech signal processing. For example, given a speech signal, estimating speaker-identity features is equivalent to speaker recognition; on the other hand, estimated latent phonetic features may be helpful for speech recognition because they contain more phonetic-related information than the acoustic features. Since the model is generative, we can also apply it to voice conversion; i.e., we just estimate acoustic features from the phonetic features that were estimated given the source speakers acoustic features along with the desired speaker-identity features. In our experiments, we discuss the effectiveness of the speech modeling through a speaker recognition, a speech (continuous phone) recognition, and a voice conversion tasks",
    "checked": true,
    "id": "0183fb21376dbc8520c1841adcb40bf1fdcca60c",
    "semantic_title": "generative acoustic-phonemic-speaker model based on three-way restricted boltzmann machine",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/toutios16_interspeech.html": {
    "title": "Articulatory Synthesis Based on Real-Time Magnetic Resonance Imaging Data",
    "volume": "main",
    "abstract": "This paper presents a methodology for articulatory synthesis of running speech in American English driven by real-time magnetic resonance imaging (rtMRI) mid-sagittal vocal-tract data. At the core of the methodology is a time-domain simulation of the propagation of sound in the vocal tract developed previously by Maeda. The first step of the methodology is the automatic derivation of air-tissue boundaries from the rtMRI data. These articulatory outlines are then modified in a systematic way in order to introduce additional precision in the formation of consonantal vocal-tract constrictions. Other elements of the methodology include a previously reported set of empirical rules for setting the time-varying characteristics of the glottis and the velopharyngeal port, and a revised sagittal-to-area conversion. Results are promising towards the development of a full-fledged text-to-speech synthesis system leveraging directly observed vocal-tract dynamics",
    "checked": true,
    "id": "280c5aad7dee9afc39309202fc0754cd432e8aaa",
    "semantic_title": "articulatory synthesis based on real-time magnetic resonance imaging data",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xie16c_interspeech.html": {
    "title": "Deep Neural Network Based Acoustic-to-Articulatory Inversion Using Phone Sequence Information",
    "volume": "main",
    "abstract": "In recent years, neural network based acoustic-to-articulatory inversion approaches have achieved the state-of-the-art performance. One major issue associated with these approaches is the lack of phone sequence information during inversion. In order to address this issue, this paper proposes an improved architecture hierarchically concatenating phone classification and articulatory inversion component DNNs to improve articulatory movement generation. On a Mandarin Chinese speech inversion task, the proposed technique consistently outperformed a range of baseline DNN and RNN inversion systems constructed using no phone sequence information, a mixture density parameter output layer, additional phone features at the input layer, or multi-task learning with additional monophone output layer target labels, measured in terms of electromagnetic articulography (EMA) root mean square error (RMSE) and correlation. Further improvements were obtained using the bottleneck features extracted from the proposed hierarchical articulatory inversion systems as auxiliary features in generalized variable parameter HMMs (GVP-HMMs) based inversion systems",
    "checked": true,
    "id": "9f998c57dc9b62d0ead627d50bf35375942e4d76",
    "semantic_title": "deep neural network based acoustic-to-articulatory inversion using phone sequence information",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16g_interspeech.html": {
    "title": "Articulatory-to-Acoustic Conversion with Cascaded Prediction of Spectral and Excitation Features Using Neural Networks",
    "volume": "main",
    "abstract": "This paper presents an articulatory-to-acoustic conversion method using electromagnetic midsagittal articulography (EMA) measurements as input features. Neural networks, including feed-forward deep neural networks (DNNs) and recurrent neural networks (RNNs) with long short-term term memory (LSTM) cells, are adopted to map EMA features towards not only spectral features (i.e. mel-cepstra) but also excitation features (i.e. power, U/V flag and F0). Then speech waveforms are reconstructed using the predicted spectral and excitation features. A cascaded prediction strategy is proposed to utilize the predicted spectral features as auxiliary input to boost the prediction of excitation features. Experimental results show that LSTM-RNN models can achieve better objective and subjective performance in articulatory-to-spectral conversion than DNNs and Gaussian mixture models (GMMs). The strategy of cascaded prediction can increase the accuracy of excitation feature prediction and the neural network-based methods also outperform the GMM-based approach when predicting power features",
    "checked": true,
    "id": "00efbbc70a8688dae5a4d2c54ada8f584eb5974e",
    "semantic_title": "articulatory-to-acoustic conversion with cascaded prediction of spectral and excitation features using neural networks",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liberatore16_interspeech.html": {
    "title": "Generating Gestural Scores from Acoustics Through a Sparse Anchor-Based Representation of Speech",
    "volume": "main",
    "abstract": "We present a procedure for generating gestural scores from speech acoustics. The procedure is based on our recent SABR (sparse, anchor-based representation) algorithm, which models the speech signal as a linear combination of acoustic anchors. We present modifications to SABR that encourage temporal smoothness by restricting the number of anchors that can be active over an analysis window. We propose that peaks in the SABR weights can be interpreted as \"keyframes\" that determine when vocal tract articulations occur. We validate the approach in two ways. First, we compare SABR keyframes to maxima in the velocity of electromagnetic articulography (EMA) pellets from an articulatory corpus. Second, we use keyframes and SABR weights to build a gestural score for the VocalTractLab (VTL) model, and compare synthetic EMA trajectories generated by VTL against those in the articulatory corpus. We find that SABR keyframes occur within 15–20 ms (on average) of EMA maxima, suggesting that SABR keyframes can be used to identify articulatory phenomena. However, comparison of synthetic and real EMA pellets show moderate correlation on tongue pellets but weak correlation on lip pellets, a result that may be due to differences between the VTL speaker model and the source speaker in our corpus",
    "checked": true,
    "id": "af0d37178fe7c7530d453abfbebb1e942a407982",
    "semantic_title": "generating gestural scores from acoustics through a sparse anchor-based representation of speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/guennec16_interspeech.html": {
    "title": "On the Suitability of Vocalic Sandwiches in a Corpus-Based TTS Engine",
    "volume": "main",
    "abstract": "Unit selection speech synthesis systems generally rely on target and concatenation costs for selecting the best unit sequence. The role of the concatenation cost is to insure that joining two voice segments will not cause any acoustic artefact to appear. For this task, acoustic distances (MFCC, F ) are typically used but in many cases, this is not enough to prevent concatenation artefacts. Among other strategies, the improvement of corpus covering by favoring units that naturally support well the joining process (vocalic sandwiches) seems to be effective on TTS. In this paper, we investigate if vocalic sandwiches can be used directly in the unit selection engine when the corpus was not created using that principle. First, the sandwich approach is directly transposed in the unit selection engine with a penalty that greatly favors concatenation on sandwich boundaries. Second, a derived fuzzy version is proposed to relax the penalty based on the concatenation cost, with respect to the cost distribution. We show that the sandwich approach, very efficient at the corpus creation step, seems to be inefficient when directly transposed in the unit selection engine. However, we observe that the fuzzy approach enhances synthesis quality, especially on sentences with high concatenation costs",
    "checked": true,
    "id": "c59135551e86277174d6cc3bd37f76dab9ac87e7",
    "semantic_title": "on the suitability of vocalic sandwiches in a corpus-based tts engine",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/moungsri16_interspeech.html": {
    "title": "Unsupervised Stress Information Labeling Using Gaussian Process Latent Variable Model for Statistical Speech Synthesis",
    "volume": "main",
    "abstract": "In Thai language, stress is an important prosodic feature that not only affects naturalness but also has a crucial role in meaning of phrase-level utterance. It is seen that a speech synthesis model that is trained with lack of stress and phrase-level information causes incorrect tones and ambiguity in meaning of synthetic speech. Our previous work has shown that manually annotated stress information improves naturalness of synthetic speech. However, a high time consumption is a drawback of the manual annotation. In this paper, we utilize an unsupervised learning technique called Bayesian Gaussian process latent variable model (Bayesian GP-LVM) to automatically put stress annotation on the given training data. Stress related features are projected onto a latent space in which syllables are easier classified into stressed/unstressed classes. We use the stressed/unstressed information as an additional context in GPR-based speech synthesis. Experimental results show that the proposed technique improves naturalness of synthetic speech as well as accuracy of stressed/unstressed classification. Moreover, the proposed technique enables us to avoid ambiguity in meaning of synthetic speech by providing intended stress position into context label sequence to be synthesized",
    "checked": true,
    "id": "27b4674003f9d9ba387e3adf3421181143a676ba",
    "semantic_title": "unsupervised stress information labeling using gaussian process latent variable model for statistical speech synthesis",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ni16_interspeech.html": {
    "title": "Using Zero-Frequency Resonator to Extract Multilingual Intonation Structure",
    "volume": "main",
    "abstract": "Human uses expressive intonation to convey linguistic and paralinguistic meaning, especially making focal prominence to give emphasis that highlights the focus of speech. Automatic extraction of dynamic intonation feature from a speech corpus and representing it in a continuous form are desired in multilingual speech synthesis. This paper presents a method to extract dynamic prosodic structure from speech signal using zero-frequency resonator to detect glottal cycle epoch and filter both voice amplitude and fundamental frequency (F0) contours. We choose stable voice F0 segments free from micro-prosodic effect to recover relevant F0 trajectory of an utterance, taking into consideration of inter-correlation of micro-prosody with phonetic segments and syllable structure of the utterance, and further filter out long-term global pitch movements. The method is evaluated by objective tests upon multilingual speech corpora including Chinese, Japanese, Korean, and Myanmar. Our experiment results show that the extracted intonation contour can match F0 contour by conventional approach in very high accuracy and the estimated long-term pitch movements demonstrate regular characteristics of intonation across languages. The proposed method is language-independent and robust to noisy speech",
    "checked": true,
    "id": "6d08184142e72969e55aae4c30af3b0039b43f14",
    "semantic_title": "using zero-frequency resonator to extract multilingual intonation structure",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yu16b_interspeech.html": {
    "title": "A DNN-HMM Approach to Story Segmentation",
    "volume": "main",
    "abstract": "Hidden Markov model (HMM) is one of the popular techniques for story segmentation, where hidden Markov states represent the topics, and the emission distributions of n-gram language model (LM) are dependent on the states. Given a text document, a Viterbi decoder finds the hidden story sequence, with a change of topic indicating a story boundary. In this paper, we propose a discriminative approach to story boundary detection. In the HMM framework, we use deep neural network (DNN) to estimate the posterior probability of topics given the bag-of-words in the local context. We call it the DNN-HMM approach. We consider the topic dependent LM as a generative modeling technique, and the DNN-HMM as the discriminative solution. Experiments on topic detection and tracking (TDT2) task show that DNN-HMM outperforms traditional n-gram LM approach significantly and achieves state-of-the-art performance",
    "checked": true,
    "id": "f96fd556db336899daa923c905710ee9a20cb397",
    "semantic_title": "a dnn-hmm approach to story segmentation",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2016/goldman16_interspeech.html": {
    "title": "The SIWIS Database: A Multilingual Speech Database with Acted Emphasis",
    "volume": "main",
    "abstract": "We describe here a collection of speech data of bilingual and trilingual speakers of English, French, German and Italian. In the context of speech to speech translation (S2ST), this database is designed for several purposes and studies: training CLSA systems (cross-language speaker adaptation), conveying emphasis through S2ST systems, and evaluating TTS systems. More precisely, 36 speakers judged as accentless (22 bilingual and 14 trilingual speakers) were recorded for a set of 171 prompts in two or three languages, amounting to a total of 24 hours of speech. These sets of prompts include 100 sentences from news, 25 sentences from Europarl, the same 25 sentences with one acted emphasised word, 20 semantically unpredictable sentences, and finally a 240-word long text. All in all, it yielded 64 bilingual session pairs of the six possible combinations of the four languages. The database is freely available for non-commercial use and scientific research purposes",
    "checked": true,
    "id": "d5199d27912744099fcc0df19451bdf151024e63",
    "semantic_title": "the siwis database: a multilingual speech database with acted emphasis",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ylmaz16b_interspeech.html": {
    "title": "Open Source Speech and Language Resources for Frisian",
    "volume": "main",
    "abstract": "In this paper, we present several open source speech and language resources for the under-resourced Frisian language. Frisian is mostly spoken in the province of Fryslân which is located in the north of the Netherlands. The native speakers of Frisian are Frisian-Dutch bilingual and often code-switch in daily conversations. The resources presented in this paper include a code-switching speech database containing radio broadcasts, a phonetic lexicon with more than 70k words and a language model trained on a text corpus with more than 38M words. With this contribution, we aim to share the Frisian resources we have collected in the scope of the FAME! project, in which a spoken document retrieval system is built for the disclosure of the regional broadcaster's radio archives. These resources enable research on code-switching and longitudinal speech and language change. Moreover, a sample automatic speech recognition (ASR) recipe for the Kaldi toolkit will also be provided online to facilitate the Frisian ASR research",
    "checked": true,
    "id": "69cccccedfd08f02262b75a5ba746e512b106a48",
    "semantic_title": "open source speech and language resources for frisian",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kathol16_interspeech.html": {
    "title": "The SRI CLEO Speaker-State Corpus",
    "volume": "main",
    "abstract": "We introduce the SRI CLEO (Conversational Language about Everyday Objects) Speaker-State Corpus of speech, video, and biosignals. The goal of the corpus is providing insight on the speech and physiological changes resulting from subtle, context-based influences on affect and cognition. Speakers were prompted by collections of pictures of neutral everyday objects and were instructed to provide speech related to any subset of the objects for a preset period of time (120 or 180 seconds depending on task) The corpus provides signals for 43 speakers under four different speaker-state conditions: (1) neutral and emotionally charged audiovisual background; (2) cognitive load; (3) time pressure; and (4) various acted emotions. Unlike previous studies that have linked speaker state to the content of the speaking task itself, the CLEO prompts remain largely pragmatically, semantically, and affectively neutral across all conditions. This framework enables for more direct comparisons across both conditions and speakers. The corpus also includes more traditional speaker tasks involving reading and free-form reporting of neutral and emotionally charged content. The explored biosignals include skin conductance, respiration, blood pressure, and ECG. The corpus is in the final stages of processing and will be made available to the research community",
    "checked": true,
    "id": "0353dafdce8e031957fbc305f282b71c8e5536e3",
    "semantic_title": "the sri cleo speaker-state corpus",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16g_interspeech.html": {
    "title": "SingaKids-Mandarin: Speech Corpus of Singaporean Children Speaking Mandarin Chinese",
    "volume": "main",
    "abstract": "We present SingaKids-Mandarin, a speech corpus of 255 Singaporean children aged 7 to 12 reading Mandarin Chinese, for a total of 125 hours of data (75 hours of speech) and 79,843 utterances. This corpus is phonetically balanced and detailed in human annotations, including phonetic transcriptions, lexical tone markings, and proficiency scoring at the utterance level. The reading scripts span a diverse set of utterance styles, covering syllable-level minimal pairs, words, phrases, sentences, and short stories. We analyze the acoustic properties of Singaporean children. We also observe that while the lack of the neutral tone is the same for Singaporean adults and children, the phonetic pronunciation patterns in these two age groups differ: although Singaporean adults tend to front their retroflex, nasal, and palatal consonants, Singaporean children show both fronting and backing in these consonants. For future work, we plan to develop computer-assisted pronunciation training (CAPT) systems with SingaKids-Mandarin",
    "checked": true,
    "id": "8491d159467ef495049a9792592124537787ed0a",
    "semantic_title": "singakids-mandarin: speech corpus of singaporean children speaking mandarin chinese",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2016/richey16_interspeech.html": {
    "title": "The SRI Speech-Based Collaborative Learning Corpus",
    "volume": "main",
    "abstract": "We introduce the SRI speech-based collaborative learning corpus, a novel collection designed for the investigation and measurement of how students collaborate together in small groups. This is a multi-speaker corpus containing high-quality audio recordings of middle school students working in groups of three to solve mathematical problems. Each student was recorded via a head-mounted noise-cancelling microphone. Each group was also recorded via a stereo microphone placed nearby. A total of 80 sessions were collected with the participation of 134 students. The average duration of a session was 20 minutes. All students spoke English; for some students, English was a second language. Sessions have been annotated with time stamps to indicate which mathematical problem the students were solving and which student was speaking. Sessions have also been hand annotated with common indicators of collaboration for each speaker (e.g., inviting others to contribute, planning) and the overall collaboration quality for each problem. The corpus will be useful to education researchers interested in collaborative learning and to speech researchers interested in children's speech, speech analytics, and speech diarization. The corpus, both audio and annotation, will be made available to researchers",
    "checked": true,
    "id": "c11d4a039e18e1c4e87b019896f47e5b3a2ec8c9",
    "semantic_title": "the sri speech-based collaborative learning corpus",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ramakrishna16_interspeech.html": {
    "title": "An Expectation Maximization Approach to Joint Modeling of Multidimensional Ratings Derived from Multiple Annotators",
    "volume": "main",
    "abstract": "Ratings from multiple human annotators are often pooled in applications where the ground truth is hidden. Examples include annotating perceived emotions and assessing quality metrics for speech and image. These ratings are not restricted to a single dimension and can be multidimensional. In this paper, we propose an Expectation-Maximization based algorithm to model such ratings. Our model assumes that there exists a latent multidimensional ground truth that can be determined from the observation features and that the ratings provided by the annotators are noisy versions of the ground truth. We test our model on a study conducted on children with autism to predict a four dimensional rating of expressivity, naturalness, pronunciation goodness and engagement. Our goal in this application is to reliably predict the individual annotator ratings which can be used to address issues of cognitive load on the annotators as well as the rating cost. We initially train a baseline directly predicting annotator ratings from the features and compare it to our model under three different settings assuming: (i) each entry in the multidimensional rating is independent of others, (ii) a joint distribution among rating dimensions exists, (iii) a partial set of ratings to predict the remaining entries is available",
    "checked": true,
    "id": "8f3ca94b5cf5e0ac3812463052fcdcd3245f6a49",
    "semantic_title": "an expectation maximization approach to joint modeling of multidimensional ratings derived from multiple annotators",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2016/matousek16_interspeech.html": {
    "title": "Voting Detector: A Combination of Anomaly Detectors to Reveal Annotation Errors in TTS Corpora",
    "volume": "main",
    "abstract": "Anomaly detection techniques were shown to help in detecting word-level annotation errors in read-speech corpora for text-to-speech synthesis. In this framework, correctly annotated words are considered as normal examples on which the detection methods are trained. Misannotated words are then taken as anomalous examples which do not conform to normal patterns of the trained detection models. In this paper we propose a concept of a voting detector — a combination of anomaly detectors in which each \"single\" detector \"votes\" on whether a testing word is annotated correctly or not. The final decision is then made by aggregating the votes. Our experiments show that voting detector has a potential to overcome each of the single anomaly detectors",
    "checked": true,
    "id": "b0348a26d4205afbe87d7a3b46b0dbe3776e4422",
    "semantic_title": "voting detector: a combination of anomaly detectors to reveal annotation errors in tts corpora",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/corralesastorgano16_interspeech.html": {
    "title": "The Magic Stone: A Video Game to Improve Communication Skills of People with Intellectual Disabilities",
    "volume": "main",
    "abstract": "The Magic Stone\" is a video game whose main aim is to help people with Down syndrome to improve communication skills that have been affected due to their disability, especially those related with prosody. The interface of the video game includes a number of elements to motivate the users to practice and train their pronunciation. The usability tests of the system have reported high degrees of satisfaction of users and trainers. Perception tests have permitted to confirm that players improve the use of prosody with the use",
    "checked": true,
    "id": "c094a053110af3fa7b83aaee627f92df7a0e31e1",
    "semantic_title": "the magic stone: a video game to improve communication skills of people with intellectual disabilities",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kelly16_interspeech.html": {
    "title": "Identifying Perceptually Similar Voices with a Speaker Recognition System Using Auto-Phonetic Features",
    "volume": "main",
    "abstract": "Assessing the perceptual similarity of voices is necessary for the creation of voice parades, along with media applications such as voice casting. These applications are normally prohibitively expensive to administer, requiring significant amounts of ‘expert listening'. The ability to automatically assess voice similarity could benefit these applications by increasing efficiency and reducing subjectivity, while enabling the use of a much larger search space of candidate voices. In this paper, the use of automatically extracted phonetic features within an i-vector speaker recognition system is proposed as a means of identifying cohorts of perceptually similar voices. Features considered include formants (F1-F4), fundamental frequency (F0), semitones of F0, and their derivatives. To demonstrate the viability of this approach, a subset of the Interspeech 2016 special session ‘Speakers In The Wild' (SITW) dataset is used in a pilot study comparing subjective listener ratings of similarity with the output of the automatic system. It is observed that the automatic system can locate cohorts of male voices with good perceptual similarity. In addition to these experiments, this proposal will be demonstrated with an application allowing a user to retrieve voices perceptually similar to their own from a large dataset",
    "checked": true,
    "id": "060f2dd15adc27e9bc171d1ef713c11b77f945ce",
    "semantic_title": "identifying perceptually similar voices with a speaker recognition system using auto-phonetic features",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/james16_interspeech.html": {
    "title": "A Real-Time Framework for Visual Feedback of Articulatory Data Using Statistical Shape Models",
    "volume": "main",
    "abstract": "We present a novel open-source framework for visualizing electromagnetic articulography (EMA) data in real-time, with a modular framework and anatomically accurate tongue and palate models derived by multilinear subspace learning",
    "checked": true,
    "id": "a3786fe66164067abd36d3a30b03cc44aa3532ec",
    "semantic_title": "a real-time framework for visual feedback of articulatory data using statistical shape models",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/marin16_interspeech.html": {
    "title": "Flexible, Rapid Authoring of Goal-Orientated, Multi-Turn Dialogues Using the Task Completion Platform",
    "volume": "main",
    "abstract": "The Task Completion Platform (TCP) is a multi-domain, multi-modal dialogue system that can host and execute large numbers of goal-orientated dialogue tasks. TCP is comprised of a task configuration language, TaskForm, and a task-independent dialogue runtime, allowing task definitions to be decoupled from the global dialogue policy used by the platform to execute the tasks. This separation enables scenario developers to rapidly develop new dialogue systems, by eliminating the need to re-implement the policy from scratch for each new task. In this paper, we introduce support for authoring tasks in a variety of dialogue styles, ranging from entirely flexible to fully system-initiative. This flexibility is enabled by a set of task-level policy override constructs, which augment or constrain the default platform-level policy to achieve the desired system behavior. We demonstrate the use of the TaskForm language to define complex, multi-turn tasks in a variety of domains and add different task-specific policy constructs to demonstrate the flexibility of the task authoring process",
    "checked": true,
    "id": "6ec9b4c25a5c497a8b07182b266ab1e012c0d035",
    "semantic_title": "flexible, rapid authoring of goal-orientated, multi-turn dialogues using the task completion platform",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/delcroix16_interspeech.html": {
    "title": "Context Adaptive Neural Network for Rapid Adaptation of Deep CNN Based Acoustic Models",
    "volume": "main",
    "abstract": "Using auxiliary input features has been seen as one of the most effective ways to adapt deep neural network (DNN)-based acoustic models to speaker or environment. However, this approach has several limitations. It only performs compensation of the bias term of the hidden layer and therefore does not fully exploit the network capabilities. Moreover, it may not be well suited for certain types of architectures such as convolutional neural networks (CNNs) because the auxiliary features have different time-frequency structures from speech features. This paper resolves these problems by extending the recently proposed context adaptive DNN (CA-DNN) framework to CNN architectures. A CA-DNN is a DNN with one or several layers factorized in sub-layers associated with an acoustic context class representing speaker or environment. The output of the factorized layer is obtained as the weighted sum of the contributions of each sub-layer, weighted by acoustic context weights that are derived from auxiliary features such as i-vectors. Importantly, a CA-DNN can compensate both bias and weight matrices. In this paper, we investigate the use of CA-DNN for deep CNN-based architectures. We demonstrate consistent performance gains for utterance level rapid adaptation on the AURORA4 task over a strong network-in-network based deep CNN architecture",
    "checked": true,
    "id": "fe63023bb7434e44b4e63dcf918c6a2f9aaba930",
    "semantic_title": "context adaptive neural network for rapid adaptation of deep cnn based acoustic models",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lim16_interspeech.html": {
    "title": "Transfer Learning with Bottleneck Feature Networks for Whispered Speech Recognition",
    "volume": "main",
    "abstract": "Previous work on whispered speech recognition has shown that acoustic models (AM) trained on whispered speech can somewhat classify unwhispered (neutral) speech sounds, but not vice versa. In fact, AMs trained purely on neutral speech completely fail to recognize whispered speech. Meanwhile, recipes used to train neutral AMs will work just as well for whispered speech, but such methods require a large volume of transcribed whispered speech which is expensive to gather. In this work, we propose and investigate the use of bottleneck feature networks to normalize differences between whispered and neutral speech modes. Our extensive experiments show that this type of speech variability can be effectively normalized. We also show that it is possible to transfer this knowledge from two source languages with whispered speech (Mandarin and English), to a new target language (Malay) without whispered speech. Furthermore, we report a substantial reduction in word error rate for cross-mode speech recognition, effectively demonstrate that it is possible to train acoustic models capable of classifying both types of speech without needing any additional whispered speech",
    "checked": true,
    "id": "4c617595d688b09500e10062021a74cbf0d4c810",
    "semantic_title": "transfer learning with bottleneck feature networks for whispered speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nagamine16b_interspeech.html": {
    "title": "Adaptation of Neural Networks Constrained by Prior Statistics of Node Co-Activations",
    "volume": "main",
    "abstract": "We propose a novel unsupervised model adaptation framework in which a neural network uses prior knowledge of the statistics of its output and hidden layer activations to update its parameters online to improve performance in mismatched environments. This idea is inspired by biological neural networks, which use feedback to dynamically adapt their computation when faced with unexpected inputs. Here, we introduce an adaptation criterion for deep neural networks based on the observation that in matched testing and training conditions, the node co-activation statistics of each layer in a neural network are relatively stable over time. The proposed method thus adapts the model layer by layer to minimize the distance between the co-activation statistics of nodes in matched versus mismatched conditions. In phoneme classification experiments, we show that such node co-activation constrained adaptation in a deep neural network model significantly improves the recognition accuracy over baseline performance when the system is tested in various novel noises not included in the training",
    "checked": true,
    "id": "7a256dffe36b6ced0b1fdbdbd94dc733a123ba5b",
    "semantic_title": "adaptation of neural networks constrained by prior statistics of node co-activations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/suzuki16_interspeech.html": {
    "title": "Domain Adaptation of CNN Based Acoustic Models Under Limited Resource Settings",
    "volume": "main",
    "abstract": "Adaptation of Automatic Speech Recognition (ASR) systems to a new domain (channel, speaker, topic, etc.) remains a significant challenge, as often, only a limited amount of target domain data for adaptation of Acoustic Models (AMs) is available. However, unlike GMMs, to date, there has not been an established, efficient method for adapting current state-of-the-art Convolutional Neural Network (CNN)-based AMs. In this paper, we explore various training algorithms for domain adaptation of CNN based speech recognition systems with limited acoustic training data resources. Our investigations illustrate the following three main contributions. First, introducing a weight decay based regularizer along with the standard cross entropy criteria can significantly improve recognition performances with as little as one hour of adaptation data. Second, the observed gains can be improved further with the state-level Minimum Bayes Risk (sMBR) based sequence training technique. In addition to supervised training with limited amounts of data, we also study the effect of introducing unsupervised data at both the initial cross-entropy and subsequent sequence training stages. Our experiments show that unsupervised data helps with cross-entropy and sequence training criteria. Third, the effect of speaker diversity in the adaptation data is also investigated where our experiments show that although there can be large variance in final performance depending on the speakers selected, regularization is required to obtain significant gains. Overall, we demonstrate that with adaptation of neural network based acoustic models, we can obtain performance improvements of up to 24.8% relative",
    "checked": true,
    "id": "3e43f98b8ac8a2c4a1471d2322bb0d595aa317ff",
    "semantic_title": "domain adaptation of cnn based acoustic models under limited resource settings",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/samarakoon16_interspeech.html": {
    "title": "Subspace LHUC for Fast Adaptation of Deep Neural Network Acoustic Models",
    "volume": "main",
    "abstract": "Recently, the learning hidden unit contributions (LHUC) method is proposed for the adaptation of deep neural network (DNN) based acoustic models for automatic speech recognition (ASR). In LHUC, a set of speaker dependent (SD) parameters is estimated to linearly recombine the hidden units in an unsupervised fashion. Although LHUC performs considerably well, the gains diminish when the availability of the adaptation data amount decreases. Moreover, the per-speaker footprint of LHUC adaptation is in thousands and it is not desirable. Therefore, in this work, we propose the subspace LHUC, where the SD parameters are estimated in a subspace and connected to various layers through a new set of adaptively trained weights. We evaluate the subspace LHUC in the Aurora4 and AMI IHM tasks. Experimental results show that the subspace LHUC outperforms standard LHUC adaptation. With utterance-level fast adaptation, the subspace LHUC achieved 11.3% and 4.5% relative improvements over the standard LHUC for the Aurora4 and AMI IHM tasks respectively. Furthermore, the subspace LHUC reduces the per-speaker footprint by 94% over the standard LHUC adaptation",
    "checked": true,
    "id": "41773a9d72c9fab27566d1bfb10c3fbef00155f4",
    "semantic_title": "subspace lhuc for fast adaptation of deep neural network acoustic models",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fainberg16_interspeech.html": {
    "title": "Improving Children's Speech Recognition Through Out-of-Domain Data Augmentation",
    "volume": "main",
    "abstract": "Children's speech poses challenges to speech recognition due to strong age-dependent anatomical variations and a lack of large, publicly-available corpora. In this paper we explore data augmentation for children's speech recognition using stochastic feature mapping (SFM) to transform out-of-domain adult data for both GMM-based and DNN-based acoustic models. We performed experiments on the English PF-STAR corpus, augmenting using WSJCAM0 and ABI. Our experimental results indicate that a DNN acoustic model for childrens speech can make use of adult data, and that out-of-domain SFM is more accurate than in-domain SFM",
    "checked": true,
    "id": "a2c957d4fa7af27ca078287d06e7d08f401b3de5",
    "semantic_title": "improving children's speech recognition through out-of-domain data augmentation",
    "citation_count": 51
  },
  "https://www.isca-speech.org/archive/interspeech_2016/metze16_interspeech.html": {
    "title": "Virtual Machines and Containers as a Platform for Experimentation",
    "volume": "main",
    "abstract": "Research on computational speech processing has traditionally relied on the availability of a relatively large and complex infrastructure, which encompasses data (text and audio), tools (feature extraction, model training, scoring, possibly on-line and off-line, etc.), glue code, and computing. Traditionally, it has been very hard to move experiments from one site to another, and to replicate experiments. With the increasing availability of shared platforms such as commercial cloud computing platforms or publicly funded super-computing centers, there is a need and an opportunity to abstract the experimental environment from the hardware, and distribute complete setups as a virtual machine, a container, or some other shareable resource, that can be deployed and worked with anywhere In this paper, we discuss our experience with this concept and present some tools that the community might find useful. We outline, as a case study, how such tools can be applied to a naturalistic language acquisition audio corpus",
    "checked": true,
    "id": "2e4a265d1ab50f271356978c67d06a17fd5b322e",
    "semantic_title": "virtual machines and containers as a platform for experimentation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/green16_interspeech.html": {
    "title": "CloudCAST — Remote Speech Technology for Speech Professionals",
    "volume": "main",
    "abstract": "Recent advances in speech technology are potentially of great benefit to the professionals who help people with speech problems: therapists, pathologists, educators and clinicians. There are 3 obstacles to progress which we seek to address in the CloudCAST project: • the design of applications deploying the technology should be user-driven; • the computing resource should be available remotely; • the software should be capable of personalisation: clinical applications demand individual solutions CloudCAST aims to provide such a resource, and in addition to gather the data produced as the applications are used, to underpin the machine learning required for further progress",
    "checked": false,
    "id": "7d990336cdc691e451402aae5a8bdb9c3a08c7e8",
    "semantic_title": "cloudcast - remote speech technology for speech professionals",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hain16_interspeech.html": {
    "title": "webASR 2 — Improved Cloud Based Speech Technology",
    "volume": "main",
    "abstract": "This paper presents the most recent developments of the webASR service (www.webasr.org), the world's first web-based fully functioning automatic speech recognition platform for scientific use. Initially released in 2008, the functionalities of webASR have recently been expanded with 3 main goals in mind: Facilitate access through a RESTful architecture, that allows for easy use through either the web interface or an API; allow the use of input metadata when available by the user to improve system performance; and increase the coverage of available systems beyond speech recognition. Several new systems for transcription, diarisation, lightly supervised alignment and translation are currently available through webASR. The results in a series of well-known benchmarks (RT'09, IWSLT'12 and MGB'15 evaluations) show how these webASR systems provides state-of-the-art performances across these tasks",
    "checked": false,
    "id": "46cccfb1a609dea47c1554e4ba81abbd67c1d246",
    "semantic_title": "webasr 2 - improved cloud based speech technology",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/plummer16_interspeech.html": {
    "title": "Sharing Speech Synthesis Software for Research and Education Within Low-Tech and Low-Resource Communities",
    "volume": "main",
    "abstract": "Parametric speech synthesis has played an integral role in speech research since the 1950s. However, software sharing is unwieldy, making replication of experiments difficult, creating obstacles to communication between laboratories, and hindering entry into research. This paper describes our use of the Speech Recognition Virtual Kitchen environment (www.speechkitchen.org) to develop an infrastructure for sharing synthesis software for research and education. We tested the infrastructure by using it in teaching a seminar on \"the speech science of speech synthesis\" to students from several of the graduate programs in linguistics at the Ohio State University. Using the virtual machines that we developed for Klatt's formant synthesis program and Kawahara's STRAIGHT speech analysis, modification, and synthesis system enabled the students to advance much further in their understanding of the basic principles underlying these acoustic-domain models by comparison to the students enrolled in a similar seminar that we taught previously without the virtual machines. At the same time, implementing these and two other virtual machines for the course did not live up to our expectations for the course, in ways that highlight the need to adapt both the Speech Kitchen environment and the synthesis software systems to the needs of low-tech, low-resource users",
    "checked": true,
    "id": "330a31045c076dd426a44b14c30a297c3fc03beb",
    "semantic_title": "sharing speech synthesis software for research and education within low-tech and low-resource communities",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sprouse16_interspeech.html": {
    "title": "The Berkeley Phonetics Machine",
    "volume": "main",
    "abstract": "The Berkeley Phonetics Machine is a Linux virtual machine image produced and used by the UC Berkeley Phonology Lab as a platform for phonetic research. It contains a full data analysis stack based on Python and R and also specialized tools for phonetic research. The machine is designed as a flexible and productive platform for established and novel research agendas that can be easily shared and reproduced. We list the software available in the machine, which includes many command-line tools for acoustic analysis and media file manipulation, as well as specialized Python libraries. We also discuss the use of this machine in the Phonology Lab and in phonetics courses. The overall experience with the machine has been positive, as faculty and graduate students are able to share and execute scripts in a common working environment. Undergraduate students have less opportunity to master the virtual machine environment but benefit from simplified instructions and fewer installation and operating problems. The primary difficulty that we have encountered has been with a few underpowered student computers that cannot run the virtual machine or do not run it well",
    "checked": true,
    "id": "4bf201e62e16fb70c8a4b7719a983555c782a8b2",
    "semantic_title": "the berkeley phonetics machine",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bates16_interspeech.html": {
    "title": "Experiences with Shared Resources for Research and Education in Speech and Language Processing",
    "volume": "main",
    "abstract": "Resource barriers can prevent capable researchers from participating in the speech and language community and can make it difficult to support learning and participation in our field at a wide variety of institutions. Sharing resources, whether software, processed data, experimental methodologies or virtual machines, can reduce the barrier to entry and potentially broaden participation in speech and language research and improve workforce development. As an introduction to the special session on Sharing Research and Education Resources for Understanding Speech Processing, we outline current trends and requirements for expanding participation in speech processing research. A qualitative research approach was used. Faculty at a variety of institutions have been interviewed and have participated in reflection writing about needs, tools, challenges, and successes. Themes from reflections were generated using a grounded theory approach and were used to code interviews for related evidence. This paper describes the educational and research challenges experienced by faculty as users of resources, rather than the details of specific resources provided. The goal is to engage in a stronger dialog between users and providers so that needs and resources are better aligned. A case study of a shared resource used at several universities highlights this dialog",
    "checked": true,
    "id": "9450610d15d9b20bf120715ee6afbfd286fd3cbb",
    "semantic_title": "experiences with shared resources for research and education in speech and language processing",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/toda16_interspeech.html": {
    "title": "The Voice Conversion Challenge 2016",
    "volume": "main",
    "abstract": "This paper describes the Voice Conversion Challenge 2016 devised by the authors to better understand different voice conversion (VC) techniques by comparing their performance on a common dataset. The task of the challenge was speaker conversion, i.e., to transform the voice identity of a source speaker into that of a target speaker while preserving the linguistic content. Using a common dataset consisting of 162 utterances for training and 54 utterances for evaluation from each of 5 source and 5 target speakers, 17 groups working in VC around the world developed their own VC systems for every combination of the source and target speakers, i.e., 25 systems in total, and generated voice samples converted by the developed systems. These samples were evaluated in terms of target speaker similarity and naturalness by 200 listeners in a controlled environment. This paper summarizes the design of the challenge, its result, and a future plan to share views about unsolved problems and challenges faced by the current VC techniques",
    "checked": true,
    "id": "038e104a7e9d966f1951e88f8896332e5ce01a15",
    "semantic_title": "the voice conversion challenge 2016",
    "citation_count": 159
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wester16_interspeech.html": {
    "title": "Analysis of the Voice Conversion Challenge 2016 Evaluation Results",
    "volume": "main",
    "abstract": "The Voice Conversion Challenge 2016 is the first Voice Conversion Challenge in which different voice conversion systems and approaches using the same voice data were compared. This paper describes the design of the evaluation, it presents the results and statistical analyses of the results",
    "checked": true,
    "id": "c1b5ab330106a510a4bf1041b93de2f772d670d8",
    "semantic_title": "analysis of the voice conversion challenge 2016 evaluation results",
    "citation_count": 65
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16h_interspeech.html": {
    "title": "The USTC System for Voice Conversion Challenge 2016: Neural Network Based Approaches for Spectrum, Aperiodicity and F0 Conversion",
    "volume": "main",
    "abstract": "This paper introduces the methods we adopt to build our system for the evaluation event of Voice Conversion Challenge (VCC) 2016. We propose to use neural network-based approaches to convert both spectral and excitation features. First, the generatively trained deep neural network (GTDNN) is adopted for spectral envelope conversion after the spectral envelopes have been pre-processed by frequency warping. Second, we propose to use a recurrent neural network (RNN) with long short-term memory (LSTM) cells for F0 trajectory conversion. In addition, we adopt a DNN for band aperiodicity conversion. Both internal tests and formal VCC evaluation results demonstrate the effectiveness of the proposed methods",
    "checked": true,
    "id": "d56fd21a000ddfdebe0ad06a9c1b8ae453718d54",
    "semantic_title": "the ustc system for voice conversion challenge 2016: neural network based approaches for spectrum, aperiodicity and f0 conversion",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mohammadi16_interspeech.html": {
    "title": "A Voice Conversion Mapping Function Based on a Stacked Joint-Autoencoder",
    "volume": "main",
    "abstract": "In this study, we propose a novel method for training a regression function and apply it to a voice conversion task. The regression function is constructed using a Stacked Joint-Autoencoder (SJAE). Previously, we have used a more primitive version of this architecture for pre-training a Deep Neural Network (DNN). Using objective evaluation criteria, we show that the lower levels of the SJAE perform best with a low degree of jointness, and higher levels with a higher degree of jointness. We demonstrate that our proposed approach generates features that do not suffer from the averaging effect inherent in back-propagation training. We also carried out subjective listening experiments to evaluate speech quality and speaker similarity. Our results show that the SJAE approach has both higher quality and similarity than a SJAE+DNN approach, where the SJAE is used for pre-training a DNN, and the fine-tuned DNN is then used for mapping. We also present the system description and results of our submission to Voice Conversion Challenge 2016",
    "checked": true,
    "id": "99e7d3849551e8410df95f97b58e7fb265721c9b",
    "semantic_title": "a voice conversion mapping function based on a stacked joint-autoencoder",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wu16c_interspeech.html": {
    "title": "Locally Linear Embedding for Exemplar-Based Spectral Conversion",
    "volume": "main",
    "abstract": "This paper describes a novel exemplar-based spectral conversion (SC) system developed by the AST (Academia Sinica, Taipei) team for the 2016 voice conversion challenge (vcc2016). The key feature of our system is that it integrates the locally linear embedding (LLE) algorithm, a manifold learning algorithm that has been successfully applied for the super-resolution task in image processing, with the conventional exemplar-based SC method. To further improve the quality of the converted speech, our system also incorporates (1) the maximum likelihood parameter generation (MLPG) algorithm, (2) the postfiltering-based global variance (GV) compensation method, and (3) a high-resolution feature extraction process. The results of subjective evaluation conducted by the vcc2016 organizer show that our LLE-exemplar-based SC system notably outperforms the baseline GMM-based system (implemented by the vcc2016 organizer). Moreover, our own internal evaluation results confirm the effectiveness of the major LLE-exemplar-based SC method and the three additional approaches with improved speech quality",
    "checked": true,
    "id": "bc66a3ec528f2b8510841b953c1ded03eb72c0ad",
    "semantic_title": "locally linear embedding for exemplar-based spectral conversion",
    "citation_count": 36
  },
  "https://www.isca-speech.org/archive/interspeech_2016/villavicencio16_interspeech.html": {
    "title": "Applying Spectral Normalisation and Efficient Envelope Estimation and Statistical Transformation for the Voice Conversion Challenge 2016",
    "volume": "main",
    "abstract": "In this work we present our entry for the Voice Conversion Challenge 2016, denoting new features to previous work on GMM-based voice conversion. We incorporate frequency warping and pitch transposition strategies to perform a normalisation of the spectral conditions, with benefits confirmed by objective and perceptual means. Moreover, the results of the challenge showed our entry among the highest performing systems in terms of perceived naturalness while maintaining the target similarity performance of GMM-based conversion",
    "checked": true,
    "id": "e32fda451c7ea70d5e522a51b1d40583c75eddb9",
    "semantic_title": "applying spectral normalisation and efficient envelope estimation and statistical transformation for the voice conversion challenge 2016",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/erro16_interspeech.html": {
    "title": "ML Parameter Generation with a Reformulated MGE Training Criterion — Participation in the Voice Conversion Challenge 2016",
    "volume": "main",
    "abstract": "This paper describes our entry to the Voice Conversion Challenge 2016. Based on the maximum likelihood parameter generation algorithm, the method is a reformulation of the minimum generation error training criterion. It uses a GMM for soft classification, a Mel-cepstral vocoder for acoustic analysis and an improved dynamic time warping procedure for source-target alignment. To compensate the oversmoothing effect, the generated parameters are filtered through a speaker-independent postfilter implemented as a linear transform in cepstral domain. The process is completed with mean and variance adaptation of the log- fundamental frequency and duration modification by a constant factor. The results of the evaluation show that the proposed system achieves a high conversion accuracy in comparison with other systems, while its naturalness scores are intermediate",
    "checked": false,
    "id": "60d2601c4938982d31870e236eb70f69875076df",
    "semantic_title": "ml parameter generation with a reformulated mge training criterion - participation in the voice conversion challenge 2016",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kobayashi16_interspeech.html": {
    "title": "The NU-NAIST Voice Conversion System for the Voice Conversion Challenge 2016",
    "volume": "main",
    "abstract": "This paper presents the NU-NAIST voice conversion (VC) system for the Voice Conversion Challenge 2016 (VCC 2016) developed by a joint team of Nagoya University and Nara Institute of Science and Technology. Statistical VC based on a Gaussian mixture model makes it possible to convert speaker identity of a source speaker' voice into that of a target speaker by converting several speech parameters. However, various factors such as parameterization errors and over-smoothing effects usually cause speech quality degradation of the converted voice. To address this issue, we have proposed a direct waveform modification technique based on spectral differential filtering and have successfully applied it to singing voice conversion where excitation features are not necessary converted. In this paper, we propose a method to apply this technique to a standard voice conversion task where excitation feature conversion is needed. The result of VCC 2016 demonstrates that the NU-NAIST VC system developed by the proposed method yields the best conversion accuracy for speaker identity (more than 70% of the correct rate) and quite high naturalness score (more than 3 of the mean opinion score). This paper presents detail descriptions of the NU-NAIST VC system and additional results of its performance evaluation",
    "checked": true,
    "id": "65cdc3c6c7cc9efadb5af507015422f2a60f682e",
    "semantic_title": "the nu-naist voice conversion system for the voice conversion challenge 2016",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2016/landerportnoy16_interspeech.html": {
    "title": "Release from Energetic Masking Caused by Repeated Patterns of Glimpsing Windows",
    "volume": "main",
    "abstract": "The study of auditory masking not only provides data for how healthy and impaired listeners perform in adverse listening conditions, and thereby approximates their ability to perceive speech in the noisy environments of everyday life, but also provides insights into the mechanisms that underly the detection and perception of speech. Previous studies, (Pollack 1955) (Festen & Plomp 1990) (Cooper et al. 2015), have manipulated noise maskers in an attempt to observe the relationship between modulation of the type or characteristics of masking noise to subjects ability to detect or recognize a target signal. In this experiment, long term average spectrum speech shaped noise maskers were modulated to allow either short or long glimpsing (Cooke 2005) windows, during which the target signal was unmasked, in one second long morse code patterns of eight windows. The results from 60 participants with normal hearing showed that subjects performed significantly better on trials of an open set word recognition task when the pattern of glimpsing windows repeated twice before presentation of the masked signal than a control with the same glimpsing windows during the signal but different beforehand and one with the same amount of noise masking in random patterns before and during the target",
    "checked": true,
    "id": "71d21407b47d07582b6f460ae5a19d9f68f19a84",
    "semantic_title": "release from energetic masking caused by repeated patterns of glimpsing windows",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gibbs16_interspeech.html": {
    "title": "Glimpsing Predictions for Natural and Vocoded Sentence Intelligibility During Modulation Masking: Effect of the Glimpse Cutoff Criterion",
    "volume": "main",
    "abstract": "This study varied the signal-to-noise ratio (SNR) cutoff criterion for acoustically defining usable perceptual glimpses that contribute to speech intelligibility. Criterion-dependent effects were determined by examining the correlation of three different acoustic glimpse metrics with intelligibility. Glimpse properties change depending on the acoustic interactions between the speech and competing noise. Therefore, these measures were investigated with different rates of competing speech that were varied using time compression or expansion. Finally, effects of temporal modulation masking and spectral segregation were examined by comparison between unprocessed (natural) and vocoded speech. Results revealed a range of SNR cutoffs that were associated with correlations between the different acoustic glimpse metrics and intelligibility. Changing the glimpse criterion strongly influenced the associations between intelligibility and two of the acoustic glimpse metrics for the different masker modulation rates. However, the proportion of target speech above the SNR cutoff was less affected by altering the cutoff criterion. These results suggest that intelligibility models should account for the perceptual contribution of different glimpse metrics or limit glimpse cutoff criteria to an SNR region (1–3 dB based on this data) that captures the perceptual utility of multiple glimpse mechanisms",
    "checked": true,
    "id": "04c8a766454d64f2025c8b1efe8a31ae9796b50d",
    "semantic_title": "glimpsing predictions for natural and vocoded sentence intelligibility during modulation masking: effect of the glimpse cutoff criterion",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xu16b_interspeech.html": {
    "title": "Temporal Envelopes in Sine-Wave Speech Recognition",
    "volume": "main",
    "abstract": "There is a long debate on the relative importance of spectral and temporal cues in speech perception theories. On the one hand, the highly-intelligible sine-wave speech (SWS) has been viewed as a representation of the global spectral structure of the speech signal. On the other hand, there is accumulating evidence showing that the temporal aspects of speech without spectral details provide sufficient speech understanding. The present study explored whether the temporal envelopes imbedded in the SWS contribute to its intelligibility. In the experiments, both SWS and natural speech signals were processed with noise and tone vocoders to remove the spectral details but to preserve the temporal envelopes. Twenty-two normal-hearing, native English-speaking adult listeners participated in sentence recognition tasks. Speech recognition performance of vocoder-processed SWS was slightly inferior to that of vocoder-processed natural speech but both reached plateau performance at 6–8 channels. Acoustic analysis further indicated that the temporal envelopes of the SWS were almost identical to those of the natural speech, with a mean correlation coefficient r = 0.949 across all sentences. The results provide strong evidence that the SWS represents both spectral and temporal structures of the speech and that the temporal envelopes imbedded in SWS carry important information for speech recognition",
    "checked": true,
    "id": "3237135c78e66a8655b137b10578995157d8e81c",
    "semantic_title": "temporal envelopes in sine-wave speech recognition",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16h_interspeech.html": {
    "title": "Understanding Periodically Interrupted Mandarin Speech",
    "volume": "main",
    "abstract": "This study investigated the effects of two parameters (i.e., interruption rate, and duty cycle of interruption) on the perception of periodically interrupted Mandarin speech. Normal-hearing listeners were instructed to identify consonant/vowel/tone/word from isolated Mandarin words and recognize Mandarin sentences when they were temporally interrupted by square wave. Results showed that consistent with earlier findings obtained with English speech, interruption with a large rate or duty cycle favored the perception of periodically interrupted Mandarin speech. In addition, for isolated Mandarin word, the perception of vowel or tone was less affected by periodical interruption than that of consonant, and under periodical interruption the perception of consonant could largely account for the recognition of Mandarin word. For Mandarin sentence, the tonal characteristics and the simpler syllable structure in Mandarin might facilitate spectral-temporal integration of the target words, which contributed to a sentence intelligibility advantage of Mandarin over English under interrupted conditions",
    "checked": true,
    "id": "5e2e80c5b6ce445a45e6b55505e5d7fd3d9355a2",
    "semantic_title": "understanding periodically interrupted mandarin speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16i_interspeech.html": {
    "title": "Factors Affecting the Intelligibility of Sine-Wave Speech",
    "volume": "main",
    "abstract": "Studies on sine-wave speech (SWS) perception suggest that formants contain sufficient information for sentence intelligibility. This study further investigated the effects of amplitude modulation, number of sine-waves, and vowel resonance in SWS recognition. Results showed that Mandarin sentences synthesized using frequency trajectories of the first two formants were highly intelligible with additional contributions from formant amplitude modulation. However, amplitude modulation significantly contributed to intelligibility when only the vowels were preserved. The present work demonstrates that the intelligibility of Mandarin SWS can be largely attributed to the frequency transition of the first two formants and is susceptible to temporal interruption",
    "checked": true,
    "id": "cd0cb014e7850237b4b642b37a7c324e599b6ac9",
    "semantic_title": "factors affecting the intelligibility of sine-wave speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hodoshima16_interspeech.html": {
    "title": "Effects of Urgent Speech and Preceding Sounds on Speech Intelligibility in Noisy and Reverberant Environments",
    "volume": "main",
    "abstract": "Public-address (PA) announcements are used to convey emergency information; however, noise and reverberation sometimes make announcements in public spaces unintelligible. Therefore, the present study investigated how combinations of speech spoken in an urgent style and preceding sounds affect speech intelligibility and perceived urgency in noisy and reverberant environments. Sentences were spoken in normal or urgent styles and preceded by either two sounds (siren sound or ocean wave-like sound) or no sounds. Eighteen young participants carried out word identification test and rated perceived urgency on five-point scales in noisy and reverberant environments. The results showed that the urgently spoken speech had significantly higher speech intelligibility than the normal speech. The urgently spoken speech preceded by the wave-like sound showed significantly higher speech intelligibility than normal speech without sounds, normal speech preceded by the siren sound, and urgently spoken speech preceded by the siren sound. The results also demonstrated that the perceived urgency was rated higher for the urgently spoken speech than that for the normal speech, regardless of the types of preceding sounds. These results suggest that appropriate combinations of speaking styles and alerting sounds will increase the intelligibility of emergency PA announcements",
    "checked": true,
    "id": "bfd79b48dfaa7c545b9883205a07ea5ac12e1c47",
    "semantic_title": "effects of urgent speech and preceding sounds on speech intelligibility in noisy and reverberant environments",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sahidullah16_interspeech.html": {
    "title": "Integrated Spoofing Countermeasures and Automatic Speaker Verification: An Evaluation on ASVspoof 2015",
    "volume": "main",
    "abstract": "It is well known that automatic speaker verification (ASV) systems can be vulnerable to spoofing. The community has responded to the threat by developing dedicated countermeasures aimed at detecting spoofing attacks. Progress in this area has accelerated over recent years, partly as a result of the first standard evaluation, ASVspoof 2015, which focused on spoofing detection in isolation from ASV. This paper investigates the integration of state-of-the-art spoofing countermeasures in combination with ASV. Two general strategies to countermeasure integration are reported: cascaded and parallel. The paper reports the first comparative evaluation of each approach performed with the ASVspoof 2015 corpus. Results indicate that, even in the case of varying spoofing attack algorithms, ASV performance remains robust when protected with a diverse set of integrated countermeasures",
    "checked": true,
    "id": "c95288bcb5c2f9e87e244e0032e48ffba9f66b7d",
    "semantic_title": "integrated spoofing countermeasures and automatic speaker verification: an evaluation on asvspoof 2015",
    "citation_count": 52
  },
  "https://www.isca-speech.org/archive/interspeech_2016/korshunov16_interspeech.html": {
    "title": "Cross-Database Evaluation of Audio-Based Spoofing Detection Systems",
    "volume": "main",
    "abstract": "Since automatic speaker verification (ASV) systems are highly vulnerable to spoofing attacks, it is important to develop mechanisms that can detect such attacks. To be practical, however, a spoofing attack detection approach should have (i) high accuracy, (ii) be well-generalized for practical attacks, and (iii) be simple and efficient. Several audio-based spoofing detection methods have been proposed recently but their evaluation is limited to less realistic databases containing homogeneous data. In this paper, we consider eight existing presentation attack detection (PAD) methods and evaluate their performance using two major publicly available speaker databases with spoofing attacks: AVspoof and ASVspoof. We first show that realistic presentation attacks (speech is replayed to PAD system) are significantly more challenging for the considered PAD methods compared to the so called ‘logical access' attacks (speech is presented to PAD system directly). Then, via a cross-database evaluation, we demonstrate that the existing methods generalize poorly when different databases or different types of attacks are used for training and testing. The results question the efficiency and practicality of the existing PAD systems, as well as, call for creation of databases with larger variety of realistic speech presentation attacks",
    "checked": true,
    "id": "d9d321353f086c1b171211dece50649850706953",
    "semantic_title": "cross-database evaluation of audio-based spoofing detection systems",
    "citation_count": 42
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sriskandaraja16_interspeech.html": {
    "title": "Investigation of Sub-Band Discriminative Information Between Spoofed and Genuine Speech",
    "volume": "main",
    "abstract": "A speaker verification system should include effective precautions against malicious spoofing attacks, and although some initial countermeasures have been recently proposed, this remains a challenging research problem. This paper investigates discrimination between spoofed and genuine speech, as a function of frequency bands, across the speech bandwidth. Findings from our investigation inform some proposed filter bank design approaches for discrimination of spoofed speech. Experiments are conducted on the Spoofing and Anti-Spoofing (SAS) corpus using the proposed frequency-selective approach demonstrates an 11% relative improvement in terms of equal error rate compared with a conventional mel filter bank",
    "checked": true,
    "id": "fa0ca73ec4ef2ffdc6c34d363438b5593885cce9",
    "semantic_title": "investigation of sub-band discriminative information between spoofed and genuine speech",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tian16_interspeech.html": {
    "title": "An Investigation of Spoofing Speech Detection Under Additive Noise and Reverberant Conditions",
    "volume": "main",
    "abstract": "Spoofing detection for automatic speaker verification (ASV), which is to discriminate between live and artificial speech, has received increasing attentions recently. However, the previous studies have been done on the clean data without significant noise. It is still not clear whether the spoofing detectors trained on clean speech can generalise well under noisy conditions. In this work, we perform an investigation of spoofing detection under additive noise and reverberant conditions. In particular, we consider five difference additive noises at three different signal-to-noise ratios (SNR), and a reverberation noise with different reverberation time (RT). Our experimental results reveal that additive noises degrade the spoofing detectors trained on clean speech significantly. However, the reverberation does not hurt the performance too much",
    "checked": true,
    "id": "85efb1c84cf5948d2ed2081316a21e02ba58c013",
    "semantic_title": "an investigation of spoofing speech detection under additive noise and reverberant conditions",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sahidullah16b_interspeech.html": {
    "title": "Robust Speaker Recognition with Combined Use of Acoustic and Throat Microphone Speech",
    "volume": "main",
    "abstract": "Accuracy of automatic speaker recognition (ASV) systems degrades severely in the presence of background noise. In this paper, we study the use of additional side information provided by a body-conducted sensor, throat microphone. Throat microphone signal is much less affected by background noise in comparison to acoustic microphone signal. This makes throat microphones potentially useful for feature extraction or speech activity detection. This paper, firstly, proposes a new prototype system for simultaneous data-acquisition of acoustic and throat microphone signals. Secondly, we study the use of this additional information for both speech activity detection, feature extraction and fusion of the acoustic and throat microphone signals. We collect a pilot database consisting of 38 subjects including both clean and noisy sessions. We carry out speaker verification experiments using Gaussian mixture model with universal background model (GMM-UBM) and i-vector based system. We have achieved considerable improvement in recognition accuracy even in highly degraded conditions",
    "checked": true,
    "id": "431ea1cfca555f4e901689091f2b71ee10c97465",
    "semantic_title": "robust speaker recognition with combined use of acoustic and throat microphone speech",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/meng16b_interspeech.html": {
    "title": "Statistical Modeling of Speaker's Voice with Temporal Co-Location for Active Voice Authentication",
    "volume": "main",
    "abstract": "Active voice authentication (AVA) is a new mode of talker authentication, in which the authentication is performed continuously on very short segments of the voice signal, which may have instantaneously undergone change of talker. AVA is necessary in providing real-time monitoring of a device authorized for a particular user. The authentication test thus cannot rely on a long history of the voice data nor any past decisions. Most conventional voice authentication techniques that operate on the assumption that the entire test utterance is from only one talker with a claimed identity (including i-vector) fail to meet this stringent requirement. This paper presents a different signal modeling technique, within a conditional vector-quantization framework and with matching short-time statistics that take into account the co-located speech codes to meet the new challenge. As one variation, the temporally co-located VQ (TC-VQ) associates each codeword with a set of Gaussian mixture models to account for the co-located distributions and a temporally co-located hidden Markov model (TC-HMM) is built upon the TC-VQ. The proposed technique achieves an window-based equal error rate in the range of 3–5% and a relative gain of 4–25% over a baseline system using traditional HMMs on the AVA database",
    "checked": true,
    "id": "02caa431f600c6f17b85bcda68ae0bdcd535af3e",
    "semantic_title": "statistical modeling of speaker's voice with temporal co-location for active voice authentication",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fischer16_interspeech.html": {
    "title": "Joint Enhancement and Coding of Speech by Incorporating Wiener Filtering in a CELP Codec",
    "volume": "main",
    "abstract": "The performance of speech communication applications in the field of mobile devices is often hampered by background noises and distortions. Therefore, noise attenuation methods are commonly used as a pre-processing method, cascaded with the speech-codec. We demonstrate that the performance of such combinations of speech enhancement and coding methods can be improved by joining the two methods into a single block. The proposed method is based on incorporating Wiener filtering into the objective function used for optimization of the quantization in code excited linear prediction (CELP)-based codecs. The benefits are that 1) the non-linear components of CELP codecs, including quantization and error feedback, are taken into account in the joint minimization function thereby improving quality and 2) by merging blocks both delay and computational complexity can be minimized. Our experiments demonstrate that the proposed joint enhancement and coding approach consistently improves subjective and objective quality. The proposed method is compatible with any CELP-based codecs without changing the bit-stream, whereby it can be readily applied in mobile phones or speech communication devices applying the concepts of CELP codecs for improving perceptual quality in adverse conditions",
    "checked": true,
    "id": "bd9bbcd868e496618b33559b69eb432e2f77a303",
    "semantic_title": "joint enhancement and coding of speech by incorporating wiener filtering in a celp codec",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16i_interspeech.html": {
    "title": "Multi-Channel Linear Prediction Based on Binaural Coherence for Speech Dereverberation",
    "volume": "main",
    "abstract": "It has been shown that the multi-channel linear prediction (MCLP) can achieve blind speech dereverberation effectively. However, it always degrades the binaural cues which are exploited for human sound localization, i.e., interaural time differences (ITD) and interaural level differences (ILD). To overcome this problem, the multiple input-single output structure of conventional MCLP is modified to a binaural input-output structure for suppressing reverberation and preserving binaural cues simultaneously. First, by employing a binaural coherence model with head shadowing effects, the variance of desired signal can be estimated the same to both ears, which can ensure no modification of ILD. Then, the variance is utilized to calculate the prediction coefficients in a maximum-likelihood (ML) sense. Finally, the desired signals can be obtained as the prediction errors in MCLP. And since the algorithm does not disturb the phase of input signal, the ITD cue is kept. Evaluations with measured binaural room impulse responses (BRIRs) show that the proposed method yields a good performance on both speech dereverberation and binaural cues preservation",
    "checked": true,
    "id": "09724b9be9a85206a966698087d5995ab0cdf50f",
    "semantic_title": "multi-channel linear prediction based on binaural coherence for speech dereverberation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/blass16_interspeech.html": {
    "title": "Single-Channel Speech Enhancement Using Double Spectrum",
    "volume": "main",
    "abstract": "Single-channel speech enhancement is often formulated in the Short-Time Fourier Transform (STFT) domain. As an alternative, several previous studies have reported advantages of speech processing using pitch-synchronous analysis and filtering in the modulation transform domain. We propose to use the Double Spectrum (DS) obtained by combining pitch-synchronous transform followed by modulation transform. The linearity and sparseness properties of DS domain are beneficial for single-channel speech enhancement. The effectiveness of the proposed DS-based speech enhancement is demonstrated by comparing it with STFT-based and modulation-based benchmarks. In contrast to the benchmark methods, the proposed method does not exploit any statistical information nor does it use temporal smoothing. The proposed method leads to an improvement of 0.3 PESQ on average for babble noise",
    "checked": true,
    "id": "335ade3efde41f59cc6c10a5b3761c7d1862f220",
    "semantic_title": "single-channel speech enhancement using double spectrum",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/drude16_interspeech.html": {
    "title": "On the Appropriateness of Complex-Valued Neural Networks for Speech Enhancement",
    "volume": "main",
    "abstract": "Although complex-valued neural networks (CVNNs) — networks which can operate with complex arithmetic — have been around for a while, they have not been given reconsideration since the breakthrough of deep network architectures. This paper presents a critical assessment whether the novel tool set of deep neural networks (DNNs) should be extended to complex-valued arithmetic. Indeed, with DNNs making inroads in speech enhancement tasks, the use of complex-valued input data, specifically the short-time Fourier transform coefficients, is an obvious consideration. In particular when it comes to performing tasks that heavily rely on phase information, such as acoustic beamforming, complex-valued algorithms are omnipresent. In this contribution we recapitulate backpropagation in CVNNs, develop complex-valued network elements, such as the split-rectified non-linearity, and compare real- and complex-valued networks on a beamforming task. We find that CVNNs hardly provide a performance gain and conclude that the effort of developing the complex-valued counterparts of the building blocks of modern deep or recurrent neural networks can hardly be justified",
    "checked": true,
    "id": "6ab1e81f409656046695bf7d8ef6498ad2bb2a63",
    "semantic_title": "on the appropriateness of complex-valued neural networks for speech enhancement",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zeiler16_interspeech.html": {
    "title": "Introducing the Turbo-Twin-HMM for Audio-Visual Speech Enhancement",
    "volume": "main",
    "abstract": "Models for automatic speech recognition (ASR) hold detailed information about spectral and spectro-temporal characteristics of clean speech signals. Using these models for speech enhancement is desirable and has been the target of past research efforts. In such model-based speech enhancement systems, a powerful ASR is imperative. To increase the recognition rates especially in low-SNR conditions, we suggest the use of the additional visual modality, which is mostly unaffected by degradations in the acoustic channel. An optimal integration of acoustic and visual information is achievable by joint inference in both modalities within the turbo-decoding framework. Thus combining turbo-decoding with Twin-HMMs for speech enhancement, notable improvements can be achieved, not only in terms of instrumental estimates of speech quality, but also in actual speech intelligibility. This is verified through listening tests, which show that in highly challenging noise conditions, average human recognition accuracy can be improved from 64% without signal processing to 80% when using the presented architecture",
    "checked": true,
    "id": "f556b975170673d11ae6c173e7d0353eb8546109",
    "semantic_title": "introducing the turbo-twin-hmm for audio-visual speech enhancement",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/spille16_interspeech.html": {
    "title": "Assessing Speech Quality in Speech-Aware Hearing Aids Based on Phoneme Posteriorgrams",
    "volume": "main",
    "abstract": "Current behind-the-ear hearing aids (HA) allow to perform spatial filtering to enhance localized sound sources; however, they often lack processing strategies that are tailored to spoken language. Hence, without a feedback about speech quality achieved by the system, spatial filtering potentially remains unused, in case of a conservative enhancement strategy, or can even be detrimental to the speech intelligibility of the output signal. In this paper we apply phoneme posteriorgrams obtained from HA signals processed with deep neural networks to measure the quality of speech representations in spatial scenes. Inverse entropy of phoneme probabilities is proposed as a measure that allows to evaluate if current hearing aid parameters are optimal for the given acoustic condition. We investigate how varying noise levels and wrong estimates of the to-be-enhanced direction affect this measure in anechoic and reverberant conditions and show our measure to provide a high reliability when varying each parameter. Experiments show that entropy as a function of the beam angle has a distinct minimum at the speaker's true position and its immediate vicinity. Thus, it can be used to determine the beam angle which optimizes the speech representation. Further, variations of the SNR cause a consistent offset of the entropy",
    "checked": true,
    "id": "c0affd28bebcf928e202bc402b262d9d8545da6d",
    "semantic_title": "assessing speech quality in speech-aware hearing aids based on phoneme posteriorgrams",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gowda16_interspeech.html": {
    "title": "Time-Varying Quasi-Closed-Phase Weighted Linear Prediction Analysis of Speech for Accurate Formant Detection and Tracking",
    "volume": "main",
    "abstract": "In this paper, we propose a new method for accurate detection, estimation and tracking of formants in speech signals using time-varying quasi-closed phase analysis (TVQCP). The proposed method combines two different methods of analysis namely, the time-varying linear prediction (TVLP) and quasi-closed phase (QCP) analysis. TVLP helps in better tracking of formant frequencies by imposing a time-continuity constraint on the linear prediction (LP) coefficients. QCP analysis, a type of weighted LP (WLP), improves the estimation accuracies of the formant frequencies by using a carefully designed weight function on the error signal that is minimized. The QCP weight function emphasizes the closed-phase region of the glottal cycle, and also weights down the regions around the main excitations. This results in reduced coupling of the subglottal cavity and the excitation source. Experimental results on natural speech signals show that the proposed method performs considerably better than the detect-and-track approach used in popular tools like Wavesurfer or Praat",
    "checked": true,
    "id": "d2d5d0ee94fa262a73b756983b1ca6f70a178221",
    "semantic_title": "time-varying quasi-closed-phase weighted linear prediction analysis of speech for accurate formant detection and tracking",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lim16b_interspeech.html": {
    "title": "Improved Depiction of Tissue Boundaries in Vocal Tract Real-Time MRI Using Automatic Off-Resonance Correction",
    "volume": "main",
    "abstract": "Real-time magnetic resonance imaging (RT-MRI) is a powerful tool to study the dynamics of vocal tract shaping during speech production. The dynamic articulators of interest include the surfaces of the lips, tongue, hard palate, soft palate, and pharyngeal airway. All of these are located at air-tissue interfaces and are vulnerable to MRI off-resonance effect due to magnetic susceptibility. In RT-MRI using spiral or radial scanning, this appears as a signal loss or blurring in images and may impair the analysis of dynamic speech data. We apply an automatic off-resonance artifact correction method to speech RT-MRI data in order to enhance the sharpness of air-tissue boundaries. We demonstrate the improvement qualitatively and using an image sharpness metric offering an improved tool for speech science research",
    "checked": true,
    "id": "bf753ead4d86601bbdc7b3f51ca8862b5d0cf3e9",
    "semantic_title": "improved depiction of tissue boundaries in vocal tract real-time mri using automatic off-resonance correction",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/blaauw16_interspeech.html": {
    "title": "Modeling and Transforming Speech Using Variational Autoencoders",
    "volume": "main",
    "abstract": "Latent generative models can learn higher-level underlying factors from complex data in an unsupervised manner. Such models can be used in a wide range of speech processing applications, including synthesis, transformation and classification. While there have been many advances in this field in recent years, the application of the resulting models to speech processing tasks is generally not explicitly considered. In this paper we apply the variational autoencoder (VAE) to the task of modeling frame-wise spectral envelopes. The VAE model has many attractive properties such as continuous latent variables, prior probability over these latent variables, a tractable lower bound on the marginal log likelihood, both generative and recognition models, and end-to-end training of deep models. We consider different aspects of training such models for speech data and compare them to more conventional models such as the Restricted Boltzmann Machine (RBM). While evaluating generative models is difficult, we try to obtain a balanced picture by considering both performance in terms of reconstruction error and when applying the model to a series of modeling and transformation tasks to get an idea of the quality of the learned features",
    "checked": true,
    "id": "63606c708d64bb894d220b95210feed73648d23f",
    "semantic_title": "modeling and transforming speech using variational autoencoders",
    "citation_count": 59
  },
  "https://www.isca-speech.org/archive/interspeech_2016/seelamantula16_interspeech.html": {
    "title": "Phase-Encoded Speech Spectrograms",
    "volume": "main",
    "abstract": "Spectrograms of speech and audio signals are time-frequency densities, and by construction, they are non-negative and do not have phase associated with them. Under certain conditions on the amount of overlap between consecutive frames and frequency sampling, it is possible to reconstruct the signal from the spectrogram. Deviating from this requirement, we develop a new technique to incorporate the phase of the signal in the spectrogram by satisfying what we call as the delta dominance condition, which in general is different from the well known minimum-phase condition. In fact, there are signals that are delta dominant but not minimum-phase and vice versa. The delta dominance condition can be satisfied in multiple ways, for example by placing a Kronecker impulse of the right amplitude or by choosing a suitable window function. A direct consequence of this novel way of constructing the spectrograms is that the phase of the signal is directly encoded or embedded in the spectrogram. We also develop a reconstruction methodology that takes such phase-encoded spectrograms and obtains the signal using the discrete Fourier transform (DFT). It is envisaged that the new class of phase-encoded spectrogram representations would find applications in various speech processing tasks such as analysis, synthesis, enhancement, and recognition",
    "checked": true,
    "id": "516462279d3475ac5ad260eefaa630297cb60b3f",
    "semantic_title": "phase-encoded speech spectrograms",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/birkholz16_interspeech.html": {
    "title": "Towards Minimally Invasive Velar State Detection in Normal and Silent Speech",
    "volume": "main",
    "abstract": "We present a portable minimally invasive system to determine the state of the velum (raised or lowered) at a sampling rate of 40 Hz that works both during normal and silent speech. The system consists of a small capsule containing a miniature loudspeaker and a miniature microphone. The capsule is inserted into one nostril by about 10 mm. The loudspeaker emits chirps with a power band from 12–24 kHz into the nostril and the microphone records the signal reflected from the nasal cavity. The chirp response differs between raised and lowered velar positions, because the velar position determines the shape of the nasal cavity in the posterior part and hence its acoustic behaviour. Reference chirp responses for raised and lowered velar positions in combination with a spectral distance measure are used to infer the state of the velum. Here we discuss critical design aspects of the system and outline future improvements. Possible applications of the device include the detection of the velar state during silent speech recognition, medical assessment of velar mobility and speech production research",
    "checked": true,
    "id": "9ab6f6bf60bbddd5927554b8c3ae9a84112d5cd2",
    "semantic_title": "towards minimally invasive velar state detection in normal and silent speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16d_interspeech.html": {
    "title": "RNN-BLSTM Based Multi-Pitch Estimation",
    "volume": "main",
    "abstract": "Multi-pitch estimation is critical in many applications, including computational auditory scene analysis (CASA), speech enhancement/separation and mixed speech analysis; however, despite much effort, it remains a challenging problem. This paper uses the PEFAC algorithm to extract features and proposes the use of recurrent neural networks with bidirectional Long Short-Term Memory (RNN-BLSTM) to model the two pitch contours of a mixture of two speech signals. Compared with feed-forward deep neural networks (DNN), which are trained on static frame-level acoustic features, RNN-BLSTM is trained on sequential frame-level features and has more power to learn pitch contour temporal dynamics. The results of evaluations using a speech dataset containing mixtures of two speech signals demonstrate that RNN-BLSTM can substantially outperform DNN in multi-pitch estimation of mixed speech signals",
    "checked": true,
    "id": "2ce4b60db05d06dc09f1d07fa1bc2a5f97fa420e",
    "semantic_title": "rnn-blstm based multi-pitch estimation",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2016/morise16_interspeech.html": {
    "title": "TUSK: A Framework for Overviewing the Performance of F0 Estimators",
    "volume": "main",
    "abstract": "This article presents a framework for overviewing the performance of fundamental frequency (F0) estimators and evaluates its effectiveness. Over the past few decades, many F0 estimators and evaluation indices have been proposed and have been evaluated using various speech databases. In speech analysis/ synthesis research, modern estimators are used as the algorithm to fulfill the demand for high-quality speech synthesis, but at the same time, they are competing with one another on minor issues. Specifically, while all of them meet the demands for high-quality speech synthesis, the result depends on the speech database used in the evaluation. Since there are various types of speech, it is inadvisable to discuss the effectiveness of each estimator on the basis of minor differences. It would be better to select the appropriate F0 estimator in accordance with the speech characteristics. The framework we propose, TUSK, does not rank the estimators but rather attempts to overview them. In TUSK, six parameters are introduced to observe the trends in the characteristics in each F0 estimator. The signal is artificially generated so that six parameters can be controllable independently. In this article, we introduce the concept of TUSK and determine its effectiveness using several modern F0 estimators",
    "checked": true,
    "id": "08074e09ace37798b1d482fc86da5956dc9aede2",
    "semantic_title": "tusk: a framework for overviewing the performance of f0 estimators",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/rengaswamy16_interspeech.html": {
    "title": "A Robust Non-Parametric and Filtering Based Approach for Glottal Closure Instant Detection",
    "volume": "main",
    "abstract": "In this paper, a novel non-parametric based glottal closure instant (GCI) detection method after filtering the speech signal through a pulse shaping filter is proposed. The pulse shaping filter essentially de-emphasises the vocal tract resonances by emphasising the frequency components containing the pitch information. The filtered signal is subjected to non-linear processing to emphasise the GCI locations. The GCI locations are finally obtained by a non-parametric histograms based approach in the detected voiced regions from the filtered speech signal. The proposed method is compared with the two state-of-the-art epoch extraction methods : Zero frequency filtering (ZFF) and SEDREAMS (both of which requires upfront knowledge of average pitch period). The performance of the method is evaluated on the complete CMU-ARCTIC dataset consisting of both speech and Electroglottograph (EGG) signals. The robustness of the proposed method to the additive white noise is evaluated with several degradation levels. The experimental results showed that the proposed method is indeed immune to noise and the obtained results are comparably better than the two state-of-the-art methods",
    "checked": true,
    "id": "aa9c6d02ee178f35933a05217e42a16e5f8c7bf1",
    "semantic_title": "a robust non-parametric and filtering based approach for glottal closure instant detection",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2016/saeidi16_interspeech.html": {
    "title": "Analysis of Face Mask Effect on Speaker Recognition",
    "volume": "main",
    "abstract": "Wearing a face mask affects the speech production. On top of that, the frequency response and radiation characteristics of the face mask — depending on the material and shape of the mask — adds to the complexity of analyzing speech under face mask. Our target is to separate the effect of muscle constriction and increased vocal effort in speech produced under face mask from sound transmission and radiation properties of face mask. In this paper, we measure up the far-field effects of wearing four different face masks; motorcycle helmet, rubber mask, surgical mask and scarf inside anechoic chamber. The measurement setup follows the recording configuration of a speech corpus used for speaker recognition experiments. In matching speech under face mask with speech under no mask, the frequency response of the respective face mask is accounted for and compensated for before acoustic feature extraction. The speaker recognition performance is reported using the state-of-the-art i-vector method for mismatched and compensated conditions in order to demonstrate the significance of knowing the type of mask and accounting for its sound transmission properties",
    "checked": true,
    "id": "feaa8f4c5d752d2a912ee61563db3d997d74ad98",
    "semantic_title": "analysis of face mask effect on speaker recognition",
    "citation_count": 34
  },
  "https://www.isca-speech.org/archive/interspeech_2016/singer16_interspeech.html": {
    "title": "Data Selection for Within-Class Covariance Estimation",
    "volume": "main",
    "abstract": "Methods for performing channel and session compensation in conjunction with subspace techniques have been a focus of considerable study recently and have led to significant gains in speaker recognition performance. While developers have typically exploited the vast archive of speaker labeled data available from earlier NIST evaluations to train the within-class and across-class covariance matrices required by these techniques, little attention has been paid to the characteristics of the data required to perform the training efficiently. This paper focuses on within-class covariance normalization (WCCN) and shows that a reduction in training data requirements can be achieved by proper data selection. In particular, it is shown that the key variables are the total amount of data and the degree of handset variability, with total calls per handset playing a smaller role. The study offers insight into efficient WCCN training data collection in real world applications",
    "checked": true,
    "id": "2bf9515f6604872ee8ec3e0bd44998fbd49c8551",
    "semantic_title": "data selection for within-class covariance estimation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ferras16_interspeech.html": {
    "title": "Inter-Task System Fusion for Speaker Recognition",
    "volume": "main",
    "abstract": "Fusion is a common approach to improving the performance of speaker recognition systems. Multiple systems using different data, features or algorithms tend to bring complementary contributions to the final decisions being made. It is known that factors such as native language or accent contribute to speaker identity. In this paper, we explore inter-task fusion approaches to incorporating side information from accent and language identification systems to improve the performance of a speaker verification system. We explore both score level and model level approaches, linear logistic regression and linear discriminant analysis respectively, reporting significant gains on accented and multi-lingual data sets of the NIST Speaker Recognition Evaluation 2008 data. Equal error rate and expected rank metrics are reported for speaker verification and speaker identification tasks",
    "checked": true,
    "id": "956aa77dd9ccbe37a70b0fb490345f33899b70da",
    "semantic_title": "inter-task system fusion for speaker recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lei16_interspeech.html": {
    "title": "Mahalanobis Metric Scoring Learned from Weighted Pairwise Constraints in I-Vector Speaker Recognition System",
    "volume": "main",
    "abstract": "The i-vector model is widely used by the state-of-the-art speaker recognition system. We proposed a new Mahalanobis metric scoring learned from weighted pairwise constraints (WPCML), which use the different weights for the empirical error of the similar and dissimilar pairs. In the new i-vector space described by the metric, the distance between the same speaker's i-vectors is small, while that of the different speakers' is large. In forming the training set, we use the traditional way in random fashion and develop a new nearest distance based way. The results on the NIST 2008 telephone data shown that our model can get better performance than the classical cosine similarity scoring. When using the nearest distance based way to form the training set, our model is better than the state-of-the-art PLDA. And the results on the NIST 2014 i-vector challenge show that our model is also better than the PLDA",
    "checked": true,
    "id": "c03f48e211ac81c3867c0e787bea3192fcfe323e",
    "semantic_title": "mahalanobis metric scoring learned from weighted pairwise constraints in i-vector speaker recognition system",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/soni16_interspeech.html": {
    "title": "Novel Subband Autoencoder Features for Detection of Spoofed Speech",
    "volume": "main",
    "abstract": "Deep Neural Network (DNN) have been extensively used in Automatic Speech Recognition (ASR) applications. Very recently, DNNs have also found application in detecting natural vs. spoofed speech at ASV spoof challenge held at INTERSPEECH 2015. Along the similar lines, in this work, we propose a new feature extraction architecture of DNN called the subband autoencoder (SBAE) for spoof detection task. The SBAE is inspired by the human auditory system and extracts features from the speech spectrum in an unsupervised manner. The features derived from SBAE are compared with state-of-the-art Mel Frequency Cepstral Coefficient (MFCC) features. The experiments were performed on ASV spoof challenge database and the performance was evaluated using Equal Error Rate (EER). It was observed that on the evaluation set, MFCC features with 36-dimensional (static+Δ+ΔΔ) features gave 4.32% EER which reduced to 2.9% when 36-dimensional SBAE features were used. Further on fusing SBAE features at score-level with MFCC, a further reduction till 1.93% EER was observed. This improvement in EER was due to the fact that the dynamics of SBAE features captured significant spoof specific characteristics leading to detect significantly even vocoder-independent speech, which is not the case for MFCC",
    "checked": true,
    "id": "dbaca69dd5f129d6758f24194743ca2f7e222926",
    "semantic_title": "novel subband autoencoder features for detection of spoofed speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mclaren16c_interspeech.html": {
    "title": "On the Issue of Calibration in DNN-Based Speaker Recognition Systems",
    "volume": "main",
    "abstract": "This article is concerned with the issue of calibration in the context of Deep Neural Network (DNN) based approaches to speaker recognition. DNNs have provided a new standard in technology when used in place of the traditional universal background model (UBM) for feature alignment, or to augment traditional features with those extracted from a bottleneck layer of the DNN. These techniques provide extremely good performance for constrained trial conditions that are well matched to development conditions. However, when applied to unseen conditions or a wide variety of conditions, some DNN-based techniques offer poor calibration performance. Through analysis on both PRISM and the recently released Speakers in the Wild (SITW) corpora, we illustrate that bottleneck features hinder calibration if used in the calculation of first-order Baum Welch statistics during i-vector extraction. We propose a hybrid alignment framework, which stems from our previous work in DNN senone alignment, that uses the bottleneck features only for the alignment of features during statistics calculation. This framework not only addresses the issue of calibration, but provides a more computationally efficient system based on bottleneck features with improved discriminative power",
    "checked": true,
    "id": "164f1944c6529c6edcdb3f055d9b2da7587c8328",
    "semantic_title": "on the issue of calibration in dnn-based speaker recognition systems",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kheder16b_interspeech.html": {
    "title": "Probabilistic Approach Using Joint Long and Short Session i-Vectors Modeling to Deal with Short Utterances for Speaker Recognition",
    "volume": "main",
    "abstract": "Speaker recognition with short utterance is highly challenging. The use of i-vectors in SR systems became a standard in the last years and many algorithms were developed to deal with the short utterances problem. We present in this paper a new technique based on modeling jointly the i-vectors corresponding to short utterances and those of long utterances. The joint distribution is estimated using a large number of i-vectors pairs (coming from short and long utterances) corresponding to the same session. The obtained distribution is then integrated in an MMSE estimator in the test phase to compute an \"improved\" version of short utterance i-vectors. We show that this technique can be used to deal with duration mismatch and that it achieves up to 40% of relative improvement in EER(%) when used on NIST data. We also apply this technique on the recently published SITW database and show that it yields 25% of EER(%) improvement compared to a regular PLDA scoring",
    "checked": true,
    "id": "c99981baa9fbb4b6c86db1d3f35c95f782a631ee",
    "semantic_title": "probabilistic approach using joint long and short session i-vectors modeling to deal with short utterances for speaker recognition",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kanagasundaram16_interspeech.html": {
    "title": "Short Utterance Variance Modelling and Utterance Partitioning for PLDA Speaker Verification",
    "volume": "main",
    "abstract": "This paper analyses the short utterance probabilistic linear discriminant analysis (PLDA) speaker verification with utterance partitioning and short utterance variance (SUV) modelling approaches. Experimental studies have found that instead of using single long-utterance as enrolment data, if long enrolled-utterance is partitioned into multiple short utterances and average of short utterance i-vectors is used as enrolled data, that improves the Gaussian PLDA (GPLDA) speaker verification. This is because short utterance i-vectors have speaker, session and utterance variations, and utterance-partitioning approach compensates the utterance variation. Subsequently, SUV-PLDA is also studied with utterance partitioning approach, and utterance-partitioning-based SUV-GPLDA system shows relative improvement of 9% and 16% in EER for NIST 2008 and NIST 2010 truncated 10sec-10sec evaluation condition as utterance-partitioning approach compensates the utterance variation and SUV modelling approach compensates the mismatch between full-length development data and short-length evaluation data",
    "checked": true,
    "id": "d7830bd0f3307152b18c9f76415296615c1e153d",
    "semantic_title": "short utterance variance modelling and utterance partitioning for plda speaker verification",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/thomsen16_interspeech.html": {
    "title": "Speaker-Dependent Dictionary-Based Speech Enhancement for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "The problem of text-dependent speaker verification under noisy conditions is becoming ever more relevant, due to increased usage for authentication in real-world applications. Classical methods for noise reduction such as spectral subtraction and Wiener filtering introduce distortion and do not perform well in this setting. In this work we compare the performance of different noise reduction methods under different noise conditions in terms of speaker verification when the text is known and the system is trained on clean data (mis-matched conditions). We furthermore propose a new approach based on dictionary-based noise reduction and compare it to the baseline methods",
    "checked": true,
    "id": "4652db9731ff86bda9d58d27965a8ecb2b1fbf82",
    "semantic_title": "speaker-dependent dictionary-based speech enhancement for text-dependent speaker verification",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yu16c_interspeech.html": {
    "title": "Text-Available Speaker Recognition System for Forensic Applications",
    "volume": "main",
    "abstract": "This paper examines a text-available speaker recognition approach targeting scenarios where the transcripts of test utterances are either available or obtainable through manual transcription. Forensic speaker recognition is one of such applications where the human supervision can be expected. In our study, we extend an existing Deep Neural Network (DNN) i-vector-based speaker recognition system to effectively incorporate text information associated with test utterances. We first show experimentally that speaker recognition performance drops significantly if the DNN output posteriors are directly replaced with their target senone, obtained from force alignment. The cause of such performance drops can be attributed to the fact that forced alignment selects only the single most probable senone as their output, which is not desirable in a current speaker recognition framework. To resolve this problem, we propose a posterior mapping approach where the relationship between forced aligned senones and its corresponding DNN posteriors are modeled. By replacing DNN output posteriors with senone mapped posteriors, a robust text-available speaker recognition system can be obtained in mismatched environments. Experiments using the proposed approach are performed on the Aurora-4 dataset",
    "checked": true,
    "id": "7e5133afe73970ef6080c0177dbeb68114070efc",
    "semantic_title": "text-available speaker recognition system for forensic applications",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hong16_interspeech.html": {
    "title": "Transfer Learning for Speaker Verification on Short Utterances",
    "volume": "main",
    "abstract": "Short utterance lacks enough discriminative information and its duration variation will propagate uncertainty into a probability linear discriminant analysis (PLDA) classifier. For speaker verification on short utterances, it can be considered as a domain with limited amount of long utterances. Therefore, transfer learning of PLDA can be adopted to learn discriminative information from other domain with a large amount of long utterances. In this paper, we explore the effectiveness of transfer learning based PLDA (TL-PLDA) on the NIST SRE and Switchboard (SWB) corpus. Experimental results showed that it could produce the largest gain of performance compared with the traditional PLDA, especially for short utterances with the duration of 5s and 10s",
    "checked": true,
    "id": "c94c07a25b42b321356c491d173b4dbca7afb198",
    "semantic_title": "transfer learning for speaker verification on short utterances",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ma16b_interspeech.html": {
    "title": "Twin Model G-PLDA for Duration Mismatch Compensation in Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "Short duration speaker verification is a challenging problem partly due to utterance duration mismatch. This paper proposes a novel method that modifies the standard Gaussian probabilistic linear discriminant analysis (G-PLDA) to use two separate generative models for i-vectors from long and short utterances which are jointly trained. The proposed twin model G-PLDA employs distinct models for i-vectors corresponding to different durations from the same speaker but shares the same latent variables. Unlike the standard G-PLDA, this twin model G-PLDA takes the differences between utterances of varying durations into account. Hyper-parameter estimation and scoring formulae for the twin model G-PLDA are presented. Experimental results obtained using NIST 2010 data show that the proposed technique leads to relative improvements of 8.5% and 15.6% when tested on utterances of 5 second and 3 second durations respectively",
    "checked": true,
    "id": "7af24750c8aeb22ba9bea144f984660bbeca76e9",
    "semantic_title": "twin model g-plda for duration mismatch compensation in text-independent speaker verification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16e_interspeech.html": {
    "title": "Universal Background Sparse Coding and Multilayer Bootstrap Network for Speaker Clustering",
    "volume": "main",
    "abstract": "We apply multilayer bootstrap network (MBN) to speaker clustering. The proposed method first extracts supervectors by a universal background model, then reduces the dimension of the high-dimensional supervectors by MBN, and finally conducts speaker clustering by clustering the low-dimensional data. We also propose an MBN-based universal background model, named universal background sparse coding. The comparison results demonstrate the effectiveness and robustness of the proposed method",
    "checked": true,
    "id": "dd6147236a515a9419b780835d69be7a495c7bd9",
    "semantic_title": "universal background sparse coding and multilayer bootstrap network for speaker clustering",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tian16b_interspeech.html": {
    "title": "Improving Deep Neural Networks Based Speaker Verification Using Unlabeled Data",
    "volume": "main",
    "abstract": "Recently, deep neural networks (DNNs) trained to predict senones have been incorporated into the conventional i-vector based speaker verification systems to provide soft frame alignments and show promising results. However, the data mismatch problem may degrade the performance since the DNN requires transcribed data (out-domain data) while the data sets (in-domain data) used for i-vector training and extraction are mostly untranscribed. In this paper, we try to address this problem by exploiting the unlabeled in-domain data during the training of the DNN, hoping the DNN can provide a more robust basis for the in-domain data. In this work, we first explore the impact of using in-domain data during the unsupervised DNN pre-training process. In addition, we decode the in-domain data using a hybrid DNN-HMM system to get its transcription, and then we retrain the DNN model with the \"labeled\" in-domain data. Experimental results on the NIST SRE 2008 and the NIST SRE 2010 databases demonstrate the effectiveness of the proposed methods",
    "checked": true,
    "id": "88ae6512dfe3816072e7950363f9b5f4e3c70aa3",
    "semantic_title": "improving deep neural networks based speaker verification using unlabeled data",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kanda16b_interspeech.html": {
    "title": "Maximum a posteriori Based Decoding for CTC Acoustic Models",
    "volume": "main",
    "abstract": "This paper presents a novel decoding framework for connectionist temporal classification (CTC)-based acoustic models (AM). Although CTC-based AM inherently has the property of a language model (LM) in itself, an external LM trained with a large text corpus is still essential to obtain the best results. In the previous literatures, a naive interpolation of the CTC-based AM score and the external LM score was used, although there is no theoretical justification for it. In this paper, we propose a theoretically more sound decoding framework derived from a maximization of the posterior probability of a word sequence given an observation. In our decoding framework, a subword LM (SLM) is newly introduced to coordinate the CTC-based AM score and the word-level LM score. In experiments with the Wall Street Journal (WSJ) corpus and Corpus of Spontaneous Japanese (CSJ), our proposed framework consistently achieved improvements of 7.4–15.3% over the conventional interpolation-based framework. In the CSJ experiment, given 586 hours of training data, the CTC-based AM finally achieved a 6.7% better word error rate than the baseline method with deep neural networks and hidden Markov models",
    "checked": true,
    "id": "721fa4f0dd8a6d4f9d07bd8167ebd68aa68f8d0e",
    "semantic_title": "maximum a posteriori based decoding for ctc acoustic models",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2016/asaei16_interspeech.html": {
    "title": "Phonetic and Phonological Posterior Search Space Hashing Exploiting Class-Specific Sparsity Structures",
    "volume": "main",
    "abstract": "This paper shows that exemplar-based speech processing using class-conditional posterior probabilities admits a highly effective search strategy relying on posteriors' intrinsic sparsity structures. The posterior probabilities are estimated for phonetic and phonological classes using deep neural network (DNN) computational framework. Exploiting the class-specific sparsity leads to a simple quantized posterior hashing procedure to reduce the search space of posterior exemplars. To that end, small number of quantized posteriors are regarded as representatives of the posterior space and used as hash keys to index subsets of neighboring exemplars. The k nearest neighbor (kNN) method is applied for posterior based classification problems. The phonetic posterior probabilities are used as exemplars for phonetic classification whereas the phonological posteriors are used as exemplars for automatic prosodic event detection. Experimental results demonstrate that posterior hashing improves the efficiency of kNN classification drastically. This work encourages the use of posteriors as discriminative exemplars appropriate for large scale speech classification tasks",
    "checked": true,
    "id": "2e4ca3ee61193ca9ee757a289714ca575d6929f6",
    "semantic_title": "phonetic and phonological posterior search space hashing exploiting class-specific sparsity structures",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tucker16_interspeech.html": {
    "title": "Model Compression Applied to Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "Several consumer speech devices feature voice interfaces that perform on-device keyword spotting to initiate user interactions. Accurate on-device keyword spotting within a tight CPU budget is crucial for such devices. Motivated by this, we investigated two ways to improve deep neural network (DNN) acoustic models for keyword spotting without increasing CPU usage. First, we used low-rank weight matrices throughout the DNN. This allowed us to increase representational power by increasing the number of hidden nodes per layer without changing the total number of multiplications. Second, we used knowledge distilled from an ensemble of much larger DNNs used only during training. We systematically evaluated these two approaches on a massive corpus of far-field utterances. Alone both techniques improve performance and together they combine to give significant reductions in false alarms and misses without increasing CPU or memory usage",
    "checked": true,
    "id": "4394d1d79d9ee624795ed8a7b3576ba9438deb8d",
    "semantic_title": "model compression applied to small-footprint keyword spotting",
    "citation_count": 85
  },
  "https://www.isca-speech.org/archive/interspeech_2016/martinez16_interspeech.html": {
    "title": "Why do ASR Systems Despite Neural Nets Still Depend on Robust Features",
    "volume": "main",
    "abstract": "To which extent can neural nets learn traditional signal processing stages of current robust ASR front-ends? Will neural nets replace the classical, often auditory-inspired feature extraction in the near future? To answer these questions, a DNN-based ASR system was trained and tested on the Aurora4 robust ASR task using various (intermediate) processing stages. Additionally, the training set was divided into several fractions to reveal the amount of data needed to account for a missing processing step on the input signal or prior knowledge about the auditory system. The DNN system was able to learn from ordinary spectrograms representations outperforming MFCC using 75% of the training set and almost as good as log-Mel-spectrograms with the full set; on the other hand, it was unable to compensate the robustness of auditory-based Gabor features, which even using 40% of the training data outperformed every other representation. The study concludes that even with deep learning approaches, current ASR systems still benefit from a suitable feature extraction",
    "checked": true,
    "id": "6a5c88a8e922b07b88b7d8e9a1289e05e8732c5a",
    "semantic_title": "why do asr systems despite neural nets still depend on robust features",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/he16b_interspeech.html": {
    "title": "An Adaptive Multi-Band System for Low Power Voice Command Recognition",
    "volume": "main",
    "abstract": "A complete voice-driven experience in applications such as wearable electronics requires always-on keyword monitoring, which is prohibitively power consuming using current speech recognition methods. In this work, we propose an ultra-low power voice command recognition system that is designed to recognize short commands such as ‘Hi Galaxy'. To achieve power-efficient designs, the system uses adaptive feature pre-selection such that only a subset of all available features are selected and extracted based on the noise spectrum. The back-end classifier, supporting adaptive feature selection, is enabled by a novel multi-band deep neural networks (DNNs) model that processes only the selected features at each decision. In experiments, our adaptive scheme achieves comparable accuracy and improved efficiency using an average of 5 spectral feature bands, than a generic fully-connected DNNs model using the full speech spectrum. The system makes a recognition decision every 40ms on 1.2s of buffered speech and consumes ~230µW of power, thus promising low-power, low-complexity and robust application-specific voice recognition",
    "checked": true,
    "id": "d465b2af3289a4b3b6d8d9ea035f28911ea2bba4",
    "semantic_title": "an adaptive multi-band system for low power voice command recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/price16_interspeech.html": {
    "title": "Memory-Efficient Modeling and Search Techniques for Hardware ASR Decoders",
    "volume": "main",
    "abstract": "This paper gives an overview of acoustic modeling and search techniques for low-power embedded ASR decoders. Our design decisions prioritize memory bandwidth, which is the main driver in system power consumption. We evaluate three acoustic modeling approaches — Gaussian mixture model (GMM), subspace GMM (SGMM) and deep neural network (DNN) — and identify tradeoffs between memory bandwidth and recognition accuracy. We also present an HMM search scheme with WFST compression and caching, predictive beam width control, and a word lattice. Our results apply to embedded system implementations using microcontrollers, DSPs, FPGAs, or ASICs",
    "checked": true,
    "id": "2f6186a8ac6861442cffc0c62b2bc67abcabe26d",
    "semantic_title": "memory-efficient modeling and search techniques for hardware asr decoders",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yang16c_interspeech.html": {
    "title": "Log-Linear System Combination Using Structured Support Vector Machines",
    "volume": "main",
    "abstract": "Building high accuracy speech recognition systems with limited language resources is a highly challenging task. Although the use of multi-language data for acoustic models yields improvements, performance is often unsatisfactory with highly limited acoustic training data. In these situations, it is possible to consider using multiple well trained acoustic models and combine the system outputs together. Unfortunately, the computational cost associated with these approaches is high as multiple decoding runs are required. To address this problem, this paper examines schemes based on log-linear score combination. This has a number of advantages over standard combination schemes. Even with limited acoustic training data, it is possible to train, for example, phone-specific combination weights, allowing detailed relationships between the available well trained models to be obtained. To ensure robust parameter estimation, this paper casts log-linear score combination into a structured support vector machine (SSVM) learning task. This yields a method to train model parameters with good generalisation properties. Here the SSVM feature space is a set of scores from well-trained individual systems. The SSVM approach is compared to lattice rescoring and confusion network combination using language packs released within the IARPA Babel program",
    "checked": true,
    "id": "2479f78970d850fd763c762ede665ea9cbca2a4a",
    "semantic_title": "log-linear system combination using structured support vector machines",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tang16b_interspeech.html": {
    "title": "Efficient Segmental Cascades for Speech Recognition",
    "volume": "main",
    "abstract": "Discriminative segmental models offer a way to incorporate flexible feature functions into speech recognition. However, their appeal has been limited by their computational requirements, due to the large number of possible segments to consider. Multi-pass cascades of segmental models introduce features of increasing complexity in different passes, where in each pass a segmental model rescores lattices produced by a previous (simpler) segmental model. In this paper, we explore several ways of making segmental cascades efficient and practical: reducing the feature set in the first pass, frame subsampling, and various pruning approaches. In experiments on phonetic recognition, we find that with a combination of such techniques, it is possible to maintain competitive performance while greatly reducing decoding, pruning, and training time",
    "checked": true,
    "id": "f7ec9de686ce500a94185d4a6ca1a4af1d0a2b31",
    "semantic_title": "efficient segmental cascades for speech recognition",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xu16c_interspeech.html": {
    "title": "A WFST Framework for Single-Pass Multi-Stream Decoding",
    "volume": "main",
    "abstract": "Combining disparate automatic speech recognition systems has long been an important strategy to improve recognition accuracy. Typically, each system requires a separate decoder; final results are derived by combining hypotheses from multiple lattices, necessitating multiple passes of decoding. We propose a novel Weighted Finite State Transducer (WFST) framework for integrating disparate systems. Our framework is different from the current popular system combination techniques in that the combination is done in one-pass decoding and allows the flexibility to combine systems at different levels of the decoding pipeline. Initial experiments with the framework achieved comparable performance as MBR-based combination which is reported to outperform ROVER and Confusion Network Combination (CNC). In this paper, we describe our methodology and present pilot study results for combining systems that use different sets of acoustic models, 1) gender-dependent GMM models, 2) MFCC and PLP features with GMM models, 3) MFCC, PLP and Filter Bank features with DNN models, and 4) SNR-specific DNN acoustic models. For each experiment, we also compared the computation time of the combined systems with their corresponding baseline systems. Our results show encouraging benefits of using the proposed framework to improve recognition performance while reducing computation time",
    "checked": true,
    "id": "500f1769c063468fa7915eac24590b7739774ce6",
    "semantic_title": "a wfst framework for single-pass multi-stream decoding",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hartmann16_interspeech.html": {
    "title": "Comparison of Multiple System Combination Techniques for Keyword Spotting",
    "volume": "main",
    "abstract": "System combination is a common approach to improving results for both speech transcription and keyword spotting — especially in the context of low-resourced languages where building multiple complementary models requires less computational effort. Using state-of-the-art CNN and DNN acoustic models, we analyze the performance, cost, and trade-offs of four system combination approaches: feature combination, joint decoding, hitlist combination, and a novel lattice combination method. Previous work has focused solely on accuracy comparisons. We show that joint decoding, lattice combination, and hitlist combination perform comparably, significantly better than feature combination. However, for practical systems, earlier combination reduces computational cost and storage requirements. Results are reported on four languages from the IARPA Babel dataset",
    "checked": true,
    "id": "19f1051e53def9c22b6a025c6f4d501091e8f0e7",
    "semantic_title": "comparison of multiple system combination techniques for keyword spotting",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/obara16_interspeech.html": {
    "title": "Rescoring by Combination of Posteriorgram Score and Subword-Matching Score for Use in Query-by-Example",
    "volume": "main",
    "abstract": "There has been much discussion recently regarding spoken term detection (STD) in speech processing communities. Query-by-Example (QbE) has also been an important topic in spoken-term detection (STD), where a query is issued using a speech signal. This paper proposes a rescoring method using a posteriorgram, which is a sequence of posterior probabilities obtained by a deep neural network (DNN) to be matched against both a speech signal of a query and spoken documents. Because direct matching between two posteriorgrams requires significant computation time, we first apply a conventional STD method that performs matching at a subword or state level, where the subword denotes an acoustic model, and the state composes a hidden Markov model of the acoustic model. Both the spoken query and the spoken documents are converted to subword sequences, using an automatic speech recognizer. After obtaining scores of candidates by subword/state matching, matching at the frame level using the posteriorgram is performed with continuous dynamic programming (CDP) verification for the top N candidates acquired by the subword/state matching. The score of the subword/state matching and the score of the posteriorgram matching are integrated and rescored, using a weighting coefficient. To reduce computation time, the proposed method is restricted to only top candidates for rescoring. Experiments for evaluation have been carried out using open test collections (Spoken-Doc tasks of NTCIR-10 workshops), and the results have demonstrated the effectiveness of the proposed method",
    "checked": true,
    "id": "1b23382f785258bccac54627edeccd9e9c180d33",
    "semantic_title": "rescoring by combination of posteriorgram score and subword-matching score for use in query-by-example",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16j_interspeech.html": {
    "title": "Phone Synchronous Decoding with CTC Lattice",
    "volume": "main",
    "abstract": "Connectionist Temporal Classification (CTC) has recently shown improved efficiency in LVCSR decoding. One popular implementation is to use a CTC model to predict the phone posteriors at each frame which are then used for Viterbi beam search on a modified WFST network. This is still within the traditional frame synchronous decoding framework. In this paper, the peaky posterior property of a CTC model is carefully investigated and it is found that ignoring blank frames will not introduce additional search errors. Based on this phenomenon, a novel phone synchronous decoding framework is proposed. Here, a phone-level CTC lattice is constructed purely using the CTC acoustic model. The resultant CTC lattice is highly compact and removes tremendous search redundancy due to blank frames. Then, the CTC lattice can be composed with the standard WFST to yield the final decoding result. The proposed approach effectively separates the acoustic evidence calculation and the search operation. This not only significantly improves online search efficiency, but also allows flexible acoustic/linguistic resources to be used. Experiments on LVCSR tasks show that phone synchronous decoding can yield an extra 2–3 times speed up compared to the traditional frame synchronous CTC decoding implementation",
    "checked": true,
    "id": "10628d44077a6bbc8d24b65c7a2529786c7b246c",
    "semantic_title": "phone synchronous decoding with ctc lattice",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sahu16_interspeech.html": {
    "title": "Speech Features for Depression Detection",
    "volume": "main",
    "abstract": "In this paper we discuss speech features that are useful in the detection of depression. Neuro-physiological changes associated with depression affect motor coordination and can disrupt articulatory precision in speech. We use the Mundt database and focus on six speakers in the database that transitioned between being depressed and not depressed based on their Hamilton depression scores. We quantify the degree of breathiness, jitter and shimmer computed from an AMDF based parameter. Measures from sustained vowels spoken in isolation show that all of these attributes can increase when a person is depressed. In this study, we focused on using features from free-flowing speech to classify the depressed state of an individual. To do so we looked at vowel regions that look the most like sustained vowels. We train an SVM for each speaker and do a speaker dependent classification of the test speech frames. Using the AMDF based feature we got a better accuracy (62–87% frame-wise accuracy for 5 out of 6 speakers) for most speakers than 13 dimensional MFCC along with its velocity and acceleration coefficients. Using the AMDF based feature, we also trained a speaker independent SVM which gave an average accuracy of 77.8% for utterance based classification",
    "checked": true,
    "id": "05f9bf206863a101abf03e8fbf5bfc680efcb12b",
    "semantic_title": "speech features for depression detection",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ariasvergara16_interspeech.html": {
    "title": "Parkinson's Disease Progression Assessment from Speech Using GMM-UBM",
    "volume": "main",
    "abstract": "The Gaussian Mixture Model Universal Background Model (GMM-UBM) approach is used to assess the Parkinson's disease (PD) progression per speaker. The disease progression is assessed individually per patient following a user modeling-approach. Voiced and unvoiced segments are extracted and grouped separately to train the models. Additionally, the Bhattacharyya distance is used to estimate the difference between the UBM and the user model. Speech recordings from 62 PD patients (34 male and 28 female) were captured from 2012 to 2015 in four recording sessions. The validation of the models is performed with recordings of 7 patients. All of the patients were diagnosed by a neurologist expert according to the MDS-UPDRS-III scale. The features used to model the speech of the patients are validated by doing a regression based on a Support Vector Regressor (SVR). According to the results, it is possible to track the disease progression with a Pearson's correlation of up to 0.60 with respect to the MDS-UPDRS-III labels",
    "checked": true,
    "id": "beea0037ab24ddf856479f6c8a853192f5f6786d",
    "semantic_title": "parkinson's disease progression assessment from speech using gmm-ubm",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2016/weiner16_interspeech.html": {
    "title": "Speech-Based Detection of Alzheimer's Disease in Conversational German",
    "volume": "main",
    "abstract": "The worldwide population is aging. With a larger population of elderly people, the numbers of people affected by cognitive impairment such as Alzheimer's disease are growing. Unfortunately, there is no known cure for Alzheimer's disease. The only way to alleviate it's serious effects is to start therapy very early before the disease has wrought too much irreversible damage. Current diagnostic procedures are neither cost nor time efficient and therefore do not meet the demands for frequent mass screening required to mitigate the consequences of cognitive impairments on the global scale We present an experiment to detect Alzheimer's disease using spontaneous conversational speech. The speech data was recorded during biographic interviews in the Interdisciplinary Longitudinal Study on Adult Development and Aging (ILSE), a large data resource on healthy and satisfying aging in middle adulthood and later life in Germany. From these recordings we extract ten speech-based features using voice activity detection and transcriptions. In an experimental setup with 98 data samples we train a linear discriminant analysis classifier to distinguish subjects with Alzheimer's disease from the control group. This setup results in an F-score of 0.8 for the detection of Alzheimer's disease, clearly showing our approach detects dementia well",
    "checked": true,
    "id": "9eb018471ead7824f3e093673669a9c088044dce",
    "semantic_title": "speech-based detection of alzheimer's disease in conversational german",
    "citation_count": 63
  },
  "https://www.isca-speech.org/archive/interspeech_2016/alghowinem16_interspeech.html": {
    "title": "Cross-Cultural Depression Recognition from Vocal Biomarkers",
    "volume": "main",
    "abstract": "No studies have investigated cross-cultural and cross-language characteristics of depressed speech. We investigated the generalisability of a vocal biomarker-based approach to depression detection in clinical interviews recorded in three countries (Australia, the USA and Germany), two languages (German and English) and different accents (Australian and American). Several approaches to training and testing within and between datasets were evaluated. Using the same experimental protocol separately within each dataset, (cross-classification) accuracy was high. Combining datasets, high accuracy was high again and consistent across language, recording environment, and culture. Training and testing between datasets, however, attenuated accuracy. These finding emphasize the importance of heterogeneous training sets for robust depression detection",
    "checked": true,
    "id": "f2fcd74d8852c36278999a8164ed00a779dd1909",
    "semantic_title": "cross-cultural depression recognition from vocal biomarkers",
    "citation_count": 38
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhou16_interspeech.html": {
    "title": "Speech Recognition in Alzheimer's Disease and in its Assessment",
    "volume": "main",
    "abstract": "Narrative, spontaneous speech can provide a valuable source of information about an individual's cognitive state. Unfortunately, clinical transcription of this type of data is typically done by hand, which is prohibitively time-consuming. In order to automate the entire process, we optimize automatic speech recognition (ASR) for participants with Alzheimer's disease (AD) in a relatively large clinical database. We extract text features from the resulting transcripts and use these features to identify AD with an SVM classifier. While the accuracy of automatic assessment decreases with increased WER, this is weakly correlated (-0.31). This relative robustness to ASR error is aided by selecting features that are resilient to ASR error",
    "checked": true,
    "id": "90272b8038d334c8587c9a7751097c5882ce3f0c",
    "semantic_title": "speech recognition in alzheimer's disease and in its assessment",
    "citation_count": 55
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pokorny16_interspeech.html": {
    "title": "Does She Speak RTT? Towards an Earlier Identification of Rett Syndrome Through Intelligent Pre-Linguistic Vocalisation Analysis",
    "volume": "main",
    "abstract": "For many years, an apparently normal early development has been regarded as a main characteristic of Rett syndrome (RTT), a severe progressive neurodevelopmental disorder almost exclusively affecting girls/females. The speech-language domain represents a key domain for the clinical diagnosis of RTT, which usually happens around three years of age. Recent studies have built upon the assumption that this domain is already affected in the prodromal period. Aiming to find RTT-specific speech-language atypicalities on signal level as early acoustic markers, we analysed more than 16 hours of home video recordings of 4 girls later diagnosed with RTT and 4 typically developing girls aged 6 to 12 months. We segmented a total of 4 678 pre-linguistic vocalisations. A comprehensive set of acoustic features was extracted from the vocalisations as basis for the classification paradigm RTT versus typical development. A promising mean unweighted recognition accuracy of 76.5% was achieved using linear kernel support vector machines and 4-fold leave-one-speaker-pair-out cross-validation. To the best of our knowledge, this is the first approach to automatically identify infants later diagnosed with RTT based on acoustic characteristics of pre-linguistic vocalisations. Our findings may build the basis for facilitating earlier identification and thus an avenue for an earlier entry into intervention",
    "checked": true,
    "id": "eb65df567535358615fe93778d9728bf727c6583",
    "semantic_title": "does she speak rtt? towards an earlier identification of rett syndrome through intelligent pre-linguistic vocalisation analysis",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pettorino16_interspeech.html": {
    "title": "Speech Rhythm in Parkinson's Disease: A Study on Italian",
    "volume": "main",
    "abstract": "Experimental studies on different languages have shown that neurogenetic disorders connected with Parkinson's disease (PD) determine a series of variations in the speech rhythm. This study aims at verifying whether the speech of PD patients presents rhythmic abnormalities compared to healthy speakers also in Italian. The read speech of 15 healthy speakers and of 11 patients with mild PD was segmented in consonantal and vocalic portions. After extracting the durations of all segments, the vowel percentage (%V) and the interval between two consecutive vowel onset points (VtoV) were calculated. The results show that %V has significantly different values in mildly affected patients as compared to controls. For Italian, %V spans between 44% and 50% for healthy subjects and between 51% and 58% for PD subjects. A positive correlation was found between %V and the number of years of PD since its insurgence. The correlation with the age at which the disease insurges is weak. With regard to VtoV, PD subjects do not speak at a significantly slower rate than healthy controls, though a trend in this direction was found. The data suggest that %V could be used as a more reliable parameter for the early diagnosis of PD than speech rate",
    "checked": true,
    "id": "9b780bf3475a5ef893c1762f36bf0b16030c3dcf",
    "semantic_title": "speech rhythm in parkinson's disease: a study on italian",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2016/anguera16_interspeech.html": {
    "title": "English Language Speech Assistant",
    "volume": "main",
    "abstract": "This show&tell demo presentation showcases ELSA Speak, an app for English Language pronunciation and intonation improvement that uses speech technology to assess the users speech and to offer consistent feedback on the errors the students make",
    "checked": true,
    "id": "fcd3139c1295f242f1ca06c6acecf5f30bff96fe",
    "semantic_title": "english language speech assistant",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2016/guo16_interspeech.html": {
    "title": "Remeeting — Deep Insights to Conversations",
    "volume": "main",
    "abstract": "Remeeting is a cloud service that helps you get insights to (spoken) conversations. Audio and video data such as recorded meetings, online conferences, sales or customer success calls are processed using speaker separation and identification, speech recognition and indexing, and an automated keyword analysis. The resulting annotated \"documents\" can be shared with others and reviewed using a web app that acts as a visual index to the meeting. Furthermore, the extracted metadata is index by a search engine to allow for efficient cross-document search. A powerful query DSL allows the user to make sophisticated queries such as \"what did X say about topic Y in the first quarter of this year\" or \"show me the keywords for all meetings where X and Y attended\". Similar to retrieval, a watchdog can be set to deliver real-time insights to operations. Use cases include productivity in meetings, compliance and policies, real time callcenter analytics and better accessibility of large archives Remeeting is leveraging, promoting and contributing to open source projects including Kaldi, Elasticsearch and Docker",
    "checked": false,
    "id": "60d006c58cb5ee71ae1914a0e14227c26431c3e1",
    "semantic_title": "remeeting - deep insights to conversations",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chan16b_interspeech.html": {
    "title": "SERAPHIM Live! — Singing Synthesis for the Performer, the Composer, and the 3D Game Developer",
    "volume": "main",
    "abstract": "The human singing voice is highly expressive instrument capable of producing a variety of complex timbres. Singing synthesis today is popular amongst composers and studio musicians accessing the technology by means of offline sequencing platforms. Only a couple of singing synthesizers are known to be equipped with both the real-time capability and the user interface to successfully target live performances. These are LIMSI's Cantor Digitalis and Yamaha's VOCALOID Keyboard. However, both systems have their own shortcomings. The former is limited to vowels and does not synthesize complete words or syllables. The latter is only real-time to the syllable level and thus requires specifications of the entire syllable before it commences in the performance. A demand remains for a singing synthesis system that truly solves the problem of real-time synthesis — a system capable of synthesizing both vowels and consonants to form entire words while being capable of synthesizing in real-time to the sub-frame level. Such a system has to be versatile enough to exhaustively present all acoustic options possible to the user for maximal control while being intelligent enough to fill in acoustic details that are too fine for human reflexes to control SERAPHIM is a real-time singing synthesizer developed in answer to this demand. This paper presents the implementation of SERAPHIM for performing musicians and studio musicians, together with how 3D game developers may use Seraphim to deploy singing in their games",
    "checked": false,
    "id": "5d627bf93138d4214561a12a2d906571fd4c3579",
    "semantic_title": "seraphim live! - singing synthesis for the performer, the composer, and the 3d game developer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/malfrere16_interspeech.html": {
    "title": "My-Own-Voice: A Web Service That Allows You to Create a Text-to-Speech Voice From Your Own Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fernald16_interspeech.html": {
    "title": "Talking with Kids Really Matters: Early Language Experience Shapes Later Life Chances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sainath16b_interspeech.html": {
    "title": "Reducing the Computational Complexity of Multimicrophone Acoustic Models with Integrated Feature Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16f_interspeech.html": {
    "title": "Neural Network Adaptive Beamforming for Robust Multichannel Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/erdogan16_interspeech.html": {
    "title": "Improved MVDR Beamforming Using Single-Channel Mask Prediction Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/guerrero16_interspeech.html": {
    "title": "Channel Selection for Distant Speech Recognition Exploiting Cepstral Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mandel16b_interspeech.html": {
    "title": "Multichannel Spatial Clustering for Robust Far-Field Automatic Speech Recognition in Mismatched Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/peddinti16_interspeech.html": {
    "title": "Far-Field ASR Without Parallel Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schuller16_interspeech.html": {
    "title": "The INTERSPEECH 2016 Computational Paralinguistics Challenge: Deception, Sincerity & Native Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schuller16b_interspeech.html": {
    "title": "The Deception Sub-Challenge: The Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/levitan16b_interspeech.html": {
    "title": "Combining Acoustic-Prosodic, Lexical, and Phonotactic Features for Automatic Deception Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/amiriparian16_interspeech.html": {
    "title": "Is Deception Emotional? An Emotion-Driven Predictive Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/montacie16_interspeech.html": {
    "title": "Prosodic Cues and Answer Type Detection for the Deception Sub-Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schuller16c_interspeech.html": {
    "title": "The Sincerity Sub-Challenge: The Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/booth16_interspeech.html": {
    "title": "Automatic Estimation of Perceived Sincerity from Spoken Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gosztolya16b_interspeech.html": {
    "title": "Estimating the Sincerity of Apologies in Speech by DNN Rank Learning and Prosodic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lee16c_interspeech.html": {
    "title": "Minimization of Regression and Ranking Losses with Shallow Neural Networks on Automatic Sincerity Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/herms16_interspeech.html": {
    "title": "Prediction of Deception and Sincerity from Speech Using Automatic Phone Recognition-Based Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16f_interspeech.html": {
    "title": "Sincerity and Deception in Speech: Two Sides of the Same Coin? A Transfer- and Multi-Task Learning Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kaya16_interspeech.html": {
    "title": "Fusing Acoustic Feature Representations for Computational Paralinguistics Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/harte16_interspeech.html": {
    "title": "Introduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/harte16b_interspeech.html": {
    "title": "Poster Overview Presentations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/harte16c_interspeech.html": {
    "title": "Discussion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/harte16d_interspeech.html": {
    "title": "Closing Remarks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/barlier16_interspeech.html": {
    "title": "A Stochastic Model for Computer-Aided Human-Human Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lejeune16_interspeech.html": {
    "title": "Highlighting Psychological Features for Predicting Child Interjections During Story Telling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sun16c_interspeech.html": {
    "title": "Hybrid Dialogue State Tracking for Real World Human-to-Human Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fotedar16_interspeech.html": {
    "title": "Automatic Recognition of Social Roles Using Long Term Role Transitions in Small Group Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/eecke16_interspeech.html": {
    "title": "On the Influence of Gender on Interruptions in Multiparty Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/beaver16_interspeech.html": {
    "title": "Detection of User Escalation in Human-Computer Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/barnaud16_interspeech.html": {
    "title": "Assessing Idiosyncrasies in a Bayesian Model of Speech Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wolters16_interspeech.html": {
    "title": "Prosodic and Linguistic Analysis of Semantic Fluency Data: A Window into Speech Production and Cognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/katz16_interspeech.html": {
    "title": "Sensorimotor Response to Visual Imagery of Tongue Displacement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/caudrelier16_interspeech.html": {
    "title": "Does Auditory-Motor Learning of Speech Transfer from the CV Syllable to the CVCV Word?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schweitzer16b_interspeech.html": {
    "title": "Exemplar Dynamics in Phonetic Convergence of Speech Rate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tuomainen16_interspeech.html": {
    "title": "Articulation Rate in Adverse Listening Conditions in Younger and Older Adults",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/olcoz16_interspeech.html": {
    "title": "Error Correction in Lightly Supervised Alignment of Broadcast Subtitles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/doulaty16_interspeech.html": {
    "title": "Automatic Genre and Show Identification of Broadcast Media",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chao16_interspeech.html": {
    "title": "Speaker-Targeted Audio-Visual Models for Speech Recognition in Cocktail-Party Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/aides16_interspeech.html": {
    "title": "Text-Dependent Audiovisual Synchrony Detection for Spoofing Detection in Mobile Person Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tao16b_interspeech.html": {
    "title": "Improving Boundary Estimation in Audiovisual Speech Activity Detection Using Bayesian Information Criterion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gergen16_interspeech.html": {
    "title": "Dynamic Stream Weighting for Turbo-Decoding-Based Audiovisual ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kruspe16_interspeech.html": {
    "title": "Retrieval of Textual Song Lyrics from Sung Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yuan16b_interspeech.html": {
    "title": "Phoneme, Phone Boundary, and Tone in Automatic Scoring of Mandarin Proficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16k_interspeech.html": {
    "title": "Tone Classification in Mandarin Chinese Using Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pannala16_interspeech.html": {
    "title": "Robust Estimation of Fundamental Frequency Using Single Frequency Filtering Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/daido16_interspeech.html": {
    "title": "A Fast and Accurate Fundamental Frequency Estimator Using Recursive Moving Average Filters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/verma16_interspeech.html": {
    "title": "Frequency Estimation from Waveforms Using Multi-Layered Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sturim16_interspeech.html": {
    "title": "Speaker Linking and Applications Using Non-Parametric Hashing Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lan16_interspeech.html": {
    "title": "Iterative PLDA Adaptation for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dubey16_interspeech.html": {
    "title": "A Speaker Diarization System for Studying Peer-Led Team Learning Groups",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/milner16_interspeech.html": {
    "title": "DNN-Based Speaker Clustering for Speaker Diarisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lapidot16_interspeech.html": {
    "title": "On the Importance of Efficient Transition Modeling for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sell16_interspeech.html": {
    "title": "Priors for Speaker Counting and Diarization with AHC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dawalatabad16_interspeech.html": {
    "title": "Two-Pass IB Based Speaker Diarization System Using Meeting-Specific ANN Based Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/oo16_interspeech.html": {
    "title": "DNN-Based Amplitude and Phase Feature Enhancement for Noise Robust Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/scherhag16_interspeech.html": {
    "title": "Unit-Selection Attack Detection Based on Unfiltered Frequency-Domain Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/monteserin16_interspeech.html": {
    "title": "Investigating the Impact of Dialect Prestige on Lexical Decision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/guo16b_interspeech.html": {
    "title": "Speaker Verification Using Short Utterances with DNN-Based Estimation of Subglottal Acoustic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/su16_interspeech.html": {
    "title": "Factor Analysis Based Speaker Verification Using ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zegers16_interspeech.html": {
    "title": "Joint Sound Source Separation and Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kumar16_interspeech.html": {
    "title": "Robust Multichannel Gender Classification from Speech in Movie Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gonzalvo16_interspeech.html": {
    "title": "Recent Advances in Google Real-Time HMM-Driven Unit Selection Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16e_interspeech.html": {
    "title": "First Step Towards End-to-End Parametric TTS Synthesis: Generating Spectral Parameters with Neural Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wen16b_interspeech.html": {
    "title": "The Parameterized Phoneme Identity Feature as a Continuous Real-Valued Vector for Neural Network Based Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/song16_interspeech.html": {
    "title": "Improved Time-Frequency Trajectory Excitation Vocoder for DNN-Based Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ohtani16_interspeech.html": {
    "title": "Voice Quality Control Using Perceptual Expressions for Statistical Parametric Speech Synthesis Based on Cluster Adaptive Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/espic16_interspeech.html": {
    "title": "Waveform Generation Based on Signal Reshaping for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhao16b_interspeech.html": {
    "title": "Speaker Representations for Speaker Adaptation in Multiple Speakers' BLSTM-RNN-Based Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zen16_interspeech.html": {
    "title": "Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hojo16_interspeech.html": {
    "title": "An Investigation of DNN-Based Speech Synthesis Using Speaker Codes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/juvela16b_interspeech.html": {
    "title": "Using Text and Acoustic Features in Predicting Glottal Excitation Waveforms for Parametric Speech Synthesis with Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tachibana16_interspeech.html": {
    "title": "Model Integration for HMM- and DNN-Based Speech Synthesis Using Product-of-Experts Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/potard16_interspeech.html": {
    "title": "Idlak Tangle: An Open Source Kaldi Based Parametric Speech Synthesiser Based on DNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lazaridis16_interspeech.html": {
    "title": "Probabilistic Amplitude Demodulation Features in Speech Synthesis for Improving Prosody",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chiang16_interspeech.html": {
    "title": "On Smoothing and Enhancing Dynamics of Pitch Contours Represented by Discrete Orthogonal Polynomials for Prosody Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/vadapalli16_interspeech.html": {
    "title": "An Investigation of Recurrent Neural Network Architectures Using Word Embeddings for Phrase Break Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16j_interspeech.html": {
    "title": "Model-Based Parametric Prosody Synthesis with Deep Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/drugman16_interspeech.html": {
    "title": "Active and Semi-Supervised Learning in ASR: Benefits on the Acoustic and Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kuznetsov16_interspeech.html": {
    "title": "Learning N-Gram Language Models from Uncertain Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/oguz16_interspeech.html": {
    "title": "Entropy Based Pruning for Non-Negative Matrix Based Language Models with Contextual Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gangireddy16_interspeech.html": {
    "title": "Unsupervised Adaptation of Recurrent Neural Network Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/halpern16_interspeech.html": {
    "title": "Contextual Prediction Models for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/deena16_interspeech.html": {
    "title": "Combining Feature and Model-Based Adaptation of RNNLMs for Multi-Genre Broadcast Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/brady16_interspeech.html": {
    "title": "A Low Cost Desktop Robot and Tele-Presence Device for Interactive Speech Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/stone16_interspeech.html": {
    "title": "Silent-Speech Command Word Recognition Using Electro-Optical Stomatography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/stanislav16b_interspeech.html": {
    "title": "An Engine for Online Video Search in Large Archives of the Holocaust Testimonies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zmolikova16_interspeech.html": {
    "title": "Data Selection by Sequence Summarizing Neural Network in Mismatch Condition Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kundu16_interspeech.html": {
    "title": "Incorporating a Generative Front-End Layer to Deep Neural Network for Noise Robust Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/markov16_interspeech.html": {
    "title": "Robust Speech Recognition Using Generalized Distillation Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shinohara16b_interspeech.html": {
    "title": "Adversarial Multi-Task Learning of Deep Neural Networks for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/poblete16_interspeech.html": {
    "title": "The Use of Locally Normalized Cepstral Coefficients (LNCC) to Improve Speaker Recognition Accuracy in Highly Reverberant Rooms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hartmann16b_interspeech.html": {
    "title": "Two-Stage Data Augmentation for Low-Resourced Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schuller16d_interspeech.html": {
    "title": "The Native Language Sub-Challenge: The Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/rajpal16_interspeech.html": {
    "title": "Native Language Identification Using Spectral and Source-Based Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jiao16_interspeech.html": {
    "title": "Accent Identification by Combining Deep Neural Networks and Recurrent Neural Networks Trained on Long and Short Term Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/keren16_interspeech.html": {
    "title": "Convolutional Neural Networks with Data Augmentation for Classifying Speakers' Native Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/senoussaoui16_interspeech.html": {
    "title": "Native Language Detection Using the I-Vector Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/huckvale16_interspeech.html": {
    "title": "Within-Speaker Features for Native Language Recognition in the Interspeech 2016 Computational Paralinguistics Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shivakumar16_interspeech.html": {
    "title": "Multimodal Fusion of Multirate Acoustic, Prosodic, and Lexical Speaker Characteristics for Native Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/abad16_interspeech.html": {
    "title": "Exploiting Phone Log-Likelihood Ratio Features for the Detection of the Native Language of Non-Native English Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gosztolya16c_interspeech.html": {
    "title": "Determining Native Language and Deception Using Phonetic Features and Classifier Combination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schuller16e_interspeech.html": {
    "title": "The INTERSPEECH 2016 Computational Paralinguistics Challenge: A Summary of Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schuller16f_interspeech.html": {
    "title": "Discussion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tabain16_interspeech.html": {
    "title": "A Preliminary Ultrasound Study of Nasal and Lateral Coronals in Arrernte",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/toutios16b_interspeech.html": {
    "title": "Illustrating the Production of the International Phonetic Alphabet Sounds Using Fast Real-Time Magnetic Resonance Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/renwick16_interspeech.html": {
    "title": "Marginal Contrast Among Romanian Vowels: Evidence from ASR and Functional Load",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fan16_interspeech.html": {
    "title": "Effects of Subglottal-Coupling and Interdental-Space on Formant Trajectories During Front-to-Back Vowel Transitions in Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/monteserin16b_interspeech.html": {
    "title": "Perceptual Lateralization of Coda Rhotic Production in Puerto Rican Spanish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yi16_interspeech.html": {
    "title": "Interaction Between Lexical Tone and Intonation: An EMA Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ming16_interspeech.html": {
    "title": "Deep Bidirectional LSTM Modeling of Timbre and Prosody for Emotional Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/thangthai16_interspeech.html": {
    "title": "Visual Speech Synthesis Using Dynamic Visemes, Contextual Features and DNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ronanki16_interspeech.html": {
    "title": "A Template-Based Approach for Speech Synthesis Intonation Generation Using LSTMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16g_interspeech.html": {
    "title": "Multi-Language Multi-Speaker Acoustic Modeling for LSTM-RNN Based Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/airaksinen16b_interspeech.html": {
    "title": "GlottDNN — A Full-Band Glottal Vocoder for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nishimura16_interspeech.html": {
    "title": "Singing Voice Synthesis Based on Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/backstrom16_interspeech.html": {
    "title": "Blind Recovery of Perceptual Models in Distributed Speech and Audio Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tang16c_interspeech.html": {
    "title": "Glimpse-Based Metrics for Predicting Speech Intelligibility in Additive Noise Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/koster16_interspeech.html": {
    "title": "Analyzing the Relation Between Overall Quality and the Quality of Individual Phases in a Telephone Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jokinen16_interspeech.html": {
    "title": "Intelligibility Enhancement at the Receiving End of the Speech Transmission System — Effects of Far-End Noise Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ganzeboom16_interspeech.html": {
    "title": "Intelligibility of Disordered Speech: Global and Detailed Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/koutsogiannaki16_interspeech.html": {
    "title": "Modulation Enhancement of Temporal Envelopes for Increasing Speech Intelligibility in Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/niehues16_interspeech.html": {
    "title": "Dynamic Transcription for Low-Latency Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/adams16_interspeech.html": {
    "title": "Learning a Translation Model from Word Lattices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zayats16_interspeech.html": {
    "title": "Disfluency Detection Using a Bidirectional LSTM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/che16_interspeech.html": {
    "title": "Sentence Boundary Detection Based on Parallel Lexical and Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/do16_interspeech.html": {
    "title": "Transferring Emphasis in Speech Translation Using Hard-Attentional Neural Network Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/le16_interspeech.html": {
    "title": "Better Evaluation of ASR in Speech Translation Context Using Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/korse16_interspeech.html": {
    "title": "Entropy Coding of Spectral Envelopes for Speech and Audio Coding Using Distribution Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/villette16_interspeech.html": {
    "title": "An Objective Evaluation Methodology for Blind Bandwidth Extension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ramo16_interspeech.html": {
    "title": "EVS Channel Aware Mode Robustness to Frame Erasures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pirhosseinloo16_interspeech.html": {
    "title": "An Interaural Magnification Algorithm for Enhancement of Naturally-Occurring Level Differences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kayser16_interspeech.html": {
    "title": "Probabilistic Spatial Filter Estimation for Signal Enhancement in Multi-Channel Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ji16_interspeech.html": {
    "title": "Improved a priori SAP Estimator in Complex Noisy Environment for Dual Channel Microphone System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/cheong16_interspeech.html": {
    "title": "A Spectral Modulation Sensitivity Weighted Pre-Emphasis Filter for Active Noise Control System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sreeram16_interspeech.html": {
    "title": "Semi-Coupled Dictionary Based Automatic Bandwidth Extension Approach for Enhancing Children's ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bonada16b_interspeech.html": {
    "title": "Bird Song Synthesis Based on Hidden Markov Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kaewtip16_interspeech.html": {
    "title": "Noise-Robust Hidden Markov Models for Limited Training Data for Within-Species Bird Phrase Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wisler16_interspeech.html": {
    "title": "A Framework for Automated Marmoset Vocalization Detection and Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/aihara16b_interspeech.html": {
    "title": "Call Alternation Between Specific Pairs of Male Frogs Revealed by a Sound-Imaging Method in Their Natural Habitat",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/guyot16_interspeech.html": {
    "title": "Sinusoidal Modelling for Ecoacoustics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/stowell16_interspeech.html": {
    "title": "Individual Identity in Songbirds: Signal Representations and Metric Learning for Locating the Information in Complex Corvid Calls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jancovic16_interspeech.html": {
    "title": "Recognition of Multiple Bird Species Based on Penalised Maximum Likelihood and HMM-Based Modelling of Individual Vocalisation Elements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/maina16_interspeech.html": {
    "title": "Cost Effective Acoustic Monitoring of Bird Species",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kohlsdorf16_interspeech.html": {
    "title": "Feature Learning and Automatic Segmentation for Dolphin Communication Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/suzuki16b_interspeech.html": {
    "title": "Localizing Bird Songs Using an Open Source Robot Audition System with a Microphone Array",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kurth16_interspeech.html": {
    "title": "Robust Detection of Multiple Bioacoustic Events with Repetitive Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/moore16_interspeech.html": {
    "title": "A Real-Time Parametric General-Purpose Mammalian Vocal Synthesiser",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/oreilly16_interspeech.html": {
    "title": "YIN-Bird: Improved Pitch Tracking for Bird Vocalisations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hsu16c_interspeech.html": {
    "title": "Mispronunciation Detection Leveraging Maximum Performance Criterion Training of Acoustic Models and Decision Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/heeman16_interspeech.html": {
    "title": "Using Clinician Annotations to Improve Automatic Speech Recognition of Stuttered Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/xie16d_interspeech.html": {
    "title": "Deep Neural Networks for Voice Quality Assessment Based on the GRBAS Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ward16_interspeech.html": {
    "title": "Automated Screening of Speech Development Issues in Children by Identifying Phonological Error Patterns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lin16_interspeech.html": {
    "title": "Automatic Pronunciation Evaluation of Non-Native Mandarin Tone by Using Multi-Level Confidence Measures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kim16c_interspeech.html": {
    "title": "Dysarthric Speech Recognition Using Kullback-Leibler Divergence-Based Hidden Markov Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/warlaumont16_interspeech.html": {
    "title": "Detection of Total Syllables and Canonical Syllables in Infant Vocalizations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/le16b_interspeech.html": {
    "title": "Improving Automatic Recognition of Aphasic Speech with AphasiaBank",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/laborde16_interspeech.html": {
    "title": "Pronunciation Assessment of Japanese Learners of French with GOP Scores and Phonetic Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/robertson16_interspeech.html": {
    "title": "Pronunciation Error Detection for New Language Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ding16_interspeech.html": {
    "title": "L2 English Rhythm in Read Speech by Chinese Students",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16h_interspeech.html": {
    "title": "Improving the Probabilistic Framework for Representing Dialogue Systems with User Response Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/song16b_interspeech.html": {
    "title": "Dialogue Session Segmentation by Embedding-Enhanced TextTiling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16i_interspeech.html": {
    "title": "Target-Based State and Tracking Algorithm for Spoken Dialogue System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shen16_interspeech.html": {
    "title": "Neural Attention Models for Sequence Classification: Analysis and Application to Key Term Extraction and Dialogue Act Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kumar16b_interspeech.html": {
    "title": "Objective Language Feature Analysis in Children with Neurodevelopmental Disorders During Autism Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/casanueva16_interspeech.html": {
    "title": "Improving Generalisation to New Speakers in Spoken Dialogue State Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tseng16b_interspeech.html": {
    "title": "Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening Comprehension Test by Machine",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ravuri16_interspeech.html": {
    "title": "How Neural Network Depth Compensates for HMM Conditional Independence Assumptions in DNN-HMM Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/palaz16_interspeech.html": {
    "title": "Jointly Learning to Locate and Classify Words Using Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/alvarez16_interspeech.html": {
    "title": "On the Efficient Representation and Execution of Deep Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/povey16_interspeech.html": {
    "title": "Purely Sequence-Trained Neural Networks for ASR Based on Lattice-Free MMI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ratajczak16_interspeech.html": {
    "title": "Virtual Adversarial Training Applied to Neural Higher-Order Factors for Phone Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wong16_interspeech.html": {
    "title": "Sequence Student-Teacher Training of Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hansen16_interspeech.html": {
    "title": "Robustness in Speech, Speaker, and Language Recognition: \"You've Got to Know Your Limitations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jokinen16b_interspeech.html": {
    "title": "The Use of Read versus Conversational Lombard Speech in Spectral Tilt Modeling for Intelligibility Enhancement in Near-End Noise Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sturim16b_interspeech.html": {
    "title": "Corpora for the Evaluation of Robust Speaker Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bertin16_interspeech.html": {
    "title": "A French Corpus for Distant-Microphone Speech Processing in Real Homes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ravanelli16_interspeech.html": {
    "title": "Realistic Multi-Microphone Data Simulation for Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gamper16_interspeech.html": {
    "title": "Synthesis of Device-Independent Noise Corpora for Realistic ASR Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/richardson16_interspeech.html": {
    "title": "Speaker Recognition Using Real vs Synthetic Parallel Data for DNN Channel Compensation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ribas16_interspeech.html": {
    "title": "Discussion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bosch16_interspeech.html": {
    "title": "Combining Data-Oriented and Process-Oriented Approaches to Modeling Reaction Time Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mcauliffe16_interspeech.html": {
    "title": "Do Listeners Learn Better from Natural Speech?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/drozdova16_interspeech.html": {
    "title": "Processing and Adaptation to Ambiguous Sounds during the Course of Perceptual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hintz16_interspeech.html": {
    "title": "The Effect of Background Noise on the Activation of Phonological and Semantic Information During Spoken-Word Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kang16_interspeech.html": {
    "title": "Relationships Between Functional Load and Auditory Confusability Under Different Speech Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kanwal16_interspeech.html": {
    "title": "The Role of Pitch in Punjabi Word Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tahon16_interspeech.html": {
    "title": "Improving TTS with Corpus-Specific Pronunciation Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mousa16_interspeech.html": {
    "title": "Deep Bidirectional Long Short-Term Memory Recurrent Neural Networks for Grapheme-to-Phoneme Conversion Utilizing Complex Many-to-Many Alignments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/esch16_interspeech.html": {
    "title": "Predicting Pronunciations with Syllabification and Stress with Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pouget16_interspeech.html": {
    "title": "Adaptive Latency for Part-of-Speech Tagging in Incremental Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dall16_interspeech.html": {
    "title": "Redefining the Linguistic Context Feature Set for HMM and DNN TTS Through Position and Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16f_interspeech.html": {
    "title": "Enhance the Word Vector with Prosodic Information for the Recurrent Neural Network Based TTS System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jeon16_interspeech.html": {
    "title": "Local Sparsity Based Online Dictionary Learning for Environment-Adaptive Speech Enhancement with Nonnegative Matrix Factorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/papadopoulos16_interspeech.html": {
    "title": "Noise Aware and Combined Noise Models for Speech Denoising in Unknown Noise Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mirsamadi16_interspeech.html": {
    "title": "Causal Speech Enhancement Combining Data-Driven Learning and Suppression Rule Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/brutti16_interspeech.html": {
    "title": "A Phase-Based Time-Frequency Masking for Multi-Channel Speech Enhancement in Domestic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/petkov16b_interspeech.html": {
    "title": "Generalizing Steady State Suppression for Enhanced Intelligibility Under Reverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yamamoto16_interspeech.html": {
    "title": "Speech Intelligibility Prediction Based on the Envelope Power Spectrum Model with the Dynamic Compressive Gammachirp Auditory Filterbank",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kawahara16b_interspeech.html": {
    "title": "Prediction and Generation of Backchannel Form for Attentive Listening Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lunsford16_interspeech.html": {
    "title": "Measuring Turn-Taking Offsets in Human-Human Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/meshorer16_interspeech.html": {
    "title": "Using Past Speaker Behavior to Better Predict Turn Transitions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bailly16_interspeech.html": {
    "title": "Quantitative Analysis of Backchannels Uttered by an Interviewer During Neuropsychological Tests",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chowdhury16_interspeech.html": {
    "title": "Predicting User Satisfaction from Turn-Taking in Spoken Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/oertel16_interspeech.html": {
    "title": "Towards Building an Attentive Artificial Listener: On the Perception of Attentiveness in Feedback Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gwon16_interspeech.html": {
    "title": "Language Recognition via Sparse Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fernando16_interspeech.html": {
    "title": "A Feature Normalisation Technique for PLLR Based Language Identification Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kv16_interspeech.html": {
    "title": "An Investigation of Deep Neural Network Architectures for Language Recognition in Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ali16_interspeech.html": {
    "title": "Automatic Dialect Detection in Arabic Broadcast Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ng16_interspeech.html": {
    "title": "Combining Weak Tokenisers for Phonotactic Language Recognition in a Resource-Constrained Setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/geng16_interspeech.html": {
    "title": "End-to-End Language Identification Using Attention-Based Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sagha16_interspeech.html": {
    "title": "Enhancing Multilingual Recognition of Emotion in Speech by Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mun16_interspeech.html": {
    "title": "Deep Neural Network Bottleneck Features for Acoustic Event Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/origlia16_interspeech.html": {
    "title": "Combining Energy and Cross-Entropy Analysis for Nuclear Segments Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/maas16_interspeech.html": {
    "title": "Anchored Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nandwana16_interspeech.html": {
    "title": "Towards Smart-Cars That Can Listen: Abnormal Acoustic Event Detection on the Road",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/girish16_interspeech.html": {
    "title": "Hierarchical Classification of Speaker and Background Noise and Estimation of SNR Using Sparse Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16g_interspeech.html": {
    "title": "Robust Sound Event Detection in Continuous Audio Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/takahashi16b_interspeech.html": {
    "title": "Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/meier16_interspeech.html": {
    "title": "Artificial Neural Network-Based Feature Combination for Spatial Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kinnunen16b_interspeech.html": {
    "title": "HAPPY Team Entry to NIST OpenSAD Challenge: A Fusion of Short-Term Unsupervised and Segment i-Vector Based Speech Activity Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pokorny16b_interspeech.html": {
    "title": "Manual versus Automated: The Challenging Routine of Infant Vocalisation Segmentation in Home Videos to Study Neuro(mal)development",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ferrer16_interspeech.html": {
    "title": "Minimizing Annotation Effort for Adaptation of Speech-Activity Detection Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/moore16b_interspeech.html": {
    "title": "Progress and Prospects for Spoken Language Technology: What Ordinary People Think",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/moore16c_interspeech.html": {
    "title": "Progress and Prospects for Spoken Language Technology: Results from Four Sexennial Surveys",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/radadia16_interspeech.html": {
    "title": "On Employing a Highly Mismatched Crowd for Speech Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hsiao16_interspeech.html": {
    "title": "Sage: The New BBN Speech Processing Platform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lee16d_interspeech.html": {
    "title": "DNN-Based Feature Enhancement Using Joint Training Framework for Robust Multichannel Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wand16_interspeech.html": {
    "title": "Deep Neural Network Frontend for Continuous EMG-Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/abraham16b_interspeech.html": {
    "title": "Overcoming Data Sparsity in Acoustic Modeling of Low-Resource Language by Borrowing Data and Model Parameters from High-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ragni16_interspeech.html": {
    "title": "Multi-Language Neural Network Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tilk16_interspeech.html": {
    "title": "Bidirectional Recurrent Neural Network with Attention Mechanism for Punctuation Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/enarvi16_interspeech.html": {
    "title": "TheanoLM — An Extensible Toolkit for Neural Network Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lanchantin16_interspeech.html": {
    "title": "Selection of Multi-Genre Broadcast Data for the Training of Automatic Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gaur16_interspeech.html": {
    "title": "Manipulating Word Lattices to Incorporate Human Corrections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fischer16b_interspeech.html": {
    "title": "Context-Aware Restaurant Recommendation for Natural Language Queries: A Formative User Study in the Automotive Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pancoast16_interspeech.html": {
    "title": "Teaming Up: Making the Most of Diverse Representations for a Novel Personalized Speech Retrieval Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mitra16b_interspeech.html": {
    "title": "Automatic Speech Transcription for Low-Resource Languages — The Case of Yoloxóchitl Mixtec (Mexico)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/asadi16_interspeech.html": {
    "title": "Real-Time Presentation Tracking Using Semantic Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wilkinson16_interspeech.html": {
    "title": "Deriving Phonetic Transcriptions and Discovering Word Segmentations for Speech-to-Speech Translation in Low-Resource Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tsujioka16_interspeech.html": {
    "title": "Unsupervised Joint Estimation of Grapheme-to-Phoneme Conversion Systems and Acoustic Model Adaptation for Non-Native Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bruguier16_interspeech.html": {
    "title": "Learning Personalized Pronunciations for Contact Name Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ge16_interspeech.html": {
    "title": "Generation and Pruning of Pronunciation Variants to Improve ASR Accuracy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pylkkonen16_interspeech.html": {
    "title": "Optimizing Speech Recognition Evaluation Using Stratified Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jurafsky16_interspeech.html": {
    "title": "Ketchup, Interdisciplinarity, and the Spread of Innovation in Speech and Language Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/scheffer16_interspeech.html": {
    "title": "Speech Ventures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tong16_interspeech.html": {
    "title": "Context Aware Mispronunciation Detection for Mandarin Pronunciation Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tao16c_interspeech.html": {
    "title": "DNN Online with iVectors Acoustic Modeling and Doc2Vec Distributed Representations for Improving Automated Speech Scoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/qian16_interspeech.html": {
    "title": "Self-Adaptive DNN for Improving Spoken Language Proficiency Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16j_interspeech.html": {
    "title": "Detecting Mispronunciations of L2 Learners and Providing Corrective Feedback Using Knowledge-Guided and Data-Driven Decision Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16g_interspeech.html": {
    "title": "Phoneme Set Design Considering Integrated Acoustic and Linguistic Features of Second Language Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/rasipuram16_interspeech.html": {
    "title": "HMM-Based Non-Native Accent Assessment Using Posterior Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shi16_interspeech.html": {
    "title": "Automatic Assessment and Error Detection of Shadowing Speech: Case of English Spoken by Japanese Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hejna16b_interspeech.html": {
    "title": "Multiplicity of the Acoustic Correlates of the Fortis-Lenis Contrast: Plosives in Aberystwyth English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/adi16_interspeech.html": {
    "title": "Automatic Measurement of Voice Onset Time and Prevoicing Using Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ghosh16_interspeech.html": {
    "title": "L1-L2 Interference: The Case of Final Devoicing of French Voiced Fricatives in Final Position by German Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yanushevskaya16_interspeech.html": {
    "title": "Perceptual Salience of Voice Source Parameters in Signaling Focal Prominence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/borsky16_interspeech.html": {
    "title": "Classification of Voice Modality Using Electroglottogram Waveforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/maekawa16_interspeech.html": {
    "title": "Voice-Quality Difference Between the Vowels in Filled Pauses and Ordinary Lexical Items",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16l_interspeech.html": {
    "title": "Generation of Emotion Control Vector Using MDS-Based Space Transformation for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jauk16_interspeech.html": {
    "title": "Direct Expressive Voice Training Based on Semantic Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ribeiro16_interspeech.html": {
    "title": "Syllable-Level Representations of Suprasegmental Features for DNN-Based Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/braunschweiler16_interspeech.html": {
    "title": "Pause Prediction from Text for Speech Synthesis with User-Definable Pause Insertion Likelihood Threshold",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/do16b_interspeech.html": {
    "title": "A Hybrid System for Continuous Word-Level Emphasis Modeling Based on HMM State Clustering and Adaptive Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zheng16_interspeech.html": {
    "title": "Improving Prosodic Boundaries Prediction for Mandarin Speech Synthesis by Using Enhanced Embedding Feature and Model Fusion Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhao16c_interspeech.html": {
    "title": "Results of The 2015 NIST Language Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lee16e_interspeech.html": {
    "title": "The 2015 NIST Language Recognition Evaluation: The Shared View of I2R, Fantastic4 and SingaMS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lu16c_interspeech.html": {
    "title": "Pair-Wise Distance Metric Learning of Neural Network Model for Spoken Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/travadi16_interspeech.html": {
    "title": "Non-Iterative Parameter Estimation for Total Variability Model Using Randomized Singular Value Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/garciaromero16_interspeech.html": {
    "title": "Stacked Long-Term TDNN for Spoken Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gelly16_interspeech.html": {
    "title": "A Divide-and-Conquer Approach for Language Identification Based on Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/hori16_interspeech.html": {
    "title": "Context-Sensitive and Role-Dependent Spoken Language Understanding Using Bidirectional and Attention LSTMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/vukotic16_interspeech.html": {
    "title": "A Step Beyond Local Observations with a Dialog Aware Bidirectional GRU Network for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16m_interspeech.html": {
    "title": "End-to-End Memory Networks with Knowledge Carryover for Multi-Turn Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/vu16_interspeech.html": {
    "title": "Sequential Convolutional Neural Networks for Slot Filling in Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/celikyilmaz16_interspeech.html": {
    "title": "A New Pre-Training Method for Training Deep Learning Models with Application to Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tafforeau16_interspeech.html": {
    "title": "Joint Syntactic and Semantic Analysis with a Multitask Deep Learning Framework for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16k_interspeech.html": {
    "title": "Exploiting Hidden-Layer Responses of Deep Neural Networks for Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/irtza16_interspeech.html": {
    "title": "Out of Set Language Modelling in Hierarchical Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/masumura16_interspeech.html": {
    "title": "Language Identification Based on Generative Modeling of Posteriorgram Sequences Extracted from Frame-by-Frame DNNs and LSTM-RNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/geng16b_interspeech.html": {
    "title": "Gating Recurrent Enhanced Memory Neural Networks on Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pesan16_interspeech.html": {
    "title": "Sequence Summarizing Neural Networks for Spoken Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kapolowicz16_interspeech.html": {
    "title": "The Role of Spectral Resolution in Foreign-Accented Speech Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/he16c_interspeech.html": {
    "title": "THU-EE System Description for NIST LRE 2015",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/jokinen16c_interspeech.html": {
    "title": "Variation in Spoken North Sami Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16h_interspeech.html": {
    "title": "Improved Music Genre Classification with Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/m16_interspeech.html": {
    "title": "Enhanced Harmonic Content and Vocal Note Based Predominant Melody Extraction from Vocal Polyphonic Music Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chen16n_interspeech.html": {
    "title": "Long Short-Term Memory for Speaker Generalization in Supervised Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kruspe16b_interspeech.html": {
    "title": "Phonotactic Language Identification for Singing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bentsen16_interspeech.html": {
    "title": "Comparing the Influence of Spectro-Temporal Integration in Computational Speech Segregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wood16_interspeech.html": {
    "title": "Blind Speech Separation with GCC-NMF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/montazeri16_interspeech.html": {
    "title": "Effects of Cochlear Hearing Loss on the Benefits of Ideal Binary Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/grais16_interspeech.html": {
    "title": "Combining Mask Estimates for Single Channel Audio Source Separation Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/riday16_interspeech.html": {
    "title": "Monaural Source Separation Using a Random Forest Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16l_interspeech.html": {
    "title": "Adaptive Group Sparsity for Non-Negative Matrix Factorization with Application to Unsupervised Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/guo16c_interspeech.html": {
    "title": "A Robust Dual-Microphone Speech Source Localization Algorithm for Reverberant Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ma16c_interspeech.html": {
    "title": "Speech Localisation in a Multitalker Mixture by Humans and Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sundar16_interspeech.html": {
    "title": "Reverberation-Robust One-Bit TDOA Based Moving Source Localization for Automatic Camera Steering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ochi16b_interspeech.html": {
    "title": "Multi-Talker Speech Recognition Based on Blind Source Separation with ad hoc Microphone Array Using Smartphones and Cloud Storage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fahringer16_interspeech.html": {
    "title": "Phase-Aware Signal Processing for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sailor16_interspeech.html": {
    "title": "Unsupervised Deep Auditory Model Using Stack of Convolutional RBMs for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/weber16_interspeech.html": {
    "title": "Interpretation of Low Dimensional Neural Network Bottleneck Features in Terms of Human Perception and Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16i_interspeech.html": {
    "title": "Compact Feedforward Sequential Memory Networks for Large Vocabulary Continuous Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tang16d_interspeech.html": {
    "title": "Future Context Attention for Unidirectional LSTM Based Acoustic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chien16_interspeech.html": {
    "title": "Hybrid Accelerated Optimization for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chan16c_interspeech.html": {
    "title": "On Online Attention-Based Speech Recognition and Joint Mandarin Character-Pinyin Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gosztolya16d_interspeech.html": {
    "title": "GMM-Free Flat Start Sequence-Discriminative DNN Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/miao16_interspeech.html": {
    "title": "Open-Domain Audio-Visual Speech Recognition: A Deep Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhao16d_interspeech.html": {
    "title": "Multidimensional Residual Learning Based on Recurrent Neural Networks for Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zeyer16_interspeech.html": {
    "title": "Towards Online-Recognition with Deep Bidirectional LSTM Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sercu16_interspeech.html": {
    "title": "Advances in Very Deep Convolutional Neural Networks for LVCSR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ghahremani16_interspeech.html": {
    "title": "Acoustic Modelling from the Signal Domain Using CNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chebotar16_interspeech.html": {
    "title": "Distilling Knowledge from Ensembles of Neural Networks for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16h_interspeech.html": {
    "title": "Triphone State-Tying via Deep Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/luyet16_interspeech.html": {
    "title": "Low-Rank Representation of Nearest Neighbor Posterior Probabilities to Enhance DNN Based Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zheng16b_interspeech.html": {
    "title": "Improving Large Vocabulary Accented Mandarin Speech Recognition with Attribute-Based I-Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shahnawazuddin16_interspeech.html": {
    "title": "Pitch-Adaptive Front-End Features for Robust Children's ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/delagua16_interspeech.html": {
    "title": "ASR Confidence Estimation with Speaker-Adapted Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dharo16_interspeech.html": {
    "title": "Automatic Correction of ASR Outputs by Using Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mallidi16_interspeech.html": {
    "title": "A Framework for Practical Multistream ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/joy16_interspeech.html": {
    "title": "DNNs for Unsupervised Extraction of Pseudo FMLLR Features Without Explicit Adaptation Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/samarakoon16b_interspeech.html": {
    "title": "Multi-Attribute Factorized Hidden Layer Adaptation for DNN Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/goo16_interspeech.html": {
    "title": "Speaker Normalization Through Feature Shifting of Linearly Transformed i-Vector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/diab16_interspeech.html": {
    "title": "Computational Approaches to Linguistic Code Switching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/arisoy16_interspeech.html": {
    "title": "Compositional Neural Network Language Models for Agglutinative Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/damavandi16_interspeech.html": {
    "title": "NN-Grams: Unifying Neural Network and n-Gram Language Models for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/haidar16_interspeech.html": {
    "title": "Recurrent Neural Network Language Model with Incremental Updated Context Information Generated Using Bag-of-Words Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/oualil16_interspeech.html": {
    "title": "Sequential Recurrent Neural Networks for Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/levit16_interspeech.html": {
    "title": "Word-Phrase-Entity Recurrent Neural Networks for Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/irie16_interspeech.html": {
    "title": "LSTM, GRU, Highway and a Bit of Attention: An Empirical Overview for Language Modeling in Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/das16b_interspeech.html": {
    "title": "Automatic Speech Recognition Using Probabilistic Transcriptions in Swahili, Amharic, and Dinka",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gauthier16b_interspeech.html": {
    "title": "Speed Perturbation and Vowel Duration Modeling for ASR in Hausa and Wolof Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/heerden16_interspeech.html": {
    "title": "Improving the Lwazi ASR Baseline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/godard16_interspeech.html": {
    "title": "Preliminary Experiments on Unsupervised Word Discovery in Mboshi",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/vetter16_interspeech.html": {
    "title": "Unsupervised Phoneme Segmentation of Previously Unseen Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/manenti16_interspeech.html": {
    "title": "CNN-Based Phone Segmentation Experiments in a Less-Represented Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/schlunz16_interspeech.html": {
    "title": "Part-of-Speech Tagging and Chunking in Text-to-Speech Synthesis for South African Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/westhuizen16_interspeech.html": {
    "title": "The Effect of Postlexical Deletion on Automatic Speech Recognition in Fast Spontaneously Spoken Zulu",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ramanarayanan16_interspeech.html": {
    "title": "A New Model of Speech Motor Control Based on Task Dynamics and State Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dabbaghchian16_interspeech.html": {
    "title": "Using a Biomechanical Model and Articulatory Data for the Numerical Production of Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wei16_interspeech.html": {
    "title": "A New Model for Acoustic Wave Propagation and Scattering in the Vocal Tract",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/szabados16_interspeech.html": {
    "title": "Uncontrolled Manifolds in Vowel Production: Assessment with a Biomechanical Model of the Tongue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/yoshinaga16_interspeech.html": {
    "title": "Experimental Validation of Sound Generated from Flow in Simplified Vocal Tract Model of Sibilant /s/",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/patri16_interspeech.html": {
    "title": "Bayesian Modeling in Speech Motor Control: A Principled Structure for the Integration of Various Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zhang16j_interspeech.html": {
    "title": "Facing Realism in Spontaneous Emotion Recognition from Speech: Feature Enhancement by Autoencoder with LSTM Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/parthasarathy16_interspeech.html": {
    "title": "Defining Emotionally Salient Regions Using Qualitative Agreement Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ghosh16b_interspeech.html": {
    "title": "Representation Learning for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16m_interspeech.html": {
    "title": "Multilingual Speech Emotion Recognition System Based on a Three-Layer Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kalinli16_interspeech.html": {
    "title": "Analysis of Multi-Lingual Emotion Recognition Using Auditory Attention Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fayek16_interspeech.html": {
    "title": "On the Correlation and Transferability of Features Between Automatic Speech Recognition and Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/valenti16_interspeech.html": {
    "title": "On the Influence of Text Content on Pass-Phrase Strength for Short-Duration Text-Dependent Automatic Speaker Authentication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/todisco16_interspeech.html": {
    "title": "Articulation Rate Filtering of CQCC Features for Automatic Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sadjadi16_interspeech.html": {
    "title": "The IBM Speaker Recognition System: Recent Advances and Error Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kheder16c_interspeech.html": {
    "title": "Probabilistic Approach Using Joint Clean and Noisy i-Vectors Modeling for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bahmaninezhad16_interspeech.html": {
    "title": "Generalized Discriminant Analysis (GDA) for Improved i-Vector Based Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/qian16b_interspeech.html": {
    "title": "Noise and Metadata Sensitive Bottleneck Features for Improving Speaker Recognition with Non-Native Speech Input",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/phan16_interspeech.html": {
    "title": "Robust Audio Event Recognition with 1-Max Pooling Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/karamanolakis16_interspeech.html": {
    "title": "Audio-Based Distributional Representations of Meaning Using a Fusion of Feature Encodings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fujita16_interspeech.html": {
    "title": "Robust DNN-Based VAD Augmented with Phone Entropy Based Rejection of Background Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/zazo16_interspeech.html": {
    "title": "Feature Learning with Raw-Waveform CLDNNs for Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/graciarena16_interspeech.html": {
    "title": "The SRI System for the NIST OpenSAD 2015 Speech Activity Detection Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/karakos16_interspeech.html": {
    "title": "Model Adaptation and Active Learning in the BBN Speech Activity Detection System for the DARPA RATS Program",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mitra16c_interspeech.html": {
    "title": "Fusion Strategies for Robust Speech Recognition and Keyword Spotting for Channel- and Noise-Degraded Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sawada16_interspeech.html": {
    "title": "Recurrent Neural Network-Based Phoneme Sequence Estimation Using Multiple ASR Systems' Outputs for Spoken Term Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kane16_interspeech.html": {
    "title": "Enhancing Data-Driven Phone Confusions Using Restricted Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ni16b_interspeech.html": {
    "title": "Rapid Update of Multilingual Deep Neural Network for Low-Resource Keyword Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/leung16_interspeech.html": {
    "title": "Toward High-Performance Language-Independent Query-by-Example Spoken Term Detection for MediaEval 2015: Post-Evaluation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/soni16b_interspeech.html": {
    "title": "Novel Subband Autoencoder Features for Non-Intrusive Quality Assessment of Noise Suppressed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/gao16_interspeech.html": {
    "title": "SNR-Based Progressive Learning of Deep Neural Network for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/sadasivan16_interspeech.html": {
    "title": "A Novel Risk-Estimation-Theoretic Framework for Speech Enhancement in Nonstationary and Non-Gaussian Noise Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/samui16_interspeech.html": {
    "title": "Two-Stage Temporal Processing for Single-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/pm16_interspeech.html": {
    "title": "A Class-Specific Speech Enhancement for Phoneme Recognition: A Dictionary Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/ogawa16_interspeech.html": {
    "title": "Robust Example Search Using Bottleneck Features for Example-Based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kumar16c_interspeech.html": {
    "title": "Speech Enhancement in Multiple-Noise Conditions Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/shivakumar16b_interspeech.html": {
    "title": "Perception Optimized Deep Denoising AutoEncoders for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kato16_interspeech.html": {
    "title": "HMM-Based Speech Enhancement Using Sub-Word Models and Noise Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16n_interspeech.html": {
    "title": "Semi-Supervised Joint Enhancement of Spectral and Cepstral Sequences of Noisy Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/chinaev16_interspeech.html": {
    "title": "A priori SNR Estimation Using a Generalized Decision Directed Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/wang16i_interspeech.html": {
    "title": "A DNN-HMM Approach to Non-Negative Matrix Factorization Based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fu16_interspeech.html": {
    "title": "SNR-Aware Convolutional Neural Network Modeling for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/li16o_interspeech.html": {
    "title": "An Iterative Phase Recovery Framework with Phase Mask for Spectral Mapping with an Application to Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16k_interspeech.html": {
    "title": "A Novel Research to Artificial Bandwidth Extension Based on Deep BLSTM Recurrent Neural Networks and Exemplar-Based Sparse Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mitra16d_interspeech.html": {
    "title": "Coping with Unseen Data Conditions: Investigating Neural Net Architectures, Robust Features, and Information Fusion for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tomashenko16_interspeech.html": {
    "title": "On the Use of Gaussian Mixture Model Framework to Improve Speaker Adaptation of Deep Neural Network Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/bosch16b_interspeech.html": {
    "title": "Analytical Assessment of Dual-Stream Merging for Noise-Robust ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/loweimi16_interspeech.html": {
    "title": "Use of Generalised Nonlinearity in Vector Taylor Series Noise Compensation for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/mimura16_interspeech.html": {
    "title": "Joint Optimization of Denoising Autoencoder and DNN Acoustic Model Based on Multi-Target Learning for Noisy Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/higuchi16_interspeech.html": {
    "title": "Optimization of Speech Enhancement Front-End with Speech Recognition-Level Criterion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/tran16_interspeech.html": {
    "title": "Factorized Linear Input Network for Acoustic Model Adaptation in Noisy Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/fujita16b_interspeech.html": {
    "title": "Data Augmentation Using Multi-Input Multi-Output Source Separation for Deep Neural Network Based Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/prasad16_interspeech.html": {
    "title": "Microphone Distance Adaptation Using Cluster Adaptive Training for Robust Far Field Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/dimitriadis16_interspeech.html": {
    "title": "An Investigation on the Use of i-Vectors for Robust ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/liu16l_interspeech.html": {
    "title": "The Sheffield Wargame Corpus — Day Two and Day Three",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/kim16d_interspeech.html": {
    "title": "Recurrent Models for Auditory Attention in Multi-Microphone Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/lee16f_interspeech.html": {
    "title": "Semi-Supervised Speaker Adaptation for In-Vehicle Speech Recognition with Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/huang16e_interspeech.html": {
    "title": "Semi-Supervised Training in Deep Learning Acoustic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/thomas16_interspeech.html": {
    "title": "Multilingual Data Selection for Low Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/das16c_interspeech.html": {
    "title": "An Investigation on Training Deep Neural Networks Using Probabilistic Transcriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/do16c_interspeech.html": {
    "title": "Analysis of Mismatched Transcriptions Generated by Humans and Machines for Under-Resourced Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/nouza16_interspeech.html": {
    "title": "ASR for South Slavic Languages Developed in Almost Automated Way",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/razavi16_interspeech.html": {
    "title": "Improving Under-Resourced Language ASR Through Latent Subword Unit Space Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/muller16_interspeech.html": {
    "title": "Language Adaptive DNNs for Improved Low Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2016/alumae16_interspeech.html": {
    "title": "Improved Multilingual Training of Stacked Neural Network Acoustic Models for Low Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}