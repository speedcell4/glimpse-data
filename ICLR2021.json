{
  "https://openreview.net/forum?id=RGJbergVIoO": {
    "title": "On the mapping between Hopfield networks and Restricted Boltzmann Machines",
    "volume": "oral",
    "abstract": "Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two important models at the interface of statistical physics, machine learning, and neuroscience. Recently, there has been interest in the relationship between HNs and RBMs, due to their similarity under the statistical mechanics formalism. An exact mapping between HNs and RBMs has been previously noted for the special case of orthogonal (\"uncorrelated\") encoded patterns. We present here an exact mapping in the case of correlated pattern HNs, which are more broadly applicable to existing datasets. Specifically, we show that any HN with $N$ binary variables and $p<N$ potentially correlated binary patterns can be transformed into an RBM with $N$ binary visible variables and $p$ gaussian hidden variables. We outline the conditions under which the reverse mapping exists, and conduct experiments on the MNIST dataset which suggest the mapping provides a useful initialization to the RBM weights. We discuss extensions, the potential importance of this correspondence for the training of RBMs, and for understanding the performance of feature extraction methods which utilize RBMs",
    "checked": true,
    "id": "593d8f1de69f6f335609e1f0e6b3ae83ce4e3116",
    "semantic_title": "on the mapping between hopfield networks and restricted boltzmann machines",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Mos9F9kDwkz": {
    "title": "Complex Query Answering with Neural Link Predictors",
    "volume": "oral",
    "abstract": "Neural link predictors are immensely useful for identifying missing edges in large scale Knowledge Graphs. However, it is still not clear how to use these models for answering more complex queries that arise in a number of domains, such as queries using logical conjunctions ($\\land$), disjunctions ($\\lor$) and existential quantifiers ($\\exists$), while accounting for missing edges. In this work, we propose a framework for efficiently answering complex queries on incomplete Knowledge Graphs. We translate each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor. We then analyse two solutions to the optimisation problem, including gradient-based and combinatorial search. In our experiments, the proposed approach produces more accurate results than state-of-the-art methods --- black-box neural models trained on millions of generated queries --- without the need of training on a large and diverse set of complex queries. Using orders of magnitude less training data, we obtain relative improvements ranging from 8% up to 40% in Hits@3 across different knowledge graphs containing factual information. Finally, we demonstrate that it is possible to explain the outcome of our model in terms of the intermediate solutions identified for each of the complex query atoms. All our source code and datasets are available online, at https://github.com/uclnlp/cqd",
    "checked": true,
    "id": "62ba6604ed24c808062f89f8f41eda3b6a566917",
    "semantic_title": "complex query answering with neural link predictors",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=Wj4ODo0uyCF": {
    "title": "Share or Not? Learning to Schedule Language-Specific Capacity for Multilingual Translation",
    "volume": "oral",
    "abstract": "Using a mix of shared and language-specific (LS) parameters has shown promise in multilingual neural machine translation (MNMT), but the question of when and where LS capacity matters most is still under-studied. We offer such a study by proposing conditional language-specific routing (CLSR). CLSR employs hard binary gates conditioned on token representations to dynamically select LS or shared paths. By manipulating these gates, it can schedule LS capacity across sub-layers in MNMT subject to the guidance of translation signals and budget constraints. Moreover, CLSR can easily scale up to massively multilingual settings. Experiments with Transformer on OPUS-100 and WMT datasets show that: 1) MNMT is sensitive to both the amount and the position of LS modeling: distributing 10%-30% LS computation to the top and/or bottom encoder/decoder layers delivers the best performance; and 2) one-to-many translation benefits more from CLSR compared to many-to-one translation, particularly with unbalanced training data. Our study further verifies the trade-off between the shared capacity and LS capacity for multilingual translation. We corroborate our analysis by confirming the soundness of our findings as foundation of our improved multilingual Transformers. Source code and models are available at https://github.com/bzhangGo/zero/tree/iclr2021_clsr",
    "checked": true,
    "id": "940214c9afde6664127ba552de2ac8fa67997769",
    "semantic_title": "share or not? learning to schedule language-specific capacity for multilingual translation",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=rsf1z-JSj87": {
    "title": "End-to-end Adversarial Text-to-Speech",
    "volume": "oral",
    "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision",
    "checked": true,
    "id": "c035cf0e1231eb196968d7255ab55827e932ec7a",
    "semantic_title": "end-to-end adversarial text-to-speech",
    "citation_count": 190,
    "authors": []
  },
  "https://openreview.net/forum?id=gvxJzw8kW4b": {
    "title": "Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity",
    "volume": "oral",
    "abstract": "While deep neural networks show great performance on fitting to the training distribution, improving the networks' generalization performance to the test distribution and robustness to the sensitivity to input perturbations still remain as a challenge. Although a number of mixup based augmentation strategies have been proposed to partially address them, it remains unclear as to how to best utilize the supervisory signal within each input data for mixup from the optimization perspective. We propose a new perspective on batch mixup and formulate the optimal construction of a batch of mixup data maximizing the data saliency measure of each individual mixup data and encouraging the supermodular diversity among the constructed mixup data. This leads to a novel discrete optimization problem minimizing the difference between submodular functions. We also propose an efficient modular approximation based iterative submodular minimization algorithm for efficient mixup computation per each minibatch suitable for minibatch based neural network training. Our experiments show the proposed method achieves the state of the art generalization, calibration, and weakly supervised localization results compared to other mixup methods. The source code is available at https://github.com/snu-mllab/Co-Mixup",
    "checked": true,
    "id": "39caa9091318480f3241117ecefba897548cc42a",
    "semantic_title": "co-mixup: saliency guided joint mixup with supermodular diversity",
    "citation_count": 187,
    "authors": []
  },
  "https://openreview.net/forum?id=Ud3DSz72nYR": {
    "title": "Contrastive Explanations for Reinforcement Learning via Embedded Self Predictions",
    "volume": "oral",
    "abstract": "We investigate a deep reinforcement learning (RL) architecture that supports explaining why a learned agent prefers one action over another. The key idea is to learn action-values that are directly represented via human-understandable properties of expected futures. This is realized via the embedded self-prediction (ESP) model, which learns said properties in terms of human provided features. Action preferences can then be explained by contrasting the future properties predicted for each action. To address cases where there are a large number of features, we develop a novel method for computing minimal sufficient explanations from an ESP. Our case studies in three domains, including a complex strategy game, show that ESP models can be effectively learned and support insightful explanations",
    "checked": true,
    "id": "2e0b1b6ac7a48e1c992cb26967c7272c5e798757",
    "semantic_title": "contrastive explanations for reinforcement learning via embedded self predictions",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=uCY5MuAxcxU": {
    "title": "Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?",
    "volume": "oral",
    "abstract": "Convolutional neural networks often dominate fully-connected counterparts in generalization performance, especially on image classification tasks. This is often explained in terms of \\textquotedblleft better inductive bias.\\textquotedblright\\ However, this has not been made mathematically rigorous, and the hurdle is that the sufficiently wide fully-connected net can always simulate the convolutional net. Thus the training algorithm plays a role. The current work describes a natural task on which a provable sample complexity gap can be shown, for standard training algorithms. We construct a single natural distribution on $\\mathbb{R}^d\\times\\{\\pm 1\\}$ on which any orthogonal-invariant algorithm (i.e. fully-connected networks trained with most gradient-based methods from gaussian initialization) requires $\\Omega(d^2)$ samples to generalize while $O(1)$ samples suffice for convolutional architectures. Furthermore, we demonstrate a single target function, learning which on all possible distributions leads to an $O(1)$ vs $\\Omega(d^2/\\varepsilon)$ gap. The proof relies on the fact that SGD on fully-connected network is orthogonal equivariant. Similar results are achieved for $\\ell_2$ regression and adaptive training algorithms, e.g. Adam and AdaGrad, which are only permutation equivariant",
    "checked": true,
    "id": "54f105cd0ccd999b9aa01636edb736489a24718c",
    "semantic_title": "why are convolutional nets more sample-efficient than fully-connected nets?",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=Pd_oMxH8IlF": {
    "title": "Iterated learning for emergent systematicity in VQA",
    "volume": "oral",
    "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR",
    "checked": true,
    "id": "268d4feafa9532329f3e744adb0cdece8bbc80a4",
    "semantic_title": "iterated learning for emergent systematicity in vqa",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=tW4QEInpni": {
    "title": "When Do Curricula Work?",
    "volume": "oral",
    "abstract": "Inspired by human learning, researchers have proposed ordering examples during training based on their difficulty. Both curriculum learning, exposing a network to easier examples early in training, and anti-curriculum learning, showing the most difficult examples first, have been suggested as improvements to the standard i.i.d. training. In this work, we set out to investigate the relative benefits of ordered learning. We first investigate the implicit curricula resulting from architectural and optimization bias and find that samples are learned in a highly consistent order. Next, to quantify the benefit of explicit curricula, we conduct extensive experiments over thousands of orderings spanning three kinds of learning: curriculum, anti-curriculum, and random-curriculum -- in which the size of the training dataset is dynamically increased over time, but the examples are randomly ordered. We find that for standard benchmark datasets, curricula have only marginal benefits, and that randomly ordered samples perform as well or better than curricula and anti-curricula, suggesting that any benefit is entirely due to the dynamic training set size. Inspired by common use cases of curriculum learning in practice, we investigate the role of limited training time budget and noisy data in the success of curriculum learning. Our experiments demonstrate that curriculum, but not anti-curriculum or random ordering can indeed improve the performance either with limited training time budget or in the existence of noisy data",
    "checked": true,
    "id": "9d2c96574019305a8c86cc5b84cb9f616ccf0eb3",
    "semantic_title": "when do curricula work?",
    "citation_count": 121,
    "authors": []
  },
  "https://openreview.net/forum?id=m5Qsh0kBQG": {
    "title": "Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients",
    "volume": "oral",
    "abstract": "Discovering the underlying mathematical expressions describing a dataset is a core challenge for artificial intelligence. This is the problem of $\\textit{symbolic regression}$. Despite recent advances in training neural networks to solve complex tasks, deep learning approaches to symbolic regression are underexplored. We propose a framework that leverages deep learning for symbolic regression via a simple idea: use a large model to search the space of small models. Specifically, we use a recurrent neural network to emit a distribution over tractable mathematical expressions and employ a novel risk-seeking policy gradient to train the network to generate better-fitting expressions. Our algorithm outperforms several baseline methods (including Eureqa, the gold standard for symbolic regression) in its ability to exactly recover symbolic expressions on a series of benchmark problems, both with and without added noise. More broadly, our contributions include a framework that can be applied to optimize hierarchical, variable-length objects under a black-box performance metric, with the ability to incorporate constraints in situ, and a risk-seeking policy gradient formulation that optimizes for best-case performance instead of expected performance",
    "checked": true,
    "id": "3ee45877f7f14c8ee4872a7249f74eef7dc2255f",
    "semantic_title": "deep symbolic regression: recovering mathematical expressions from data via risk-seeking policy gradients",
    "citation_count": 334,
    "authors": []
  },
  "https://openreview.net/forum?id=rJA5Pz7lHKb": {
    "title": "Improved Autoregressive Modeling with Distribution Smoothing",
    "volume": "oral",
    "abstract": "While autoregressive models excel at image compression, their sample quality is often lacking. Although not realistic, generated images often have high likelihood according to the model, resembling the case of adversarial examples. Inspired by a successful adversarial defense method, we incorporate randomized smoothing into autoregressive generative modeling. We first model a smoothed version of the data distribution, and then reverse the smoothing process to recover the original data distribution. This procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world image datasets while obtaining competitive likelihoods on synthetic datasets",
    "checked": true,
    "id": "0649abe307a6d76e3dfd78382aa224b82286ebe8",
    "semantic_title": "improved autoregressive modeling with distribution smoothing",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=PxTIG12RRHS": {
    "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
    "volume": "oral",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model",
    "checked": true,
    "id": "633e2fbfc0b21e959a244100937c5853afca4853",
    "semantic_title": "score-based generative modeling through stochastic differential equations",
    "citation_count": 7012,
    "authors": []
  },
  "https://openreview.net/forum?id=KvyxFqZS_D": {
    "title": "Global Convergence of Three-layer Neural Networks in the Mean Field Regime",
    "volume": "oral",
    "abstract": "In the mean field regime, neural networks are appropriately scaled so that as the width tends to infinity, the learning dynamics tends to a nonlinear and nontrivial dynamical limit, known as the mean field limit. This lends a way to study large-width neural networks via analyzing the mean field limit. Recent works have successfully applied such analysis to two-layer networks and provided global convergence guarantees. The extension to multilayer ones however has been a highly challenging puzzle, and little is known about the optimization efficiency in the mean field regime when there are more than two layers. In this work, we prove a global convergence result for unregularized feedforward three-layer networks in the mean field regime. We first develop a rigorous framework to establish the mean field limit of three-layer networks under stochastic gradient descent training. To that end, we propose the idea of a neuronal embedding, which comprises of a fixed probability space that encapsulates neural networks of arbitrary sizes. The identified mean field limit is then used to prove a global convergence guarantee under suitable regularity and convergence mode assumptions, which – unlike previous works on two-layer networks – does not rely critically on convexity. Underlying the result is a universal approximation property, natural of neural networks, which importantly is shown to hold at any finite training time (not necessarily at convergence) via an algebraic topology argument",
    "checked": true,
    "id": "13d2a545707741d6a7ac2679c5cf0a2a4821bab1",
    "semantic_title": "global convergence of three-layer neural networks in the mean field regime",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=PKubaeJkw3": {
    "title": "Rethinking Architecture Selection in Differentiable NAS",
    "volume": "oral",
    "abstract": "Differentiable Neural Architecture Search is one of the most popular Neural Architecture Search (NAS) methods for its search efficiency and simplicity, accomplished by jointly optimizing the model weight and architecture parameters in a weight-sharing supernet via gradient-based algorithms. At the end of the search phase, the operations with the largest architecture parameters will be selected to form the final architecture, with the implicit assumption that the values of architecture parameters reflect the operation strength. While much has been discussed about the supernet's optimization, the architecture selection process has received little attention. We provide empirical and theoretical analysis to show that the magnitude of architecture parameters does not necessarily indicate how much the operation contributes to the supernet's performance. We propose an alternative perturbation-based architecture selection that directly measures each operation's influence on the supernet. We re-evaluate several differentiable NAS methods with the proposed architecture selection and find that it is able to extract significantly improved architectures from the underlying supernets consistently. Furthermore, we find that several failure modes of DARTS can be greatly alleviated with the proposed selection method, indicating that much of the poor generalization observed in DARTS can be attributed to the failure of magnitude-based architecture selection rather than entirely the optimization of its supernet",
    "checked": true,
    "id": "958cc02ad685721d64e6321aa93e3125884ed253",
    "semantic_title": "rethinking architecture selection in differentiable nas",
    "citation_count": 181,
    "authors": []
  },
  "https://openreview.net/forum?id=0XXpJ4OtjW": {
    "title": "Evolving Reinforcement Learning Algorithms",
    "volume": "oral",
    "abstract": "We propose a method for meta-learning reinforcement learning algorithms by searching over the space of computational graphs which compute the loss function for a value-based model-free RL agent to optimize. The learned algorithms are domain-agnostic and can generalize to new environments not seen during training. Our method can both learn from scratch and bootstrap off known existing algorithms, like DQN, enabling interpretable modifications which improve performance. Learning from scratch on simple classical control and gridworld tasks, our method rediscovers the temporal-difference (TD) algorithm. Bootstrapped from DQN, we highlight two learned algorithms which obtain good generalization performance over other classical control tasks, gridworld type tasks, and Atari games. The analysis of the learned algorithm behavior shows resemblance to recently proposed RL algorithms that address overestimation in value-based methods",
    "checked": true,
    "id": "431e873121643a3e02eeebc4efe95920a3396d7a",
    "semantic_title": "evolving reinforcement learning algorithms",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=yWkP7JuHX1": {
    "title": "Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering",
    "volume": "oral",
    "abstract": "Differentiable rendering has paved the way to training neural networks to perform \"inverse graphics\" tasks such as predicting 3D geometry from monocular photographs. To train high performing models, most of the current approaches rely on multi-view imagery which are not readily available in practice. Recent Generative Adversarial Networks (GANs) that synthesize images, in contrast, seem to acquire 3D knowledge implicitly during training: object viewpoints can be manipulated by simply manipulating the latent codes. However, these latent codes often lack further physical interpretation and thus GANs cannot easily be inverted to perform explicit 3D reasoning. In this paper, we aim to extract and disentangle 3D knowledge learned by generative models by utilizing differentiable renderers. Key to our approach is to exploit GANs as a multi-view data generator to train an inverse graphics network using an off-the-shelf differentiable renderer, and the trained inverse graphics network as a teacher to disentangle the GAN's latent code into interpretable 3D properties. The entire architecture is trained iteratively using cycle consistency losses. We show that our approach significantly outperforms state-of-the-art inverse graphics networks trained on existing datasets, both quantitatively and via user studies. We further showcase the disentangled GAN as a controllable 3D \"neural renderer\", complementing traditional graphics renderers",
    "checked": true,
    "id": "288e170e5771200653b2f65d4837b74295b2c258",
    "semantic_title": "image gans meet differentiable rendering for inverse graphics and interpretable 3d neural rendering",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=wWK7yXkULyh": {
    "title": "MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training",
    "volume": "oral",
    "abstract": "Recent advances by practitioners in the deep learning community have breathed new life into Locality Sensitive Hashing (LSH), using it to reduce memory and time bottlenecks in neural network (NN) training. However, while LSH has sub-linear guarantees for approximate near-neighbor search in theory, it is known to have inefficient query time in practice due to its use of random hash functions. Moreover, when model parameters are changing, LSH suffers from update overhead. This work is motivated by an observation that model parameters evolve slowly, such that the changes do not always require an LSH update to maintain performance. This phenomenon points to the potential for a reduction in update time and allows for a modified learnable version of data-dependent LSH to improve query time at a low cost. We use the above insights to build MONGOOSE, an end-to-end LSH framework for efficient NN training. In particular, MONGOOSE is equipped with a scheduling algorithm to adaptively perform LSH updates with provable guarantees and learnable hash functions to improve query efficiency. Empirically, we validate MONGOOSE on large-scale deep learning models for recommendation systems and language modeling. We find that it achieves up to 8% better accuracy compared to previous LSH approaches, with $6.5 \\times$ speed-up and $6\\times$ reduction in memory usage",
    "checked": true,
    "id": "b745b5512ad3b1f652dc0cbb5ddf5a940f397f7d",
    "semantic_title": "mongoose: a learnable lsh framework for efficient neural network training",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=Ysuv-WOFeKR": {
    "title": "Parrot: Data-Driven Behavioral Priors for Reinforcement Learning",
    "volume": "oral",
    "abstract": "Reinforcement learning provides a general framework for flexible decision making and control, but requires extensive data collection for each new task that an agent needs to learn. In other machine learning fields, such as natural language processing or computer vision, pre-training on large, previously collected datasets to bootstrap learning for new tasks has emerged as a powerful paradigm to reduce data requirements when learning a new task. In this paper, we ask the following question: how can we enable similarly useful pre-training for RL agents? We propose a method for pre-training behavioral priors that can capture complex input-output relationships observed in successful trials from a wide range of previously seen tasks, and we show how this learned prior can be used for rapidly learning new tasks without impeding the RL agent's ability to try out novel behaviors. We demonstrate the effectiveness of our approach in challenging robotic manipulation domains involving image observations and sparse reward functions, where our method outperforms prior works by a substantial margin. Additional materials can be found on our project website: https://sites.google.com/view/parrot-rl",
    "checked": true,
    "id": "f5275f5eb6569ddb5ba9a959ede09875d56e3bac",
    "semantic_title": "parrot: data-driven behavioral priors for reinforcement learning",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=DktZb97_Fx": {
    "title": "SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness",
    "volume": "oral",
    "abstract": "In this paper, we cast fair machine learning as invariant machine learning. We first formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias",
    "checked": true,
    "id": "2c0f18df2f23207f2139140ab45819b0b23879be",
    "semantic_title": "sensei: sensitive set invariance for enforcing individual fairness",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=rC8sJ4i6kaH": {
    "title": "Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data",
    "volume": "oral",
    "abstract": "Self-training algorithms, which train a model to fit pseudolabels predicted by another previously-learned model, have been very successful for learning with unlabeled data using neural networks. However, the current theoretical understanding of self-training only applies to linear models. This work provides a unified theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervised learning. At the core of our analysis is a simple but realistic \"expansion\" assumption, which states that a low-probability subset of the data must expand to a neighborhood with large probability relative to the subset. We also assume that neighborhoods of examples in different classes have minimal overlap. We prove that under these assumptions, the minimizers of population objectives based on self-training and input-consistency regularization will achieve high accuracy with respect to ground-truth labels. By using off-the-shelf generalization bounds, we immediately convert this result to sample complexity guarantees for neural nets that are polynomial in the margin and Lipschitzness. Our results help explain the empirical successes of recently proposed self-training algorithms which use input consistency regularization",
    "checked": true,
    "id": "5a41200eeee536e101b6e462014e7396f4841c28",
    "semantic_title": "theoretical analysis of self-training with deep networks on unlabeled data",
    "citation_count": 235,
    "authors": []
  },
  "https://openreview.net/forum?id=wb3wxCObbRT": {
    "title": "Growing Efficient Deep Networks by Structured Continuous Sparsification",
    "volume": "oral",
    "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives. Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters. By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training. For example, we achieve $49.7\\%$ inference FLOPs and $47.4\\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\\%$ top-1 validation accuracy --- all without any dedicated fine-tuning stage. Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods",
    "checked": true,
    "id": "910c8790bc7eb7bb0c66fa470f243c1cf2626119",
    "semantic_title": "growing efficient deep networks by structured continuous sparsification",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=RmB-88r9dL": {
    "title": "VCNet and Functional Targeted Regularization For Learning Causal Effects of Continuous Treatments",
    "volume": "oral",
    "abstract": "Motivated by the rising abundance of observational data with continuous treatments, we investigate the problem of estimating the average dose-response curve (ADRF). Available parametric methods are limited in their model space, and previous attempts in leveraging neural network to enhance model expressiveness relied on partitioning continuous treatment into blocks and using separate heads for each block; this however produces in practice discontinuous ADRFs. Therefore, the question of how to adapt the structure and training of neural network to estimate ADRFs remains open. This paper makes two important contributions. First, we propose a novel varying coefficient neural network (VCNet) that improves model expressiveness while preserving continuity of the estimated ADRF. Second, to improve finite sample performance, we generalize targeted regularization to obtain a doubly robust estimator of the whole ADRF curve",
    "checked": true,
    "id": "7bb7b8406d5aa3149adf121dc257def529e59cf8",
    "semantic_title": "vcnet and functional targeted regularization for learning causal effects of continuous treatments",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=NzTU59SYbNq": {
    "title": "EigenGame: PCA as a Nash Equilibrium",
    "volume": "oral",
    "abstract": "We present a novel view on principal components analysis as a competitive game in which each approximate eigenvector is controlled by a player whose goal is to maximize their own utility function. We analyze the properties of this PCA game and the behavior of its gradient based updates. The resulting algorithm---which combines elements from Oja's rule with a generalized Gram-Schmidt orthogonalization---is naturally decentralized and hence parallelizable through message passing. We demonstrate the scalability of the algorithm with experiments on large image datasets and neural network activations. We discuss how this new view of PCA as a differentiable game can lead to further algorithmic developments and insights",
    "checked": true,
    "id": "17281d4de91cdd5c480f6f35a5e1a33b7abb18d2",
    "semantic_title": "eigengame: pca as a nash equilibrium",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=xpx9zj7CUlY": {
    "title": "Randomized Automatic Differentiation",
    "volume": "oral",
    "abstract": "The successes of deep learning, variational inference, and many other fields have been aided by specialized implementations of reverse-mode automatic differentiation (AD) to compute gradients of mega-dimensional objectives. The AD techniques underlying these tools were designed to compute exact gradients to numerical precision, but modern machine learning models are almost always trained with stochastic gradient descent. Why spend computation and memory on exact (minibatch) gradients only to use them for stochastic optimization? We develop a general framework and approach for randomized automatic differentiation (RAD), which can allow unbiased gradient estimates to be computed with reduced memory in return for variance. We examine limitations of the general approach, and argue that we must leverage problem specific structure to realize benefits. We develop RAD techniques for a variety of simple neural network architectures, and show that for a fixed memory budget, RAD converges in fewer iterations than using a small batch size for feedforward networks, and in a similar number for recurrent networks. We also show that RAD can be applied to scientific computing, and use it to develop a low-memory stochastic gradient method for optimizing the control parameters of a linear reaction-diffusion PDE representing a fission reactor",
    "checked": true,
    "id": "766bd9633ea0ed0ee2d8cc11ab083322666a3db9",
    "semantic_title": "randomized automatic differentiation",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=jWkw45-9AbL": {
    "title": "A Distributional Approach to Controlled Text Generation",
    "volume": "oral",
    "abstract": "We propose a Distributional Approach for addressing Controlled Text Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both \"pointwise'\" and \"distributional\" constraints over the target LM — to our knowledge, the first model with such generality —while minimizing KL divergence from the initial LM distribution. The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient. We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM. We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models. Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence. Code available at https://github.com/naver/gdc",
    "checked": true,
    "id": "07fd366a8ebdefe54cdb57d87c81dcd22de25a91",
    "semantic_title": "a distributional approach to controlled text generation",
    "citation_count": 120,
    "authors": []
  },
  "https://openreview.net/forum?id=YicbFdNTTy": {
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "volume": "oral",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train",
    "checked": true,
    "id": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
    "semantic_title": "an image is worth 16x16 words: transformers for image recognition at scale",
    "citation_count": 43733,
    "authors": []
  },
  "https://openreview.net/forum?id=XSLF1XFq5h": {
    "title": "Getting a CLUE: A Method for Explaining Uncertainty Estimates",
    "volume": "oral",
    "abstract": "Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input's prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty",
    "checked": true,
    "id": "aa9df1b1d7ca16ff3357a662714d15fd5340dce4",
    "semantic_title": "getting a clue: a method for explaining uncertainty estimates",
    "citation_count": 117,
    "authors": []
  },
  "https://openreview.net/forum?id=PULSD5qI2N1": {
    "title": "Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime",
    "volume": "oral",
    "abstract": "We analyze the convergence of the averaged stochastic gradient descent for overparameterized two-layer neural networks for regression problems. It was recently found that a neural tangent kernel (NTK) plays an important role in showing the global convergence of gradient-based methods under the NTK regime, where the learning dynamics for overparameterized neural networks can be almost characterized by that for the associated reproducing kernel Hilbert space (RKHS). However, there is still room for a convergence rate analysis in the NTK regime. In this study, we show that the averaged stochastic gradient descent can achieve the minimax optimal convergence rate, with the global convergence guarantee, by exploiting the complexities of the target function and the RKHS associated with the NTK. Moreover, we show that the target function specified by the NTK of a ReLU network can be learned at the optimal convergence rate through a smooth approximation of a ReLU network under certain conditions",
    "checked": true,
    "id": "54e35cfa780f6444cfb3750ec9bb0c4d3e83ee16",
    "semantic_title": "optimal rates for averaged stochastic gradient descent under neural tangent kernel regime",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=-2FCwDKRREu": {
    "title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction",
    "volume": "oral",
    "abstract": "We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference",
    "checked": true,
    "id": "518b827e340c26582b5093401283a4f5cff605b9",
    "semantic_title": "learning invariant representations for reinforcement learning without reconstruction",
    "citation_count": 492,
    "authors": []
  },
  "https://openreview.net/forum?id=dYeAHXnpWJ4": {
    "title": "Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability",
    "volume": "oral",
    "abstract": "Current methods for the interpretability of discriminative deep neural networks commonly rely on the model's input-gradients, i.e., the gradients of the output logits w.r.t. the inputs. The common assumption is that these input-gradients contain information regarding $p_{\\theta} ( y\\mid \\mathbf{x} )$, the model's discriminative capabilities, thus justifying their use for interpretability. However, in this work, we show that these input-gradients can be arbitrarily manipulated as a consequence of the shift-invariance of softmax without changing the discriminative function. This leaves an open question: given that input-gradients can be arbitrary, why are they highly structured and explanatory in standard models? In this work, we re-interpret the logits of standard softmax-based classifiers as unnormalized log-densities of the data distribution and show that input-gradients can be viewed as gradients of a class-conditional generative model $p_{\\theta}(\\mathbf{x} \\mid y)$ implicit in the discriminative model. This leads us to hypothesize that the highly structured and explanatory nature of input-gradients may be due to the alignment of this class-conditional model $p_{\\theta}(\\mathbf{x} \\mid y)$ with that of the ground truth data distribution $p_{\\text{data}} (\\mathbf{x} \\mid y)$. We test this hypothesis by studying the effect of density alignment on gradient explanations. To achieve this density alignment, we use an algorithm called score-matching, and propose novel approximations to this algorithm to enable training large-scale models. Our experiments show that improving the alignment of the implicit density model with the data distribution enhances gradient structure and explanatory power while reducing this alignment has the opposite effect. This also leads us to conjecture that unintended density alignment in standard neural network training may explain the highly structured nature of input-gradients observed in practice. Overall, our finding that input-gradients capture information regarding an implicit generative model implies that we need to re-think their use for interpreting discriminative models",
    "checked": true,
    "id": "d8a9a7c7b051453b04f6d52769b21ab62ac91628",
    "semantic_title": "rethinking the role of gradient-based attribution methods for model interpretability",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=cPZOyoDloxl": {
    "title": "SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments",
    "volume": "oral",
    "abstract": "Every living organism struggles against disruptive environmental forces to carve out and maintain an orderly niche. We propose that such a struggle to achieve and preserve order might offer a principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing reinforcement learning (SMiRL). SMiRL alternates between learning a density model to evaluate the surprise of a stimulus, and improving the policy to seek more predictable stimuli. The policy seeks out stable and repeatable situations that counteract the environment's prevailing sources of entropy. This might include avoiding other hostile agents, or finding a stable, balanced pose for a bipedal robot in the face of disturbance forces. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls, and navigate to escape enemies in a maze without any task-specific reward supervision. We further show that SMiRL can be used together with standard task rewards to accelerate reward-driven learning",
    "checked": true,
    "id": "c4c30bd9fc777f675e9a3cb8b5bb76d44a5d3d6b",
    "semantic_title": "smirl: surprise minimizing reinforcement learning in unstable environments",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=rALA0Xo6yNJ": {
    "title": "Learning to Reach Goals via Iterated Supervised Learning",
    "volume": "oral",
    "abstract": "Current reinforcement learning (RL) algorithms can be brittle and difficult to use, especially when learning goal-reaching behaviors from sparse rewards. Although supervised imitation learning provides a simple and stable alternative, it requires access to demonstrations from a human supervisor. In this paper, we study RL algorithms that use imitation learning to acquire goal reaching policies from scratch, without the need for expert demonstrations or a value function. In lieu of demonstrations, we leverage the property that any trajectory is a successful demonstration for reaching the final state in that same trajectory. We propose a simple algorithm in which an agent continually relabels and imitates the trajectories it generates to progressively learn goal-reaching behaviors from scratch. Each iteration, the agent collects new trajectories using the latest policy, and maximizes the likelihood of the actions along these trajectories under the goal that was actually reached, so as to improve the policy. We formally show that this iterated supervised learning procedure optimizes a bound on the RL objective, derive performance bounds of the learned policy, and empirically demonstrate improved goal-reaching performance and robustness over current RL algorithms in several benchmark tasks",
    "checked": true,
    "id": "831e7cbafed2dca05db1e7f5ef16d1a7614f44ec",
    "semantic_title": "learning to reach goals via iterated supervised learning",
    "citation_count": 188,
    "authors": []
  },
  "https://openreview.net/forum?id=O3Y56aqpChA": {
    "title": "Self-training For Few-shot Transfer Across Extreme Task Differences",
    "volume": "oral",
    "abstract": "Most few-shot learning techniques are pre-trained on a large, labeled \"base dataset\". In problem domains where such large labeled datasets are not available for pre-training (e.g., X-ray, satellite images), one must resort to pre-training in a different \"source\" problem domain (e.g., ImageNet), which can be very different from the desired target task. Traditional few-shot and transfer learning techniques fail in the presence of such extreme differences between the source and target tasks. In this paper, we present a simple and effective solution to tackle this extreme domain gap: self-training a source domain representation on unlabeled data from the target domain. We show that this improves one-shot performance on the target domain by 2.9 points on average on the challenging BSCD-FSL benchmark consisting of datasets from multiple domains",
    "checked": true,
    "id": "1d16d4cdc3fcce26e2c2097d13896ec09683eee3",
    "semantic_title": "self-training for few-shot transfer across extreme task differences",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=B7v4QMR6Z9w": {
    "title": "Federated Learning Based on Dynamic Regularization",
    "volume": "oral",
    "abstract": "We propose a novel federated learning method for distributively training neural network models, where the server orchestrates cooperation between a subset of randomly chosen devices in each round. We view Federated Learning problem primarily from a communication perspective and allow more device level computations to save transmission costs. We point out a fundamental dilemma, in that the minima of the local-device level empirical loss are inconsistent with those of the global empirical loss. Different from recent prior works, that either attempt inexact minimization or utilize devices for parallelizing gradient computation, we propose a dynamic regularizer for each device at each round, so that in the limit the global and device solutions are aligned. We demonstrate both through empirical results on real and synthetic data as well as analytical results that our scheme leads to efficient training, in both convex and non-convex settings, while being fully agnostic to device heterogeneity and robust to large number of devices, partial participation and unbalanced data",
    "checked": true,
    "id": "5a3d70689925df014c46d1cd50dfc8a368cb4c86",
    "semantic_title": "federated learning based on dynamic regularization",
    "citation_count": 812,
    "authors": []
  },
  "https://openreview.net/forum?id=Mk6PZtgAgfq": {
    "title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator",
    "volume": "oral",
    "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models",
    "checked": true,
    "id": "e61db54fe46bd13e1ef7a6ce293f4db202e3317e",
    "semantic_title": "rao-blackwellizing the straight-through gumbel-softmax gradient estimator",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=F3s69XzWOia": {
    "title": "Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies",
    "volume": "oral",
    "abstract": "Circuits of biological neurons, such as in the functional parts of the brain can be modeled as networks of coupled oscillators. Inspired by the ability of these systems to express a rich set of outputs while keeping (gradients of) state variables bounded, we propose a novel architecture for recurrent neural networks. Our proposed RNN is based on a time-discretization of a system of second-order ordinary differential equations, modeling networks of controlled nonlinear oscillators. We prove precise bounds on the gradients of the hidden states, leading to the mitigation of the exploding and vanishing gradient problem for this RNN. Experiments show that the proposed RNN is comparable in performance to the state of the art on a variety of benchmarks, demonstrating the potential of this architecture to provide stable and accurate RNNs for processing complex sequential data",
    "checked": true,
    "id": "fac144db777ddc3d85bb314087889689293affa0",
    "semantic_title": "coupled oscillatory recurrent neural network (cornn): an accurate and (gradient) stable architecture for learning long time dependencies",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=a-xFK8Ymz5J": {
    "title": "DiffWave: A Versatile Diffusion Model for Audio Synthesis",
    "volume": "oral",
    "abstract": "In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations",
    "checked": true,
    "id": "34bf13e58c7226d615afead0c0f679432502940e",
    "semantic_title": "diffwave: a versatile diffusion model for audio synthesis",
    "citation_count": 1517,
    "authors": []
  },
  "https://openreview.net/forum?id=QIRlze3I6hX": {
    "title": "Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency",
    "volume": "oral",
    "abstract": "At the heart of many robotics problems is the challenge of learning correspondences across domains. For instance, imitation learning requires obtaining correspondence between humans and robots; sim-to-real requires correspondence between physics simulators and real hardware; transfer learning requires correspondences between different robot environments. In this paper, we propose to learn correspondence across such domains emphasizing on differing modalities (vision and internal state), physics parameters (mass and friction), and morphologies (number of limbs). Importantly, correspondences are learned using unpaired and randomly collected data from the two domains. We propose dynamics cycles that align dynamic robotic behavior across two domains using a cycle consistency constraint. Once this correspondence is found, we can directly transfer the policy trained on one domain to the other, without needing any additional fine-tuning on the second domain. We perform experiments across a variety of problem domains, both in simulation and on real robots. Our framework is able to align uncalibrated monocular video of a real robot arm to dynamic state-action trajectories of a simulated arm without paired data. Video demonstrations of our results are available at: https://sites.google.com/view/cycledynamics",
    "checked": true,
    "id": "0003b8fef7d5e048a9870cdafdec27af129ae990",
    "semantic_title": "learning cross-domain correspondence for control with dynamics cycle-consistency",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=gZ9hCDWe6ke": {
    "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
    "volume": "oral",
    "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR",
    "checked": true,
    "id": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504",
    "semantic_title": "deformable detr: deformable transformers for end-to-end object detection",
    "citation_count": 5327,
    "authors": []
  },
  "https://openreview.net/forum?id=UuchYL8wSZo": {
    "title": "Learning Generalizable Visual Representations via Interactive Gameplay",
    "volume": "oral",
    "abstract": "A growing body of research suggests that embodied gameplay, prevalent not just in human cultures but across a variety of animal species including turtles and ravens, is critical in developing the neural flexibility for creative problem solving, decision making, and socialization. Comparatively little is known regarding the impact of embodied gameplay upon artificial agents. While recent work has produced agents proficient in abstract games, these environments are far removed the real world and thus these agents can provide little insight into the advantages of embodied play. Hiding games, such as hide-and-seek, played universally, provide a rich ground for studying the impact of embodied gameplay on representation learning in the context of perspective taking, secret keeping, and false belief understanding. Here we are the first to show that embodied adversarial reinforcement learning agents playing Cache, a variant of hide-and-seek, in a high fidelity, interactive, environment, learn generalizable representations of their observations encoding information such as object permanence, free space, and containment. Moving closer to biologically motivated learning strategies, our agents' representations, enhanced by intentionality and memory, are developed through interaction and play. These results serve as a model for studying how facets of vision develop through interaction, provide an experimental framework for assessing what is learned by artificial agents, and demonstrates the value of moving from large, static, datasets towards experiential, interactive, representation learning",
    "checked": false,
    "id": "f79af91cb0f8d0968c2b8dbea48a4d142764e798",
    "semantic_title": "learning generalized transformation equivariant representations via autoencoding transformations",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=3AOj0RCNC2": {
    "title": "Gradient Projection Memory for Continual Learning",
    "volume": "oral",
    "abstract": "The ability to learn continually without forgetting the past tasks is a desired attribute for artificial learning systems. Existing approaches to enable such learning in artificial neural networks usually rely on network growth, importance based weight update or replay of old data from the memory. In contrast, we propose a novel approach where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find the bases of these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner and store them in the memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that such orthogonal gradient descent induces minimum to no interference with the past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse image classification datasets with short and long sequences of tasks and report better or on-par performance compared to the state-of-the-art approaches",
    "checked": true,
    "id": "dd41d2d8fd2d3ea2c6ec74d07b456db942bfa8de",
    "semantic_title": "gradient projection memory for continual learning",
    "citation_count": 306,
    "authors": []
  },
  "https://openreview.net/forum?id=kmG8vRXTFv": {
    "title": "Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "757a3c0e2a43f3d15b91c52fae32b081a6a66e3a",
    "semantic_title": "augmenting physical models with deep networks for complex dynamics forecasting",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=0-uUGPbIjD": {
    "title": "Human-Level Performance in No-Press Diplomacy via Equilibrium Search",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "7f79ac114d30c2c7dae91075210fbfda90c9d76f",
    "semantic_title": "human-level performance in no-press diplomacy via equilibrium search",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=UH-cmocLJC": {
    "title": "How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "6b989b8327db3a7212141c59c1569f0219775058",
    "semantic_title": "how neural networks extrapolate: from feedforward to graph neural networks",
    "citation_count": 319,
    "authors": []
  },
  "https://openreview.net/forum?id=Ua6zuk0WRH": {
    "title": "Rethinking Attention with Performers",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
    "semantic_title": "rethinking attention with performers",
    "citation_count": 1665,
    "authors": []
  },
  "https://openreview.net/forum?id=nIAxjsniDzg": {
    "title": "What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "f6264b11ec0dd9133f1d88a5288a3267a93182f8",
    "semantic_title": "what matters for on-policy deep actor-critic methods? a large-scale study",
    "citation_count": 188,
    "authors": []
  },
  "https://openreview.net/forum?id=uAX8q61EVRu": {
    "title": "Neural Synthesis of Binaural Speech From Mono Audio",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "3a1285aa98156839271347c2b8ead71975ba3ebc",
    "semantic_title": "neural synthesis of binaural speech from mono audio",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=mSAKhLYLSsl": {
    "title": "Dataset Condensation with Gradient Matching",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "5a94bcc168330318d3020aa4d41bd73cf68ab285",
    "semantic_title": "dataset condensation with gradient matching",
    "citation_count": 529,
    "authors": []
  },
  "https://openreview.net/forum?id=HajQFbx_yB": {
    "title": "Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "1a180e019dc2d194feb39a053ddfa60a780c248e",
    "semantic_title": "scalable learning and map inference for nonsymmetric determinantal point processes",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=iAX0l6Cz8ub": {
    "title": "Geometry-aware Instance-reweighted Adversarial Training",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "99a599d8fe56529f47e78243ed61250190f96196",
    "semantic_title": "geometry-aware instance-reweighted adversarial training",
    "citation_count": 282,
    "authors": []
  },
  "https://openreview.net/forum?id=JWOiYxMG92s": {
    "title": "Free Lunch for Few-shot Learning: Distribution Calibration",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "48c6d0d7e3fabf4692bd03fc8b7263e55ee1d584",
    "semantic_title": "free lunch for few-shot learning: distribution calibration",
    "citation_count": 330,
    "authors": []
  },
  "https://openreview.net/forum?id=EbIDjBynYJ8": {
    "title": "Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "0cc9a9ae5dd2fcb8039ba80d6cd561114c0791fa",
    "semantic_title": "towards nonlinear disentanglement in natural data with temporal sparse coding",
    "citation_count": 135,
    "authors": []
  },
  "https://openreview.net/forum?id=FGqiDsBUKL0": {
    "title": "Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "7d7d189796efa8fbd3f516b183954bc36f262f3f",
    "semantic_title": "do 2d gans know 3d shape? unsupervised 3d shape reconstruction from 2d image gans",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=q3KSThy2GwB": {
    "title": "Practical Real Time Recurrent Learning with a Sparse Approximation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "efbcb368bdc1f27ec13584d65034321837169ae1",
    "semantic_title": "practical real time recurrent learning with a sparse approximation",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=zv-typ1gPxA": {
    "title": "Retrieval-Augmented Generation for Code Summarization via Hybrid GNN",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "7c0b558bf433c5aaaf774cd5d3c767bfd3dbe123",
    "semantic_title": "retrieval-augmented generation for code summarization via hybrid gnn",
    "citation_count": 169,
    "authors": []
  },
  "https://openreview.net/forum?id=2m0g1wEafh": {
    "title": "Benefit of deep learning with non-convex noisy gradient descent: Provable excess risk bound and superiority to kernel methods",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "77c530612412c36c3d2b1ee0a1cea6b37a7f518f",
    "semantic_title": "benefit of deep learning with non-convex noisy gradient descent: provable excess risk bound and superiority to kernel methods",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=xppLmXCbOw1": {
    "title": "Self-supervised Visual Reinforcement Learning with Object-centric Representations",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "7c1b75ab7bed79163d3d830a6a83a4023b496ebb",
    "semantic_title": "self-supervised visual reinforcement learning with object-centric representations",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=PUkhWz65dy5": {
    "title": "Discovering a set of policies for the worst case reward",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "7530e6bbe8c624c88c47e6bea4d58869428974e3",
    "semantic_title": "discovering a set of policies for the worst case reward",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=8PS8m9oYtNy": {
    "title": "Implicit Normalizing Flows",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "68e710c9f040ffb52a2d102c110ad238521b325c",
    "semantic_title": "implicit normalizing flows",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=Jnspzp-oIZE": {
    "title": "Gauge Equivariant Mesh CNNs: Anisotropic convolutions on geometric graphs",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "b626e35f2d30ac54a77e6d80b6fa8d01fa68971f",
    "semantic_title": "gauge equivariant mesh cnns: anisotropic convolutions on geometric graphs",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=EGdFhBzmAwB": {
    "title": "Generalization bounds via distillation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1488259fc61220397eef585a857cb5b34cb7f2db",
    "semantic_title": "generalization bounds via distillation",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=roNqYL0_XP": {
    "title": "Learning Mesh-Based Simulation with Graph Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "9e20f6874feaaf7c9994f9875b1d9cab17a2fd59",
    "semantic_title": "learning mesh-based simulation with graph networks",
    "citation_count": 842,
    "authors": []
  },
  "https://openreview.net/forum?id=JiYq3eqTKY": {
    "title": "On Statistical Bias In Active Learning: How and When to Fix It",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1920ed0e7799410009d11cd7584550b9a57d5c93",
    "semantic_title": "on statistical bias in active learning: how and when to fix it",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=LmUJqB1Cz8": {
    "title": "Winning the L2RPN Challenge: Power Grid Management via Semi-Markov Afterstate Actor-Critic",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "690c1ae40fef28f58d5ef35916e9c2693677ef13",
    "semantic_title": "winning the l2rpn challenge: power grid management via semi-markov afterstate actor-critic",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=lxHgXYN4bwl": {
    "title": "Expressive Power of Invariant and Equivariant Graph Neural Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1784a18105dafb85e2c53c86a9736fc7b8b71316",
    "semantic_title": "expressive power of invariant and equivariant graph neural networks",
    "citation_count": 112,
    "authors": []
  },
  "https://openreview.net/forum?id=kHSu4ebxFXY": {
    "title": "MARS: Markov Molecular Sampling for Multi-objective Drug Discovery",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "73bc1497aca1a3cd0aa9c48e4d35be5cf40d2a9c",
    "semantic_title": "mars: markov molecular sampling for multi-objective drug discovery",
    "citation_count": 150,
    "authors": []
  },
  "https://openreview.net/forum?id=NeRdBeTionN": {
    "title": "On Self-Supervised Image Representations for GAN Evaluation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "6fcf0d22745b1edbb46a1e86b68efda0523d5edf",
    "semantic_title": "on self-supervised image representations for gan evaluation",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=_XYzwxPIQu6": {
    "title": "Identifying nonlinear dynamical systems with multiple time scales and long-range dependencies",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "be8e2098a004aa109f65966ee266ffaf431f8289",
    "semantic_title": "identifying nonlinear dynamical systems with multiple time scales and long-range dependencies",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=_WnwtieRHxM": {
    "title": "Understanding the role of importance weighting for deep learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "689d3394c674b8cb2906fa8ffb1c80ecc76e69d0",
    "semantic_title": "understanding the role of importance weighting for deep learning",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=3UDSdyIcBDA": {
    "title": "RMSprop converges with proper hyper-parameter",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "c7f93c7a6c8646b6df94ff7f5eaf230f3f285e1f",
    "semantic_title": "rmsprop converges with proper hyper-parameter",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=XJk19XzGq2J": {
    "title": "The Intrinsic Dimension of Images and Its Impact on Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2841a41690bfbdeb552b845a85c3b76d586084ae",
    "semantic_title": "the intrinsic dimension of images and its impact on learning",
    "citation_count": 282,
    "authors": []
  },
  "https://openreview.net/forum?id=ZPa2SyGcbwh": {
    "title": "Learning with Feature-Dependent Label Noise: A Progressive Approach",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "80f76b959f6bb243ebb67bdb395ca38b4373d49b",
    "semantic_title": "learning with feature-dependent label noise: a progressive approach",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=O7ms4LFdsX": {
    "title": "Disentangled Recurrent Wasserstein Autoencoder",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "072ad179f7ca9e5ee232e8fadd05d193bca79a8c",
    "semantic_title": "disentangled recurrent wasserstein autoencoder",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=opHLcXxYTC_": {
    "title": "Influence Estimation for Generative Adversarial Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a1bc580f9eaed7ab5f0505e7aa2d99f794606faf",
    "semantic_title": "influence estimation for generative adversarial networks",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=Pbj8H_jEHYv": {
    "title": "Orthogonalizing Convolutional Layers with the Cayley Transform",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "9f0e0a59a4b3d689df8470b1218d2574244c26d6",
    "semantic_title": "orthogonalizing convolutional layers with the cayley transform",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=lVgB2FUbzuQ": {
    "title": "Predicting Infectiousness for Proactive Contact Tracing",
    "volume": "spotlight",
    "abstract": "The COVID-19 pandemic has spread rapidly worldwide, overwhelming manual contact tracing in many countries and resulting in widespread lockdowns for emergency containment. Large-scale digital contact tracing (DCT) has emerged as a potential solution to resume economic and social activity while minimizing spread of the virus. Various DCT methods have been proposed, each making trade-offs be-tween privacy, mobility restrictions, and public health. The most common approach, binary contact tracing (BCT), models infection as a binary event, informed only by an individual's test results, with corresponding binary recommendations that either all or none of the individual's contacts quarantine. BCT ignores the inherent uncertainty in contacts and the infection process, which could be used to tailor messaging to high-risk individuals, and prompt proactive testing or earlier warnings. It also does not make use of observations such as symptoms or pre-existing medical conditions, which could be used to make more accurate infectiousness predictions. In this paper, we use a recently-proposed COVID-19 epidemiological simulator to develop and test methods that can be deployed to a smartphone to locally and proactively predict an individual's infectiousness (risk of infecting others) based on their contact history and other information, while respecting strong privacy constraints. Predictions are used to provide personalized recommendations to the individual via an app, as well as to send anonymized messages to the individual's contacts, who use this information to better predict their own infectiousness, an approach we call proactive contact tracing (PCT). Similarly to other works, we find that compared to no tracing, all DCT methods tested are able to reduce spread of the disease and thus save lives, even at low adoption rates, strongly supporting a role for DCT methods in managing the pandemic. Further, we find a deep-learning based PCT method which improves over BCT for equivalent average mobility, suggesting PCT could help in safe re-opening and second-wave prevention",
    "checked": true,
    "id": "59078a88b2a94636bd2bfb6b4186f6ab8a7813bc",
    "semantic_title": "predicting infectiousness for proactive contact tracing",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=04LZCAxMSco": {
    "title": "Learning a Latent Simplex in Input Sparsity Time",
    "volume": "spotlight",
    "abstract": "We consider the problem of learning a latent $k$-vertex simplex $K\\in\\mathbb{R}^d$, given $\\mathbf{A}\\in\\mathbb{R}^{d\\times n}$, which can be viewed as $n$ data points that are formed by randomly perturbing some latent points in $K$, possibly beyond $K$. A large class of latent variable models, such as adversarial clustering, mixed membership stochastic block models, and topic models can be cast in this view of learning a latent simplex. Bhattacharyya and Kannan (SODA 2020) give an algorithm for learning such a $k$-vertex latent simplex in time roughly $O(k\\cdot\\text{nnz}(\\mathbf{A}))$, where $\\text{nnz}(\\mathbf{A})$ is the number of non-zeros in $\\mathbf{A}$. We show that the dependence on $k$ in the running time is unnecessary given a natural assumption about the mass of the top $k$ singular values of $\\mathbf{A}$, which holds in many of these applications. Further, we show this assumption is necessary, as otherwise an algorithm for learning a latent simplex would imply a better low rank approximation algorithm than what is known. We obtain a spectral low-rank approximation to $\\mathbf{A}$ in input-sparsity time and show that the column space thus obtained has small $\\sin\\Theta$ (angular) distance to the right top-$k$ singular space of $\\mathbf{A}$. Our algorithm then selects $k$ points in the low-rank subspace with the largest inner product (in absolute value) with $k$ carefully chosen random vectors. By working in the low-rank subspace, we avoid reading the entire matrix in each iteration and thus circumvent the $\\Theta(k\\cdot\\text{nnz}(\\mathbf{A}))$ running time",
    "checked": true,
    "id": "71d0156086aa83f7c4249c17d6df23fe9abae30d",
    "semantic_title": "learning a latent simplex in input-sparsity time",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=87ZwsaQNHPZ": {
    "title": "CPT: Efficient Deep Neural Network Training via Cyclic Precision",
    "volume": "spotlight",
    "abstract": "Low-precision deep neural network (DNN) training has gained tremendous attention as reducing precision is one of the most effective knobs for boosting DNNs' training time/energy efficiency. In this paper, we attempt to explore low-precision training from a new perspective as inspired by recent findings in understanding DNN training: we conjecture that DNNs' precision might have a similar effect as the learning rate during DNN training, and advocate dynamic precision along the training trajectory for further boosting the time/energy efficiency of DNN training. Specifically, we propose Cyclic Precision Training (CPT) to cyclically vary the precision between two boundary values which can be identified using a simple precision range test within the first few training epochs. Extensive simulations and ablation studies on five datasets and eleven models demonstrate that CPT's effectiveness is consistent across various models/tasks (including classification and language modeling). Furthermore, through experiments and visualization we show that CPT helps to (1) converge to a wider minima with a lower generalization error and (2) reduce training variance which we believe opens up a new design knob for simultaneously improving the optimization and efficiency of DNN training",
    "checked": true,
    "id": "badefa14a1b7bc192dfe899ce80e934b716106b6",
    "semantic_title": "cpt: efficient deep neural network training via cyclic precision",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=w_7JMpGZRh0": {
    "title": "Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration",
    "volume": "spotlight",
    "abstract": "In this paper, we introduce Watch-And-Help (WAH), a challenge for testing social intelligence in agents. In WAH, an AI agent needs to help a human-like agent perform a complex household task efficiently. To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration). For this challenge, we build VirtualHome-Social, a multi-agent household environment, and provide a benchmark including both planning and learning based baselines. We evaluate the performance of AI agents with the human-like agent as well as and with real humans using objective metrics and subjective user ratings. Experimental results demonstrate that our challenge and virtual environment enable a systematic evaluation on the important aspects of machine social intelligence at scale",
    "checked": true,
    "id": "c562477737cc35e08d5a84aef01163ee4652d796",
    "semantic_title": "watch-and-help: a challenge for social perception and human-ai collaboration",
    "citation_count": 136,
    "authors": []
  },
  "https://openreview.net/forum?id=F1vEjWK-lH_": {
    "title": "Gradient Vaccine: Investigating and Improving Multi-task Optimization in Massively Multilingual Models",
    "volume": "spotlight",
    "abstract": "Massively multilingual models subsuming tens or even hundreds of languages pose great challenges to multi-task optimization. While it is a common practice to apply a language-agnostic procedure optimizing a joint multilingual task objective, how to properly characterize and take advantage of its underlying problem structure for improving optimization efficiency remains under-explored. In this paper, we attempt to peek into the black-box of multilingual optimization through the lens of loss function geometry. We find that gradient similarity measured along the optimization trajectory is an important signal, which correlates well with not only language proximity but also the overall model performance. Such observation helps us to identify a critical limitation of existing gradient-based multi-task learning methods, and thus we derive a simple and scalable optimization procedure, named Gradient Vaccine, which encourages more geometrically aligned parameter updates for close tasks. Empirically, our method obtains significant model performance gains on multilingual machine translation and XTREME benchmark tasks for multilingual language models. Our work reveals the importance of properly measuring and utilizing language proximity in multilingual optimization, and has broader implications for multi-task learning beyond multilingual modeling",
    "checked": true,
    "id": "78d9f4a6bc0f53d0594a752f0934a3689b4460d8",
    "semantic_title": "gradient vaccine: investigating and improving multi-task optimization in massively multilingual models",
    "citation_count": 204,
    "authors": []
  },
  "https://openreview.net/forum?id=xCcdBRQEDW": {
    "title": "PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable Physics",
    "volume": "spotlight",
    "abstract": "Simulated virtual environments serve as one of the main driving forces behind developing and evaluating skill learning algorithms. However, existing environments typically only simulate rigid body physics. Additionally, the simulation process usually does not provide gradients that might be useful for planning and control optimizations. We introduce a new differentiable physics benchmark called PasticineLab, which includes a diverse collection of soft body manipulation tasks. In each task, the agent uses manipulators to deform the plasticine into a desired configuration. The underlying physics engine supports differentiable elastic and plastic deformation using the DiffTaichi system, posing many under-explored challenges to robotic agents. We evaluate several existing reinforcement learning (RL) methods and gradient-based methods on this benchmark. Experimental results suggest that 1) RL-based approaches struggle to solve most of the tasks efficiently; 2) gradient-based approaches, by optimizing open-loop control sequences with the built-in differentiable physics engine, can rapidly find a solution within tens of iterations, but still fall short on multi-stage tasks that require long-term planning. We expect that PlasticineLab will encourage the development of novel algorithms that combine differentiable physics and RL for more complex physics-based skill learning tasks. PlasticineLab will be made publicly available",
    "checked": true,
    "id": "15c59c5cd0e393dda0b411995310510719d03a0f",
    "semantic_title": "plasticinelab: a soft-body manipulation benchmark with differentiable physics",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=QYjO70ACDK": {
    "title": "Distributional Sliced-Wasserstein and Applications to Generative Modeling",
    "volume": "spotlight",
    "abstract": "Sliced-Wasserstein distance (SW) and its variant, Max Sliced-Wasserstein distance (Max-SW), have been used widely in the recent years due to their fast computation and scalability even when the probability measures lie in a very high dimensional space. However, SW requires many unnecessary projection samples to approximate its value while Max-SW only uses the most important projection, which ignores the information of other useful directions. In order to account for these weaknesses, we propose a novel distance, named Distributional Sliced-Wasserstein distance (DSW), that finds an optimal distribution over projections that can balance between exploring distinctive projecting directions and the informativeness of projections themselves. We show that the DSW is a generalization of Max-SW, and it can be computed efficiently by searching for the optimal push-forward measure over a set of probability measures over the unit sphere satisfying certain regularizing constraints that favor distinct directions. Finally, we conduct extensive experiments with large-scale datasets to demonstrate the favorable performances of the proposed distances over the previous sliced-based distances in generative modeling applications",
    "checked": true,
    "id": "a27b1f09cc31bfa8b313f5e31d529be6d75bbd34",
    "semantic_title": "distributional sliced-wasserstein and applications to generative modeling",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=b9PoimzZFJ": {
    "title": "Systematic generalisation with group invariant predictions",
    "volume": "spotlight",
    "abstract": "We consider situations where the presence of dominant simpler correlations with the target variable in a training set can cause an SGD-trained neural network to be less reliant on more persistently correlating complex features. When the non-persistent, simpler correlations correspond to non-semantic background factors, a neural network trained on this data can exhibit dramatic failure upon encountering systematic distributional shift, where the correlating background features are recombined with different objects. We perform an empirical study on three synthetic datasets, showing that group invariance methods across inferred partitionings of the training set can lead to significant improvements at such test-time situations. We also suggest a simple invariance penalty, showing with experiments on our setups that it can perform better than alternatives. We find that even without assuming access to any systematically shifted validation sets, one can still find improvements over an ERM-trained reference model",
    "checked": true,
    "id": "359c56a068e4a84a9f3b78f43b53fe3b333c0ba0",
    "semantic_title": "systematic generalisation with group invariant predictions",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=bnY0jm4l59": {
    "title": "Memory Optimization for Deep Networks",
    "volume": "spotlight",
    "abstract": "Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by $32\\times$ over the last five years, the total available memory only grew by $2.5\\times$. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by $3\\times$ for various PyTorch models, with a 9-16$\\%$ overhead in computation. For the same computation cost, MONeT requires 1.2-1.8$\\times$ less memory than current state-of-the-art automated checkpointing frameworks. Our code will be made publicly available upon acceptance",
    "checked": true,
    "id": "b5697ed0c72b318d291545871fbd3a7a6e59e3cf",
    "semantic_title": "memory optimization for deep networks",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=_0kaDkv3dVf": {
    "title": "HW-NAS-Bench: Hardware-Aware Neural Architecture Search Benchmark",
    "volume": "spotlight",
    "abstract": "HardWare-aware Neural Architecture Search (HW-NAS) has recently gained tremendous attention by automating the design of deep neural networks deployed in more resource-constrained daily life devices. Despite its promising performance, developing optimal HW-NAS solutions can be prohibitively challenging as it requires cross-disciplinary knowledge in the algorithm, micro-architecture, and device-specific compilation. First, to determine the hardware-cost to be incorporated into the NAS process, existing works mostly adopt either pre-collected hardware-cost look-up tables or device-specific hardware-cost models. The former can be time-consuming due to the required knowledge of the device's compilation method and how to set up the measurement pipeline, while building the latter is often a barrier for non-hardware experts like NAS researchers. Both of them limit the development of HW-NAS innovations and impose a barrier-to-entry to non-hardware experts. Second, similar to generic NAS, it can be notoriously difficult to benchmark HW-NAS algorithms due to their significant required computational resources and the differences in adopted search spaces, hyperparameters, and hardware devices. To this end, we develop HW-NAS-Bench, the first public dataset for HW-NAS research which aims to democratize HW-NAS research to non-hardware experts and make HW-NAS research more reproducible and accessible. To design HW-NAS-Bench, we carefully collected the measured/estimated hardware performance (e.g., energy cost and latency) of all the networks in the search spaces of both NAS-Bench-201 and FBNet, on six hardware devices that fall into three categories (i.e., commercial edge devices, FPGA, and ASIC). Furthermore, we provide a comprehensive analysis of the collected measurements in HW-NAS-Bench to provide insights for HW-NAS research. Finally, we demonstrate exemplary user cases to (1) show that HW-NAS-Bench allows non-hardware experts to perform HW-NAS by simply querying our pre-measured dataset and (2) verify that dedicated device-specific HW-NAS can indeed lead to optimal accuracy-cost trade-offs. The codes and all collected data are available at https://github.com/RICE-EIC/HW-NAS-Bench",
    "checked": true,
    "id": "a10daed04b387cdb6b9c71a623994bc083599c84",
    "semantic_title": "hw-nas-bench: hardware-aware neural architecture search benchmark",
    "citation_count": 112,
    "authors": []
  },
  "https://openreview.net/forum?id=HgLO8yalfwc": {
    "title": "Regularized Inverse Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Inverse Reinforcement Learning (IRL) aims to facilitate a learner's ability to imitate expert behavior by acquiring reward functions that explain the expert's decisions. Regularized IRLapplies strongly convex regularizers to the learner's policy in order to avoid the expert's behavior being rationalized by arbitrary constant rewards, also known as degenerate solutions. We propose tractable solutions, and practical methods to obtain them, for regularized IRL. Current methods are restricted to the maximum-entropy IRL framework, limiting them to Shannon-entropy regularizers, as well as proposing solutions that are intractable in practice. We present theoretical backing for our proposed IRL method's applicability to both discrete and continuous controls, empirically validating our performance on a variety of tasks",
    "checked": true,
    "id": "973fbe99e9b32f9da7e976e11141eec05ba577d3",
    "semantic_title": "regularized inverse reinforcement learning",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=30EvkP2aQLD": {
    "title": "What are the Statistical Limits of Offline RL with Linear Function Approximation?",
    "volume": "spotlight",
    "abstract": "Offline reinforcement learning seeks to utilize offline (observational) data to guide the learning of (causal) sequential decision making strategies. The hope is that offline reinforcement learning coupled with function approximation methods (to deal with the curse of dimensionality) can provide a means to help alleviate the excessive sample complexity burden in modern sequential decision making problems. However, the extent to which this broader approach can be effective is not well understood, where the literature largely consists of sufficient conditions. This work focuses on the basic question of what are necessary representational and distributional conditions that permit provable sample-efficient offline reinforcement learning. Perhaps surprisingly, our main result shows that even if: i) we have realizability in that the true value function of \\emph{every} policy is linear in a given set of features and 2) our off-policy data has good coverage over all features (under a strong spectral condition), any algorithm still (information-theoretically) requires a number of offline samples that is exponential in the problem horizon to non-trivially estimate the value of \\emph{any} given policy. Our results highlight that sample-efficient offline policy evaluation is not possible unless significantly stronger conditions hold; such conditions include either having low distribution shift (where the offline data distribution is close to the distribution of the policy to be evaluated) or significantly stronger representational conditions (beyond realizability)",
    "checked": true,
    "id": "bf53ddef241c58303b509faad4ec7514d3a6fc14",
    "semantic_title": "what are the statistical limits of offline rl with linear function approximation?",
    "citation_count": 165,
    "authors": []
  },
  "https://openreview.net/forum?id=zrT3HcsWSAt": {
    "title": "Behavioral Cloning from Noisy Demonstrations",
    "volume": "spotlight",
    "abstract": "We consider the problem of learning an optimal expert behavior policy given noisy demonstrations that contain observations from both optimal and non-optimal expert behaviors. Popular imitation learning algorithms, such as generative adversarial imitation learning, assume that (clear) demonstrations are given from optimal expert policies but not the non-optimal ones, and thus often fail to imitate the optimal expert behaviors given the noisy demonstrations. Prior works that address the problem require (1) learning policies through environment interactions in the same fashion as reinforcement learning, and (2) annotating each demonstration with confidence scores or rankings. However, such environment interactions and annotations in real-world settings take impractically long training time and a significant human effort. In this paper, we propose an imitation learning algorithm to address the problem without any environment interactions and annotations associated with the non-optimal demonstrations. The proposed algorithm learns ensemble policies with a generalized behavioral cloning (BC) objective function where we exploit another policy already learned by BC. Experimental results show that the proposed algorithm can learn behavior policies that are much closer to the optimal policies than ones learned by BC",
    "checked": true,
    "id": "f27fee37a1388d083315713155dee1b46e78ad48",
    "semantic_title": "behavioral cloning from noisy demonstrations",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=8yKEo06dKNo": {
    "title": "How Does Mixup Help With Robustness and Generalization?",
    "volume": "spotlight",
    "abstract": "Mixup is a popular data augmentation technique based on on convex combinations of pairs of examples and their labels. This simple technique has shown to substantially improve both the model's robustness as well as the generalization of the trained model. However, it is not well-understood why such improvement occurs. In this paper, we provide theoretical analysis to demonstrate how using Mixup in training helps model robustness and generalization. For robustness, we show that minimizing the Mixup loss corresponds to approximately minimizing an upper bound of the adversarial loss. This explains why models obtained by Mixup training exhibits robustness to several kinds of adversarial attacks such as Fast Gradient Sign Method (FGSM). For generalization, we prove that Mixup augmentation corresponds to a specific type of data-adaptive regularization which reduces overfitting. Our analysis provides new insights and a framework to understand Mixup",
    "checked": true,
    "id": "92ed2e34501903d922d74f28a012d6e337418fa4",
    "semantic_title": "how does mixup help with robustness and generalization?",
    "citation_count": 256,
    "authors": []
  },
  "https://openreview.net/forum?id=LSFCEb3GYU7": {
    "title": "Emergent Symbols through Binding in External Memory",
    "volume": "spotlight",
    "abstract": "A key aspect of human intelligence is the ability to infer abstract rules directly from high-dimensional sensory data, and to do so given only a limited amount of training experience. Deep neural network algorithms have proven to be a powerful tool for learning directly from high-dimensional data, but currently lack this capacity for data-efficient induction of abstract rules, leading some to argue that symbol-processing mechanisms will be necessary to account for this capacity. In this work, we take a step toward bridging this gap by introducing the Emergent Symbol Binding Network (ESBN), a recurrent network augmented with an external memory that enables a form of variable-binding and indirection. This binding mechanism allows symbol-like representations to emerge through the learning process without the need to explicitly incorporate symbol-processing machinery, enabling the ESBN to learn rules in a manner that is abstracted away from the particular entities to which those rules apply. Across a series of tasks, we show that this architecture displays nearly perfect generalization of learned rules to novel entities given only a limited number of training examples, and outperforms a number of other competitive neural network architectures",
    "checked": true,
    "id": "c4791fe1dc9c17bb939c18ac6d9b3ab819661a96",
    "semantic_title": "emergent symbols through binding in external memory",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=JBAa9we1AL": {
    "title": "Individually Fair Gradient Boosting",
    "volume": "spotlight",
    "abstract": "We consider the task of enforcing individual fairness in gradient boosting. Gradient boosting is a popular method for machine learning from tabular data, which arise often in applications where algorithmic fairness is a concern. At a high level, our approach is a functional gradient descent on a (distributionally) robust loss function that encodes our intuition of algorithmic fairness for the ML task at hand. Unlike prior approaches to individual fairness that only work with smooth ML models, our approach also works with non-smooth models such as decision trees. We show that our algorithm converges globally and generalizes. We also demonstrate the efficacy of our algorithm on three ML problems susceptible to algorithmic bias",
    "checked": true,
    "id": "44f20764dbed5a309f992f4e651c5980e606ebb4",
    "semantic_title": "individually fair gradient boosting",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=US-TP-xnXI": {
    "title": "Structured Prediction as Translation between Augmented Natural Languages",
    "volume": "spotlight",
    "abstract": "We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks, and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics",
    "checked": true,
    "id": "1cb3f6d545b68db3e7fc6055dcf44099c3ac4672",
    "semantic_title": "structured prediction as translation between augmented natural languages",
    "citation_count": 296,
    "authors": []
  },
  "https://openreview.net/forum?id=O-XJwyoIF-k": {
    "title": "Minimum Width for Universal Approximation",
    "volume": "spotlight",
    "abstract": "The universal approximation property of width-bounded networks has been studied as a dual of classical universal approximation results on depth-bounded networks. However, the critical width enabling the universal approximation has not been exactly characterized in terms of the input dimension $d_x$ and the output dimension $d_y$. In this work, we provide the first definitive result in this direction for networks using the ReLU activation functions: The minimum width required for the universal approximation of the $L^p$ functions is exactly $\\max\\{d_x+1,d_y\\}$. We also prove that the same conclusion does not hold for the uniform approximation with ReLU, but does hold with an additional threshold activation function. Our proof technique can be also used to derive a tighter upper bound on the minimum width required for the universal approximation using networks with general activation functions",
    "checked": true,
    "id": "f63405f53db3b1016f565f555fc8fa409f02fdbd",
    "semantic_title": "minimum width for universal approximation",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=LGgdb4TS4Z": {
    "title": "Topology-Aware Segmentation Using Discrete Morse Theory",
    "volume": "spotlight",
    "abstract": "In the segmentation of fine-scale structures from natural and biomedical images, per-pixel accuracy is not the only metric of concern. Topological correctness, such as vessel connectivity and membrane closure, is crucial for downstream analysis tasks. In this paper, we propose a new approach to train deep image segmentation networks for better topological accuracy. In particular, leveraging the power of discrete Morse theory (DMT), we identify global structures, including 1D skeletons and 2D patches, which are important for topological accuracy. Trained with a novel loss based on these global structures, the network performance is significantly improved especially near topologically challenging locations (such as weak spots of connections and membranes). On diverse datasets, our method achieves superior performance on both the DICE score and topological metrics",
    "checked": true,
    "id": "7d3b1cdc8b9fe6347feebfec7175f231ce158330",
    "semantic_title": "topology-aware segmentation using discrete morse theory",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=WznmQa42ZAx": {
    "title": "Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking",
    "volume": "spotlight",
    "abstract": "Graph neural networks (GNNs) have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs (e.g. syntactic trees or co-reference structures) contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected $L_0$ norm. We use our technique as an attribution method to analyze GNN models for two tasks -- question answering and semantic role labeling -- providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions",
    "checked": true,
    "id": "dcb5f0c180e138e3dae95d6e65237ef10063474c",
    "semantic_title": "interpreting graph neural networks for nlp with differentiable edge masking",
    "citation_count": 231,
    "authors": []
  },
  "https://openreview.net/forum?id=yr1mzrH3IC": {
    "title": "Regularization Matters in Policy Optimization - An Empirical Study on Continuous Control",
    "volume": "spotlight",
    "abstract": "Deep Reinforcement Learning (Deep RL) has been receiving increasingly more attention thanks to its encouraging performance on a variety of control tasks. Yet, conventional regularization techniques in training neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods, possibly because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs. In this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find conventional regularization techniques on the policy networks can often bring large improvement, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization. In addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. We hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg",
    "checked": true,
    "id": "975e33347fd841ded9ebcd77d928b243ea786a8c",
    "semantic_title": "regularization matters in policy optimization - an empirical study on continuous control",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=mLcmdlEUxy-": {
    "title": "Recurrent Independent Mechanisms",
    "volume": "spotlight",
    "abstract": "We explore the hypothesis that learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes that only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and compete with each other so they are updated only at time steps where they are most relevant. We show that this leads to specialization amongst the RIMs, which in turn allows for remarkably improved generalization on tasks where some factors of variation differ systematically between training and evaluation",
    "checked": true,
    "id": "67a9dde04f367efc903b6d06097df9bdd9887ae7",
    "semantic_title": "recurrent independent mechanisms",
    "citation_count": 340,
    "authors": []
  },
  "https://openreview.net/forum?id=hvdKKV2yt7T": {
    "title": "Dataset Inference: Ownership Resolution in Machine Learning",
    "volume": "spotlight",
    "abstract": "With increasingly more data and computation involved in their training, machine learning models constitute valuable intellectual property. This has spurred interest in model stealing, which is made more practical by advances in learning with partial, little, or no supervision. Existing defenses focus on inserting unique watermarks in a model's decision surface, but this is insufficient: the watermarks are not sampled from the training distribution and thus are not always preserved during model stealing. In this paper, we make the key observation that knowledge contained in the stolen model's training set is what is common to all stolen copies. The adversary's goal, irrespective of the attack employed, is always to extract this knowledge or its by-products. This gives the original model's owner a strong advantage over the adversary: model owners have access to the original training data. We thus introduce $\\textit{dataset inference}$, the process of identifying whether a suspected model copy has private knowledge from the original model's dataset, as a defense against model stealing. We develop an approach for dataset inference that combines statistical testing with the ability to estimate the distance of multiple data points to the decision boundary. Our experiments on CIFAR10, SVHN, CIFAR100 and ImageNet show that model owners can claim with confidence greater than 99% that their model (or dataset as a matter of fact) was stolen, despite only exposing 50 of the stolen model's training points. Dataset inference defends against state-of-the-art attacks even when the adversary is adaptive. Unlike prior work, it does not require retraining or overfitting the defended model",
    "checked": true,
    "id": "b3086fbbc678a7616ac390a41945f45e0d0ab001",
    "semantic_title": "dataset inference: ownership resolution in machine learning",
    "citation_count": 120,
    "authors": []
  },
  "https://openreview.net/forum?id=1YLJDvSx6J4": {
    "title": "Learning from Protein Structure with Geometric Vector Perceptrons",
    "volume": "spotlight",
    "abstract": "Learning on 3D structures of large biomolecules is emerging as a distinct area in machine learning, but there has yet to emerge a unifying network architecture that simultaneously leverages the geometric and relational aspects of the problem domain. To address this gap, we introduce geometric vector perceptrons, which extend standard dense layers to operate on collections of Euclidean vectors. Graph neural networks equipped with such layers are able to perform both geometric and relational reasoning on efficient representations of macromolecules. We demonstrate our approach on two important problems in learning from protein structure: model quality assessment and computational protein design. Our approach improves over existing classes of architectures on both problems, including state-of-the-art convolutional neural networks and graph neural networks. We release our code at https://github.com/drorlab/gvp",
    "checked": true,
    "id": "5e7047851d05b2ecef5de451dda5404acda726de",
    "semantic_title": "learning from protein structure with geometric vector perceptrons",
    "citation_count": 508,
    "authors": []
  },
  "https://openreview.net/forum?id=GJwMHetHc73": {
    "title": "Unsupervised Object Keypoint Learning using Local Spatial Predictability",
    "volume": "spotlight",
    "abstract": "We propose PermaKey, a novel approach to representation learning based on object keypoints. It leverages the predictability of local image regions from spatial neighborhoods to identify salient regions that correspond to object parts, which are then converted to keypoints. Unlike prior approaches, it utilizes predictability to discover object keypoints, an intrinsic property of objects. This ensures that it does not overly bias keypoints to focus on characteristics that are not unique to objects, such as movement, shape, colour etc. We demonstrate the efficacy of PermaKey on Atari where it learns keypoints corresponding to the most salient object parts and is robust to certain visual distractors. Further, on downstream RL tasks in the Atari domain we demonstrate how agents equipped with our keypoints outperform those using competing alternatives, even on challenging environments with moving backgrounds or distractor objects",
    "checked": true,
    "id": "c34a5507fb6123cf4130538c518e7562f1572ff2",
    "semantic_title": "unsupervised object keypoint learning using local spatial predictability",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=zWy1uxjDdZJ": {
    "title": "Fast Geometric Projections for Local Robustness Certification",
    "volume": "spotlight",
    "abstract": "Local robustness ensures that a model classifies all inputs within an $\\ell_p$-ball consistently, which precludes various forms of adversarial inputs. In this paper, we present a fast procedure for checking local robustness in feed-forward neural networks with piecewise-linear activation functions. Such networks partition the input space into a set of convex polyhedral regions in which the network's behavior is linear; hence, a systematic search for decision boundaries within the regions around a given input is sufficient for assessing robustness. Crucially, we show how the regions around a point can be analyzed using simple geometric projections, thus admitting an efficient, highly-parallel GPU implementation that excels particularly for the $\\ell_2$ norm, where previous work has been less effective. Empirically we find this approach to be far more precise than many approximate verification approaches, while at the same time performing multiple orders of magnitude faster than complete verifiers, and scaling to much deeper networks",
    "checked": true,
    "id": "df1ff25efac423a37527349044630614bf4f8396",
    "semantic_title": "fast geometric projections for local robustness certification",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=QtTKTdVrFBB": {
    "title": "Random Feature Attention",
    "volume": "spotlight",
    "abstract": "Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep. While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length. We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers. RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism. Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines. In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets. Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints",
    "checked": true,
    "id": "9ed25f101f19ea735ca300848948ed64064b97ca",
    "semantic_title": "random feature attention",
    "citation_count": 370,
    "authors": []
  },
  "https://openreview.net/forum?id=6Tm1mposlrM": {
    "title": "Sharpness-aware Minimization for Efficiently Improving Generalization",
    "volume": "spotlight",
    "abstract": "In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by the connection between geometry of the loss landscape and generalization---including a generalization bound that we prove here---we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-{10, 100}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels",
    "checked": true,
    "id": "a2cd073b57be744533152202989228cb4122270a",
    "semantic_title": "sharpness-aware minimization for efficiently improving generalization",
    "citation_count": 1415,
    "authors": []
  },
  "https://openreview.net/forum?id=3Aoft6NWFej": {
    "title": "PMI-Masking: Principled masking of correlated spans",
    "volume": "spotlight",
    "abstract": "Masking tokens uniformly at random constitutes a common flaw in the pretraining of Masked Language Models (MLMs) such as BERT. We show that such uniform masking allows an MLM to minimize its training objective by latching onto shallow local signals, leading to pretraining inefficiency and suboptimal downstream performance. To address this flaw, we propose PMI-Masking, a principled masking strategy based on the concept of Pointwise Mutual Information (PMI), which jointly masks a token n-gram if it exhibits high collocation over the corpus. PMI-Masking motivates, unifies, and improves upon prior more heuristic approaches that attempt to address the drawback of random uniform token masking, such as whole-word masking, entity/phrase masking, and random-span masking. Specifically, we show experimentally that PMI-Masking reaches the performance of prior masking approaches in half the training time, and consistently improves performance at the end of pretraining",
    "checked": true,
    "id": "518cb6d4247bdebf21e2811f296b0c7372602a0a",
    "semantic_title": "pmi-masking: principled masking of correlated spans",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=KUDUoRsEphu": {
    "title": "Learning Incompressible Fluid Dynamics from Scratch - Towards Fast, Differentiable Fluid Models that Generalize",
    "volume": "spotlight",
    "abstract": "Fast and stable fluid simulations are an essential prerequisite for applications ranging from computer-generated imagery to computer-aided design in research and development. However, solving the partial differential equations of incompressible fluids is a challenging task and traditional numerical approximation schemes come at high computational costs. Recent deep learning based approaches promise vast speed-ups but do not generalize to new fluid domains, require fluid simulation data for training, or rely on complex pipelines that outsource major parts of the fluid simulation to traditional methods. In this work, we propose a novel physics-constrained training approach that generalizes to new fluid domains, requires no fluid simulation data, and allows convolutional neural networks to map a fluid state from time-point t to a subsequent state at time t+dt in a single forward pass. This simplifies the pipeline to train and evaluate neural fluid models. After training, the framework yields models that are capable of fast fluid simulations and can handle various fluid phenomena including the Magnus effect and Kármán vortex streets. We present an interactive real-time demo to show the speed and generalization capabilities of our trained models. Moreover, the trained neural networks are efficient differentiable fluid solvers as they offer a differentiable update step to advance the fluid simulation in time. We exploit this fact in a proof-of-concept optimal control experiment. Our models significantly outperform a recent differentiable fluid solver in terms of computational speed and accuracy",
    "checked": true,
    "id": "e6443acbc6ea9568ecbca57d1763f84681f7af9a",
    "semantic_title": "learning incompressible fluid dynamics from scratch - towards fast, differentiable fluid models that generalize",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=rumv7QmLUue": {
    "title": "A Gradient Flow Framework For Analyzing Network Pruning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "77f6796b250eac27432d7c36449c57b7d689db49",
    "semantic_title": "a gradient flow framework for analyzing network pruning",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=ks5nebunVn_": {
    "title": "Towards Robustness Against Natural Language Word Substitutions",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f659031ceb7bbdcb7b0690742f35e2924fd1ed75",
    "semantic_title": "towards robustness against natural language word substitutions",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=R4aWTjmrEKM": {
    "title": "Iterative Empirical Game Solving via Single Policy Best Response",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "0debec877210ece112da2ede80681605c77d1887",
    "semantic_title": "iterative empirical game solving via single policy best response",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=o_V-MjyyGV_": {
    "title": "Self-Supervised Policy Adaptation during Deployment",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "798786f2d7f31b5361d700d3891a72e1096e5c8e",
    "semantic_title": "self-supervised policy adaptation during deployment",
    "citation_count": 168,
    "authors": []
  },
  "https://openreview.net/forum?id=YTWGvpFOQD-": {
    "title": "Differentially Private Learning Needs Better Features (or Much More Data)",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f864273db01ce9b728c3c16b08a5f7b22b917efb",
    "semantic_title": "differentially private learning needs better features (or much more data)",
    "citation_count": 275,
    "authors": []
  },
  "https://openreview.net/forum?id=uCQfPZwRaUu": {
    "title": "Data-Efficient Reinforcement Learning with Self-Predictive Representations",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "7c4356ec0dca6e6df0af7a882e2cd1571c8bf3dc",
    "semantic_title": "data-efficient reinforcement learning with self-predictive representations",
    "citation_count": 331,
    "authors": []
  },
  "https://openreview.net/forum?id=wS0UFjsNYjn": {
    "title": "Meta-GMVAE: Mixture of Gaussian VAE for Unsupervised Meta-Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "c073ec91babb4c0384f4914a8ecc7774aa902aaf",
    "semantic_title": "meta-gmvae: mixture of gaussian vae for unsupervised meta-learning",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=0N8jUH4JMv6": {
    "title": "Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1fde90108b8984c5d7c7f6d06fa4919a2a5bbd36",
    "semantic_title": "implicit convex regularizers of cnn architectures: convex optimization of two- and three-layer networks in polynomial time",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=GY6-6sTvGaf": {
    "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "6568423cfaca7e24c88ea208cb0e67129e43aa9b",
    "semantic_title": "image augmentation is all you need: regularizing deep reinforcement learning from pixels",
    "citation_count": 806,
    "authors": []
  },
  "https://openreview.net/forum?id=Vfs_2RnOD0H": {
    "title": "Dynamic Tensor Rematerialization",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "3c55dd7b8da5c7b47e91b2e749c264f50d007cd4",
    "semantic_title": "dynamic tensor rematerialization",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=VqzVhqxkjH1": {
    "title": "Deep Neural Network Fingerprinting by Conferrable Adversarial Examples",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "5effd428e3012061ba3d8b32f1952c2938c7ab7b",
    "semantic_title": "deep neural network fingerprinting by conferrable adversarial examples",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=UcoXdfrORC": {
    "title": "Model-Based Visual Planning with Self-Supervised Functional Distances",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "9a689727d040a6bf122f16ea50884d5cd5258321",
    "semantic_title": "model-based visual planning with self-supervised functional distances",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=YmqAnY0CMEy": {
    "title": "Mathematical Reasoning via Self-supervised Skip-tree Training",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "45bb43cdc35324fea4350ed335c500d4a5fd6ef5",
    "semantic_title": "mathematical reasoning via self-supervised skip-tree training",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=eMP1j9efXtX": {
    "title": "DeepAveragers: Offline Reinforcement Learning By Solving Derived Non-Parametric MDPs",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "14e6502e4a63635ef01790471f45f369de72a1c0",
    "semantic_title": "deepaveragers: offline reinforcement learning by solving derived non-parametric mdps",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=p-NZIuwqhI4": {
    "title": "On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "6ad0c5c675d562c6855927b6b3279de4e68dc0ea",
    "semantic_title": "on the theory of implicit deep learning: global convergence with implicit layers",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=yHeg4PbFHh": {
    "title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "30a156f17ca8f54aa14d01d32c2315c11fcbe723",
    "semantic_title": "bustle: bottom-up program-synthesis through learning-guided exploration",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=qYda4oLEc1": {
    "title": "The Traveling Observer Model: Multi-task Learning Through Spatial Variable Embeddings",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "090e15ad813a07b00215dcb74ac57a947716188c",
    "semantic_title": "the traveling observer model: multi-task learning through spatial variable embeddings",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=NECTfffOvn1": {
    "title": "Fidelity-based Deep Adiabatic Scheduling",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "8a6df3c492e0ba13e64ea0b8ac97a961bf863c32",
    "semantic_title": "fidelity-based deep adiabatic scheduling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cri3xz59ga": {
    "title": "Deciphering and Optimizing Multi-Task Learning: a Random Matrix Approach",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "4283e256359f0364eed4c5c866bdab0f864fbbc5",
    "semantic_title": "deciphering and optimizing multi-task learning: a random matrix approach",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=tilovEHA3YS": {
    "title": "Learning-based Support Estimation in Sublinear Time",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "bd5c32f003c670366e5d62c59fe02e747a470647",
    "semantic_title": "learning-based support estimation in sublinear time",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=iAmZUo0DxC0": {
    "title": "Unlearnable Examples: Making Personal Data Unexploitable",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "9a1090c590474190df976bf5a91c3e5cc1a30864",
    "semantic_title": "unlearnable examples: making personal data unexploitable",
    "citation_count": 206,
    "authors": []
  },
  "https://openreview.net/forum?id=g-wu9TMPODo": {
    "title": "How Benign is Benign Overfitting ?",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "dfb6250ae1c8f4d0ec3e28ed84596f77704485ab",
    "semantic_title": "how benign is benign overfitting?",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=5k8F6UU39V": {
    "title": "Autoregressive Entity Retrieval",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "572c12e81319ccd47cc0c637c82efadd03fd05ab",
    "semantic_title": "autoregressive entity retrieval",
    "citation_count": 462,
    "authors": []
  },
  "https://openreview.net/forum?id=SRDuJssQud": {
    "title": "Neural Approximate Sufficient Statistics for Implicit Models",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "9dfe5d4fe11d5a708e9988e9a1464fd8afe3b797",
    "semantic_title": "neural approximate sufficient statistics for implicit models",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=sSjqmfsk95O": {
    "title": "Large Scale Image Completion via Co-Modulated Generative Adversarial Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "39c2ae603902ea664adf9e74914117f79df2e612",
    "semantic_title": "large scale image completion via co-modulated generative adversarial networks",
    "citation_count": 302,
    "authors": []
  },
  "https://openreview.net/forum?id=6s7ME_X5_Un": {
    "title": "DDPNOpt: Differential Dynamic Programming Neural Optimizer",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "925e6e1e9dec14569c3fe06d238f961443b3b48e",
    "semantic_title": "differential dynamic programming neural optimizer",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=MuSYkd1hxRP": {
    "title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "301c48248ba3a4b6d726aac8ec4c3f335e3712e9",
    "semantic_title": "geometry-aware gradient algorithms for neural architecture search",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=rcQdycl0zyk": {
    "title": "Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with 1 / n Parameters",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1d5f5df837139d4ae8af23e3634295594c5b85db",
    "semantic_title": "beyond fully-connected layers with quaternions: parameterization of hypercomplex multiplications with 1/n parameters",
    "citation_count": 86,
    "authors": []
  },
  "https://openreview.net/forum?id=uXl3bZLkr3c": {
    "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "180c78b132f6369a384d22a9529551d86c8788d3",
    "semantic_title": "tent: fully test-time adaptation by entropy minimization",
    "citation_count": 1189,
    "authors": []
  },
  "https://openreview.net/forum?id=Oos98K9Lv-k": {
    "title": "Neural Topic Model via Optimal Transport",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a40b66c7650ea38e27ed5885585a10e696bbfcc9",
    "semantic_title": "neural topic model via optimal transport",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=9xC2tWEwBD": {
    "title": "A Panda? No, It's a Sloth: Slowdown Attacks on Adaptive Multi-Exit Neural Network Inference",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "14392f4d79d97a53670f9755eef7b275fc8224c0",
    "semantic_title": "a panda? no, it's a sloth: slowdown attacks on adaptive multi-exit neural network inference",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=Ut1vF_q_vC": {
    "title": "Are Neural Rankers still Outperformed by Gradient Boosted Decision Trees?",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "ed91feefa0b3de7155d510fbd527527068a71c8e",
    "semantic_title": "are neural rankers still outperformed by gradient boosted decision trees?",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=qda7-sVg84": {
    "title": "Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "7428f65393c19a6ca6381693767cb4f643a49a5c",
    "semantic_title": "contrastive behavioral similarity embeddings for generalization in reinforcement learning",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=RLRXCV6DbEJ": {
    "title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "3e577c9bdc82cb7fed337a74f90bbc4505fdfb69",
    "semantic_title": "very deep vaes generalize autoregressive models and can outperform them on images",
    "citation_count": 359,
    "authors": []
  },
  "https://openreview.net/forum?id=9EsrXMzlFQY": {
    "title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors",
    "volume": "spotlight",
    "abstract": "Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new{asynchronous RED (Async-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of Async-RED is further reduced by using a random subset of measurements at every iteration. We present a complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate Async-RED on image recovery using pre-trained deep denoisers as priors",
    "checked": true,
    "id": "13df91191a54e860d24a86fac76920e6a22505c3",
    "semantic_title": "async-red: a provably convergent asynchronous block parallel stochastic method using deep denoising priors",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=6puCSjH3hwA": {
    "title": "A Good Image Generator Is What You Need for High-Resolution Video Synthesis",
    "volume": "spotlight",
    "abstract": "Image and video synthesis are closely related areas aiming at generating content from noise. While rapid progress has been demonstrated in improving image-based models to handle large resolutions, high-quality renderings, and wide variations in image content, achieving comparable video generation results remains problematic. We present a framework that leverages contemporary image generators to render high-resolution videos. We frame the video synthesis problem as discovering a trajectory in the latent space of a pre-trained and fixed image generator. Not only does such a framework render high-resolution videos, but it also is an order of magnitude more computationally efficient. We introduce a motion generator that discovers the desired trajectory, in which content and motion are disentangled. With such a representation, our framework allows for a broad range of applications, including content and motion manipulation. Furthermore, we introduce a new task, which we call cross-domain video synthesis, in which the image and motion generators are trained on disjoint datasets belonging to different domains. This allows for generating moving objects for which the desired video data is not available. Extensive experiments on various datasets demonstrate the advantages of our methods over existing video generation techniques. Code will be released at https://github.com/snap-research/MoCoGAN-HD",
    "checked": true,
    "id": "3618e503068e5f0e4f17ad1557a9bd6692daea79",
    "semantic_title": "a good image generator is what you need for high-resolution video synthesis",
    "citation_count": 189,
    "authors": []
  },
  "https://openreview.net/forum?id=0zvfm-nZqQs": {
    "title": "Undistillable: Making A Nasty Teacher That CANNOT teach students",
    "volume": "spotlight",
    "abstract": "Knowledge Distillation (KD) is a widely used technique to transfer knowledge from pre-trained teacher models to (usually more lightweight) student models. However, in certain situations, this technique is more of a curse than a blessing. For instance, KD poses a potential risk of exposing intellectual properties (IPs): even if a trained machine learning model is released in ``black boxes'' (e.g., as executable software or APIs without open-sourcing code), it can still be replicated by KD through imitating input-output behaviors. To prevent this unwanted effect of KD, this paper introduces and investigates a concept called $\\textit{Nasty Teacher}$: a specially trained teacher network that yields nearly the same performance as a normal one, but would significantly degrade the performance of student models learned by imitating it. We propose a simple yet effective algorithm to build the nasty teacher, called $\\textit{self-undermining knowledge distillation}$. Specifically, we aim to maximize the difference between the output of the nasty teacher and a normal pre-trained network. Extensive experiments on several datasets demonstrate that our method is effective on both standard KD and data-free KD, providing the desirable KD-immunity to model owners for the first time. We hope our preliminary study can draw more awareness and interest in this new practical problem of both social and legal importance. Our codes and pre-trained models can be found at: $\\url{https://github.com/VITA-Group/Nasty-Teacher}$",
    "checked": true,
    "id": "55da1c1b1c5fd16c91a26dce223f94c2592122e9",
    "semantic_title": "undistillable: making a nasty teacher that cannot teach students",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=EqoXe2zmhrh": {
    "title": "Support-set bottlenecks for video-text representation learning",
    "volume": "spotlight",
    "abstract": "The dominant paradigm for learning video-text representations – noise contrastive learning – increases the similarity of the representations of pairs of samples that are known to be related, such as text and video from the same sample, and pushes away the representations of all other pairs. We posit that this last behaviour is too strict, enforcing dissimilar representations even for samples that are semantically-related – for example, visually similar videos or ones that share the same depicted action. In this paper, we propose a novel method that alleviates this by leveraging a generative model to naturally push these related samples together: each sample's caption must be reconstructed as a weighted combination of a support set of visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples, unlike noise contrastive learning. Our proposed method outperforms others by a large margin on MSR-VTT, VATEX, ActivityNet, and MSVD for video-to-text and text-to-video retrieval",
    "checked": true,
    "id": "78bc767ebd02c0cc690fdb334c37bf64cfaf0115",
    "semantic_title": "support-set bottlenecks for video-text representation learning",
    "citation_count": 251,
    "authors": []
  },
  "https://openreview.net/forum?id=wpSWuz_hyqA": {
    "title": "Grounded Language Learning Fast and Slow",
    "volume": "spotlight",
    "abstract": "Recent work has shown that large text-based neural language models acquire a surprising propensity for one-shot learning. Here, we show that an agent situated in a simulated 3D world, and endowed with a novel dual-coding external memory, can exhibit similar one-shot word learning when trained with conventional RL algorithms. After a single introduction to a novel object via visual perception and language (\"This is a dax\"), the agent can manipulate the object as instructed (\"Put the dax on the bed\"), combining short-term, within-episode knowledge of the nonsense word with long-term lexical and motor knowledge. We find that, under certain training conditions and with a particular memory writing mechanism, the agent's one-shot word-object binding generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects. We further show how dual-coding memory can be exploited as a signal for intrinsic motivation, stimulating the agent to seek names for objects that may be useful later. Together, the results demonstrate that deep neural networks can exploit meta-learning, episodic memory and an explicitly multi-modal environment to account for 'fast-mapping', a fundamental pillar of human cognitive development and a potentially transformative capacity for artificial agents",
    "checked": true,
    "id": "1c39625ed65389cfb9d268f93f406455665f201b",
    "semantic_title": "grounded language learning fast and slow",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=zDy_nQCXiIj": {
    "title": "GAN \"Steerability\" without optimization",
    "volume": "spotlight",
    "abstract": "Recent research has shown remarkable success in revealing \"steering\" directions in the latent spaces of pre-trained GANs. These directions correspond to semantically meaningful image transformations (e.g., shift, zoom, color manipulations), and have the same interpretable effect across all categories that the GAN can generate. Some methods focus on user-specified transformations, while others discover transformations in an unsupervised manner. However, all existing techniques rely on an optimization procedure to expose those directions, and offer no control over the degree of allowed interaction between different transformations. In this paper, we show that \"steering\" trajectories can be computed in closed form directly from the generator's weights without any form of training or optimization. This applies to user-prescribed geometric transformations, as well as to unsupervised discovery of more complex effects. Our approach allows determining both linear and nonlinear trajectories, and has many advantages over previous methods. In particular, we can control whether one transformation is allowed to come on the expense of another (e.g., zoom-in with or without allowing translation to keep the object centered). Moreover, we can determine the natural end-point of the trajectory, which corresponds to the largest extent to which a transformation can be applied without incurring degradation. Finally, we show how transferring attributes between images can be achieved without optimization, even across different categories",
    "checked": true,
    "id": "bfbb573504d446d32a85c9182db4a9d2edbbcdd6",
    "semantic_title": "gan steerability without optimization",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=80FMcTSZ6J0": {
    "title": "Noise against noise: stochastic label noise helps combat inherent label noise",
    "volume": "spotlight",
    "abstract": "The noise in stochastic gradient descent (SGD) provides a crucial implicit regularization effect, previously studied in optimization by analyzing the dynamics of parameter updates. In this paper, we are interested in learning with noisy labels, where we have a collection of samples with potential mislabeling. We show that a previously rarely discussed SGD noise, induced by stochastic label noise (SLN), mitigates the effects of inherent label noise. In contrast, the common SGD noise directly applied to model parameters does not. We formalize the differences and connections of SGD noise variants, showing that SLN induces SGD noise dependent on the sharpness of output landscape and the confidence of output probability, which may help escape from sharp minima and prevent overconfidence. SLN not only improves generalization in its simplest form but also boosts popular robust training methods, including sample selection and label correction. Specifically, we present an enhanced algorithm by applying SLN to label correction. Our code is released",
    "checked": true,
    "id": "29891cba979efd7a5a332f2210a3a9aa5192a805",
    "semantic_title": "noise against noise: stochastic label noise helps combat inherent label noise",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=5m3SEczOV8L": {
    "title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models",
    "volume": "spotlight",
    "abstract": "Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection",
    "checked": true,
    "id": "0d79c6737849bea78d5bd96c0894d9ec61190089",
    "semantic_title": "vaebm: a symbiosis between variational autoencoders and energy-based models",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=HHSEKOnPvaO": {
    "title": "Graph-Based Continual Learning",
    "volume": "spotlight",
    "abstract": "Despite significant advances, continual learning models still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. Rehearsal approaches alleviate the problem by maintaining and replaying a small episodic memory of previous samples, often implemented as an array of independent memory slots. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning",
    "checked": true,
    "id": "45a8b9a22a9794f2a586a4ffc006058625b7327e",
    "semantic_title": "graph-based continual learning",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=pBqLS-7KYAF": {
    "title": "Sparse Quantized Spectral Clustering",
    "volume": "spotlight",
    "abstract": "Given a large data matrix, sparsifying, quantizing, and/or performing other entry-wise nonlinear operations can have numerous benefits, ranging from speeding up iterative algorithms for core numerical linear algebra problems to providing nonlinear filters to design state-of-the-art neural network models. Here, we exploit tools from random matrix theory to make precise statements about how the eigenspectrum of a matrix changes under such nonlinear transformations. In particular, we show that very little change occurs in the informative eigenstructure, even under drastic sparsification/quantization, and consequently that very little downstream performance loss occurs when working with very aggressively sparsified or quantized spectral clustering problems. We illustrate how these results depend on the nonlinearity, we characterize a phase transition beyond which spectral clustering becomes possible, and we show when such nonlinear transformations can introduce spurious non-informative eigenvectors",
    "checked": true,
    "id": "6ce863bb510b204edb49aa90bf764da73810768a",
    "semantic_title": "sparse quantized spectral clustering",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=xTJEN-ggl1b": {
    "title": "LambdaNetworks: Modeling long-range Interactions without Attention",
    "volume": "spotlight",
    "abstract": "We present lambda layers -- an alternative framework to self-attention -- for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Lambda layers capture such interactions by transforming available contexts into linear functions, termed lambdas, and applying these linear functions to each input separately. Similar to linear attention, lambda layers bypass expensive attention maps, but in contrast, they model both content and position-based interactions which enables their application to large structured inputs such as images. The resulting neural network architectures, LambdaNetworks, significantly outperform their convolutional and attentional counterparts on ImageNet classification, COCO object detection and instance segmentation, while being more computationally efficient. Additionally, we design LambdaResNets, a family of hybrid architectures across different scales, that considerably improves the speed-accuracy tradeoff of image classification models. LambdaResNets reach excellent accuracies on ImageNet while being 3.2 - 4.4x faster than the popular EfficientNets on modern machine learning accelerators. In large-scale semi-supervised training with an additional 130M pseudo-labeled images, LambdaResNets achieve up to 86.7% ImageNet accuracy while being 9.5x faster than EfficientNet NoisyStudent and 9x faster than a Vision Transformer with comparable accuracies",
    "checked": true,
    "id": "cec7872b194aadf54140578b9be52939eb1112e9",
    "semantic_title": "lambdanetworks: modeling long-range interactions without attention",
    "citation_count": 181,
    "authors": []
  },
  "https://openreview.net/forum?id=MLSvqIHRidA": {
    "title": "Contrastive Divergence Learning is a Time Reversal Adversarial Game",
    "volume": "spotlight",
    "abstract": "Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step)",
    "checked": true,
    "id": "1a8104680a2b3c03109cbe33fda666876fc90e37",
    "semantic_title": "contrastive divergence learning is a time reversal adversarial game",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=LwEQnp6CYev": {
    "title": "Quantifying Differences in Reward Functions",
    "volume": "spotlight",
    "abstract": "For many tasks, the reward function is inaccessible to introspection or too complex to be specified procedurally, and must instead be learned from user data. Prior work has evaluated learned reward functions by evaluating policies optimized for the learned reward. However, this method cannot distinguish between the learned reward function failing to reflect user preferences and the policy optimization process failing to optimize the learned reward. Moreover, this method can only tell us about behavior in the evaluation environment, but the reward may incentivize very different behavior in even a slightly different deployment environment. To address these problems, we introduce the Equivalent-Policy Invariant Comparison (EPIC) distance to quantify the difference between two reward functions directly, without a policy optimization step. We prove EPIC is invariant on an equivalence class of reward functions that always induce the same optimal policy. Furthermore, we find EPIC can be efficiently approximated and is more robust than baselines to the choice of coverage distribution. Finally, we show that EPIC distance bounds the regret of optimal policies even under different transition dynamics, and we confirm empirically that it predicts policy training success. Our source code is available at https://github.com/HumanCompatibleAI/evaluating-rewards",
    "checked": true,
    "id": "18f44e605082388ffc0a59b895ff70b01cfc0034",
    "semantic_title": "quantifying differences in reward functions",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=37nvvqkCo5": {
    "title": "Long-tail learning via logit adjustment",
    "volume": "spotlight",
    "abstract": "Real-world classification problems typically exhibit an imbalanced or long-tailed label distribution, wherein many labels have only a few associated samples. This poses a challenge for generalisation on such labels, and also makes naive learning biased towards dominant labels. In this paper, we present a statistical framework that unifies and generalises several recent proposals to cope with these challenges. Our framework revisits the classic idea of logit adjustment based on the label frequencies, which encourages a large relative margin between logits of rare positive versus dominant negative labels. This yields two techniques for long-tail learning, where such adjustment is either applied post-hoc to a trained model, or enforced in the loss during training. These techniques are statistically grounded, and practically effective on four real-world datasets with long-tailed label distributions",
    "checked": true,
    "id": "4f65f604d3bf5fa91634484a3232b426267b71ef",
    "semantic_title": "long-tail learning via logit adjustment",
    "citation_count": 732,
    "authors": []
  },
  "https://openreview.net/forum?id=S0UdquAnr9k": {
    "title": "Locally Free Weight Sharing for Network Width Search",
    "volume": "spotlight",
    "abstract": "Searching for network width is an effective way to slim deep neural networks with hardware budgets. With this aim, a one-shot supernet is usually leveraged as a performance evaluator to rank the performance \\wrt~different width. Nevertheless, current methods mainly follow a manually fixed weight sharing pattern, which is limited to distinguish the performance gap of different width. In this paper, to better evaluate each width, we propose a locally free weight sharing strategy (CafeNet) accordingly. In CafeNet, weights are more freely shared, and each width is jointly indicated by its base channels and free channels, where free channels are supposed to locate freely in a local zone to better represent each width. Besides, we propose to further reduce the search space by leveraging our introduced FLOPs-sensitive bins. As a result, our CafeNet can be trained stochastically and get optimized within a min-min strategy. Extensive experiments on ImageNet, CIFAR-10, CelebA and MS COCO dataset have verified our superiority comparing to other state-of-the-art baselines. For example, our method can further boost the benchmark NAS network EfficientNet-B0 by 0.41\\% via searching its width more delicately",
    "checked": true,
    "id": "3fd03da05802b1a714cbafeec43b1b178eeb6672",
    "semantic_title": "locally free weight sharing for network width search",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=OthEq8I5v1": {
    "title": "Mutual Information State Intrinsic Control",
    "volume": "spotlight",
    "abstract": "Reinforcement learning has been shown to be highly successful at many challenging tasks. However, success heavily relies on well-shaped rewards. Intrinsically motivated RL attempts to remove this constraint by defining an intrinsic reward function. Motivated by the self-consciousness concept in psychology, we make a natural assumption that the agent knows what constitutes itself, and propose a new intrinsic objective that encourages the agent to have maximum control on the environment. We mathematically formalize this reward as the mutual information between the agent state and the surrounding state under the current agent policy. With this new intrinsic motivation, we are able to outperform previous methods, including being able to complete the pick-and-place task for the first time without using any task reward. A video showing experimental results is available at https://youtu.be/AUCwc9RThpk",
    "checked": true,
    "id": "4d12862b0c6daee864f6f5537270d95b88560ebc",
    "semantic_title": "mutual information state intrinsic control",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=WiGQBFuVRv": {
    "title": "Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows",
    "volume": "spotlight",
    "abstract": "Time series forecasting is often fundamental to scientific and engineering problems and enables decision making. With ever increasing data set sizes, a trivial solution to scale up predictions is to assume independence between interacting time series. However, modeling statistical dependencies can improve accuracy and enable analysis of interaction effects. Deep learning methods are well suited for this problem, but multi-variate models often assume a simple parametric distribution and do not scale to high dimensions. In this work we model the multi-variate temporal dynamics of time series via an autoregressive deep learning model, where the data distribution is represented by a conditioned normalizing flow. This combination retains the power of autoregressive models, such as good performance in extrapolation into the future, with the flexibility of flows as a general purpose high-dimensional distribution model, while remaining computationally tractable. We show that it improves over the state-of-the-art for standard metrics on many real-world data sets with several thousand interacting time-series",
    "checked": true,
    "id": "be2a43bfd092781058e2a1597335061d3dc5d5ce",
    "semantic_title": "multi-variate probabilistic time series forecasting via conditioned normalizing flows",
    "citation_count": 187,
    "authors": []
  },
  "https://openreview.net/forum?id=dyaIRud1zXg": {
    "title": "Information Laundering for Model Privacy",
    "volume": "spotlight",
    "abstract": "In this work, we propose information laundering, a novel framework for enhancing model privacy. Unlike data privacy that concerns the protection of raw data information, model privacy aims to protect an already-learned model that is to be deployed for public use. The private model can be obtained from general learning methods, and its deployment means that it will return a deterministic or random response for a given input query. An information-laundered model consists of probabilistic components that deliberately maneuver the intended input and output for queries of the model, so the model's adversarial acquisition is less likely. Under the proposed framework, we develop an information-theoretic principle to quantify the fundamental tradeoffs between model utility and privacy leakage and derive the optimal design",
    "checked": true,
    "id": "d2212a1c37fad937fe807dbbb44fe78396ddfd3a",
    "semantic_title": "information laundering for model privacy",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=v9c7hr9ADKx": {
    "title": "UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers",
    "volume": "spotlight",
    "abstract": "Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games). In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster)",
    "checked": true,
    "id": "21ba210633380911c1e6941d798f7eb2a67f620e",
    "semantic_title": "updet: universal multi-agent rl via policy decoupling with transformers",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=xvxPuCkCNPO": {
    "title": "Correcting experience replay for multi-agent communication",
    "volume": "spotlight",
    "abstract": "We consider the problem of learning to communicate using multi-agent reinforcement learning (MARL). A common approach is to learn off-policy, using data sampled from a replay buffer. However, messages received in the past may not accurately reflect the current communication policy of each agent, and this complicates learning. We therefore introduce a 'communication correction' which accounts for the non-stationarity of observed communication induced by multi-agent learning. It works by relabelling the received message to make it likely under the communicator's current policy, and thus be a better reflection of the receiver's current environment. To account for cases in which agents are both senders and receivers, we introduce an ordered relabelling scheme. Our correction is computationally efficient and can be integrated with a range of off-policy algorithms. We find in our experiments that it substantially improves the ability of communicating MARL systems to learn across a variety of cooperative and competitive tasks",
    "checked": true,
    "id": "e6283a93c69cd1583bba0d54ff0a86b643f09900",
    "semantic_title": "correcting experience replay for multi-agent communication",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=zQTezqCCtNx": {
    "title": "Improving Adversarial Robustness via Channel-wise Activation Suppressing",
    "volume": "spotlight",
    "abstract": "The study of adversarial examples and their activations have attracted significant attention for secure and robust learning with deep neural networks (DNNs). Different from existing works, in this paper, we highlight two new characteristics of adversarial examples from the channel-wise activation perspective: 1) the activation magnitudes of adversarial examples are higher than that of natural examples; and 2) the channels are activated more uniformly by adversarial examples than natural examples. We find that, while the state-of-the-art defense adversarial training has addressed the first issue of high activation magnitude via training on adversarial examples, the second issue of uniform activation remains. This motivates us to suppress redundant activations from being activated by adversarial perturbations during the adversarial training process, via a Channel-wise Activation Suppressing (CAS) training strategy. We show that CAS can train a model that inherently suppresses adversarial activations, and can be easily applied to existing defense methods to further improve their robustness. Our work provides a simplebut generic training strategy for robustifying the intermediate layer activations of DNNs",
    "checked": true,
    "id": "df872e72e87a85f9b5cd28da06ace46386462fde",
    "semantic_title": "improving adversarial robustness via channel-wise activation suppressing",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=D9I3drBz4UC": {
    "title": "Long-tailed Recognition by Routing Diverse Distribution-Aware Experts",
    "volume": "spotlight",
    "abstract": "Natural data are often long-tail distributed over semantic classes. Existing recognition methods tackle this imbalanced classification by placing more emphasis on the tail data, through class re-balancing/re-weighting or ensembling over different data groups, resulting in increased tail accuracies but reduced head accuracies. We take a dynamic view of the training data and provide a principled model bias and variance analysis as the training data fluctuates: Existing long-tail classifiers invariably increase the model variance and the head-tail model bias gap remains large, due to more and larger confusion with hard negatives for the tail. We propose a new long-tailed classifier called RoutIng Diverse Experts (RIDE). It reduces the model variance with multiple experts, reduces the model bias with a distribution-aware diversity loss, reduces the computational cost with a dynamic expert routing module. RIDE outperforms the state-of-the-art by 5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks. It is also a universal framework that is applicable to various backbone networks, long-tailed algorithms and training mechanisms for consistent performance gains. Our code is available at: https://github.com/frank-xwang/RIDE-LongTailRecognition",
    "checked": true,
    "id": "d618752d2e666d7b25f1bd6c7c3bd7c056e19d96",
    "semantic_title": "long-tailed recognition by routing diverse distribution-aware experts",
    "citation_count": 402,
    "authors": []
  },
  "https://openreview.net/forum?id=Tp7kI90Htd": {
    "title": "Generalization in data-driven models of primary visual cortex",
    "volume": "spotlight",
    "abstract": "Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input. Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron's receptive field position. With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7% compared to the previous state-of-the-art network. We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal. When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16",
    "checked": true,
    "id": "10c8f1586cbf19d6aa16e86897bb558df7132a8b",
    "semantic_title": "generalization in data-driven models of primary visual cortex",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=Rhsu5qD36cL": {
    "title": "Sequential Density Ratio Estimation for Simultaneous Optimization of Speed and Accuracy",
    "volume": "spotlight",
    "abstract": "Classifying sequential data as early and as accurately as possible is a challenging yet critical problem, especially when a sampling cost is high. One algorithm that achieves this goal is the sequential probability ratio test (SPRT), which is known as Bayes-optimal: it can keep the expected number of data samples as small as possible, given the desired error upper-bound. However, the original SPRT makes two critical assumptions that limit its application in real-world scenarios: (i) samples are independently and identically distributed, and (ii) the likelihood of the data being derived from each class can be calculated precisely. Here, we propose the SPRT-TANDEM, a deep neural network-based SPRT algorithm that overcomes the above two obstacles. The SPRT-TANDEM sequentially estimates the log-likelihood ratio of two alternative hypotheses by leveraging a novel Loss function for Log-Likelihood Ratio estimation (LLLR) while allowing correlations up to $N (\\in \\mathbb{N})$ preceding samples. In tests on one original and two public video databases, Nosaic MNIST, UCF101, and SiW, the SPRT-TANDEM achieves statistically significantly better classification accuracy than other baseline classifiers, with a smaller number of data samples. The code and Nosaic MNIST are publicly available at https://github.com/TaikiMiyagawa/SPRT-TANDEM",
    "checked": true,
    "id": "b894cb4685fb62b68ed01204dbe68ad8f4af59dc",
    "semantic_title": "sequential density ratio estimation for simultaneous optimization of speed and accuracy",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=eNdiU_DbM9": {
    "title": "Uncertainty Sets for Image Classifiers using Conformal Prediction",
    "volume": "spotlight",
    "abstract": "Convolutional image classifiers can achieve high predictive accuracy, but quantifying their uncertainty remains an unresolved challenge, hindering their deployment in consequential settings. Existing uncertainty quantification techniques, such as Platt scaling, attempt to calibrate the network's probability estimates, but they do not have formal guarantees. We present an algorithm that modifies any classifier to output a predictive set containing the true label with a user-specified probability, such as 90%. The algorithm is simple and fast like Platt scaling, but provides a formal finite-sample coverage guarantee for every model and dataset. Our method modifies an existing conformal prediction algorithm to give more stable predictive sets by regularizing the small scores of unlikely classes after Platt scaling. In experiments on both Imagenet and Imagenet-V2 with ResNet-152 and other classifiers, our scheme outperforms existing approaches, achieving coverage with sets that are often factors of 5 to 10 smaller than a stand-alone Platt scaling baseline",
    "checked": true,
    "id": "9cd7c6d19dedc5395155492e0193f399bbabec49",
    "semantic_title": "uncertainty sets for image classifiers using conformal prediction",
    "citation_count": 352,
    "authors": []
  },
  "https://openreview.net/forum?id=9OHFhefeB86": {
    "title": "Graph Convolution with Low-rank Learnable Local Filters",
    "volume": "spotlight",
    "abstract": "Geometric variations like rotation, scaling, and viewpoint changes pose a significant challenge to visual understanding. One common solution is to directly model certain intrinsic structures, e.g., using landmarks. However, it then becomes non-trivial to build effective deep models, especially when the underlying non-Euclidean grid is irregular and coarse. Recent deep models using graph convolutions provide an appropriate framework to handle such non-Euclidean data, but many of them, particularly those based on global graph Laplacians, lack expressiveness to capture local features required for representation of signals lying on the non-Euclidean grid. The current paper introduces a new type of graph convolution with learnable low-rank local filters, which is provably more expressive than previous spectral graph convolution methods. The model also provides a unified framework for both spectral and spatial graph convolutions. To improve model robustness, regularization by local graph Laplacians is introduced. The representation stability against input graph data perturbation is theoretically proved, making use of the graph filter locality and the local graph regularization. Experiments on spherical mesh data, real-world facial expression recognition/skeleton-based action recognition data, and data with simulated graph noise show the empirical advantage of the proposed model",
    "checked": true,
    "id": "c81531d46ca6b01e4881a216fa8079d52392d7c8",
    "semantic_title": "graph convolution with low-rank learnable local filters",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=m1CD7tPubNy": {
    "title": "Mind the Pad -- CNNs Can Develop Blind Spots",
    "volume": "spotlight",
    "abstract": "We show how feature maps in convolutional networks are susceptible to spatial bias. Due to a combination of architectural choices, the activation at certain locations is systematically elevated or weakened. The major source of this bias is the padding mechanism. Depending on several aspects of convolution arithmetic, this mechanism can apply the padding unevenly, leading to asymmetries in the learned weights. We demonstrate how such bias can be detrimental to certain tasks such as small object detection: the activation is suppressed if the stimulus lies in the impacted area, leading to blind spots and misdetection. We explore alternative padding methods and propose solutions for analyzing and mitigating spatial bias",
    "checked": true,
    "id": "f24f62fabec6bca02658c320ff9b43c84947c5de",
    "semantic_title": "mind the pad - cnns can develop blind spots",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=QfTXQiGYudJ": {
    "title": "Stabilized Medical Image Attacks",
    "volume": "spotlight",
    "abstract": "Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, a threat to these systems arises that adversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a negative influence on human healthcare. There is a need to investigate potential adversarial attacks to robustify deep medical diagnosis systems. On the other side, there are several modalities of medical images (e.g., CT, fundus, and endoscopic image) of which each type is significantly different from others. It is more challenging to generate adversarial perturbations for different types of medical images. In this paper, we propose an image-based medical adversarial attack method to consistently produce adversarial perturbations on medical images. The objective function of our method consists of a loss deviation term and a loss stabilization term. The loss deviation term increases the divergence between the CNN prediction of an adversarial example and its ground truth label. Meanwhile, the loss stabilization term ensures similar CNN predictions of this example and its smoothed input. From the perspective of the whole iterations for perturbation generation, the proposed loss stabilization term exhaustively searches the perturbation space to smooth the single spot for local optimum escape. We further analyze the KL-divergence of the proposed loss function and find that the loss stabilization term makes the perturbations updated towards a fixed objective spot while deviating from the ground truth. This stabilization ensures the proposed medical attack effective for different types of medical images while producing perturbations in small variance. Experiments on several medical image analysis benchmarks including the recent COVID-19 dataset show the stability of the proposed method",
    "checked": true,
    "id": "b7ac3263c9acd53f2699e5872cba156c353e9772",
    "semantic_title": "stabilized medical image attacks",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=73WTGs96kho": {
    "title": "Net-DNF: Effective Deep Modeling of Tabular Data",
    "volume": "poster",
    "abstract": "A challenging open question in deep learning is how to handle tabular data. Unlike domains such as image and natural language processing, where deep architectures prevail, there is still no widely accepted neural architecture that dominates tabular data. As a step toward bridging this gap, we present Net-DNF a novel generic architecture whose inductive bias elicits models whose structure corresponds to logical Boolean formulas in disjunctive normal form (DNF) over affine soft-threshold decision terms. Net-DNFs also promote localized decisions that are taken over small subsets of the features. We present an extensive experiments showing that Net-DNFs significantly and consistently outperform fully connected networks over tabular data. With relatively few hyperparameters, Net-DNFs open the door to practical end-to-end handling of tabular data using neural networks. We present ablation studies, which justify the design choices of Net-DNF including the inductive bias elements, namely, Boolean formulation, locality, and feature selection",
    "checked": true,
    "id": "af5151a0b22be3cb9a107c6af563b3603156246b",
    "semantic_title": "net-dnf: effective deep modeling of tabular data",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=mNtmhaDkAr": {
    "title": "Predicting Inductive Biases of Pre-Trained Models",
    "volume": "poster",
    "abstract": "Most current NLP systems are based on a pre-train-then-fine-tune paradigm, in which a large neural network is first trained in a self-supervised way designed to encourage the network to extract broadly-useful linguistic features, and then fine-tuned for a specific task of interest. Recent work attempts to understand why this recipe works and explain when it fails. Currently, such analyses have produced two sets of apparently-contradictory results. Work that analyzes the representations that result from pre-training (via \"probing classifiers\") finds evidence that rich features of linguistic structure can be decoded with high accuracy, but work that analyzes model behavior after fine-tuning (via \"challenge sets\") indicates that decisions are often not based on such structure but rather on spurious heuristics specific to the training set. In this work, we test the hypothesis that the extent to which a feature influences a model's decisions can be predicted using a combination of two factors: The feature's \"extractability\" after pre-training (measured using information-theoretic probing techniques), and the \"evidence\" available during fine-tuning (defined as the feature's co-occurrence rate with the label). In experiments with both synthetic and natural language data, we find strong evidence (statistically significant correlations) supporting this hypothesis",
    "checked": true,
    "id": "a33b4a2002161a18bc7eb929566d77fd5178c2e9",
    "semantic_title": "predicting inductive biases of pre-trained models",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=CBmJwzneppz": {
    "title": "Optimism in Reinforcement Learning with Generalized Linear Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "741ebf91225327bbd11e0e4fa08e329d45637b13",
    "semantic_title": "optimism in reinforcement learning with generalized linear function approximation",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=oyZxhRI2RiE": {
    "title": "SCoRe: Pre-Training for Context Representation in Conversational Semantic Parsing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ff1d3698b8d5f942e6a0775e173720210429b8ae",
    "semantic_title": "score: pre-training for context representation in conversational semantic parsing",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=ECuvULjFQia": {
    "title": "A teacher-student framework to distill future trajectories",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "72e67cf701a59dbdcec58ee61548267985f00acb",
    "semantic_title": "a teacher-student framework to distill future trajectories",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=USCNapootw": {
    "title": "Certify or Predict: Boosting Certified Robustness with Compositional Architectures",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "48b7c4785693ba582f65b266756ba1946cef4bcb",
    "semantic_title": "certify or predict: boosting certified robustness with compositional architectures",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=8VXvj1QNRl1": {
    "title": "On the Transfer of Disentangled Representations in Realistic Settings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4f3ec3b817189927171e26b3232910609a5a33a1",
    "semantic_title": "on the transfer of disentangled representations in realistic settings",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=sCZbhBvqQaU": {
    "title": "Robust Reinforcement Learning on State Observations with Learned Optimal Adversary",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1a627d2a169d71563109546da590a7cceb0b349a",
    "semantic_title": "robust reinforcement learning on state observations with learned optimal adversary",
    "citation_count": 176,
    "authors": []
  },
  "https://openreview.net/forum?id=Hf3qXoiNkR": {
    "title": "Learning from others' mistakes: Avoiding dataset biases without modeling them",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "734f85727161f27bc7b295f0140a905363202d3f",
    "semantic_title": "learning from others' mistakes: avoiding dataset biases without modeling them",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=nVZtXBI6LNn": {
    "title": "Fast and Complete: Enabling Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "01fe33d7147cfb08d4542402089535ed911b4024",
    "semantic_title": "fast and complete: enabling complete neural network verification with rapid and massively parallel incomplete verifiers",
    "citation_count": 196,
    "authors": []
  },
  "https://openreview.net/forum?id=bgQek2O63w": {
    "title": "Self-supervised Adversarial Robustness for the Low-label, High-data Regime",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a8e5c059d2acc2030663a088cd21cd198641974f",
    "semantic_title": "self-supervised adversarial robustness for the low-label, high-data regime",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=ZDnzZrTqU9N": {
    "title": "Modeling the Second Player in Distributionally Robust Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5ede529879d162d2779d410a5775d3f6cd6be3f4",
    "semantic_title": "modeling the second player in distributionally robust optimization",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=JFKR3WqwyXR": {
    "title": "Neural Jump Ordinary Differential Equations: Consistent Continuous-Time Prediction and Filtering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cb6937dc0968baa7484455468e6972ab660663cd",
    "semantic_title": "neural jump ordinary differential equations: consistent continuous-time prediction and filtering",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=0O_cQfw6uEh": {
    "title": "Gradient Origin Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0cf5e829a745058a84ea29b8827ac7acb3f66e61",
    "semantic_title": "gradient origin networks",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=rWZz3sJfCkm": {
    "title": "Efficient Generalized Spherical CNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "59b418a99f915e8bb44b642531e2c2b76a8f8a4b",
    "semantic_title": "efficient generalized spherical cnns",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=XPZIaotutsD": {
    "title": "DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583",
    "semantic_title": "deberta: decoding-enhanced bert with disentangled attention",
    "citation_count": 2852,
    "authors": []
  },
  "https://openreview.net/forum?id=-6vS_4Kfz0": {
    "title": "Optimizing Memory Placement using Evolutionary Graph Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "95cb5128f2cb9fb7fb94f2ce62cf1fb62361cc77",
    "semantic_title": "optimizing memory placement using evolutionary graph reinforcement learning",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=V8jrrnwGbuc": {
    "title": "On the geometry of generalization and memorization in deep neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5a41802f417aa7e55d76bfe5a61dae2141a7131b",
    "semantic_title": "on the geometry of generalization and memorization in deep neural networks",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=8xeBUgD8u9": {
    "title": "Continual learning in recurrent neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a935d162c604b19b32b61f0b9f05f954498d5407",
    "semantic_title": "continual learning in recurrent neural networks",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=01olnfLIbD": {
    "title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8dc712493df0a46fef830a6c3be64899880100c4",
    "semantic_title": "witches' brew: industrial scale data poisoning via gradient matching",
    "citation_count": 232,
    "authors": []
  },
  "https://openreview.net/forum?id=oFp8Mx_V5FL": {
    "title": "Overfitting for Fun and Profit: Instance-Adaptive Data Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "df80288c17b75cf9417cbf9c8e6b278deb81a0b8",
    "semantic_title": "overfitting for fun and profit: instance-adaptive data compression",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=6zaTwpNSsQ2": {
    "title": "A Block Minifloat Representation for Training Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "37b2417957088d20300a05fecc595c18ba926408",
    "semantic_title": "a block minifloat representation for training deep neural networks",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=9p2ekP904Rs": {
    "title": "Representation Learning via Invariant Causal Mechanisms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "57835c5ad5424f94ee75901c3113730f3900e656",
    "semantic_title": "representation learning via invariant causal mechanisms",
    "citation_count": 255,
    "authors": []
  },
  "https://openreview.net/forum?id=D_KeYoqCYC": {
    "title": "Sparse encoding for more-interpretable feature-selecting representations in probabilistic matrix factorization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "302c5388dfc37671ce109d65349a3c8cf0746788",
    "semantic_title": "sparse encoding for more-interpretable feature-selecting representations in probabilistic matrix factorization",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=J3OUycKwz-": {
    "title": "Mapping the Timescale Organization of Neural Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7c12489708184df6f6a8d9b9f3d8e100b6684500",
    "semantic_title": "mapping the timescale organization of neural language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=C0qJUx5dxFb": {
    "title": "Neural networks with late-phase weights",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b2344edc5f46d7a7cf6f72549ef9fd9a32585f17",
    "semantic_title": "neural networks with late-phase weights",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=Mu2ZxFctAI": {
    "title": "Uncertainty-aware Active Learning for Optimal Bayesian Classifier",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7499293ac5a03c966464bfae06ef44c1b5a7f3b9",
    "semantic_title": "uncertainty-aware active learning for optimal bayesian classifier",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=HxzSxSxLOJZ": {
    "title": "ResNet After All: Neural ODEs and Their Numerical Solution",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "59d9b7073424d2bb8f923e5c167fec21959448c8",
    "semantic_title": "resnet after all: neural odes and their numerical solution",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=_IM-AfFhna9": {
    "title": "Generalized Variational Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6b41ca9988f832737430b24bdcf37000e1af62ea",
    "semantic_title": "generalized variational continual learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=a2gqxKDvYys": {
    "title": "Mind the Gap when Conditioning Amortised Inference in Sequential Latent-Variable Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a86bbc845782a316b25137c45d9bd18c6ea05734",
    "semantic_title": "mind the gap when conditioning amortised inference in sequential latent-variable models",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=SK7A5pdrgov": {
    "title": "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2342b32e245989103dbc56d6f07f1400f4fd2e06",
    "semantic_title": "causalworld: a robotic manipulation benchmark for causal structure and transfer learning",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=fylclEqgvgd": {
    "title": "Transformer protein language models are unsupervised structure learners",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "043e5e0cf3129284683260976c10d98c7f121f35",
    "semantic_title": "transformer protein language models are unsupervised structure learners",
    "citation_count": 307,
    "authors": []
  },
  "https://openreview.net/forum?id=27acGyyI1BY": {
    "title": "Neural ODE Processes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f9dc6861bfe07c0db3d3bcb9d40c47efc816392a",
    "semantic_title": "neural ode processes",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=qbH974jKUVy": {
    "title": "The role of Disentanglement in Generalisation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f654d364231122738e8e3cd016da91cfbe3899a9",
    "semantic_title": "the role of disentanglement in generalisation",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=eU776ZYxEpz": {
    "title": "Learning to live with Dale's principle: ANNs with separate excitatory and inhibitory units",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "29f6a33bf15f44f3ecd3bb3a7aacc0d2da16b7b2",
    "semantic_title": "learning to live with dale's principle: anns with separate excitatory and inhibitory units",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=7EDgLu9reQD": {
    "title": "SALD: Sign Agnostic Learning with Derivatives",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "7e91d3e42fafcb5c3c1f411b30f5c10a3ad2bd6b",
    "semantic_title": "sal++: sign agnostic learning with derivatives",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=TaYhv-q1Xit": {
    "title": "Ringing ReLUs: Harmonic Distortion Analysis of Nonlinear Feedforward Networks",
    "volume": "poster",
    "abstract": "In this paper, we apply harmonic distortion analysis to understand the effect of nonlinearities in the spectral domain. Each nonlinear layer creates higher-frequency harmonics, which we call \"blueshift\", whose magnitude increases with network depth, thereby increasing the \"roughness\" of the output landscape. Unlike differential models (such as vanishing gradients, sharpness), this provides a more global view of how network architectures behave across larger areas of their parameter domain. For example, the model predicts that residual connections are able to counter the effect by dampening corresponding higher frequency modes. We empirically verify the connection between blueshift and architectural choices, and provide evidence for a connection with trainability",
    "checked": true,
    "id": "f2cf28e9e5d51129476007a64c6b6547be5aed6b",
    "semantic_title": "ringing relus: harmonic distortion analysis of nonlinear feedforward networks",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=VD_ozqvBy4W": {
    "title": "CoCon: A Self-Supervised Approach for Controlled Text Generation",
    "volume": "poster",
    "abstract": "Pretrained Transformer-based language models (LMs) display remarkable natural language generation capabilities. With their immense potential, controlling text generation of such LMs is getting attention. While there are studies that seek to control high-level attributes (such as sentiment and topic) of generated text, there is still a lack of more precise control over its content at the word- and phrase-level. Here, we propose Content-Conditioner (CoCon) to control an LM's output text with a content input, at a fine-grained level. In our self-supervised approach, the CoCon block learns to help the LM complete a partially-observed text sequence by conditioning with content inputs that are withheld from the LM. Through experiments, we show that CoCon can naturally incorporate target content into generated texts and control high-level text attributes in a zero-shot manner",
    "checked": true,
    "id": "9b975cd0e9cb330300062916c72df3d63e1db207",
    "semantic_title": "cocon: a self-supervised approach for controlled text generation",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=o3iritJHLfO": {
    "title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech",
    "volume": "poster",
    "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters",
    "checked": true,
    "id": "29e87771996a94d3c0aac39eaf6614a19dbcf5ca",
    "semantic_title": "bidirectional variational inference for non-autoregressive text-to-speech",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=aUX5Plaq7Oy": {
    "title": "Learning continuous-time PDEs from sparse data with graph neural networks",
    "volume": "poster",
    "abstract": "The behavior of many dynamical systems follow complex, yet still unknown partial differential equations (PDEs). While several machine learning methods have been proposed to learn PDEs directly from data, previous methods are limited to discrete-time approximations or make the limiting assumption of the observations arriving at regular grids. We propose a general continuous-time differential model for dynamical systems whose governing equations are parameterized by message passing graph neural networks. The model admits arbitrary space and time discretizations, which removes constraints on the locations of observation points and time intervals between the observations. The model is trained with continuous-time adjoint method enabling efficient neural PDE inference. We demonstrate the model's ability to work with unstructured grids, arbitrary time steps, and noisy observations. We compare our method with existing approaches on several well-known physical systems that involve first and higher-order PDEs with state-of-the-art predictive performance",
    "checked": true,
    "id": "82977309d976e8146a5376df06b35394b02d47f3",
    "semantic_title": "learning continuous-time pdes from sparse data with graph neural networks",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=CU0APx9LMaL": {
    "title": "NAS-Bench-ASR: Reproducible Neural Architecture Search for Speech Recognition",
    "volume": "poster",
    "abstract": "Powered by innovations in novel architecture design, noise tolerance techniques and increasing model capacity, Automatic Speech Recognition (ASR) has made giant strides in reducing word-error-rate over the past decade. ASR models are often trained with tens of thousand hours of high quality speech data to produce state-of-the-art (SOTA) results. Industry-scale ASR model training thus remains computationally heavy and time-consuming, and consequently has attracted little attention in adopting automatic techniques. On the other hand, Neural Architecture Search (NAS) has gained a lot of interest in the recent years thanks to its successes in discovering efficient architectures, often outperforming handcrafted alternatives. However, by changing the standard training process into a bi-level optimisation problem, NAS approaches often require significantly more time and computational power compared to single-model training, and at the same time increase complexity of the overall process. As a result, NAS has been predominately applied to problems which do not require as extensive training as ASR, and even then reproducibility of NAS algorithms is often problematic. Lately, a number of benchmark datasets has been introduced to address reproducibility issues by pro- viding NAS researchers with information about performance of different models obtained through exhaustive evaluation. However, these datasets focus mainly on computer vision and NLP tasks and thus suffer from limited coverage of application domains. In order to increase diversity in the existing NAS benchmarks, and at the same time provide systematic study of the effects of architectural choices for ASR, we release NAS-Bench-ASR – the first NAS benchmark for ASR models. The dataset consists of 8, 242 unique models trained on the TIMIT audio dataset for three different target epochs, and each starting from three different initializations. The dataset also includes runtime measurements of all the models on a diverse set of hardware platforms. Lastly, we show that identified good cell structures in our search space for TIMIT transfer well to a much larger LibriSpeech dataset",
    "checked": true,
    "id": "d0cf9923b9f9f58522b1407df60e6f96d4588f29",
    "semantic_title": "nas-bench-asr: reproducible neural architecture search for speech recognition",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=ULQdiUTHe3y": {
    "title": "Collective Robustness Certificates: Exploiting Interdependence in Graph Neural Networks",
    "volume": "poster",
    "abstract": "In tasks like node classification, image segmentation, and named-entity recognition we have a classifier that simultaneously outputs multiple predictions (a vector of labels) based on a single input, i.e. a single graph, image, or document respectively. Existing adversarial robustness certificates consider each prediction independently and are thus overly pessimistic for such tasks. They implicitly assume that an adversary can use different perturbed inputs to attack different predictions, ignoring the fact that we have a single shared input. We propose the first collective robustness certificate which computes the number of predictions that are simultaneously guaranteed to remain stable under perturbation, i.e. cannot be attacked. We focus on Graph Neural Networks and leverage their locality property - perturbations only affect the predictions in a close neighborhood - to fuse multiple single-node certificates into a drastically stronger collective certificate. For example, on the Citeseer dataset our collective certificate for node classification increases the average number of certifiable feature perturbations from $7$ to $351$",
    "checked": true,
    "id": "46e95e212e74fd2b5f9f090dcf7f646651f04f76",
    "semantic_title": "collective robustness certificates: exploiting interdependence in graph neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_mQp5cr_iNy": {
    "title": "Adversarially Guided Actor-Critic",
    "volume": "poster",
    "abstract": "Despite definite success in deep reinforcement learning problems, actor-critic algorithms are still confronted with sample inefficiency in complex environments, particularly in tasks where efficient exploration is a bottleneck. These methods consider a policy (the actor) and a value function (the critic) whose respective losses are built using different motivations and approaches. This paper introduces a third protagonist: the adversary. While the adversary mimics the actor by minimizing the KL-divergence between their respective action distributions, the actor, in addition to learning to solve the task, tries to differentiate itself from the adversary predictions. This novel objective stimulates the actor to follow strategies that could not have been correctly predicted from previous trajectories, making its behavior innovative in tasks where the reward is extremely rare. Our experimental analysis shows that the resulting Adversarially Guided Actor-Critic (AGAC) algorithm leads to more exhaustive exploration. Notably, AGAC outperforms current state-of-the-art methods on a set of various hard-exploration and procedurally-generated tasks",
    "checked": true,
    "id": "60056a89051545aae35d721adc62677f2ea3ee05",
    "semantic_title": "adversarially guided actor-critic",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=OGg9XnKxFAH": {
    "title": "Training independent subnetworks for robust prediction",
    "volume": "poster",
    "abstract": "Recent approaches to efficiently ensemble neural networks have shown that strong robustness and uncertainty performance can be achieved with a negligible gain in parameters over the original network. However, these methods still require multiple forward passes for prediction, leading to a significant runtime cost. In this work, we show a surprising result: the benefits of using multiple predictions can be achieved 'for free' under a single model's forward pass. In particular, we show that, using a multi-input multi-output (MIMO) configuration, one can utilize a single model's capacity to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the subnetworks, we improve model robustness without increasing compute. We observe a significant improvement in negative log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet, and their out-of-distribution variants compared to previous methods",
    "checked": true,
    "id": "4ea5678069a6c4213f53972872a211d780f9f42b",
    "semantic_title": "training independent subnetworks for robust prediction",
    "citation_count": 214,
    "authors": []
  },
  "https://openreview.net/forum?id=chPj_I5KMHG": {
    "title": "Grounding Language to Autonomously-Acquired Skills via Goal Generation",
    "volume": "poster",
    "abstract": "We are interested in the autonomous acquisition of repertoires of skills. Language-conditioned reinforcement learning (LC-RL) approaches are great tools in this quest, as they allow to express abstract goals as sets of constraints on the states. However, most LC-RL agents are not autonomous and cannot learn without external instructions and feedback. Besides, their direct language condition cannot account for the goal-directed behavior of pre-verbal infants and strongly limits the expression of behavioral diversity for a given language input. To resolve these issues, we propose a new conceptual approach to language-conditioned RL: the Language-Goal-Behavior architecture (LGB). LGB decouples skill learning and language grounding via an intermediate semantic representation of the world. To showcase the properties of LGB, we present a specific implementation called DECSTR. DECSTR is an intrinsically motivated learning agent endowed with an innate semantic representation describing spatial relations between physical objects. In a first stage G -> B, it freely explores its environment and targets self-generated semantic configurations. In a second stage (L -> G), it trains a language-conditioned goal generator to generate semantic goals that match the constraints expressed in language-based inputs. We showcase the additional properties of LGB w.r.t. both an end-to-end LC-RL approach and a similar approach leveraging non-semantic, continuous intermediate representations. Intermediate semantic representations help satisfy language commands in a diversity of ways, enable strategy switching after a failure and facilitate language grounding",
    "checked": true,
    "id": "0b22f92be890b720eaa97fa75a49560f11ff2ab2",
    "semantic_title": "grounding language to autonomously-acquired skills via goal generation",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=tL89RnzIiCd": {
    "title": "Hopfield Networks is All You Need",
    "volume": "poster",
    "abstract": "We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: \\url{https://github.com/ml-jku/hopfield-layers}",
    "checked": true,
    "id": "804a6d7c23335bbca6eec3b7d3c8366dcbe395a5",
    "semantic_title": "hopfield networks is all you need",
    "citation_count": 458,
    "authors": []
  },
  "https://openreview.net/forum?id=qYZD-AO1Vn": {
    "title": "Differentiable Trust Region Layers for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "Trust region methods are a popular tool in reinforcement learning as they yield robust policy updates in continuous and discrete action spaces. However, enforcing such trust regions in deep reinforcement learning is difficult. Hence, many approaches, such as Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), are based on approximations. Due to those approximations, they violate the constraints or fail to find the optimal solution within the trust region. Moreover, they are difficult to implement, often lack sufficient exploration, and have been shown to depend on seemingly unrelated implementation choices. In this work, we propose differentiable neural network layers to enforce trust regions for deep Gaussian policies via closed-form projections. Unlike existing methods, those layers formalize trust regions for each state individually and can complement existing reinforcement learning algorithms. We derive trust region projections based on the Kullback-Leibler divergence, the Wasserstein L2 distance, and the Frobenius norm for Gaussian distributions. We empirically demonstrate that those projection layers achieve similar or better results than existing methods while being almost agnostic to specific implementation choices. The code is available at https://git.io/Jthb0",
    "checked": true,
    "id": "0efa2efa49a1923954d69eaa9f898af22f63a983",
    "semantic_title": "differentiable trust region layers for deep reinforcement learning",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=ONBPHFZ7zG4": {
    "title": "Temporally-Extended ε-Greedy Exploration",
    "volume": "poster",
    "abstract": "Recent work on exploration in reinforcement learning (RL) has led to a series of increasingly complex solutions to the problem. This increase in complexity often comes at the expense of generality. Recent empirical studies suggest that, when applied to a broader set of domains, some sophisticated exploration methods are outperformed by simpler counterparts, such as ε-greedy. In this paper we propose an exploration algorithm that retains the simplicity of ε-greedy while reducing dithering. We build on a simple hypothesis: the main limitation of ε-greedy exploration is its lack of temporal persistence, which limits its ability to escape local optima. We propose a temporally extended form of ε-greedy that simply repeats the sampled action for a random duration. It turns out that, for many duration distributions, this suffices to improve exploration on a large set of domains. Interestingly, a class of distributions inspired by ecological models of animal foraging behaviour yields particularly strong performance",
    "checked": true,
    "id": "5de6d8b4436f6598f5bcba00bc07d864e962f1fb",
    "semantic_title": "temporally-extended {\\epsilon}-greedy exploration",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=TuK6agbdt27": {
    "title": "Learning Associative Inference Using Fast Weight Memory",
    "volume": "poster",
    "abstract": "Humans can quickly associate stimuli to solve problems in novel contexts. Our novel neural network model learns state representations of facts that can be composed to perform such associative inference. To this end, we augment the LSTM model with an associative memory, dubbed \\textit{Fast Weight Memory} (FWM). Through differentiable operations at every step of a given input sequence, the LSTM \\textit{updates and maintains} compositional associations stored in the rapidly changing FWM weights. Our model is trained end-to-end by gradient descent and yields excellent performance on compositional language reasoning problems, meta-reinforcement-learning for POMDPs, and small-scale word-level language modelling",
    "checked": true,
    "id": "5e11e806d24dd80ecf0f91e7aacedbba8d9fd6fc",
    "semantic_title": "learning associative inference using fast weight memory",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=xoHdgbQJohv": {
    "title": "Multiscale Score Matching for Out-of-Distribution Detection",
    "volume": "poster",
    "abstract": "We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for $L$ levels of noise. Once trained, we calculate the noisy score estimates for $N$ in-distribution samples and take the L2-norms across the input dimensions (resulting in an $N$x$L$ matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this $L$-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models",
    "checked": true,
    "id": "703ffd7ab0bdfcb091400ebb9c7b92446204831f",
    "semantic_title": "multiscale score matching for out-of-distribution detection",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=gJYlaqL8i8": {
    "title": "Learning to Sample with Local and Global Contexts in Experience Replay Buffer",
    "volume": "poster",
    "abstract": "Experience replay, which enables the agents to remember and reuse experience from the past, has played a significant role in the success of off-policy reinforcement learning (RL). To utilize the experience replay efficiently, the existing sampling methods allow selecting out more meaningful experiences by imposing priorities on them based on certain metrics (e.g. TD-error). However, they may result in sampling highly biased, redundant transitions since they compute the sampling rate for each transition independently, without consideration of its importance in relation to other transitions. In this paper, we aim to address the issue by proposing a new learning-based sampling method that can compute the relative importance of transition. To this end, we design a novel permutation-equivariant neural architecture that takes contexts from not only features of each transition (local) but also those of others (global) as inputs. We validate our framework, which we refer to as Neural Experience Replay Sampler (NERS), on multiple benchmark tasks for both continuous and discrete control tasks and show that it can significantly improve the performance of various off-policy RL methods. Further analysis confirms that the improvements of the sample efficiency indeed are due to sampling diverse and meaningful transitions by NERS that considers both local and global contexts",
    "checked": true,
    "id": "49f54e261633cd53034d85f777b393f41001c289",
    "semantic_title": "learning to sample with local and global contexts in experience replay buffer",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=tV6oBfuyLTQ": {
    "title": "Parameter-Based Value Functions",
    "volume": "poster",
    "abstract": "Traditional off-policy actor-critic Reinforcement Learning (RL) algorithms learn value functions of a single target policy. However, when value functions are updated to track the learned policy, they forget potentially useful information about old policies. We introduce a class of value functions called Parameter-Based Value Functions (PBVFs) whose inputs include the policy parameters. They can generalize across different policies. PBVFs can evaluate the performance of any policy given a state, a state-action pair, or a distribution over the RL agent's initial states. First we show how PBVFs yield novel off-policy policy gradient theorems. Then we derive off-policy actor-critic algorithms based on PBVFs trained by Monte Carlo or Temporal Difference methods. We show how learned PBVFs can zero-shot learn new policies that outperform any policy seen during training. Finally our algorithms are evaluated on a selection of discrete and continuous control tasks using shallow policies and deep neural networks. Their performance is comparable to state-of-the-art methods",
    "checked": true,
    "id": "071f7a580657c000dc911f933d0386b3493fbb0c",
    "semantic_title": "parameter-based value functions",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=t86MwoUCCNe": {
    "title": "New Bounds For Distributed Mean Estimation and Variance Reduction",
    "volume": "poster",
    "abstract": "We consider the problem of distributed mean estimation (DME), in which $n$ machines are each given a local $d$-dimensional vector $\\mathbf x_v \\in \\mathbb R^d$, and must cooperate to estimate the mean of their inputs $\\mathbf \\mu = \\frac 1n\\sum_{v = 1}^n \\mathbf x_v$, while minimizing total communication cost. DME is a fundamental construct in distributed machine learning, and there has been considerable work on variants of this problem, especially in the context of distributed variance reduction for stochastic gradients in parallel SGD. Previous work typically assumes an upper bound on the norm of the input vectors, and achieves an error bound in terms of this norm. However, in many real applications, the input vectors are concentrated around the correct output $\\mathbf \\mu$, but $\\mathbf \\mu$ itself has large norm. In such cases, previous output error bounds perform poorly. In this paper, we show that output error bounds need not depend on input norm. We provide a method of quantization which allows distributed mean estimation to be performed with solution quality dependent only on the distance between inputs, not on input norm, and show an analogous result for distributed variance reduction. The technique is based on a new connection with lattice theory. We also provide lower bounds showing that the communication to error trade-off of our algorithms is asymptotically optimal. As the lattices achieving optimal bounds under $\\ell_2$-norm can be computationally impractical, we also present an extension which leverages easy-to-use cubic lattices, and is loose only up to a logarithmic factor in $d$. We show experimentally that our method yields practical improvements for common applications, relative to prior approaches",
    "checked": true,
    "id": "a50307a3a567966368c3e2b4c4b374b9d75766d3",
    "semantic_title": "new bounds for distributed mean estimation and variance reduction",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=cR91FAodFMe": {
    "title": "Learning to Set Waypoints for Audio-Visual Navigation",
    "volume": "poster",
    "abstract": "In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation",
    "checked": true,
    "id": "8170b0ee2d609a4a7a0f3d5816fea70851b032b9",
    "semantic_title": "learning to set waypoints for audio-visual navigation",
    "citation_count": 110,
    "authors": []
  },
  "https://openreview.net/forum?id=K5j7D81ABvt": {
    "title": "Disambiguating Symbolic Expressions in Informal Documents",
    "volume": "poster",
    "abstract": "We propose the task of \\emph{disambiguating} symbolic expressions in informal STEM documents in the form of \\LaTeX files -- that is, determining their precise semantics and abstract syntax tree -- as a neural machine translation task. We discuss the distinct challenges involved and present a dataset with roughly 33,000 entries. We evaluated several baseline models on this dataset, which failed to yield even syntactically valid \\LaTeX before overfitting. Consequently, we describe a methodology using a \\emph{transformer} language model pre-trained on sources obtained from \\url{arxiv.org}, which yields promising results despite the small size of the dataset. We evaluate our model using a plurality of dedicated techniques, taking syntax and semantics of symbolic expressions into account",
    "checked": true,
    "id": "e7b959c2e2bc3e823ba9d51213f604530c242912",
    "semantic_title": "disambiguating symbolic expressions in informal documents",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=5NA1PinlGFu": {
    "title": "Colorization Transformer",
    "volume": "poster",
    "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available at https://github.com/google-research/google-research/tree/master/coltran",
    "checked": true,
    "id": "30f326353dfeed21216c1cf98d3c42d794fa054e",
    "semantic_title": "colorization transformer",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=SZ3wtsXfzQR": {
    "title": "Theoretical bounds on estimation error for meta-learning",
    "volume": "poster",
    "abstract": "Machine learning models have traditionally been developed under the assumption that the training and test distributions match exactly. However, recent success in few-shot learning and related problems are encouraging signs that these models can be adapted to more realistic settings where train and test distributions differ. Unfortunately, there is severely limited theoretical support for these algorithms and little is known about the difficulty of these problems. In this work, we provide novel information-theoretic lower-bounds on minimax rates of convergence for algorithms that are trained on data from multiple sources and tested on novel data. Our bounds depend intuitively on the information shared between sources of data, and characterize the difficulty of learning in this setting for arbitrary algorithms. We demonstrate these bounds on a hierarchical Bayesian model of meta-learning, computing both upper and lower bounds on parameter estimation via maximum-a-posteriori inference",
    "checked": true,
    "id": "3aa54ef5d0bc888f69c92edaacc090049eea1922",
    "semantic_title": "theoretical bounds on estimation error for meta-learning",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=kvhzKz-_DMF": {
    "title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning",
    "volume": "poster",
    "abstract": "While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task. We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting. Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks. Our code is publicly available in https://github.com/rabeehk/vibert",
    "checked": true,
    "id": "a4b9530c820b5fdb9edd869555cc7a2c2a1d21ad",
    "semantic_title": "variational information bottleneck for effective low-resource fine-tuning",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=IqtonxWI0V3": {
    "title": "TropEx: An Algorithm for Extracting Linear Terms in Deep Neural Networks",
    "volume": "poster",
    "abstract": "Deep neural networks with rectified linear (ReLU) activations are piecewise linear functions, where hyperplanes partition the input space into an astronomically high number of linear regions. Previous work focused on counting linear regions to measure the network's expressive power and on analyzing geometric properties of the hyperplane configurations. In contrast, we aim to understand the impact of the linear terms on network performance, by examining the information encoded in their coefficients. To this end, we derive TropEx, a nontrivial tropical algebra-inspired algorithm to systematically extract linear terms based on data. Applied to convolutional and fully-connected networks, our algorithm uncovers significant differences in how the different networks utilize linear regions for generalization. This underlines the importance of systematic linear term exploration, to better understand generalization in neural networks trained with complex data sets",
    "checked": true,
    "id": "54f35ae5c2d38d0150ddf601470ed80792e136fa",
    "semantic_title": "tropex: an algorithm for extracting linear terms in deep neural networks",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=dx4b7lm8jMM": {
    "title": "Seq2Tens: An Efficient Representation of Sequences by Low-Rank Tensor Projections",
    "volume": "poster",
    "abstract": "Sequential data such as time series, video, or text can be challenging to analyse as the ordered structure gives rise to complex dependencies. At the heart of this is non-commutativity, in the sense that reordering the elements of a sequence can completely change its meaning. We use a classical mathematical object -- the free algebra -- to capture this non-commutativity. To address the innate computational complexity of this algebra, we use compositions of low-rank tensor projections. This yields modular and scalable building blocks that give state-of-the-art performance on standard benchmarks such as multivariate time series classification, mortality prediction and generative models for video",
    "checked": true,
    "id": "79ed5d46d114f8b6de92329694907570b769f408",
    "semantic_title": "seq2tens: an efficient representation of sequences by low-rank tensor projections",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=TVjLza1t4hI": {
    "title": "Representation learning for improved interpretability and classification accuracy of clinical factors from EEG",
    "volume": "poster",
    "abstract": "Despite extensive standardization, diagnostic interviews for mental health disorders encompass substantial subjective judgment. Previous studies have demonstrated that EEG-based neural measures can function as reliable objective correlates of depression, or even predictors of depression and its course. However, their clinical utility has not been fully realized because of 1) the lack of automated ways to deal with the inherent noise associated with EEG data at scale, and 2) the lack of knowledge of which aspects of the EEG signal may be markers of a clinical disorder. Here we adapt an unsupervised pipeline from the recent deep representation learning literature to address these problems by 1) learning a disentangled representation using $\\beta$-VAE to denoise the signal, and 2) extracting interpretable features associated with a sparse set of clinical labels using a Symbol-Concept Association Network (SCAN). We demonstrate that our method is able to outperform the canonical hand-engineered baseline classification method on a number of factors, including participant age and depression diagnosis. Furthermore, our method recovers a representation that can be used to automatically extract denoised Event Related Potentials (ERPs) from novel, single EEG trajectories, and supports fast supervised re-mapping to various clinical labels, allowing clinicians to re-use a single EEG representation regardless of updates to the standardized diagnostic system. Finally, single factors of the learned disentangled representations often correspond to meaningful markers of clinical factors, as automatically detected by SCAN, allowing for human interpretability and post-hoc expert analysis of the recommendations made by the model",
    "checked": true,
    "id": "723b7790853f03d13cdc5e2d4b18aef49d639dae",
    "semantic_title": "representation learning for improved interpretability and classification accuracy of clinical factors from eeg",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=Xh5eMZVONGF": {
    "title": "Language-Agnostic Representation Learning of Source Code from Structure and Context",
    "volume": "poster",
    "abstract": "Source code (Context) and its parsed abstract syntax tree (AST; Structure) are two complementary representations of the same computer program. Traditionally, designers of machine learning models have relied predominantly either on Structure or Context. We propose a new model, which jointly learns on Context and Structure of source code. In contrast to previous approaches, our model uses only language-agnostic features, i.e., source code and features that can be computed directly from the AST. Besides obtaining state-of-the-art on monolingual code summarization on all five programming languages considered in this work, we propose the first multilingual code summarization model. We show that jointly training on non-parallel data from multiple programming languages improves results on all individual languages, where the strongest gains are on low-resource languages. Remarkably, multilingual training only from Context does not lead to the same improvements, highlighting the benefits of combining Structure and Context for representation learning on code",
    "checked": true,
    "id": "05e396e79a2f88f0b8f8d99f5ab36ab3efa95c14",
    "semantic_title": "language-agnostic representation learning of source code from structure and context",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=5Y21V0RDBV": {
    "title": "Generalized Multimodal ELBO",
    "volume": "poster",
    "abstract": "Multiple data types naturally co-occur when describing real-world phenomena and learning from them is a long-standing goal in machine learning research. However, existing self-supervised generative models approximating an ELBO are not able to fulfill all desired requirements of multimodal models: their posterior approximation functions lead to a trade-off between the semantic coherence and the ability to learn the joint data distribution. We propose a new, generalized ELBO formulation for multimodal data that overcomes these limitations. The new objective encompasses two previous methods as special cases and combines their benefits without compromises. In extensive experiments, we demonstrate the advantage of the proposed method compared to state-of-the-art models in self-supervised, generative learning tasks",
    "checked": true,
    "id": "8de5f12826bd6726f541c8155191a3127eec3710",
    "semantic_title": "generalized multimodal elbo",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=p5uylG94S68": {
    "title": "Model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose?",
    "volume": "poster",
    "abstract": "We contribute to micro-data model-based reinforcement learning (MBRL) by rigorously comparing popular generative models using a fixed (random shooting) control agent. We find that on an environment that requires multimodal posterior predictives, mixture density nets outperform all other models by a large margin. When multimodality is not required, our surprising finding is that we do not need probabilistic posterior predictives: deterministic models are on par, in fact they consistently (although non-significantly) outperform their probabilistic counterparts. We also found that heteroscedasticity at training time, perhaps acting as a regularizer, improves predictions at longer horizons. At the methodological side, we design metrics and an experimental protocol which can be used to evaluate the various models, predicting their asymptotic performance when using them on the control problem. Using this framework, we improve the state-of-the-art sample complexity of MBRL on Acrobot by two to four folds, using an aggressive training schedule which is outside of the hyperparameter interval usually considered",
    "checked": true,
    "id": "2e274d55a5da52fa1c410f161982dab4a33466c7",
    "semantic_title": "model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose?",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=04ArenGOz3": {
    "title": "Set Prediction without Imposing Structure as Conditional Density Estimation",
    "volume": "poster",
    "abstract": "Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdefined cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper we propose an alternative to training via set losses, by viewing learning as conditional density estimation. Our learning framework fits deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reflecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce different plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions",
    "checked": true,
    "id": "9da628bd954e2250ec3a8aec9670eab096575c2d",
    "semantic_title": "set prediction without imposing structure as conditional density estimation",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=NX1He-aFO_F": {
    "title": "Learning Value Functions in Deep Policy Gradients using Residual Variance",
    "volume": "poster",
    "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights",
    "checked": true,
    "id": "6374643b741622ae24b8c1a735e8be0a905df84c",
    "semantic_title": "learning value functions in deep policy gradients using residual variance",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=MBOyiNnYthd": {
    "title": "IDF++: Analyzing and Improving Integer Discrete Flows for Lossless Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "23a108d165feaca935b99432f883ce57f01ef8b6",
    "semantic_title": "idf++: analyzing and improving integer discrete flows for lossless compression",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=agHLCOBM5jP": {
    "title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "331e7a89345464efd53dbefdddf7cb3c4f47187a",
    "semantic_title": "fully unsupervised diversity denoising with convolutional variational autoencoders",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=1FvkSpWosOl": {
    "title": "Is Attention Better Than Matrix Decomposition?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f829a355de02c08567927154d3045a6eb5425c91",
    "semantic_title": "is attention better than matrix decomposition?",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=NomEDgIEBwE": {
    "title": "Improving Transformation Invariance in Contrastive Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "43a160b76a38d2aa913bc78fea2873e2b1bebc7d",
    "semantic_title": "improving transformation invariance in contrastive representation learning",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=rq_Qr0c1Hyo": {
    "title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4fa32fec61c50f8339a05e097dacebe71cf9ab8e",
    "semantic_title": "on the origin of implicit regularization in stochastic gradient descent",
    "citation_count": 210,
    "authors": []
  },
  "https://openreview.net/forum?id=Qun8fv4qSby": {
    "title": "Transient Non-stationarity and Generalisation in Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "90974d9e0df8466a50338601e839fa0ea69c9872",
    "semantic_title": "transient non-stationarity and generalisation in deep reinforcement learning",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=oxnp2q-PGL4": {
    "title": "Lossless Compression of Structured Convolutional Models via Lifting",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0b99559725062338ec1a56cd9d0d8371932f9f14",
    "semantic_title": "lossless compression of structured convolutional models via lifting",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=-qh0M9XWxnv": {
    "title": "Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "064c71e4bf43e49e3a7cefc29b570c60cbfcfc4f",
    "semantic_title": "analyzing the expressive power of graph neural networks in a spectral perspective",
    "citation_count": 168,
    "authors": []
  },
  "https://openreview.net/forum?id=ZsZM-4iMQkH": {
    "title": "A unifying view on implicit bias in training linear neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6010bf788eef9825682ea09068ead4617bf46c26",
    "semantic_title": "a unifying view on implicit bias in training linear neural networks",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=TQt98Ya7UMP": {
    "title": "Balancing Constraints and Rewards with Meta-Gradient D4PG",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "eecc04e4751ef623ecd9f9e69e9601c9431152d2",
    "semantic_title": "balancing constraints and rewards with meta-gradient d4pg",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=lmTWnm3coJJ": {
    "title": "Robust Curriculum Learning: from clean label detection to noisy label self-correction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "66d9cb93003f61e56f825d4bc62023ceceacace4",
    "semantic_title": "robust curriculum learning: from clean label detection to noisy label self-correction",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=xnC8YwKUE3k": {
    "title": "Clairvoyance: A Pipeline Toolkit for Medical Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b628204e8718c49b26f7d3bb58692595c378aa31",
    "semantic_title": "clairvoyance: a pipeline toolkit for medical time series",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=w2Z2OwVNeK": {
    "title": "Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0203baf1af219fad578f055ee5c9481ca4123b2c",
    "semantic_title": "plan-based relaxed reward shaping for goal-directed tasks",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=-Hs_otp2RB": {
    "title": "Improving VAEs' Robustness to Adversarial Attack",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7ab3551d85efbc3c844dcf5714f2a5d3e8bafcf0",
    "semantic_title": "improving vaes' robustness to adversarial attack",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=4T489T4yav": {
    "title": "Differentiable Segmentation of Sequences",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1ab5db66bea0f628ca8b0c70e53b6971ecab695a",
    "semantic_title": "differentiable segmentation of sequences",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=kyaIeYj4zZ": {
    "title": "GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8b2cbb2f101b025c16e12d0d7628f65e5378e10d",
    "semantic_title": "grappa: grammar-augmented pre-training for table semantic parsing",
    "citation_count": 262,
    "authors": []
  },
  "https://openreview.net/forum?id=t0TaKv0Gx6Z": {
    "title": "Sliced Kernelized Stein Discrepancy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "90a36c56d3e763f2336dfe3f3ec932b3f41b4c18",
    "semantic_title": "sliced kernelized stein discrepancy",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=P0p33rgyoE": {
    "title": "Variational Intrinsic Control Revisited",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c092f4d29e3d9dcc92750d845a9b9978182643c1",
    "semantic_title": "variational intrinsic control revisited",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=pHXfe1cOmA": {
    "title": "HyperDynamics: Meta-Learning Object and Agent Dynamics with Hypernetworks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "116a9a94df0213b11aa0585af81109e853f9deb3",
    "semantic_title": "hyperdynamics: meta-learning object and agent dynamics with hypernetworks",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=AHOs7Sm5H7R": {
    "title": "Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "27558603527494688876cbd0cf5af53af5127f4a",
    "semantic_title": "towards resolving the implicit bias of gradient descent for matrix factorization: greedy low-rank learning",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=6isfR3JCbi": {
    "title": "Private Post-GAN Boosting",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "34ceeaef8f0569ca107b72622c14cbec15bf778f",
    "semantic_title": "private post-gan boosting",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=IX3Nnir2omJ": {
    "title": "Characterizing signal propagation to close the performance gap in unnormalized ResNets",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "feeae38fd404fdc17cad19d80461843059216fde",
    "semantic_title": "characterizing signal propagation to close the performance gap in unnormalized resnets",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=KmykpuSrjcq": {
    "title": "Prototypical Contrastive Learning of Unsupervised Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8bf6c69bae0956db13aa9129fedc69fdc1256dce",
    "semantic_title": "prototypical contrastive learning of unsupervised representations",
    "citation_count": 996,
    "authors": []
  },
  "https://openreview.net/forum?id=Ec85b0tUwbA": {
    "title": "Hyperbolic Neural Networks++",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "8c315c669a9e5f37ffcfb68b060e39d05de02d9f",
    "semantic_title": "hyperbolic neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-N7PBXqOUJZ": {
    "title": "Lipschitz Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bbc89fa342c06cf2216884238c531b1f6434e61d",
    "semantic_title": "lipschitz recurrent neural networks",
    "citation_count": 113,
    "authors": []
  },
  "https://openreview.net/forum?id=Rd138pWXMvG": {
    "title": "A statistical theory of cold posteriors in deep neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6f5a417658bfa3ef86e787e837e2c47b10a2a699",
    "semantic_title": "a statistical theory of cold posteriors in deep neural networks",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=ebS5NUfoMKL": {
    "title": "Boost then Convolve: Gradient Boosting Meets Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "07d38f062da2f13e3ff532d630aacc3e8dcaccca",
    "semantic_title": "boost then convolve: gradient boosting meets graph neural networks",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=TGFO0DbD_pk": {
    "title": "Genetic Soft Updates for Policy Evolution in Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f667c3a0c904db9d55c225c0411833abbc38a561",
    "semantic_title": "genetic soft updates for policy evolution in deep reinforcement learning",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=5l9zj5G7vDY": {
    "title": "Spatially Structured Recurrent Modules",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ffcea46f57ef1f5f65686e2424976e1af8689e78",
    "semantic_title": "s2rms: spatially structured recurrent modules",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=nzpLWnVAyah": {
    "title": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8b9d77d5e52a70af37451d3db3d32781b83ea054",
    "semantic_title": "on the stability of fine-tuning bert: misconceptions, explanations, and strong baselines",
    "citation_count": 367,
    "authors": []
  },
  "https://openreview.net/forum?id=rRFIni1CYmy": {
    "title": "End-to-End Egospheric Spatial Memory",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "26926181a8e0a4d2f2fd1478aee63201974de91c",
    "semantic_title": "end-to-end egospheric spatial memory",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=jM76BCb6F9m": {
    "title": "LEAF: A Learnable Frontend for Audio Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1192660d960d44ae7416a3b7562e87c5f338d691",
    "semantic_title": "leaf: a learnable frontend for audio classification",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=Qr0aRliE_Hb": {
    "title": "Simple Augmentation Goes a Long Way: ADRL for DNN Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7533636f292f37c90acd7241aae6719454ece1dd",
    "semantic_title": "simple augmentation goes a long way: adrl for dnn quantization",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=krz7T0xU9Z_": {
    "title": "The inductive bias of ReLU networks on orthogonally separable data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "71452416e60234dc6fa32109af2fef7a684e06f1",
    "semantic_title": "the inductive bias of relu networks on orthogonally separable data",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=7_G8JySGecm": {
    "title": "Monte-Carlo Planning and Learning with Language Action Value Estimates",
    "volume": "poster",
    "abstract": "Interactive Fiction (IF) games provide a useful testbed for language-based reinforcement learning agents, posing significant challenges of natural language understanding, commonsense reasoning, and non-myopic planning in the combinatorial search space. Agents based on standard planning algorithms struggle to play IF games due to the massive search space of language actions. Thus, language-grounded planning is a key ability of such agents, since inferring the consequence of language action based on semantic understanding can drastically improve search. In this paper, we introduce Monte-Carlo planning with Language Action Value Estimates (MC-LAVE) that combines a Monte-Carlo tree search with language-driven exploration. MC-LAVE invests more search effort into semantically promising language actions using locally optimistic language value estimates, yielding a significant reduction in the effective search space of language actions. We then present a reinforcement learning approach via MC-LAVE, which alternates between MC-LAVE planning and supervised learning of the self-generated language actions. In the experiments, we demonstrate that our method achieves new high scores in various IF games",
    "checked": true,
    "id": "09e28122994e7879ebf4b4d38b9c80c47fe6be3c",
    "semantic_title": "monte-carlo planning and learning with language action value estimates",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=v_1Soh8QUNc": {
    "title": "Learning Energy-Based Models by Diffusion Recovery Likelihood",
    "volume": "poster",
    "abstract": "While energy-based models (EBMs) exhibit a number of desirable properties, training and sampling on high-dimensional datasets remains challenging. Inspired by recent progress on diffusion probabilistic models, we present a diffusion recovery likelihood method to tractably learn and sample from a sequence of EBMs trained on increasingly noisy versions of a dataset. Each EBM is trained with recovery likelihood, which maximizes the conditional probability of the data at a certain noise level given their noisy versions at a higher noise level. Optimizing recovery likelihood is more tractable than marginal likelihood, as sampling from the conditional distributions is much easier than sampling from the marginal distributions. After training, synthesized images can be generated by the sampling process that initializes from Gaussian white noise distribution and progressively samples the conditional distributions at decreasingly lower noise levels. Our method generates high fidelity samples on various image datasets. On unconditional CIFAR-10 our method achieves FID 9.58 and inception score 8.30, superior to the majority of GANs. Moreover, we demonstrate that unlike previous work on EBMs, our long-run MCMC samples from the conditional distributions do not diverge and still represent realistic images, allowing us to accurately estimate the normalized density of data even for high-dimensional datasets. Our implementation is available at \\url{https://github.com/ruiqigao/recovery_likelihood}",
    "checked": true,
    "id": "90695f261c12265fb2694fe89cf390aad029a7dc",
    "semantic_title": "learning energy-based models by diffusion recovery likelihood",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=wQRlSUZ5V7B": {
    "title": "Capturing Label Characteristics in VAEs",
    "volume": "poster",
    "abstract": "We present a principled approach to incorporating labels in variational autoencoders (VAEs) that captures the rich characteristic information associated with those labels. While prior work has typically conflated these by learning latent variables that directly correspond to label values, we argue this is contrary to the intended effect of supervision in VAEs—capturing rich label characteristics with the latents. For example, we may want to capture the characteristics of a face that make it look young, rather than just the age of the person. To this end, we develop a novel VAE model, the characteristic capturing VAE (CCVAE), which \"reparameterizes\" supervision through auxiliary variables and a concomitant variational objective. Through judicious structuring of mappings between latent and auxiliary variables, we show that the CCVAE can effectively learn meaningful representations of the characteristics of interest across a variety of supervision schemes. In particular, we show that the CCVAE allows for more effective and more general interventions to be performed, such as smooth traversals within the characteristics for a given label, diverse conditional generation, and transferring characteristics across datapoints",
    "checked": true,
    "id": "5c6c4dac75e8579f2748c58f7ed970a1750068eb",
    "semantic_title": "capturing label characteristics in vaes",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=Fmg_fQYUejf": {
    "title": "Linear Mode Connectivity in Multitask and Continual Learning",
    "volume": "poster",
    "abstract": "Continual (sequential) training and multitask (simultaneous) training are often attempting to solve the same overall objective: to find a solution that performs well on all considered tasks. The main difference is in the training regimes, where continual learning can only have access to one task at a time, which for neural networks typically leads to catastrophic forgetting. That is, the solution found for a subsequent task does not perform well on the previous ones anymore. However, the relationship between the different minima that the two training regimes arrive at is not well understood. What sets them apart? Is there a local structure that could explain the difference in performance achieved by the two different schemes? Motivated by recent work showing that different minima of the same task are typically connected by very simple curves of low error, we investigate whether multitask and continual solutions are similarly connected. We empirically find that indeed such connectivity can be reliably achieved and, more interestingly, it can be done by a linear path, conditioned on having the same initialization for both. We thoroughly analyze this observation and discuss its significance for the continual learning process. Furthermore, we exploit this finding to propose an effective algorithm that constrains the sequentially learned minima to behave as the multitask solution. We show that our method outperforms several state of the art continual learning algorithms on various vision benchmarks",
    "checked": true,
    "id": "9e2084eda22a62be53f9d70ab6628fae7b400e1b",
    "semantic_title": "linear mode connectivity in multitask and continual learning",
    "citation_count": 150,
    "authors": []
  },
  "https://openreview.net/forum?id=hkMoYYEkBoI": {
    "title": "Computational Separation Between Convolutional and Fully-Connected Networks",
    "volume": "poster",
    "abstract": "Convolutional neural networks (CNN) exhibit unmatched performance in a multitude of computer vision tasks. However, the advantage of using convolutional networks over fully-connected networks is not understood from a theoretical perspective. In this work, we show how convolutional networks can leverage locality in the data, and thus achieve a computational advantage over fully-connected networks. Specifically, we show a class of problems that can be efficiently solved using convolutional networks trained with gradient-descent, but at the same time is hard to learn using a polynomial-size fully-connected network",
    "checked": true,
    "id": "1e24ddc732a5a0187783b4a221a44e26def6d434",
    "semantic_title": "computational separation between convolutional and fully-connected networks",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=xpFFI_NtgpW": {
    "title": "Rethinking Embedding Coupling in Pre-trained Language Models",
    "volume": "poster",
    "abstract": "We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art pre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to significantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By reallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on standard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that allocating additional capacity to the output embedding provides benefits to the model that persist through the fine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the number of parameters at the fine-tuning stage",
    "checked": true,
    "id": "bc87279d4b32a425377ff18ab63f7ecf95ff228c",
    "semantic_title": "rethinking embedding coupling in pre-trained language models",
    "citation_count": 144,
    "authors": []
  },
  "https://openreview.net/forum?id=vyY0jnWG-tK": {
    "title": "Physics-aware, probabilistic model order reduction with guaranteed stability",
    "volume": "poster",
    "abstract": "Given (small amounts of) time-series' data from a high-dimensional, fine-grained, multiscale dynamical system, we propose a generative framework for learning an effective, lower-dimensional, coarse-grained dynamical model that is predictive of the fine-grained system's long-term evolution but also of its behavior under different initial conditions. We target fine-grained models as they arise in physical applications (e.g. molecular dynamics, agent-based models), the dynamics of which are strongly non-stationary but their transition to equilibrium is governed by unknown slow processes which are largely inaccessible by brute-force simulations. Approaches based on domain knowledge heavily rely on physical insight in identifying temporally slow features and fail to enforce the long-term stability of the learned dynamics. On the other hand, purely statistical frameworks lack interpretability and rely on large amounts of expensive simulation data (long and multiple trajectories) as they cannot infuse domain knowledge. The generative framework proposed achieves the aforementioned desiderata by employing a flexible prior on the complex plane for the latent, slow processes, and an intermediate layer of physics-motivated latent variables that reduces reliance on data and imbues inductive bias. In contrast to existing schemes, it does not require the a priori definition of projection operators from the fine-grained description and addresses simultaneously the tasks of dimensionality reduction and model estimation. We demonstrate its efficacy and accuracy in multiscale physical systems of particle dynamics where probabilistic, long-term predictions of phenomena not contained in the training data are produced",
    "checked": true,
    "id": "7869928bc7c5809b05760367acc2eb5fd2a0e8c7",
    "semantic_title": "physics-aware, probabilistic model order reduction with guaranteed stability",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=-Lr-u0b42he": {
    "title": "Disentangling 3D Prototypical Networks for Few-Shot Concept Learning",
    "volume": "poster",
    "abstract": "We present neural architectures that disentangle RGB-D images into objects' shapes and styles and a map of the background scene, and explore their applications for few-shot 3D object detection and few-shot concept classification. Our networks incorporate architectural biases that reflect the image formation process, 3D geometry of the world scene, and shape-style interplay. They are trained end-to-end self-supervised by predicting views in static scenes, alongside a small number of 3D object boxes. Objects and scenes are represented in terms of 3D feature grids in the bottleneck of the network. We show the proposed 3D neural representations are compositional: they can generate novel 3D scene feature maps by mixing object shapes and styles, resizing and adding the resulting object 3D feature maps over background scene feature maps. We show object detectors trained on hallucinated 3D neural scenes generalize better to novel environments. We show classifiers for object categories, color, materials, and spatial relationships trained over the disentangled 3D feature sub-spaces generalize better with dramatically fewer exemplars over the current state-of-the-art, and enable a visual question answering system that uses them as its modules to generalize one-shot to novel objects in the scene",
    "checked": true,
    "id": "9e9be84ee3793f9bc951b7680368d1b8ec9113a2",
    "semantic_title": "disentangling 3d prototypical networks for few-shot concept learning",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=kE3vd639uRW": {
    "title": "LiftPool: Bidirectional ConvNet Pooling",
    "volume": "poster",
    "abstract": "Pooling is a critical operation in convolutional neural networks for increasing receptive fields and improving robustness to input variations. Most existing pooling operations downsample the feature maps, which is a lossy process. Moreover, they are not invertible: upsampling a downscaled feature map can not recover the lost information in the downsampling. By adopting the philosophy of the classical Lifting Scheme from signal processing, we propose LiftPool for bidirectional pooling layers, including LiftDownPool and LiftUpPool. LiftDownPool decomposes a feature map into various downsized sub-bands, each of which contains information with different frequencies. As the pooling function in LiftDownPool is perfectly invertible, by performing LiftDownPool backward, a corresponding up-pooling layer LiftUpPool is able to generate a refined upsampled feature map using the detail subbands, which is useful for image-to-image translation challenges. Experiments show the proposed methods achieve better results on image classification and semantic segmentation, using various backbones. Moreover, LiftDownPool offers better robustness to input corruptions and perturbations",
    "checked": true,
    "id": "b7815c4a743ba26e10904647fc5916aa45cbf701",
    "semantic_title": "liftpool: bidirectional convnet pooling",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=4TSiOTkKe5P": {
    "title": "Latent Convergent Cross Mapping",
    "volume": "poster",
    "abstract": "Discovering causal structures of temporal processes is a major tool of scientific inquiry because it helps us better understand and explain the mechanisms driving a phenomenon of interest, thereby facilitating analysis, reasoning, and synthesis for such systems. However, accurately inferring causal structures within a phenomenon based on observational data only is still an open problem. Indeed, this type of data usually consists in short time series with missing or noisy values for which causal inference is increasingly difficult. In this work, we propose a method to uncover causal relations in chaotic dynamical systems from short, noisy and sporadic time series (that is, incomplete observations at infrequent and irregular intervals) where the classical convergent cross mapping (CCM) fails. Our method works by learning a Neural ODE latent process modeling the state-space dynamics of the time series and by checking the existence of a continuous map between the resulting processes. We provide theoretical analysis and show empirically that Latent-CCM can reliably uncover the true causal pattern, unlike traditional methods",
    "checked": true,
    "id": "b0397ad6df0e67b1c8373edbe0a3b7d215b77b57",
    "semantic_title": "latent convergent cross mapping",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=yvQKLaqNE6M": {
    "title": "You Only Need Adversarial Supervision for Semantic Image Synthesis",
    "volume": "poster",
    "abstract": "Despite their recent successes, GAN models for semantic image synthesis still suffer from poor image quality when trained with only adversarial supervision. Historically, additionally employing the VGG-based perceptual loss has helped to overcome this issue, significantly improving the synthesis quality, but at the same time limiting the progress of GAN models for semantic image synthesis. In this work, we propose a novel, simplified GAN model, which needs only adversarial supervision to achieve high quality results. We re-design the discriminator as a semantic segmentation network, directly using the given semantic label maps as the ground truth for training. By providing stronger supervision to the discriminator as well as to the generator through spatially- and semantically-aware discriminator feedback, we are able to synthesize images of higher fidelity with better alignment to their input label maps, making the use of the perceptual loss superfluous. Moreover, we enable high-quality multi-modal image synthesis through global and local sampling of a 3D noise tensor injected into the generator, which allows complete or partial image change. We show that images synthesized by our model are more diverse and follow the color and texture distributions of real images more closely. We achieve an average improvement of $6$ FID and $5$ mIoU points over the state of the art across different datasets using only adversarial supervision",
    "checked": true,
    "id": "8f7d4d61292886c111b1fe11f524ecaa8101de27",
    "semantic_title": "you only need adversarial supervision for semantic image synthesis",
    "citation_count": 190,
    "authors": []
  },
  "https://openreview.net/forum?id=wXgk_iCiYGo": {
    "title": "A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima",
    "volume": "poster",
    "abstract": "Stochastic Gradient Descent (SGD) and its variants are mainstream methods for training deep networks in practice. SGD is known to find a flat minimum that often generalizes well. However, it is mathematically unclear how deep learning can select a flat minimum among so many minima. To answer the question quantitatively, we develop a density diffusion theory to reveal how minima selection quantitatively depends on the minima sharpness and the hyperparameters. To the best of our knowledge, we are the first to theoretically and empirically prove that, benefited from the Hessian-dependent covariance of stochastic gradient noise, SGD favors flat minima exponentially more than sharp minima, while Gradient Descent (GD) with injected white noise favors flat minima only polynomially more than sharp minima. We also reveal that either a small learning rate or large-batch training requires exponentially many iterations to escape from minima in terms of the ratio of the batch size and learning rate. Thus, large-batch training cannot search flat minima efficiently in a realistic computational time",
    "checked": true,
    "id": "0d6b4c7bb6246ad6ddca0fa44b19f1bda43e91fa",
    "semantic_title": "a diffusion theory for deep learning dynamics: stochastic gradient descent exponentially favors flat minima",
    "citation_count": 137,
    "authors": []
  },
  "https://openreview.net/forum?id=euDnVs0Ynts": {
    "title": "Robust Learning of Fixed-Structure Bayesian Networks in Nearly-Linear Time",
    "volume": "poster",
    "abstract": "We study the problem of learning Bayesian networks where an $\\epsilon$-fraction of the samples are adversarially corrupted. We focus on the fully-observable case where the underlying graph structure is known. In this work, we present the first nearly-linear time algorithm for this problem with a dimension-independent error guarantee. Previous robust algorithms with comparable error guarantees are slower by at least a factor of $(d/\\epsilon)$, where $d$ is the number of variables in the Bayesian network and $\\epsilon$ is the fraction of corrupted samples. Our algorithm and analysis are considerably simpler than those in previous work. We achieve this by establishing a direct connection between robust learning of Bayesian networks and robust mean estimation. As a subroutine in our algorithm, we develop a robust mean estimation algorithm whose runtime is nearly-linear in the number of nonzeros in the input samples, which may be of independent interest",
    "checked": true,
    "id": "7427493f3233580b28c949baace2461a92f37706",
    "semantic_title": "robust learning of fixed-structure bayesian networks in nearly-linear time",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UvBPbpvHRj-": {
    "title": "Activation-level uncertainty in deep neural networks",
    "volume": "poster",
    "abstract": "Current approaches for uncertainty estimation in deep learning often produce too confident results. Bayesian Neural Networks (BNNs) model uncertainty in the space of weights, which is usually high-dimensional and limits the quality of variational approximations. The more recent functional BNNs (fBNNs) address this only partially because, although the prior is specified in the space of functions, the posterior approximation is still defined in terms of stochastic weights. In this work we propose to move uncertainty from the weights (which are deterministic) to the activation function. Specifically, the activations are modelled with simple 1D Gaussian Processes (GP), for which a triangular kernel inspired by the ReLu non-linearity is explored. Our experiments show that activation-level stochasticity provides more reliable uncertainty estimates than BNN and fBNN, whereas it performs competitively in standard prediction tasks. We also study the connection with deep GPs, both theoretically and empirically. More precisely, we show that activation-level uncertainty requires fewer inducing points and is better suited for deep architectures",
    "checked": true,
    "id": "a4e645c2adebb01c25f2229bac69bbfb6e049bab",
    "semantic_title": "activation-level uncertainty in deep neural networks",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=2CjEVW-RGOJ": {
    "title": "SkipW: Resource Adaptable RNN with Strict Upper Computational Limit",
    "volume": "poster",
    "abstract": "We introduce Skip-Window, a method to allow recurrent neural networks (RNNs) to trade off accuracy for computational cost during the analysis of a sequence. Similarly to existing approaches, Skip-Window extends existing RNN cells by adding a mechanism to encourage the model to process fewer inputs. Unlike existing approaches, Skip-Window is able to respect a strict computational budget, making this model more suitable for limited hardware. We evaluate this approach on two datasets: a human activity recognition task and adding task. Our results show that Skip-Window is able to exceed the accuracy of existing approaches for a lower computational cost while strictly limiting said cost",
    "checked": true,
    "id": "3edffde19f424327315956402f3aa181b66c435b",
    "semantic_title": "skipw: resource adaptable rnn with strict upper computational limit",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bEoxzW_EXsa": {
    "title": "Wasserstein-2 Generative Networks",
    "volume": "poster",
    "abstract": "We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation",
    "checked": true,
    "id": "445ba554b6668ccd4a5c75f148430f7b99d5cefa",
    "semantic_title": "wasserstein-2 generative networks",
    "citation_count": 112,
    "authors": []
  },
  "https://openreview.net/forum?id=JkfYjnOEo6M": {
    "title": "Group Equivariant Stand-Alone Self-Attention For Vision",
    "volume": "poster",
    "abstract": "We provide a general self-attention formulation to impose group equivariance to arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate consistent improvements of GSA-Nets over non-equivariant self-attention networks",
    "checked": true,
    "id": "7a5d1a7646ce1884ad76a0e177f956ae4d77c722",
    "semantic_title": "group equivariant stand-alone self-attention for vision",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=3tFAs5E-Pe": {
    "title": "Continuous Wasserstein-2 Barycenter Estimation without Minimax Optimization",
    "volume": "poster",
    "abstract": "Wasserstein barycenters provide a geometric notion of the weighted average of probability measures based on optimal transport. In this paper, we present a scalable algorithm to compute Wasserstein-2 barycenters given sample access to the input measures, which are not restricted to being discrete. While past approaches rely on entropic or quadratic regularization, we employ input convex neural networks and cycle-consistency regularization to avoid introducing bias. As a result, our approach does not resort to minimax optimization. We provide theoretical analysis on error bounds as well as empirical evidence of the effectiveness of the proposed approach in low-dimensional qualitative scenarios and high-dimensional quantitative experiments",
    "checked": true,
    "id": "e2ac06efafb020e5815bd95620632b6340de7a3d",
    "semantic_title": "continuous wasserstein-2 barycenter estimation without minimax optimization",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=tGZu6DlbreV": {
    "title": "RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs",
    "volume": "poster",
    "abstract": "This paper studies learning logic rules for reasoning on knowledge graphs. Logic rules provide interpretable explanations when used for prediction as well as being able to generalize to other tasks, and hence are critical to learn. Existing methods either suffer from the problem of searching in a large search space (e.g., neural logic programming) or ineffective optimization due to sparse rewards (e.g., techniques based on reinforcement learning). To address these limitations, this paper proposes a probabilistic model called RNNLogic. RNNLogic treats logic rules as a latent variable, and simultaneously trains a rule generator as well as a reasoning predictor with logic rules. We develop an EM-based algorithm for optimization. In each iteration, the reasoning predictor is updated to explore some generated logic rules for reasoning. Then in the E-step, we select a set of high-quality rules from all generated rules with both the rule generator and reasoning predictor via posterior inference; and in the M-step, the rule generator is updated with the rules selected in the E-step. Experiments on four datasets prove the effectiveness of RNNLogic",
    "checked": true,
    "id": "32783724e06cca7328ddcc61e59b026cf624e102",
    "semantic_title": "rnnlogic: learning logic rules for reasoning on knowledge graphs",
    "citation_count": 185,
    "authors": []
  },
  "https://openreview.net/forum?id=N0M_4BkQ05i": {
    "title": "Selective Classification Can Magnify Disparities Across Groups",
    "volume": "poster",
    "abstract": "Selective classification, in which models can abstain on uncertain predictions, is a natural approach to improving accuracy in settings where errors are costly but abstentions are manageable. In this paper, we find that while selective classification can improve average accuracies, it can simultaneously magnify existing accuracy disparities between various groups within a population, especially in the presence of spurious correlations. We observe this behavior consistently across five vision and NLP datasets. Surprisingly, increasing abstentions can even decrease accuracies on some groups. To better understand this phenomenon, we study the margin distribution, which captures the model's confidences over all predictions. For symmetric margin distributions, we prove that whether selective classification monotonically improves or worsens accuracy is fully determined by the accuracy at full coverage (i.e., without any abstentions) and whether the distribution satisfies a property we call left-log-concavity. Our analysis also shows that selective classification tends to magnify full-coverage accuracy disparities. Motivated by our analysis, we train distributionally-robust models that achieve similar full-coverage accuracies across groups and show that selective classification uniformly improves each group on these models. Altogether, our results suggest that selective classification should be used with care and underscore the importance of training models to perform equally well across groups at full coverage",
    "checked": true,
    "id": "35610aa2be06c4af4ab944cda51de6fbad14b527",
    "semantic_title": "selective classification can magnify disparities across groups",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=Ogga20D2HO-": {
    "title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning",
    "volume": "poster",
    "abstract": "Federated learning (FL) allows edge devices to collectively learn a model without directly sharing data within each device, thus preserving privacy and eliminating the need to store data globally. While there are promising results under the assumption of independent and identically distributed (iid) local data, current state-of-the-art algorithms suffer a performance degradation as the heterogeneity of local data across clients increases. To resolve this issue, we propose a simple framework, \\emph{Mean Augmented Federated Learning (MAFL)}, where clients send and receive \\emph{averaged} local data, subject to the privacy requirements of target applications. Under our framework, we propose a new augmentation algorithm, named \\emph{FedMix}, which is inspired by a phenomenal yet simple data augmentation method, Mixup, but does not require local raw data to be directly shared among devices. Our method shows greatly improved performance in the standard benchmark datasets of FL, under highly non-iid federated settings, compared to conventional algorithms",
    "checked": true,
    "id": "e1e17e17abc51b99a95c9ae8d67a6909924f3986",
    "semantic_title": "fedmix: approximation of mixup under mean augmented federated learning",
    "citation_count": 175,
    "authors": []
  },
  "https://openreview.net/forum?id=jznizqvr15J": {
    "title": "In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness",
    "volume": "poster",
    "abstract": "Consider a prediction setting with few in-distribution labeled examples and many unlabeled examples both in- and out-of-distribution (OOD). The goal is to learn a model which performs well both in-distribution and OOD. In these settings, auxiliary information is often cheaply available for every input. How should we best leverage this auxiliary information for the prediction task? Empirically across three image and time-series datasets, and theoretically in a multi-task linear regression setting, we show that (i) using auxiliary information as input features improves in-distribution error but can hurt OOD error; but (ii) using auxiliary information as outputs of auxiliary pre-training tasks improves OOD error. To get the best of both worlds, we introduce In-N-Out, which first trains a model with auxiliary inputs and uses it to pseudolabel all the in-distribution inputs, then pre-trains a model on OOD auxiliary outputs and fine-tunes this model with the pseudolabels (self-training). We show both theoretically and empirically that In-N-Out outperforms auxiliary inputs or outputs alone on both in-distribution and OOD error",
    "checked": true,
    "id": "0f4374e62ae889dd2a35dc4c97b9a0510146fa87",
    "semantic_title": "in-n-out: pre-training and self-training using auxiliary information for out-of-distribution robustness",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=hSjxQ3B7GWq": {
    "title": "Sample-Efficient Automated Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "Despite significant progress in challenging problems across various domains, applying state-of-the-art deep reinforcement learning (RL) algorithms remains challenging due to their sensitivity to the choice of hyperparameters. This sensitivity can partly be attributed to the non-stationarity of the RL problem, potentially requiring different hyperparameter settings at various stages of the learning process. Additionally, in the RL setting, hyperparameter optimization (HPO) requires a large number of environment interactions, hindering the transfer of the successes in RL to real-world applications. In this work, we tackle the issues of sample-efficient and dynamic HPO in RL. We propose a population-based automated RL (AutoRL) framework to meta-optimize arbitrary off-policy RL algorithms. In this framework, we optimize the hyperparameters and also the neural architecture while simultaneously training the agent. By sharing the collected experience across the population, we substantially increase the sample efficiency of the meta-optimization. We demonstrate the capabilities of our sample-efficient AutoRL approach in a case study with the popular TD3 algorithm in the MuJoCo benchmark suite, where we reduce the number of environment interactions needed for meta-optimization by up to an order of magnitude compared to population-based training",
    "checked": true,
    "id": "283f975e221f56974f30a3b12c7fb015c0c77377",
    "semantic_title": "sample-efficient automated deep reinforcement learning",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=whE31dn74cL": {
    "title": "A Temporal Kernel Approach for Deep Learning with Continuous-time Information",
    "volume": "poster",
    "abstract": "Sequential deep learning models such as RNN, causal CNN and attention mechanism do not readily consume continuous-time information. Discretizing the temporal data, as we show, causes inconsistency even for simple continuous-time processes. Current approaches often handle time in a heuristic manner to be consistent with the existing deep learning architectures and implementations. In this paper, we provide a principled way to characterize continuous-time systems using deep learning tools. Notably, the proposed approach applies to all the major deep learning architectures and requires little modifications to the implementation. The critical insight is to represent the continuous-time system by composing neural networks with a temporal kernel, where we gain our intuition from the recent advancements in understanding deep learning with Gaussian process and neural tangent kernel. To represent the temporal kernel, we introduce the random feature approach and convert the kernel learning problem to spectral density estimation under reparameterization. We further prove the convergence and consistency results even when the temporal kernel is non-stationary, and the spectral density is misspecified. The simulations and real-data experiments demonstrate the empirical effectiveness of our temporal kernel approach in a broad range of settings",
    "checked": true,
    "id": "ad594fcaf59b705180466c14df1aefcdab009dbd",
    "semantic_title": "a temporal kernel approach for deep learning with continuous-time information",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=VErQxgyrbfn": {
    "title": "Convex Regularization behind Neural Reconstruction",
    "volume": "poster",
    "abstract": "Neural networks have shown tremendous potential for reconstructing high-resolution images in inverse problems. The non-convex and opaque nature of neural networks, however, hinders their utility in sensitive applications such as medical imaging. To cope with this challenge, this paper advocates a convex duality framework that makes a two-layer fully-convolutional ReLU denoising network amenable to convex optimization. The convex dual network not only offers the optimum training with convex solvers, but also facilitates interpreting training and prediction. In particular, it implies training neural networks with weight decay regularization induces path sparsity while the prediction is piecewise linear filtering. A range of experiments with MNIST and fastMRI datasets confirm the efficacy of the dual network optimization problem",
    "checked": true,
    "id": "a0886a4e611b211c4baf7b2fa5971efcb505c9de",
    "semantic_title": "convex regularization behind neural reconstruction",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=fGF8qAqpXXG": {
    "title": "Vector-output ReLU Neural Network Problems are Copositive Programs: Convex Analysis of Two Layer Networks and Polynomial-time Algorithms",
    "volume": "poster",
    "abstract": "We describe the convex semi-infinite dual of the two-layer vector-output ReLU neural network training problem. This semi-infinite dual admits a finite dimensional representation, but its support is over a convex set which is difficult to characterize. In particular, we demonstrate that the non-convex neural network training problem is equivalent to a finite-dimensional convex copositive program. Our work is the first to identify this strong connection between the global optima of neural networks and those of copositive programs. We thus demonstrate how neural networks implicitly attempt to solve copositive programs via semi-nonnegative matrix factorization, and draw key insights from this formulation. We describe the first algorithms for provably finding the global minimum of the vector output neural network training problem, which are polynomial in the number of samples for a fixed data rank, yet exponential in the dimension. However, in the case of convolutional architectures, the computational complexity is exponential in only the filter size and polynomial in all other parameters. We describe the circumstances in which we can find the global optimum of this neural network training problem exactly with soft-thresholded SVD, and provide a copositive relaxation which is guaranteed to be exact for certain classes of problems, and which corresponds with the solution of Stochastic Gradient Descent in practice",
    "checked": true,
    "id": "49d4a9e019b19b7a294d5590fcf43154663f2524",
    "semantic_title": "vector-output relu neural network problems are copositive programs: convex analysis of two layer networks and polynomial-time algorithms",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=5NsEIflpbSv": {
    "title": "Learning Better Structured Representations Using Low-rank Adaptive Label Smoothing",
    "volume": "poster",
    "abstract": "Training with soft targets instead of hard targets has been shown to improve performance and calibration of deep neural networks. Label smoothing is a popular way of computing soft targets, where one-hot encoding of a class is smoothed with a uniform distribution. Owing to its simplicity, label smoothing has found wide-spread use for training deep neural networks on a wide variety of tasks, ranging from image and text classification to machine translation and semantic parsing. Complementing recent empirical justification for label smoothing, we obtain PAC-Bayesian generalization bounds for label smoothing and show that the generalization error depends on the choice of the noise (smoothing) distribution. Then we propose low-rank adaptive label smoothing (LORAS): a simple yet novel method for training with learned soft targets that generalizes label smoothing and adapts to the latent structure of the label space in structured prediction tasks. Specifically, we evaluate our method on semantic parsing tasks and show that training with appropriately smoothed soft targets can significantly improve accuracy and model calibration, especially in low-resource settings. Used in conjunction with pre-trained sequence-to-sequence models, our method achieves state of the art performance on four semantic parsing data sets. LORAS can be used with any model, improves performance and implicit model calibration without increasing the number of model parameters, and can be scaled to problems with large label spaces containing tens of thousands of labels",
    "checked": true,
    "id": "394b10cfc658efbf69ec22f3f20dc5fde4affedf",
    "semantic_title": "learning better structured representations using low-rank adaptive label smoothing",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=eo6U4CAwVmg": {
    "title": "Training GANs with Stronger Augmentations via Contrastive Discriminator",
    "volume": "poster",
    "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD",
    "checked": true,
    "id": "f3f1c2bc4cfb4c86c418ea312cacefaf6db0065e",
    "semantic_title": "training gans with stronger augmentations via contrastive discriminator",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=y06VOYLcQXa": {
    "title": "Private Image Reconstruction from System Side Channels Using Generative Models",
    "volume": "poster",
    "abstract": "System side channels denote effects imposed on the underlying system and hardware when running a program, such as its accessed CPU cache lines. Side channel analysis (SCA) allows attackers to infer program secrets based on observed side channel signals. Given the ever-growing adoption of machine learning as a service (MLaaS), image analysis software on cloud platforms has been exploited by reconstructing private user images from system side channels. Nevertheless, to date, SCA is still highly challenging, requiring technical knowledge of victim software's internal operations. For existing SCA attacks, comprehending such internal operations requires heavyweight program analysis or manual efforts. This research proposes an attack framework to reconstruct private user images processed by media software via system side channels. The framework forms an effective workflow by incorporating convolutional networks, variational autoencoders, and generative adversarial networks. Our evaluation of two popular side channels shows that the reconstructed images consistently match user inputs, making privacy leakage attacks more practical. We also show surprising results that even one-bit data read/write pattern side channels, which are deemed minimally informative, can be used to reconstruct quality images using our framework",
    "checked": true,
    "id": "777d11340b9279e78a872f6373aac11b8834e6b4",
    "semantic_title": "private image reconstruction from system side channels using generative models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=ac288vnG_7U": {
    "title": "Learning to Make Decisions via Submodular Regularization",
    "volume": "poster",
    "abstract": "Many sequential decision making tasks can be viewed as combinatorial optimization problems over a large number of actions. When the cost of evaluating an action is high, even a greedy algorithm, which iteratively picks the best action given the history, is prohibitive to run. In this paper, we aim to learn a greedy heuristic for sequentially selecting actions as a surrogate for invoking the expensive oracle when evaluating an action. In particular, we focus on a class of combinatorial problems that can be solved via submodular maximization (either directly on the objective function or via submodular surrogates). We introduce a data-driven optimization framework based on the submodular-norm loss, a novel loss function that encourages the resulting objective to exhibit diminishing returns. Our framework outputs a surrogate objective that is efficient to train, approximately submodular, and can be made permutation-invariant. The latter two properties allow us to prove strong approximation guarantees for the learned greedy heuristic. Furthermore, we show that our model can be easily integrated with modern deep imitation learning pipelines for sequential prediction tasks. We demonstrate the performance of our algorithm on a variety of batched and sequential optimization tasks, including set cover, active learning, and Bayesian optimization for protein engineering",
    "checked": true,
    "id": "239eadb202a429d7bd64c69a492c5eccf83b2002",
    "semantic_title": "learning to make decisions via submodular regularization",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=3T9iFICe0Y9": {
    "title": "The Recurrent Neural Tangent Kernel",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "65d6365ca79fd78b966a794c05c7148317b9dee1",
    "semantic_title": "the recurrent neural tangent kernel",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=9uvhpyQwzM_": {
    "title": "Evaluation of Similarity-based Explanations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9782870723374d3dfb49a8042dd73800a71b7601",
    "semantic_title": "evaluation of similarity-based explanations",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=8xLkv08d70T": {
    "title": "Adaptive Procedural Task Generation for Hard-Exploration Problems",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e423c07b36936ddce137bce009b318f2c2741be5",
    "semantic_title": "adaptive procedural task generation for hard-exploration problems",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=dx11_7vm5_r": {
    "title": "Linear Last-iterate Convergence in Constrained Saddle-point Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "954931aa9a4b17e60d534f017ef600f7a87d31e2",
    "semantic_title": "linear last-iterate convergence in constrained saddle-point optimization",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=tiqI7w64JG2": {
    "title": "On Graph Neural Networks versus Graph-Augmented MLPs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "74d14abc4c9abd78415d6ae75d3313a5636ac8bc",
    "semantic_title": "on graph neural networks versus graph-augmented mlps",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=9SS69KwomAM": {
    "title": "Solving Compositional Reinforcement Learning Problems via Task Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "761427520e163f79869813122f4ca6eacbe27cbe",
    "semantic_title": "solving compositional reinforcement learning problems via task reduction",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=VJnrYcnRc6": {
    "title": "Conditional Generative Modeling via Learning the Latent Space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "09d7708c4460c74212d09ad7ea9293b210133207",
    "semantic_title": "conditional generative modeling via learning the latent space",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=kDnal_bbb-E": {
    "title": "DialoGraph: Incorporating Interpretable Strategy-Graph Networks into Negotiation Dialogues",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2785ec102c3d2888ae9d29c8d3fba2a666b5b3af",
    "semantic_title": "dialograph: incorporating interpretable strategy-graph networks into negotiation dialogues",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=eEn8KTtJOx": {
    "title": "WaNet - Imperceptible Warping-based Backdoor Attack",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "b5b0bbbe50f10d77138f089f96e157b5cb37d99f",
    "semantic_title": "l-red: efficient post-training detection of imperceptible backdoor attacks without access to the training set",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=B5VvQrI49Pa": {
    "title": "Nonseparable Symplectic Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1ab8bae9fa2df31c81aac4226e12094c616f8a22",
    "semantic_title": "nonseparable symplectic neural networks",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=lvRTC669EY_": {
    "title": "Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "78d4285fbcb06fcf3289e099c19abae7e7ed95a3",
    "semantic_title": "discovering diverse multi-agent strategic behavior via reward randomization",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=9ITXiTrAoT": {
    "title": "Multi-timescale Representation Learning in LSTM Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a833db6a27527fe5b85a3b161fc4317397e6b065",
    "semantic_title": "multi-timescale representation learning in lstm language models",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=HHiiQKWsOcV": {
    "title": "Explaining the Efficacy of Counterfactually Augmented Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "24fcdaf969089e6a411f7cebc9274bbc53c25e42",
    "semantic_title": "explaining the efficacy of counterfactually-augmented data",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=fAbkE6ant2": {
    "title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "32a81aef8063274b1d8cc770a7f6dcfd8efe5336",
    "semantic_title": "revisiting locally supervised learning: an alternative to end-to-end training",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=fgd7we_uZa6": {
    "title": "How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8f1b6eca0bdeb8cf0173d950b1157f290439cead",
    "semantic_title": "how much over-parameterization is sufficient to learn deep relu networks?",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=RqCC_00Bg7V": {
    "title": "Blending MPC & Value Function Approximation for Efficient Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d4ce35cc18f37c41fdb89013d9ec0c5fd59bf1b7",
    "semantic_title": "blending mpc & value function approximation for efficient reinforcement learning",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=T1XmO8ScKim": {
    "title": "Probabilistic Numeric Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1f5a66f818d286c48d2c5e84f8ae35be4cb0ac47",
    "semantic_title": "probabilistic numeric convolutional neural networks",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=TNkPBBYFkXg": {
    "title": "HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f9b3ed20d6da7dfbfbd8a58e2bde173e5e9c768c",
    "semantic_title": "heterofl: computation and communication efficient federated learning for heterogeneous clients",
    "citation_count": 573,
    "authors": []
  },
  "https://openreview.net/forum?id=Ov_sMNau-PF": {
    "title": "Semantic Re-tuning with Contrastive Tension",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cbd78779af4e83fe101ba3f7ba4d4786388d12d8",
    "semantic_title": "semantic re-tuning with contrastive tension",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=l-PrrQrK0QR": {
    "title": "Dataset Meta-Learning from Kernel Ridge-Regression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8212605d274d5e68bcedf990728f4f5c26f88168",
    "semantic_title": "dataset meta-learning from kernel ridge-regression",
    "citation_count": 261,
    "authors": []
  },
  "https://openreview.net/forum?id=1GTma8HwlYp": {
    "title": "AUXILIARY TASK UPDATE DECOMPOSITION: THE GOOD, THE BAD AND THE NEUTRAL",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "06da85d0191857d0e48a3565e84e017ff76aa8dc",
    "semantic_title": "auxiliary task update decomposition: the good, the bad and the neutral",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=Lc28QAB4ypz": {
    "title": "Fast And Slow Learning Of Recurrent Independent Mechanisms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b124114c0f9a4145892e328f19df7850333f207e",
    "semantic_title": "fast and slow learning of recurrent independent mechanisms",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=YHdeAO61l6T": {
    "title": "Auction Learning as a Two-Player Game",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1db7a459b6062beb3ce81b564e4df16935c717f2",
    "semantic_title": "auction learning as a two-player game",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=TR-Nj6nFx42": {
    "title": "A PAC-Bayesian Approach to Generalization Bounds for Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bb681868f002199ac29fef0102173e61fc56825d",
    "semantic_title": "a pac-bayesian approach to generalization bounds for graph neural networks",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=zx_uX-BO7CH": {
    "title": "Contextual Transformation Networks for Online Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "93e9cb14bccb3c7a11ca77fcd4d2e98e71195b58",
    "semantic_title": "contextual transformation networks for online continual learning",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=ahAUv8TI2Mz": {
    "title": "Adaptive and Generative Zero-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "931cbe1dac39e8dd1378b5752fd0ed128199e08f",
    "semantic_title": "adaptive and generative zero-shot learning",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=_i3ASPp12WS": {
    "title": "Online Adversarial Purification based on Self-supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2338d9517a47e6fee1ff1f3b8998bdaa622bbfcb",
    "semantic_title": "online adversarial purification based on self-supervised learning",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=N6JECD-PI5w": {
    "title": "FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "df157cb42b574c3f46b269504c18375bfa5bc5b1",
    "semantic_title": "fairfil: contrastive neural debiasing method for pretrained text encoders",
    "citation_count": 106,
    "authors": []
  },
  "https://openreview.net/forum?id=HIGSa_3kOx3": {
    "title": "Reset-Free Lifelong Learning with Skill-Space Planning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "25c310d4f7bc581a94455a1e96373d924ec736ae",
    "semantic_title": "reset-free lifelong learning with skill-space planning",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=u2YNJPcQlwq": {
    "title": "Efficient Empowerment Estimation for Unsupervised Stabilization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "532b7a2cd054465f66ec582b55e128bdb842c28f",
    "semantic_title": "efficient empowerment estimation for unsupervised stabilization",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=UFGEelJkLu5": {
    "title": "MixKD: Towards Efficient Distillation of Large-scale Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e8f6eb89897c5880a99748f23c3d3763346eea45",
    "semantic_title": "mixkd: towards efficient distillation of large-scale language models",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=h2EbJ4_wMVq": {
    "title": "CaPC Learning: Confidential and Private Collaborative Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "12bc3df669b64666c9fac71e918c9761f6ed5b71",
    "semantic_title": "capc learning: confidential and private collaborative learning",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=OmtmcPkkhT": {
    "title": "Multiplicative Filter Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4a1c3bf359b9b654a23f340f28c9b73c8050371c",
    "semantic_title": "multiplicative filter networks",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=V6BjBgku7Ro": {
    "title": "Planning from Pixels using Inverse Dynamics Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0bf1a78aeefa158a80b23fd5b57a5586a32eb7c1",
    "semantic_title": "planning from pixels using inverse dynamics models",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=yFJ67zTeI2": {
    "title": "Semi-supervised Keypoint Localization",
    "volume": "poster",
    "abstract": "Knowledge about the locations of keypoints of an object in an image can assist in fine-grained classification and identification tasks, particularly for the case of objects that exhibit large variations in poses that greatly influence their visual appearance, such as wild animals. However, supervised training of a keypoint detection network requires annotating a large image dataset for each animal species, which is a labor-intensive task. To reduce the need for labeled data, we propose to learn simultaneously keypoint heatmaps and pose invariant keypoint representations in a semi-supervised manner using a small set of labeled images along with a larger set of unlabeled images. Keypoint representations are learnt with a semantic keypoint consistency constraint that forces the keypoint detection network to learn similar features for the same keypoint across the dataset. Pose invariance is achieved by making keypoint representations for the image and its augmented copies closer together in feature space. Our semi-supervised approach significantly outperforms previous methods on several benchmarks for human and animal body landmark localization",
    "checked": true,
    "id": "ed8a5583f874c19fed7759ee51675e41c53e7438",
    "semantic_title": "semi-supervised keypoint localization",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=d8Q1mt2Ghw": {
    "title": "Emergent Road Rules In Multi-Agent Driving Environments",
    "volume": "poster",
    "abstract": "For autonomous vehicles to safely share the road with human drivers, autonomous vehicles must abide by specific \"road rules\" that human drivers have agreed to follow. \"Road rules\" include rules that drivers are required to follow by law – such as the requirement that vehicles stop at red lights – as well as more subtle social rules – such as the implicit designation of fast lanes on the highway. In this paper, we provide empirical evidence that suggests that – instead of hard-coding road rules into self-driving algorithms – a scalable alternative may be to design multi-agent environments in which road rules emerge as optimal solutions to the problem of maximizing traffic flow. We analyze what ingredients in driving environments cause the emergence of these road rules and find that two crucial factors are noisy perception and agents' spatial density. We provide qualitative and quantitative evidence of the emergence of seven social driving behaviors, ranging from obeying traffic signals to following lanes, all of which emerge from training agents to drive quickly to destinations without colliding. Our results add empirical support for the social road rules that countries worldwide have agreed on for safe, efficient driving",
    "checked": true,
    "id": "95978530dc134f879533efcda05f16f6d5d1c29a",
    "semantic_title": "emergent road rules in multi-agent driving environments",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=v5gjXpmR8J": {
    "title": "SSD: A Unified Framework for Self-Supervised Outlier Detection",
    "volume": "poster",
    "abstract": "We ask the following question: what training information is required to design an effective outlier/out-of-distribution (OOD) detector, i.e., detecting samples that lie far away from training distribution? Since unlabeled data is easily accessible for many applications, the most compelling approach is to develop detectors based on only unlabeled in-distribution data. However, we observe that most existing detectors based on unlabeled data perform poorly, often equivalent to a random prediction. In contrast, existing state-of-the-art OOD detectors achieve impressive performance but require access to fine-grained data labels for supervised training. We propose SSD, an outlier detector based on only unlabeled in-distribution data. We use self-supervised representation learning followed by a Mahalanobis distance based detection in the feature space. We demonstrate that SSD outperforms most existing detectors based on unlabeled data by a large margin. Additionally, SSD even achieves performance on par, and sometimes even better, with supervised training based detectors. Finally, we expand our detection framework with two key extensions. First, we formulate few-shot OOD detection, in which the detector has access to only one to five samples from each class of the targeted OOD dataset. Second, we extend our framework to incorporate training data labels, if available. We find that our novel detection framework based on SSD displays enhanced performance with these extensions, and achieves state-of-the-art performance. Our code is publicly available at https://github.com/inspire-group/SSD",
    "checked": true,
    "id": "0fe615dc0a422100e85cfb7e26c9306c481f6c75",
    "semantic_title": "ssd: a unified framework for self-supervised outlier detection",
    "citation_count": 355,
    "authors": []
  },
  "https://openreview.net/forum?id=VbLH04pRA3": {
    "title": "ECONOMIC HYPERPARAMETER OPTIMIZATION WITH BLENDED SEARCH STRATEGY",
    "volume": "poster",
    "abstract": "We study the problem of using low cost to search for hyperparameter configurations in a large search space with heterogeneous evaluation cost and model quality. We propose a blended search strategy to combine the strengths of global and local search, and prioritize them on the fly with the goal of minimizing the total cost spent in finding good configurations. Our approach demonstrates robust performance for tuning both tree-based models and deep neural networks on a large AutoML benchmark, as well as superior performance in model quality, time, and resource consumption for a production transformer-based NLP model fine-tuning task",
    "checked": false,
    "id": "8e4127163441ac9e5e7467eca0980a4e0eb693d7",
    "semantic_title": "optimization of convolutional neural network hyperparameters for automatic classification of adult mosquitoes",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=KYPz4YsCPj": {
    "title": "Inductive Representation Learning in Temporal Networks via Causal Anonymous Walks",
    "volume": "poster",
    "abstract": "Temporal networks serve as abstractions of many real-world dynamic systems. These networks typically evolve according to certain laws, such as the law of triadic closure, which is universal in social networks. Inductive representation learning of temporal networks should be able to capture such laws and further be applied to systems that follow the same laws but have not been unseen during the training stage. Previous works in this area depend on either network node identities or rich edge attributes and typically fail to extract these laws. Here, we propose {\\em Causal Anonymous Walks (CAWs)} to inductively represent a temporal network. CAWs are extracted by temporal random walks and work as automatic retrieval of temporal network motifs to represent network dynamics while avoiding the time-consuming selection and counting of those motifs. CAWs adopt a novel anonymization strategy that replaces node identities with the hitting counts of the nodes based on a set of sampled walks to keep the method inductive, and simultaneously establish the correlation between motifs. We further propose a neural-network model CAW-N to encode CAWs, and pair it with a CAW sampling strategy with constant memory and time cost to support online training and inference. CAW-N is evaluated to predict links over 6 real temporal networks and uniformly outperforms previous SOTA methods by averaged 15\\% AUC gain in the inductive setting. CAW-N also outperforms previous methods in 5 out of the 6 networks in the transductive setting",
    "checked": true,
    "id": "b67d88a20c82c635fdeb9e7e39b7f24a4991be6f",
    "semantic_title": "inductive representation learning in temporal networks via causal anonymous walks",
    "citation_count": 251,
    "authors": []
  },
  "https://openreview.net/forum?id=qZzy5urZw9": {
    "title": "Robust Overfitting may be mitigated by properly learned smoothening",
    "volume": "poster",
    "abstract": "A recent study (Rice et al., 2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting",
    "checked": true,
    "id": "e3f41f4b6b4e3ab740176f022bcad522ad4c38ec",
    "semantic_title": "robust overfitting may be mitigated by properly learned smoothening",
    "citation_count": 190,
    "authors": []
  },
  "https://openreview.net/forum?id=tH6_VWZjoq": {
    "title": "Local Search Algorithms for Rank-Constrained Convex Optimization",
    "volume": "poster",
    "abstract": "We propose greedy and local search algorithms for rank-constrained convex optimization, namely solving $\\underset{\\mathrm{rank}(A)\\leq r^*}{\\min}\\, R(A)$ given a convex function $R:\\mathbb{R}^{m\\times n}\\rightarrow \\mathbb{R}$ and a parameter $r^*$. These algorithms consist of repeating two steps: (a) adding a new rank-1 matrix to $A$ and (b) enforcing the rank constraint on $A$. We refine and improve the theoretical analysis of Shalev-Shwartz et al. (2011), and show that if the rank-restricted condition number of $R$ is $\\kappa$, a solution $A$ with rank $O(r^*\\cdot \\min\\{\\kappa \\log \\frac{R(\\mathbf{0})-R(A^*)}{\\epsilon}, \\kappa^2\\})$ and $R(A) \\leq R(A^*) + \\epsilon$ can be recovered, where $A^*$ is the optimal solution. This significantly generalizes associated results on sparse convex optimization, as well as rank-constrained convex optimization for smooth functions. We then introduce new practical variants of these algorithms that have superior runtime and recover better solutions in practice. We demonstrate the versatility of these methods on a wide range of applications involving matrix completion and robust principal component analysis",
    "checked": true,
    "id": "39dce697171b3f03575de54e970e48b50c96bbb9",
    "semantic_title": "local search algorithms for rank-constrained convex optimization",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=vcopnwZ7bC": {
    "title": "Learning Task Decomposition with Ordered Memory Policy Network",
    "volume": "poster",
    "abstract": "Many complex real-world tasks are composed of several levels of subtasks. Humans leverage these hierarchical structures to accelerate the learning process and achieve better generalization. In this work, we study the inductive bias and propose Ordered Memory Policy Network (OMPN) to discover subtask hierarchy by learning from demonstration. The discovered subtask hierarchy could be used to perform task decomposition, recovering the subtask boundaries in an unstructured demonstration. Experiments on Craft and Dial demonstrate that our model can achieve higher task decomposition performance under both unsupervised and weakly supervised settings, comparing with strong baselines. OMPN can also be directly applied to partially observable environments and still achieve higher task decomposition performance. Our visualization further confirms that the subtask hierarchy can emerge in our model 1",
    "checked": true,
    "id": "0b33c826480ab88116bd33a6c21d9665e466ccad",
    "semantic_title": "learning task decomposition with ordered memory policy network",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=tYxG_OMs9WE": {
    "title": "Property Controllable Variational Autoencoder via Invertible Mutual Dependence",
    "volume": "poster",
    "abstract": "Deep generative models have made important progress towards modeling complex, high dimensional data via learning latent representations. Their usefulness is nevertheless often limited by a lack of control over the generative process or a poor understanding of the latent representation. To overcome these issues, attention is now focused on discovering latent variables correlated to the data properties and ways to manipulate these properties. This paper presents the new Property controllable VAE (PCVAE), where a new Bayesian model is proposed to inductively bias the latent representation using explicit data properties via novel group-wise and property-wise disentanglement. Each data property corresponds seamlessly to a latent variable, by innovatively enforcing invertible mutual dependence between them. This allows us to move along the learned latent dimensions to control specific properties of the generated data with great precision. Quantitative and qualitative evaluations confirm that the PCVAE outperforms the existing models by up to 28% in capturing and 65% in manipulating the desired properties",
    "checked": true,
    "id": "1d8a6ab7db213d427098195a4ef08d45fd3dfc35",
    "semantic_title": "property controllable variational autoencoder via invertible mutual dependence",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=bhCDO_cEGCz": {
    "title": "Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning",
    "volume": "poster",
    "abstract": "We study the problem of dynamic visual reasoning on raw videos. This is a challenging problem; currently, state-of-the-art models often require dense supervision on physical object properties and events from simulation, which are impractical to obtain in real life. In this paper, we present the Dynamic Concept Learner (DCL), a unified framework that grounds physical objects and events from video and language. DCL first adopts a trajectory extractor to track each object over time and to represent it as a latent, object-centric feature vector. Building upon this object-centric representation, DCL learns to approximate the dynamic interaction among objects using graph networks. DCL further incorporates a semantic parser to parse question into semantic programs and, finally, a program executor to run the program to answer the question, levering the learned dynamics model. After training, DCL can detect and associate objects across the frames, ground visual properties and physical events, understand the causal relationship between events, make future and counterfactual predictions, and leverage these extracted presentations for answering queries. DCL achieves state-of-the-art performance on CLEVRER, a challenging causal video reasoning dataset, even without using ground-truth attributes and collision labels from simulations for training. We further test DCL on a newly proposed video-retrieval and event localization dataset derived from CLEVRER, showing its strong generalization capacity",
    "checked": true,
    "id": "7c399773001dc47de23038f4eb1f822bb6a11131",
    "semantic_title": "grounding physical concepts of objects and events through dynamic visual reasoning",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=c_E8kFWfhp0": {
    "title": "gradSim: Differentiable simulation for system identification and visuomotor control",
    "volume": "poster",
    "abstract": "In this paper, we tackle the problem of estimating object physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current best solutions to the problem require precise 3D labels which are labor intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. In this work we present gradSim, a framework that overcomes the dependence on 3D supervision by combining differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This unique combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Furthermore, our unified computation graph across dynamics and rendering engines enables the learning of challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to/better than techniques that require precise 3D labels",
    "checked": true,
    "id": "92c89db048fb825d2c81b086d7bd82ed230f685b",
    "semantic_title": "gradsim: differentiable simulation for system identification and visuomotor control",
    "citation_count": 107,
    "authors": []
  },
  "https://openreview.net/forum?id=RmcPm9m3tnk": {
    "title": "Generative Scene Graph Networks",
    "volume": "poster",
    "abstract": "Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks",
    "checked": true,
    "id": "b1a400b5c09a653da2269e459219ecc5705cba25",
    "semantic_title": "generative scene graph networks",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=_kxlwvhOodK": {
    "title": "Decentralized Attribution of Generative Models",
    "volume": "poster",
    "abstract": "Growing applications of generative models have led to new threats such as malicious personation and digital copyright infringement. One solution to these threats is model attribution, i.e., the identification of user-end models where the contents under question are generated. Existing studies showed empirical feasibility of attribution through a centralized classifier trained on all existing user-end models. However, this approach is not scalable in a reality where the number of models ever grows. Neither does it provide an attributability guarantee. To this end, this paper studies decentralized attribution, which relies on binary classifiers associated with each user-end model. Each binary classifier is parameterized by a user-specific key and distinguishes its associated model distribution from the authentic data distribution. We develop sufficient conditions of the keys that guarantee an attributability lower bound. Our method is validated on MNIST, CelebA, and FFHQ datasets. We also examine the trade-off between generation quality and robustness of attribution against adversarial post-processes",
    "checked": true,
    "id": "38805607d448927c8e7d5fb59e3ff58c0c1cc73e",
    "semantic_title": "decentralized attribution of generative models",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=71zCSP_HuBN": {
    "title": "Individually Fair Rankings",
    "volume": "poster",
    "abstract": "We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure. The crux of our method is an optimal transport-based regularizer that enforces individual fairness and an efficient algorithm for optimizing the regularizer. We show that our approach leads to certifiably individually fair LTR models and demonstrate the efficacy of our method on ranking tasks subject to demographic biases",
    "checked": true,
    "id": "eb083948b91bbbc0e4bb80dd39883edbd1af64f1",
    "semantic_title": "individually fair rankings",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=LkFG3lB13U5": {
    "title": "Adaptive Federated Optimization",
    "volume": "poster",
    "abstract": "Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Standard federated optimization methods such as Federated Averaging (FedAvg) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including Adagrad, Adam, and Yogi, and analyze their convergence in the presence of heterogeneous data for general non-convex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning",
    "checked": true,
    "id": "47c528344fedb6cb67a38e43d095b41c34715330",
    "semantic_title": "adaptive federated optimization",
    "citation_count": 1507,
    "authors": []
  },
  "https://openreview.net/forum?id=1AoMhc_9jER": {
    "title": "GANs Can Play Lottery Tickets Too",
    "volume": "poster",
    "abstract": "Deep generative adversarial networks (GANs) have gained growing popularity in numerous scenarios, while usually suffer from high parameter complexities for resource-constrained real-world applications. However, the compression of GANs has less been explored. A few works show that heuristically applying compression techniques normally leads to unsatisfactory results, due to the notorious training instability of GANs. In parallel, the lottery ticket hypothesis shows prevailing success on discriminative models, in locating sparse matching subnetworks capable of training in isolation to full model performance. In this work, we for the first time study the existence of such trainable matching subnetworks in deep GANs. For a range of GANs, we certainly find matching subnetworks at $67\\%$-$74\\%$ sparsity. We observe that with or without pruning discriminator has a minor effect on the existence and quality of matching subnetworks, while the initialization weights used in the discriminator plays a significant role. We then show the powerful transferability of these subnetworks to unseen tasks. Furthermore, extensive experimental results demonstrate that our found subnetworks substantially outperform previous state-of-the-art GAN compression approaches in both image generation (e.g. SNGAN) and image-to-image translation GANs (e.g. CycleGAN). Codes available at https://github.com/VITA-Group/GAN-LTH",
    "checked": true,
    "id": "53c3ac3d3f454bf355fb1a1931e40da2fe1d650e",
    "semantic_title": "gans can play lottery tickets too",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=DiQD7FWL233": {
    "title": "Improving Relational Regularized Autoencoders with Spherical Sliced Fused Gromov Wasserstein",
    "volume": "poster",
    "abstract": "Relational regularized autoencoder (RAE) is a framework to learn the distribution of data by minimizing a reconstruction loss together with a relational regularization on the prior of latent space. A recent attempt to reduce the inner discrepancy between the prior and aggregated posterior distributions is to incorporate sliced fused Gromov-Wasserstein (SFG) between these distributions. That approach has a weakness since it treats every slicing direction similarly, meanwhile several directions are not useful for the discriminative task. To improve the discrepancy and consequently the relational regularization, we propose a new relational discrepancy, named spherical sliced fused Gromov Wasserstein (SSFG), that can find an important area of projections characterized by a von Mises-Fisher distribution. Then, we introduce two variants of SSFG to improve its performance. The first variant, named mixture spherical sliced fused Gromov Wasserstein (MSSFG), replaces the vMF distribution by a mixture of von Mises-Fisher distributions to capture multiple important areas of directions that are far from each other. The second variant, named power spherical sliced fused Gromov Wasserstein (PSSFG), replaces the vMF distribution by a power spherical distribution to improve the sampling time of the vMF distribution in high dimension settings. We then apply the new discrepancies to the RAE framework to achieve its new variants. Finally, we conduct extensive experiments to show that the new autoencoders have favorable performance in learning latent manifold structure, image generation, and reconstruction",
    "checked": true,
    "id": "129fed5e7c229b20efb9a0a07a4f7b403e147eb8",
    "semantic_title": "improving relational regularized autoencoders with spherical sliced fused gromov wasserstein",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=hPWj1qduVw8": {
    "title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues",
    "volume": "poster",
    "abstract": "Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues",
    "checked": true,
    "id": "608605f307363796dd959507b4bb96a777fda8cf",
    "semantic_title": "learning reasoning paths over semantic graphs for video-grounded dialogues",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=Z4R1vxLbRLO": {
    "title": "Extreme Memorization via Scale of Initialization",
    "volume": "poster",
    "abstract": "We construct an experimental setup in which changing the scale of initialization strongly impacts the implicit regularization induced by SGD, interpolating from good generalization performance to completely memorizing the training set while making little progress on the test set. Moreover, we find that the extent and manner in which generalization ability is affected depends on the activation and loss function used, with sin activation being the most extreme. In the case of the homogeneous ReLU activation, we show that this behavior can be attributed to the loss function. Our empirical investigation reveals that increasing the scale of initialization correlates with misalignment of representations and gradients across examples in the same class. This insight allows us to device an alignment measure over gradients and representations which can capture this phenomenon. We demonstrate that our alignment measure correlates with generalization of deep models trained on image classification tasks",
    "checked": true,
    "id": "18c330bc388a53630074ec0473bfbdb460dabeb0",
    "semantic_title": "extreme memorization via scale of initialization",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=4RbdgBh9gE": {
    "title": "Teaching with Commentaries",
    "volume": "poster",
    "abstract": "Effective training of deep neural networks can be challenging, and there remain many open questions on how to best learn these models. Recently developed methods to improve neural network training examine teaching: providing learned information during the training process to improve downstream model performance. In this paper, we take steps towards extending the scope of teaching. We propose a flexible teaching framework using commentaries, learned meta-information helpful for training on a particular task. We present gradient-based methods to learn commentaries, leveraging recent work on implicit differentiation for scalability. We explore diverse applications of commentaries, from weighting training examples, to parameterising label-dependent data augmentation policies, to representing attention masks that highlight salient image regions. We find that commentaries can improve training speed and/or performance, and provide insights about the dataset and training process. We also observe that commentaries generalise: they can be reused when training new models to obtain performance benefits, suggesting a use-case where commentaries are stored with a dataset and leveraged in future for improved model training",
    "checked": true,
    "id": "1ac1d3eb086c42d72a0509a42f744f626e8b5711",
    "semantic_title": "teaching with commentaries",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=-ODN6SbiUU": {
    "title": "In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "The recent research in semi-supervised learning (SSL) is mostly dominated by consistency regularization based methods which achieve strong performance. However, they heavily rely on domain-specific data augmentations, which are not easy to generate for all data modalities. Pseudo-labeling (PL) is a general SSL approach that does not have this constraint but performs relatively poorly in its original formulation. We argue that PL underperforms due to the erroneous high confidence predictions from poorly calibrated models; these predictions generate many incorrect pseudo-labels, leading to noisy training. We propose an uncertainty-aware pseudo-label selection (UPS) framework which improves pseudo labeling accuracy by drastically reducing the amount of noise encountered in the training process. Furthermore, UPS generalizes the pseudo-labeling process, allowing for the creation of negative pseudo-labels; these negative pseudo-labels can be used for multi-label classification as well as negative learning to improve the single-label classification. We achieve strong performance when compared to recent SSL methods on the CIFAR-10 and CIFAR-100 datasets. Also, we demonstrate the versatility of our method on the video dataset UCF-101 and the multi-label dataset Pascal VOC",
    "checked": true,
    "id": "a21792db1c8d80c1d1f8525dab4959cc60b8e0ea",
    "semantic_title": "in defense of pseudo-labeling: an uncertainty-aware pseudo-label selection framework for semi-supervised learning",
    "citation_count": 539,
    "authors": []
  },
  "https://openreview.net/forum?id=MDsQkFP1Aw": {
    "title": "Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of On-Screen Sounds",
    "volume": "poster",
    "abstract": "Recent progress in deep learning has enabled many advances in sound separation and visual scene understanding. However, extracting sound sources which are apparent in natural videos remains an open problem. In this work, we present AudioScope, a novel audio-visual sound separation framework that can be trained without supervision to isolate on-screen sound sources from real in-the-wild videos. Prior audio-visual separation work assumed artificial limitations on the domain of sound classes (e.g., to speech or music), constrained the number of sources, and required strong sound separation or visual segmentation labels. AudioScope overcomes these limitations, operating on an open domain of sounds, with variable numbers of sources, and without labels or prior visual segmentation. The training procedure for AudioScope uses mixture invariant training (MixIT) to separate synthetic mixtures of mixtures (MoMs) into individual sources, where noisy labels for mixtures are provided by an unsupervised audio-visual coincidence model. Using the noisy labels, along with attention between video and audio features, AudioScope learns to identify audio-visual similarity and to suppress off-screen sounds. We demonstrate the effectiveness of our approach using a dataset of video clips extracted from open-domain YFCC100m video data. This dataset contains a wide diversity of sound classes recorded in unconstrained conditions, making the application of previous methods unsuitable. For evaluation and semi-supervised experiments, we collected human labels for presence of on-screen and off-screen sounds on a small subset of clips",
    "checked": true,
    "id": "04e4d11bda6da95070e4c406b0400ea00161c6d2",
    "semantic_title": "into the wild with audioscope: unsupervised audio-visual separation of on-screen sounds",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=bjkX6Kzb5H": {
    "title": "Cut out the annotator, keep the cutout: better segmentation with weak supervision",
    "volume": "poster",
    "abstract": "Constructing large, labeled training datasets for segmentation models is an expensive and labor-intensive process. This is a common challenge in machine learning, addressed by methods that require few or no labeled data points such as few-shot learning (FSL) and weakly-supervised learning (WS). Such techniques, however, have limitations when applied to image segmentation---FSL methods often produce noisy results and are strongly dependent on which few datapoints are labeled, while WS models struggle to fully exploit rich image information. We propose a framework that fuses FSL and WS for segmentation tasks, enabling users to train high-performing segmentation networks with very few hand-labeled training points. We use FSL models as weak sources in a WS framework, requiring a very small set of reference labeled images, and introduce a new WS model that focuses on key areas---areas with contention among noisy labels---of the image to fuse these weak sources. Empirically, we evaluate our proposed approach over seven well-motivated segmentation tasks. We show that our methods can achieve within 1.4 Dice points compared to fully supervised networks while only requiring five hand-labeled training points. Compared to existing FSL methods, our approach improves performance by a mean 3.6 Dice points over the next-best method",
    "checked": true,
    "id": "641f26f1f1656e346ff6dc6e5d07428148b34b44",
    "semantic_title": "cut out the annotator, keep the cutout: better segmentation with weak supervision",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=Ozk9MrX1hvA": {
    "title": "CoDA: Contrast-enhanced and Diversity-promoting Data Augmentation for Natural Language Understanding",
    "volume": "poster",
    "abstract": "Data augmentation has been demonstrated as an effective strategy for improving model generalization and data efficiency. However, due to the discrete nature of natural language, designing label-preserving transformations for text data tends to be more challenging. In this paper, we propose a novel data augmentation frame-work dubbed CoDA, which synthesizes diverse and informative augmented examples by integrating multiple transformations organically. Moreover, a contrastive regularization is introduced to capture the global relationship among all the data samples. A momentum encoder along with a memory bank is further leveraged to better estimate the contrastive loss. To verify the effectiveness of the proposed framework, we apply CoDA to Transformer-based models on a wide range of natural language understanding tasks. On the GLUE benchmark, CoDA gives rise to an average improvement of 2.2%while applied to the Roberta-large model. More importantly, it consistently exhibits stronger results relative to several competitive data augmentation and adversarial training baselines (including the low-resource settings). Extensive experiments show that the proposed contrastive objective can be flexibly combined with various data augmentation approaches to further boost their performance, highlighting the wide applicability of the CoDA framework",
    "checked": true,
    "id": "77f08ec1fc1a26d5e2c493be06a305d1480ad1c0",
    "semantic_title": "coda: contrast-enhanced and diversity-promoting data augmentation for natural language understanding",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=EQfpYwF3-b": {
    "title": "Deep Learning meets Projective Clustering",
    "volume": "poster",
    "abstract": "A common approach for compressing Natural Language Processing (NLP) networks is to encode the embedding layer as a matrix $A\\in\\mathbb{R}^{n\\times d}$, compute its rank-$j$ approximation $A_j$ via SVD (Singular Value Decomposition), and then factor $A_j$ into a pair of matrices that correspond to smaller fully-connected layers to replace the original embedding layer. Geometrically, the rows of $A$ represent points in $\\mathbb{R}^d$, and the rows of $A_j$ represent their projections onto the $j$-dimensional subspace that minimizes the sum of squared distances (``errors'') to the points. In practice, these rows of $A$ may be spread around $k>1$ subspaces, so factoring $A$ based on a single subspace may lead to large errors that turn into large drops in accuracy. Inspired by \\emph{projective clustering} from computational geometry, we suggest replacing this subspace by a set of $k$ subspaces, each of dimension $j$, that minimizes the sum of squared distances over every point (row in $A$) to its \\emph{closest} subspace. Based on this approach, we provide a novel architecture that replaces the original embedding layer by a set of $k$ small layers that operate in parallel and are then recombined with a single fully-connected layer. Extensive experimental results on the GLUE benchmark yield networks that are both more accurate and smaller compared to the standard matrix factorization (SVD). For example, we further compress DistilBERT by reducing the size of the embedding layer by $40\\%$ while incurring only a $0.5\\%$ average drop in accuracy over all nine GLUE tasks, compared to a $2.8\\%$ drop using the existing SVD approach. On RoBERTa we achieve $43\\%$ compression of the embedding layer with less than a $0.8\\%$ average drop in accuracy as compared to a $3\\%$ drop previously",
    "checked": true,
    "id": "a80029ac0fd89bd0296d3d1f440ad8247f027b72",
    "semantic_title": "deep learning meets projective clustering",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=b7g3_ZMHnT0": {
    "title": "Learning to Deceive Knowledge Graph Augmented Models via Targeted Perturbation",
    "volume": "poster",
    "abstract": "Knowledge graphs (KGs) have helped neural models improve performance on various knowledge-intensive tasks, like question answering and item recommendation. By using attention over the KG, such KG-augmented models can also \"explain\" which KG information was most relevant for making a given prediction. In this paper, we question whether these models are really behaving as we expect. We show that, through a reinforcement learning policy (or even simple heuristics), one can produce deceptively perturbed KGs, which maintain the downstream performance of the original KG while significantly deviating from the original KG's semantics and structure. Our findings raise doubts about KG-augmented models' ability to reason about KG information and give sensible explanations",
    "checked": true,
    "id": "1076e085c973ff4eacf7a840cc09191d7e7ed3ed",
    "semantic_title": "learning to deceive knowledge graph augmented models via targeted perturbation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m4UCf24r0Y": {
    "title": "Knowledge Distillation as Semiparametric Inference",
    "volume": "poster",
    "abstract": "A popular approach to model compression is to train an inexpensive student model to mimic the class probabilities of a highly accurate but cumbersome teacher model. Surprisingly, this two-step knowledge distillation process often leads to higher accuracy than training the student directly on labeled data. To explain and enhance this phenomenon, we cast knowledge distillation as a semiparametric inference problem with the optimal student model as the target, the unknown Bayes class probabilities as nuisance, and the teacher probabilities as a plug-in nuisance estimate. By adapting modern semiparametric tools, we derive new guarantees for the prediction error of standard distillation and develop two enhancements—cross-fitting and loss correction—to mitigate the impact of teacher overfitting and underfitting on student performance. We validate our findings empirically on both tabular and image data and observe consistent improvements from our knowledge distillation enhancements",
    "checked": true,
    "id": "6dde2751467e286e72cf6a49ae90e8bbdcd3e91f",
    "semantic_title": "knowledge distillation as semiparametric inference",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=Ti87Pv5Oc8": {
    "title": "Meta-Learning with Neural Tangent Kernels",
    "volume": "poster",
    "abstract": "Model Agnostic Meta-Learning (MAML) has emerged as a standard framework for meta-learning, where a meta-model is learned with the ability of fast adapting to new tasks. However, as a double-looped optimization problem, MAML needs to differentiate through the whole inner-loop optimization path for every outer-loop training step, which may lead to both computational inefficiency and sub-optimal solutions. In this paper, we generalize MAML to allow meta-learning to be defined in function spaces, and propose the first meta-learning paradigm in the Reproducing Kernel Hilbert Space (RKHS) induced by the meta-model's Neural Tangent Kernel (NTK). Within this paradigm, we introduce two meta-learning algorithms in the RKHS, which no longer need a sub-optimal iterative inner-loop adaptation as in the MAML framework. We achieve this goal by 1) replacing the adaptation with a fast-adaptive regularizer in the RKHS; and 2) solving the adaptation analytically based on the NTK theory. Extensive experimental studies demonstrate advantages of our paradigm in both efficiency and quality of solutions compared to related meta-learning algorithms. Another interesting feature of our proposed methods is that they are demonstrated to be more robust to adversarial attacks and out-of-distribution adaptation than popular baselines, as demonstrated in our experiments",
    "checked": true,
    "id": "86e1e0fbb4388d7216973c6385428c11a505e7b5",
    "semantic_title": "meta-learning with neural tangent kernels",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=9r30XCjf5Dt": {
    "title": "Vulnerability-Aware Poisoning Mechanism for Online RL with Unknown Dynamics",
    "volume": "poster",
    "abstract": "Poisoning attacks on Reinforcement Learning (RL) systems could take advantage of RL algorithm's vulnerabilities and cause failure of the learning. However, prior works on poisoning RL usually either unrealistically assume the attacker knows the underlying Markov Decision Process (MDP), or directly apply the poisoning methods in supervised learning to RL. In this work, we build a generic poisoning framework for online RL via a comprehensive investigation of heterogeneous poisoning models in RL. Without any prior knowledge of the MDP, we propose a strategic poisoning algorithm called Vulnerability-Aware Adversarial Critic Poison (VA2C-P), which works for on-policy deep RL agents, closing the gap that no poisoning method exists for policy-based RL agents. VA2C-P uses a novel metric, stability radius in RL, that measures the vulnerability of RL algorithms. Experiments on multiple deep RL agents and multiple environments show that our poisoning algorithm successfully prevents agents from learning a good policy or teaches the agents to converge to a target policy, with a limited attacking budget",
    "checked": true,
    "id": "9f00a4f3b0628f861e1e7b929db0befd564d7bae",
    "semantic_title": "vulnerability-aware poisoning mechanism for online rl with unknown dynamics",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=ZTFeSBIX9C": {
    "title": "Understanding and Improving Lexical Choice in Non-Autoregressive Translation",
    "volume": "poster",
    "abstract": "Knowledge distillation (KD) is essential for training non-autoregressive translation (NAT) models by reducing the complexity of the raw data with an autoregressive teacher model. In this study, we empirically show that as a side effect of this training, the lexical choice errors on low-frequency words are propagated to the NAT model from the teacher model. To alleviate this problem, we propose to expose the raw data to NAT models to restore the useful information of low-frequency words, which are missed in the distilled data. To this end, we introduce an extra Kullback-Leibler divergence term derived by comparing the lexical choice of NAT model and that embedded in the raw data. Experimental results across language pairs and model architectures demonstrate the effectiveness and universality of the proposed approach. Extensive analyses confirm our claim that our approach improves performance by reducing the lexical choice errors on low-frequency words. Encouragingly, our approach pushes the SOTA NAT performance on the WMT14 English-German and WMT16 Romanian-English datasets up to 27.8 and 33.8 BLEU points, respectively",
    "checked": true,
    "id": "24a1767f6731abaeb21f8fa745b7e02fd4bbf39f",
    "semantic_title": "understanding and improving lexical choice in non-autoregressive translation",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=H6ATjJ0TKdf": {
    "title": "Layer-adaptive Sparsity for the Magnitude-based Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9227d5897abbf297a34d447e94a802a714b8eab2",
    "semantic_title": "layer-adaptive sparsity for the magnitude-based pruning",
    "citation_count": 211,
    "authors": []
  },
  "https://openreview.net/forum?id=n1HD8M6WGn": {
    "title": "Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9ee27d4df0a0e7032c520cb5f26567fb06769ad1",
    "semantic_title": "understanding and improving encoder layer fusion in sequence-to-sequence learning",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=-M0QkvBGTTq": {
    "title": "SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "29e34b721a72357ab83717643c1566f977e6a761",
    "semantic_title": "saliencymix: a saliency guided data augmentation strategy for better regularization",
    "citation_count": 232,
    "authors": []
  },
  "https://openreview.net/forum?id=_zx8Oka09eF": {
    "title": "Are wider nets better given the same number of parameters?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9e104d440540d2ffc9caaa0952a9e5f7f9344ba9",
    "semantic_title": "are wider nets better given the same number of parameters?",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=jP1vTH3inC": {
    "title": "Discovering Non-monotonic Autoregressive Orderings with Variational Inference",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "063eee315e864f0842d3074629dccc4bb36d19e7",
    "semantic_title": "discovering non-monotonic autoregressive orderings with variational inference",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=IgIk8RRT-Z": {
    "title": "CompOFA – Compound Once-For-All Networks for Faster Multi-Platform Deployment",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "17820d356a6441183c954d1b0d5ca5ceb1ff0045",
    "semantic_title": "compofa: compound once-for-all networks for faster multi-platform deployment",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=mCtadqIxOJ": {
    "title": "Representing Partial Programs with Blended Abstract Semantics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4116e65ef8a05c82c0fd739d98ca72e50802cf83",
    "semantic_title": "representing partial programs with blended abstract semantics",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=TYXs_y84xRj": {
    "title": "PolarNet: Learning to Optimize Polar Keypoints for Keypoint Based Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "58ace7a2cd38838e8b6c21a1789a18d7e1c5101f",
    "semantic_title": "polarnet: learning to optimize polar keypoints for keypoint based object detection",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=Cnon5ezMHtu": {
    "title": "Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8d84c38f5fce1bd1b4ae1d55400c8fb7fa5d19c8",
    "semantic_title": "neural architecture search on imagenet in four gpu hours: a theoretically inspired perspective",
    "citation_count": 244,
    "authors": []
  },
  "https://openreview.net/forum?id=8qDwejCuCN": {
    "title": "Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e19ca3c11fd45dfc9bc6e43ddf9b03b6c798e66d",
    "semantic_title": "unsupervised representation learning for time series with temporal neighborhood coding",
    "citation_count": 306,
    "authors": []
  },
  "https://openreview.net/forum?id=KJNcAkY8tY4": {
    "title": "Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d21806115a79c960298cfca45a49b24682cac71a",
    "semantic_title": "do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth",
    "citation_count": 286,
    "authors": []
  },
  "https://openreview.net/forum?id=cu7IUiOhujH": {
    "title": "Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "96c22a88ec3b9d3799daa41098555ab665c24ea8",
    "semantic_title": "supervised contrastive learning for pre-trained language model fine-tuning",
    "citation_count": 512,
    "authors": []
  },
  "https://openreview.net/forum?id=tlV90jvZbw": {
    "title": "Early Stopping in Deep Networks: Double Descent and How to Eliminate it",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "002614a13b02f331be5e739a5a4a31e19ca28a60",
    "semantic_title": "early stopping in deep networks: double descent and how to eliminate it",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=F8whUO8HNbP": {
    "title": "Contrastive Syn-to-Real Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "732f76b0a2aa5d255d60cb6e29257d9fb292a6f4",
    "semantic_title": "contrastive syn-to-real generalization",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=kWSeGEeHvF8": {
    "title": "Benchmarks for Deep Off-Policy Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d86dfdbb8eab91cf23b81c541b4f741f88b7d756",
    "semantic_title": "benchmarks for deep off-policy evaluation",
    "citation_count": 106,
    "authors": []
  },
  "https://openreview.net/forum?id=3k20LAiHYL2": {
    "title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "abaadb4c6affc4d874c4f59bfac60686e851cb5e",
    "semantic_title": "pre-training text-to-text transformers for concept-centric common sense",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=8E1-f3VhX1o": {
    "title": "Combining Label Propagation and Simple Models out-performs Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f1e5e65941617604923225cc4bf464e370fcae67",
    "semantic_title": "combining label propagation and simple models out-performs graph neural networks",
    "citation_count": 285,
    "authors": []
  },
  "https://openreview.net/forum?id=_X_4Akcd8Re": {
    "title": "Learning Long-term Visual Dynamics with Region Proposal Interaction Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4053a225b189852df0ef5d24bc5c400987778ac4",
    "semantic_title": "learning long-term visual dynamics with region proposal interaction networks",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=a3wKPZpGtCF": {
    "title": "Chaos of Learning Beyond Zero-sum and Coordination via Game Decompositions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "181bc029f499b4876f9903a9eac57df5f6be109c",
    "semantic_title": "chaos of learning beyond zero-sum and coordination via game decompositions",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=dgd4EJqsbW5": {
    "title": "Control-Aware Representations for Model-based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b50da2e0bf200bb481725d92e5e3c80f8273dacc",
    "semantic_title": "control-aware representations for model-based reinforcement learning",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=sRA5rLNpmQc": {
    "title": "Provably robust classification of adversarial examples with detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "73dd7ff60ba551fcf1b7b13fdf65ae29d6e2f37c",
    "semantic_title": "provably robust classification of adversarial examples with detection",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=_TM6rT7tXke": {
    "title": "Return-Based Contrastive Representation Learning for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f37a60d5c352b322e0a1a1e852e980a1e9e903b3",
    "semantic_title": "return-based contrastive representation learning for reinforcement learning",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=ijJZbomCJIm": {
    "title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "eba3ad34d897c6c6a9c875585f05f93d0db8ec15",
    "semantic_title": "adversarially-trained deep nets transfer better: illustration on image classification",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=v9hAX77--cZ": {
    "title": "Learning Structural Edits via Incremental Tree Transformations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1756376bf7cf0d0a7bec881d663b57907a361ecf",
    "semantic_title": "learning structural edits via incremental tree transformations",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=hWr3e3r-oH5": {
    "title": "Cross-Attentional Audio-Visual Fusion for Weakly-Supervised Action Localization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "da016a4ab022122635419d782b9513653bcd152a",
    "semantic_title": "cross-attentional audio-visual fusion for weakly-supervised action localization",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=BUlyHkzjgmA": {
    "title": "Improved Estimation of Concentration Under ℓ p -Norm Distance Metrics Using Half Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ba8d84deb3633076a135e6b885da609eb7b1c7e0",
    "semantic_title": "improved estimation of concentration under $\\ell_p$-norm distance metrics using half spaces",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=MyHwDabUHZm": {
    "title": "Beyond Categorical Label Representations for Image Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "18d02ad0cf4bc9184a38aaacb72642b7a2d2bf18",
    "semantic_title": "beyond categorical label representations for image classification",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=JCRblSgs34Z": {
    "title": "Fantastic Four: Differentiable and Efficient Bounds on Singular Values of Convolution Layers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "efb0782e46159be5184040811ab5a93fdf78a746",
    "semantic_title": "fantastic four: differentiable and efficient bounds on singular values of convolution layers",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=iOnhIy-a-0n": {
    "title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3b603fee9faf45a5bbf87a414e9fb8d0cc930a88",
    "semantic_title": "accelerating convergence of replica exchange stochastic gradient mcmc via variance reduction",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=Pzj6fzU6wkj": {
    "title": "IsarStep: a Benchmark for High-level Mathematical Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "593499b654360101682edec1dd711fa7c09f6971",
    "semantic_title": "isarstep: a benchmark for high-level mathematical reasoning",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=VVdmjgu7pKM": {
    "title": "Factorizing Declarative and Procedural Knowledge in Structured, Dynamical Environments",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "336f5ebe4a0e03697e87df9c5cb519fa4db39aa5",
    "semantic_title": "factorizing declarative and procedural knowledge in structured, dynamical environments",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=hx1IXFHAw7R": {
    "title": "Provable Rich Observation Reinforcement Learning with Combinatorial Latent States",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c2ebce87f856fada7ac735fdc9fe7549bfd5b583",
    "semantic_title": "provable rich observation reinforcement learning with combinatorial latent states",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=hJmtwocEqzc": {
    "title": "LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "73263d9c1ce3e0a105aa9d79d6a87d111d0465e3",
    "semantic_title": "lowkey: leveraging adversarial attacks to protect social media users from facial recognition",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=7t1FcJUWhi3": {
    "title": "Neural Networks for Learning Counterfactual G-Invariances from Single Environments",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "98602b3c90f0b6659faf974eb1c1144e42e6d042",
    "semantic_title": "neural networks for learning counterfactual g-invariances from single environments",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=CYO5T-YjWZV": {
    "title": "Simple Spectral Graph Convolution",
    "volume": "poster",
    "abstract": "Graph Convolutional Networks (GCNs) are leading methods for learning graph representations. However, without specially designed architectures, the performance of GCNs degrades quickly with increased depth. As the aggregated neighborhood size and neural network depth are two completely orthogonal aspects of graph representation, several methods focus on summarizing the neighborhood by aggregating K-hop neighborhoods of nodes while using shallow neural networks. However, these methods still encounter oversmoothing, and suffer from high computation and storage costs. In this paper, we use a modified Markov Diffusion Kernel to derive a variant of GCN called Simple Spectral Graph Convolution (SSGC). Our spectral analysis shows that our simple spectral graph convolution used in SSGC is a trade-off of low- and high-pass filter bands which capture the global and local contexts of each node. We provide two theoretical claims which demonstrate that we can aggregate over a sequence of increasingly larger neighborhoods compared to competitors while limiting severe oversmoothing. Our experimental evaluations show that SSGC with a linear learner is competitive in text and node classification tasks. Moreover, SSGC is comparable to other state-of-the-art methods for node clustering and community prediction tasks",
    "checked": true,
    "id": "69f1bb893516207b2a02bf3673edc46fb5cf1f94",
    "semantic_title": "simple spectral graph convolution",
    "citation_count": 303,
    "authors": []
  },
  "https://openreview.net/forum?id=LhY8QdUGSuw": {
    "title": "Anatomy of Catastrophic Forgetting: Hidden Representations and Task Semantics",
    "volume": "poster",
    "abstract": "Catastrophic forgetting is a recurring challenge to developing versatile deep learning models. Despite its ubiquity, there is limited understanding of its connections to neural network (hidden) representations and task semantics. In this paper, we address this important knowledge gap. Through quantitative analysis of neural representations, we find that deeper layers are disproportionately responsible for forgetting, with sequential training resulting in an erasure of earlier task representational subspaces. Methods to mitigate forgetting stabilize these deeper layers, but show diversity on precise effects, with some increasing feature reuse while others store task representations orthogonally, preventing interference. These insights also enable the development of an analytic argument and empirical picture relating forgetting to task semantic similarity, where we find that maximal forgetting occurs for task sequences with intermediate similarity",
    "checked": true,
    "id": "1dddfe2c8c3cce6ae7c18f7ecb89fbe664057269",
    "semantic_title": "anatomy of catastrophic forgetting: hidden representations and task semantics",
    "citation_count": 184,
    "authors": []
  },
  "https://openreview.net/forum?id=o81ZyBCojoA": {
    "title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning",
    "volume": "poster",
    "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\\textit{meta-initialization}$ of model parameters (that we call $\\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\\textit{how}$ robust regularization can $\\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning",
    "checked": true,
    "id": "118a605ad954c8f8e1ad65941429d0fd2c14c918",
    "semantic_title": "on fast adversarial robustness adaptation in model-agnostic meta-learning",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=42kiJ7n_8xO": {
    "title": "The geometry of integration in text classification RNNs",
    "volume": "poster",
    "abstract": "Despite the widespread application of recurrent neural networks (RNNs), a unified understanding of how RNNs solve particular tasks remains elusive. In particular, it is unclear what dynamical patterns arise in trained RNNs, and how those pat-terns depend on the training dataset or task. This work addresses these questions in the context of text classification, building on earlier work studying the dynamics of binary sentiment-classification networks (Maheswaranathan et al., 2019). We study text-classification tasks beyond the binary case, exploring the dynamics ofRNNs trained on both natural and synthetic datasets. These dynamics, which we find to be both interpretable and low-dimensional, share a common mechanism across architectures and datasets: specifically, these text-classification networks use low-dimensional attractor manifolds to accumulate evidence for each class as they process the text. The dimensionality and geometry of the attractor manifold are determined by the structure of the training dataset, with the dimensionality reflecting the number of scalar quantities the network remembers in order to classify.In categorical classification, for example, we show that this dimensionality is one less than the number of classes. Correlations in the dataset, such as those induced by ordering, can further reduce the dimensionality of the attractor manifold; we show how to predict this reduction using simple word-count statistics computed on the training dataset. To the degree that integration of evidence towards a decision is a common computational primitive, this work continues to lay the foundation for using dynamical systems techniques to study the inner workings of RNNs",
    "checked": true,
    "id": "b895ec37d0bc703b155c617ebb9f3e61d826d237",
    "semantic_title": "the geometry of integration in text classification rnns",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=2AL06y9cDE-": {
    "title": "Towards Robust Neural Networks via Close-loop Control",
    "volume": "poster",
    "abstract": "Despite their success in massive engineering applications, deep neural networks are vulnerable to various perturbations due to their black-box nature. Recent study has shown that a deep neural network can misclassify the data even if the input data is perturbed by an imperceptible amount. In this paper, we address the robustness issue of neural networks by a novel close-loop control method from the perspective of dynamic systems. Instead of modifying the parameters in a fixed neural network architecture, a close-loop control process is added to generate control signals adaptively for the perturbed or corrupted data. We connect the robustness of neural networks with optimal control using the geometrical information of underlying data to design the control objective. The detailed analysis shows how the embedding manifolds of state trajectory affect error estimation of the proposed method. Our approach can simultaneously maintain the performance on clean data and improve the robustness against many types of data perturbations. It can also further improve the performance of robustly trained neural networks against different perturbations. To the best of our knowledge, this is the first work that improves the robustness of neural networks with close-loop control",
    "checked": true,
    "id": "1a452afbaf41306dbc9cbeb5cdedb85b85eacb0f",
    "semantic_title": "towards robust neural networks via close-loop control",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=MBpHUFrcG2x": {
    "title": "Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows",
    "volume": "poster",
    "abstract": "We introduce Projected Latent Markov Chain Monte Carlo (PL-MCMC), a technique for sampling from the exact conditional distributions learned by normalizing flows. As a conditional sampling method, PL-MCMC enables Monte Carlo Expectation Maximization (MC-EM) training of normalizing flows from incomplete data. Through experimental tests applying normalizing flows to missing data tasks for a variety of data sets, we demonstrate the efficacy of PL-MCMC for conditional sampling from normalizing flows",
    "checked": true,
    "id": "3d04a6572e19642881dd9b20d070e8b33800e519",
    "semantic_title": "projected latent markov chain monte carlo: conditional sampling of normalizing flows",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=fSTD6NFIW_b": {
    "title": "Understanding the failure modes of out-of-distribution generalization",
    "volume": "poster",
    "abstract": "Empirical studies suggest that machine learning models often rely on features, such as the background, that may be spuriously correlated with the label only during training time, resulting in poor accuracy during test-time. In this work, we identify the fundamental factors that give rise to this behavior, by explaining why models fail this way even in easy-to-learn tasks where one would expect these models to succeed. In particular, through a theoretical study of gradient-descent-trained linear classifiers on some easy-to-learn tasks, we uncover two complementary failure modes. These modes arise from how spurious correlations induce two kinds of skews in the data: one geometric in nature and another, statistical. Finally, we construct natural modifications of image classification datasets to understand when these failure modes can arise in practice. We also design experiments to isolate the two failure modes when training modern neural networks on these datasets",
    "checked": true,
    "id": "1b4a54670bb4fe15bcb0d06de0391d5b6d10ace2",
    "semantic_title": "understanding the failure modes of out-of-distribution generalization",
    "citation_count": 181,
    "authors": []
  },
  "https://openreview.net/forum?id=p8agn6bmTbr": {
    "title": "Usable Information and Evolution of Optimal Representations During Training",
    "volume": "poster",
    "abstract": "We introduce a notion of usable information contained in the representation learned by a deep network, and use it to study how optimal representations for the task emerge during training. We show that the implicit regularization coming from training with Stochastic Gradient Descent with a high learning-rate and small batch size plays an important role in learning minimal sufficient representations for the task. In the process of arriving at a minimal sufficient representation, we find that the content of the representation changes dynamically during training. In particular, we find that semantically meaningful but ultimately irrelevant information is encoded in the early transient dynamics of training, before being later discarded. In addition, we evaluate how perturbing the initial part of training impacts the learning dynamics and the resulting representations. We show these effects on both perceptual decision-making tasks inspired by neuroscience literature, as well as on standard image classification tasks",
    "checked": true,
    "id": "7a2628958ed2e3dd60e84d3ac380cda66109935e",
    "semantic_title": "usable information and evolution of optimal representations during training",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=R0a0kFI3dJx": {
    "title": "Adaptive Extra-Gradient Methods for Min-Max Optimization and Games",
    "volume": "poster",
    "abstract": "We present a new family of min-max optimization algorithms that automatically exploit the geometry of the gradient data observed at earlier iterations to perform more informative extra-gradient steps in later ones. Thanks to this adaptation mechanism, the proposed method automatically detects whether the problem is smooth or not, without requiring any prior tuning by the optimizer. As a result, the algorithm simultaneously achieves order-optimal convergence rates, \\ie it converges to an $\\varepsilon$-optimal solution within $\\mathcal{O}(1/\\varepsilon)$ iterations in smooth problems, and within $\\mathcal{O}(1/\\varepsilon^2)$ iterations in non-smooth ones. Importantly, these guarantees do not require any of the standard boundedness or Lipschitz continuity conditions that are typically assumed in the literature; in particular, they apply even to problems with singularities (such as resource allocation problems and the like). This adaptation is achieved through the use of a geometric apparatus based on Finsler metrics and a suitably chosen mirror-prox template that allows us to derive sharp convergence rates for the methods at hand",
    "checked": true,
    "id": "93d715bb46005880ebc9d32cb64344cf1697be07",
    "semantic_title": "adaptive extra-gradient methods for min-max optimization and games",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=OPyWRrcjVQw": {
    "title": "Shapley explainability on the data manifold",
    "volume": "poster",
    "abstract": "Explainability in AI is crucial for model development, compliance with regulation, and providing operational nuance to predictions. The Shapley framework for explainability attributes a model's predictions to its input features in a mathematically principled and model-agnostic way. However, general implementations of Shapley explainability make an untenable assumption: that the model's features are uncorrelated. In this work, we demonstrate unambiguous drawbacks of this assumption and develop two solutions to Shapley explainability that respect the data manifold. One solution, based on generative modelling, provides flexible access to data imputations; the other directly learns the Shapley value-function, providing performance and stability at the cost of flexibility. While \"off-manifold\" Shapley values can (i) give rise to incorrect explanations, (ii) hide implicit model dependence on sensitive attributes, and (iii) lead to unintelligible explanations in higher-dimensional data, on-manifold explainability overcomes these problems",
    "checked": true,
    "id": "4c35e7cb5731c6f281e98b524d4e922c9fb88b3b",
    "semantic_title": "shapley explainability on the data manifold",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=QFYnKlBJYR": {
    "title": "Reinforcement Learning with Random Delays",
    "volume": "poster",
    "abstract": "Action and observation delays commonly occur in many Reinforcement Learning applications, such as remote control scenarios. We study the anatomy of randomly delayed environments, and show that partially resampling trajectory fragments in hindsight allows for off-policy multi-step value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor-Critic with significantly better performance in environments with delays. This is shown theoretically and also demonstrated practically on a delay-augmented version of the MuJoCo continuous control benchmark",
    "checked": true,
    "id": "10efeca4a3237504985963fff25ef34fb4808737",
    "semantic_title": "reinforcement learning with random delays",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=NcFEZOi-rLa": {
    "title": "Shape or Texture: Understanding Discriminative Features in CNNs",
    "volume": "poster",
    "abstract": "Contrasting the previous evidence that neurons in the later layers of a Convolutional Neural Network (CNN) respond to complex object shapes, recent studies have shown that CNNs actually exhibit a 'texture bias': given an image with both texture and shape cues (e.g., a stylized image), a CNN is biased towards predicting the category corresponding to the texture. However, these previous studies conduct experiments on the final classification output of the network, and fail to robustly evaluate the bias contained (i) in the latent representations, and (ii) on a per-pixel level. In this paper, we design a series of experiments that overcome these issues. We do this with the goal of better understanding what type of shape information contained in the network is discriminative, where shape information is encoded, as well as when the network learns about object shape during training. We show that a network learns the majority of overall shape information at the first few epochs of training and that this information is largely encoded in the last few layers of a CNN. Finally, we show that the encoding of shape does not imply the encoding of localized per-pixel semantic information. The experimental results and findings provide a more accurate understanding of the behaviour of current CNNs, thus helping to inform future design choices",
    "checked": true,
    "id": "fa3d0599f8a082add349b5b09a208136489dae34",
    "semantic_title": "shape or texture: understanding discriminative features in cnns",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=Iw4ZGwenbXf": {
    "title": "NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control",
    "volume": "poster",
    "abstract": "In this work we propose the use of adaptive stochastic search as a building block for general, non-convex optimization operations within deep neural network architectures. Specifically, for an objective function located at some layer in the network and parameterized by some network parameters, we employ adaptive stochastic search to perform optimization over its output. This operation is differentiable and does not obstruct the passing of gradients during backpropagation, thus enabling us to incorporate it as a component in end-to-end learning. We study the proposed optimization module's properties and benchmark it against two existing alternatives on a synthetic energy-based structured prediction task, and further showcase its use in stochastic optimal control applications",
    "checked": false,
    "id": "625052a8c9df7c69f0325e22a0b88ec080b2b7b0",
    "semantic_title": "non-convex optimization via adaptive stochastic search for end-to-end learning and control",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=Ovp8dvB8IBH": {
    "title": "Negative Data Augmentation",
    "volume": "poster",
    "abstract": "Data augmentation is often used to enlarge datasets with synthetic samples generated in accordance with the underlying data distribution. To enable a wider range of augmentations, we explore negative data augmentation strategies (NDA) that intentionally create out-of-distribution samples. We show that such negative out-of-distribution samples provide information on the support of the data distribution, and can be leveraged for generative modeling and representation learning. We introduce a new GAN training objective where we use NDA as an additional source of synthetic data for the discriminator. We prove that under suitable conditions, optimizing the resulting objective still recovers the true data distribution but can directly bias the generator towards avoiding samples that lack the desired structure. Empirically, models trained with our method achieve improved conditional/unconditional image generation along with improved anomaly detection capabilities. Further, we incorporate the same negative data augmentation strategy in a contrastive learning framework for self-supervised representation learning on images and videos, achieving improved performance on downstream image classification, object detection, and action recognition tasks. These results suggest that prior knowledge on what does not constitute valid data is an effective form of weak supervision across a range of unsupervised learning tasks",
    "checked": true,
    "id": "b4beb15b524c583cd828300605bab66dc3caf386",
    "semantic_title": "negative data augmentation",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=jHefDGsorp5": {
    "title": "Molecule Optimization by Explainable Evolution",
    "volume": "poster",
    "abstract": "Optimizing molecules for desired properties is a fundamental yet challenging task in chemistry, material science, and drug discovery. This paper develops a novel algorithm for optimizing molecular properties via an Expectation-Maximization (EM) like explainable evolutionary process. The algorithm is designed to mimic human experts in the process of searching for desirable molecules and alternate between two stages: the first stage on explainable local search which identifies rationales, i.e., critical subgraph patterns accounting for desired molecular properties, and the second stage on molecule completion which explores the larger space of molecules containing good rationales. We test our approach against various baselines on a real-world multi-property optimization task where each method is given the same number of queries to the property oracle. We show that our evolution-by-explanation algorithm is 79% better than the best baseline in terms of a generic metric combining aspects such as success rate, novelty, and diversity. Human expert evaluation on optimized molecules shows that 60% of top molecules obtained from our methods are deemed successful",
    "checked": true,
    "id": "8b1f79bc91be1768b73e7df78cf113036f2a30b7",
    "semantic_title": "molecule optimization by explainable evolution",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=VcB4QkSfyO": {
    "title": "Estimating Lipschitz constants of monotone deep equilibrium models",
    "volume": "poster",
    "abstract": "Several methods have been proposed in recent years to provide bounds on the Lipschitz constants of deep networks, which can be used to provide robustness guarantees, generalization bounds, and characterize the smoothness of decision boundaries. However, existing bounds get substantially weaker with increasing depth of the network, which makes it unclear how to apply such bounds to recently proposed models such as the deep equilibrium (DEQ) model, which can be viewed as representing an infinitely-deep network. In this paper, we show that monotone DEQs, a recently-proposed subclass of DEQs, have Lipschitz constants that can be bounded as a simple function of the strong monotonicity parameter of the network. We derive simple-yet-tight bounds on both the input-output mapping and the weight-output mapping defined by these networks, and demonstrate that they are small relative to those for comparable standard DNNs. We show that one can use these bounds to design monotone DEQ models, even with e.g. multi-scale convolutional structure, that still have constraints on the Lipschitz constant. We also highlight how to use these bounds to develop PAC-Bayes generalization bounds that do not depend on any depth of the network, and which avoid the exponential depth-dependence of comparable DNN bounds",
    "checked": true,
    "id": "ebc596f8f17c67923c1382916c2687f5d6d12c08",
    "semantic_title": "estimating lipschitz constants of monotone deep equilibrium models",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=3q5IqUrkcF": {
    "title": "Implicit Gradient Regularization",
    "volume": "poster",
    "abstract": "Gradient descent can be surprisingly good at optimizing deep neural networks without overfitting and without explicit regularization. We find that the discrete steps of gradient descent implicitly regularize models by penalizing gradient descent trajectories that have large loss gradients. We call this Implicit Gradient Regularization (IGR) and we use backward error analysis to calculate the size of this regularization. We confirm empirically that implicit gradient regularization biases gradient descent toward flat minima, where test errors are small and solutions are robust to noisy parameter perturbations. Furthermore, we demonstrate that the implicit gradient regularization term can be used as an explicit regularizer, allowing us to control this gradient regularization directly. More broadly, our work indicates that backward error analysis is a useful theoretical approach to the perennial question of how learning rate, model size, and parameter regularization interact to determine the properties of overparameterized models optimized with gradient descent",
    "checked": true,
    "id": "060eb1ad5da6a059320c244532ad5c9c0ab52485",
    "semantic_title": "implicit gradient regularization",
    "citation_count": 157,
    "authors": []
  },
  "https://openreview.net/forum?id=YCXrx6rRCXO": {
    "title": "Faster Binary Embeddings for Preserving Euclidean Distances",
    "volume": "poster",
    "abstract": "We propose a fast, distance-preserving, binary embedding algorithm to transform a high-dimensional dataset $\\mathcal{T}\\subseteq\\mathbb{R}^n$ into binary sequences in the cube $\\{\\pm 1\\}^m$. When $\\mathcal{T}$ consists of well-spread (i.e., non-sparse) vectors, our embedding method applies a stable noise-shaping quantization scheme to $A x$ where $A\\in\\mathbb{R}^{m\\times n}$ is a sparse Gaussian random matrix. This contrasts with most binary embedding methods, which usually use $x\\mapsto \\mathrm{sign}(Ax)$ for the embedding. Moreover, we show that Euclidean distances among the elements of $\\mathcal{T}$ are approximated by the $\\ell_1$ norm on the images of $\\{\\pm 1\\}^m$ under a fast linear transformation. This again contrasts with standard methods, where the Hamming distance is used instead. Our method is both fast and memory efficient, with time complexity $O(m)$ and space complexity $O(m)$ on well-spread data. When the data is not well-spread, we show that the approach still works provided that data is transformed via a Walsh-Hadamard matrix, but now the cost is $O(n\\log n)$ per data point. Further, we prove that the method is accurate and its associated error is comparable to that of a continuous valued Johnson-Lindenstrauss embedding plus a quantization error that admits a polynomial decay as the embedding dimension $m$ increases. Thus the length of the binary codes required to achieve a desired accuracy is quite small, and we show it can even be compressed further without compromising the accuracy. To illustrate our results, we test the proposed method on natural images and show that it achieves strong performance",
    "checked": true,
    "id": "03c5afa7595fb7eba7ecd268889cf92355e47ba5",
    "semantic_title": "faster binary embeddings for preserving euclidean distances",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=23ZjUGpjcc": {
    "title": "Scalable Transfer Learning with Expert Models",
    "volume": "poster",
    "abstract": "Transfer of pre-trained representations can improve sample efficiency and reduce computational requirements for new tasks. However, representations used for transfer are usually generic, and are not tailored to a particular distribution of downstream tasks. We explore the use of expert representations for transfer with a simple, yet effective, strategy. We train a diverse set of experts by exploiting existing label structures, and use cheap-to-compute performance proxies to select the relevant expert for each target task. This strategy scales the process of transferring to new tasks, since it does not revisit the pre-training data during transfer. Accordingly, it requires little extra compute per target task, and results in a speed-up of 2-3 orders of magnitude compared to competing approaches. Further, we provide an adapter-based architecture able to compress many experts into a single model. We evaluate our approach on two different data sources and demonstrate that it outperforms baselines on over 20 diverse vision tasks in both cases",
    "checked": true,
    "id": "beffe798cff58416e06372904de204dcf9fb9157",
    "semantic_title": "scalable transfer learning with expert models",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=lqU2cs3Zca": {
    "title": "Signatory: differentiable computations of the signature and logsignature transforms, on both CPU and GPU",
    "volume": "poster",
    "abstract": "Signatory is a library for calculating and performing functionality related to the signature and logsignature transforms. The focus is on machine learning, and as such includes features such as CPU parallelism, GPU support, and backpropagation. To our knowledge it is the first GPU-capable library for these operations. Signatory implements new features not available in previous libraries, such as efficient precomputation strategies. Furthermore, several novel algorithmic improvements are introduced, producing substantial real-world speedups even on the CPU without parallelism. The library operates as a Python wrapper around C++, and is compatible with the PyTorch ecosystem. It may be installed directly via \\texttt{pip}. Source code, documentation, examples, benchmarks and tests may be found at \\texttt{\\url{https://github.com/patrick-kidger/signatory}}. The license is Apache-2.0",
    "checked": true,
    "id": "eb284f2c2d8611e2a5cbfe9bdc6e070d700a6e0f",
    "semantic_title": "signatory: differentiable computations of the signature and logsignature transforms, on both cpu and gpu",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=IFqrg1p5Bc": {
    "title": "Distance-Based Regularisation of Deep Networks for Fine-Tuning",
    "volume": "poster",
    "abstract": "We investigate approaches to regularisation during fine-tuning of deep neural networks. First we provide a neural network generalisation bound based on Rademacher complexity that uses the distance the weights have moved from their initial values. This bound has no direct dependence on the number of weights and compares favourably to other bounds when applied to convolutional networks. Our bound is highly relevant for fine-tuning, because providing a network with a good initialisation based on transfer learning means that learning can modify the weights less, and hence achieve tighter generalisation. Inspired by this, we develop a simple yet effective fine-tuning algorithm that constrains the hypothesis class to a small sphere centred on the initial pre-trained weights, thus obtaining provably better generalisation performance than conventional transfer learning. Empirical evaluation shows that our algorithm works well, corroborating our theoretical results. It outperforms both state of the art fine-tuning competitors, and penalty-based alternatives that we show do not directly constrain the radius of the search space",
    "checked": true,
    "id": "3a807ae90aceba10b0d3150506084dc4fae69def",
    "semantic_title": "distance-based regularisation of deep networks for fine-tuning",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=iWLByfvUhN": {
    "title": "Decoupling Global and Local Representations via Invertible Generative Flows",
    "volume": "poster",
    "abstract": "In this work, we propose a new generative model that is capable of automatically decoupling global and local representations of images in an entirely unsupervised setting, by embedding a generative flow in the VAE framework to model the decoder. Specifically, the proposed model utilizes the variational auto-encoding framework to learn a (low-dimensional) vector of latent variables to capture the global information of an image, which is fed as a conditional input to a flow-based invertible decoder with architecture borrowed from style transfer literature. Experimental results on standard image benchmarks demonstrate the effectiveness of our model in terms of density estimation, image generation and unsupervised representation learning. Importantly, this work demonstrates that with only architectural inductive biases, a generative model with a likelihood-based objective is capable of learning decoupled representations, requiring no explicit supervision. The code for our model is available at \\url{https://github.com/XuezheMax/wolf}",
    "checked": true,
    "id": "3e384108a9cb09acfaa96c38aa1502fbf08ab771",
    "semantic_title": "decoupling global and local representations via invertible generative flows",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=C3qvk5IQIJY": {
    "title": "Understanding Over-parameterization in Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "A broad class of unsupervised deep learning methods such as Generative Adversarial Networks (GANs) involve training of overparameterized models where the number of parameters of the model exceeds a certain threshold. Indeed, most successful GANs used in practice are trained using overparameterized generator and discriminator networks, both in terms of depth and width. A large body of work in supervised learning have shown the importance of model overparameterization in the convergence of the gradient descent (GD) to globally optimal solutions. In contrast, the unsupervised setting and GANs in particular involve non-convex concave mini-max optimization problems that are often trained using Gradient Descent/Ascent (GDA). The role and benefits of model overparameterization in the convergence of GDA to a global saddle point in non-convex concave problems is far less understood. In this work, we present a comprehensive analysis of the importance of model overparameterization in GANs both theoretically and empirically. We theoretically show that in an overparameterized GAN model with a $1$-layer neural network generator and a linear discriminator, GDA converges to a global saddle point of the underlying non-convex concave min-max problem. To the best of our knowledge, this is the first result for global convergence of GDA in such settings. Our theory is based on a more general result that holds for a broader class of nonlinear generators and discriminators that obey certain assumptions (including deeper generators and random feature discriminators). Our theory utilizes and builds upon a novel connection with the convergence analysis of linear time-varying dynamical systems which may have broader implications for understanding the convergence behavior of GDA for non-convex concave problems involving overparameterized models. We also empirically study the role of model overparameterization in GANs using several large-scale experiments on CIFAR-10 and Celeb-A datasets. Our experiments show that overparameterization improves the quality of generated samples across various model architectures and datasets. Remarkably, we observe that overparameterization leads to faster and more stable convergence behavior of GDA across the board",
    "checked": false,
    "id": "6f585a16bf7744f0788f803da0549bd555c87ea3",
    "semantic_title": "understanding estimation and generalization error of generative adversarial networks",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=A2gNouoXE7": {
    "title": "Filtered Inner Product Projection for Crosslingual Embedding Alignment",
    "volume": "poster",
    "abstract": "Due to widespread interest in machine translation and transfer learning, there are numerous algorithms for mapping multiple embeddings to a shared representation space. Recently, these algorithms have been studied in the setting of bilingual lexicon induction where one seeks to align the embeddings of a source and a target language such that translated word pairs lie close to one another in a common representation space. In this paper, we propose a method, Filtered Inner Product Projection (FIPP), for mapping embeddings to a common representation space. As semantic shifts are pervasive across languages and domains, FIPP first identifies the common geometric structure in both embeddings and then, only on the common structure, aligns the Gram matrices of these embeddings. FIPP is applicable even when the source and target embeddings are of differing dimensionalities. Additionally, FIPP provides computational benefits in ease of implementation and is faster to compute than current approaches. Following the baselines in Glavas et al. 2019, we evaluate FIPP both in the context of bilingual lexicon induction and downstream language tasks. We show that FIPP outperforms existing methods on the XLING BLI dataset for most language pairs while also providing robust performance across downstream tasks",
    "checked": true,
    "id": "d36ab43c91f29e3e4979a74fc74a03c18cb52953",
    "semantic_title": "filtered inner product projection for crosslingual embedding alignment",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=YUGG2tFuPM": {
    "title": "Deep Partition Aggregation: Provable Defenses against General Poisoning Attacks",
    "volume": "poster",
    "abstract": "Adversarial poisoning attacks distort training data in order to corrupt the test-time behavior of a classifier. A provable defense provides a certificate for each test sample, which is a lower bound on the magnitude of any adversarial distortion of the training set that can corrupt the test sample's classification. We propose two novel provable defenses against poisoning attacks: (i) Deep Partition Aggregation (DPA), a certified defense against a general poisoning threat model, defined as the insertion or deletion of a bounded number of samples to the training set --- by implication, this threat model also includes arbitrary distortions to a bounded number of images and/or labels; and (ii) Semi-Supervised DPA (SS-DPA), a certified defense against label-flipping poisoning attacks. DPA is an ensemble method where base models are trained on partitions of the training set determined by a hash function. DPA is related to both subset aggregation, a well-studied ensemble method in classical machine learning, as well as to randomized smoothing, a popular provable defense against evasion (inference) attacks. Our defense against label-flipping poison attacks, SS-DPA, uses a semi-supervised learning algorithm as its base classifier model: each base classifier is trained using the entire unlabeled training set in addition to the labels for a partition. SS-DPA significantly outperforms the existing certified defense for label-flipping attacks (Rosenfeld et al., 2020) on both MNIST and CIFAR-10: provably tolerating, for at least half of test images, over 600 label flips (vs. < 200 label flips) on MNIST and over 300 label flips (vs. 175 label flips) on CIFAR-10. Against general poisoning attacks where no prior certified defenses exists, DPA can certify $\\geq$ 50% of test images against over 500 poison image insertions on MNIST, and nine insertions on CIFAR-10. These results establish new state-of-the-art provable defenses against general and label-flipping poison attacks. Code is available at https://github.com/alevine0/DPA",
    "checked": true,
    "id": "6cf1116dc8b431b2471bb5407d9a8be0eb067c2d",
    "semantic_title": "deep partition aggregation: provable defense against general poisoning attacks",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=9GBZBPn0Jx": {
    "title": "HalentNet: Multimodal Trajectory Forecasting with Hallucinative Intents",
    "volume": "poster",
    "abstract": "Motion forecasting is essential for making intelligent decisions in robotic navigation. As a result, the multi-agent behavioral prediction has become a core component of modern human-robot interaction applications such as autonomous driving. Due to various intentions and interactions among agents, agent trajectories can have multiple possible futures. Hence, the motion forecasting model's ability to cover possible modes becomes essential to enable accurate prediction. Towards this goal, we introduce HalentNet to better model the future motion distribution in addition to a traditional trajectory regression learning objective by incorporating generative augmentation losses. We model intents with unsupervised discrete random variables whose training is guided by a collaboration between two key signals: A discriminative loss that encourages intents' diversity and a hallucinative loss that explores intent transitions (i.e., mixed intents) and encourages their smoothness. This regulates the neural network behavior to be more accurately predictive on uncertain scenarios due to the active yet careful exploration of possible future agent behavior. Our model's learned representation leads to better and more semantically meaningful coverage of the trajectory distribution. Our experiments show that our method can improve over the state-of-the-art trajectory forecasting benchmarks, including vehicles and pedestrians, for about 20% on average FDE and 50% on road boundary violation rate when predicting 6 seconds future. We also conducted human experiments to show that our predicted trajectories received 39.6% more votes than the runner-up approach and 32.2% more votes than our variant without hallucinative mixed intent loss. The code will be released soon",
    "checked": true,
    "id": "84b6c6923127e01f820636273bfc00b06c998f0b",
    "semantic_title": "halentnet: multimodal trajectory forecasting with hallucinative intents",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=O6LPudowNQm": {
    "title": "INT: An Inequality Benchmark for Evaluating Generalization in Theorem Proving",
    "volume": "poster",
    "abstract": "In learning-assisted theorem proving, one of the most critical challenges is to generalize to theorems unlike those seen at training time. In this paper, we introduce INT, an INequality Theorem proving benchmark designed to test agents' generalization ability. INT is based on a theorem generator, which provides theoretically infinite data and allows us to measure 6 different types of generalization, each reflecting a distinct challenge, characteristic of automated theorem proving. In addition, provides a fast theorem proving environment with sequence-based and graph-based interfaces, conducive to performing learning-based research. We introduce base-lines with architectures including transformers and graph neural networks (GNNs)for INT. Using INT, we find that transformer-based agents achieve stronger test performance for most of the generalization tasks, despite having much larger out-of-distribution generalization gaps than GNNs. We further find that the addition of Monte Carlo Tree Search (MCTS) at test time helps to prove new theorems",
    "checked": true,
    "id": "016863a86189c4e8ccecf9a36c4406c439a8a84c",
    "semantic_title": "int: an inequality benchmark for evaluating generalization in theorem proving",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=lgNx56yZh8a": {
    "title": "Bayesian Few-Shot Classification with One-vs-Each Pólya-Gamma Augmented Gaussian Processes",
    "volume": "poster",
    "abstract": "Few-shot classification (FSC), the task of adapting a classifier to unseen classes given a small labeled dataset, is an important step on the path toward human-like machine learning. Bayesian methods are well-suited to tackling the fundamental issue of overfitting in the few-shot scenario because they allow practitioners to specify prior beliefs and update those beliefs in light of observed data. Contemporary approaches to Bayesian few-shot classification maintain a posterior distribution over model parameters, which is slow and requires storage that scales with model size. Instead, we propose a Gaussian process classifier based on a novel combination of Pólya-Gamma augmentation and the one-vs-each softmax approximation that allows us to efficiently marginalize over functions rather than model parameters. We demonstrate improved accuracy and uncertainty quantification on both standard few-shot classification benchmarks and few-shot domain transfer tasks",
    "checked": true,
    "id": "2aeada5a75c3e5a6eb2e1589d6cd5da06215af8b",
    "semantic_title": "bayesian few-shot classification with one-vs-each pólya-gamma augmented gaussian processes",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=l35SB-_raSQ": {
    "title": "A Hypergradient Approach to Robust Regression without Correspondence",
    "volume": "poster",
    "abstract": "We consider a regression problem, where the correspondence between the input and output data is not available. Such shuffled data are commonly observed in many real world problems. Take flow cytometry as an example: the measuring instruments are unable to preserve the correspondence between the samples and the measurements. Due to the combinatorial nature of the problem, most of the existing methods are only applicable when the sample size is small, and are limited to linear regression models. To overcome such bottlenecks, we propose a new computational framework --- ROBOT --- for the shuffled regression problem, which is applicable to large data and complex models. Specifically, we propose to formulate regression without correspondence as a continuous optimization problem. Then by exploiting the interaction between the regression model and the data correspondence, we propose to develop a hypergradient approach based on differentiable programming techniques. Such a hypergradient approach essentially views the data correspondence as an operator of the regression model, and therefore it allows us to find a better descent direction for the model parameters by differentiating through the data correspondence. ROBOT is quite general, and can be further extended to an inexact correspondence setting, where the input and output data are not necessarily exactly aligned. Thorough numerical experiments show that ROBOT achieves better performance than existing methods in both linear and nonlinear regression tasks, including real-world applications such as flow cytometry and multi-object tracking",
    "checked": true,
    "id": "e01eeb3fc6e681cd2edac0be9f6ef6493b485574",
    "semantic_title": "a hypergradient approach to robust regression without correspondence",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=W3Wf_wKmqm9": {
    "title": "C-Learning: Horizon-Aware Cumulative Accessibility Estimation",
    "volume": "poster",
    "abstract": "Multi-goal reaching is an important problem in reinforcement learning needed to achieve algorithmic generalization. Despite recent advances in this field, current algorithms suffer from three major challenges: high sample complexity, learning only a single way of reaching the goals, and difficulties in solving complex motion planning tasks. In order to address these limitations, we introduce the concept of cumulative accessibility functions, which measure the reachability of a goal from a given state within a specified horizon. We show that these functions obey a recurrence relation, which enables learning from offline interactions. We also prove that optimal cumulative accessibility functions are monotonic in the planning horizon. Additionally, our method can trade off speed and reliability in goal-reaching by suggesting multiple paths to a single goal depending on the provided horizon. We evaluate our approach on a set of multi-goal discrete and continuous control tasks. We show that our method outperforms state-of-the-art goal-reaching algorithms in success rate, sample complexity, and path optimality. Our code is available at https://github.com/layer6ai-labs/CAE, and additional visualizations can be found at https://sites.google.com/view/learning-cae/",
    "checked": true,
    "id": "d6a02799e3b71f137b15ffb0a460cc7b3080ec56",
    "semantic_title": "c-learning: horizon-aware cumulative accessibility estimation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=W1G1JZEIy5_": {
    "title": "MIROSTAT: A NEURAL TEXT DECODING ALGORITHM THAT DIRECTLY CONTROLS PERPLEXITY",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e8f1d69cdf4d92350ce237e2d6a615ebe2e52e43",
    "semantic_title": "mirostat: a neural text decoding algorithm that directly controls perplexity",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=bB2drc7DPuB": {
    "title": "Global optimality of softmax policy gradient with single hidden layer neural networks in the mean-field regime",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "262851f264c6a0b10bad077289c52c4668156378",
    "semantic_title": "global optimality of softmax policy gradient with single hidden layer neural networks in the mean-field regime",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=guetrIHLFGI": {
    "title": "The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "339b5d3316d13062d936b335aab06e9da48a5c17",
    "semantic_title": "the deep bootstrap framework: good online learners are good offline generalizers",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=vYVI1CHPaQg": {
    "title": "A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0ffe2dde5dc78775bcf0c116661664845937b499",
    "semantic_title": "a better alternative to error feedback for communication-efficient distributed learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=9FWas6YbmB3": {
    "title": "DrNAS: Dirichlet Neural Architecture Search",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "11b430ae91c5b7a3e51f86235818f74fd8b449ef",
    "semantic_title": "drnas: dirichlet neural architecture search",
    "citation_count": 105,
    "authors": []
  },
  "https://openreview.net/forum?id=FmMKSO4e8JK": {
    "title": "Offline Model-Based Optimization via Normalized Maximum Likelihood Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3a6a35fe9871a702add7c9396267b64efc773234",
    "semantic_title": "offline model-based optimization via normalized maximum likelihood estimation",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=enoVQWLsfyL": {
    "title": "Viewmaker Networks: Learning Views for Unsupervised Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "600881eb8a24caee65633f796d5a7ef40c1cc9b0",
    "semantic_title": "viewmaker networks: learning views for unsupervised representation learning",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=vXj_ucZQ4hA": {
    "title": "Robust Pruning at Initialization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6d60d0dd613fb1bc9b6a52f6b3e8b65599cade5a",
    "semantic_title": "robust pruning at initialization",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=CHLhSw9pSw8": {
    "title": "Single-Photon Image Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e86099c24d7825fa36a484981eff11474debe3b3",
    "semantic_title": "single-photon image classification",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=xfmSoxdxFCG": {
    "title": "Can a Fruit Fly Learn Word Embeddings?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "65d77663124f850bc5a87efb286cffd2e31ba8c1",
    "semantic_title": "can a fruit fly learn word embeddings?",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=ajOrOhQOsYx": {
    "title": "A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7379045f1abe69ec6f89c53293ab1bb41fa156ec",
    "semantic_title": "a wigner-eckart theorem for group equivariant convolution kernels",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=dKg5D1Z1Lm": {
    "title": "Non-asymptotic Confidence Intervals of Off-policy Evaluation: Primal and Dual Bounds",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8990c62f00fc31c3dabdba049eb0a1d888c3ee40",
    "semantic_title": "non-asymptotic confidence intervals of off-policy evaluation: primal and dual bounds",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=9YlaeLfuhJF": {
    "title": "Model Patching: Closing the Subgroup Performance Gap with Data Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9207480a5cd071a3e85f408082b09283413cbfa5",
    "semantic_title": "model patching: closing the subgroup performance gap with data augmentation",
    "citation_count": 121,
    "authors": []
  },
  "https://openreview.net/forum?id=fw-BHZ1KjxJ": {
    "title": "SOLAR: Sparse Orthogonal Learned and Random Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b2881852a6b072c731c0a54001d2908b488cd86e",
    "semantic_title": "solar: sparse orthogonal learned and random embeddings",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=snOgiCYZgJ7": {
    "title": "Neural representation and generation for RNA secondary structures",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "590b227307e0c33ac6128005234b783e8ec6584b",
    "semantic_title": "neural representation and generation for rna secondary structures",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=vVjIW3sEc1s": {
    "title": "A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "01400290c7db96c4d665d1c29519c42ba47401e0",
    "semantic_title": "a mathematical exploration of why language models help solve downstream tasks",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=gl3D-xY7wLq": {
    "title": "Noise or Signal: The Role of Image Backgrounds in Object Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5c63fc87400a4d3afea63ab8a068a47249f815c2",
    "semantic_title": "noise or signal: the role of image backgrounds in object recognition",
    "citation_count": 397,
    "authors": []
  },
  "https://openreview.net/forum?id=yUxUNaj2Sl": {
    "title": "Does enhanced shape bias improve neural network robustness to common corruptions?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3797437ea7990e99cbba6e94d402650a842ba738",
    "semantic_title": "does enhanced shape bias improve neural network robustness to common corruptions?",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=LXMSvPmsm0g": {
    "title": "Long Live the Lottery: The Existence of Winning Tickets in Lifelong Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "05e2ca9357bcf542a33b3f97310d9f477cd0776f",
    "semantic_title": "long live the lottery: the existence of winning tickets in lifelong learning",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=MjvduJCsE4": {
    "title": "Exploring the Uncertainty Properties of Neural Networks' Implicit Priors in the Infinite-Width Limit",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7edf5e8e02301d3826d634faeda547b6cb28e8fe",
    "semantic_title": "exploring the uncertainty properties of neural networks' implicit priors in the infinite-width limit",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=TK_6nNb_C7q": {
    "title": "Hierarchical Autoregressive Modeling for Neural Video Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8cd7485c4065832cfdfe63e8b53542463b7a3106",
    "semantic_title": "hierarchical autoregressive modeling for neural video compression",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=ujmgfuxSLrO": {
    "title": "DeLighT: Deep and Light-weight Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b08c360ddf899923aebf25913706b4f03e54eccd",
    "semantic_title": "delight: deep and light-weight transformer",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=nkIDwI6oO4_": {
    "title": "Learning A Minimax Optimizer: A Pilot Study",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "36f6f7eba02b03a65bd3005893ec1e488c726d8e",
    "semantic_title": "learning a minimax optimizer: a pilot study",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=zElset1Klrp": {
    "title": "Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c65467138600a36577045a8dac0cbe3287092f6b",
    "semantic_title": "fuzzy tiling activations: a simple approach to learning sparse representations online",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=dgtpE6gKjHn": {
    "title": "FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "60ba0a0c600df527b87525469dcde42f1925cdcc",
    "semantic_title": "fedbe: making bayesian model ensemble applicable to federated learning",
    "citation_count": 269,
    "authors": []
  },
  "https://openreview.net/forum?id=7uVcpu-gMD": {
    "title": "Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "649c758b0e59ddedaae37a3757e8eabdba664e5a",
    "semantic_title": "are neural nets modular? inspecting functional modularity through differentiable weight masks",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=-bxf89v3Nx": {
    "title": "Calibration tests beyond classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b54d5ea54cfe0f0f8ccdcb564717f19d857f522f",
    "semantic_title": "calibration tests beyond classification",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=3jjmdp7Hha": {
    "title": "Meta Back-Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fcdac45272543b4f8b8eaa59d66044d1b7018494",
    "semantic_title": "meta back-translation",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=vujTf_I8Kmc": {
    "title": "Attentional Constellation Nets for Few-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ab6413f67becd89dd7676c5cf5e260e13b42ddfd",
    "semantic_title": "attentional constellation nets for few-shot learning",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=jN5y-zb5Q7m": {
    "title": "Uncertainty Estimation in Autoregressive Structured Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0921322cf6ea34d1852f13cb67eeac9d1f863518",
    "semantic_title": "uncertainty estimation in autoregressive structured prediction",
    "citation_count": 280,
    "authors": []
  },
  "https://openreview.net/forum?id=d7KBjmI3GmQ": {
    "title": "Measuring Massive Multitask Language Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "814a4f680b9ba6baba23b93499f4b48af1a27678",
    "semantic_title": "measuring massive multitask language understanding",
    "citation_count": 4936,
    "authors": []
  },
  "https://openreview.net/forum?id=ixpSxO9flk3": {
    "title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "13b154ca78b245e3125ed67b90438ec50b859df3",
    "semantic_title": "no mcmc for me: amortized sampling for fast and stable training of energy-based models",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=dNy_RKzJacY": {
    "title": "Aligning AI With Shared Human Values",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "65906e6027246ae9e4ecd18d6e019a24505c842e",
    "semantic_title": "aligning ai with shared human values",
    "citation_count": 594,
    "authors": []
  },
  "https://openreview.net/forum?id=7pgFL2Dkyyy": {
    "title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2c1572ff4a708397405210f780006b7dd2274ec8",
    "semantic_title": "class normalization for (continual)? generalized zero-shot learning",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=hb1sDDSLbV": {
    "title": "Learning explanations that are hard to vary",
    "volume": "poster",
    "abstract": "In this paper, we investigate the principle that good explanations are hard to vary in the context of deep learning. We show that averaging gradients across examples -- akin to a logical OR of patterns -- can favor memorization and `patchwork' solutions that sew together different strategies, instead of identifying invariances. To inspect this, we first formalize a notion of consistency for minima of the loss surface, which measures to what extent a minimum appears only when examples are pooled. We then propose and experimentally validate a simple alternative algorithm based on a logical AND, that focuses on invariances and prevents memorization in a set of real-world tasks. Finally, using a synthetic dataset with a clear distinction between invariant and spurious mechanisms, we dissect learning signals and compare this approach to well-established regularizers",
    "checked": true,
    "id": "6972010d883e9746ecdf8147168caeccef6dcdb3",
    "semantic_title": "learning explanations that are hard to vary",
    "citation_count": 191,
    "authors": []
  },
  "https://openreview.net/forum?id=NSBrFgJAHg": {
    "title": "Degree-Quant: Quantization-Aware Training for Graph Neural Networks",
    "volume": "poster",
    "abstract": "Graph neural networks (GNNs) have demonstrated strong performance on a wide variety of tasks due to their ability to model non-uniform structured data. Despite their promise, there exists little research exploring methods to make them more efficient at inference time. In this work, we explore the viability of training quantized GNNs, enabling the usage of low precision integer arithmetic during inference. For GNNs seemingly unimportant choices in quantization implementation cause dramatic changes in performance. We identify the sources of error that uniquely arise when attempting to quantize GNNs, and propose an architecturally-agnostic and stable method, Degree-Quant, to improve performance over existing quantization-aware training baselines commonly used on other architectures, such as CNNs. We validate our method on six datasets and show, unlike previous quantization attempts, that models generalize to unseen graphs. Models trained with Degree-Quant for INT8 quantization perform as well as FP32 models in most cases; for INT4 models, we obtain up to 26% gains over the baselines. Our work enables up to 4.7x speedups on CPU when using INT8 arithmetic",
    "checked": true,
    "id": "9c0e855382de7e708c8eea7b4d5cf792bcd4a326",
    "semantic_title": "degree-quant: quantization-aware training for graph neural networks",
    "citation_count": 147,
    "authors": []
  },
  "https://openreview.net/forum?id=PbEHqvFtcS": {
    "title": "Byzantine-Resilient Non-Convex Stochastic Gradient Descent",
    "volume": "poster",
    "abstract": "We study adversary-resilient stochastic distributed optimization, in which $m$ machines can independently compute stochastic gradients, and cooperate to jointly optimize over their local objective functions. However, an $\\alpha$-fraction of the machines are Byzantine, in that they may behave in arbitrary, adversarial ways. We consider a variant of this procedure in the challenging non-convex case. Our main result is a new algorithm SafeguardSGD, which can provably escape saddle points and find approximate local minima of the non-convex objective. The algorithm is based on a new concentration filtering technique, and its sample and time complexity bounds match the best known theoretical bounds in the stochastic, distributed setting when no Byzantine machines are present. Our algorithm is very practical: it improves upon the performance of all prior methods when training deep neural networks, it is relatively lightweight, and it is the first method to withstand two recently-proposed Byzantine attacks",
    "checked": true,
    "id": "029c33e39236f01e83a952c4203d86e07a6c1532",
    "semantic_title": "byzantine-resilient non-convex stochastic gradient descent",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=dOcQK-f4byz": {
    "title": "Teaching Temporal Logics to Neural Networks",
    "volume": "poster",
    "abstract": "We study two fundamental questions in neuro-symbolic computing: can deep learning tackle challenging problems in logics end-to-end, and can neural networks learn the semantics of logics. In this work we focus on linear-time temporal logic (LTL), as it is widely used in verification. We train a Transformer on the problem to directly predict a solution, i.e. a trace, to a given LTL formula. The training data is generated with classical solvers, which, however, only provide one of many possible solutions to each formula. We demonstrate that it is sufficient to train on those particular solutions to formulas, and that Transformers can predict solutions even to formulas from benchmarks from the literature on which the classical solver timed out. Transformers also generalize to the semantics of the logics: while they often deviate from the solutions found by the classical solvers, they still predict correct solutions to most formulas",
    "checked": true,
    "id": "b32e631ae7779d93b9979c61c5b920a76342063e",
    "semantic_title": "teaching temporal logics to neural networks",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=7dpmlkBuJFC": {
    "title": "Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification",
    "volume": "poster",
    "abstract": "Differentially private SGD (DP-SGD) is one of the most popular methods for solving differentially private empirical risk minimization (ERM). Due to its noisy perturbation on each gradient update, the error rate of DP-SGD scales with the ambient dimension $p$, the number of parameters in the model. Such dependence can be problematic for over-parameterized models where $p \\gg n$, the number of training samples. Existing lower bounds on private ERM show that such dependence on $p$ is inevitable in the worst case. In this paper, we circumvent the dependence on the ambient dimension by leveraging a low-dimensional structure of gradient space in deep networks---that is, the stochastic gradients for deep nets usually stay in a low dimensional subspace in the training process. We propose Projected DP-SGD that performs noise reduction by projecting the noisy gradients to a low-dimensional subspace, which is given by the top gradient eigenspace on a small public dataset. We provide a general sample complexity analysis on the public dataset for the gradient subspace identification problem and demonstrate that under certain low-dimensional assumptions the public sample complexity only grows logarithmically in $p$. Finally, we provide a theoretical analysis and empirical evaluations to show that our method can substantially improve the accuracy of DP-SGD in the high privacy regime (corresponding to low privacy loss $\\epsilon$)",
    "checked": true,
    "id": "465c52d9aa7b451a6ced3fafb377bac1b7da1ca1",
    "semantic_title": "bypassing the ambient dimension: private sgd with gradient subspace identification",
    "citation_count": 112,
    "authors": []
  },
  "https://openreview.net/forum?id=PpshD0AXfA": {
    "title": "Generative Time-series Modeling with Fourier Flows",
    "volume": "poster",
    "abstract": "Generating synthetic time-series data is crucial in various application domains, such as medical prognosis, wherein research is hamstrung by the lack of access to data due to concerns over privacy. Most of the recently proposed methods for generating synthetic time-series rely on implicit likelihood modeling using generative adversarial networks (GANs)—but such models can be difficult to train, and may jeopardize privacy by \"memorizing\" temporal patterns in training data. In this paper, we propose an explicit likelihood model based on a novel class of normalizing flows that view time-series data in the frequency-domain rather than the time-domain. The proposed flow, dubbed a Fourier flow, uses a discrete Fourier transform (DFT) to convert variable-length time-series with arbitrary sampling periods into fixed-length spectral representations, then applies a (data-dependent) spectral filter to the frequency-transformed time-series. We show that, by virtue of the DFT analytic properties, the Jacobian determinants and inverse mapping for the Fourier flow can be computed efficiently in linearithmic time, without imposing explicit structural constraints as in existing flows such as NICE (Dinh et al. (2014)), RealNVP (Dinh et al. (2016)) and GLOW (Kingma & Dhariwal (2018)). Experiments show that Fourier flows perform competitively compared to state-of-the-art baselines",
    "checked": true,
    "id": "da21bb9843e1bcd0c50a0652585d2906c70abb18",
    "semantic_title": "generative time-series modeling with fourier flows",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=jphnJNOwe36": {
    "title": "Overparameterisation and worst-case generalisation: friend or foe?",
    "volume": "poster",
    "abstract": "Overparameterised neural networks have demonstrated the remarkable ability to perfectly fit training samples, while still generalising to unseen test samples. However, several recent works have revealed that such models' good average performance does not always translate to good worst-case performance: in particular, they may perform poorly on subgroups that are under-represented in the training set. In this paper, we show that in certain settings, overparameterised models' performance on under-represented subgroups may be improved via post-hoc processing. Specifically, such models' bias can be restricted to their classification layers, and manifest as structured prediction shifts for rare subgroups. We detail two post-hoc correction techniques to mitigate this bias, which operate purely on the outputs of standard model training. We empirically verify that with such post-hoc correction, overparameterisation can improve average and worst-case performance",
    "checked": true,
    "id": "0aae10ade8fc9e58e177e034b794fce45c32fde8",
    "semantic_title": "overparameterisation and worst-case generalisation: friend or foe?",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=TgSVWXw22FQ": {
    "title": "Improving Zero-Shot Voice Style Transfer via Disentangled Representation Learning",
    "volume": "poster",
    "abstract": "Voice style transfer, also called voice conversion, seeks to modify one speaker's voice to generate speech as if it came from another (target) speaker. Previous works have made progress on voice conversion with parallel training data and pre-known speakers. However, zero-shot voice style transfer, which learns from non-parallel data and generates voices for previously unseen speakers, remains a challenging problem. In this paper we propose a novel zero-shot voice transfer method via disentangled representation learning. The proposed method first encodes speaker-related style and voice content of each input voice into separate low-dimensional embedding spaces, and then transfers to a new voice by combining the source content embedding and target style embedding through a decoder. With information-theoretic guidance, the style and content embedding spaces are representative and (ideally) independent of each other. On real-world datasets, our method outperforms other baselines and obtains state-of-the-art results in terms of transfer accuracy and voice naturalness",
    "checked": true,
    "id": "1e3476b4f6f8b85d7e1a891dbb94caf80ee5fe35",
    "semantic_title": "improving zero-shot voice style transfer via disentangled representation learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=60j5LygnmD": {
    "title": "Meta-learning with negative learning rates",
    "volume": "poster",
    "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative values. These results help clarify under what circumstances meta-learning performs best",
    "checked": true,
    "id": "4454a763c891afb3fb8fa6567a367d05b1938e97",
    "semantic_title": "meta-learning with negative learning rates",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=PS3IMnScugk": {
    "title": "Learning to Recombine and Resample Data For Compositional Generalization",
    "volume": "poster",
    "abstract": "Flexible neural sequence models outperform grammar- and automaton-based counterparts on a variety of tasks. However, neural models perform poorly in settings requiring compositional generalization beyond the training data—particularly to rare or unseen subsequences. Past work has found symbolic scaffolding (e.g. grammars or automata) essential in these settings. We describe R&R, a learned data augmentation scheme that enables a large category of compositional generalizations without appeal to latent symbolic structure. R&R has two components: recombination of original training examples via a prototype-based generative model and resampling of generated examples to encourage extrapolation. Training an ordinary neural sequence model on a dataset augmented with recombined and resampled examples significantly improves generalization in two language processing problems—instruction following (SCAN) and morphological analysis (SIGMORPHON 2018)—where R&R enables learning of new constructions and tenses from as few as eight initial examples",
    "checked": true,
    "id": "5cc8ea815bd05be3b28519b489afe6de278a4209",
    "semantic_title": "learning to recombine and resample data for compositional generalization",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=4IwieFS44l": {
    "title": "Fooling a Complete Neural Network Verifier",
    "volume": "poster",
    "abstract": "The efficient and accurate characterization of the robustness of neural networks to input perturbation is an important open problem. Many approaches exist including heuristic and exact (or complete) methods. Complete methods are expensive but their mathematical formulation guarantees that they provide exact robustness metrics. However, this guarantee is valid only if we assume that the verified network applies arbitrary-precision arithmetic and the verifier is reliable. In practice, however, both the networks and the verifiers apply limited-precision floating point arithmetic. In this paper, we show that numerical roundoff errors can be exploited to craft adversarial networks, in which the actual robustness and the robustness computed by a state-of-the-art complete verifier radically differ. We also show that such adversarial networks can be used to insert a backdoor into any network in such a way that the backdoor is completely missed by the verifier. The attack is easy to detect in its naive form but, as we show, the adversarial network can be transformed to make its detection less trivial. We offer a simple defense against our particular attack based on adding a very small perturbation to the network weights. However, our conjecture is that other numerical attacks are possible, and exact verification has to take into account all the details of the computation executed by the verified networks, which makes the problem significantly harder",
    "checked": true,
    "id": "1f4229f0cafc65c2a5f8b7a7f0ae0fc2c79017b9",
    "semantic_title": "fooling a complete neural network verifier",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=BM---bH_RSh": {
    "title": "UMEC: Unified model and embedding compression for efficient recommendation systems",
    "volume": "poster",
    "abstract": "The recommendation system (RS) plays an important role in the content recommendation and retrieval scenarios. The core part of the system is the Ranking neural network, which is usually a bottleneck of whole system performance during online inference. In this work, we propose a unified model and embedding compression (UMEC) framework to hammer an efficient neural network-based recommendation system. Our framework jointly learns input feature selection and neural network compression together, and solve them as an end-to-end resource-constrained optimization problem using ADMM. Our method outperforms other baselines in terms of neural network Flops, sparse embedding feature size and the number of sparse embedding features. We evaluate our method on the public benchmark of DLRM, trained over the Kaggle Criteo dataset. The codes can be found at https://github.com/VITA-Group/UMEC",
    "checked": true,
    "id": "37cd69f55703f1bd3448016cfe30ee2e32252b27",
    "semantic_title": "umec: unified model and embedding compression for efficient recommendation systems",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=4dXmpCDGNp7": {
    "title": "Evaluations and Methods for Explanation through Robustness Analysis",
    "volume": "poster",
    "abstract": "Feature based explanations, that provide importance of each feature towards the model prediction, is arguably one of the most intuitive ways to explain a model. In this paper, we establish a novel set of evaluation criteria for such feature based explanations by robustness analysis. In contrast to existing evaluations which require us to specify some way to \"remove\" features that could inevitably introduces biases and artifacts, we make use of the subtler notion of smaller adversarial perturbations. By optimizing towards our proposed evaluation criteria, we obtain new explanations that are loosely necessary and sufficient for a prediction. We further extend the explanation to extract the set of features that would move the current prediction to a target class by adopting targeted adversarial attack for the robustness analysis. Through experiments across multiple domains and a user study, we validate the usefulness of our evaluation criteria and our derived explanations",
    "checked": true,
    "id": "20e0abdfed000269d3eabe1313223701030c2ed9",
    "semantic_title": "evaluations and methods for explanation through robustness analysis",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=HCSgyPUfeDj": {
    "title": "Learning and Evaluating Representations for Deep One-Class Classification",
    "volume": "poster",
    "abstract": "We present a two-stage framework for deep one-class classification. We first learn self-supervised representations from one-class data, and then build one-class classifiers on learned representations. The framework not only allows to learn better representations, but also permits building one-class classifiers that are faithful to the target task. We argue that classifiers inspired by the statistical perspective in generative or discriminative models are more effective than existing approaches, such as a normality score from a surrogate classifier. We thoroughly evaluate different self-supervised representation learning algorithms under the proposed framework for one-class classification. Moreover, we present a novel distribution-augmented contrastive learning that extends training distributions via data augmentation to obstruct the uniformity of contrastive representations. In experiments, we demonstrate state-of-the-art performance on visual domain one-class classification benchmarks, including novelty and anomaly detection. Finally, we present visual explanations, confirming that the decision-making process of deep one-class classifiers is intuitive to humans. The code is available at https://github.com/google-research/deep_representation_one_class",
    "checked": true,
    "id": "498e003901f8287e89e5064477cd22dd47e49d61",
    "semantic_title": "learning and evaluating representations for deep one-class classification",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=v8b3e5jN66j": {
    "title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations",
    "volume": "poster",
    "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection",
    "checked": true,
    "id": "3a1dea12746651884fa1c4349aa71327d789ae6b",
    "semantic_title": "conditional negative sampling for contrastive learning of visual representations",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=qpsl2dR9twy": {
    "title": "Communication in Multi-Agent Reinforcement Learning: Intention Sharing",
    "volume": "poster",
    "abstract": "Communication is one of the core components for learning coordinated behavior in multi-agent systems. In this paper, we propose a new communication scheme named Intention Sharing (IS) for multi-agent reinforcement learning in order to enhance the coordination among agents. In the proposed IS scheme, each agent generates an imagined trajectory by modeling the environment dynamics and other agents' actions. The imagined trajectory is the simulated future trajectory of each agent based on the learned model of the environment dynamics and other agents and represents each agent's future action plan. Each agent compresses this imagined trajectory capturing its future action plan to generate its intention message for communication by applying an attention mechanism to learn the relative importance of the components in the imagined trajectory based on the received message from other agents. Numeral results show that the proposed IS scheme outperforms other communication schemes in multi-agent reinforcement learning",
    "checked": true,
    "id": "9ed9ff62749f450247d1f6be9d1c14142fa63f05",
    "semantic_title": "communication in multi-agent reinforcement learning: intention sharing",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=tkAtoZkcUnm": {
    "title": "Neural Thompson Sampling",
    "volume": "poster",
    "abstract": "Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $O(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory",
    "checked": true,
    "id": "139f3e4cd5d8aa70104be62cbfd7d73690d681c1",
    "semantic_title": "neural thompson sampling",
    "citation_count": 125,
    "authors": []
  },
  "https://openreview.net/forum?id=7R7fAoUygoa": {
    "title": "Optimal Regularization can Mitigate Double Descent",
    "volume": "poster",
    "abstract": "Recent empirical and theoretical studies have shown that many learning algorithms -- from linear regression to neural networks -- can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as \"double descent\", has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned $\\ell_2$ regularization achieves monotonic test performance as we grow either the sample size or the model size. We also demonstrate empirically that optimally-tuned $\\ell_2$ regularization can mitigate double descent for more general models, including neural networks. Our results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization",
    "checked": true,
    "id": "4df60eaad8933ae16eb8744fe2cc7229fbe4879a",
    "semantic_title": "optimal regularization can mitigate double descent",
    "citation_count": 134,
    "authors": []
  },
  "https://openreview.net/forum?id=8HhkbjrWLdE": {
    "title": "Separation and Concentration in Deep Networks",
    "volume": "poster",
    "abstract": "Numerical experiments demonstrate that deep neural network classifiers progressively separate class distributions around their mean, achieving linear separability on the training set, and increasing the Fisher discriminant ratio. We explain this mechanism with two types of operators. We prove that a rectifier without biases applied to sign-invariant tight frames can separate class means and increase Fisher ratios. On the opposite, a soft-thresholding on tight frames can reduce within-class variabilities while preserving class means. Variance reduction bounds are proved for Gaussian mixture models. For image classification, we show that separation of class means can be achieved with rectified wavelet tight frames that are not learned. It defines a scattering transform. Learning $1 \\times 1$ convolutional tight frames along scattering channels and applying a soft-thresholding reduces within-class variabilities. The resulting scattering network reaches the classification accuracy of ResNet-18 on CIFAR-10 and ImageNet, with fewer layers and no learned biases",
    "checked": true,
    "id": "d13d7c36fa3e49ff70fef052756563f024a87f1b",
    "semantic_title": "separation and concentration in deep networks",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=rgFNuJHHXv": {
    "title": "Group Equivariant Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "Recent improvements in generative adversarial visual synthesis incorporate real and fake image transformation in a self-supervised setting, leading to increased stability and perceptual fidelity. However, these approaches typically involve image augmentations via additional regularizers in the GAN objective and thus spend valuable network capacity towards approximating transformation equivariance instead of their desired task. In this work, we explicitly incorporate inductive symmetry priors into the network architectures via group-equivariant convolutional networks. Group-convolutions have higher expressive power with fewer samples and lead to better gradient feedback between generator and discriminator. We show that group-equivariance integrates seamlessly with recent techniques for GAN training across regularizers, architectures, and loss functions. We demonstrate the utility of our methods for conditional synthesis by improving generation in the limited data regime across symmetric imaging datasets and even find benefits for natural images with preferred orientation",
    "checked": true,
    "id": "67b3930ff102f7f4055eee61b01c94e371532c77",
    "semantic_title": "group equivariant generative adversarial networks",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=hpH98mK5Puk": {
    "title": "InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective",
    "volume": "poster",
    "abstract": "Large-scale language models such as BERT have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing the threats of textual adversarial attacks. We aim to address this problem from an information-theoretic perspective, and propose InfoBERT, a novel learning framework for robust ﬁne-tuning of pre-trained language models. InfoBERT contains two mutual-information-based regularizers for model training: (i) an Information Bottleneck regularizer, which suppresses noisy mutual information between the input and the feature representation; and (ii) a Robust Feature regularizer, which increases the mutual information between local robust features and global features. We provide a principled way to theoretically analyze and improve the robustness of representation learning for language models in both standard and adversarial training. Extensive experiments demonstrate that InfoBERT achieves state-of-the-art robust accuracy over several adversarial datasets on Natural Language Inference (NLI) and Question Answering (QA) tasks. Our code is available at https://github.com/AI-secure/InfoBERT",
    "checked": true,
    "id": "a79c6fd3da1c3eafc228a0e429846ff048027689",
    "semantic_title": "infobert: improving robustness of language models from an information theoretic perspective",
    "citation_count": 120,
    "authors": []
  },
  "https://openreview.net/forum?id=sjuuTm4vj0": {
    "title": "Using latent space regression to analyze and leverage compositionality in GANs",
    "volume": "poster",
    "abstract": "In recent years, Generative Adversarial Networks have become ubiquitous in both research and public perception, but how GANs convert an unstructured latent code to a high quality output is still an open question. In this work, we investigate regression into the latent space as a probe to understand the compositional properties of GANs. We find that combining the regressor and a pretrained generator provides a strong image prior, allowing us to create composite images from a collage of random image parts at inference time while maintaining global consistency. To compare compositional properties across different generators, we measure the trade-offs between reconstruction of the unrealistic input and image quality of the regenerated samples. We find that the regression approach enables more localized editing of individual image parts compared to direct editing in the latent space, and we conduct experiments to quantify this independence effect. Our method is agnostic to the semantics of edits, and does not require labels or predefined concepts during training. Beyond image composition, our method extends to a number of related applications, such as image inpainting or example-based image editing, which we demonstrate on several GANs and datasets, and because it uses only a single forward pass, it can operate in real-time. Code is available on our project page: https://chail.github.io/latent-composition/",
    "checked": true,
    "id": "16beac3066280e8895f8fa4b9ff66203e1a2f629",
    "semantic_title": "using latent space regression to analyze and leverage compositionality in gans",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=giit4HdDNa": {
    "title": "Go with the flow: Adaptive control for Neural ODEs",
    "volume": "poster",
    "abstract": "Despite their elegant formulation and lightweight memory cost, neural ordinary differential equations (NODEs) suffer from known representational limitations. In particular, the single flow learned by NODEs cannot express all homeomorphisms from a given data space to itself, and their static weight parameterization restricts the type of functions they can learn compared to discrete architectures with layer-dependent weights. Here, we describe a new module called neurally-controlled ODE (N-CODE) designed to improve the expressivity of NODEs. The parameters of N-CODE modules are dynamic variables governed by a trainable map from initial or current activation state, resulting in forms of open-loop and closed-loop control, respectively. A single module is sufficient for learning a distribution on non-autonomous flows that adaptively drive neural representations. We provide theoretical and empirical evidence that N-CODE circumvents limitations of previous NODEs models and show how increased model expressivity manifests in several supervised and unsupervised learning problems. These favorable empirical results indicate the potential of using data- and activity-dependent plasticity in neural networks across numerous domains",
    "checked": true,
    "id": "dccf736fe0884ebac4f9ad187cb8249b3122efe2",
    "semantic_title": "go with the flow: adaptive control for neural odes",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=7aL-OtQrBWD": {
    "title": "A Learning Theoretic Perspective on Local Explainability",
    "volume": "poster",
    "abstract": "In this paper, we explore connections between interpretable machine learning and learning theory through the lens of local approximation explanations. First, we tackle the traditional problem of performance generalization and bound the test-time predictive accuracy of a model using a notion of how locally explainable it is. Second, we explore the novel problem of explanation generalization which is an important concern for a growing class of finite sample-based local approximation explanations. Finally, we validate our theoretical results empirically and show that they reflect what can be seen in practice",
    "checked": true,
    "id": "1df7fa194f3cc72ba6fb9541fe413defbaf4cfb4",
    "semantic_title": "a learning theoretic perspective on local explainability",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=XI-OJ5yyse": {
    "title": "CopulaGNN: Towards Integrating Representational and Correlational Roles of Graphs in Graph Neural Networks",
    "volume": "poster",
    "abstract": "Graph-structured data are ubiquitous. However, graphs encode diverse types of information and thus play different roles in data representation. In this paper, we distinguish the \\textit{representational} and the \\textit{correlational} roles played by the graphs in node-level prediction tasks, and we investigate how Graph Neural Network (GNN) models can effectively leverage both types of information. Conceptually, the representational information provides guidance for the model to construct better node features; while the correlational information indicates the correlation between node outcomes conditional on node features. Through a simulation study, we find that many popular GNN models are incapable of effectively utilizing the correlational information. By leveraging the idea of the copula, a principled way to describe the dependence among multivariate random variables, we offer a general solution. The proposed Copula Graph Neural Network (CopulaGNN) can take a wide range of GNN models as base models and utilize both representational and correlational information stored in the graphs. Experimental results on two types of regression tasks verify the effectiveness of the proposed method",
    "checked": true,
    "id": "7e2cffd6341939869a3e48c78e06c738d18f7d60",
    "semantic_title": "copulagnn: towards integrating representational and correlational roles of graphs in graph neural networks",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=jh-rTtvkGeM": {
    "title": "Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability",
    "volume": "poster",
    "abstract": "We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum eigenvalue of the training loss Hessian hovers just above the value $2 / \\text{(step size)}$, and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training. We hope that our findings will inspire future efforts aimed at rigorously understanding optimization at the Edge of Stability",
    "checked": true,
    "id": "026bb8a1066f50ddc8797e1341353603149a8cb8",
    "semantic_title": "gradient descent on neural networks typically occurs at the edge of stability",
    "citation_count": 293,
    "authors": []
  },
  "https://openreview.net/forum?id=SlrqM9_lyju": {
    "title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly",
    "volume": "poster",
    "abstract": "The learning rate (LR) schedule is one of the most important hyper-parameters needing careful tuning in training DNNs. However, it is also one of the least automated parts of machine learning systems and usually costs significant manual effort and computing. Though there are pre-defined LR schedules and optimizers with adaptive LR, they introduce new hyperparameters that need to be tuned separately for different tasks/datasets. In this paper, we consider the question: Can we automatically tune the LR over the course of training without human involvement? We propose an efficient method, AutoLRS, which automatically optimizes the LR for each training stage by modeling training dynamics. AutoLRS aims to find an LR that minimizes the validation loss, every $\\tau$ steps. We formulate it as black-box optimization and solve it by Bayesian optimization (BO). However, collecting training instances for BO requires a system to evaluate each LR queried by BO's acquisition function for $\\tau$ steps, which is prohibitively expensive in practice. Instead, we apply each candidate LR for only $\\tau'\\ll\\tau$ steps and train an exponential model to predict the validation loss after $\\tau$ steps. This mutual-training process between BO and the exponential model allows us to bound the number of training steps invested in the BO search. We demonstrate the advantages and the generality of AutoLRS through extensive experiments of training DNNs from diverse domains and using different optimizers. The LR schedules auto-generated by AutoLRS leads to a speedup of $1.22\\times$, $1.43\\times$, and $1.5\\times$ when training ResNet-50, Transformer, and BERT, respectively, compared to the LR schedules in their original papers, and an average speedup of $1.31\\times$ over state-of-the-art highly tuned LR schedules",
    "checked": true,
    "id": "76488b0743c9553b7b1d7ec46afe107ea60a67ca",
    "semantic_title": "autolrs: automatic learning-rate schedule by bayesian optimization on the fly",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=kmqjgSNXby": {
    "title": "Autoregressive Dynamics Models for Offline Policy Evaluation and Optimization",
    "volume": "poster",
    "abstract": "Standard dynamics models for continuous control make use of feedforward computation to predict the conditional distribution of next state and reward given current state and action using a multivariate Gaussian with a diagonal covariance structure. This modeling choice assumes that different dimensions of the next state and reward are conditionally independent given the current state and action and may be driven by the fact that fully observable physics-based simulation environments entail deterministic transition dynamics. In this paper, we challenge this conditional independence assumption and propose a family of expressive autoregressive dynamics models that generate different dimensions of the next state and reward sequentially conditioned on previous dimensions. We demonstrate that autoregressive dynamics models indeed outperform standard feedforward models in log-likelihood on heldout transitions. Furthermore, we compare different model-based and model-free off-policy evaluation (OPE) methods on RL Unplugged, a suite of offline MuJoCo datasets, and find that autoregressive dynamics models consistently outperform all baselines, achieving a new state-of-the-art. Finally, we show that autoregressive dynamics models are useful for offline policy optimization by serving as a way to enrich the replay buffer through data augmentation and improving performance using model-based planning",
    "checked": true,
    "id": "7e687699b6c2e075091cebe4b7b8dbd4dc3a7406",
    "semantic_title": "autoregressive dynamics models for offline policy evaluation and optimization",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=IrM64DGB21": {
    "title": "On the role of planning in model-based deep reinforcement learning",
    "volume": "poster",
    "abstract": "Model-based planning is often thought to be necessary for deep, careful reasoning and generalization in artificial agents. While recent successes of model-based reinforcement learning (MBRL) with deep function approximation have strengthened this hypothesis, the resulting diversity of model-based methods has also made it difficult to track which components drive success and why. In this paper, we seek to disentangle the contributions of recent methods by focusing on three questions: (1) How does planning benefit MBRL agents? (2) Within planning, what choices drive performance? (3) To what extent does planning improve generalization? To answer these questions, we study the performance of MuZero (Schrittwieser et al., 2019), a state-of-the-art MBRL algorithm with strong connections and overlapping components with many other MBRL algorithms. We perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. Our results suggest the following: (1) Planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks. (3) Planning alone is insufficient to drive strong generalization. These results indicate where and how to utilize planning in reinforcement learning settings, and highlight a number of open questions for future MBRL research",
    "checked": true,
    "id": "8b5bc6b5e42b2ea39e65cb14c06d8a819a03a496",
    "semantic_title": "on the role of planning in model-based deep reinforcement learning",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=0cmMMy8J5q": {
    "title": "Zero-Cost Proxies for Lightweight NAS",
    "volume": "poster",
    "abstract": "Neural Architecture Search (NAS) is quickly becoming the standard methodology to design neural network models. However, NAS is typically compute-intensive because multiple models need to be evaluated before choosing the best one. To reduce the computational power and time needed, a proxy task is often used for evaluating each model instead of full training. In this paper, we evaluate conventional reduced-training proxies and quantify how well they preserve ranking between neural network models during search when compared with the rankings produced by final trained accuracy. We propose a series of zero-cost proxies, based on recent pruning literature, that use just a single minibatch of training data to compute a model's score. Our zero-cost proxies use 3 orders of magnitude less computation but can match and even outperform conventional proxies. For example, Spearman's rank correlation coefficient between final validation accuracy and our best zero-cost proxy on NAS-Bench-201 is 0.82, compared to 0.61 for EcoNAS (a recently proposed reduced-training proxy). Finally, we use these zero-cost proxies to enhance existing NAS search algorithms such as random search, reinforcement learning, evolutionary search and predictor-based search. For all search methodologies and across three different NAS datasets, we are able to significantly improve sample efficiency, and thereby decrease computation, by using our zero-cost proxies. For example on NAS-Bench-101, we achieved the same accuracy 4$\\times$ quicker than the best previous result. Our code is made public at: https://github.com/mohsaied/zero-cost-nas",
    "checked": true,
    "id": "6dc84d38f6d8d964456b127d6f45c5b4de73bb86",
    "semantic_title": "zero-cost proxies for lightweight nas",
    "citation_count": 264,
    "authors": []
  },
  "https://openreview.net/forum?id=ehJqJQk9cw": {
    "title": "Personalized Federated Learning with First Order Model Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "30dba214afa23aa38340d9035ebfdbd77a135411",
    "semantic_title": "personalized federated learning with first order model optimization",
    "citation_count": 314,
    "authors": []
  },
  "https://openreview.net/forum?id=d-XzF81Wg1": {
    "title": "Deconstructing the Regularization of BatchNorm",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5b4aae4cbeae599ecc5c91e7ef58a758389f695d",
    "semantic_title": "deconstructing the regularization of batchnorm",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=193sEnKY1ij": {
    "title": "No Cost Likelihood Manipulation at Test Time for Making Better Mistakes in Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "776321e72310f9a8fd37e9cb8ad27935366e08ba",
    "semantic_title": "no cost likelihood manipulation at test time for making better mistakes in deep networks",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=l0mSUROpwY": {
    "title": "Intrinsic-Extrinsic Convolution and Pooling for Learning on 3D Protein Structures",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "18939e167782868f9d5c63e1c7908c1bf70eb284",
    "semantic_title": "intrinsic-extrinsic convolution and pooling for learning on 3d protein structures",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=45uOPa46Kh": {
    "title": "Generative Language-Grounded Policy in Vision-and-Language Navigation with Bayes' Rule",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3435e193998ec4118f51bbb608a843b0e123661b",
    "semantic_title": "generative language-grounded policy in vision-and-language navigation with bayes' rule",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=90JprVrJBO": {
    "title": "Learning a Latent Search Space for Routing Problems using Variational Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "168526097c6c1e8a2010ce9d2ec434319a5fb948",
    "semantic_title": "learning a latent search space for routing problems using variational autoencoders",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=0oabwyZbOu": {
    "title": "Mastering Atari with Discrete World Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b44bb1762640ed72091fd5f5fdc20719a6dc24af",
    "semantic_title": "mastering atari with discrete world models",
    "citation_count": 905,
    "authors": []
  },
  "https://openreview.net/forum?id=I4c4K9vBNny": {
    "title": "Spatial Dependency Networks: Neural Layers for Improved Generative Image Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0019f027ef77749d95fa2751185046302dc6b264",
    "semantic_title": "spatial dependency networks: neural layers for improved generative image modeling",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=uR9LaO_QxF": {
    "title": "Efficient Transformers in Reinforcement Learning using Actor-Learner Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cd37fee4da0d4483322d6fa3cc67af9ed8c07be6",
    "semantic_title": "efficient transformers in reinforcement learning using actor-learner distillation",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=ipUPfYxWZvM": {
    "title": "IOT: Instance-wise Layer Reordering for Transformer Structures",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f77d9e0b14598df68d6cfb64c5dd70916ee29aea",
    "semantic_title": "iot: instance-wise layer reordering for transformer structures",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=GFsU8a0sGB": {
    "title": "Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "59477007095a68ba551a588df756fb9873a14b72",
    "semantic_title": "federated learning via posterior averaging: a new perspective and practical algorithms",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=Nc3TJqbcl3": {
    "title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7d2ea736f9827cc8652dbaf5eafcbd5c6c7af82b",
    "semantic_title": "extracting strong policies for robotics tasks from zero-order trajectory optimizers",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=mQPBmvyAuk": {
    "title": "BREEDS: Benchmarks for Subpopulation Shift",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "767c6702045f2290012a259744db9edb4d55bcb8",
    "semantic_title": "breeds: benchmarks for subpopulation shift",
    "citation_count": 176,
    "authors": []
  },
  "https://openreview.net/forum?id=NQbnPjPYaG6": {
    "title": "On the Impossibility of Global Convergence in Multi-Loss Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "71065d3de6d962596915d3b91d981d7871a3fc68",
    "semantic_title": "on the impossibility of global convergence in multi-loss optimization",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=OHgnfSrn2jv": {
    "title": "Efficient Wasserstein Natural Gradients for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6e1ee4042e627e17128ff38adc550c305e539a85",
    "semantic_title": "efficient wasserstein natural gradients for reinforcement learning",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=C70cp4Cn32": {
    "title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b97d07c6dc6132dbcbddf203d4a3f2d2a803f414",
    "semantic_title": "multi-level local sgd for heterogeneous hierarchical networks",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=QoWatN-b8T": {
    "title": "Kanerva++: Extending the Kanerva Machine With Differentiable, Locally Block Allocated Latent Memory",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c04dca87cc9330f57a1de3a3cc7c0d2085910772",
    "semantic_title": "kanerva++: extending the kanerva machine with differentiable, locally block allocated latent memory",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=hsFN92eQEla": {
    "title": "EVALUATION OF NEURAL ARCHITECTURES TRAINED WITH SQUARE LOSS VS CROSS-ENTROPY IN CLASSIFICATION TASKS",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bcc48c5f68a387c89c75cd80d52ef52284db3c3a",
    "semantic_title": "evaluation of neural architectures trained with square loss vs cross-entropy in classification tasks",
    "citation_count": 174,
    "authors": []
  },
  "https://openreview.net/forum?id=-bdp_8Itjwp": {
    "title": "Self-supervised Learning from a Multi-view Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ce88a95ec5f226b66cfce6770fcecbd1dc5748b3",
    "semantic_title": "self-supervised learning from a multi-view perspective",
    "citation_count": 192,
    "authors": []
  },
  "https://openreview.net/forum?id=j9Rv7qdXjd": {
    "title": "Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2764af5084331e91e386d498ed53a2f4ce0eb354",
    "semantic_title": "interpretable neural architecture search via bayesian optimisation with weisfeiler-lehman kernels",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=UoaQUQREMOs": {
    "title": "CT-Net: Channel Tensorization Network for Video Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "780bd9eaa032be199ae5e6b7c239e5a4ae8f42ed",
    "semantic_title": "ct-net: channel tensorization network for video classification",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=tqOvYpjPax2": {
    "title": "Intraclass clustering: an implicit learning ability that regularizes DNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4b4df14b4b4b7cd1dd69093cc9cbccfe35a709ec",
    "semantic_title": "intraclass clustering: an implicit learning ability that regularizes dnns",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=fmOOI2a3tQP": {
    "title": "Learning Robust State Abstractions for Hidden-Parameter Block MDPs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9835d0d85faa36d2c27bea806487b988935e92a2",
    "semantic_title": "learning robust state abstractions for hidden-parameter block mdps",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=FX0vR39SJ5q": {
    "title": "Isometric Transformation Invariant and Equivariant Graph Convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a66aef5f4d60aa6faee9011d8fc42af722fb94d3",
    "semantic_title": "isometric transformation invariant and equivariant graph convolutional networks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=P6_q1BRxY8Q": {
    "title": "Learning Safe Multi-agent Control with Decentralized Neural Barrier Certificates",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "133dd31096fb36803e65001ba106767b2be65bd0",
    "semantic_title": "learning safe multi-agent control with decentralized neural barrier certificates",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=xCxXwTzx4L1": {
    "title": "ChipNet: Budget-Aware Pruning with Heaviside Continuous Approximations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1e11c7a586298625113f1bb0cdff505fe767a3a0",
    "semantic_title": "chipnet: budget-aware pruning with heaviside continuous approximations",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=Drynvt7gg4L": {
    "title": "AdaSpeech: Adaptive Text to Speech for Custom Voice",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4c055698c68b21c4e8f25a5c1b3439f9f7f3be62",
    "semantic_title": "adaspeech: adaptive text to speech for custom voice",
    "citation_count": 196,
    "authors": []
  },
  "https://openreview.net/forum?id=YLewtnvKgR7": {
    "title": "Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bc03b69ade305dada3024e580a387de57b75a894",
    "semantic_title": "estimating and evaluating regression predictive uncertainty in deep object detectors",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=QubpWYfdNry": {
    "title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "42a37d42369c08f15f55a57e522d0a177da20ab7",
    "semantic_title": "domain-robust visual imitation learning with mutual information constraints",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=e12NDM7wkEY": {
    "title": "Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5085d0386c77c0e25335b8f169e609509e302b85",
    "semantic_title": "clustering-friendly representation learning via instance discrimination and feature decorrelation",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=9GsFOUyUPi": {
    "title": "Progressive Skeletonization: Trimming more fat from a network at initialization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0b5c5571ab9dd88ebf010e8dc6898d8078d8e877",
    "semantic_title": "progressive skeletonization: trimming more fat from a network at initialization",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=8Sqhl-nF50": {
    "title": "On the Curse of Memory in Recurrent Neural Networks: Approximation and Optimization Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d4d20bcbb002f575a6f8fd9db9a2822b23f3a0bd",
    "semantic_title": "on the curse of memory in recurrent neural networks: approximation and optimization analysis",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=7aogOj_VYO0": {
    "title": "Do not Let Privacy Overbill Utility: Gradient Embedding Perturbation for Private Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ee500b621cc59f76b3396c6c5f136be8e9c44726",
    "semantic_title": "do not let privacy overbill utility: gradient embedding perturbation for private learning",
    "citation_count": 117,
    "authors": []
  },
  "https://openreview.net/forum?id=z5Z023VBmDZ": {
    "title": "More or Less: When and How to Build Convolutional Neural Network Ensembles",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "46d4c96a2952ede68b8907de5aad1f3c9f5f48df",
    "semantic_title": "more or less: when and how to build convolutional neural network ensembles",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=3RLN4EPMdYd": {
    "title": "Revisiting Hierarchical Approach for Persistent Long-Term Video Prediction",
    "volume": "poster",
    "abstract": "Learning to predict the long-term future of video frames is notoriously challenging due to the inherent ambiguities in a distant future and dramatic amplification of prediction error over time. Despite the recent advances in the literature, existing approaches are limited to moderately short-term prediction (less than a few seconds), while extrapolating it to a longer future quickly leads to destruction in structure and content. In this work, we revisit the hierarchical models in video prediction. Our method generates future frames by first estimating a sequence of dense semantic structures and subsequently translating the estimated structures to pixels by video-to-video translation model. Despite the simplicity, we show that modeling structures and their dynamics in categorical structure space with stochastic sequential estimator leads to surprisingly successful long-term prediction. We evaluate our method on two challenging video prediction scenarios, \\emph{car driving} and \\emph{human dancing}, and demonstrate that it can generate complicated scene structures and motions over a very long time horizon (\\ie~thousands frames), setting a new standard of video prediction with orders of magnitude longer prediction time than existing approaches. Video results are available at https://1konny.github.io/HVP/",
    "checked": true,
    "id": "8e9f455548d6f5088647c504cbe675a78971622c",
    "semantic_title": "revisiting hierarchical approach for persistent long-term video prediction",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=jEYKjPE1xYN": {
    "title": "Symmetry-Aware Actor-Critic for 3D Molecular Design",
    "volume": "poster",
    "abstract": "Automating molecular design using deep reinforcement learning (RL) has the potential to greatly accelerate the search for novel materials. Despite recent progress on leveraging graph representations to design molecules, such methods are fundamentally limited by the lack of three-dimensional (3D) information. In light of this, we propose a novel actor-critic architecture for 3D molecular design that can generate molecular structures unattainable with previous approaches. This is achieved by exploiting the symmetries of the design process through a rotationally covariant state-action representation based on a spherical harmonics series expansion. We demonstrate the benefits of our approach on several 3D molecular design tasks, where we find that building in such symmetries significantly improves generalization and the quality of generated molecules",
    "checked": true,
    "id": "d11ac1bbfb9148ff177a9443b3700695fb5995e1",
    "semantic_title": "symmetry-aware actor-critic for 3d molecular design",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=vQzcqQWIS0q": {
    "title": "Learnable Embedding sizes for Recommender Systems",
    "volume": "poster",
    "abstract": "The embedding-based representation learning is commonly used in deep learning recommendation models to map the raw sparse features to dense vectors. The traditional embedding manner that assigns a uniform size to all features has two issues. First, the numerous features inevitably lead to a gigantic embedding table that causes a high memory usage cost. Second, it is likely to cause the over-fitting problem for those features that do not require too large representation capacity. Existing works that try to address the problem always cause a significant drop in recommendation performance or suffers from the limitation of unaffordable training time cost. In this paper, we proposed a novel approach, named PEP (short for Plug-in Embedding Pruning), to reduce the size of the embedding table while avoiding the drop of recommendation accuracy. PEP prunes embedding parameter where the pruning threshold(s) can be adaptively learned from data. Therefore we can automatically obtain a mixed-dimension embedding-scheme by pruning redundant parameters for each feature. PEP is a general framework that can plug in various base recommendation models. Extensive experiments demonstrate it can efficiently cut down embedding parameters and boost the base model's performance. Specifically, it achieves strong recommendation performance while reducing 97-99% parameters. As for the computation cost, PEP only brings an additional 20-30% time cost compare with base models",
    "checked": true,
    "id": "7a7e23b1973c6555958a3e4bcf1bcc96b9065c31",
    "semantic_title": "learnable embedding sizes for recommender systems",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=pAbm1qfheGk": {
    "title": "Learning Neural Generative Dynamics for Molecular Conformation Generation",
    "volume": "poster",
    "abstract": "We study how to generate molecule conformations (i.e., 3D structures) from a molecular graph. Traditional methods, such as molecular dynamics, sample conformations via computationally expensive simulations. Recently, machine learning methods have shown great potential by training on a large collection of conformation data. Challenges arise from the limited model capacity for capturing complex distributions of conformations and the difficulty in modeling long-range dependencies between atoms. Inspired by the recent progress in deep generative models, in this paper, we propose a novel probabilistic framework to generate valid and diverse conformations given a molecular graph. We propose a method combining the advantages of both flow-based and energy-based models, enjoying: (1) a high model capacity to estimate the multimodal conformation distribution; (2) explicitly capturing the complex long-range dependencies between atoms in the observation space. Extensive experiments demonstrate the superior performance of the proposed method on several benchmarks, including conformation generation and distance modeling tasks, with a significant improvement over existing generative models for molecular conformation sampling",
    "checked": true,
    "id": "bf8483f233ab19f600de940a1bfce6cc323cb91d",
    "semantic_title": "learning neural generative dynamics for molecular conformation generation",
    "citation_count": 120,
    "authors": []
  },
  "https://openreview.net/forum?id=NjF772F4ZZR": {
    "title": "Learning the Pareto Front with Hypernetworks",
    "volume": "poster",
    "abstract": "Multi-objective optimization (MOO) problems are prevalent in machine learning. These problems have a set of optimal solutions, called the Pareto front, where each point on the front represents a different trade-off between possibly conflicting objectives. Recent MOO methods can target a specific desired ray in loss space however, most approaches still face two grave limitations: (i) A separate model has to be trained for each point on the front; and (ii) The exact trade-off must be known before the optimization process. Here, we tackle the problem of learning the entire Pareto front, with the capability of selecting a desired operating point on the front after training. We call this new setup Pareto-Front Learning (PFL). We describe an approach to PFL implemented using HyperNetworks, which we term Pareto HyperNetworks (PHNs). PHN learns the entire Pareto front simultaneously using a single hypernetwork, which receives as input a desired preference vector and returns a Pareto-optimal model whose loss vector is in the desired ray. The unified model is runtime efficient compared to training multiple models and generalizes to new operating points not used during training. We evaluate our method on a wide set of problems, from multi-task regression and classification to fairness. PHNs learn the entire Pareto front at roughly the same time as learning a single point on the front and at the same time reach a better solution set. PFL opens the door to new applications where models are selected based on preferences that are only available at run time",
    "checked": true,
    "id": "d96711ced185b92a011f3e51191497e8ae4b8559",
    "semantic_title": "learning the pareto front with hypernetworks",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=Y87Ri-GNHYu": {
    "title": "Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning",
    "volume": "poster",
    "abstract": "Complex, multi-task problems have proven to be difficult to solve efficiently in a sparse-reward reinforcement learning setting. In order to be sample efficient, multi-task learning requires reuse and sharing of low-level policies. To facilitate the automatic decomposition of hierarchical tasks, we propose the use of step-by-step human demonstrations in the form of natural language instructions and action trajectories. We introduce a dataset of such demonstrations in a crafting-based grid world. Our model consists of a high-level language generator and low-level policy, conditioned on language. We find that human demonstrations help solve the most complex tasks. We also find that incorporating natural language allows the model to generalize to unseen tasks in a zero-shot setting and to learn quickly from a few demonstrations. Generalization is not only reflected in the actions of the agent, but also in the generated natural language instructions in unseen tasks. Our approach also gives our trained agent interpretable behaviors because it is able to generate a sequence of high-level descriptions of its actions",
    "checked": true,
    "id": "0d6a4e45acde6f47d704ed0752f17f7ab52223af",
    "semantic_title": "ask your humans: using human instructions to improve generalization in reinforcement learning",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=ZW0yXJyNmoG": {
    "title": "Taming GANs with Lookahead-Minmax",
    "volume": "poster",
    "abstract": "Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead–minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead–minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources",
    "checked": true,
    "id": "0e50e8ff4bd85f6dfe1708010ed0e56348f60dfe",
    "semantic_title": "taming gans with lookahead-minmax",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=1Jv6b0Zq3qi": {
    "title": "Uncertainty in Gradient Boosting via Ensembles",
    "volume": "poster",
    "abstract": "For many practical, high-risk applications, it is essential to quantify uncertainty in a model's predictions to avoid costly mistakes. While predictive uncertainty is widely studied for neural networks, the topic seems to be under-explored for models based on gradient boosting. However, gradient boosting often achieves state-of-the-art results on tabular data. This work examines a probabilistic ensemble-based framework for deriving uncertainty estimates in the predictions of gradient boosting classification and regression models. We conducted experiments on a range of synthetic and real datasets and investigated the applicability of ensemble approaches to gradient boosting models that are themselves ensembles of decision trees. Our analysis shows that ensembles of gradient boosting models successfully detect anomalous inputs while having limited ability to improve the predicted total uncertainty. Importantly, we also propose a concept of a virtual ensemble to get the benefits of an ensemble via only one gradient boosting model, which significantly reduces complexity",
    "checked": true,
    "id": "4528b9f6eb57cb7f180060150877616c956a63fd",
    "semantic_title": "uncertainty in gradient boosting via ensembles",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=eLfqMl3z3lq": {
    "title": "Adversarial score matching and improved sampling for image generation",
    "volume": "poster",
    "abstract": "Denoising Score Matching with Annealed Langevin Sampling (DSM-ALS) has recently found success in generative modeling. The approach works by first training a neural network to estimate the score of a distribution, and then using Langevin dynamics to sample from the data distribution assumed by the score network. Despite the convincing visual quality of samples, this method appears to perform worse than Generative Adversarial Networks (GANs) under the Fréchet Inception Distance, a standard metric for generative models. We show that this apparent gap vanishes when denoising the final Langevin samples using the score network. In addition, we propose two improvements to DSM-ALS: 1) Consistent Annealed Sampling as a more stable alternative to Annealed Langevin Sampling, and 2) a hybrid training formulation, composed of both Denoising Score Matching and adversarial objectives. By combining these two techniques and exploring different network architectures, we elevate score matching methods and obtain results competitive with state-of-the-art image generation on CIFAR-10",
    "checked": true,
    "id": "22c3badd79d4ee60892705b34c59807a6e828850",
    "semantic_title": "adversarial score matching and improved sampling for image generation",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=piLPYqxtWuA": {
    "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech",
    "volume": "poster",
    "abstract": "Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/",
    "checked": true,
    "id": "1623d6ffb6efd94d21537db2b96b91a196842aef",
    "semantic_title": "fastspeech 2: fast and high-quality end-to-end text to speech",
    "citation_count": 1451,
    "authors": []
  },
  "https://openreview.net/forum?id=DEa4JdMWRHp": {
    "title": "Interpretable Models for Granger Causality Using Self-explaining Neural Networks",
    "volume": "poster",
    "abstract": "Exploratory analysis of time series data can yield a better understanding of complex dynamical systems. Granger causality is a practical framework for analysing interactions in sequential data, applied in a wide range of domains. In this paper, we propose a novel framework for inferring multivariate Granger causality under nonlinear dynamics based on an extension of self-explaining neural networks. This framework is more interpretable than other neural-network-based techniques for inferring Granger causality, since in addition to relational inference, it also allows detecting signs of Granger-causal effects and inspecting their variability over time. In comprehensive experiments on simulated data, we show that our framework performs on par with several powerful baseline methods at inferring Granger causality and that it achieves better performance at inferring interaction signs. The results suggest that our framework is a viable and more interpretable alternative to sparse-input neural networks for inferring Granger causality",
    "checked": true,
    "id": "732fe890e5fa6e7378fb936aaa3192f2d98d4af2",
    "semantic_title": "interpretable models for granger causality using self-explaining neural networks",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=sy4Kg_ZQmS7": {
    "title": "Learning Deep Features in Instrumental Variable Regression",
    "volume": "poster",
    "abstract": "Instrumental variable (IV) regression is a standard strategy for learning causal relationships between confounded treatment and outcome variables from observational data by using an instrumental variable, which affects the outcome only through the treatment. In classical IV regression, learning proceeds in two stages: stage 1 performs linear regression from the instrument to the treatment; and stage 2 performs linear regression from the treatment to the outcome, conditioned on the instrument. We propose a novel method, deep feature instrumental variable regression (DFIV), to address the case where relations between instruments, treatments, and outcomes may be nonlinear. In this case, deep neural nets are trained to define informative nonlinear features on the instruments and treatments. We propose an alternating training regime for these features to ensure good end-to-end performance when composing stages 1 and 2, thus obtaining highly flexible feature maps in a computationally efficient manner. DFIV outperforms recent state-of-the-art methods on challenging IV benchmarks, including settings involving high dimensional image data. DFIV also exhibits competitive performance in off-policy policy evaluation for reinforcement learning, which can be understood as an IV regression task",
    "checked": true,
    "id": "bbf19a76e51cf5c2d2ed853b48f470554bf4ee39",
    "semantic_title": "learning deep features in instrumental variable regression",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=z9k8BWL-_2u": {
    "title": "Statistical inference for individual fairness",
    "volume": "poster",
    "abstract": "As we rely on machine learning (ML) models to make more consequential decisions, the issue of ML models perpetuating unwanted social biases has come to the fore of the public's and the research community's attention. In this paper, we focus on the problem of detecting violations of individual fairness in ML models. We formalize the problem as measuring the susceptibility of ML models against a form of adversarial attack and develop a suite of inference tools for the adversarial loss. The tools allow practitioners to assess the individual fairness of ML models in a statistically-principled way: form confidence intervals for the adversarial loss and test hypotheses of model fairness with (asymptotic) non-coverage/Type I error rate control. We demonstrate the utility of our tools in a real-world case study",
    "checked": true,
    "id": "de2eecf29162c9505d68bc04ab0529b20b2dcc5b",
    "semantic_title": "statistical inference for individual fairness",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=AY8zfZm0tDd": {
    "title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model",
    "volume": "poster",
    "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio $\\gg 1$; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio $\\gg 1$",
    "checked": true,
    "id": "736590f70e7f2dc464c1c62491cfa8adb4d718f3",
    "semantic_title": "randomized ensembled double q-learning: learning fast without a model",
    "citation_count": 291,
    "authors": []
  },
  "https://openreview.net/forum?id=ADWd4TJO13G": {
    "title": "Lifelong Learning of Compositional Structures",
    "volume": "poster",
    "abstract": "A hallmark of human intelligence is the ability to construct self-contained chunks of knowledge and adequately reuse them in novel combinations for solving different yet structurally related problems. Learning such compositional structures has been a significant challenge for artificial systems, due to the combinatorial nature of the underlying search problem. To date, research into compositional learning has largely proceeded separately from work on lifelong or continual learning. We integrate these two lines of work to present a general-purpose framework for lifelong learning of compositional structures that can be used for solving a stream of related tasks. Our framework separates the learning process into two broad stages: learning how to best combine existing components in order to assimilate a novel problem, and learning how to adapt the set of existing components to accommodate the new problem. This separation explicitly handles the trade-off between the stability required to remember how to solve earlier tasks and the flexibility required to solve new tasks, as we show empirically in an extensive evaluation",
    "checked": true,
    "id": "a714514a989a943d08f8ad25e307df2bf7150c6c",
    "semantic_title": "lifelong learning of compositional structures",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=Rcmk0xxIQV": {
    "title": "QPLEX: Duplex Dueling Multi-Agent Q-Learning",
    "volume": "poster",
    "abstract": "We explore value-based multi-agent reinforcement learning (MARL) in the popular paradigm of centralized training with decentralized execution (CTDE). CTDE has an important concept, Individual-Global-Max (IGM) principle, which requires the consistency between joint and local action selections to support efficient local decision-making. However, in order to achieve scalability, existing MARL methods either limit representation expressiveness of their value function classes or relax the IGM consistency, which may suffer from instability risk or may not perform well in complex domains. This paper presents a novel MARL approach, called duPLEX dueling multi-agent Q-learning (QPLEX), which takes a duplex dueling network architecture to factorize the joint value function. This duplex dueling structure encodes the IGM principle into the neural network architecture and thus enables efficient value function learning. Theoretical analysis shows that QPLEX achieves a complete IGM function class. Empirical experiments on StarCraft II micromanagement tasks demonstrate that QPLEX significantly outperforms state-of-the-art baselines in both online and offline data collection settings, and also reveal that QPLEX achieves high sample efficiency and can benefit from offline datasets without additional online exploration",
    "checked": true,
    "id": "052c100d45f949c06e8419b504e319b442cb3f0a",
    "semantic_title": "qplex: duplex dueling multi-agent q-learning",
    "citation_count": 470,
    "authors": []
  },
  "https://openreview.net/forum?id=--gvHfE3Xf5": {
    "title": "Meta-Learning of Structured Task Distributions in Humans and Machines",
    "volume": "poster",
    "abstract": "In recent years, meta-learning, in which a model is trained on a family of tasks (i.e. a task distribution), has emerged as an approach to training neural networks to perform tasks that were previously assumed to require structured representations, making strides toward closing the gap between humans and machines. However, we argue that evaluating meta-learning remains a challenge, and can miss whether meta-learning actually uses the structure embedded within the tasks. These meta-learners might therefore still be significantly different from humans learners. To demonstrate this difference, we first define a new meta-reinforcement learning task in which a structured task distribution is generated using a compositional grammar. We then introduce a novel approach to constructing a \"null task distribution\" with the same statistical complexity as this structured task distribution but without the explicit rule-based structure used to generate the structured task. We train a standard meta-learning agent, a recurrent network trained with model-free reinforcement learning, and compare it with human performance across the two task distributions. We find a double dissociation in which humans do better in the structured task distribution whereas agents do better in the null task distribution -- despite comparable statistical complexity. This work highlights that multiple strategies can achieve reasonable meta-test performance, and that careful construction of control task distributions is a valuable way to understand which strategies meta-learners acquire, and how they might differ from humans",
    "checked": true,
    "id": "15ad6f868acb05e836e88774aa2b71b0e082e5aa",
    "semantic_title": "meta-learning of structured task distributions in humans and machines",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=8X2eaSZxTP": {
    "title": "PC2WF: 3D Wireframe Reconstruction from Raw Point Clouds",
    "volume": "poster",
    "abstract": "We introduce PC2WF, the first end-to-end trainable deep network architecture to convert a 3D point cloud into a wireframe model. The network takes as input an unordered set of 3D points sampled from the surface of some object, and outputs a wireframe of that object, i.e., a sparse set of corner points linked by line segments. Recovering the wireframe is a challenging task, where the numbers of both vertices and edges are different for every instance, and a-priori unknown. Our architecture gradually builds up the model: It starts by encoding the points into feature vectors. Based on those features, it identifies a pool of candidate vertices, then prunes those candidates to a final set of corner vertices and refines their locations. Next, the corners are linked with an exhaustive set of candidate edges, which is again pruned to obtain the final wireframe. All steps are trainable, and errors can be backpropagated through the entire sequence. We validate the proposed model on a publicly available synthetic dataset, for which the ground truth wireframes are accessible, as well as on a new real-world dataset. Our model produces wireframe abstractions of good quality and outperforms several baselines",
    "checked": true,
    "id": "75f25a80d5ea871de855f09aa5d558a6fcc993d8",
    "semantic_title": "pc2wf: 3d wireframe reconstruction from raw point clouds",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=9l0K4OM-oXE": {
    "title": "Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks",
    "volume": "poster",
    "abstract": "Deep neural networks (DNNs) are known vulnerable to backdoor attacks, a training time attack that injects a trigger pattern into a small proportion of training data so as to control the model's prediction at the test time. Backdoor attacks are notably dangerous since they do not affect the model's performance on clean examples, yet can fool the model to make the incorrect prediction whenever the trigger pattern appears during testing. In this paper, we propose a novel defense framework Neural Attention Distillation (NAD) to erase backdoor triggers from backdoored DNNs. NAD utilizes a teacher network to guide the finetuning of the backdoored student network on a small clean subset of data such that the intermediate-layer attention of the student network aligns with that of the teacher network. The teacher network can be obtained by an independent finetuning process on the same clean subset. We empirically show, against 6 state-of-the-art backdoor attacks, NAD can effectively erase the backdoor triggers using only 5\\% clean training data without causing obvious performance degradation on clean examples. Our code is available at https://github.com/bboylyg/NAD",
    "checked": true,
    "id": "4d4e5c0c691e42b7078208598d585caacc2e34c2",
    "semantic_title": "neural attention distillation: erasing backdoor triggers from deep neural networks",
    "citation_count": 452,
    "authors": []
  },
  "https://openreview.net/forum?id=hiq1rHO8pNT": {
    "title": "HyperGrid Transformers: Towards A Single Model for Multiple Tasks",
    "volume": "poster",
    "abstract": "Achieving state-of-the-art performance on natural language understanding tasks typically relies on fine-tuning a fresh model for every task. Consequently, this approach leads to a higher overall parameter cost, along with higher technical maintenance for serving multiple models. Learning a single multi-task model that is able to do well for all the tasks has been a challenging and yet attractive proposition. In this paper, we propose HyperGrid Transformers, a new Transformer architecture that leverages task-conditioned hyper networks for controlling its feed-forward layers. Specifically, we propose a decomposable hypernetwork that learns grid-wise projections that help to specialize regions in weight matrices for different tasks. In order to construct the proposed hypernetwork, our method learns the interactions and composition between a global (task-agnostic) state and a local task-specific state. We conduct an extensive set of experiments on GLUE/SuperGLUE. On the SuperGLUE test set, we match the performance of the state-of-the-art while being $16$ times more parameter efficient. Our method helps bridge the gap between fine-tuning and multi-task learning approaches",
    "checked": true,
    "id": "942aec6e9dddcaa6700e14a2f6e1b77164a092cb",
    "semantic_title": "hypergrid transformers: towards a single model for multiple tasks",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=qVyeW-grC2k": {
    "title": "Long Range Arena : A Benchmark for Efficient Transformers",
    "volume": "poster",
    "abstract": "Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, Long Range Arena, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. Long Range Arena paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle",
    "checked": true,
    "id": "7e9ff94476f41041c75e253e84f487db00e9c861",
    "semantic_title": "long range arena: a benchmark for efficient transformers",
    "citation_count": 754,
    "authors": []
  },
  "https://openreview.net/forum?id=j1RMMKeP2gR": {
    "title": "Acting in Delayed Environments with Non-Stationary Markov Policies",
    "volume": "poster",
    "abstract": "The standard Markov Decision Process (MDP) formulation hinges on the assumption that an action is executed immediately after it was chosen. However, assuming it is often unrealistic and can lead to catastrophic failures in applications such as robotic manipulation, cloud computing, and finance. We introduce a framework for learning and planning in MDPs where the decision-maker commits actions that are executed with a delay of $m$ steps. The brute-force state augmentation baseline where the state is concatenated to the last $m$ committed actions suffers from an exponential complexity in $m$, as we show for policy iteration. We then prove that with execution delay, deterministic Markov policies in the original state-space are sufficient for attaining maximal reward, but need to be non-stationary. As for stationary Markov policies, we show they are sub-optimal in general. Consequently, we devise a non-stationary Q-learning style model-based algorithm that solves delayed execution tasks without resorting to state-augmentation. Experiments on tabular, physical, and Atari domains reveal that it converges quickly to high performance even for substantial delays, while standard approaches that either ignore the delay or rely on state-augmentation struggle or fail due to divergence. The code is available at \\url{https://github.com/galdl/rl_delay_basic.git}",
    "checked": true,
    "id": "8aa6248fa36de344c3cd6b3e19ae95c5bc3fa87a",
    "semantic_title": "acting in delayed environments with non-stationary markov policies",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=q_S44KLQ_Aa": {
    "title": "Neurally Augmented ALISTA",
    "volume": "poster",
    "abstract": "It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets. In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging",
    "checked": true,
    "id": "68e4b45d99ec0ffa075d540a91fc27385d8f6707",
    "semantic_title": "neurally augmented alista",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=vhKe9UFbrJo": {
    "title": "Relating by Contrasting: A Data-efficient Framework for Multimodal Generative Models",
    "volume": "poster",
    "abstract": "Multimodal learning for generative models often refers to the learning of abstract concepts from the commonality of information in multiple modalities, such as vision and language. While it has proven effective for learning generalisable representations, the training of such models often requires a large amount of related multimodal data that shares commonality, which can be expensive to come by. To mitigate this, we develop a novel contrastive framework for generative model learning, allowing us to train the model not just by the commonality between modalities, but by the distinction between \"related\" and \"unrelated\" multimodal data. We show in experiments that our method enables data-efficient multimodal learning on challenging datasets for various multimodal VAE models. We also show that under our proposed framework, the generative model can accurately identify related samples from unrelated ones, making it possible to make use of the plentiful unlabeled, unpaired multimodal data",
    "checked": true,
    "id": "f2f3c91fad164c8919699159e8eab511f03d0fe2",
    "semantic_title": "relating by contrasting: a data-efficient framework for multimodal generative models",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=-GLNZeVDuik": {
    "title": "Categorical Normalizing Flows via Continuous Transformations",
    "volume": "poster",
    "abstract": "Despite their popularity, to date, the application of normalizing flows on categorical data stays limited. The current practice of using dequantization to map discrete data to a continuous space is inapplicable as categorical data has no intrinsic order. Instead, categorical data have complex and latent relations that must be inferred, like the synonymy between words. In this paper, we investigate Categorical Normalizing Flows, that is normalizing flows for categorical data. By casting the encoding of categorical data in continuous space as a variational inference problem, we jointly optimize the continuous representation and the model likelihood. Using a factorized decoder, we introduce an inductive bias to model any interactions in the normalizing flow. As a consequence, we do not only simplify the optimization compared to having a joint decoder, but also make it possible to scale up to a large number of categories that is currently impossible with discrete normalizing flows. Based on Categorical Normalizing Flows, we propose GraphCNF a permutation-invariant generative model on graphs. GraphCNF implements a three step approach modeling the nodes, edges, and adjacency matrix stepwise to increase efficiency. On molecule generation, GraphCNF outperforms both one-shot and autoregressive flow-based state-of-the-art",
    "checked": true,
    "id": "6670ce9fa7dbc271612078f2b3f05c12e0281aeb",
    "semantic_title": "categorical normalizing flows via continuous transformations",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=aCgLmfhIy_f": {
    "title": "Prototypical Representation Learning for Relation Extraction",
    "volume": "poster",
    "abstract": "Recognizing relations between entities is a pivotal task of relational learning. Learning relation representations from distantly-labeled datasets is difficult because of the abundant label noise and complicated expressions in human language. This paper aims to learn predictive, interpretable, and robust relation representations from distantly-labeled data that are effective in different settings, including supervised, distantly supervised, and few-shot learning. Instead of solely relying on the supervision from noisy labels, we propose to learn prototypes for each relation from contextual information to best explore the intrinsic semantics of relations. Prototypes are representations in the feature space abstracting the essential semantics of relations between entities in sentences. We learn prototypes based on objectives with clear geometric interpretation, where the prototypes are unit vectors uniformly dispersed in a unit ball, and statement embeddings are centered at the end of their corresponding prototype vectors on the surface of the ball. This approach allows us to learn meaningful, interpretable prototypes for the final classification. Results on several relation learning tasks show that our model significantly outperforms the previous state-of-the-art models. We further demonstrate the robustness of the encoder and the interpretability of prototypes with extensive experiments",
    "checked": true,
    "id": "5d8e85d12c218f56f3c50351fa2ddc3da098b31c",
    "semantic_title": "prototypical representation learning for relation extraction",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=0PtUPB9z6qK": {
    "title": "Generalized Energy Based Models",
    "volume": "poster",
    "abstract": "We introduce the Generalized Energy Based Model (GEBM) for generative modelling. These models combine two trained components: a base distribution (generally an implicit model), which can learn the support of data with low intrinsic dimension in a high dimensional space; and an energy function, to refine the probability mass on the learned support. Both the energy function and base jointly constitute the final model, unlike GANs, which retain only the base distribution (the \"generator\"). GEBMs are trained by alternating between learning the energy and the base. We show that both training stages are well-defined: the energy is learned by maximising a generalized likelihood, and the resulting energy-based loss provides informative gradients for learning the base. Samples from the posterior on the latent space of the trained model can be obtained via MCMC, thus finding regions in this space that produce better quality samples. Empirically, the GEBM samples on image-generation tasks are of much better quality than those from the learned generator alone, indicating that all else being equal, the GEBM will outperform a GAN of the same complexity. When using normalizing flows as base measures, GEBMs succeed on density modelling tasks returning comparable performance to direct maximum likelihood of the same networks",
    "checked": true,
    "id": "131d51ce9a3f32266a7e85b2f32a659edc649745",
    "semantic_title": "generalized energy based models",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=Y9McSeEaqUh": {
    "title": "Predicting Classification Accuracy When Adding New Unobserved Classes",
    "volume": "poster",
    "abstract": "Multiclass classifiers are often designed and evaluated only on a sample from the classes on which they will eventually be applied. Hence, their final accuracy remains unknown. In this work we study how a classifier's performance over the initial class sample can be used to extrapolate its expected accuracy on a larger, unobserved set of classes. For this, we define a measure of separation between correct and incorrect classes that is independent of the number of classes: the \"reversed ROC\" (rROC), which is obtained by replacing the roles of classes and data-points in the common ROC. We show that the classification accuracy is a function of the rROC in multiclass classifiers, for which the learned representation of data from the initial class sample remains unchanged when new classes are added. Using these results we formulate a robust neural-network-based algorithm, \"CleaneX\", which learns to estimate the accuracy of such classifiers on arbitrarily large sets of classes. Unlike previous methods, our method uses both the observed accuracies of the classifier and densities of classification scores, and therefore achieves remarkably better predictions than current state-of-the-art methods on both simulations and real datasets of object detection, face recognition, and brain decoding",
    "checked": true,
    "id": "33391f5117b1e93548b0f297de1e36ee4a61157a",
    "semantic_title": "predicting classification accuracy when adding new unobserved classes",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=lQdXeXDoWtI": {
    "title": "In Search of Lost Domain Generalization",
    "volume": "poster",
    "abstract": "The goal of domain generalization algorithms is to predict well on distributions different from those seen during training. While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions---datasets, network architectures, and model selection criteria---render fair comparisons difficult. The goal of this paper is to understand how useful domain generalization algorithms are in realistic settings. As a first step, we realize that model selection is non-trivial for domain generalization tasks, and we argue that algorithms without a model selection criterion remain incomplete. Next we implement DomainBed, a testbed for domain generalization including seven benchmarks, fourteen algorithms, and three model selection criteria. When conducting extensive experiments using DomainBed we find that when carefully implemented and tuned, ERM outperforms the state-of-the-art in terms of average performance. Furthermore, no algorithm included in DomainBed outperforms ERM by more than one point when evaluated under the same experimental conditions. We hope that the release of DomainBed, alongside contributions from fellow researchers, will streamline reproducible and rigorous advances in domain generalization",
    "checked": true,
    "id": "6a5efb990b6558c21d9fdded4884c00ba152cb7c",
    "semantic_title": "in search of lost domain generalization",
    "citation_count": 1187,
    "authors": []
  },
  "https://openreview.net/forum?id=kEnBH98BGs5": {
    "title": "Estimating informativeness of samples with Smooth Unique Information",
    "volume": "poster",
    "abstract": "We define a notion of information that an individual sample provides to the training of a neural network, and we specialize it to measure both how much a sample informs the final weights and how much it informs the function computed by the weights. Though related, we show that these quantities have a qualitatively different behavior. We give efficient approximations of these quantities using a linearized network and demonstrate empirically that the approximation is accurate for real-world architectures, such as pre-trained ResNets. We apply these measures to several problems, such as dataset summarization, analysis of under-sampled classes, comparison of informativeness of different data sources, and detection of adversarial and corrupted examples. Our work generalizes existing frameworks, but enjoys better computational properties for heavily over-parametrized models, which makes it possible to apply it to real-world networks",
    "checked": true,
    "id": "1b53e8652676a1806d0d2abc5d7abcd212709046",
    "semantic_title": "estimating informativeness of samples with smooth unique information",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=45NZvF1UHam": {
    "title": "Identifying Physical Law of Hamiltonian Systems via Meta-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "87174a4df7567e52771d15f949d459791145379b",
    "semantic_title": "identifying physical law of hamiltonian systems via meta-learning",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=dyjPVUc2KB": {
    "title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "616ef21345e4063c2f1227fc64225e7e65a66473",
    "semantic_title": "adapting to reward progressivity via spectral reinforcement learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=rsogjAnYs4z": {
    "title": "Understanding the effects of data parallelism and sparsity on neural network training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9713b351df45932a6c496c283e1bf5f664c87c69",
    "semantic_title": "understanding the effects of data parallelism and sparsity on neural network training",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=TtYSU29zgR": {
    "title": "Primal Wasserstein Imitation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "25287b593d2642b1627b96a79c5ff8d3c8ec1f5c",
    "semantic_title": "primal wasserstein imitation learning",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=Ptaz_zIFbX": {
    "title": "Prediction and generalisation over directed actions by grid cells",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "085a8cd9324c12da35d66e6164de3e250683eb68",
    "semantic_title": "prediction and generalisation over directed actions by grid cells",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=1rxHOBjeDUW": {
    "title": "Drop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "05ad9fc4ba9fad5aade4b700e8505d37eefd0519",
    "semantic_title": "drop-bottleneck: learning discrete compressed representation for noise-robust exploration",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=umIdUL8rMH": {
    "title": "BOIL: Towards Representation Change for Few-shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a925908789343e12b9ec62e2b4f7681c248866c4",
    "semantic_title": "boil: towards representation change for few-shot learning",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=ee6W5UgQLa": {
    "title": "MultiModalQA: complex question answering over text, tables and images",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d365978adf0a5c9c6028820857e015617856256b",
    "semantic_title": "multimodalqa: complex question answering over text, tables and images",
    "citation_count": 167,
    "authors": []
  },
  "https://openreview.net/forum?id=EoFNy62JGd": {
    "title": "Neural gradients are near-lognormal: improved quantized and sparse training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9df893d2b8ac20271c0317a72e97cc9720954f0b",
    "semantic_title": "neural gradients are near-lognormal: improved quantized and sparse training",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=e8W-hsu_q5": {
    "title": "Group Equivariant Conditional Neural Processes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bc716f31b93eac93f627e04039da7f6c4d126165",
    "semantic_title": "group equivariant conditional neural processes",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=3hGNqpI4WS": {
    "title": "Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "79ebde314ab90d066cee3b82193ef05666323394",
    "semantic_title": "deployment-efficient reinforcement learning via model-based offline optimization",
    "citation_count": 151,
    "authors": []
  },
  "https://openreview.net/forum?id=tnSo6VRLmT": {
    "title": "Efficient Conformal Prediction via Cascaded Inference with Expanded Admission",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5c5683527a738c3051eaa8dc02322632fea4aa5c",
    "semantic_title": "efficient conformal prediction via cascaded inference with expanded admission",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=sTeoJiB4uR": {
    "title": "Reducing the Computational Cost of Deep Generative Models with Binary Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0daf387e612363a3746778326aaaa49467b5e9d6",
    "semantic_title": "reducing the computational cost of deep generative models with binary neural networks",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=0aW6lYOYB7d": {
    "title": "Large-width functional asymptotics for deep Gaussian neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e7174887d624d6b2533061861bba18815f314cf0",
    "semantic_title": "large-width functional asymptotics for deep gaussian neural networks",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=9z_dNsC4B5t": {
    "title": "MetaNorm: Learning to Normalize Few-Shot Batches Across Domains",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e3db1323a61cf02daa93d936bd6c02426df82b07",
    "semantic_title": "metanorm: learning to normalize few-shot batches across domains",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=QpNz8r_Ri2Y": {
    "title": "Representation Balancing Offline Model-based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "61ce91bd0a925bb24c4d236fd14a5f27c3f5b43f",
    "semantic_title": "representation balancing offline model-based reinforcement learning",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=AWOSz_mMAPx": {
    "title": "Local Convergence Analysis of Gradient Descent Ascent with Finite Timescale Separation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f4465442a9b850a2c5b71a63fff0d24396b15f2c",
    "semantic_title": "local convergence analysis of gradient descent ascent with finite timescale separation",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=E3Ys6a1NTGT": {
    "title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "86fa0f350b0ede3a86e31aa2900af551531ee570",
    "semantic_title": "the importance of pessimism in fixed-dataset policy optimization",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=9EKHN1jOlA": {
    "title": "Uncertainty Estimation and Calibration with Finite-State Probabilistic RNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a83e36e203a4e90c2af814320092fc8f78f4ce28",
    "semantic_title": "uncertainty estimation and calibration with finite-state probabilistic rnns",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=jrA5GAccy_": {
    "title": "Empirical or Invariant Risk Minimization? A Sample Complexity Perspective",
    "volume": "poster",
    "abstract": "Recently, invariant risk minimization (IRM) was proposed as a promising solution to address out-of-distribution (OOD) generalization. However, it is unclear when IRM should be preferred over the widely-employed empirical risk minimization (ERM) framework. In this work, we analyze both these frameworks from the perspective of sample complexity, thus taking a firm step towards answering this important question. We find that depending on the type of data generation mechanism, the two approaches might have very different finite sample and asymptotic behavior. For example, in the covariate shift setting we see that the two approaches not only arrive at the same asymptotic solution, but also have similar finite sample behavior with no clear winner. For other distribution shifts such as those involving confounders or anti-causal variables, however, the two approaches arrive at different asymptotic solutions where IRM is guaranteed to be close to the desired OOD solutions in the finite sample regime, while ERM is biased even asymptotically. We further investigate how different factors --- the number of environments, complexity of the model, and IRM penalty weight --- impact the sample complexity of IRM in relation to its distance from the OOD solutions",
    "checked": true,
    "id": "fbf50ed8e09b3268770029af30b01d31973f77d0",
    "semantic_title": "empirical or invariant risk minimization? a sample complexity perspective",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=zeFrfgyZln": {
    "title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval",
    "volume": "poster",
    "abstract": "Conducting text retrieval in a learned dense representation space has many intriguing advantages. Yet dense retrieval (DR) often underperforms word-based sparse retrieval. In this paper, we first theoretically show the bottleneck of dense retrieval is the domination of uninformative negatives sampled in mini-batch training, which yield diminishing gradient norms, large gradient variances, and slow convergence. We then propose Approximate nearest neighbor Negative Contrastive Learning (ANCE), which selects hard training negatives globally from the entire corpus. Our experiments demonstrate the effectiveness of ANCE on web search, question answering, and in a commercial search engine, showing ANCE dot-product retrieval nearly matches the accuracy of BERT-based cascade IR pipeline. We also empirically validate our theory that negative sampling with ANCE better approximates the oracle importance sampling procedure and improves learning convergence",
    "checked": true,
    "id": "c9b8593db099869fe7254aa1fa53f3c9073b0176",
    "semantic_title": "approximate nearest neighbor negative contrastive learning for dense text retrieval",
    "citation_count": 1268,
    "authors": []
  },
  "https://openreview.net/forum?id=YNnpaAKeCfx": {
    "title": "FairBatch: Batch Selection for Model Fairness",
    "volume": "poster",
    "abstract": "Training a fair machine learning model is essential to prevent demographic disparity. Existing techniques for improving model fairness require broad changes in either data preprocessing or model training, rendering themselves difficult-to-adopt for potentially already complex machine learning systems. We address this problem via the lens of bilevel optimization. While keeping the standard training algorithm as an inner optimizer, we incorporate an outer optimizer so as to equip the inner problem with an additional functionality: Adaptively selecting minibatch sizes for the purpose of improving model fairness. Our batch selection algorithm, which we call FairBatch, implements this optimization and supports prominent fairness measures: equal opportunity, equalized odds, and demographic parity. FairBatch comes with a significant implementation benefit -- it does not require any modification to data preprocessing or model training. For instance, a single-line change of PyTorch code for replacing batch selection part of model training suffices to employ FairBatch. Our experiments conducted both on synthetic and benchmark real data demonstrate that FairBatch can provide such functionalities while achieving comparable (or even greater) performances against the state of the arts. Furthermore, FairBatch can readily improve fairness of any pre-trained model simply via fine-tuning. It is also compatible with existing batch selection techniques intended for different purposes, such as faster convergence, thus gracefully achieving multiple purposes",
    "checked": true,
    "id": "d688594d2aac080f657b7be251e89cca6a7df165",
    "semantic_title": "fairbatch: batch selection for model fairness",
    "citation_count": 134,
    "authors": []
  },
  "https://openreview.net/forum?id=tIjRAiFmU3y": {
    "title": "An Unsupervised Deep Learning Approach for Real-World Image Denoising",
    "volume": "poster",
    "abstract": "Designing an unsupervised image denoising approach in practical applications is a challenging task due to the complicated data acquisition process. In the real-world case, the noise distribution is so complex that the simplified additive white Gaussian (AWGN) assumption rarely holds, which significantly deteriorates the Gaussian denoisers' performance. To address this problem, we apply a deep neural network that maps the noisy image into a latent space in which the AWGN assumption holds, and thus any existing Gaussian denoiser is applicable. More specifically, the proposed neural network consists of the encoder-decoder structure and approximates the likelihood term in the Bayesian framework. Together with a Gaussian denoiser, the neural network can be trained with the input image itself and does not require any pre-training in other datasets. Extensive experiments on real-world noisy image datasets have shown that the combination of neural networks and Gaussian denoisers improves the performance of the original Gaussian denoisers by a large margin. In particular, the neural network+BM3D method significantly outperforms other unsupervised denoising approaches and is competitive with supervised networks such as DnCNN, FFDNet, and CBDNet",
    "checked": true,
    "id": "51da598da3fc475884299ff498e224abd333a87b",
    "semantic_title": "an unsupervised deep learning approach for real-world image denoising",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=WesiCoRVQ15": {
    "title": "When Optimizing f -Divergence is Robust with Label Noise",
    "volume": "poster",
    "abstract": "We show when maximizing a properly defined $f$-divergence measure with respect to a classifier's predictions and the supervised labels is robust with label noise. Leveraging its variational form, we derive a nice decoupling property for a family of $f$-divergence measures when label noise presents, where the divergence is shown to be a linear combination of the variational difference defined on the clean distribution and a bias term introduced due to the noise. The above derivation helps us analyze the robustness of different $f$-divergence functions. With established robustness, this family of $f$-divergence functions arises as useful metrics for the problem of learning with noisy labels, which do not require the specification of the labels' noise rate. When they are possibly not robust, we propose fixes to make them so. In addition to the analytical results, we present thorough experimental evidence. Our code is available at https://github.com/UCSC-REAL/Robust-f-divergence-measures",
    "checked": true,
    "id": "0abff557bf1c38a1fbbbeaa01dea66cd1ac5e988",
    "semantic_title": "when optimizing f-divergence is robust with label noise",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=-QxT4mJdijq": {
    "title": "Meta-learning Symmetries by Reparameterization",
    "volume": "poster",
    "abstract": "Many successful deep learning architectures are equivariant to certain transformations in order to conserve parameters and improve generalization: most famously, convolution layers are equivariant to shifts of the input. This approach only works when practitioners know the symmetries of the task and can manually construct an architecture with the corresponding equivariances. Our goal is an approach for learning equivariances from data, without needing to design custom task-specific architectures. We present a method for learning and encoding equivariances into networks by learning corresponding parameter sharing patterns from data. Our method can provably represent equivariance-inducing parameter sharing for any finite group of symmetry transformations. Our experiments suggest that it can automatically learn to encode equivariances to common transformations used in image processing tasks",
    "checked": true,
    "id": "2e072998dd7b40e9e514b2bb43c8074bd5aa43d2",
    "semantic_title": "meta-learning symmetries by reparameterization",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=GH7QRzUDdXG": {
    "title": "A Geometric Analysis of Deep Generative Image Models and Its Applications",
    "volume": "poster",
    "abstract": "Generative adversarial networks (GANs) have emerged as a powerful unsupervised method to model the statistical patterns of real-world data sets, such as natural images. These networks are trained to map random inputs in their latent space to new samples representative of the learned data. However, the structure of the latent space is hard to intuit due to its high dimensionality and the non-linearity of the generator, which limits the usefulness of the models. Understanding the latent space requires a way to identify input codes for existing real-world images (inversion), and a way to identify directions with known image transformations (interpretability). Here, we use a geometric framework to address both issues simultaneously. We develop an architecture-agnostic method to compute the Riemannian metric of the image manifold created by GANs. The eigen-decomposition of the metric isolates axes that account for different levels of image variability. An empirical analysis of several pretrained GANs shows that image variation around each position is concentrated along surprisingly few major axes (the space is highly anisotropic) and the directions that create this large variation are similar at different positions in the space (the space is homogeneous). We show that many of the top eigenvectors correspond to interpretable transforms in the image space, with a substantial part of eigenspace corresponding to minor transforms which could be compressed out. This geometric understanding unifies key previous results related to GAN interpretability. We show that the use of this metric allows for more efficient optimization in the latent space (e.g. GAN inversion) and facilitates unsupervised discovery of interpretable axes. Our results illustrate that defining the geometry of the GAN image manifold can serve as a general framework for understanding GANs",
    "checked": true,
    "id": "c4c57c766bdfab05f3fe5b135073cb188e7fcf63",
    "semantic_title": "a geometric analysis of deep generative image models and its applications",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=tC6iW2UUbJf": {
    "title": "What Makes Instance Discrimination Good for Transfer Learning?",
    "volume": "poster",
    "abstract": "Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation. It comes as a surprise that image annotations would be better left unused for transfer learning. In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models? From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations. Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category",
    "checked": true,
    "id": "199d88fb9ec7430ca653f4c066b02aa7c3b4dd98",
    "semantic_title": "what makes instance discrimination good for transfer learning?",
    "citation_count": 170,
    "authors": []
  },
  "https://openreview.net/forum?id=EMHoBG0avc1": {
    "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval",
    "volume": "poster",
    "abstract": "We propose a simple and efficient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previous work, our method does not require access to any corpus-specific information, such as inter-document hyperlinks or human-annotated entity markers, and can be applied to any unstructured text corpus. Our system also yields a much better efficiency-accuracy trade-off, matching the best published accuracy on HotpotQA while being 10 times faster at inference time",
    "checked": true,
    "id": "3684491d62db5c3e5602375271e4b339bbf416ee",
    "semantic_title": "answering complex open-domain questions with multi-hop dense retrieval",
    "citation_count": 197,
    "authors": []
  },
  "https://openreview.net/forum?id=xgGS6PmzNq6": {
    "title": "On Dyadic Fairness: Exploring and Mitigating Bias in Graph Connections",
    "volume": "poster",
    "abstract": "Disparate impact has raised serious concerns in machine learning applications and its societal impacts. In response to the need of mitigating discrimination, fairness has been regarded as a crucial property in algorithmic design. In this work, we study the problem of disparate impact on graph-structured data. Specifically, we focus on dyadic fairness, which articulates a fairness concept that a predictive relationship between two instances should be independent of the sensitive attributes. Based on this, we theoretically relate the graph connections to dyadic fairness on link predictive scores in learning graph neural networks, and reveal that regulating weights on existing edges in a graph contributes to dyadic fairness conditionally. Subsequently, we propose our algorithm, \\textbf{FairAdj}, to empirically learn a fair adjacency matrix with proper graph structural constraints for fair link prediction, and in the meanwhile preserve predictive accuracy as much as possible. Empirical validation demonstrates that our method delivers effective dyadic fairness in terms of various statistics, and at the same time enjoys a favorable fairness-utility tradeoff",
    "checked": true,
    "id": "4742859e6e27fe5eda18dbe14752ed6a4871d356",
    "semantic_title": "on dyadic fairness: exploring and mitigating bias in graph connections",
    "citation_count": 122,
    "authors": []
  },
  "https://openreview.net/forum?id=CF-ZIuSMXRz": {
    "title": "Spatio-Temporal Graph Scattering Transform",
    "volume": "poster",
    "abstract": "Although spatio-temporal graph neural networks have achieved great empirical success in handling multiple correlated time series, they may be impractical in some real-world scenarios due to a lack of sufficient high-quality training data. Furthermore, spatio-temporal graph neural networks lack theoretical interpretation. To address these issues, we put forth a novel mathematically designed framework to analyze spatio-temporal data. Our proposed spatio-temporal graph scattering transform (ST-GST) extends traditional scattering transform to the spatio-temporal domain. It performs iterative applications of spatio-temporal graph wavelets and nonlinear activation functions, which can be viewed as a forward pass of spatio-temporal graph convolutional networks without training. Since all the filter coefficients in ST-GST are mathematically designed, it is promising for the real-world scenarios with limited training data, and also allows for a theoretical analysis, which shows that the proposed ST-GST is stable to small perturbations of input signals and structures. Finally, our experiments show that i) ST-GST outperforms spatio-temporal graph convolutional networks by an increase of 35% in accuracy for MSR Action3D dataset; ii) it is better and computationally more efficient to design the transform based on separable spatio-temporal graphs than the joint ones; and iii) nonlinearity in ST-GST is critical to empirical performance",
    "checked": true,
    "id": "29d22a4939e3497d5a9b6a3e1c47584854a46000",
    "semantic_title": "spatio-temporal graph scattering transform",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=XjYgR6gbCEc": {
    "title": "MODALS: Modality-agnostic Automated Data Augmentation in the Latent Space",
    "volume": "poster",
    "abstract": "Data augmentation is an efficient way to expand a training dataset by creating additional artificial data. While data augmentation is found to be effective in improving the generalization capabilities of models for various machine learning tasks, the underlying augmentation methods are usually manually designed and carefully evaluated for each data modality separately, like image processing functions for image data and word-replacing rules for text data. In this work, we propose an automated data augmentation approach called MODALS (Modality-agnostic Automated Data Augmentation in the Latent Space) to augment data for any modality in a generic way. MODALS exploits automated data augmentation to fine-tune four universal data transformation operations in the latent space to adapt the transform to data of different modalities. Through comprehensive experiments, we demonstrate the effectiveness of MODALS on multiple datasets for text, tabular, time-series and image modalities",
    "checked": true,
    "id": "d1b11e6c58e4a66ecb4f0dfd20a21b4099c4ede5",
    "semantic_title": "modals: modality-agnostic automated data augmentation in the latent space",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=0IOX0YcCdTn": {
    "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
    "volume": "poster",
    "abstract": "Given a simple request like Put a washed apple in the kitchen fridge, humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text-based policies in TextWorld (Côté et al., 2018) and then execute goals from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, and visual scene understanding)",
    "checked": true,
    "id": "398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
    "semantic_title": "alfworld: aligning text and embodied environments for interactive learning",
    "citation_count": 482,
    "authors": []
  },
  "https://openreview.net/forum?id=jXe91kq3jAq": {
    "title": "Latent Skill Planning for Exploration and Transfer",
    "volume": "poster",
    "abstract": "To quickly solve new tasks in complex environments, intelligent agents need to build up reusable knowledge. For example, a learned world model captures knowledge about the environment that applies to new tasks. Similarly, skills capture general behaviors that can apply to new tasks. In this paper, we investigate how these two approaches can be integrated into a single reinforcement learning agent. Specifically, we leverage the idea of partial amortization for fast adaptation at test time. For this, actions are produced by a policy that is learned over time while the skills it conditions on are chosen using online planning. We demonstrate the benefits of our design decisions across a suite of challenging locomotion tasks and demonstrate improved sample efficiency in single tasks as well as in transfer from one task to another, as compared to competitive baselines. Videos are available at: https://sites.google.com/view/latent-skill-planning/",
    "checked": true,
    "id": "22a8ab2f4cd0777ebc93d8e414535c03d4d57615",
    "semantic_title": "latent skill planning for exploration and transfer",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=djwS0m4Ft_A": {
    "title": "Evaluating the Disentanglement of Deep Generative Models through Manifold Topology",
    "volume": "poster",
    "abstract": "Learning disentangled representations is regarded as a fundamental task for improving the generalization, robustness, and interpretability of generative models. However, measuring disentanglement has been challenging and inconsistent, often dependent on an ad-hoc external model or specific to a certain dataset. To address this, we present a method for quantifying disentanglement that only uses the generative model, by measuring the topological similarity of conditional submanifolds in the learned representation. This method showcases both unsupervised and supervised variants. To illustrate the effectiveness and applicability of our method, we empirically evaluate several state-of-the-art models across multiple datasets. We find that our method ranks models similarly to existing methods. We make our code publicly available at https://github.com/stanfordmlgroup/disentanglement",
    "checked": true,
    "id": "6363dd8404cad824f293ecc7cf67fe89330733ad",
    "semantic_title": "evaluating the disentanglement of deep generative models through manifold topology",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=g11CZSghXyY": {
    "title": "Combining Ensembles and Data Augmentation Can Harm Your Calibration",
    "volume": "poster",
    "abstract": "Ensemble methods which average over multiple neural network predictions are a simple approach to improve a model's calibration and robustness. Similarly, data augmentation techniques, which encode prior information in the form of invariant feature transformations, are effective for improving calibration and robustness. In this paper, we show a surprising pathology: combining ensembles and data augmentation can harm model calibration. This leads to a trade-off in practice, whereby improved accuracy by combining the two techniques comes at the expense of calibration. On the other hand, selecting only one of the techniques ensures good uncertainty estimates at the expense of accuracy. We investigate this pathology and identify a compounding under-confidence among methods which marginalize over sets of weights and data augmentation techniques which soften labels. Finally, we propose a simple correction, achieving the best of both worlds with significant accuracy and calibration gains over using only ensembles or data augmentation individually. Applying the correction produces new state-of-the art in uncertainty calibration and robustness across CIFAR-10, CIFAR-100, and ImageNet",
    "checked": true,
    "id": "8bc63c8bd96b40d8d0f5329e06e0a96eaf214c7f",
    "semantic_title": "combining ensembles and data augmentation can harm your calibration",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=eom0IUrF__F": {
    "title": "CoCo: Controllable Counterfactuals for Evaluating Dialogue State Trackers",
    "volume": "poster",
    "abstract": "Dialogue state trackers have made significant progress on benchmark datasets, but their generalization capability to novel and realistic scenarios beyond the held- out conversations is less understood. We propose controllable counterfactuals (COCO) to bridge this gap and evaluate dialogue state tracking (DST) models on novel scenarios, i.e., would the system successfully tackle the request if the user responded differently but still consistently with the dialogue flow? COCO leverages turn-level belief states as counterfactual conditionals to produce novel conversation scenarios in two steps: (i) counterfactual goal generation at turn- level by dropping and adding slots followed by replacing slot values, (ii) counterfactual conversation generation that is conditioned on (i) and consistent with the dialogue flow. Evaluating state-of-the-art DST models on MultiWOZ dataset with COCO-generated counterfactuals results in a significant performance drop of up to 30.8% (from 49.4% to 18.6%) in absolute joint goal accuracy. In comparison, widely used techniques like paraphrasing only affect the accuracy by at most 2%. Human evaluations show that COCO-generated conversations perfectly reflect the underlying user goal with more than 95% accuracy and are as human-like as the original conversations, further strengthening its reliability and promise to be adopted as part of the robustness evaluation of DST models",
    "checked": true,
    "id": "16e4a6b20f1d8bff37fcfd5671e21a13b41d242f",
    "semantic_title": "coco: controllable counterfactuals for evaluating dialogue state trackers",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=QkRbdiiEjM": {
    "title": "AdaGCN: Adaboosting Graph Convolutional Networks into Deep Models",
    "volume": "poster",
    "abstract": "The design of deep graph models still remains to be investigated and the crucial part is how to explore and exploit the knowledge from different hops of neighbors in an efficient way. In this paper, we propose a novel RNN-like deep graph neural network architecture by incorporating AdaBoost into the computation of network; and the proposed graph convolutional network called AdaGCN~(Adaboosting Graph Convolutional Network) has the ability to efficiently extract knowledge from high-order neighbors of current nodes and then integrates knowledge from different hops of neighbors into the network in an Adaboost way. Different from other graph neural networks that directly stack many graph convolution layers, AdaGCN shares the same base neural network architecture among all ``layers'' and is recursively optimized, which is similar to an RNN. Besides, We also theoretically established the connection between AdaGCN and existing graph convolutional methods, presenting the benefits of our proposal. Finally, extensive experiments demonstrate the consistent state-of-the-art prediction performance on graphs across different label rates and the computational advantage of our approach AdaGCN~\\footnote{Code is available at \\url{https://github.com/datake/AdaGCN}.}",
    "checked": true,
    "id": "6c6c265c6d1de08f03fed0da0604bef5307fbcec",
    "semantic_title": "adagcn: adaboosting graph convolutional networks into deep models",
    "citation_count": 86,
    "authors": []
  },
  "https://openreview.net/forum?id=kBVJ2NtiY-": {
    "title": "Learning What To Do by Simulating the Past",
    "volume": "poster",
    "abstract": "Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state. Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a specific skill in MuJoCo environments given a single state sampled from the optimal policy for that skill",
    "checked": true,
    "id": "4a78cd72b184bec55e05db0e2df25d63765de068",
    "semantic_title": "learning what to do by simulating the past",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=PrzjugOsDeE": {
    "title": "CcGAN: Continuous Conditional Generative Adversarial Networks for Image Generation",
    "volume": "poster",
    "abstract": "This work proposes the continuous conditional generative adversarial network (CcGAN), the first generative model for image generation conditional on continuous, scalar conditions (termed regression labels). Existing conditional GANs (cGANs) are mainly designed for categorical conditions (e.g., class labels); conditioning on a continuous label is mathematically distinct and raises two fundamental problems: (P1) Since there may be very few (even zero) real images for some regression labels, minimizing existing empirical versions of cGAN losses (a.k.a. empirical cGAN losses) often fails in practice; (P2) Since regression labels are scalar and infinitely many, conventional label input methods (e.g., combining a hidden map of the generator/discriminator with a one-hot encoded label) are not applicable. The proposed CcGAN solves the above problems, respectively, by (S1) reformulating existing empirical cGAN losses to be appropriate for the continuous scenario; and (S2) proposing a novel method to incorporate regression labels into the generator and the discriminator. The reformulation in (S1) leads to two novel empirical discriminator losses, termed the hard vicinal discriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL) respectively, and a novel empirical generator loss. The error bounds of a discriminator trained with HVDL and SVDL are derived under mild assumptions in this work. A new benchmark dataset, RC-49, is also proposed for generative image modeling conditional on regression labels. Our experiments on the Circular 2-D Gaussians, RC-49, and UTKFace datasets show that CcGAN is able to generate diverse, high-quality samples from the image distribution conditional on a given regression label. Moreover, in these experiments, CcGAN substantially outperforms cGAN both visually and quantitatively",
    "checked": true,
    "id": "f70a9bcd4cc1dffdcb2611441db2a70047a750b1",
    "semantic_title": "ccgan: continuous conditional generative adversarial networks for image generation",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=7I12hXRi8F": {
    "title": "ANOCE: Analysis of Causal Effects with Multiple Mediators via Constrained Structural Learning",
    "volume": "poster",
    "abstract": "In the era of causal revolution, identifying the causal effect of an exposure on the outcome of interest is an important problem in many areas, such as epidemics, medicine, genetics, and economics. Under a general causal graph, the exposure may have a direct effect on the outcome and also an indirect effect regulated by a set of mediators. An analysis of causal effects that interprets the causal mechanism contributed through mediators is hence challenging but on demand. To the best of our knowledge, there are no feasible algorithms that give an exact decomposition of the indirect effect on the level of individual mediators, due to common interaction among mediators in the complex graph. In this paper, we establish a new statistical framework to comprehensively characterize causal effects with multiple mediators, namely, ANalysis Of Causal Effects (ANOCE), with a newly introduced definition of the mediator effect, under the linear structure equation model. We further propose a constrained causal structure learning method by incorporating a novel identification constraint that specifies the temporal causal relationship of variables. The proposed algorithm is applied to investigate the causal effects of 2020 Hubei lockdowns on reducing the spread of the coronavirus in Chinese major cities out of Hubei",
    "checked": true,
    "id": "d22037765e1aae7beaf6909562e29971fc240844",
    "semantic_title": "anoce: analysis of causal effects with multiple mediators via constrained structural learning",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=3X64RLgzY6O": {
    "title": "Direction Matters: On the Implicit Bias of Stochastic Gradient Descent with Moderate Learning Rate",
    "volume": "poster",
    "abstract": "Understanding the algorithmic bias of stochastic gradient descent (SGD) is one of the key challenges in modern machine learning and deep learning theory. Most of the existing works, however, focus on very small or even infinitesimal learning rate regime, and fail to cover practical scenarios where the learning rate is moderate and annealing. In this paper, we make an initial attempt to characterize the particular regularization effect of SGD in the moderate learning rate regime by studying its behavior for optimizing an overparameterized linear regression problem. In this case, SGD and GD are known to converge to the unique minimum-norm solution; however, with the moderate and annealing learning rate, we show that they exhibit different directional bias: SGD converges along the large eigenvalue directions of the data matrix, while GD goes after the small eigenvalue directions. Furthermore, we show that such directional bias does matter when early stopping is adopted, where the SGD output is nearly optimal but the GD output is suboptimal. Finally, our theory explains several folk arts in practice used for SGD hyperparameter tuning, such as (1) linearly scaling the initial learning rate with batch size; and (2) overrunning SGD with high learning rate even when the loss stops decreasing",
    "checked": true,
    "id": "89cd1a37012317393ad230ef3b93a286978649f0",
    "semantic_title": "direction matters: on the implicit regularization effect of stochastic gradient descent with moderate learning rate",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=Eql5b1_hTE4": {
    "title": "Robust early-learning: Hindering the memorization of noisy labels",
    "volume": "poster",
    "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods",
    "checked": true,
    "id": "bafb088c459188fd22fc20eb3af6b731d4856629",
    "semantic_title": "robust early-learning: hindering the memorization of noisy labels",
    "citation_count": 259,
    "authors": []
  },
  "https://openreview.net/forum?id=GMgHyUPrXa": {
    "title": "A Design Space Study for LISTA and Beyond",
    "volume": "poster",
    "abstract": "In recent years, great success has been witnessed in building problem-specific deep networks from unrolling iterative algorithms, for solving inverse problems and beyond. Unrolling is believed to incorporate the model-based prior with the learning capacity of deep learning. This paper revisits \\textit{the role of unrolling as a design approach for deep networks}: to what extent its resulting special architecture is superior, and can we find better? Using LISTA for sparse recovery as a representative example, we conduct the first thorough \\textit{design space study} for the unrolled models. Among all possible variations, we focus on extensively varying the connectivity patterns and neuron types, leading to a gigantic design space arising from LISTA. To efficiently explore this space and identify top performers, we leverage the emerging tool of neural architecture search (NAS). We carefully examine the searched top architectures in a number of settings, and are able to discover networks that consistently better than LISTA. We further present more visualization and analysis to ``open the black box\", and find that the searched top architectures demonstrate highly consistent and potentially transferable patterns. We hope our study to spark more reflections and explorations on how to better mingle model-based optimization prior and data-driven learning",
    "checked": true,
    "id": "fd5b0172edfbc55f8ca8e165aa1e9e29a541c6e2",
    "semantic_title": "a design space study for lista and beyond",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Qk-Wq5AIjpq": {
    "title": "PAC Confidence Predictions for Deep Neural Network Classifiers",
    "volume": "poster",
    "abstract": "A key challenge for deploying deep neural networks (DNNs) in safety critical settings is the need to provide rigorous ways to quantify their uncertainty. In this paper, we propose a novel algorithm for constructing predicted classification confidences for DNNs that comes with provable correctness guarantees. Our approach uses Clopper-Pearson confidence intervals for the Binomial distribution in conjunction with the histogram binning approach to calibrated prediction. In addition, we demonstrate how our predicted confidences can be used to enable downstream guarantees in two settings: (i) fast DNN inference, where we demonstrate how to compose a fast but inaccurate DNN with an accurate but slow DNN in a rigorous way to improve performance without sacrificing accuracy, and (ii) safe planning, where we guarantee safety when using a DNN to predict whether a given action is safe based on visual observations. In our experiments, we demonstrate that our approach can be used to provide guarantees for state-of-the-art DNNs",
    "checked": true,
    "id": "3c3d007b159a74a08f75c1190722790d4930ddee",
    "semantic_title": "pac confidence predictions for deep neural network classifiers",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=LiX3ECzDPHZ": {
    "title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback",
    "volume": "poster",
    "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze or neural activity measured by a brain implant. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, a large-scale observational study on handwriting samples from 60 users, and a pilot study with one participant using an electrocorticography-based brain-computer interface. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning",
    "checked": true,
    "id": "8c6dc25db6c2e5e90b03d0ecd328b74e3f4cd0c4",
    "semantic_title": "x2t: training an x-to-text typing interface with online learning from user feedback",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=WEHSlH5mOk": {
    "title": "Discrete Graph Structure Learning for Forecasting Multiple Time Series",
    "volume": "poster",
    "abstract": "Time series forecasting is an extensively studied subject in statistics, economics, and computer science. Exploration of the correlation and causation among the variables in a multivariate time series shows promise in enhancing the performance of a time series model. When using deep neural networks as forecasting models, we hypothesize that exploiting the pairwise information among multiple (multivariate) time series also improves their forecast. If an explicit graph structure is known, graph neural networks (GNNs) have been demonstrated as powerful tools to exploit the structure. In this work, we propose learning the structure simultaneously with the GNN if the graph is unknown. We cast the problem as learning a probabilistic graph model through optimizing the mean performance over the graph distribution. The distribution is parameterized by a neural network so that discrete graphs can be sampled differentiably through reparameterization. Empirical evaluations show that our method is simpler, more efficient, and better performing than a recently proposed bilevel learning approach for graph structure learning, as well as a broad array of forecasting models, either deep or non-deep learning based, and graph or non-graph based",
    "checked": true,
    "id": "401930b2dc738a5a67d136bc9a2d04461c5bf93a",
    "semantic_title": "discrete graph structure learning for forecasting multiple time series",
    "citation_count": 248,
    "authors": []
  },
  "https://openreview.net/forum?id=CGQ6ENUMX6": {
    "title": "Task-Agnostic Morphology Evolution",
    "volume": "poster",
    "abstract": "Deep reinforcement learning primarily focuses on learning behavior, usually overlooking the fact that an agent's function is largely determined by form. So, how should one go about finding a morphology fit for solving tasks in a given environment? Current approaches that co-adapt morphology and behavior use a specific task's reward as a signal for morphology optimization. However, this often requires expensive policy optimization and results in task-dependent morphologies that are not built to generalize. In this work, we propose a new approach, Task-Agnostic Morphology Evolution (TAME), to alleviate both of these issues. Without any task or reward specification, TAME evolves morphologies by only applying randomly sampled action primitives on a population of agents. This is accomplished using an information-theoretic objective that efficiently ranks agents by their ability to reach diverse states in the environment and the causality of their actions. Finally, we empirically demonstrate that across 2D, 3D, and manipulation environments TAME can evolve morphologies that match the multi-task performance of those learned with task supervised algorithms. Our code and videos can be found at https://sites.google.com/view/task-agnostic-evolution",
    "checked": true,
    "id": "f89af36631d45126faf2d23b81c4a767d4f91d56",
    "semantic_title": "task-agnostic morphology evolution",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=aDjoksTpXOP": {
    "title": "Deep Equals Shallow for ReLU Networks in Kernel Regimes",
    "volume": "poster",
    "abstract": "Deep networks are often considered to be more expressive than shallow ones in terms of approximation. Indeed, certain functions can be approximated by deep networks provably more efficiently than by shallow ones, however, no tractable algorithms are known for learning such deep models. Separately, a recent line of work has shown that deep networks trained with gradient descent may behave like (tractable) kernel methods in a certain over-parameterized regime, where the kernel is determined by the architecture and initialization, and this paper focuses on approximation for such kernels. We show that for ReLU activations, the kernels derived from deep fully-connected networks have essentially the same approximation properties as their shallow two-layer counterpart, namely the same eigenvalue decay for the corresponding integral operator. This highlights the limitations of the kernel framework for understanding the benefits of such deep architectures. Our main theoretical result relies on characterizing such eigenvalue decays through differentiability properties of the kernel function, which also easily applies to the study of other kernels defined on the sphere",
    "checked": true,
    "id": "959c999a48125ddcceba3cc1b0fa62bbbf89c745",
    "semantic_title": "deep equals shallow for relu networks in kernel regimes",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=5jzlpHvvRk": {
    "title": "Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search",
    "volume": "poster",
    "abstract": "Designing proper loss functions for vision tasks has been a long-standing research direction to advance the capability of existing models. For object detection, the well-established classification and regression loss functions have been carefully designed by considering diverse learning challenges (e.g. class imbalance, hard negative samples, and scale variances). Inspired by the recent progress in network architecture search, it is interesting to explore the possibility of discovering new loss function formulations via directly searching the primitive operation combinations. So that the learned losses not only fit for diverse object detection challenges to alleviate huge human efforts, but also have better alignment with evaluation metric and good mathematical convergence property. Beyond the previous auto-loss works on face recognition and image classification, our work makes the first attempt to discover new loss functions for the challenging object detection from primitive operation levels and finds the searched losses are insightful. We propose an effective convergence-simulation driven evolutionary search algorithm, called CSE-Autoloss, for speeding up the search progress by regularizing the mathematical rationality of loss candidates via two progressive convergence simulation modules: convergence property verification and model optimization simulation. CSE-Autoloss involves the search space (i.e. 21 mathematical operators, 3 constant-type inputs, and 3 variable-type inputs) that cover a wide range of the possible variants of existing losses and discovers best-searched loss function combination within a short time (around 1.5 wall-clock days with 20x speedup in comparison to the vanilla evolutionary algorithm). We conduct extensive evaluations of loss function search on popular detectors and validate the good generalization capability of searched losses across diverse architectures and various datasets. Our experiments show that the best-discovered loss function combinations outperform default combinations (Cross-entropy/Focal loss for classification and L1 loss for regression) by 1.1% and 0.8% in terms of mAP for two-stage and one-stage detectors on COCO respectively. Our searched losses are available at https://github.com/PerdonLiu/CSE-Autoloss",
    "checked": true,
    "id": "3c49b14137703eb2fd777bdc4896918c0db38d4f",
    "semantic_title": "loss function discovery for object detection via convergence-simulation driven search",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=jxdXSW9Doc": {
    "title": "Effective Distributed Learning with Random Features: Improved Bounds and Algorithms",
    "volume": "poster",
    "abstract": "In this paper, we study the statistical properties of distributed kernel ridge regression together with random features (DKRR-RF), and obtain optimal generalization bounds under the basic setting, which can substantially relax the restriction on the number of local machines in the existing state-of-art bounds. Specifically, we first show that the simple combination of divide-and-conquer technique and random features can achieve the same statistical accuracy as the exact KRR in expectation requiring only $\\mathcal{O}(|\\mathcal{D}|)$ memory and $\\mathcal{O}(|\\mathcal{D}|^{1.5})$ time. Then, beyond the generalization bounds in expectation that demonstrate the average information for multiple trails, we derive generalization bounds in probability to capture the learning performance for a single trail. Finally, we propose an effective communication strategy to further improve the performance of DKRR-RF, and validate the theoretical bounds via numerical experiments",
    "checked": true,
    "id": "18c1ebb93ef9625c80c1324760840028c1185c34",
    "semantic_title": "effective distributed learning with random features: improved bounds and algorithms",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=AhElGnhU2BV": {
    "title": "On InstaHide, Phase Retrieval, and Sparse Matrix Factorization",
    "volume": "poster",
    "abstract": "In this work, we examine the security of InstaHide, a scheme recently proposed by \\cite{hsla20} for preserving the security of private datasets in the context of distributed learning. To generate a synthetic training example to be shared among the distributed learners, InstaHide takes a convex combination of private feature vectors and randomly flips the sign of each entry of the resulting vector with probability 1/2. A salient question is whether this scheme is secure in any provable sense, perhaps under a plausible complexity-theoretic assumption. The answer to this turns out to be quite subtle and closely related to the average-case complexity of a multi-task, missing-data version of the classic problem of phase retrieval that is interesting in its own right. Motivated by this connection, under the standard distributional assumption that the public/private feature vectors are isotropic Gaussian, we design an algorithm that can actually recover a private vector using only the public vectors and a sequence of synthetic vectors generated by InstaHide",
    "checked": true,
    "id": "5acca7c62ef943d46ff298426d5af310aa5b02de",
    "semantic_title": "on instahide, phase retrieval, and sparse matrix factorization",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=Vd7lCMvtLqg": {
    "title": "Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies",
    "volume": "poster",
    "abstract": "Learning continuous representations of discrete objects such as text, users, movies, and URLs lies at the heart of many applications including language and user modeling. When using discrete objects as input to neural networks, we often ignore the underlying structures (e.g., natural groupings and similarities) and embed the objects independently into individual vectors. As a result, existing methods do not scale to large vocabulary sizes. In this paper, we design a simple and efficient embedding algorithm that learns a small set of anchor embeddings and a sparse transformation matrix. We call our method Anchor & Transform (ANT) as the embeddings of discrete objects are a sparse linear combination of the anchors, weighted according to the transformation matrix. ANT is scalable, flexible, and end-to-end trainable. We further provide a statistical interpretation of our algorithm as a Bayesian nonparametric prior for embeddings that encourages sparsity and leverages natural groupings among objects. By deriving an approximate inference algorithm based on Small Variance Asymptotics, we obtain a natural extension that automatically learns the optimal number of anchors instead of having to tune it as a hyperparameter. On text classification, language modeling, and movie recommendation benchmarks, we show that ANT is particularly suitable for large vocabulary sizes and demonstrates stronger performance with fewer parameters (up to 40x compression) as compared to existing compression baselines",
    "checked": true,
    "id": "39d5aec7ccd7d03037118ce6ae08e2b9318ba0b3",
    "semantic_title": "anchor & transform: learning sparse embeddings for large vocabularies",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=jDdzh5ul-d": {
    "title": "Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning",
    "volume": "poster",
    "abstract": "Federated learning (FL) is a distributed machine learning architecture that leverages a large number of workers to jointly learn a model with decentralized data. FL has received increasing attention in recent years thanks to its data privacy protection, communication efficiency and a linear speedup for convergence in training (i.e., convergence performance increases linearly with respect to the number of workers). However, existing studies on linear speedup for convergence are only limited to the assumptions of i.i.d. datasets across workers and/or full worker participation, both of which rarely hold in practice. So far, it remains an open question whether or not the linear speedup for convergence is achievable under non-i.i.d. datasets with partial worker participation in FL. In this paper, we show that the answer is affirmative. Specifically, we show that the federated averaging (FedAvg) algorithm (with two-sided learning rates) on non-i.i.d. datasets in non-convex settings achieves a convergence rate $\\mathcal{O}(\\frac{1}{\\sqrt{mKT}} + \\frac{1}{T})$ for full worker participation and a convergence rate $\\mathcal{O}(\\frac{\\sqrt{K}}{\\sqrt{nT}} + \\frac{1}{T})$ for partial worker participation, where $K$ is the number of local steps, $T$ is the number of total communication rounds, $m$ is the total worker number and $n$ is the worker number in one communication round if for partial worker participation. Our results also reveal that the local steps in FL could help the convergence and show that the maximum number of local steps can be improved to $T/m$ in full worker participation. We conduct extensive experiments on MNIST and CIFAR-10 to verify our theoretical results",
    "checked": true,
    "id": "433000baf18bb4403681fde5740bccd1fa2034a9",
    "semantic_title": "achieving linear speedup with partial worker participation in non-iid federated learning",
    "citation_count": 268,
    "authors": []
  },
  "https://openreview.net/forum?id=vK9WrZ0QYQ": {
    "title": "Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS",
    "volume": "poster",
    "abstract": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions, when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS, when it is restricted to the sphere $\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$",
    "checked": true,
    "id": "4ddc0757d280e51524fadcfec150be11f9b48fde",
    "semantic_title": "deep neural tangent kernel and laplace kernel have the same rkhs",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=w2mYg3d0eot": {
    "title": "Fast convergence of stochastic subgradient method under interpolation",
    "volume": "poster",
    "abstract": "This paper studies the behaviour of the stochastic subgradient descent (SSGD) method applied to over-parameterized nonsmooth optimization problems that satisfy an interpolation condition. By leveraging the composite structure of the empirical risk minimization problems, we prove that SSGD converges, respectively, with rates $O(1/\\epsilon)$ and $O(\\log(1/\\epsilon))$ for convex and strongly-convex objectives when interpolation holds. These rates coincide with established rates for the stochastic gradient descent (SGD) method applied to smooth problems that also satisfy an interpolation condition. Our analysis provides a partial explanation for the empirical observation that sometimes SGD and SSGD behave similarly for training smooth and nonsmooth machine learning models. We also prove that the rate $O(1/\\epsilon)$ is optimal for the subgradient method in the convex and interpolation setting",
    "checked": true,
    "id": "fcc7a7305a1b502bc3f40462c06bd5c61c5cd0a7",
    "semantic_title": "fast convergence of stochastic subgradient method under interpolation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=PH5PH9ZO_4": {
    "title": "Generating Adversarial Computer Programs using Optimized Obfuscations",
    "volume": "poster",
    "abstract": "Machine learning (ML) models that learn and predict properties of computer programs are increasingly being adopted and deployed. These models have demonstrated success in applications such as auto-completing code, summarizing large programs, and detecting bugs and malware in programs. In this work, we investigate principled ways to adversarially perturb a computer program to fool such learned models, and thus determine their adversarial robustness. We use program obfuscations, which have conventionally been used to avoid attempts at reverse engineering programs, as adversarial perturbations. These perturbations modify programs in ways that do not alter their functionality but can be crafted to deceive an ML model when making a decision. We provide a general formulation for an adversarial program that allows applying multiple obfuscation transformations to a program in any language. We develop first-order optimization algorithms to efficiently determine two key aspects -- which parts of the program to transform, and what transformations to use. We show that it is important to optimize both these aspects to generate the best adversarially perturbed program. Due to the discrete nature of this problem, we also propose using randomized smoothing to improve the attack loss landscape to ease optimization. We evaluate our work on Python and Java programs on the problem of program summarization. We show that our best attack proposal achieves a $52\\%$ improvement over a state-of-the-art attack generation approach for programs trained on a \\textsc{seq2seq} model. We further show that our formulation is better at training models that are robust to adversarial attacks",
    "checked": true,
    "id": "2e05413a737a7fe823c97e12c2ddc10d4a4c9dc0",
    "semantic_title": "generating adversarial computer programs using optimized obfuscations",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=tc5qisoB-C": {
    "title": "C-Learning: Learning to Achieve Goals via Recursive Classification",
    "volume": "poster",
    "abstract": "We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned reinforcement learning (RL), is centered around learning a conditional probability density function over future states. Instead of directly estimating this density function, we indirectly estimate this density function by training a classifier to predict whether an observation comes from the future. Via Bayes' rule, predictions from our classifier can be transformed into predictions over future states. Importantly, an off-policy variant of our algorithm allows us to predict the future state distribution of a new policy, without collecting new experience. This variant allows us to optimize functionals of a policy's future state distribution, such as the density of reaching a particular goal state. While conceptually similar to Q-learning, our work lays a principled foundation for goal-conditioned RL as density estimation, providing justification for goal-conditioned methods used in prior work. This foundation makes hypotheses about Q-learning, including the optimal goal-sampling ratio, which we confirm experimentally. Moreover, our proposed method is competitive with prior goal-conditioned RL methods",
    "checked": true,
    "id": "f0901642e339d17b3eb66daae112f5d62556c637",
    "semantic_title": "c-learning: learning to achieve goals via recursive classification",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=eqBwg3AcIAK": {
    "title": "Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers",
    "volume": "poster",
    "abstract": "We propose a simple, practical, and intuitive approach for domain adaptation in reinforcement learning. Our approach stems from the idea that the agent's experience in the source domain should look similar to its experience in the target domain. Building off of a probabilistic view of RL, we achieve this goal by compensating for the difference in dynamics by modifying the reward function. This modified reward function is simple to estimate by learning auxiliary classifiers that distinguish source-domain transitions from target-domain transitions. Intuitively, the agent is penalized for transitions that would indicate that the agent is interacting with the source domain, rather than the target domain. Formally, we prove that applying our method in the source domain is guaranteed to obtain a near-optimal policy for the target domain, provided that the source and target domains satisfy a lightweight assumption. Our approach is applicable to domains with continuous states and actions and does not require learning an explicit model of the dynamics. On discrete and continuous control tasks, we illustrate the mechanics of our approach and demonstrate its scalability to high-dimensional~tasks",
    "checked": true,
    "id": "ae712addbdd969c3d1f207fcaec1ce9f8b8741e3",
    "semantic_title": "off-dynamics reinforcement learning: training for transfer with domain classifiers",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=KtH8W3S_RE": {
    "title": "Multi-resolution modeling of a discrete stochastic process identifies causes of cancer",
    "volume": "poster",
    "abstract": "Detection of cancer-causing mutations within the vast and mostly unexplored human genome is a major challenge. Doing so requires modeling the background mutation rate, a highly non-stationary stochastic process, across regions of interest varying in size from one to millions of positions. Here, we present the split-Poisson-Gamma (SPG) distribution, an extension of the classical Poisson-Gamma formulation, to model a discrete stochastic process at multiple resolutions. We demonstrate that the probability model has a closed-form posterior, enabling efficient and accurate linear-time prediction over any length scale after the parameters of the model have been inferred a single time. We apply our framework to model mutation rates in tumors and show that model parameters can be accurately inferred from high-dimensional epigenetic data using a convolutional neural network, Gaussian process, and maximum-likelihood estimation. Our method is both more accurate and more efficient than existing models over a large range of length scales. We demonstrate the usefulness of multi-resolution modeling by detecting genomic elements that drive tumor emergence and are of vastly differing sizes",
    "checked": true,
    "id": "3d9a3486ca5ed6ab332f361f5440e147c8cc5e2f",
    "semantic_title": "multi-resolution modeling of a discrete stochastic process identifies causes of cancer",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=8Ln-Bq0mZcy": {
    "title": "On the Critical Role of Conventions in Adaptive Human-AI Collaboration",
    "volume": "poster",
    "abstract": "Humans can quickly adapt to new partners in collaborative tasks (e.g. playing basketball), because they understand which fundamental skills of the task (e.g. how to dribble, how to shoot) carry over across new partners. Humans can also quickly adapt to similar tasks with the same partners by carrying over conventions that they have developed (e.g. raising hand signals pass the ball), without learning to coordinate from scratch. To collaborate seamlessly with humans, AI agents should adapt quickly to new partners and new tasks as well. However, current approaches have not attempted to distinguish between the complexities intrinsic to a task and the conventions used by a partner, and more generally there has been little focus on leveraging conventions for adapting to new settings. In this work, we propose a learning framework that teases apart rule-dependent representation from convention-dependent representation in a principled way. We show that, under some assumptions, our rule-dependent representation is a sufficient statistic of the distribution over best-response strategies across partners. Using this separation of representations, our agents are able to adapt quickly to new partners, and to coordinate with old partners on new tasks in a zero-shot manner. We experimentally validate our approach on three collaborative tasks varying in complexity: a contextual multi-armed bandit, a block placing task, and the card game Hanabi",
    "checked": true,
    "id": "53d035f903070b8b8ea47be79f9957738f63373b",
    "semantic_title": "on the critical role of conventions in adaptive human-ai collaboration",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=NsMLjcFaO8O": {
    "title": "WaveGrad: Estimating Gradients for Waveform Generation",
    "volume": "poster",
    "abstract": "This paper introduces WaveGrad, a conditional model for waveform generation which estimates gradients of the data density. The model is built on prior work on score matching and diffusion probabilistic models. It starts from a Gaussian white noise signal and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad offers a natural way to trade inference speed for sample quality by adjusting the number of refinement steps, and bridges the gap between non-autoregressive and autoregressive models in terms of audio quality. We find that it can generate high fidelity audio samples using as few as six iterations. Experiments reveal WaveGrad to generate high fidelity audio, outperforming adversarial non-autoregressive baselines and matching a strong likelihood-based autoregressive baseline using fewer sequential operations. Audio samples are available at https://wavegrad.github.io/",
    "checked": true,
    "id": "685af6d2bcdff7170574643b2c5ab4fbcc36f597",
    "semantic_title": "wavegrad: estimating gradients for waveform generation",
    "citation_count": 806,
    "authors": []
  },
  "https://openreview.net/forum?id=FOyuZ26emy": {
    "title": "A Critique of Self-Expressive Deep Subspace Clustering",
    "volume": "poster",
    "abstract": "Subspace clustering is an unsupervised clustering technique designed to cluster data that is supported on a union of linear subspaces, with each subspace defining a cluster with dimension lower than the ambient space. Many existing formulations for this problem are based on exploiting the self-expressive property of linear subspaces, where any point within a subspace can be represented as linear combination of other points within the subspace. To extend this approach to data supported on a union of non-linear manifolds, numerous studies have proposed learning an embedding of the original data using a neural network which is regularized by a self-expressive loss function on the data in the embedded space to encourage a union of linear subspaces prior on the data in the embedded space. Here we show that there are a number of potential flaws with this approach which have not been adequately addressed in prior work. In particular, we show the model formulation is often ill-posed in that it can lead to a degenerate embedding of the data, which need not correspond to a union of subspaces at all and is poorly suited for clustering. We validate our theoretical results experimentally and also repeat prior experiments reported in the literature, where we conclude that a significant portion of the previously claimed performance benefits can be attributed to an ad-hoc post processing step rather than the deep subspace clustering model",
    "checked": true,
    "id": "0f49c53693d0c4a6cd9e1a20ff6b8d58491d2858",
    "semantic_title": "a critique of self-expressive deep subspace clustering",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=unI5ucw_Jk": {
    "title": "Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning",
    "volume": "poster",
    "abstract": "Understanding human behavior from observed data is critical for transparency and accountability in decision-making. Consider real-world settings such as healthcare, in which modeling a decision-maker's policy is challenging—with no access to underlying states, no knowledge of environment dynamics, and no allowance for live experimentation. We desire learning a data-driven representation of decision- making behavior that (1) inheres transparency by design, (2) accommodates partial observability, and (3) operates completely offline. To satisfy these key criteria, we propose a novel model-based Bayesian method for interpretable policy learning (\"Interpole\") that jointly estimates an agent's (possibly biased) belief-update process together with their (possibly suboptimal) belief-action mapping. Through experiments on both simulated and real-world data for the problem of Alzheimer's disease diagnosis, we illustrate the potential of our approach as an investigative device for auditing, quantifying, and understanding human decision-making behavior",
    "checked": true,
    "id": "31189438c9f40905e0d87ecc3a88e88c2165c015",
    "semantic_title": "explaining by imitating: understanding decisions by interpretable policy learning",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=Srmggo3b3X6": {
    "title": "For self-supervised learning, Rationality implies generalization, provably",
    "volume": "poster",
    "abstract": "We prove a new upper bound on the generalization gap of classifiers that are obtained by first using self-supervision to learn a representation $r$ of the training~data, and then fitting a simple (e.g., linear) classifier $g$ to the labels. Specifically, we show that (under the assumptions described below) the generalization gap of such classifiers tends to zero if $\\mathsf{C}(g) \\ll n$, where $\\mathsf{C}(g)$ is an appropriately-defined measure of the simple classifier $g$'s complexity, and $n$ is the number of training samples. We stress that our bound is independent of the complexity of the representation $r$. We do not make any structural or conditional-independence assumptions on the representation-learning task, which can use the same training dataset that is later used for classification. Rather, we assume that the training procedure satisfies certain natural noise-robustness (adding small amount of label noise causes small degradation in performance) and rationality (getting the wrong label is not better than getting no label at all) conditions that widely hold across many standard architectures. We also conduct an extensive empirical study of the generalization gap and the quantities used in our assumptions for a variety of self-supervision based algorithms, including SimCLR, AMDIM and BigBiGAN, on the CIFAR-10 and ImageNet datasets. We show that, unlike standard supervised classifiers, these algorithms display small generalization gap, and the bounds we prove on this gap are often non vacuous",
    "checked": true,
    "id": "e2c80fd059ba9986da37e498330979dc5976beb7",
    "semantic_title": "for self-supervised learning, rationality implies generalization, provably",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=JbuYF437WB6": {
    "title": "Directed Acyclic Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c6337dc83db09c9648ae850c71937eb8e5fd7a43",
    "semantic_title": "directed acyclic graph neural networks",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=N3zUDGN5lO": {
    "title": "My Body is a Cage: the Role of Morphology in Graph-Based Incompatible Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "85cb1d20ae5ba3f547dc76d2cc73677652900d9b",
    "semantic_title": "my body is a cage: the role of morphology in graph-based incompatible control",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=3SV-ZePhnZM": {
    "title": "Incremental few-shot learning via vector quantization in deep embedded space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4b7a308de0bb4ed36584e7bc1bf7b15b07d5e40e",
    "semantic_title": "incremental few-shot learning via vector quantization in deep embedded space",
    "citation_count": 106,
    "authors": []
  },
  "https://openreview.net/forum?id=cO1IH43yUF": {
    "title": "Revisiting Few-sample BERT Fine-tuning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "056935031bc5cf0aeeaa0946320de26e14a1817e",
    "semantic_title": "revisiting few-sample bert fine-tuning",
    "citation_count": 449,
    "authors": []
  },
  "https://openreview.net/forum?id=84gjULz1t5": {
    "title": "Linear Convergent Decentralized Optimization with Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2502dc4523fe70b02857cb58174dda314eb0a8bf",
    "semantic_title": "linear convergent decentralized optimization with compression",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=Ldau9eHU-qO": {
    "title": "Learning from Demonstration with Weakly Supervised Disentanglement",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "31727b59155cd883106a18a57c631a958e394a35",
    "semantic_title": "learning from demonstration with weakly supervised disentanglement",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=wta_8Hx2KD": {
    "title": "Incorporating Symmetry into Deep Dynamics Models for Improved Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "412d0b5aa97e0713259d203e76f8a57b0357e0c9",
    "semantic_title": "incorporating symmetry into deep dynamics models for improved generalization",
    "citation_count": 180,
    "authors": []
  },
  "https://openreview.net/forum?id=BbNIbVPJ-42": {
    "title": "The Risks of Invariant Risk Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1e76e2fbf27198986271a672f462dc38d790d00f",
    "semantic_title": "the risks of invariant risk minimization",
    "citation_count": 318,
    "authors": []
  },
  "https://openreview.net/forum?id=V5j-jdoDDP": {
    "title": "Scaling Symbolic Methods using Gradients for Neural Model Explanation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "04362f8cad633c9a71b99024c8c577a542fa5ea4",
    "semantic_title": "scaling symbolic methods using gradients for neural model explanation",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=ct8_a9h1M": {
    "title": "Contextual Dropout: An Efficient Sample-Dependent Dropout Module",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0c6b8ec01311daba0cdf7c9ea79291329f7de3bf",
    "semantic_title": "contextual dropout: an efficient sample-dependent dropout module",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=CR1XOQ0UTh-": {
    "title": "Contrastive Learning with Hard Negative Samples",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7097137596f6755675f6aafcdd80969a747322ae",
    "semantic_title": "contrastive learning with hard negative samples",
    "citation_count": 817,
    "authors": []
  },
  "https://openreview.net/forum?id=6puUoArESGp": {
    "title": "Debiasing Concept-based Explanations with Causal Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "42c061c1e329b157e6dc4f27f40bed534bddc871",
    "semantic_title": "debiasing concept-based explanations with causal analysis",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=TBIzh9b5eaz": {
    "title": "Risk-Averse Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7e4520c972c88bbbe3bb3d15a89208f77756aae2",
    "semantic_title": "risk-averse offline reinforcement learning",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=6UdQLhqJyFD": {
    "title": "Parameter Efficient Multimodal Transformers for Video Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "aed0f85b14e2c52e0c1850c93503bd637ff8119b",
    "semantic_title": "parameter efficient multimodal transformers for video representation learning",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=ZcKPWuhG6wy": {
    "title": "Tradeoffs in Data Augmentation: An Empirical Study",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9da85492d28671cd9e513ab1a2c8ea3c772786ae",
    "semantic_title": "tradeoffs in data augmentation: an empirical study",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=eJIJF3-LoZO": {
    "title": "Concept Learners for Few-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9483d411aac8741e89fb4ad5313adbfa652c3a5a",
    "semantic_title": "concept learners for few-shot learning",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=q8qLAbQBupm": {
    "title": "Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b93de30557605a4d7fe688524dc38dd52abc59e0",
    "semantic_title": "neural mechanics: symmetry and broken conservation laws in deep learning dynamics",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=DNl5s5BXeBn": {
    "title": "Fair Mixup: Fairness via Interpolation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3470256133fe185031791e600f84376262bd9015",
    "semantic_title": "fair mixup: fairness via interpolation",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=UwGY2qjqoLD": {
    "title": "Heating up decision boundaries: isocapacitory saturation, adversarial scenarios and generalization bounds",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "329c246b5916c6d0fa066fbfd3586fa02a9c4ab7",
    "semantic_title": "heating up decision boundaries: isocapacitory saturation, adversarial scenarios and generalization bounds",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=xHKVVHGDOEk": {
    "title": "Influence Functions in Deep Learning Are Fragile",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "098076a2c90e42c81b843bf339446427c2ff02ed",
    "semantic_title": "influence functions in deep learning are fragile",
    "citation_count": 246,
    "authors": []
  },
  "https://openreview.net/forum?id=pW2Q2xLwIMD": {
    "title": "Few-Shot Learning via Learning the Representation, Provably",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3217f8d996b873410d720987f79790321cb2f877",
    "semantic_title": "few-shot learning via learning the representation, provably",
    "citation_count": 265,
    "authors": []
  },
  "https://openreview.net/forum?id=jMPcEkJpdD": {
    "title": "Self-Supervised Learning of Compressed Video Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "35c80e047de3cf45ede8092ff3827135d7ccd4fc",
    "semantic_title": "self-supervised learning of compressed video representations",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=Ig53hpHxS4": {
    "title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e9495ee15e9dcc844095ef5071dd663470d4d564",
    "semantic_title": "flowtron: an autoregressive flow-based generative network for text-to-speech synthesis",
    "citation_count": 122,
    "authors": []
  },
  "https://openreview.net/forum?id=RSU17UoKfJF": {
    "title": "R-GAP: Recursive Gradient Attack on Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2ae3f6b82dd8503da380b373520e0d0713627cf2",
    "semantic_title": "r-gap: recursive gradient attack on privacy",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=iQQK02mxVIT": {
    "title": "Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "5e2c1a7e662d1a23bf3ec90b91c366810600e8af",
    "semantic_title": "why resampling outperforms reweighting for correcting sampling bias",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=2VXyy9mIyU3": {
    "title": "Learning with Instance-Dependent Label Noise: A Sample Sieve Approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "599ed9357448d8c55e2dc7f4f12224d5c6dd1fcc",
    "semantic_title": "learning with instance-dependent label noise: a sample sieve approach",
    "citation_count": 200,
    "authors": []
  },
  "https://openreview.net/forum?id=43VKWxg_Sqr": {
    "title": "Unsupervised Audiovisual Synthesis via Exemplar Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "25163583e46d42879c457f1b6f3eed95302d3364",
    "semantic_title": "unsupervised audiovisual synthesis via exemplar autoencoders",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=pqZV_srUVmK": {
    "title": "Single-Timescale Actor-Critic Provably Finds Globally Optimal Policy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9c17c98b40028c4239639f41b08d58353b75acc1",
    "semantic_title": "single-timescale actor-critic provably finds globally optimal policy",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=oZIvHV04XgC": {
    "title": "Wandering within a world: Online contextualized few-shot learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9f774c0710b887064f309f5a639c0679eccbb6f2",
    "semantic_title": "wandering within a world: online contextualized few-shot learning",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=Q1jmmQz72M2": {
    "title": "Neural Delay Differential Equations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7500229c7160547b11d8dc1144f1567e33546332",
    "semantic_title": "neural delay differential equations",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=0OlrLvrsHwQ": {
    "title": "Learning Parametrised Graph Shift Operators",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1c295507aa5c724b1b30156c514e43e4c457790b",
    "semantic_title": "learning parametrised graph shift operators",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=6FqKiVAdI3Y": {
    "title": "DOP: Off-Policy Multi-Agent Decomposed Policy Gradients",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "4b3fbe0194e7bacc6ce7b5e9a0e826e04b1548b3",
    "semantic_title": "off-policy multi-agent decomposed policy gradients",
    "citation_count": 174,
    "authors": []
  },
  "https://openreview.net/forum?id=lf7st0bJIA5": {
    "title": "Unsupervised Discovery of 3D Physical Objects from Video",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a5a28056d84506f9bc4f3245e961e9d2f943e915",
    "semantic_title": "unsupervised discovery of 3d physical objects from video",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=OqtLIabPTit": {
    "title": "Exploring Balanced Feature Spaces for Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b65bc8bd4e8900c38eb09bc1cbccea2499a86627",
    "semantic_title": "exploring balanced feature spaces for representation learning",
    "citation_count": 236,
    "authors": []
  },
  "https://openreview.net/forum?id=n7wIfYPdVet": {
    "title": "Auxiliary Learning by Implicit Differentiation",
    "volume": "poster",
    "abstract": "Training neural networks with auxiliary tasks is a common practice for improving the performance on a main task of interest. Two main challenges arise in this multi-task learning setting: (i) designing useful auxiliary tasks; and (ii) combining auxiliary tasks into a single coherent loss. Here, we propose a novel framework, AuxiLearn, that targets both challenges based on implicit differentiation. First, when useful auxiliaries are known, we propose learning a network that combines all losses into a single coherent objective function. This network can learn non-linear interactions between tasks. Second, when no useful auxiliary task is known, we describe how to learn a network that generates a meaningful, novel auxiliary task. We evaluate AuxiLearn in a series of tasks and domains, including image segmentation and learning with attributes in the low data regime, and find that it consistently outperforms competing methods",
    "checked": true,
    "id": "5043e91d4f36ada5504e1f7fa8b534c614a8ad98",
    "semantic_title": "auxiliary learning by implicit differentiation",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=i80OPhOCVH2": {
    "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications",
    "volume": "poster",
    "abstract": "Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008), one of the major problems in training GNNs was their struggle to propagate information between distant nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. In this paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffers from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights. Our code is available at https://github.com/tech-srl/bottleneck/",
    "checked": true,
    "id": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
    "semantic_title": "on the bottleneck of graph neural networks and its practical implications",
    "citation_count": 715,
    "authors": []
  },
  "https://openreview.net/forum?id=L7WD8ZdscQ5": {
    "title": "The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods",
    "volume": "poster",
    "abstract": "The adaptive stochastic gradient descent (SGD) with momentum has been widely adopted in deep learning as well as convex optimization. In practice, the last iterate is commonly used as the final solution. However, the available regret analysis and the setting of constant momentum parameters only guarantee the optimal convergence of the averaged solution. In this paper, we fill this theory-practice gap by investigating the convergence of the last iterate (referred to as {\\it individual convergence}), which is a more difficult task than convergence analysis of the averaged solution. Specifically, in the constrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB) method, in which the step size is only updated using the exponential moving average strategy, attains an individual convergence rate of $O(\\frac{1}{\\sqrt{t}})$, as opposed to that of $O(\\frac{\\log t}{\\sqrt {t}})$ of SGD, where $t$ is the number of iterations. Our new analysis not only shows how the HB momentum and its time-varying weight help us to achieve the acceleration in convex optimization but also gives valuable hints how the momentum parameters should be scheduled in deep learning. Empirical results validate the correctness of our convergence analysis in optimizing convex functions and demonstrate the improved performance of the adaptive HB methods in training deep networks",
    "checked": true,
    "id": "55d0c4c556f5298930111b7e968912738c5a7f3c",
    "semantic_title": "the role of momentum parameters in the optimal convergence of adaptive polyak's heavy-ball methods",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=xjXg0bnoDmS": {
    "title": "Entropic gradient descent algorithms and wide flat minima",
    "volume": "poster",
    "abstract": "The properties of flat minima in the empirical risk landscape of neural networks have been debated for some time. Increasing evidence suggests they possess better generalization capabilities with respect to sharp ones. In this work we first discuss the relationship between alternative measures of flatness: The local entropy, which is useful for analysis and algorithm development, and the local energy, which is easier to compute and was shown empirically in extensive tests on state-of-the-art networks to be the best predictor of generalization capabilities. We show semi-analytically in simple controlled scenarios that these two measures correlate strongly with each other and with generalization. Then, we extend the analysis to the deep learning scenario by extensive numerical validations. We study two algorithms, Entropy-SGD and Replicated-SGD, that explicitly include the local entropy in the optimization objective. We devise a training schedule by which we consistently find flatter minima (using both flatness measures), and improve the generalization error for common architectures (e.g. ResNet, EfficientNet)",
    "checked": true,
    "id": "7b8e694b5032a2ab2b40993c5cd20d31d8f5b28a",
    "semantic_title": "entropic gradient descent algorithms and wide flat minima",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=XLfdzwNKzch": {
    "title": "SEDONA: Search for Decoupled Neural Networks toward Greedy Block-wise Learning",
    "volume": "poster",
    "abstract": "Backward locking and update locking are well-known sources of inefficiency in backpropagation that prevent from concurrently updating layers. Several works have recently suggested using local error signals to train network blocks asynchronously to overcome these limitations. However, they often require numerous iterations of trial-and-error to find the best configuration for local training, including how to decouple network blocks and which auxiliary networks to use for each block. In this work, we propose a differentiable search algorithm named SEDONA to automate this process. Experimental results show that our algorithm can consistently discover transferable decoupled architectures for VGG and ResNet variants, and significantly outperforms the ones trained with end-to-end backpropagation and other state-of-the-art greedy-leaning methods in CIFAR-10, Tiny-ImageNet and ImageNet",
    "checked": true,
    "id": "b57d4098fa63a330e201df09fe088d7cc0027251",
    "semantic_title": "sedona: search for decoupled neural networks toward greedy block-wise learning",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=tu29GQT0JFy": {
    "title": "not-MIWAE: Deep Generative Modelling with Missing not at Random Data",
    "volume": "poster",
    "abstract": "When a missing process depends on the missing values themselves, it needs to be explicitly modelled and taken into account while doing likelihood-based inference. We present an approach for building and fitting deep latent variable models (DLVMs) in cases where the missing process is dependent on the missing data. Specifically, a deep neural network enables us to flexibly model the conditional distribution of the missingness pattern given the data. This allows for incorporating prior information about the type of missingness (e.g.~self-censoring) into the model. Our inference technique, based on importance-weighted variational inference, involves maximising a lower bound of the joint likelihood. Stochastic gradients of the bound are obtained by using the reparameterisation trick both in latent space and data space. We show on various kinds of data sets and missingness patterns that explicitly modelling the missing process can be invaluable",
    "checked": true,
    "id": "a6bf259a29fbe2593720d1b62697edc825292c14",
    "semantic_title": "not-miwae: deep generative modelling with missing not at random data",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=9G5MIc-goqB": {
    "title": "Reweighting Augmented Samples by Minimizing the Maximal Expected Loss",
    "volume": "poster",
    "abstract": "Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model",
    "checked": true,
    "id": "601e490881aa6baac15640b0dfcc0e7ad2731475",
    "semantic_title": "reweighting augmented samples by minimizing the maximal expected loss",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=Xv_s64FiXTv": {
    "title": "Learning to Represent Action Values as a Hypergraph on the Action Vertices",
    "volume": "poster",
    "abstract": "Action-value estimation is a critical component of many reinforcement learning (RL) methods whereby sample complexity relies heavily on how fast a good estimator for action value can be learned. By viewing this problem through the lens of representation learning, good representations of both state and action can facilitate action-value estimation. While advances in deep learning have seamlessly driven progress in learning state representations, given the specificity of the notion of agency to RL, little attention has been paid to learning action representations. We conjecture that leveraging the combinatorial structure of multi-dimensional action spaces is a key ingredient for learning good representations of action. To test this, we set forth the action hypergraph networks framework---a class of functions for learning action representations in multi-dimensional discrete action spaces with a structural inductive bias. Using this framework we realise an agent class based on a combination with deep Q-networks, which we dub hypergraph Q-networks. We show the effectiveness of our approach on a myriad of domains: illustrative prediction problems under minimal confounding effects, Atari 2600 games, and discretised physical control benchmarks",
    "checked": true,
    "id": "467658037d60be494126f8c00974fb30593403f1",
    "semantic_title": "learning to represent action values as a hypergraph on the action vertices",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=vsU0efpivw": {
    "title": "Shapley Explanation Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "899e489944063f0313ef01b33a8d4ce6ca386321",
    "semantic_title": "shapley explanation networks",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=mCLVeEpplNE": {
    "title": "NBDT: Neural-Backed Decision Tree",
    "volume": "poster",
    "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees",
    "checked": true,
    "id": "511c19acb526452aedc6ba6d4f738f9e916ea24d",
    "semantic_title": "nbdt: neural-backed decision tree",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=St1giarCHLP": {
    "title": "Denoising Diffusion Implicit Models",
    "volume": "poster",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error",
    "checked": true,
    "id": "014576b866078524286802b1d0e18628520aa886",
    "semantic_title": "denoising diffusion implicit models",
    "citation_count": 8048,
    "authors": []
  },
  "https://openreview.net/forum?id=qrwe7XHTmYb": {
    "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
    "volume": "poster",
    "abstract": "Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost,ease of programming, and efficient implementation on parallel devices. In this paper we demonstrate conditional computation as a remedy to the above mentioned impediments, and demonstrate its efficacy and utility. We make extensive use of GShard, a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler to enable large scale models with up to trillions of parameters. GShard and conditional computation enable us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts. We demonstrate that such a giant model with 600 billion parameters can efficiently be trained on 2048 TPU v3 cores in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art",
    "checked": true,
    "id": "1882f194cb43828852cc052887671e55a80f945a",
    "semantic_title": "gshard: scaling giant models with conditional computation and automatic sharding",
    "citation_count": 1261,
    "authors": []
  },
  "https://openreview.net/forum?id=CZ8Y3NzuVzO": {
    "title": "What Should Not Be Contrastive in Contrastive Learning",
    "volume": "poster",
    "abstract": "Recent self-supervised contrastive methods have been able to produce impressive transferable visual representations by learning to be invariant to different data augmentations. However, these methods implicitly assume a particular set of representational invariances (e.g., invariance to color), and can perform poorly when a downstream task violates this assumption (e.g., distinguishing red vs. yellow cars). We introduce a contrastive learning framework which does not require prior knowledge of specific, task-dependent invariances. Our model learns to capture varying and invariant factors for visual representations by constructing separate embedding spaces, each of which is invariant to all but one augmentation. We use a multi-head network with a shared backbone which captures information across each augmentation and alone outperforms all baselines on downstream tasks. We further find that the concatenation of the invariant and varying spaces performs best across all tasks we investigate, including coarse-grained, fine-grained, and few-shot downstream classification tasks, and various data corruptions",
    "checked": true,
    "id": "c3a0059e69a10c8fe70efab423761e378fdab74b",
    "semantic_title": "what should not be contrastive in contrastive learning",
    "citation_count": 307,
    "authors": []
  },
  "https://openreview.net/forum?id=7FNqrcPtieT": {
    "title": "On Data-Augmentation and Consistency-Based Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "Recently proposed consistency-based Semi-Supervised Learning (SSL) methods such as the Pi-model, temporal ensembling, the mean teacher, or the virtual adversarial training, achieve the state of the art results in several SSL tasks. These methods can typically reach performances that are comparable to their fully supervised counterparts while using only a fraction of labelled examples. Despite these methodological advances, the understanding of these methods is still relatively limited. To make progress, we analyse (variations of) the Pi-model in settings where analytically tractable results can be obtained. We establish links with Manifold Tangent Classifiers and demonstrate that the quality of the perturbations is key to obtaining reasonable SSL performances. Furthermore, we propose a simple extension of the Hidden Manifold Model that naturally incorporates data-augmentation schemes and offers a tractable framework for understanding SSL methods",
    "checked": true,
    "id": "b4cc976bd6fd45aa8ad91859a962d4b90ba580c3",
    "semantic_title": "on data-augmentation and consistency-based semi-supervised learning",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=Qm7R_SdqTpT": {
    "title": "Diverse Video Generation using a Gaussian Process Trigger",
    "volume": "poster",
    "abstract": "Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos as well as multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a GP to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. We leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of on-going sequences. In particular, we use the variance of GP over the output function space to trigger a change in the action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences",
    "checked": true,
    "id": "f286529cf63202f927a4ff7c450b918b42b128a9",
    "semantic_title": "diverse video generation using a gaussian process trigger",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=0pxiMpCyBtr": {
    "title": "Monotonic Kronecker-Factored Lattice",
    "volume": "poster",
    "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model",
    "checked": true,
    "id": "f33956ca6730433e35bd4a24b9b6194524473a4e",
    "semantic_title": "monotonic kronecker-factored lattice",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=MJAqnaC2vO1": {
    "title": "Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation",
    "volume": "poster",
    "abstract": "Designing proper loss functions is essential in training deep networks. Especially in the field of semantic segmentation, various evaluation metrics have been proposed for diverse scenarios. Despite the success of the widely adopted cross-entropy loss and its variants, the mis-alignment between the loss functions and evaluation metrics degrades the network performance. Meanwhile, manually designing loss functions for each specific metric requires expertise and significant manpower. In this paper, we propose to automate the design of metric-specific loss functions by searching differentiable surrogate losses for each metric. We substitute the non-differentiable operations in the metrics with parameterized functions, and conduct parameter search to optimize the shape of loss surfaces. Two constraints are introduced to regularize the search space and make the search efficient. Extensive experiments on PASCAL VOC and Cityscapes demonstrate that the searched surrogate losses outperform the manually designed loss functions consistently. The searched losses can generalize well to other datasets and networks. Code shall be released at https://github.com/fundamentalvision/Auto-Seg-Loss",
    "checked": true,
    "id": "98f2cb7e6baa140e1b5e72fd5af9357c678c176b",
    "semantic_title": "auto seg-loss: searching metric surrogates for semantic segmentation",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=l0V53bErniB": {
    "title": "Combining Physics and Machine Learning for Network Flow Estimation",
    "volume": "poster",
    "abstract": "The flow estimation problem consists of predicting missing edge flows in a network (e.g., traffic, power, and water) based on partial observations. These missing flows depend both on the underlying \\textit{physics} (edge features and a flow conservation law) as well as the observed edge flows. This paper introduces an optimization framework for computing missing edge flows and solves the problem using bilevel optimization and deep learning. More specifically, we learn regularizers that depend on edge features (e.g., number of lanes in a road, the resistance of a power line) using neural networks. Empirical results show that our method accurately predicts missing flows, outperforming the best baseline, and is able to capture relevant physical properties in traffic and power networks",
    "checked": true,
    "id": "998c3b6ac109da36df480d9868806f52c60e8487",
    "semantic_title": "combining physics and machine learning for network flow estimation",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=pmj131uIL9H": {
    "title": "NeMo: Neural Mesh Models of Contrastive Features for Robust 3D Pose Estimation",
    "volume": "poster",
    "abstract": "3D pose estimation is a challenging but important task in computer vision. In this work, we show that standard deep learning approaches to 3D pose estimation are not robust to partial occlusion. Inspired by the robustness of generative vision models to partial occlusion, we propose to integrate deep neural networks with 3D generative representations of objects into a unified neural architecture that we term NeMo. In particular, NeMo learns a generative model of neural feature activations at each vertex on a dense 3D mesh. Using differentiable rendering we estimate the 3D object pose by minimizing the reconstruction error between NeMo and the feature representation of the target image. To avoid local optima in the reconstruction loss, we train the feature extractor to maximize the distance between the individual feature representations on the mesh using contrastive learning. Our extensive experiments on PASCAL3D+, occluded-PASCAL3D+ and ObjectNet3D show that NeMo is much more robust to partial occlusion compared to standard deep networks, while retaining competitive performance on non-occluded data. Interestingly, our experiments also show that NeMo performs reasonably well even when the mesh representation only crudely approximates the true object geometry with a cuboid, hence revealing that the detailed 3D geometry is not needed for accurate 3D pose estimation",
    "checked": true,
    "id": "74c61e2d2f12e56015741bcc2340f14a776e8606",
    "semantic_title": "nemo: neural mesh models of contrastive features for robust 3d pose estimation",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=Wi5KUNlqWty": {
    "title": "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision",
    "volume": "poster",
    "abstract": "Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines",
    "checked": true,
    "id": "dc9b9c5a76df331409890eea0d01e3cf3459a526",
    "semantic_title": "how to find your friendly neighborhood: graph attention design with self-supervision",
    "citation_count": 267,
    "authors": []
  },
  "https://openreview.net/forum?id=gV3wdEOGy_V": {
    "title": "MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering",
    "volume": "poster",
    "abstract": "We present Mixture of Contrastive Experts (MiCE), a unified probabilistic clustering framework that simultaneously exploits the discriminative representations learned by contrastive learning and the semantic structures captured by a latent mixture model. Motivated by the mixture of experts, MiCE employs a gating function to partition an unlabeled dataset into subsets according to the latent semantics and multiple experts to discriminate distinct subsets of instances assigned to them in a contrastive learning manner. To solve the nontrivial inference and learning problems caused by the latent variables, we further develop a scalable variant of the Expectation-Maximization (EM) algorithm for MiCE and provide proof of the convergence. Empirically, we evaluate the clustering performance of MiCE on four widely adopted natural image datasets. MiCE achieves significantly better results than various previous methods and a strong contrastive learning baseline",
    "checked": true,
    "id": "158bb664b0b7d7dec5a2f8cb3ae20c4b67fd1d6e",
    "semantic_title": "mice: mixture of contrastive experts for unsupervised image clustering",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=TSRTzJnuEBS": {
    "title": "Anytime Sampling for Autoregressive Models via Ordered Autoencoding",
    "volume": "poster",
    "abstract": "Autoregressive models are widely used for tasks such as image and audio generation. The sampling process of these models, however, does not allow interruptions and cannot adapt to real-time computational resources. This challenge impedes the deployment of powerful autoregressive models, which involve a slow sampling process that is sequential in nature and typically scales linearly with respect to the data dimension. To address this difficulty, we propose a new family of autoregressive models that enables anytime sampling. Inspired by Principal Component Analysis, we learn a structured representation space where dimensions are ordered based on their importance with respect to reconstruction. Using an autoregressive model in this latent space, we trade off sample quality for computational efficiency by truncating the generation process before decoding into the original data space. Experimentally, we demonstrate in several image and audio generation tasks that sample quality degrades gracefully as we reduce the computational budget for sampling. The approach suffers almost no loss in sample quality (measured by FID) using only 60\\% to 80\\% of all latent dimensions for image data. Code is available at https://github.com/Newbeeer/Anytime-Auto-Regressive-Model",
    "checked": true,
    "id": "222066c35e9be2a161521bf022623e38d03c514b",
    "semantic_title": "anytime sampling for autoregressive models via ordered autoencoding",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=KTlJT1nof6d": {
    "title": "Initialization and Regularization of Factorized Neural Layers",
    "volume": "poster",
    "abstract": "Factorized layers—operations parameterized by products of two or more matrices—occur in a variety of deep learning contexts, including compressed model training, certain types of knowledge distillation, and multi-head self-attention architectures. We study how to initialize and regularize deep nets containing such layers, examining two simple, understudied schemes, spectral initialization and Frobenius decay, for improving their performance. The guiding insight is to design optimization routines for these networks that are as close as possible to that of their well-tuned, non-decomposed counterparts; we back this intuition with an analysis of how the initialization and regularization schemes impact training with gradient descent, drawing on modern attempts to understand the interplay of weight-decay and batch-normalization. Empirically, we highlight the benefits of spectral initialization and Frobenius decay across a variety of settings. In model compression, we show that they enable low-rank methods to significantly outperform both unstructured sparsity and tensor methods on the task of training low-memory residual networks; analogs of the schemes also improve the performance of tensor decomposition techniques. For knowledge distillation, Frobenius decay enables a simple, overcomplete baseline that yields a compact model from over-parameterized training without requiring retraining with or pruning a teacher network. Finally, we show how both schemes applied to multi-head attention lead to improved performance on both translation and unsupervised pre-training",
    "checked": true,
    "id": "f93f2476972228de142fde13913bccbec76859b8",
    "semantic_title": "initialization and regularization of factorized neural layers",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=U4XLJhqwNF1": {
    "title": "CO2: Consistent Contrast for Unsupervised Visual Representation Learning",
    "volume": "poster",
    "abstract": "Contrastive learning has recently been a core for unsupervised visual representation learning. Without human annotation, the common practice is to perform an instance discrimination task: Given a query image crop, label crops from the same image as positives, and crops from other randomly sampled images as negatives. An important limitation of this label assignment is that it can not reflect the heterogeneous similarity of the query crop to crops from other images, but regarding them as equally negative. To address this issue, inspired by consistency regularization in semi-supervised learning, we propose Consistent Contrast (CO2), which introduces a consistency term into unsupervised contrastive learning framework. The consistency term takes the similarity of the query crop to crops from other images as unlabeled, and the corresponding similarity of a positive crop as a pseudo label. It then encourages consistency between these two similarities. Empirically, CO2 improves Momentum Contrast (MoCo) by 2.9% top-1 accuracy on ImageNet linear protocol, 3.8% and 1.1% top-5 accuracy on 1% and 10% labeled semi-supervised settings. It also transfers to image classification, object detection, and semantic segmentation on PASCAL VOC. This shows that CO2 learns better visual representations for downstream tasks",
    "checked": true,
    "id": "b6f54f6d3a0cb9d3f1244c63773c40b0f5a1e224",
    "semantic_title": "co2: consistent contrast for unsupervised visual representation learning",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=V1ZHVxJ6dSS": {
    "title": "DC3: A learning method for optimization with hard constraints",
    "volume": "poster",
    "abstract": "Large optimization problems with hard constraints arise in many settings, yet classical solvers are often prohibitively slow, motivating the use of deep networks as cheap \"approximate solvers.\" Unfortunately, naive deep learning approaches typically cannot enforce the hard constraints of such problems, leading to infeasible solutions. In this work, we present Deep Constraint Completion and Correction (DC3), an algorithm to address this challenge. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. We demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid. In both cases, DC3 achieves near-optimal objective values while preserving feasibility",
    "checked": true,
    "id": "ac879e53927cdf7c3b8b5c00bc3ff13959f11d97",
    "semantic_title": "dc3: a learning method for optimization with hard constraints",
    "citation_count": 202,
    "authors": []
  },
  "https://openreview.net/forum?id=5lhWG3Hj2By": {
    "title": "Enforcing robust control guarantees within neural network policies",
    "volume": "poster",
    "abstract": "When designing controllers for safety-critical systems, practitioners often face a challenging tradeoff between robustness and performance. While robust control methods provide rigorous guarantees on system stability under certain worst-case disturbances, they often yield simple controllers that perform poorly in the average (non-worst) case. In contrast, nonlinear control methods trained using deep learning have achieved state-of-the-art performance on many control tasks, but often lack robustness guarantees. In this paper, we propose a technique that combines the strengths of these two approaches: constructing a generic nonlinear control policy class, parameterized by neural networks, that nonetheless enforces the same provable robustness criteria as robust control. Specifically, our approach entails integrating custom convex-optimization-based projection layers into a neural network-based policy. We demonstrate the power of this approach on several domains, improving in average-case performance over existing robust control methods and in worst-case stability over (non-robust) deep RL methods",
    "checked": true,
    "id": "7915568a86a196c06bb70fb98b3bc076a5a18b8d",
    "semantic_title": "enforcing robust control guarantees within neural network policies",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=BVSM0x3EDK6": {
    "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
    "volume": "poster",
    "abstract": "While successful for various computer vision tasks, deep neural networks have shown to be vulnerable to texture style shifts and small perturbations to which humans are robust. In this work, we show that the robustness of neural networks can be greatly improved through the use of random convolutions as data augmentation. Random convolutions are approximately shape-preserving and may distort local textures. Intuitively, randomized convolutions create an infinite number of new domains with similar global shapes but random local texture. Therefore, we explore using outputs of multi-scale random convolutions as new images or mixing them with the original images during training. When applying a network trained with our approach to unseen domains, our method consistently improves the performance on domain generalization benchmarks and is scalable to ImageNet. In particular, in the challenging scenario of generalizing to the sketch domain in PACS and to ImageNet-Sketch, our method outperforms state-of-art methods by a large margin. More interestingly, our method can benefit downstream tasks by providing a more robust pretrained visual representation",
    "checked": true,
    "id": "b27ad18e20d27efe8a9fbc54b1c2dcef8b2da19f",
    "semantic_title": "robust and generalizable visual representation learning via random convolutions",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=3SqrRe8FWQ-": {
    "title": "WrapNet: Neural Net Inference with Ultra-Low-Precision Arithmetic",
    "volume": "poster",
    "abstract": "Low-precision neural networks represent both weights and activations with few bits, drastically reducing the cost of multiplications. Meanwhile, these products are accumulated using high-precision (typically 32-bit) additions. Additions dominate the arithmetic complexity of inference in quantized (e.g., binary) nets, and high precision is needed to avoid overflow. To further optimize inference, we propose WrapNet, an architecture that adapts neural networks to use low-precision (8-bit) additions while achieving classification accuracy comparable to their 32-bit counterparts. We achieve resilience to low-precision accumulation by inserting a cyclic activation layer that makes results invariant to overflow. We demonstrate the efficacy of our approach using both software and hardware platforms",
    "checked": true,
    "id": "91df6b1c9c11d55593f90ff28b098402a98ac2a3",
    "semantic_title": "wrapnet: neural net inference with ultra-low-precision arithmetic",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=MtEE0CktZht": {
    "title": "Rank the Episodes: A Simple Approach for Exploration in Procedurally-Generated Environments",
    "volume": "poster",
    "abstract": "Exploration under sparse reward is a long-standing challenge of model-free reinforcement learning. The state-of-the-art methods address this challenge by introducing intrinsic rewards to encourage exploration in novel states or uncertain environment dynamics. Unfortunately, methods based on intrinsic rewards often fall short in procedurally-generated environments, where a different environment is generated in each episode so that the agent is not likely to visit the same state more than once. Motivated by how humans distinguish good exploration behaviors by looking into the entire episode, we introduce RAPID, a simple yet effective episode-level exploration method for procedurally-generated environments. RAPID regards each episode as a whole and gives an episodic exploration score from both per-episode and long-term views. Those highly scored episodes are treated as good exploration behaviors and are stored in a small ranking buffer. The agent then imitates the episodes in the buffer to reproduce the past good exploration behaviors. We demonstrate our method on several procedurally-generated MiniGrid environments, a first-person-view 3D Maze navigation task from MiniWorld, and several sparse MuJoCo tasks. The results show that RAPID significantly outperforms the state-of-the-art intrinsic reward strategies in terms of sample efficiency and final performance. The code is available at https://github.com/daochenzha/rapid",
    "checked": true,
    "id": "a188c3b58e71657ecfcddc37359aca51213e2187",
    "semantic_title": "rank the episodes: a simple approach for exploration in procedurally-generated environments",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=gwFTuzxJW0": {
    "title": "Stochastic Security: Adversarial Defense Using Long-Run Dynamics of Energy-Based Models",
    "volume": "poster",
    "abstract": "The vulnerability of deep networks to adversarial attacks is a central problem for deep learning from the perspective of both cognition and security. The current most successful defense method is to train a classifier using adversarial images created during learning. Another defense approach involves transformation or purification of the original input to remove adversarial signals before the image is classified. We focus on defending naturally-trained classifiers using Markov Chain Monte Carlo (MCMC) sampling with an Energy-Based Model (EBM) for adversarial purification. In contrast to adversarial training, our approach is intended to secure highly vulnerable pre-existing classifiers. To our knowledge, no prior defensive transformation is capable of securing naturally-trained classifiers, and our method is the first to validate a post-training defense approach that is distinct from current successful defenses which modify classifier training. The memoryless behavior of long-run MCMC sampling will eventually remove adversarial signals, while metastable behavior preserves consistent appearance of MCMC samples after many steps to allow accurate long-run prediction. Balancing these factors can lead to effective purification and robust classification. We evaluate adversarial defense with an EBM using the strongest known attacks against purification. Our contributions are 1) an improved method for training EBM's with realistic long-run MCMC samples for effective purification, 2) an Expectation-Over-Transformation (EOT) defense that resolves ambiguities for evaluating stochastic defenses and from which the EOT attack naturally follows, and 3) state-of-the-art adversarial defense for naturally-trained classifiers and competitive defense compared to adversarial training on CIFAR-10, SVHN, and CIFAR-100. Our code and pre-trained models are available at https://github.com/point0bar1/ebm-defense",
    "checked": true,
    "id": "d966edcc545f2b6a8ee2403da237eafc2330e048",
    "semantic_title": "stochastic security: adversarial defense using long-run dynamics of energy-based models",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=7wCBOfJ8hJM": {
    "title": "Nearest Neighbor Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "20d51f8e449b59c7e140f7a7eec9ab4d4d6f80ea",
    "semantic_title": "nearest neighbor machine translation",
    "citation_count": 291,
    "authors": []
  },
  "https://openreview.net/forum?id=yqPnIRhHtZv": {
    "title": "Learning Hyperbolic Representations of Topological Features",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5ac187d38060044c0add3df95e3cc05c9030d600",
    "semantic_title": "learning hyperbolic representations of topological features",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=8nl0k08uMi": {
    "title": "Selectivity considered harmful: evaluating the causal impact of class selectivity in DNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f60ce46af07db12e1540200ec0193ce1d8915d4e",
    "semantic_title": "selectivity considered harmful: evaluating the causal impact of class selectivity in dnns",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=IMPA6MndSXU": {
    "title": "Integrating Categorical Semantics into Unsupervised Domain Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "09b09a624cd126b78b3416c8839bc4e4325e1cfe",
    "semantic_title": "integrating categorical semantics into unsupervised domain translation",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=8wqCDnBmnrT": {
    "title": "Zero-shot Synthesis with Group-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1bf443a6a98dd0cbc22be3b8e82da4b60093e41d",
    "semantic_title": "zero-shot synthesis with group-supervised learning",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=g21u6nlbPzn": {
    "title": "VA-RED 2 : Video Adaptive Redundancy Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "e30b1ca378d245ec14a3dfcf09a493273882405c",
    "semantic_title": "va-red2: video adaptive redundancy reduction",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=YWtLZvLmud7": {
    "title": "BERTology Meets Biology: Interpreting Attention in Protein Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2b364917b0c51e91fcf2ab9c1d66a14ed4b44c03",
    "semantic_title": "bertology meets biology: interpreting attention in protein language models",
    "citation_count": 299,
    "authors": []
  },
  "https://openreview.net/forum?id=J8_GttYLFgr": {
    "title": "Trajectory Prediction using Equivariant Continuous Convolution",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "38e52bbcfb67029822c4284e53d15225b478b90f",
    "semantic_title": "trajectory prediction using equivariant continuous convolution",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=XOjv2HxIF6i": {
    "title": "Unsupervised Meta-Learning through Latent-Space Interpolation in Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "64bc52c8349cd62cc561f11c78cbb9b914064819",
    "semantic_title": "unsupervised meta-learning through latent-space interpolation in generative models",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=mEdwVCRJuX4": {
    "title": "Heteroskedastic and Imbalanced Deep Learning with Adaptive Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "92b46eeed09d9ff130901b97af38b55e4f4bdc2d",
    "semantic_title": "heteroskedastic and imbalanced deep learning with adaptive regularization",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=O9bnihsFfXU": {
    "title": "Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "63b4843f3789838331168315ea6fa406f0f973b1",
    "semantic_title": "implicit under-parameterization inhibits data-efficient deep reinforcement learning",
    "citation_count": 129,
    "authors": []
  },
  "https://openreview.net/forum?id=ESG-DMKQKsD": {
    "title": "Bowtie Networks: Generative Modeling for Joint Few-Shot Recognition and Novel-View Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "98297bece690c0460f6d01fbb1131389a36df049",
    "semantic_title": "bowtie networks: generative modeling for joint few-shot recognition and novel-view synthesis",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=c9-WeM-ceB": {
    "title": "Saliency is a Possible Red Herring When Diagnosing Poor Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "23b87f50e600efae796c7737a5c57ac1fe93afd3",
    "semantic_title": "saliency is a possible red herring when diagnosing poor generalization",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=Qm8UNVCFdh": {
    "title": "What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0944c6bd883bf3cf2d7d4f765ab5266dd1482595",
    "semantic_title": "what can you learn from your muscles? learning visual representation from human interactions",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=iaO86DUuKi": {
    "title": "Conservative Safety Critics for Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a334f9897a330abddf99cfec0b5a70f751e9497b",
    "semantic_title": "conservative safety critics for exploration",
    "citation_count": 143,
    "authors": []
  },
  "https://openreview.net/forum?id=4c0J6lwQ4_": {
    "title": "Multi-Time Attention Networks for Irregularly Sampled Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b940f6e62bb8bc64ea17331077e5c4d73191278c",
    "semantic_title": "multi-time attention networks for irregularly sampled time series",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=6DOZ8XNNfGN": {
    "title": "Graph Traversal with Tensor Functionals: A Meta-Algorithm for Scalable Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9fed4c8b88333f912ae217fbd0db0f864870d398",
    "semantic_title": "graph traversal with tensor functionals: a meta-algorithm for scalable learning",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=YmA86Zo-P_t": {
    "title": "What they do when in doubt: a study of inductive biases in seq2seq learners",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "032399b7fc693a9fc12bb26d6be8c02d77dd397a",
    "semantic_title": "what they do when in doubt: a study of inductive biases in seq2seq learners",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=YtMG5ex0ou": {
    "title": "Tomographic Auto-Encoder: Unsupervised Bayesian Recovery of Corrupted Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f8d73165f7c27e36283621117fb6ecaa51f73d06",
    "semantic_title": "tomographic auto-encoder: unsupervised bayesian recovery of corrupted data",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=V69LGwJ0lIN": {
    "title": "OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0a321a38ba98499f17a2423f84972de29a5b2e7f",
    "semantic_title": "opal: offline primitive discovery for accelerating offline reinforcement learning",
    "citation_count": 163,
    "authors": []
  },
  "https://openreview.net/forum?id=ZzwDy_wiWv": {
    "title": "Knowledge distillation via softmax regression representation learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3555f47e781e25cf6e1b80d0daf0c39ad1a2d705",
    "semantic_title": "knowledge distillation via softmax regression representation learning",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=X4y_10OX-hX": {
    "title": "Large Associative Memory Problem in Neurobiology and Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6a8092b98771b8157437e71e351ae1231bdd8259",
    "semantic_title": "large associative memory problem in neurobiology and machine learning",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=tHgJoMfy6nI": {
    "title": "Remembering for the Right Reasons: Explanations Reduce Catastrophic Forgetting",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2c826ded75942bf6e0cd9b6ae1f6db372ee92f27",
    "semantic_title": "remembering for the right reasons: explanations reduce catastrophic forgetting",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=O-6Pm_d_Q-": {
    "title": "Deep Networks and the Multiple Manifold Problem",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4d5bb81da39b39b8158f63a0babc3c8e24c730cc",
    "semantic_title": "deep networks and the multiple manifold problem",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=gLWj29369lW": {
    "title": "Interpreting Knowledge Graph Relation Representation from Word Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "366e9d016ec238d965f3c3c4674c6cba86760636",
    "semantic_title": "interpreting knowledge graph relation representation from word embeddings",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=MIDckA56aD": {
    "title": "Learning perturbation sets for robust machine learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "857b1c3f171afb3cdf9df23d23e5d0cdfaa83efb",
    "semantic_title": "learning perturbation sets for robust machine learning",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=ZK6vTvb84s": {
    "title": "A Trainable Optimal Transport Embedding for Feature Aggregation and its Relationship to Attention",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b8ca07fc7e7e70b6fd0f0b3b4921e4dabe11cd23",
    "semantic_title": "a trainable optimal transport embedding for feature aggregation and its relationship to attention",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=TTUVg6vkNjK": {
    "title": "RODE: Learning Roles to Decompose Multi-Agent Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5764095b0186a3fc3832c1052aa14996a5927edc",
    "semantic_title": "rode: learning roles to decompose multi-agent tasks",
    "citation_count": 202,
    "authors": []
  },
  "https://openreview.net/forum?id=4qR3coiNaIv": {
    "title": "Scalable Bayesian Inverse Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "03308bd92c5993f6828950aaea2f38f5f7d9cd8d",
    "semantic_title": "scalable bayesian inverse reinforcement learning",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=h0de3QWtGG": {
    "title": "Learning \"What-if\" Explanations for Sequential Decision-Making",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d96fe15cfff06c4661996e5f0bcc45618e759ef7",
    "semantic_title": "learning \"what-if\" explanations for sequential decision-making",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=-gfhS00XfKj": {
    "title": "Learning advanced mathematical computations from examples",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d9610589189e0821500516994dcee543a558b70c",
    "semantic_title": "learning advanced mathematical computations from examples",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=qkLMTphG5-h": {
    "title": "Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b6c4a3f913ee819afa49e115f9739fe31ecf13f5",
    "semantic_title": "repurposing pretrained models for robust out-of-domain few-shot learning",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=5jRVa89sZk": {
    "title": "Empirical Analysis of Unlabeled Entity Problem in Named Entity Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "95fceae74f4dbe582675a5bd6457702349939e72",
    "semantic_title": "empirical analysis of unlabeled entity problem in named entity recognition",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=eQe8DEWNN2W": {
    "title": "Calibration of Neural Networks using Splines",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "af2549305c2839e85367a07db10366670161afba",
    "semantic_title": "calibration of neural networks using splines",
    "citation_count": 112,
    "authors": []
  },
  "https://openreview.net/forum?id=17VnwXYZyhH": {
    "title": "Probing BERT in Hyperbolic Spaces",
    "volume": "poster",
    "abstract": "Recently, a variety of probing tasks are proposed to discover linguistic properties learned in contextualized word embeddings. Many of these works implicitly assume these embeddings lay in certain metric spaces, typically the Euclidean space. This work considers a family of geometrically special spaces, the hyperbolic spaces, that exhibit better inductive biases for hierarchical structures and may better reveal linguistic hierarchies encoded in contextualized representations. We introduce a $\\textit{Poincaré probe}$, a structural probe projecting these embeddings into a Poincaré subspace with explicitly defined hierarchies. We focus on two probing objectives: (a) dependency trees where the hierarchy is defined as head-dependent structures; (b) lexical sentiments where the hierarchy is defined as the polarity of words (positivity and negativity). We argue that a key desideratum of a probe is its sensitivity to the existence of linguistic structures. We apply our probes on BERT, a typical contextualized embedding model. In a syntactic subspace, our probe better recovers tree structures than Euclidean probes, revealing the possibility that the geometry of BERT syntax may not necessarily be Euclidean. In a sentiment subspace, we reveal two possible meta-embeddings for positive and negative sentiments and show how lexically-controlled contextualization would change the geometric localization of embeddings. We demonstrate the findings with our Poincaré probe via extensive experiments and visualization. Our results can be reproduced at https://github.com/FranxYao/PoincareProbe",
    "checked": true,
    "id": "4072a2333682941d23755e9b7e1e3a6d899683c6",
    "semantic_title": "probing bert in hyperbolic spaces",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=Zbc-ue9p_rE": {
    "title": "Refining Deep Generative Models via Discriminator Gradient Flow",
    "volume": "poster",
    "abstract": "Deep generative modeling has seen impressive advances in recent years, to the point where it is now commonplace to see simulated samples (e.g., images) that closely resemble real-world data. However, generation quality is generally inconsistent for any given model and can vary dramatically between samples. We introduce Discriminator Gradient $f$low (DG$f$low), a new technique that improves generated samples via the gradient flow of entropy-regularized $f$-divergences between the real and the generated data distributions. The gradient flow takes the form of a non-linear Fokker-Plank equation, which can be easily simulated by sampling from the equivalent McKean-Vlasov process. By refining inferior samples, our technique avoids wasteful sample rejection used by previous methods (DRS & MH-GAN). Compared to existing works that focus on specific GAN variants, we show our refinement approach can be applied to GANs with vector-valued critics and even other deep generative models such as VAEs and Normalizing Flows. Empirical results on multiple synthetic, image, and text datasets demonstrate that DG$f$low leads to significant improvement in the quality of generated samples for a variety of generative models, outperforming the state-of-the-art Discriminator Optimal Transport (DOT) and Discriminator Driven Latent Sampling (DDLS) methods",
    "checked": true,
    "id": "38a60a6fde1769f92062cf2295d3809c223abdd8",
    "semantic_title": "refining deep generative models via discriminator gradient flow",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=BtZhsSGNRNi": {
    "title": "Coping with Label Shift via Distributionally Robust Optimisation",
    "volume": "poster",
    "abstract": "The label shift problem refers to the supervised learning setting where the train and test label distributions do not match. Existing work addressing label shift usually assumes access to an unlabelled test sample. This sample may be used to estimate the test label distribution, and to then train a suitably re-weighted classifier. While approaches using this idea have proven effective, their scope is limited as it is not always feasible to access the target domain; further, they require repeated retraining if the model is to be deployed in multiple test environments. Can one instead learn a single classifier that is robust to arbitrary label shifts from a broad family? In this paper, we answer this question by proposing a model that minimises an objective based on distributionally robust optimisation (DRO). We then design and analyse a gradient descent-proximal mirror ascent algorithm tailored for large-scale problems to optimise the proposed objective. Finally, through experiments on CIFAR-100 and ImageNet, we show that our technique can significantly improve performance over a number of baselines in settings where label shift is present",
    "checked": true,
    "id": "8246f0d3799290be5ed47254f6b88b601fa98230",
    "semantic_title": "coping with label shift via distributionally robust optimisation",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=XAS3uKeFWj": {
    "title": "Variational State-Space Models for Localisation and Dense 3D Mapping in 6 DoF",
    "volume": "poster",
    "abstract": "We solve the problem of 6-DoF localisation and 3D dense reconstruction in spatial environments as approximate Bayesian inference in a deep state-space model. Our approach leverages both learning and domain knowledge from multiple-view geometry and rigid-body dynamics. This results in an expressive predictive model of the world, often missing in current state-of-the-art visual SLAM solutions. The combination of variational inference, neural networks and a differentiable raycaster ensures that our model is amenable to end-to-end gradient-based optimisation. We evaluate our approach on realistic unmanned aerial vehicle flight data, nearing the performance of state-of-the-art visual-inertial odometry systems. We demonstrate the applicability of the model to generative prediction and planning",
    "checked": true,
    "id": "32d88170812ea8295536a4ecb1976b94ce5622ec",
    "semantic_title": "variational state-space models for localisation and dense 3d mapping in 6 dof",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=bJxgv5C3sYc": {
    "title": "Few-Shot Bayesian Optimization with Deep Kernel Surrogates",
    "volume": "poster",
    "abstract": "Hyperparameter optimization (HPO) is a central pillar in the automation of machine learning solutions and is mainly performed via Bayesian optimization, where a parametric surrogate is learned to approximate the black box response function (e.g. validation error). Unfortunately, evaluating the response function is computationally intensive. As a remedy, earlier work emphasizes the need for transfer learning surrogates which learn to optimize hyperparameters for an algorithm from other tasks. In contrast to previous work, we propose to rethink HPO as a few-shot learning problem in which we train a shared deep surrogate model to quickly adapt (with few response evaluations) to the response function of a new task. We propose the use of a deep kernel network for a Gaussian process surrogate that is meta-learned in an end-to-end fashion in order to jointly approximate the response functions of a collection of training data sets. As a result, the novel few-shot optimization of our deep kernel surrogate leads to new state-of-the-art results at HPO compared to several recent methods on diverse metadata sets",
    "checked": true,
    "id": "aea3f03299ff0cfea9b394f5559aa1c173f9876f",
    "semantic_title": "few-shot bayesian optimization with deep kernel surrogates",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=T6AxtOaWydQ": {
    "title": "i -Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning",
    "volume": "poster",
    "abstract": "Contrastive representation learning has shown to be effective to learn representations from unlabeled data. However, much progress has been made in vision domains relying on data augmentations carefully designed using domain knowledge. In this work, we propose i-Mix, a simple yet effective domain-agnostic regularization strategy for improving contrastive representation learning. We cast contrastive learning as training a non-parametric classifier by assigning a unique virtual class to each data in a batch. Then, data instances are mixed in both the input and virtual label spaces, providing more augmented data during training. In experiments, we demonstrate that i-Mix consistently improves the quality of learned representations across domains, including image, speech, and tabular data. Furthermore, we confirm its regularization effect via extensive ablation studies across model and dataset sizes. The code is available at https://github.com/kibok90/imix",
    "checked": true,
    "id": "c709a3ac669aa334d6a0e9544f9191c5516da8a2",
    "semantic_title": "i-mix: a domain-agnostic strategy for contrastive representation learning",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=bM4Iqfg8M2k": {
    "title": "Graph Information Bottleneck for Subgraph Recognition",
    "volume": "poster",
    "abstract": "Given the input graph and its label/property, several key problems of graph learning, such as finding interpretable subgraphs, graph denoising and graph compression, can be attributed to the fundamental problem of recognizing a subgraph of the original one. This subgraph shall be as informative as possible, yet contains less redundant and noisy structure. This problem setting is closely related to the well-known information bottleneck (IB) principle, which, however, has less been studied for the irregular graph data and graph neural networks (GNNs). In this paper, we propose a framework of Graph Information Bottleneck (GIB) for the subgraph recognition problem in deep graph learning. Under this framework, one can recognize the maximally informative yet compressive subgraph, named IB-subgraph. However, the GIB objective is notoriously hard to optimize, mostly due to the intractability of the mutual information of irregular graph data and the unstable optimization process. In order to tackle these challenges, we propose: i) a GIB objective based-on a mutual information estimator for the irregular graph data; ii) a bi-level optimization scheme to maximize the GIB objective; iii) a connectivity loss to stabilize the optimization process. We evaluate the properties of the IB-subgraph in three application scenarios: improvement of graph classification, graph interpretation and graph denoising. Extensive experiments demonstrate that the information-theoretic IB-subgraph enjoys superior graph properties",
    "checked": true,
    "id": "1a901533b09269bafa1222ad5f2f5eb0279915ac",
    "semantic_title": "graph information bottleneck for subgraph recognition",
    "citation_count": 161,
    "authors": []
  },
  "https://openreview.net/forum?id=09-528y2Fgf": {
    "title": "Rethinking Positional Encoding in Language Pre-training",
    "volume": "poster",
    "abstract": "In this work, we investigate the positional encoding methods used in language pre-training (e.g., BERT) and identify several problems in the existing formulations. First, we show that in the absolute positional encoding, the addition operation applied on positional embeddings and word embeddings brings mixed correlations between the two heterogeneous information resources. It may bring unnecessary randomness in the attention and further limit the expressiveness of the model. Second, we question whether treating the position of the symbol \\texttt{[CLS]} the same as other words is a reasonable design, considering its special role (the representation of the entire sentence) in the downstream tasks. Motivated from above analysis, we propose a new positional encoding method called \\textbf{T}ransformer with \\textbf{U}ntied \\textbf{P}ositional \\textbf{E}ncoding (TUPE). In the self-attention module, TUPE computes the word contextual correlation and positional correlation separately with different parameterizations and then adds them together. This design removes the mixed and noisy correlations over heterogeneous embeddings and offers more expressiveness by using different projection matrices. Furthermore, TUPE unties the \\texttt{[CLS]} symbol from other positions, making it easier to capture information from all positions. Extensive experiments and ablation studies on GLUE benchmark demonstrate the effectiveness of the proposed method. Codes and models are released at \\url{https://github.com/guolinke/TUPE}",
    "checked": true,
    "id": "8256f48f759cf85044db251cc512f965834945b3",
    "semantic_title": "rethinking positional encoding in language pre-training",
    "citation_count": 303,
    "authors": []
  },
  "https://openreview.net/forum?id=6k7VdojAIK": {
    "title": "Practical Massively Parallel Monte-Carlo Tree Search Applied to Molecular Design",
    "volume": "poster",
    "abstract": "It is common practice to use large computational resources to train neural networks, known from many examples, such as reinforcement learning applications. However, while massively parallel computing is often used for training models, it is rarely used to search solutions for combinatorial optimization problems. This paper proposes a novel massively parallel Monte-Carlo Tree Search (MP-MCTS) algorithm that works efficiently for a 1,000 worker scale on a distributed memory environment using multiple compute nodes and applies it to molecular design. This paper is the first work that applies distributed MCTS to a real-world and non-game problem. Existing works on large-scale parallel MCTS show efficient scalability in terms of the number of rollouts up to 100 workers. Still, they suffer from the degradation in the quality of the solutions. MP-MCTS maintains the search quality at a larger scale. By running MP-MCTS on 256 CPU cores for only 10 minutes, we obtained candidate molecules with similar scores to non-parallel MCTS running for 42 hours. Moreover, our results based on parallel MCTS (combined with a simple RNN model) significantly outperform existing state-of-the-art work. Our method is generic and is expected to speed up other applications of MCTS",
    "checked": true,
    "id": "89b26dec9c4acb601cc9ae99737a998d8cae955e",
    "semantic_title": "practical massively parallel monte-carlo tree search applied to molecular design",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=S724o4_WB3": {
    "title": "When does preconditioning help or hurt generalization?",
    "volume": "poster",
    "abstract": "While second order optimizers such as natural gradient descent (NGD) often speed up optimization, their effect on generalization has been called into question. This work presents a more nuanced view on how the \\textit{implicit bias} of optimizers affects the comparison of generalization properties. We provide an exact asymptotic bias-variance decomposition of the generalization error of preconditioned ridgeless regression in the overparameterized regime, and consider the inverse population Fisher information matrix (used in NGD) as a particular example. We determine the optimal preconditioner $\\boldsymbol{P}$ for both the bias and variance, and find that the relative generalization performance of different optimizers depends on label noise and ``shape'' of the signal (true parameters): when the labels are noisy, the model is misspecified, or the signal is misaligned with the features, NGD can achieve lower risk; conversely, GD generalizes better under clean labels, a well-specified model, or aligned signal. Based on this analysis, we discuss several approaches to manage the bias-variance tradeoff, and the potential benefit of interpolating between first- and second-order updates. We then extend our analysis to regression in the reproducing kernel Hilbert space and demonstrate that preconditioning can lead to more efficient decrease in the population risk. Lastly, we empirically compare the generalization error of first- and second-order optimizers in neural network experiments, and observe robust trends matching our theoretical analysis",
    "checked": true,
    "id": "af4fe7104ef94619920df684b5731ebfd58ddca9",
    "semantic_title": "when does preconditioning help or hurt generalization?",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=JoCR4h9O3Ew": {
    "title": "ARMOURED: Adversarially Robust MOdels using Unlabeled data by REgularizing Diversity",
    "volume": "poster",
    "abstract": "Adversarial attacks pose a major challenge for modern deep neural networks. Recent advancements show that adversarially robust generalization requires a large amount of labeled data for training. If annotation becomes a burden, can unlabeled data help bridge the gap? In this paper, we propose ARMOURED, an adversarially robust training method based on semi-supervised learning that consists of two components. The first component applies multi-view learning to simultaneously optimize multiple independent networks and utilizes unlabeled data to enforce labeling consistency. The second component reduces adversarial transferability among the networks via diversity regularizers inspired by determinantal point processes and entropy maximization. Experimental results show that under small perturbation budgets, ARMOURED is robust against strong adaptive adversaries. Notably, ARMOURED does not rely on generating adversarial samples during training. When used in combination with adversarial training, ARMOURED yields competitive performance with the state-of-the-art adversarially-robust benchmarks on SVHN and outperforms them on CIFAR-10, while offering higher clean accuracy",
    "checked": true,
    "id": "2bc747f61085bcd4c0cfa868ac6cfc92b0c1ce37",
    "semantic_title": "armoured: adversarially robust models using unlabeled data by regularizing diversity",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=aD1_5zowqV": {
    "title": "Learning Energy-Based Generative Models via Coarse-to-Fine Expanding and Sampling",
    "volume": "poster",
    "abstract": "Energy-based models (EBMs) parameterized by neural networks can be trained by the Markov chain Monte Carlo (MCMC) sampling-based maximum likelihood estimation. Despite the recent significant success of EBMs in image generation, the current approaches to train EBMs are unstable and have difficulty synthesizing diverse and high-fidelity images. In this paper, we propose to train EBMs via a multistage coarse-to-fine expanding and sampling strategy, which starts with learning a coarse-level EBM from images at low resolution and then gradually transits to learn a finer-level EBM from images at higher resolution by expanding the energy function as the learning progresses. The proposed framework is computationally efficient with smooth learning and sampling. It achieves the best performance on image generation amongst all EBMs and is the first successful EBM to synthesize high-fidelity images at $512\\times512$ resolution. It can also be useful for image restoration and out-of-distribution detection. Lastly, the proposed framework is further generalized to the one-sided unsupervised image-to-image translation and beats baseline methods in terms of model size and training budget. We also present a gradient-based generative saliency method to interpret the translation dynamics",
    "checked": true,
    "id": "4f2f0869c4698afc71fe983e457aacc2cc234501",
    "semantic_title": "learning energy-based generative models via coarse-to-fine expanding and sampling",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=U_mat0b9iv": {
    "title": "Multi-Prize Lottery Ticket Hypothesis: Finding Accurate Binary Neural Networks by Pruning A Randomly Weighted Network",
    "volume": "poster",
    "abstract": "Recently, Frankle & Carbin (2019) demonstrated that randomly-initialized dense networks contain subnetworks that once found can be trained to reach test accuracy comparable to the trained dense network. However, finding these high performing trainable subnetworks is expensive, requiring iterative process of training and pruning weights. In this paper, we propose (and prove) a stronger Multi-Prize Lottery Ticket Hypothesis: A sufficiently over-parameterized neural network with random weights contains several subnetworks (winning tickets) that (a) have comparable accuracy to a dense target network with learned weights (prize 1), (b) do not require any further training to achieve prize 1 (prize 2), and (c) is robust to extreme forms of quantization (i.e., binary weights and/or activation) (prize 3). This provides a new paradigm for learning compact yet highly accurate binary neural networks simply by pruning and quantizing randomly weighted full precision neural networks. We also propose an algorithm for finding multi-prize tickets (MPTs) and test it by performing a series of experiments on CIFAR-10 and ImageNet datasets. Empirical results indicate that as models grow deeper and wider, multi-prize tickets start to reach similar (and sometimes even higher) test accuracy compared to their significantly larger and full-precision counterparts that have been weight-trained. Without ever updating the weight values, our MPTs-1/32 not only set new binary weight network state-of-the-art (SOTA) Top-1 accuracy -- 94.8% on CIFAR-10 and 74.03% on ImageNet -- but also outperform their full-precision counterparts by 1.78% and 0.76%, respectively. Further, our MPT-1/1 achieves SOTA Top-1 accuracy (91.9%) for binary neural networks on CIFAR-10. Code and pre-trained models are available at: https://github.com/chrundle/biprop",
    "checked": true,
    "id": "a52d17eac54b145cbc2b2c823f32b9e76be2595d",
    "semantic_title": "multi-prize lottery ticket hypothesis: finding accurate binary neural networks by pruning a randomly weighted network",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=cTbIjyrUVwJ": {
    "title": "Learning Accurate Entropy Model with Global Reference for Image Compression",
    "volume": "poster",
    "abstract": "In recent deep image compression neural networks, the entropy model plays a critical role in estimating the prior distribution of deep image encodings. Existing methods combine hyperprior with local context in the entropy estimation function. This greatly limits their performance due to the absence of a global vision. In this work, we propose a novel Global Reference Model for image compression to effectively leverage both the local and the global context information, leading to an enhanced compression rate. The proposed method scans decoded latents and then finds the most relevant latent to assist the distribution estimating of the current latent. A by-product of this work is the innovation of a mean-shifting GDN module that further improves the performance. Experimental results demonstrate that the proposed model outperforms the rate-distortion performance of most of the state-of-the-art methods in the industry",
    "checked": true,
    "id": "68a0d2855416a95cac648d63424c493ed1fb8b1a",
    "semantic_title": "learning accurate entropy model with global reference for image compression",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=te7PVH1sPxJ": {
    "title": "Convex Potential Flows: Universal Probability Distributions with Optimal Transport and Convex Optimization",
    "volume": "poster",
    "abstract": "Flow-based models are powerful tools for designing probabilistic models with tractable density. This paper introduces Convex Potential Flows (CP-Flow), a natural and efficient parameterization of invertible models inspired by the optimal transport (OT) theory. CP-Flows are the gradient map of a strongly convex neural potential function. The convexity implies invertibility and allows us to resort to convex optimization to solve the convex conjugate for efficient inversion. To enable maximum likelihood training, we derive a new gradient estimator of the log-determinant of the Jacobian, which involves solving an inverse-Hessian vector product using the conjugate gradient method. The gradient estimator has constant-memory cost, and can be made effectively unbiased by reducing the error tolerance level of the convex optimization routine. Theoretically, we prove that CP-Flows are universal density approximators and are optimal in the OT sense. Our empirical results show that CP-Flow performs competitively on standard benchmarks of density estimation and variational inference",
    "checked": true,
    "id": "7156cef2a8f1425928224bd7ff39b6958b5fda8c",
    "semantic_title": "convex potential flows: universal probability distributions with optimal transport and convex optimization",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=6t_dLShIUyZ": {
    "title": "Greedy-GQ with Variance Reduction: Finite-time Analysis and Improved Complexity",
    "volume": "poster",
    "abstract": "Greedy-GQ is a value-based reinforcement learning (RL) algorithm for optimal control. Recently, the finite-time analysis of Greedy-GQ has been developed under linear function approximation and Markovian sampling, and the algorithm is shown to achieve an $\\epsilon$-stationary point with a sample complexity in the order of $\\mathcal{O}(\\epsilon^{-3})$. Such a high sample complexity is due to the large variance induced by the Markovian samples. In this paper, we propose a variance-reduced Greedy-GQ (VR-Greedy-GQ) algorithm for off-policy optimal control. In particular, the algorithm applies the SVRG-based variance reduction scheme to reduce the stochastic variance of the two time-scale updates. We study the finite-time convergence of VR-Greedy-GQ under linear function approximation and Markovian sampling and show that the algorithm achieves a much smaller bias and variance error than the original Greedy-GQ. In particular, we prove that VR-Greedy-GQ achieves an improved sample complexity that is in the order of $\\mathcal{O}(\\epsilon^{-2})$. We further compare the performance of VR-Greedy-GQ with that of Greedy-GQ in various RL experiments to corroborate our theoretical findings",
    "checked": true,
    "id": "10b7c0e3fb4174fb4400c0e49e6e73908e3c8a45",
    "semantic_title": "greedy-gq with variance reduction: finite-time analysis and improved complexity",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=cP5IcoAkfKa": {
    "title": "Large Batch Simulation for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "We accelerate deep reinforcement learning-based training in visually complex 3D environments by two orders of magnitude over prior work, realizing end-to-end training speeds of over 19,000 frames of experience per second on a single GPU and up to 72,000 frames per second on a single eight-GPU machine. The key idea of our approach is to design a 3D renderer and embodied navigation simulator around the principle of \"batch simulation\": accepting and executing large batches of requests simultaneously. Beyond exposing large amounts of work at once, batch simulation allows implementations to amortize in-memory storage of scene assets, rendering work, data loading, and synchronization costs across many simulation requests, dramatically improving the number of simulated agents per GPU and overall simulation throughput. To balance DNN inference and training costs with faster simulation, we also build a computationally efficient policy DNN that maintains high task performance, and modify training algorithms to maintain sample efficiency when training with large mini-batches. By combining batch simulation and DNN performance optimizations, we demonstrate that PointGoal navigation agents can be trained in complex 3D environments on a single GPU in 1.5 days to 97% of the accuracy of agents trained on a prior state-of-the-art system using a 64-GPU cluster over three days. We provide open-source reference implementations of our batch 3D renderer and simulator to facilitate incorporation of these ideas into RL systems",
    "checked": true,
    "id": "543cd616f7f43d76b18cd6beae4f0a2ba5da9b61",
    "semantic_title": "large batch simulation for deep reinforcement learning",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=MaZFq7bJif7": {
    "title": "Hopper: Multi-hop Transformer for Spatiotemporal Reasoning",
    "volume": "poster",
    "abstract": "This paper considers the problem of spatiotemporal object-centric reasoning in videos. Central to our approach is the notion of object permanence, i.e., the ability to reason about the location of objects as they move through the video while being occluded, contained or carried by other objects. Existing deep learning based approaches often suffer from spatiotemporal biases when applied to video reasoning problems. We propose Hopper, which uses a Multi-hop Transformer for reasoning object permanence in videos. Given a video and a localization query, Hopper reasons over image and object tracks to automatically hop over critical frames in an iterative fashion to predict the final position of the object of interest. We demonstrate the effectiveness of using a contrastive loss to reduce spatiotemporal biases. We evaluate over CATER dataset and find that Hopper achieves 73.2% Top-1 accuracy using just 1 FPS by hopping through just a few critical frames. We also demonstrate Hopper can perform long-term reasoning by building a CATER-h dataset that requires multi-step reasoning to localize objects of interest correctly",
    "checked": true,
    "id": "cab65b1c4b3dbddee7be6a7a9f764629b020882e",
    "semantic_title": "hopper: multi-hop transformer for spatiotemporal reasoning",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=fmtSg8591Q": {
    "title": "Efficient Reinforcement Learning in Factored MDPs with Application to Constrained RL",
    "volume": "poster",
    "abstract": "Reinforcement learning (RL) in episodic, factored Markov decision processes (FMDPs) is studied. We propose an algorithm called FMDP-BF, which leverages the factorization structure of FMDP. The regret of FMDP-BF is shown to be exponentially smaller than that of optimal algorithms designed for non-factored MDPs, and improves on the best previous result for FMDPs~\\citep{osband2014near} by a factor of $\\sqrt{nH|\\mathcal{S}_i|}$, where $|\\mathcal{S}_i|$ is the cardinality of the factored state subspace, $H$ is the planning horizon and $n$ is the number of factored transition. To show the optimality of our bounds, we also provide a lower bound for FMDP, which indicates that our algorithm is near-optimal w.r.t. timestep $T$, horizon $H$ and factored state-action subspace cardinality. Finally, as an application, we study a new formulation of constrained RL, known as RL with knapsack constraints (RLwK), and provides the first sample-efficient algorithm based on FMDP-BF",
    "checked": true,
    "id": "e4c30f8e2fc6577fc04efbe8f14a557869a19fd4",
    "semantic_title": "efficient reinforcement learning in factored mdps with application to constrained rl",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=MJIve1zgR_": {
    "title": "Unbiased Teacher for Semi-Supervised Object Detection",
    "volume": "poster",
    "abstract": "Semi-supervised learning, i.e., training networks with both labeled and unlabeled data, has made significant progress recently. However, existing works have primarily focused on image classification tasks and neglected object detection which requires more annotation effort. In this work, we revisit the Semi-Supervised Object Detection (SS-OD) and identify the pseudo-labeling bias issue in SS-OD. To address this, we introduce Unbiased Teacher, a simple yet effective approach that jointly trains a student and a gradually progressing teacher in a mutually-beneficial manner. Together with a class-balance loss to downweight overly confident pseudo-labels, Unbiased Teacher consistently improved state-of-the-art methods by significant margins on COCO-standard, COCO-additional, and VOC datasets. Specifically, Unbiased Teacher achieves 6.8 absolute mAP improvements against state-of-the-art method when using 1% of labeled data on MS-COCO, achieves around 10 mAP improvements against the supervised baseline when using only 0.5, 1, 2% of labeled data on MS-COCO",
    "checked": true,
    "id": "fd9d3f4de3e340137011ae470aab7591a62b6e38",
    "semantic_title": "unbiased teacher for semi-supervised object detection",
    "citation_count": 499,
    "authors": []
  },
  "https://openreview.net/forum?id=D3PcGLdMx0": {
    "title": "MELR: Meta-Learning via Modeling Episode-Level Relationships for Few-Shot Learning",
    "volume": "poster",
    "abstract": "Most recent few-shot learning (FSL) approaches are based on episodic training whereby each episode samples few training instances (shots) per class to imitate the test condition. However, this strict adhering to test condition has a negative side effect, that is, the trained model is susceptible to the poor sampling of few shots. In this work, for the first time, this problem is addressed by exploiting inter-episode relationships. Specifically, a novel meta-learning via modeling episode-level relationships (MELR) framework is proposed. By sampling two episodes containing the same set of classes for meta-training, MELR is designed to ensure that the meta-learned model is robust against the presence of poorly-sampled shots in the meta-test stage. This is achieved through two key components: (1) a Cross-Episode Attention Module (CEAM) to improve the ability of alleviating the effects of poorly-sampled shots, and (2) a Cross-Episode Consistency Regularization (CECR) to enforce that the two classifiers learned from the two episodes are consistent even when there are unrepresentative instances. Extensive experiments for non-transductive standard FSL on two benchmarks show that our MELR achieves 1.0%-5.0% improvements over the baseline (i.e., ProtoNet) used for FSL in our model and outperforms the latest competitors under the same settings",
    "checked": true,
    "id": "53bd17596ecad58a85708cc1775bbbd4ec62836d",
    "semantic_title": "melr: meta-learning via modeling episode-level relationships for few-shot learning",
    "citation_count": 104,
    "authors": []
  },
  "https://openreview.net/forum?id=6BRLOfrMhW": {
    "title": "Partitioned Learned Bloom Filters",
    "volume": "poster",
    "abstract": "Bloom filters are space-efficient probabilistic data structures that are used to test whether an element is a member of a set, and may return false positives. Recently, variations referred to as learned Bloom filters were developed that can provide improved performance in terms of the rate of false positives, by using a learned model for the represented set. However, previous methods for learned Bloom filters do not take full advantage of the learned model. Here we show how to frame the problem of optimal model utilization as an optimization problem, and using our framework derive algorithms that can achieve near-optimal performance in many cases",
    "checked": true,
    "id": "e7a7ce163a286bc826bd9a10cdbca3777b4fe149",
    "semantic_title": "partitioned learned bloom filter",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=AAes_3W-2z": {
    "title": "Wasserstein Embedding for Graph Learning",
    "volume": "poster",
    "abstract": "We present Wasserstein Embedding for Graph Learning (WEGL), a novel and fast framework for embedding entire graphs in a vector space, in which various machine learning models are applicable for graph-level prediction tasks. We leverage new insights on defining similarity between graphs as a function of the similarity between their node embedding distributions. Specifically, we use the Wasserstein distance to measure the dissimilarity between node embeddings of different graphs. Unlike prior work, we avoid pairwise calculation of distances between graphs and reduce the computational complexity from quadratic to linear in the number of graphs. WEGL calculates Monge maps from a reference distribution to each node embedding and, based on these maps, creates a fixed-sized vector representation of the graph. We evaluate our new graph embedding approach on various benchmark graph-property prediction tasks, showing state-of-the-art classification performance while having superior computational efficiency. The code is available at https://github.com/navid-naderi/WEGL",
    "checked": true,
    "id": "463f490d3bded6e527b0838da8495ed6441da25a",
    "semantic_title": "wasserstein embedding for graph learning",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=MxaY4FzOTa": {
    "title": "High-Capacity Expert Binary Networks",
    "volume": "poster",
    "abstract": "Network binarization is a promising hardware-aware direction for creating efficient deep models. Despite its memory and computational advantages, reducing the accuracy gap between binary models and their real-valued counterparts remains an unsolved challenging research problem. To this end, we make the following 3 contributions: (a) To increase model capacity, we propose Expert Binary Convolution, which, for the first time, tailors conditional computing to binary networks by learning to select one data-specific expert binary filter at a time conditioned on input features. (b) To increase representation capacity, we propose to address the inherent information bottleneck in binary networks by introducing an efficient width expansion mechanism which keeps the binary operations within the same budget. (c) To improve network design, we propose a principled binary network growth mechanism that unveils a set of network topologies of favorable properties. Overall, our method improves upon prior work, with no increase in computational cost, by $\\sim6 \\%$, reaching a groundbreaking $\\sim 71\\%$ on ImageNet classification. Code will be made available $\\href{https://www.adrianbulat.com/binary-networks}{here}$",
    "checked": true,
    "id": "06c4ba737d5bae10de2476ccc902bec6fb76cfc2",
    "semantic_title": "high-capacity expert binary networks",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=Cz3dbFm5u-": {
    "title": "SAFENet: A Secure, Accurate and Fast Neural Network Inference",
    "volume": "poster",
    "abstract": "The advances in neural networks have driven many companies to provide prediction services to users in a wide range of applications. However, current prediction systems raise privacy concerns regarding the user's private data. A cryptographic neural network inference service is an efficient way to allow two parties to execute neural network inference without revealing either party's data or model. Nevertheless, existing cryptographic neural network inference services suffer from huge running latency; in particular, the latency of communication-expensive cryptographic activation function is 3 orders of magnitude higher than plaintext-domain activation function. And activations are the necessary components of the modern neural networks. Therefore, slow cryptographic activation has become the primary obstacle of efficient cryptographic inference. In this paper, we propose a new technique, called SAFENet, to enable a Secure, Accurate and Fast nEural Network inference service. To speedup secure inference and guarantee inference accuracy, SAFENet includes channel-wise activation approximation with multiple-degree options. This is implemented by keeping the most useful activation channels and replacing the remaining, less useful, channels with various-degree polynomials. SAFENet also supports mixed-precision activation approximation by automatically assigning different replacement ratios to various layer; further increasing the approximation ratio and reducing inference latency. Our experimental results show SAFENet obtains the state-of-the-art inference latency and performance, reducing latency by $38\\% \\sim 61\\%$ or improving accuracy by $1.8\\% \\sim 4\\%$ over prior techniques on various encrypted datasets",
    "checked": true,
    "id": "be942d0c5640ceab1b7e72e106af413b0f842a28",
    "semantic_title": "safenet: a secure, accurate and fast neural network inference",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=Gu5WqN9J3Fn": {
    "title": "Learning Manifold Patch-Based Representations of Man-Made Shapes",
    "volume": "poster",
    "abstract": "Choosing the right representation for geometry is crucial for making 3D models compatible with existing applications. Focusing on piecewise-smooth man-made shapes, we propose a new representation that is usable in conventional CAD modeling pipelines and can also be learned by deep neural networks. We demonstrate its benefits by applying it to the task of sketch-based modeling. Given a raster image, our system infers a set of parametric surfaces that realize the input in 3D. To capture piecewise smooth geometry, we learn a special shape representation: a deformable parametric template composed of Coons patches. Naively training such a system, however, is hampered by non-manifold artifacts in the parametric shapes and by a lack of data. To address this, we introduce loss functions that bias the network to output non-self-intersecting shapes and implement them as part of a fully self-supervised system, automatically generating both shape templates and synthetic training data. We develop a testbed for sketch-based modeling, demonstrate shape interpolation, and provide comparison to related work",
    "checked": true,
    "id": "35c72580d82e7b8a16d4f452a6e44f401570c1d3",
    "semantic_title": "learning manifold patch-based representations of man-made shapes",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=-IXhmY16R3M": {
    "title": "Universal approximation power of deep residual neural networks via nonlinear control theory",
    "volume": "poster",
    "abstract": "In this paper, we explain the universal approximation capabilities of deep residual neural networks through geometric nonlinear control. Inspired by recent work establishing links between residual networks and control systems, we provide a general sufficient condition for a residual network to have the power of universal approximation by asking the activation function, or one of its derivatives, to satisfy a quadratic differential equation. Many activation functions used in practice satisfy this assumption, exactly or approximately, and we show this property to be sufficient for an adequately deep neural network with $n+1$ neurons per layer to approximate arbitrarily well, on a compact set and with respect to the supremum norm, any continuous function from $\\mathbb{R}^n$ to $\\mathbb{R}^n$. We further show this result to hold for very simple architectures for which the weights only need to assume two values. The first key technical contribution consists of relating the universal approximation problem to controllability of an ensemble of control systems corresponding to a residual network and to leverage classical Lie algebraic techniques to characterize controllability. The second technical contribution is to identify monotonicity as the bridge between controllability of finite ensembles and uniform approximability on compact sets",
    "checked": true,
    "id": "ac4d9ff742c5b778934e3eaee1c3cf9ed6c82232",
    "semantic_title": "universal approximation power of deep residual neural networks via nonlinear control theory",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=kW_zpEmMLdP": {
    "title": "Learning Neural Event Functions for Ordinary Differential Equations",
    "volume": "poster",
    "abstract": "The existing Neural ODE formulation relies on an explicit knowledge of the termination time. We extend Neural ODEs to implicitly defined termination criteria modeled by neural event functions, which can be chained together and differentiated through. Neural Event ODEs are capable of modeling discrete and instantaneous changes in a continuous-time system, without prior knowledge of when these changes should occur or how many such changes should exist. We test our approach in modeling hybrid discrete- and continuous- systems such as switching dynamical systems and collision in multi-body systems, and we propose simulation-based training of point processes with applications in discrete control",
    "checked": false,
    "id": "2fdcad1cfaca5f42634cc3f5c0bfd4ec0fa3716d",
    "semantic_title": "transfer learning using neural ordinary differential equations",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=XQQA6-So14": {
    "title": "Neural Spatio-Temporal Point Processes",
    "volume": "poster",
    "abstract": "We propose a new class of parameterizations for spatio-temporal point processes which leverage Neural ODEs as a computational method and enable flexible, high-fidelity models of discrete events that are localized in continuous time and space. Central to our approach is a combination of continuous-time neural networks with two novel neural architectures, \\ie, Jump and Attentive Continuous-time Normalizing Flows. This approach allows us to learn complex distributions for both the spatial and temporal domain and to condition non-trivially on the observed event history. We validate our models on data sets from a wide variety of contexts such as seismology, epidemiology, urban mobility, and neuroscience",
    "checked": true,
    "id": "289322a72c5693f5942054019a8e90bd3adebd94",
    "semantic_title": "neural spatio-temporal point processes",
    "citation_count": 106,
    "authors": []
  },
  "https://openreview.net/forum?id=LVotkZmYyDi": {
    "title": "Proximal Gradient Descent-Ascent: Variable Convergence under KŁ Geometry",
    "volume": "poster",
    "abstract": "The gradient descent-ascent (GDA) algorithm has been widely applied to solve minimax optimization problems. In order to achieve convergent policy parameters for minimax optimization, it is important that GDA generates convergent variable sequences rather than convergent sequences of function value or gradient norm. However, the variable convergence of GDA has been proved only under convexity geometries, and it is lack of understanding in general nonconvex minimax optimization. This paper fills such a gap by studying the convergence of a more general proximal-GDA for regularized nonconvex-strongly-concave minimax optimization. Specifically, we show that proximal-GDA admits a novel Lyapunov function, which monotonically decreases in the minimax optimization process and drives the variable sequences to a critical point. By leveraging this Lyapunov function and the KL geometry that parameterizes the local geometries of general nonconvex functions, we formally establish the variable convergence of proximal-GDA to a certain critical point $x^*$, i.e., $x_t\\to x^*, y_t\\to y^*(x^*)$. Furthermore, over the full spectrum of the KL-parameterized geometry, we show that proximal-GDA achieves different types of convergence rates ranging from sublinear convergence up to finite-step convergence, depending on the geometry associated with the KL parameter. This is the first theoretical result on the variable convergence for nonconvex minimax optimization",
    "checked": true,
    "id": "619713fd242ad771e788a183b77285cba9dd143b",
    "semantic_title": "proximal gradient descent-ascent: variable convergence under kł geometry",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=n6jl7fLxrP": {
    "title": "Adaptive Universal Generalized PageRank Graph Neural Network",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0ee0801ba010a441403f9ed666ef9bf006b3aa07",
    "semantic_title": "adaptive universal generalized pagerank graph neural network",
    "citation_count": 766,
    "authors": []
  },
  "https://openreview.net/forum?id=MmCRswl1UYl": {
    "title": "Open Question Answering over Tables and Text",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "93d3e45395117e21214d404c8753b578c29266d1",
    "semantic_title": "open question answering over tables and text",
    "citation_count": 192,
    "authors": []
  },
  "https://openreview.net/forum?id=RovX-uQ1Hua": {
    "title": "Text Generation by Learning from Demonstrations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7b20d003dcde3106ed8f178e0083963775d7c15a",
    "semantic_title": "text generation by learning from demonstrations",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=K5YasWXZT3O": {
    "title": "Tilted Empirical Risk Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1f6de95137e96872274eedae1beb1bd55f03c57a",
    "semantic_title": "tilted empirical risk minimization",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=pGIHq1m7PU": {
    "title": "Explainable Subgraph Reasoning for Forecasting on Temporal Knowledge Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "08ede1cbadd5c631f93c0f952ac7ca99605d8a21",
    "semantic_title": "explainable subgraph reasoning for forecasting on temporal knowledge graphs",
    "citation_count": 165,
    "authors": []
  },
  "https://openreview.net/forum?id=ufZN2-aehFa": {
    "title": "Bayesian Context Aggregation for Neural Processes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5d2726260d6e52f3d9443b94cec3c116d5dec3cb",
    "semantic_title": "bayesian context aggregation for neural processes",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=q-cnWaaoUTH": {
    "title": "Conformation-Guided Molecular Representation with Hamiltonian Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "69bab843432803a8f69ffa7c33779f205e843841",
    "semantic_title": "hamnet: conformation-guided molecular representation with hamiltonian neural networks",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=ETBc_MIMgoX": {
    "title": "Learning with AMIGo: Adversarially Motivated Intrinsic Goals",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "df8a0bed6d685fee9c5ad418f4834e9537878de2",
    "semantic_title": "learning with amigo: adversarially motivated intrinsic goals",
    "citation_count": 129,
    "authors": []
  },
  "https://openreview.net/forum?id=dV19Yyi1fS3": {
    "title": "Training with Quantization Noise for Extreme Model Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0171ad4cc87cc7db25b4ec3169e293eed9a13b39",
    "semantic_title": "training with quantization noise for extreme model compression",
    "citation_count": 248,
    "authors": []
  },
  "https://openreview.net/forum?id=Jacdvfjicf7": {
    "title": "Interpreting and Boosting Dropout from a Game-Theoretic View",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9017fba9c4268dabc15e4bd8a99c6992625f9585",
    "semantic_title": "interpreting and boosting dropout from a game-theoretic view",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=DILxQP08O3B": {
    "title": "VTNet: Visual Transformer Network for Object Goal Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bcc16c38e479a2351c1ce4ec982b97353cb0d5d0",
    "semantic_title": "vtnet: visual transformer network for object goal navigation",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=QO9-y8also-": {
    "title": "Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ad2dc971f3712b055c65e3f93cf15cabf2b6c951",
    "semantic_title": "exemplary natural images explain cnn activations better than state-of-the-art feature visualization",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=AICNpd8ke-m": {
    "title": "Multi-Class Uncertainty Calibration via Mutual Information Maximization-based Binning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ecafa739f42dea74c23d7a3bb4ab613aeb86d1aa",
    "semantic_title": "multi-class uncertainty calibration via mutual information maximization-based binning",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=-_Zp7r2-cGK": {
    "title": "A Discriminative Gaussian Mixture Model with Sparsity",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fb2efb2b6a074fedc6ac54594b28689da17a72fb",
    "semantic_title": "a discriminative gaussian mixture model with sparsity",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=OOsR8BzCnl5": {
    "title": "Trusted Multi-View Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0fa224f1de6fe38cdeae3cd40de5249475cddf60",
    "semantic_title": "trusted multi-view classification",
    "citation_count": 179,
    "authors": []
  },
  "https://openreview.net/forum?id=xzqLpqRzxLq": {
    "title": "IEPT: Instance-Level and Episode-Level Pretext Tasks for Few-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6d79b85981aca97363a46fc7ee1a0c3f3ff3130d",
    "semantic_title": "iept: instance-level and episode-level pretext tasks for few-shot learning",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=KpfasTaLUpq": {
    "title": "Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ac6535d096fc79dde2d9ce0329e0626b79ede7f0",
    "semantic_title": "deep encoder, shallow decoder: reevaluating non-autoregressive machine translation",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=ldxlzGYWDmW": {
    "title": "Effective Abstract Reasoning with Dual-Contrast Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "7c102ad4766787ff7cd98b649bba024278e16051",
    "semantic_title": "uk stroke forum 2019 abstract supplement",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=onxoVA9FxMw": {
    "title": "On Position Embeddings in BERT",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dc35daba3fb34b2e6a5b12530badb7b799262bbf",
    "semantic_title": "on position embeddings in bert",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=o966_Is_nPA": {
    "title": "Neural Pruning via Growing Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "400080a724d55bbdd3cfc1c54f0aae6af4ec7879",
    "semantic_title": "neural pruning via growing regularization",
    "citation_count": 156,
    "authors": []
  },
  "https://openreview.net/forum?id=l-LGlk4Yl6G": {
    "title": "Mixed-Features Vectors and Subspace Splitting",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "75bfdf15cc615a69159065f928bf7ecfbb23f0dd",
    "semantic_title": "mixed-features vectors and subspace splitting",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r-gPPHEjpmw": {
    "title": "Hierarchical Reinforcement Learning by Discovering Intrinsic Options",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "005acb881061eb8137e9d36a05a6a0bdf0026b61",
    "semantic_title": "hierarchical reinforcement learning by discovering intrinsic options",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=r28GdiQF7vM": {
    "title": "Sharper Generalization Bounds for Learning with Gradient-dominated Objective Functions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8da78b1be38bb14c706c49405ec728e617ed936c",
    "semantic_title": "sharper generalization bounds for learning with gradient-dominated objective functions",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=Naqw7EHIfrv": {
    "title": "Representation Learning for Sequence Data with Deep Autoencoding Predictive Components",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0665091220e2c22ac2e6f6d63a97ae8a092d51b4",
    "semantic_title": "representation learning for sequence data with deep autoencoding predictive components",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=H0syOoy3Ash": {
    "title": "Average-case Acceleration for Bilinear Games and Normal Matrices",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "edc845f87b4ccdc97568410d157ed103c0a493d1",
    "semantic_title": "average-case acceleration for bilinear games and normal matrices",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=qzBUIzq5XR2": {
    "title": "Learning Task-General Representations with Generative Neuro-Symbolic Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "27ad6a5a17a75d1879f47a21a6d07f56ce87cab9",
    "semantic_title": "learning task-general representations with generative neuro-symbolic modeling",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=PObuuGVrGaZ": {
    "title": "Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1c38fb9ee7a86755c184a37bcb654786150038c5",
    "semantic_title": "is label smoothing truly incompatible with knowledge distillation: an empirical study",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=aYuZO9DIdnn": {
    "title": "The Unreasonable Effectiveness of Patches in Deep Convolutional Kernels Methods",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d6cd672bd7c186b6bb74b9f0ca29d877902bbf32",
    "semantic_title": "the unreasonable effectiveness of patches in deep convolutional kernels methods",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=uxpzitPEooJ": {
    "title": "Graph Coarsening with Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bf4209bf37f86a1fe77153fcb10eb541c18e23b2",
    "semantic_title": "graph coarsening with neural networks",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=0IO5VdnSAaH": {
    "title": "On the Universality of the Double Descent Peak in Ridgeless Regression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "55fc38656da779216eedf8bbbfd30878f5593635",
    "semantic_title": "on the universality of the double descent peak in ridgeless regression",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=Yz-XtK5RBxB": {
    "title": "Deep Repulsive Clustering of Ordered Data Based on Order-Identity Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e2d8017a34bff9e3e9c517f11b89246e1f7f683e",
    "semantic_title": "deep repulsive clustering of ordered data based on order-identity decomposition",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=F-mvpFpn_0q": {
    "title": "Rapid Task-Solving in Novel Environments",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6f505f9b8611ea5d1fcf4405a6abb42e0c0c27f1",
    "semantic_title": "rapid task-solving in novel environments",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=WAISmwsqDsb": {
    "title": "DINO: A Conditional Energy-Based GAN for Domain Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "25b02f72bd767accb132e5d9b68242bf69f40f7d",
    "semantic_title": "dino: a conditional energy-based gan for domain translation",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=eIHYL6fpbkA": {
    "title": "Removing Undesirable Feature Contributions Using Out-of-Distribution Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2be5c2f23b997e1cb22479dc1d5d1735997ad4be",
    "semantic_title": "removing undesirable feature contributions using out-of-distribution data",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=JHcqXGaqiGn": {
    "title": "Accurate Learning of Graph Representations with Graph Multiset Pooling",
    "volume": "poster",
    "abstract": "Graph neural networks have been widely used on modeling graph data, achieving impressive results on node classification and link prediction tasks. Yet, obtaining an accurate representation for a graph further requires a pooling function that maps a set of node representations into a compact form. A simple sum or average over all node representations considers all node features equally without consideration of their task relevance, and any structural dependencies among them. Recently proposed hierarchical graph pooling methods, on the other hand, may yield the same representation for two different graphs that are distinguished by the Weisfeiler-Lehman test, as they suboptimally preserve information from the node features. To tackle these limitations of existing graph pooling methods, we first formulate the graph pooling problem as a multiset encoding problem with auxiliary information about the graph structure, and propose a Graph Multiset Transformer (GMT) which is a multi-head attention based global pooling layer that captures the interaction between nodes according to their structural dependencies. We show that GMT satisfies both injectiveness and permutation invariance, such that it is at most as powerful as the Weisfeiler-Lehman graph isomorphism test. Moreover, our methods can be easily extended to the previous node clustering approaches for hierarchical graph pooling. Our experimental results show that GMT significantly outperforms state-of-the-art graph pooling methods on graph classification benchmarks with high memory and time efficiency, and obtains even larger performance gain on graph reconstruction and generation tasks",
    "checked": true,
    "id": "12b03b224a737c3cb8399b3d3b318d94bc04a0b5",
    "semantic_title": "accurate learning of graph representations with graph multiset pooling",
    "citation_count": 181,
    "authors": []
  },
  "https://openreview.net/forum?id=ce6CFXBh30h": {
    "title": "Federated Semi-Supervised Learning with Inter-Client Consistency & Disjoint Learning",
    "volume": "poster",
    "abstract": "While existing federated learning approaches mostly require that clients have fully-labeled data to train on, in realistic settings, data obtained at the client-side often comes without any accompanying labels. Such deficiency of labels may result from either high labeling cost, or difficulty of annotation due to the requirement of expert knowledge. Thus the private data at each client may be either partly labeled, or completely unlabeled with labeled data being available only at the server, which leads us to a new practical federated learning problem, namely Federated Semi-Supervised Learning (FSSL). In this work, we study two essential scenarios of FSSL based on the location of the labeled data. The first scenario considers a conventional case where clients have both labeled and unlabeled data (labels-at-client), and the second scenario considers a more challenging case, where the labeled data is only available at the server (labels-at-server). We then propose a novel method to tackle the problems, which we refer to as Federated Matching (FedMatch). FedMatch improves upon naive combinations of federated learning and semi-supervised learning approaches with a new inter-client consistency loss and decomposition of the parameters for disjoint learning on labeled and unlabeled data. Through extensive experimental validation of our method in the two different scenarios, we show that our method outperforms both local semi-supervised learning and baselines which naively combine federated learning with semi-supervised learning",
    "checked": false,
    "id": "51b61fbf3339433c7ffdbfa9c946185fb49317da",
    "semantic_title": "federated semi-supervised learning with inter-client consistency",
    "citation_count": 217,
    "authors": []
  },
  "https://openreview.net/forum?id=Wga_hrCa3P3": {
    "title": "Contrastive Learning with Adversarial Perturbations for Conditional Text Generation",
    "volume": "poster",
    "abstract": "Recently, sequence-to-sequence (seq2seq) models with the Transformer architecture have achieved remarkable performance on various conditional text generation tasks, such as machine translation. However, most of them are trained with teacher forcing with the ground truth label given at each time step, without being exposed to incorrectly generated tokens during training, which hurts its generalization to unseen inputs, that is known as the \"exposure bias\" problem. In this work, we propose to solve the conditional text generation problem by contrasting positive pairs with negative pairs, such that the model is exposed to various valid or incorrect perturbations of the inputs, for improved generalization. However, training the model with naïve contrastive learning framework using random non-target sequences as negative examples is suboptimal, since they are easily distinguishable from the correct output, especially so with models pretrained with large text corpora. Also, generating positive examples requires domain-specific augmentation heuristics which may not generalize over diverse domains. To tackle this problem, we propose a principled method to generate positive and negative samples for contrastive learning of seq2seq models. Specifically, we generate negative examples by adding small perturbations to the input sequence to minimize its conditional likelihood, and positive examples by adding large perturbations while enforcing it to have a high conditional likelihood. Such `\"hard'' positive and negative pairs generated using our method guides the model to better distinguish correct outputs from incorrect ones. We empirically show that our proposed method significantly improves the generalization of the seq2seq on three text generation tasks --- machine translation, text summarization, and question generation",
    "checked": true,
    "id": "7b00dcca92337be90a4c4c100a5704dc1932c303",
    "semantic_title": "contrastive learning with adversarial perturbations for conditional text generation",
    "citation_count": 109,
    "authors": []
  },
  "https://openreview.net/forum?id=FZ1oTwcXchK": {
    "title": "Optimal Conversion of Conventional Artificial Neural Networks to Spiking Neural Networks",
    "volume": "poster",
    "abstract": "Spiking neural networks (SNNs) are biology-inspired artificial neural networks (ANNs) that comprise of spiking neurons to process asynchronous discrete signals. While more efficient in power consumption and inference speed on the neuromorphic hardware, SNNs are usually difficult to train directly from scratch with spikes due to the discreteness. As an alternative, many efforts have been devoted to converting conventional ANNs into SNNs by copying the weights from ANNs and adjusting the spiking threshold potential of neurons in SNNs. Researchers have designed new SNN architectures and conversion algorithms to diminish the conversion error. However, an effective conversion should address the difference between the SNN and ANN architectures with an efficient approximation of the loss function, which is missing in the field. In this work, we analyze the conversion error by recursive reduction to layer-wise summation and propose a novel strategic pipeline that transfers the weights to the target SNN by combining threshold balance and soft-reset mechanisms. This pipeline enables almost no accuracy loss between the converted SNNs and conventional ANNs with only $\\sim1/10$ of the typical SNN simulation time. Our method is promising to get implanted onto embedded platforms with better support of SNNs with limited energy and memory. Codes are available at https://github.com/Jackn0/snn_optimal_conversion_pipeline",
    "checked": true,
    "id": "05711a79bf9e00bc7ebf8cbbcdb2d0d2f2c6679b",
    "semantic_title": "optimal conversion of conventional artificial neural networks to spiking neural networks",
    "citation_count": 213,
    "authors": []
  },
  "https://openreview.net/forum?id=EKV158tSfwv": {
    "title": "Efficient Continual Learning with Modular Networks and Task-Driven Priors",
    "volume": "poster",
    "abstract": "Existing literature in Continual Learning (CL) has focused on overcoming catastrophic forgetting, the inability of the learner to recall how to perform tasks observed in the past. There are however other desirable properties of a CL system, such as the ability to transfer knowledge from previous tasks and to scale memory and compute sub-linearly with the number of tasks. Since most current benchmarks focus only on forgetting using short streams of tasks, we first propose a new suite of benchmarks to probe CL algorithms across these new axes. Finally, we introduce a new modular architecture, whose modules represent atomic skills that can be composed to perform a certain task. Learning a task reduces to figuring out which past modules to re-use, and which new modules to instantiate to solve the current task. Our learning algorithm leverages a task-driven prior over the exponential search space of all possible ways to combine modules, enabling efficient learning on long streams of tasks. Our experiments show that this modular architecture and learning algorithm perform competitively on widely used CL benchmarks while yielding superior performance on the more challenging benchmarks we introduce in this work. The Benchmark is publicly available at https://github.com/facebookresearch/CTrLBenchmark",
    "checked": true,
    "id": "56912f12c35af9579999b45fe6ab7d5b9f090df6",
    "semantic_title": "efficient continual learning with modular networks and task-driven priors",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=6NFBvWlRXaG": {
    "title": "On the Universality of Rotation Equivariant Point Cloud Networks",
    "volume": "poster",
    "abstract": "Learning functions on point clouds has applications in many fields, including computer vision, computer graphics, physics, and chemistry. Recently, there has been a growing interest in neural architectures that are invariant or equivariant to all three shape-preserving transformations of point clouds: translation, rotation, and permutation. In this paper, we present a first study of the approximation power of these architectures. We first derive two sufficient conditions for an equivariant architecture to have the universal approximation property, based on a novel characterization of the space of equivariant polynomials. We then use these conditions to show that two recently suggested models, Tensor field Networks and SE3-Transformers, are universal, and for devising two other novel universal architectures",
    "checked": true,
    "id": "30827cfdabbb8e1f02c6fd7c188d5a7e6b05c6da",
    "semantic_title": "on the universality of rotation equivariant point cloud networks",
    "citation_count": 86,
    "authors": []
  },
  "https://openreview.net/forum?id=ATp1nW2FuZL": {
    "title": "Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces",
    "volume": "poster",
    "abstract": "Recent research has proposed neural architectures for solving combinatorial problems in structured output spaces. In many such problems, there may exist multiple solutions for a given input, e.g. a partially filled Sudoku puzzle may have many completions satisfying all constraints. Further, we are often interested in finding any \"one\" of the possible solutions, without any preference between them. Existing approaches completely ignore this solution multiplicity. In this paper, we argue that being oblivious to the presence of multiple solutions can severely hamper their training ability. Our contribution is two-fold. First, we formally define the task of learning one-of-many solutions for combinatorial problems in structured output spaces, which is applicable for solving several problems of interest such as N-Queens, and Sudoku. Second, we present a generic learning framework that adapts an existing prediction network for a combinatorial problem to handle solution multiplicity. Our framework uses a selection module, whose goal is to dynamically determine, for every input, the solution that is most effective for training the network parameters in any given learning iteration. We propose an RL based approach to jointly train the selection module with the prediction network. Experiments on three different domains, and using two different prediction networks, demonstrate that our framework significantly improves the accuracy in our setting, obtaining up to 21 pt gain over the baselines",
    "checked": true,
    "id": "51c62d63c6204deecb24a1d3f9ea8e0a42d23817",
    "semantic_title": "neural learning of one-of-many solutions for combinatorial problems in structured output spaces",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=SHvF5xaueVn": {
    "title": "GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images",
    "volume": "poster",
    "abstract": "We tackle a challenging blind image denoising problem, in which only single distinct noisy images are available for training a denoiser, and no information about noise is known, except for it being zero-mean, additive, and independent of the clean image. In such a setting, which often occurs in practice, it is not possible to train a denoiser with the standard discriminative training or with the recently developed Noise2Noise (N2N) training; the former requires the underlying clean image for the given noisy image, and the latter requires two independently realized noisy image pair for a clean image. To that end, we propose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise) method that first learns a generative model that can 1) simulate the noise in the given noisy images and 2) generate a rough, noisy estimates of the clean images, then 3) iteratively trains a denoiser with subsequently synthesized noisy image pairs (as in N2N), obtained from the generative model. In results, we show the denoiser trained with our GAN2GAN achieves an impressive denoising performance on both synthetic and real-world datasets for the blind denoising setting; it almost approaches the performance of the standard discriminatively-trained or N2N-trained models that have more information than ours, and it significantly outperforms the recent baseline for the same setting, \\textit{e.g.}, Noise2Void, and a more conventional yet strong one, BM3D. The official code of our method is available at https://github.com/csm9493/GAN2GAN",
    "checked": true,
    "id": "a40a41206214e7c400a7bc67867af95b6328e2f6",
    "semantic_title": "gan2gan: generative noise learning for blind denoising with single noisy images",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=F2v4aqEL6ze": {
    "title": "CPR: Classifier-Projection Regularization for Continual Learning",
    "volume": "poster",
    "abstract": "We propose a general, yet simple patch that can be applied to existing regularization-based continual learning methods called classifier-projection regularization (CPR). Inspired by both recent results on neural networks with wide local minima and information theory, CPR adds an additional regularization term that maximizes the entropy of a classifier's output probability. We demonstrate that this additional term can be interpreted as a projection of the conditional probability given by a classifier's output to the uniform distribution. By applying the Pythagorean theorem for KL divergence, we then prove that this projection may (in theory) improve the performance of continual learning methods. In our extensive experimental results, we apply CPR to several state-of-the-art regularization-based continual learning methods and benchmark performance on popular image recognition datasets. Our results demonstrate that CPR indeed promotes a wide local minima and significantly improves both accuracy and plasticity while simultaneously mitigating the catastrophic forgetting of baseline continual learning methods. The codes and scripts for this work are available at https://github.com/csm9493/CPR_CL",
    "checked": true,
    "id": "4a339a899a48d74545ad056e71e3b0107987c7f3",
    "semantic_title": "cpr: classifier-projection regularization for continual learning",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=1OCTOShAmqB": {
    "title": "On the Dynamics of Training Attention Models",
    "volume": "poster",
    "abstract": "The attention mechanism has been widely used in deep neural networks as a model component. By now, it has become a critical building block in many state-of-the-art natural language models. Despite its great success established empirically, the working mechanism of attention has not been investigated at a sufficient theoretical depth to date. In this paper, we set up a simple text classification task and study the dynamics of training a simple attention-based classification model using gradient descent. In this setting, we show that, for the discriminative words that the model should attend to, a persisting identity exists relating its embedding and the inner product of its key and the query. This allows us to prove that training must converge to attending to the discriminative words when the attention output is classified by a linear classifier. Experiments are performed, which validate our theoretical analysis and provide further insights",
    "checked": true,
    "id": "4d5c9a2a55b64bcf2cd01c5aa954776575c98500",
    "semantic_title": "on the dynamics of training attention models",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=OMNB1G5xzd4": {
    "title": "Model-Based Offline Planning",
    "volume": "poster",
    "abstract": "Offline learning is a key part of making reinforcement learning (RL) useable in real systems. Offline RL looks at scenarios where there is data from a system's operation, but no direct access to the system when learning a policy. Recent work on training RL policies from offline data has shown results both with model-free policies learned directly from the data, or with planning on top of learnt models of the data. Model-free policies tend to be more performant, but are more opaque, harder to command externally, and less easy to integrate into larger systems. We propose an offline learner that generates a model that can be used to control the system directly through planning. This allows us to have easily controllable policies directly from data, without ever interacting with the system. We show the performance of our algorithm, Model-Based Offline Planning (MBOP) on a series of robotics-inspired tasks, and demonstrate its ability leverage planning to respect environmental constraints. We are able to find near-optimal polices for certain simulated systems from as little as 50 seconds of real-time system interaction, and create zero-shot goal-conditioned policies on a series of environments",
    "checked": true,
    "id": "3cb8e96faba73efa027fa858e2a78cd1fc3c6e4d",
    "semantic_title": "model-based offline planning",
    "citation_count": 155,
    "authors": []
  },
  "https://openreview.net/forum?id=kLbhLJ8OT12": {
    "title": "Modelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System",
    "volume": "poster",
    "abstract": "Designing task-oriented dialogue systems is a challenging research topic, since it needs not only to generate utterances fulfilling user requests but also to guarantee the comprehensibility. Many previous works trained end-to-end (E2E) models with supervised learning (SL), however, the bias in annotated system utterances remains as a bottleneck. Reinforcement learning (RL) deals with the problem through using non-differentiable evaluation metrics (e.g., the success rate) as rewards. Nonetheless, existing works with RL showed that the comprehensibility of generated system utterances could be corrupted when improving the performance on fulfilling user requests. In our work, we (1) propose modelling the hierarchical structure between dialogue policy and natural language generator (NLG) with the option framework, called HDNO, where the latent dialogue act is applied to avoid designing specific dialogue act representations; (2) train HDNO via hierarchical reinforcement learning (HRL), as well as suggest the asynchronous updates between dialogue policy and NLG during training to theoretically guarantee their convergence to a local maximizer; and (3) propose using a discriminator modelled with language models as an additional reward to further improve the comprehensibility. We test HDNO on MultiWoz 2.0 and MultiWoz 2.1, the datasets on multi-domain dialogues, in comparison with word-level E2E model trained with RL, LaRL and HDSA, showing improvements on the performance evaluated by automatic evaluation metrics and human evaluation. Finally, we demonstrate the semantic meanings of latent dialogue acts to show the explanability for HDNO",
    "checked": true,
    "id": "cf4ce485d4971b17c8733188da65065d153a68a5",
    "semantic_title": "modelling hierarchical structure between dialogue policy and natural language generator with option framework for task-oriented dialogue system",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=NTEz-6wysdb": {
    "title": "Distilling Knowledge from Reader to Retriever for Question Answering",
    "volume": "poster",
    "abstract": "The task of information retrieval is an important component of many natural language processing systems, such as open domain question answering. While traditional methods were based on hand-crafted features, continuous representations based on neural networks recently obtained competitive results. A challenge of using such methods is to obtain supervised data to train the retriever model, corresponding to pairs of query and support documents. In this paper, we propose a technique to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever. We evaluate our method on question answering, obtaining state-of-the-art results",
    "checked": true,
    "id": "2b4bc49a3b23229a060609380752666b24b435fb",
    "semantic_title": "distilling knowledge from reader to retriever for question answering",
    "citation_count": 270,
    "authors": []
  },
  "https://openreview.net/forum?id=hr-3PMvDpil": {
    "title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers",
    "volume": "poster",
    "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches",
    "checked": true,
    "id": "0427939b1539c681147b97e3ce5ddb57633b8281",
    "semantic_title": "efficient certified defenses against patch attacks on image classifiers",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=lU5Rs_wCweN": {
    "title": "Taking Notes on the Fly Helps Language Pre-Training",
    "volume": "poster",
    "abstract": "How to make unsupervised language pre-training more efficient and less resource-intensive is an important research direction in NLP. In this paper, we focus on improving the efficiency of language pre-training methods through providing better data utilization. It is well-known that in language data corpus, words follow a heavy-tail distribution. A large proportion of words appear only very few times and the embeddings of rare words are usually poorly optimized. We argue that such embeddings carry inadequate semantic signals, which could make the data utilization inefficient and slow down the pre-training of the entire model. To mitigate this problem, we propose Taking Notes on the Fly (TNF), which takes notes for rare words on the fly during pre-training to help the model understand them when they occur next time. Specifically, TNF maintains a note dictionary and saves a rare word's contextual information in it as notes when the rare word occurs in a sentence. When the same rare word occurs again during training, the note information saved beforehand can be employed to enhance the semantics of the current sentence. By doing so, TNF provides a better data utilization since cross-sentence information is employed to cover the inadequate semantics caused by rare words in the sentences. We implement TNF on both BERT and ELECTRA to check its efficiency and effectiveness. Experimental results show that TNF's training time is 60% less than its backbone pre-training models when reaching the same performance. When trained with same number of iterations, TNF outperforms its backbone methods on most of downstream tasks and the average GLUE score. Code is attached in the supplementary material",
    "checked": true,
    "id": "8988742cb5658634e2a173d0d4baaaab17304229",
    "semantic_title": "taking notes on the fly helps language pre-training",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=dlEJsyHGeaL": {
    "title": "Graph Edit Networks",
    "volume": "poster",
    "abstract": "While graph neural networks have made impressive progress in classification and regression, few approaches to date perform time series prediction on graphs, and those that do are mostly limited to edge changes. We suggest that graph edits are a more natural interface for graph-to-graph learning. In particular, graph edits are general enough to describe any graph-to-graph change, not only edge changes; they are sparse, making them easier to understand for humans and more efficient computationally; and they are local, avoiding the need for pooling layers in graph neural networks. In this paper, we propose a novel output layer - the graph edit network - which takes node embeddings as input and generates a sequence of graph edits that transform the input graph to the output graph. We prove that a mapping between the node sets of two graphs is sufficient to construct training data for a graph edit network and that an optimal mapping yields edit scripts that are almost as short as the graph edit distance between the graphs. We further provide a proof-of-concept empirical evaluation on several graph dynamical systems, which are difficult to learn for baselines from the literature",
    "checked": true,
    "id": "f45e441b34c8b6825926b027fd440cedfa64f9df",
    "semantic_title": "graph edit networks",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=8cpHIfgY4Dj": {
    "title": "FOCAL: Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization",
    "volume": "poster",
    "abstract": "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks",
    "checked": true,
    "id": "2cef71e693c3c1e6cc48e9387d9e3bec1ad1b5bb",
    "semantic_title": "focal: efficient fully-offline meta-reinforcement learning via distance metric learning and behavior regularization",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=33rtZ4Sjwjn": {
    "title": "Effective and Efficient Vote Attack on Capsule Networks",
    "volume": "poster",
    "abstract": "Standard Convolutional Neural Networks (CNNs) can be easily fooled by images with small quasi-imperceptible artificial perturbations. As alternatives to CNNs, the recently proposed Capsule Networks (CapsNets) are shown to be more robust to white-box attack than CNNs under popular attack protocols. Besides, the class-conditional reconstruction part of CapsNets is also used to detect adversarial examples. In this work, we investigate the adversarial robustness of CapsNets, especially how the inner workings of CapsNets change when the output capsules are attacked. The first observation is that adversarial examples misled CapsNets by manipulating the votes from primary capsules. Another observation is the high computational cost, when we directly apply multi-step attack methods designed for CNNs to attack CapsNets, due to the computationally expensive routing mechanism. Motivated by these two observations, we propose a novel vote attack where we attack votes of CapsNets directly. Our vote attack is not only effective, but also efficient by circumventing the routing process. Furthermore, we integrate our vote attack into the detection-aware attack paradigm, which can successfully bypass the class-conditional reconstruction based detection method. Extensive experiments demonstrate the superior attack performance of our vote attack on CapsNets",
    "checked": true,
    "id": "a385fe3de8679357cdb1985f4dc46b3d6af4130d",
    "semantic_title": "effective and efficient vote attack on capsule networks",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=rkQuFUmUOg3": {
    "title": "Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets",
    "volume": "poster",
    "abstract": "Despite the success of recent Neural Architecture Search (NAS) methods on various tasks which have shown to output networks that largely outperform human-designed networks, conventional NAS methods have mostly tackled the optimization of searching for the network architecture for a single task (dataset), which does not generalize well across multiple tasks (datasets). Moreover, since such task-specific methods search for a neural architecture from scratch for every given task, they incur a large computational cost, which is problematic when the time and monetary budget are limited. In this paper, we propose an efficient NAS framework that is trained once on a database consisting of datasets and pretrained networks and can rapidly search for a neural architecture for a novel dataset. The proposed MetaD2A (Meta Dataset-to-Architecture) model can stochastically generate graphs (architectures) from a given set (dataset) via a cross-modal latent space learned with amortized meta-learning. Moreover, we also propose a meta-performance predictor to estimate and select the best architecture without direct training on target datasets. The experimental results demonstrate that our model meta-learned on subsets of ImageNet-1K and architectures from NAS-Bench 201 search space successfully generalizes to multiple unseen datasets including CIFAR-10 and CIFAR-100, with an average search time of 33 GPU seconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than NSGANetV2, a transferable NAS method, with comparable performance. We believe that the MetaD2A proposes a new research direction for rapid NAS as well as ways to utilize the knowledge from rich databases of datasets and architectures accumulated over the past years. Code is available at https://github.com/HayeonLee/MetaD2A",
    "checked": true,
    "id": "067df3c59a57ab8a03dd1ede1ee6ee0af3adc491",
    "semantic_title": "rapid neural architecture search by learning to generate graphs from datasets",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=edJ_HipawCa": {
    "title": "Impact of Representation Learning in Linear Bandits",
    "volume": "poster",
    "abstract": "We study how representation learning can improve the efficiency of bandit problems. We study the setting where we play $T$ linear bandits with dimension $d$ concurrently, and these $T$ bandit tasks share a common $k (\\ll d)$ dimensional linear representation. For the finite-action setting, we present a new algorithm which achieves $\\widetilde{O}(T\\sqrt{kN} + \\sqrt{dkNT})$ regret, where $N$ is the number of rounds we play for each bandit. When $T$ is sufficiently large, our algorithm significantly outperforms the naive algorithm (playing $T$ bandits independently) that achieves $\\widetilde{O}(T\\sqrt{d N})$ regret. We also provide an $\\Omega(T\\sqrt{kN} + \\sqrt{dkNT})$ regret lower bound, showing that our algorithm is minimax-optimal up to poly-logarithmic factors. Furthermore, we extend our algorithm to the infinite-action setting and obtain a corresponding regret bound which demonstrates the benefit of representation learning in certain regimes. We also present experiments on synthetic and real-world data to illustrate our theoretical findings and demonstrate the effectiveness of our proposed algorithms",
    "checked": true,
    "id": "401084c502524652ea335023406606d72857ea34",
    "semantic_title": "impact of representation learning in linear bandits",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=gwnoVHIES05": {
    "title": "Creative Sketch Generation",
    "volume": "poster",
    "abstract": "Sketching or doodling is a popular creative activity that people engage in. However, most existing work in automatic sketch understanding or generation has focused on sketches that are quite mundane. In this work, we introduce two datasets of creative sketches -- Creative Birds and Creative Creatures -- containing 10k sketches each along with part annotations. We propose DoodlerGAN -- a part-based Generative Adversarial Network (GAN) -- to generate unseen compositions of novel part appearances. Quantitative evaluations as well as human studies demonstrate that sketches generated by our approach are more creative and of higher quality than existing approaches. In fact, in Creative Birds, subjects prefer sketches generated by DoodlerGAN over those drawn by humans!",
    "checked": true,
    "id": "9cc1c31cf4c399d7749815037479844b9d8b151c",
    "semantic_title": "creative sketch generation",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=068E_JSq9O": {
    "title": "Self-supervised Representation Learning with Relative Predictive Coding",
    "volume": "poster",
    "abstract": "This paper introduces Relative Predictive Coding (RPC), a new contrastive representation learning objective that maintains a good balance among training stability, minibatch size sensitivity, and downstream task performance. The key to the success of RPC is two-fold. First, RPC introduces the relative parameters to regularize the objective for boundedness and low variance. Second, RPC contains no logarithm and exponential score functions, which are the main cause of training instability in prior contrastive objectives. We empirically verify the effectiveness of RPC on benchmark vision and speech self-supervised learning tasks. Lastly, we relate RPC with mutual information (MI) estimation, showing RPC can be used to estimate MI with low variance",
    "checked": true,
    "id": "c47a26236cbd5446c4b1897ec5eafa8a9bfbce54",
    "semantic_title": "self-supervised representation learning with relative predictive coding",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=uz5uw6gM0m": {
    "title": "One Network Fits All? Modular versus Monolithic Task Formulations in Neural Networks",
    "volume": "poster",
    "abstract": "Can deep learning solve multiple, very different tasks simultaneously? We investigate how the representations of the underlying tasks affect the ability of a single neural network to learn them jointly. We present theoretical and empirical findings that a single neural network is capable of simultaneously learning multiple tasks from a combined data set, for a variety of methods for representing tasks---for example, when the distinct tasks are encoded by well-separated clusters or decision trees over some task-code attributes. Indeed, more strongly, we present a novel analysis that shows that families of simple programming-like constructs for the codes encoding the tasks are learnable by two-layer neural networks with standard training. We study more generally how the complexity of learning such combined tasks grows with the complexity of the task codes; we find that learning many tasks can be provably hard, even though the individual tasks are easy to learn. We provide empirical support for the usefulness of the learning bounds by training networks on clusters, decision trees, and SQL-style aggregation",
    "checked": true,
    "id": "d67c96c7948b9b0e6c9f9ea553ea0e6186e2b9dc",
    "semantic_title": "one network fits all? modular versus monolithic task formulations in neural networks",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=de11dbHzAMF": {
    "title": "Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data",
    "volume": "poster",
    "abstract": "Multi-Task Learning (MTL) networks have emerged as a promising method for transferring learned knowledge across different tasks. However, MTL must deal with challenges such as: overfitting to low resource tasks, catastrophic forgetting, and negative task transfer, or learning interference. Often, in Natural Language Processing (NLP), a separate model per task is needed to obtain the best performance. However, many fine-tuning approaches are both parameter inefficient, i.e., potentially involving one new model per task, and highly susceptible to losing knowledge acquired during pretraining. We propose a novel Transformer based Hypernetwork Adapter consisting of a new conditional attention mechanism as well as a set of task-conditioned modules that facilitate weight sharing. Through this construction, we achieve more efficient parameter sharing and mitigate forgetting by keeping half of the weights of a pretrained model fixed. We also use a new multi-task data sampling strategy to mitigate the negative effects of data imbalance across tasks. Using this approach, we are able to surpass single task fine-tuning methods while being parameter and data efficient (using around 66% of the data). Compared to other BERT Large methods on GLUE, our 8-task model surpasses other Adapter methods by 2.8% and our 24-task model outperforms by 0.7-1.0% models that use MTL and single task fine-tuning. We show that a larger variant of our single multi-task model approach performs competitively across 26 NLP tasks and yields state-of-the-art results on a number of test and development sets",
    "checked": true,
    "id": "55c4a747855c74210919c45f7899e1f79e4c97f5",
    "semantic_title": "conditionally adaptive multi-task learning: improving transfer learning in nlp using fewer parameters & less data",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=04cII6MumYV": {
    "title": "A Universal Representation Transformer Layer for Few-Shot Image Classification",
    "volume": "poster",
    "abstract": "Few-shot classification aims to recognize unseen classes when presented with only a small number of samples. We consider the problem of multi-domain few-shot image classification, where unseen classes and examples come from diverse data sources. This problem has seen growing interest and has inspired the development of benchmarks such as Meta-Dataset. A key challenge in this multi-domain setting is to effectively integrate the feature representations from the diverse set of training domains. Here, we propose a Universal Representation Transformer (URT) layer, that meta-learns to leverage universal features for few-shot classification by dynamically re-weighting and composing the most appropriate domain-specific representations. In experiments, we show that URT sets a new state-of-the-art result on Meta-Dataset. Specifically, it achieves top-performance on the highest number of data sources compared to competing methods. We analyze variants of URT and present a visualization of the attention score heatmaps that sheds light on how the model performs cross-domain generalization",
    "checked": true,
    "id": "6e1efe22d5696269aff7addcb438f77ff6cc2508",
    "semantic_title": "a universal representation transformer layer for few-shot image classification",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=-mWcQVLPSPy": {
    "title": "Isometric Propagation Network for Generalized Zero-shot Learning",
    "volume": "poster",
    "abstract": "Zero-shot learning (ZSL) aims to classify images of an unseen class only based on a few attributes describing that class but no access to any training sample. A popular strategy is to learn a mapping between the semantic space of class attributes and the visual space of images based on the seen classes and their data. Thus, an unseen class image can be ideally mapped to its corresponding class attributes. The key challenge is how to align the representations in the two spaces. For most ZSL settings, the attributes for each seen/unseen class are only represented by a vector while the seen-class data provide much more information. Thus, the imbalanced supervision from the semantic and the visual space can make the learned mapping easily overfitting to the seen classes. To resolve this problem, we propose Isometric Propagation Network (IPN), which learns to strengthen the relation between classes within each space and align the class dependency in the two spaces. Specifically, IPN learns to propagate the class representations on an auto-generated graph within each space. In contrast to only aligning the resulted static representation, we regularize the two dynamic propagation procedures to be isometric in terms of the two graphs' edge weights per step by minimizing a consistency loss between them. IPN achieves state-of-the-art performance on three popular ZSL benchmarks. To evaluate the generalization capability of IPN, we further build two larger benchmarks with more diverse unseen classes and demonstrate the advantages of IPN on them",
    "checked": true,
    "id": "26147e2b3f82417b56f61ee01b02979396b7e1b7",
    "semantic_title": "isometric propagation network for generalized zero-shot learning",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=IMPnRXEWpvr": {
    "title": "Towards Impartial Multi-task Learning",
    "volume": "poster",
    "abstract": "Multi-task learning (MTL) has been widely used in representation learning. However, naively training all tasks simultaneously may lead to the partial training issue, where specific tasks are trained more adequately than others. In this paper, we propose to learn multiple tasks impartially. Specifically, for the task-shared parameters, we optimize the scaling factors via a closed-form solution, such that the aggregated gradient (sum of raw gradients weighted by the scaling factors) has equal projections onto individual tasks. For the task-specific parameters, we dynamically weigh the task losses so that all of them are kept at a comparable scale. Further, we find the above gradient balance and loss balance are complementary and thus propose a hybrid balance method to further improve the performance. Our impartial multi-task learning (IMTL) can be end-to-end trained without any heuristic hyper-parameter tuning, and is general to be applied on all kinds of losses without any distribution assumption. Moreover, our IMTL can converge to similar results even when the task losses are designed to have different scales, and thus it is scale-invariant. We extensively evaluate our IMTL on the standard MTL benchmarks including Cityscapes, NYUv2 and CelebA. It outperforms existing loss weighting methods under the same experimental settings",
    "checked": true,
    "id": "45c0828baec1dd53b81f1b2635788fdf27d0792d",
    "semantic_title": "towards impartial multi-task learning",
    "citation_count": 186,
    "authors": []
  },
  "https://openreview.net/forum?id=Uu1Nw-eeTxJ": {
    "title": "On Learning Universal Representations Across Languages",
    "volume": "poster",
    "abstract": "Recent studies have demonstrated the overwhelming advantage of cross-lingual pre-trained models (PTMs), such as multilingual BERT and XLM, on cross-lingual NLP tasks. However, existing approaches essentially capture the co-occurrence among tokens through involving the masked language model (MLM) objective with token-level cross entropy. In this work, we extend these approaches to learn sentence-level representations and show the effectiveness on cross-lingual understanding and generation. Specifically, we propose a Hierarchical Contrastive Learning (HiCTL) method to (1) learn universal representations for parallel sentences distributed in one or multiple languages and (2) distinguish the semantically-related words from a shared cross-lingual vocabulary for each sentence. We conduct evaluations on two challenging cross-lingual tasks, XTREME and machine translation. Experimental results show that the HiCTL outperforms the state-of-the-art XLM-R by an absolute gain of 4.2% accuracy on the XTREME benchmark as well as achieves substantial improvements on both of the high resource and low-resource English$\\rightarrow$X translation tasks over strong baselines",
    "checked": true,
    "id": "311909621177c397c6b7099beff32332124f7d46",
    "semantic_title": "on learning universal representations across languages",
    "citation_count": 86,
    "authors": []
  },
  "https://openreview.net/forum?id=xYGNO86OWDH": {
    "title": "Isotropy in the Contextual Embedding Space: Clusters and Manifolds",
    "volume": "poster",
    "abstract": "The geometric properties of contextual embedding spaces for deep language models such as BERT and ERNIE, have attracted considerable attention in recent years. Investigations on the contextual embeddings demonstrate a strong anisotropic space such that most of the vectors fall within a narrow cone, leading to high cosine similarities. It is surprising that these LMs are as successful as they are, given that most of their embedding vectors are as similar to one another as they are. In this paper, we argue that the isotropy indeed exists in the space, from a different but more constructive perspective. We identify isolated clusters and low dimensional manifolds in the contextual embedding space, and introduce tools to both qualitatively and quantitatively analyze them. We hope the study in this paper could provide insights towards a better understanding of the deep language models",
    "checked": true,
    "id": "2a8bb26654c015f663670d1e0745f870071d5507",
    "semantic_title": "isotropy in the contextual embedding space: clusters and manifolds",
    "citation_count": 117,
    "authors": []
  },
  "https://openreview.net/forum?id=0-EYBhgw80y": {
    "title": "MoPro: Webly Supervised Learning with Momentum Prototypes",
    "volume": "poster",
    "abstract": "We propose a webly-supervised representation learning method that does not suffer from the annotation unscalability of supervised learning, nor the computation unscalability of self-supervised learning. Most existing works on webly-supervised representation learning adopt a vanilla supervised learning method without accounting for the prevalent noise in the training data, whereas most prior methods in learning with label noise are less effective for real-world large-scale noisy data. We propose momentum prototypes (MoPro), a simple contrastive learning method that achieves online label noise correction, out-of-distribution sample removal, and representation learning. MoPro achieves state-of-the-art performance on WebVision, a weakly-labeled noisy dataset. MoPro also shows superior performance when the pretrained model is transferred to down-stream image classification and detection tasks. It outperforms the ImageNet supervised pretrained model by +10.5 on 1-shot classification on VOC, and outperforms the best self-supervised pretrained model by +17.3 when finetuned on 1% of ImageNet labeled samples. Furthermore, MoPro is more robust to distribution shifts. Code and pretrained models are available at https://github.com/salesforce/MoPro",
    "checked": true,
    "id": "1cb29798801b315d6287aae1093f5432f54673dc",
    "semantic_title": "mopro: webly supervised learning with momentum prototypes",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=jLoC4ez43PZ": {
    "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4083958684292f6fa2f5c7fd4f9be975e80145b6",
    "semantic_title": "graphcodebert: pre-training code representations with data flow",
    "citation_count": 1205,
    "authors": []
  },
  "https://openreview.net/forum?id=HOFxeCutxZR": {
    "title": "Enjoy Your Editing: Controllable GANs for Image Editing via Latent Space Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3cdac7e1a3904a9458f55694d1dc6e6374659a02",
    "semantic_title": "enjoy your editing: controllable gans for image editing via latent space navigation",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=c8P9NQVtmnO": {
    "title": "Fourier Neural Operator for Parametric Partial Differential Equations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2f7dc1ee85e9f6a97810c66016e09ffeed684f03",
    "semantic_title": "fourier neural operator for parametric partial differential equations",
    "citation_count": 2621,
    "authors": []
  },
  "https://openreview.net/forum?id=vYeQQ29Tbvx": {
    "title": "Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "84ef2cf4f73bb4eae7ae63fbca04a4d774b75ac7",
    "semantic_title": "training batchnorm and only batchnorm: on the expressive power of random features in cnns",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=1Fqg133qRaI": {
    "title": "Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6c4fe31504d47b8547e47267c0cb4efa464f022b",
    "semantic_title": "towards faster and stabilized gan training for high-fidelity few-shot image synthesis",
    "citation_count": 244,
    "authors": []
  },
  "https://openreview.net/forum?id=iKQAk8a2kM0": {
    "title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1045b77db15e56cc1c87d35fe90332164f6ac5a8",
    "semantic_title": "targeted attack against deep neural networks via flipping limited weight bits",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=6YEQUn0QICG": {
    "title": "FedBN: Federated Learning on Non-IID Features via Local Batch Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2c0f4711c9c124a8dc056eaee82a2ca5ef276da8",
    "semantic_title": "fedbn: federated learning on non-iid features via local batch normalization",
    "citation_count": 860,
    "authors": []
  },
  "https://openreview.net/forum?id=Iz3zU3M316D": {
    "title": "AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e11e8d81c68cc782f564ed78e595b66790719804",
    "semantic_title": "adamp: slowing down the slowdown for momentum optimizers on scale-invariant weights",
    "citation_count": 137,
    "authors": []
  },
  "https://openreview.net/forum?id=wxRwhSdORKG": {
    "title": "Learning Subgoal Representations with Slow Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "01e80034058d471e3e15ced46b54647aa443af15",
    "semantic_title": "learning subgoal representations with slow dynamics",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=X76iqnUbBjz": {
    "title": "A Unified Approach to Interpreting and Boosting Adversarial Transferability",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "47b4744162537f40572cdd723f8f37fb489a3e75",
    "semantic_title": "a unified approach to interpreting and boosting adversarial transferability",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=dFwBosAcJkN": {
    "title": "Perceptual Adversarial Robustness: Defense Against Unseen Threat Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "473a854a939eca4bf39420ff496f8e24e223d460",
    "semantic_title": "perceptual adversarial robustness: defense against unseen threat models",
    "citation_count": 193,
    "authors": []
  },
  "https://openreview.net/forum?id=OQ08SN70M1V": {
    "title": "Better Fine-Tuning by Reducing Representational Collapse",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b88c11922cac84e5ea902f82d27ae21c3dda2e04",
    "semantic_title": "better fine-tuning by reducing representational collapse",
    "citation_count": 212,
    "authors": []
  },
  "https://openreview.net/forum?id=K9bw7vqp_s": {
    "title": "Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "5e38dc1ccf33ac1df09b8eb6476f110cb3d1966f",
    "semantic_title": "learning n: m fine-grained structured sparse neural networks from scratch",
    "citation_count": 253,
    "authors": []
  },
  "https://openreview.net/forum?id=OMizHuea_HB": {
    "title": "Active Contrastive Learning of Audio-Visual Video Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3505a5ab5fdc55a37479e8a1610862ad3314884b",
    "semantic_title": "active contrastive learning of audio-visual video representations",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=-TwO99rbVRu": {
    "title": "PseudoSeg: Designing Pseudo Labels for Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "217dd8145b5fd7ae7eaa921fe1e98b03039904ad",
    "semantic_title": "pseudoseg: designing pseudo labels for semantic segmentation",
    "citation_count": 310,
    "authors": []
  },
  "https://openreview.net/forum?id=BXewfAYMmJw": {
    "title": "Counterfactual Generative Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "011fbfe79c738b84c1bfcdabcd752977524b1363",
    "semantic_title": "counterfactual generative networks",
    "citation_count": 129,
    "authors": []
  },
  "https://openreview.net/forum?id=Q4EUywJIkqr": {
    "title": "Contemplating Real-World Object Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "149a9180d1a6c140d267a2cda133448b12b81321",
    "semantic_title": "contemplating real-world object classification",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=A5VV3UyIQz": {
    "title": "Explainable Deep One-Class Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "16a67491ed4bdb6293d1c2be35b0e8bae962cdeb",
    "semantic_title": "explainable deep one-class classification",
    "citation_count": 202,
    "authors": []
  },
  "https://openreview.net/forum?id=po-DLlBuAuz": {
    "title": "Batch Reinforcement Learning Through Continuation Method",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "69e40b090c15a426957f2a8879fdfaa7f45052ff",
    "semantic_title": "batch reinforcement learning through continuation method",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=LucJxySuJcE": {
    "title": "Protecting DNNs from Theft using an Ensemble of Diverse Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2c68b364549724dc2cbb479243e6dc4c33bb26e7",
    "semantic_title": "protecting dnns from theft using an ensemble of diverse models",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=6xHJ37MVxxp": {
    "title": "Domain Generalization with MixStyle",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4f6eafafc9563a5b904535078df7e74afe39ef59",
    "semantic_title": "domain generalization with mixstyle",
    "citation_count": 793,
    "authors": []
  },
  "https://openreview.net/forum?id=aGfU_xziEX8": {
    "title": "Efficient Inference of Flexible Interaction in Spiking-neuron Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7f4e7e8f4a8d6f32e4cfbd35c4f3892da1b90c05",
    "semantic_title": "efficient inference of flexible interaction in spiking-neuron networks",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=R2ZlTVPx0Gk": {
    "title": "DICE: Diversity in Deep Ensembles via Conditional Redundancy Adversarial Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a3ec0076139420a34d2079a029b0836b890e5d3c",
    "semantic_title": "dice: diversity in deep ensembles via conditional redundancy adversarial estimation",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=N33d7wjgzde": {
    "title": "Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d49c5ca5fd043f00b8400e4390d57d55a3c9daf8",
    "semantic_title": "universal weakly supervised segmentation by pixel-to-segment contrastive learning",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=Db4yerZTYkz": {
    "title": "Shape-Texture Debiased Neural Network Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b7826eb413218602e152e9affc96e14833bbb360",
    "semantic_title": "shape-texture debiased neural network training",
    "citation_count": 109,
    "authors": []
  },
  "https://openreview.net/forum?id=IDFQI9OY6K": {
    "title": "Interactive Weak Supervision: Learning Useful Heuristics for Data Labeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c50ac3bb4fbed9c92de1afb88bdb889716b6d469",
    "semantic_title": "interactive weak supervision: learning useful heuristics for data labeling",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=8e6BrwU6AjQ": {
    "title": "MoVie: Revisiting Modulated Convolutions for Visual Counting and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ef7a22c4eca639e2f2f035f0a95e8184aa9cfff1",
    "semantic_title": "movie: revisiting modulated convolutions for visual counting and beyond",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=uKhGRvM8QNH": {
    "title": "Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ba28c939b3e1f66d60e212457b2d76973ec7846f",
    "semantic_title": "improve object detection with feature-based knowledge distillation: towards accurate and efficient detectors",
    "citation_count": 216,
    "authors": []
  },
  "https://openreview.net/forum?id=YwpZmcAehZ": {
    "title": "Revisiting Dynamic Convolution via Matrix Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7cda4cc432db2797758d8fd1f2c4703523fd36d0",
    "semantic_title": "revisiting dynamic convolution via matrix decomposition",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=GTGb3M_KcUl": {
    "title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b8a4e778549f89da6352ae18208654f32621d0db",
    "semantic_title": "dynatune: dynamic tensor program optimization in deep neural network compilation",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=blfSjHeFM_e": {
    "title": "MALI: A memory efficient and reverse accurate integrator for Neural ODEs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "57afbedf7e3f51f952ce28f92dccf8349133a523",
    "semantic_title": "mali: a memory efficient and reverse accurate integrator for neural odes",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=Xb8xvrtB8Ce": {
    "title": "Bag of Tricks for Adversarial Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "67f74fe9d46f88661573003f8f1f12967ae49fa3",
    "semantic_title": "bag of tricks for adversarial training",
    "citation_count": 229,
    "authors": []
  },
  "https://openreview.net/forum?id=rABUmU3ulQh": {
    "title": "Learning to Generate 3D Shapes with Generative Cellular Automata",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "98ec1203d439de0f7d3fd65d215852c3d6ebadad",
    "semantic_title": "learning to generate 3d shapes with generative cellular automata",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=AHm3dbp7D1D": {
    "title": "SEED: Self-supervised Distillation For Visual Representation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2ff7d8d79c1ab50c1826d965475a1eb32db0c133",
    "semantic_title": "seed: self-supervised distillation for visual representation",
    "citation_count": 197,
    "authors": []
  },
  "https://openreview.net/forum?id=vLaHRtHvfFp": {
    "title": "PDE-Driven Spatiotemporal Disentanglement",
    "volume": "poster",
    "abstract": "A recent line of work in the machine learning community addresses the problem of predicting high-dimensional spatiotemporal phenomena by leveraging specific tools from the differential equations theory. Following this direction, we propose in this article a novel and general paradigm for this task based on a resolution method for partial differential equations: the separation of variables. This inspiration allows us to introduce a dynamical interpretation of spatiotemporal disentanglement. It induces a principled model based on learning disentangled spatial and temporal representations of a phenomenon to accurately predict future observations. We experimentally demonstrate the performance and broad applicability of our method against prior state-of-the-art models on physical and synthetic video datasets",
    "checked": true,
    "id": "fef00d097117b48129f0925e9c0165496cc024b1",
    "semantic_title": "pde-driven spatiotemporal disentanglement",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=gIHd-5X324": {
    "title": "Rethinking Soft Labels for Knowledge Distillation: A Bias–Variance Tradeoff Perspective",
    "volume": "poster",
    "abstract": "Knowledge distillation is an effective approach to leverage a well-trained network or an ensemble of them, named as the teacher, to guide the training of a student network. The outputs from the teacher network are used as soft labels for supervising the training of a new network. Recent studies (M ̈uller et al., 2019; Yuan et al., 2020) revealed an intriguing property of the soft labels that making labels soft serves as a good regularization to the student network. From the perspective of statistical learning, regularization aims to reduce the variance, however how bias and variance change is not clear for training with soft labels. In this paper, we investigate the bias-variance tradeoff brought by distillation with soft labels. Specifically, we observe that during training the bias-variance tradeoff varies sample-wisely. Further, under the same distillation temperature setting, we observe that the distillation performance is negatively associated with the number of some specific samples, which are named as regularization samples since these samples lead to bias increasing and variance decreasing. Nevertheless, we empirically find that completely filtering out regularization samples also deteriorates distillation performance. Our discoveries inspired us to propose the novel weighted soft labels to help the network adaptively handle the sample-wise bias-variance tradeoff. Experiments on standard evaluation benchmarks validate the effectiveness of our method. Our code is available in the supplementary",
    "checked": true,
    "id": "be51e9141ae2af4daf3a1ba745ad3ff66a5990f3",
    "semantic_title": "rethinking soft labels for knowledge distillation: a bias-variance tradeoff perspective",
    "citation_count": 178,
    "authors": []
  },
  "https://openreview.net/forum?id=H8UHdhWG6A3": {
    "title": "Distributed Momentum for Byzantine-resilient Stochastic Gradient Descent",
    "volume": "poster",
    "abstract": "Byzantine-resilient Stochastic Gradient Descent (SGD) aims at shielding model training from Byzantine faults, be they ill-labeled training datapoints, exploited software/hardware vulnerabilities, or malicious worker nodes in a distributed setting. Two recent attacks have been challenging state-of-the-art defenses though, often successfully precluding the model from even fitting the training set. The main identified weakness in current defenses is their requirement of a sufficiently low variance-norm ratio for the stochastic gradients. We propose a practical method which, despite increasing the variance, reduces the variance-norm ratio, mitigating the identified weakness. We assess the effectiveness of our method over 736 different training configurations, comprising the 2 state-of-the-art attacks and 6 defenses. For confidence and reproducibility purposes, each configuration is run 5 times with specified seeds (1 to 5), totalling 3680 runs. In our experiments, when the attack is effective enough to decrease the highest observed top-1 cross-accuracy by at least 20% compared to the unattacked run, our technique systematically increases back the highest observed accuracy, and is able to recover at least 20% in more than 60% of the cases",
    "checked": true,
    "id": "e175f23a38f7d589266b6e059ff8e2782d9a365b",
    "semantic_title": "distributed momentum for byzantine-resilient stochastic gradient descent",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=uQfOy7LrlTR": {
    "title": "Scaling the Convex Barrier with Active Sets",
    "volume": "poster",
    "abstract": "Tight and efficient neural network bounding is of critical importance for the scaling of neural network verification systems. A number of efficient specialised dual solvers for neural network bounds have been presented recently, but they are often too loose to verify more challenging properties. This lack of tightness is linked to the weakness of the employed relaxation, which is usually a linear program of size linear in the number of neurons. While a tighter linear relaxation for piecewise linear activations exists, it comes at the cost of exponentially many constraints and thus currently lacks an efficient customised solver. We alleviate this deficiency via a novel dual algorithm that realises the full potential of the new relaxation by operating on a small active set of dual variables. Our method recovers the strengths of the new relaxation in the dual space: tightness and a linear separation oracle. At the same time, it shares the benefits of previous dual approaches for weaker relaxations: massive parallelism, GPU implementation, low cost per iteration and valid bounds at any time. As a consequence, we obtain better bounds than off-the-shelf solvers in only a fraction of their running time and recover the speed-accuracy trade-offs of looser dual solvers if the computational budget is small. We demonstrate that this results in significant formal verification speed-ups",
    "checked": true,
    "id": "69e1fab8fb4d599420cea6803f02e06fe9e0d11b",
    "semantic_title": "scaling the convex barrier with active sets",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=TiXl51SCNw8": {
    "title": "BSQ: Exploring Bit-Level Sparsity for Mixed-Precision Neural Network Quantization",
    "volume": "poster",
    "abstract": "Mixed-precision quantization can potentially achieve the optimal tradeoff between performance and compression rate of deep neural networks, and thus, have been widely investigated. However, it lacks a systematic method to determine the exact quantization scheme. Previous methods either examine only a small manually-designed search space or utilize a cumbersome neural architecture search to explore the vast search space. These approaches cannot lead to an optimal quantization scheme efficiently. This work proposes bit-level sparsity quantization (BSQ) to tackle the mixed-precision quantization from a new angle of inducing bit-level sparsity. We consider each bit of quantized weights as an independent trainable variable and introduce a differentiable bit-sparsity regularizer. BSQ can induce all-zero bits across a group of weight elements and realize the dynamic precision reduction, leading to a mixed-precision quantization scheme of the original model. Our method enables the exploration of the full mixed-precision space with a single gradient-based optimization process, with only one hyperparameter to tradeoff the performance and compression. BSQ achieves both higher accuracy and higher bit reduction on various model architectures on the CIFAR-10 and ImageNet datasets comparing to previous methods",
    "checked": true,
    "id": "2f7741e0b19c5fd927bd0cae7de915c01c220563",
    "semantic_title": "bsq: exploring bit-level sparsity for mixed-precision neural network quantization",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=Pz_dcqfcKW8": {
    "title": "Dual-mode ASR: Unify and Improve Streaming ASR with Full-context Modeling",
    "volume": "poster",
    "abstract": "Streaming automatic speech recognition (ASR) aims to emit each hypothesized word as quickly and accurately as possible, while full-context ASR waits for the completion of a full speech utterance before emitting completed hypotheses. In this work, we propose a unified framework, Dual-mode ASR, to train a single end-to-end ASR model with shared weights for both streaming and full-context speech recognition. We show that the latency and accuracy of streaming ASR significantly benefit from weight sharing and joint training of full-context ASR, especially with inplace knowledge distillation during the training. The Dual-mode ASR framework can be applied to recent state-of-the-art convolution-based and transformer-based ASR networks. We present extensive experiments with two state-of-the-art ASR networks, ContextNet and Conformer, on two datasets, a widely used public dataset LibriSpeech and a large-scale dataset MultiDomain. Experiments and ablation studies demonstrate that Dual-mode ASR not only simplifies the workflow of training and deploying streaming and full-context ASR models, but also significantly improves both emission latency and recognition accuracy of streaming ASR. With Dual-mode ASR, we achieve new state-of-the-art streaming ASR results on both LibriSpeech and MultiDomain in terms of accuracy and latency",
    "checked": true,
    "id": "de9c16610ed1181710debba81d89a39dbde1fb50",
    "semantic_title": "dual-mode asr: unify and improve streaming asr with full-context modeling",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=pzpytjk3Xb2": {
    "title": "Policy-Driven Attack: Learning to Query for Hard-label Black-box Adversarial Examples",
    "volume": "poster",
    "abstract": "To craft black-box adversarial examples, adversaries need to query the victim model and take proper advantage of its feedback. Existing black-box attacks generally suffer from high query complexity, especially when only the top-1 decision (i.e., the hard-label prediction) of the victim model is available. In this paper, we propose a novel hard-label black-box attack named Policy-Driven Attack, to reduce the query complexity. Our core idea is to learn promising search directions of the adversarial examples using a well-designed policy network in a novel reinforcement learning formulation, in which the queries become more sensible. Experimental results demonstrate that our method can significantly reduce the query complexity in comparison with existing state-of-the-art hard-label black-box attacks on various image classification benchmark datasets. Code and models for reproducing our results are available at https://github.com/ZiangYan/pda.pytorch",
    "checked": true,
    "id": "e6902aee95f52edb6a38bd364b82e1ff68afd5ff",
    "semantic_title": "policy-driven attack: learning to query for hard-label black-box adversarial examples",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=O3bqkf_Puys": {
    "title": "PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences",
    "volume": "poster",
    "abstract": "Point cloud sequences are irregular and unordered in the spatial dimension while exhibiting regularities and order in the temporal dimension. Therefore, existing grid based convolutions for conventional video processing cannot be directly applied to spatio-temporal modeling of raw point cloud sequences. In this paper, we propose a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequences. Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution is used to model the dynamics of the spatial regions along the time dimension. Furthermore, we incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequences in a hierarchical manner. Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequences",
    "checked": true,
    "id": "1b10aa9cf95b41a19317dee2e8cc11c2edd73418",
    "semantic_title": "pstnet: point spatio-temporal convolution on point cloud sequences",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=Cb54AMqHQFP": {
    "title": "Network Pruning That Matters: A Case Study on Retraining Variants",
    "volume": "poster",
    "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning",
    "checked": true,
    "id": "dfe2efeea8889a1937be16538220f5e3477d42fb",
    "semantic_title": "network pruning that matters: a case study on retraining variants",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=lWaz5a9lcFU": {
    "title": "EEC: Learning to Encode and Regenerate Images for Continual Learning",
    "volume": "poster",
    "abstract": "The two main impediments to continual learning are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17% over state-of-the-art methods on benchmark datasets, while requiring 78% less storage space",
    "checked": true,
    "id": "557c15c4e3306676b8a4d053c7295880608d3fd0",
    "semantic_title": "eec: learning to encode and regenerate images for continual learning",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=KLH36ELmwIB": {
    "title": "DARTS-: Robustly Stepping out of Performance Collapse Without Indicators",
    "volume": "poster",
    "abstract": "Despite the fast development of differentiable architecture search (DARTS), it suffers from a standing instability issue regarding searching performance, which extremely limits its application. Existing robustifying methods draw clues from the outcome instead of finding out the causing factor. Various indicators such as Hessian eigenvalues are proposed as a signal of performance collapse, and the searching should be stopped once an indicator reaches a preset threshold. However, these methods tend to easily reject good architectures if thresholds are inappropriately set, let alone the searching is intrinsically noisy. In this paper, we undertake a more subtle and direct approach to resolve the collapse. We first demonstrate that skip connections with a learnable architectural coefficient can easily recover from a disadvantageous state and become dominant. We conjecture that skip connections profit too much from this privilege, hence causing the collapse for the derived model. Therefore, we propose to factor out this benefit with an auxiliary skip connection, ensuring a fairer competition for all operations. Extensive experiments on various datasets verify that our approach can substantially improve the robustness of DARTS. Our code is available at https://github.com/Meituan-AutoML/DARTS-",
    "checked": true,
    "id": "10daddaa1e1ed27e88b08b1c124d800de865c5e3",
    "semantic_title": "darts-: robustly stepping out of performance collapse without indicators",
    "citation_count": 170,
    "authors": []
  },
  "https://openreview.net/forum?id=9QLRCVysdlO": {
    "title": "BiPointNet: Binary Neural Network for Point Clouds",
    "volume": "poster",
    "abstract": "To alleviate the resource constraint for real-time point cloud applications that run on edge devices, in this paper we present BiPointNet, the first model binarization approach for efficient deep learning on point clouds. We discover that the immense performance drop of binarized models for point clouds mainly stems from two challenges: aggregation-induced feature homogenization that leads to a degradation of information entropy, and scale distortion that hinders optimization and invalidates scale-sensitive structures. With theoretical justifications and in-depth analysis, our BiPointNet introduces Entropy-Maximizing Aggregation (EMA) to modulate the distribution before aggregation for the maximum information entropy, and Layer-wise Scale Recovery (LSR) to efficiently restore feature representation capacity. Extensive experiments show that BiPointNet outperforms existing binarization methods by convincing margins, at the level even comparable with the full precision counterpart. We highlight that our techniques are generic, guaranteeing significant improvements on various fundamental tasks and mainstream backbones. Moreover, BiPointNet gives an impressive 14.7× speedup and 18.9× storage saving on real-world resource-constrained devices",
    "checked": true,
    "id": "404bf88bd0194e2ad1de79b4730c70bff5c841d4",
    "semantic_title": "bipointnet: binary neural network for point clouds",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=bM3L3I_853": {
    "title": "AdaFuse: Adaptive Temporal Fusion Network for Efficient Action Recognition",
    "volume": "poster",
    "abstract": "Temporal modelling is the key for efficient video action recognition. While understanding temporal information can improve recognition accuracy for dynamic actions, removing temporal redundancy and reusing past features can significantly save computation leading to efficient action recognition. In this paper, we introduce an adaptive temporal fusion network, called AdaFuse, that dynamically fuses channels from current and past feature maps for strong temporal modelling. Specifically, the necessary information from the historical convolution feature maps is fused with current pruned feature maps with the goal of improving both recognition accuracy and efficiency. In addition, we use a skipping operation to further reduce the computation cost of action recognition. Extensive experiments on SomethingV1 & V2, Jester and Mini-Kinetics show that our approach can achieve about 40% computation savings with comparable accuracy to state-of-the-art methods. The project page can be found at https://mengyuest.github.io/AdaFuse/",
    "checked": true,
    "id": "0b4f9d1c5dbf8e8f4735a5eb0434f264116b47ba",
    "semantic_title": "adafuse: adaptive temporal fusion network for efficient action recognition",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=M88oFvqp_9": {
    "title": "Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains",
    "volume": "poster",
    "abstract": "We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars). The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively. This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains. Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains",
    "checked": true,
    "id": "5e2b2ca2a088b86307e00b4534b18dc8d8ed7809",
    "semantic_title": "generating furry cars: disentangling object shape and appearance across multiple domains",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Ig-VyQc-MLK": {
    "title": "Pruning Neural Networks at Initialization: Why Are We Missing the Mark?",
    "volume": "poster",
    "abstract": "Recent work has explored the possibility of pruning neural networks at initialization. We assess proposals for doing so: SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), and magnitude pruning. Although these methods surpass the trivial baseline of random pruning, they remain below the accuracy of magnitude pruning after training, and we endeavor to understand why. We show that, unlike pruning after training, randomly shuffling the weights these methods prune within each layer or sampling new initial values preserves or improves accuracy. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune. This property suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both",
    "checked": true,
    "id": "0932abfd0fb90e8a28f7bd195633c9891bfd7ecb",
    "semantic_title": "pruning neural networks at initialization: why are we missing the mark?",
    "citation_count": 242,
    "authors": []
  },
  "https://openreview.net/forum?id=POWv6hDd9XH": {
    "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction",
    "volume": "poster",
    "abstract": "We study the challenging task of neural network quantization without end-to-end retraining, called Post-training Quantization (PTQ). PTQ usually requires a small subset of training data but produces less powerful quantized models than Quantization-Aware Training (QAT). In this work, we propose a novel PTQ framework, dubbed BRECQ, which pushes the limits of bitwidth in PTQ down to INT2 for the first time. BRECQ leverages the basic building blocks in neural networks and reconstructs them one-by-one. In a comprehensive theoretical study of the second-order error, we show that BRECQ achieves a good balance between cross-layer dependency and generalization error. To further employ the power of quantization, the mixed precision technique is incorporated in our framework by approximating the inter-layer and intra-layer sensitivity. Extensive experiments on various handcrafted and searched neural architectures are conducted for both image classification and object detection tasks. And for the first time we prove that, without bells and whistles, PTQ can attain 4-bit ResNet and MobileNetV2 comparable with QAT and enjoy 240 times faster production of quantized models. Codes are available at https://github.com/yhhhli/BRECQ",
    "checked": true,
    "id": "6dffdeb81ebc761a8cce1e36b8d140d5b8d2a0e3",
    "semantic_title": "brecq: pushing the limit of post-training quantization by block reconstruction",
    "citation_count": 464,
    "authors": []
  }
}