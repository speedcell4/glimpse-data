{
  "https://openreview.net/forum?id=RGJbergVIoO": {
    "title": "On the mapping between Hopfield networks and Restricted Boltzmann Machines",
    "volume": "oral",
    "abstract": "Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two important models at the interface of statistical physics, machine learning, and neuroscience. Recently, there has been interest in the relationship between HNs and RBMs, due to their similarity under the statistical mechanics formalism. An exact mapping between HNs and RBMs has been previously noted for the special case of orthogonal (\"uncorrelated\") encoded patterns. We present here an exact mapping in the case of correlated pattern HNs, which are more broadly applicable to existing datasets. Specifically, we show that any HN with $N$ binary variables and $p<N$ potentially correlated binary patterns can be transformed into an RBM with $N$ binary visible variables and $p$ gaussian hidden variables. We outline the conditions under which the reverse mapping exists, and conduct experiments on the MNIST dataset which suggest the mapping provides a useful initialization to the RBM weights. We discuss extensions, the potential importance of this correspondence for the training of RBMs, and for understanding the performance of feature extraction methods which utilize RBMs",
    "checked": true,
    "id": "593d8f1de69f6f335609e1f0e6b3ae83ce4e3116",
    "semantic_title": "on the mapping between hopfield networks and restricted boltzmann machines",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Mos9F9kDwkz": {
    "title": "Complex Query Answering with Neural Link Predictors",
    "volume": "oral",
    "abstract": "Neural link predictors are immensely useful for identifying missing edges in large scale Knowledge Graphs. However, it is still not clear how to use these models for answering more complex queries that arise in a number of domains, such as queries using logical conjunctions ($\\land$), disjunctions ($\\lor$) and existential quantifiers ($\\exists$), while accounting for missing edges. In this work, we propose a framework for efficiently answering complex queries on incomplete Knowledge Graphs. We translate each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor. We then analyse two solutions to the optimisation problem, including gradient-based and combinatorial search. In our experiments, the proposed approach produces more accurate results than state-of-the-art methods --- black-box neural models trained on millions of generated queries --- without the need of training on a large and diverse set of complex queries. Using orders of magnitude less training data, we obtain relative improvements ranging from 8% up to 40% in Hits@3 across different knowledge graphs containing factual information. Finally, we demonstrate that it is possible to explain the outcome of our model in terms of the intermediate solutions identified for each of the complex query atoms. All our source code and datasets are available online, at https://github.com/uclnlp/cqd",
    "checked": true,
    "id": "62ba6604ed24c808062f89f8f41eda3b6a566917",
    "semantic_title": "complex query answering with neural link predictors",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=Wj4ODo0uyCF": {
    "title": "Share or Not? Learning to Schedule Language-Specific Capacity for Multilingual Translation",
    "volume": "oral",
    "abstract": "Using a mix of shared and language-specific (LS) parameters has shown promise in multilingual neural machine translation (MNMT), but the question of when and where LS capacity matters most is still under-studied. We offer such a study by proposing conditional language-specific routing (CLSR). CLSR employs hard binary gates conditioned on token representations to dynamically select LS or shared paths. By manipulating these gates, it can schedule LS capacity across sub-layers in MNMT subject to the guidance of translation signals and budget constraints. Moreover, CLSR can easily scale up to massively multilingual settings. Experiments with Transformer on OPUS-100 and WMT datasets show that: 1) MNMT is sensitive to both the amount and the position of LS modeling: distributing 10%-30% LS computation to the top and/or bottom encoder/decoder layers delivers the best performance; and 2) one-to-many translation benefits more from CLSR compared to many-to-one translation, particularly with unbalanced training data. Our study further verifies the trade-off between the shared capacity and LS capacity for multilingual translation. We corroborate our analysis by confirming the soundness of our findings as foundation of our improved multilingual Transformers. Source code and models are available at https://github.com/bzhangGo/zero/tree/iclr2021_clsr",
    "checked": true,
    "id": "940214c9afde6664127ba552de2ac8fa67997769",
    "semantic_title": "share or not? learning to schedule language-specific capacity for multilingual translation",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=rsf1z-JSj87": {
    "title": "End-to-end Adversarial Text-to-Speech",
    "volume": "oral",
    "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision",
    "checked": true,
    "id": "c035cf0e1231eb196968d7255ab55827e932ec7a",
    "semantic_title": "end-to-end adversarial text-to-speech",
    "citation_count": 190,
    "authors": []
  },
  "https://openreview.net/forum?id=gvxJzw8kW4b": {
    "title": "Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity",
    "volume": "oral",
    "abstract": "While deep neural networks show great performance on fitting to the training distribution, improving the networks' generalization performance to the test distribution and robustness to the sensitivity to input perturbations still remain as a challenge. Although a number of mixup based augmentation strategies have been proposed to partially address them, it remains unclear as to how to best utilize the supervisory signal within each input data for mixup from the optimization perspective. We propose a new perspective on batch mixup and formulate the optimal construction of a batch of mixup data maximizing the data saliency measure of each individual mixup data and encouraging the supermodular diversity among the constructed mixup data. This leads to a novel discrete optimization problem minimizing the difference between submodular functions. We also propose an efficient modular approximation based iterative submodular minimization algorithm for efficient mixup computation per each minibatch suitable for minibatch based neural network training. Our experiments show the proposed method achieves the state of the art generalization, calibration, and weakly supervised localization results compared to other mixup methods. The source code is available at https://github.com/snu-mllab/Co-Mixup",
    "checked": true,
    "id": "39caa9091318480f3241117ecefba897548cc42a",
    "semantic_title": "co-mixup: saliency guided joint mixup with supermodular diversity",
    "citation_count": 188,
    "authors": []
  },
  "https://openreview.net/forum?id=Ud3DSz72nYR": {
    "title": "Contrastive Explanations for Reinforcement Learning via Embedded Self Predictions",
    "volume": "oral",
    "abstract": "We investigate a deep reinforcement learning (RL) architecture that supports explaining why a learned agent prefers one action over another. The key idea is to learn action-values that are directly represented via human-understandable properties of expected futures. This is realized via the embedded self-prediction (ESP) model, which learns said properties in terms of human provided features. Action preferences can then be explained by contrasting the future properties predicted for each action. To address cases where there are a large number of features, we develop a novel method for computing minimal sufficient explanations from an ESP. Our case studies in three domains, including a complex strategy game, show that ESP models can be effectively learned and support insightful explanations",
    "checked": true,
    "id": "2e0b1b6ac7a48e1c992cb26967c7272c5e798757",
    "semantic_title": "contrastive explanations for reinforcement learning via embedded self predictions",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=uCY5MuAxcxU": {
    "title": "Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?",
    "volume": "oral",
    "abstract": "Convolutional neural networks often dominate fully-connected counterparts in generalization performance, especially on image classification tasks. This is often explained in terms of \\textquotedblleft better inductive bias.\\textquotedblright\\ However, this has not been made mathematically rigorous, and the hurdle is that the sufficiently wide fully-connected net can always simulate the convolutional net. Thus the training algorithm plays a role. The current work describes a natural task on which a provable sample complexity gap can be shown, for standard training algorithms. We construct a single natural distribution on $\\mathbb{R}^d\\times\\{\\pm 1\\}$ on which any orthogonal-invariant algorithm (i.e. fully-connected networks trained with most gradient-based methods from gaussian initialization) requires $\\Omega(d^2)$ samples to generalize while $O(1)$ samples suffice for convolutional architectures. Furthermore, we demonstrate a single target function, learning which on all possible distributions leads to an $O(1)$ vs $\\Omega(d^2/\\varepsilon)$ gap. The proof relies on the fact that SGD on fully-connected network is orthogonal equivariant. Similar results are achieved for $\\ell_2$ regression and adaptive training algorithms, e.g. Adam and AdaGrad, which are only permutation equivariant",
    "checked": true,
    "id": "54f105cd0ccd999b9aa01636edb736489a24718c",
    "semantic_title": "why are convolutional nets more sample-efficient than fully-connected nets?",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=Pd_oMxH8IlF": {
    "title": "Iterated learning for emergent systematicity in VQA",
    "volume": "oral",
    "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR",
    "checked": true,
    "id": "268d4feafa9532329f3e744adb0cdece8bbc80a4",
    "semantic_title": "iterated learning for emergent systematicity in vqa",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=tW4QEInpni": {
    "title": "When Do Curricula Work?",
    "volume": "oral",
    "abstract": "Inspired by human learning, researchers have proposed ordering examples during training based on their difficulty. Both curriculum learning, exposing a network to easier examples early in training, and anti-curriculum learning, showing the most difficult examples first, have been suggested as improvements to the standard i.i.d. training. In this work, we set out to investigate the relative benefits of ordered learning. We first investigate the implicit curricula resulting from architectural and optimization bias and find that samples are learned in a highly consistent order. Next, to quantify the benefit of explicit curricula, we conduct extensive experiments over thousands of orderings spanning three kinds of learning: curriculum, anti-curriculum, and random-curriculum -- in which the size of the training dataset is dynamically increased over time, but the examples are randomly ordered. We find that for standard benchmark datasets, curricula have only marginal benefits, and that randomly ordered samples perform as well or better than curricula and anti-curricula, suggesting that any benefit is entirely due to the dynamic training set size. Inspired by common use cases of curriculum learning in practice, we investigate the role of limited training time budget and noisy data in the success of curriculum learning. Our experiments demonstrate that curriculum, but not anti-curriculum or random ordering can indeed improve the performance either with limited training time budget or in the existence of noisy data",
    "checked": true,
    "id": "9d2c96574019305a8c86cc5b84cb9f616ccf0eb3",
    "semantic_title": "when do curricula work?",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=m5Qsh0kBQG": {
    "title": "Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients",
    "volume": "oral",
    "abstract": "Discovering the underlying mathematical expressions describing a dataset is a core challenge for artificial intelligence. This is the problem of $\\textit{symbolic regression}$. Despite recent advances in training neural networks to solve complex tasks, deep learning approaches to symbolic regression are underexplored. We propose a framework that leverages deep learning for symbolic regression via a simple idea: use a large model to search the space of small models. Specifically, we use a recurrent neural network to emit a distribution over tractable mathematical expressions and employ a novel risk-seeking policy gradient to train the network to generate better-fitting expressions. Our algorithm outperforms several baseline methods (including Eureqa, the gold standard for symbolic regression) in its ability to exactly recover symbolic expressions on a series of benchmark problems, both with and without added noise. More broadly, our contributions include a framework that can be applied to optimize hierarchical, variable-length objects under a black-box performance metric, with the ability to incorporate constraints in situ, and a risk-seeking policy gradient formulation that optimizes for best-case performance instead of expected performance",
    "checked": true,
    "id": "3ee45877f7f14c8ee4872a7249f74eef7dc2255f",
    "semantic_title": "deep symbolic regression: recovering mathematical expressions from data via risk-seeking policy gradients",
    "citation_count": 336,
    "authors": []
  },
  "https://openreview.net/forum?id=rJA5Pz7lHKb": {
    "title": "Improved Autoregressive Modeling with Distribution Smoothing",
    "volume": "oral",
    "abstract": "While autoregressive models excel at image compression, their sample quality is often lacking. Although not realistic, generated images often have high likelihood according to the model, resembling the case of adversarial examples. Inspired by a successful adversarial defense method, we incorporate randomized smoothing into autoregressive generative modeling. We first model a smoothed version of the data distribution, and then reverse the smoothing process to recover the original data distribution. This procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world image datasets while obtaining competitive likelihoods on synthetic datasets",
    "checked": true,
    "id": "0649abe307a6d76e3dfd78382aa224b82286ebe8",
    "semantic_title": "improved autoregressive modeling with distribution smoothing",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=PxTIG12RRHS": {
    "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
    "volume": "oral",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model",
    "checked": true,
    "id": "633e2fbfc0b21e959a244100937c5853afca4853",
    "semantic_title": "score-based generative modeling through stochastic differential equations",
    "citation_count": 7168,
    "authors": []
  },
  "https://openreview.net/forum?id=KvyxFqZS_D": {
    "title": "Global Convergence of Three-layer Neural Networks in the Mean Field Regime",
    "volume": "oral",
    "abstract": "In the mean field regime, neural networks are appropriately scaled so that as the width tends to infinity, the learning dynamics tends to a nonlinear and nontrivial dynamical limit, known as the mean field limit. This lends a way to study large-width neural networks via analyzing the mean field limit. Recent works have successfully applied such analysis to two-layer networks and provided global convergence guarantees. The extension to multilayer ones however has been a highly challenging puzzle, and little is known about the optimization efficiency in the mean field regime when there are more than two layers. In this work, we prove a global convergence result for unregularized feedforward three-layer networks in the mean field regime. We first develop a rigorous framework to establish the mean field limit of three-layer networks under stochastic gradient descent training. To that end, we propose the idea of a neuronal embedding, which comprises of a fixed probability space that encapsulates neural networks of arbitrary sizes. The identified mean field limit is then used to prove a global convergence guarantee under suitable regularity and convergence mode assumptions, which – unlike previous works on two-layer networks – does not rely critically on convexity. Underlying the result is a universal approximation property, natural of neural networks, which importantly is shown to hold at any finite training time (not necessarily at convergence) via an algebraic topology argument",
    "checked": true,
    "id": "13d2a545707741d6a7ac2679c5cf0a2a4821bab1",
    "semantic_title": "global convergence of three-layer neural networks in the mean field regime",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=PKubaeJkw3": {
    "title": "Rethinking Architecture Selection in Differentiable NAS",
    "volume": "oral",
    "abstract": "Differentiable Neural Architecture Search is one of the most popular Neural Architecture Search (NAS) methods for its search efficiency and simplicity, accomplished by jointly optimizing the model weight and architecture parameters in a weight-sharing supernet via gradient-based algorithms. At the end of the search phase, the operations with the largest architecture parameters will be selected to form the final architecture, with the implicit assumption that the values of architecture parameters reflect the operation strength. While much has been discussed about the supernet's optimization, the architecture selection process has received little attention. We provide empirical and theoretical analysis to show that the magnitude of architecture parameters does not necessarily indicate how much the operation contributes to the supernet's performance. We propose an alternative perturbation-based architecture selection that directly measures each operation's influence on the supernet. We re-evaluate several differentiable NAS methods with the proposed architecture selection and find that it is able to extract significantly improved architectures from the underlying supernets consistently. Furthermore, we find that several failure modes of DARTS can be greatly alleviated with the proposed selection method, indicating that much of the poor generalization observed in DARTS can be attributed to the failure of magnitude-based architecture selection rather than entirely the optimization of its supernet",
    "checked": true,
    "id": "958cc02ad685721d64e6321aa93e3125884ed253",
    "semantic_title": "rethinking architecture selection in differentiable nas",
    "citation_count": 183,
    "authors": []
  },
  "https://openreview.net/forum?id=0XXpJ4OtjW": {
    "title": "Evolving Reinforcement Learning Algorithms",
    "volume": "oral",
    "abstract": "We propose a method for meta-learning reinforcement learning algorithms by searching over the space of computational graphs which compute the loss function for a value-based model-free RL agent to optimize. The learned algorithms are domain-agnostic and can generalize to new environments not seen during training. Our method can both learn from scratch and bootstrap off known existing algorithms, like DQN, enabling interpretable modifications which improve performance. Learning from scratch on simple classical control and gridworld tasks, our method rediscovers the temporal-difference (TD) algorithm. Bootstrapped from DQN, we highlight two learned algorithms which obtain good generalization performance over other classical control tasks, gridworld type tasks, and Atari games. The analysis of the learned algorithm behavior shows resemblance to recently proposed RL algorithms that address overestimation in value-based methods",
    "checked": true,
    "id": "431e873121643a3e02eeebc4efe95920a3396d7a",
    "semantic_title": "evolving reinforcement learning algorithms",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=yWkP7JuHX1": {
    "title": "Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering",
    "volume": "oral",
    "abstract": "Differentiable rendering has paved the way to training neural networks to perform \"inverse graphics\" tasks such as predicting 3D geometry from monocular photographs. To train high performing models, most of the current approaches rely on multi-view imagery which are not readily available in practice. Recent Generative Adversarial Networks (GANs) that synthesize images, in contrast, seem to acquire 3D knowledge implicitly during training: object viewpoints can be manipulated by simply manipulating the latent codes. However, these latent codes often lack further physical interpretation and thus GANs cannot easily be inverted to perform explicit 3D reasoning. In this paper, we aim to extract and disentangle 3D knowledge learned by generative models by utilizing differentiable renderers. Key to our approach is to exploit GANs as a multi-view data generator to train an inverse graphics network using an off-the-shelf differentiable renderer, and the trained inverse graphics network as a teacher to disentangle the GAN's latent code into interpretable 3D properties. The entire architecture is trained iteratively using cycle consistency losses. We show that our approach significantly outperforms state-of-the-art inverse graphics networks trained on existing datasets, both quantitatively and via user studies. We further showcase the disentangled GAN as a controllable 3D \"neural renderer\", complementing traditional graphics renderers",
    "checked": true,
    "id": "288e170e5771200653b2f65d4837b74295b2c258",
    "semantic_title": "image gans meet differentiable rendering for inverse graphics and interpretable 3d neural rendering",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=wWK7yXkULyh": {
    "title": "MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training",
    "volume": "oral",
    "abstract": "Recent advances by practitioners in the deep learning community have breathed new life into Locality Sensitive Hashing (LSH), using it to reduce memory and time bottlenecks in neural network (NN) training. However, while LSH has sub-linear guarantees for approximate near-neighbor search in theory, it is known to have inefficient query time in practice due to its use of random hash functions. Moreover, when model parameters are changing, LSH suffers from update overhead. This work is motivated by an observation that model parameters evolve slowly, such that the changes do not always require an LSH update to maintain performance. This phenomenon points to the potential for a reduction in update time and allows for a modified learnable version of data-dependent LSH to improve query time at a low cost. We use the above insights to build MONGOOSE, an end-to-end LSH framework for efficient NN training. In particular, MONGOOSE is equipped with a scheduling algorithm to adaptively perform LSH updates with provable guarantees and learnable hash functions to improve query efficiency. Empirically, we validate MONGOOSE on large-scale deep learning models for recommendation systems and language modeling. We find that it achieves up to 8% better accuracy compared to previous LSH approaches, with $6.5 \\times$ speed-up and $6\\times$ reduction in memory usage",
    "checked": true,
    "id": "b745b5512ad3b1f652dc0cbb5ddf5a940f397f7d",
    "semantic_title": "mongoose: a learnable lsh framework for efficient neural network training",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=Ysuv-WOFeKR": {
    "title": "Parrot: Data-Driven Behavioral Priors for Reinforcement Learning",
    "volume": "oral",
    "abstract": "Reinforcement learning provides a general framework for flexible decision making and control, but requires extensive data collection for each new task that an agent needs to learn. In other machine learning fields, such as natural language processing or computer vision, pre-training on large, previously collected datasets to bootstrap learning for new tasks has emerged as a powerful paradigm to reduce data requirements when learning a new task. In this paper, we ask the following question: how can we enable similarly useful pre-training for RL agents? We propose a method for pre-training behavioral priors that can capture complex input-output relationships observed in successful trials from a wide range of previously seen tasks, and we show how this learned prior can be used for rapidly learning new tasks without impeding the RL agent's ability to try out novel behaviors. We demonstrate the effectiveness of our approach in challenging robotic manipulation domains involving image observations and sparse reward functions, where our method outperforms prior works by a substantial margin. Additional materials can be found on our project website: https://sites.google.com/view/parrot-rl",
    "checked": true,
    "id": "f5275f5eb6569ddb5ba9a959ede09875d56e3bac",
    "semantic_title": "parrot: data-driven behavioral priors for reinforcement learning",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=DktZb97_Fx": {
    "title": "SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness",
    "volume": "oral",
    "abstract": "In this paper, we cast fair machine learning as invariant machine learning. We first formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias",
    "checked": true,
    "id": "2c0f18df2f23207f2139140ab45819b0b23879be",
    "semantic_title": "sensei: sensitive set invariance for enforcing individual fairness",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=rC8sJ4i6kaH": {
    "title": "Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data",
    "volume": "oral",
    "abstract": "Self-training algorithms, which train a model to fit pseudolabels predicted by another previously-learned model, have been very successful for learning with unlabeled data using neural networks. However, the current theoretical understanding of self-training only applies to linear models. This work provides a unified theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervised learning. At the core of our analysis is a simple but realistic \"expansion\" assumption, which states that a low-probability subset of the data must expand to a neighborhood with large probability relative to the subset. We also assume that neighborhoods of examples in different classes have minimal overlap. We prove that under these assumptions, the minimizers of population objectives based on self-training and input-consistency regularization will achieve high accuracy with respect to ground-truth labels. By using off-the-shelf generalization bounds, we immediately convert this result to sample complexity guarantees for neural nets that are polynomial in the margin and Lipschitzness. Our results help explain the empirical successes of recently proposed self-training algorithms which use input consistency regularization",
    "checked": true,
    "id": "5a41200eeee536e101b6e462014e7396f4841c28",
    "semantic_title": "theoretical analysis of self-training with deep networks on unlabeled data",
    "citation_count": 235,
    "authors": []
  },
  "https://openreview.net/forum?id=wb3wxCObbRT": {
    "title": "Growing Efficient Deep Networks by Structured Continuous Sparsification",
    "volume": "oral",
    "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives. Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters. By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training. For example, we achieve $49.7\\%$ inference FLOPs and $47.4\\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\\%$ top-1 validation accuracy --- all without any dedicated fine-tuning stage. Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods",
    "checked": true,
    "id": "910c8790bc7eb7bb0c66fa470f243c1cf2626119",
    "semantic_title": "growing efficient deep networks by structured continuous sparsification",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=RmB-88r9dL": {
    "title": "VCNet and Functional Targeted Regularization For Learning Causal Effects of Continuous Treatments",
    "volume": "oral",
    "abstract": "Motivated by the rising abundance of observational data with continuous treatments, we investigate the problem of estimating the average dose-response curve (ADRF). Available parametric methods are limited in their model space, and previous attempts in leveraging neural network to enhance model expressiveness relied on partitioning continuous treatment into blocks and using separate heads for each block; this however produces in practice discontinuous ADRFs. Therefore, the question of how to adapt the structure and training of neural network to estimate ADRFs remains open. This paper makes two important contributions. First, we propose a novel varying coefficient neural network (VCNet) that improves model expressiveness while preserving continuity of the estimated ADRF. Second, to improve finite sample performance, we generalize targeted regularization to obtain a doubly robust estimator of the whole ADRF curve",
    "checked": true,
    "id": "7bb7b8406d5aa3149adf121dc257def529e59cf8",
    "semantic_title": "vcnet and functional targeted regularization for learning causal effects of continuous treatments",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=NzTU59SYbNq": {
    "title": "EigenGame: PCA as a Nash Equilibrium",
    "volume": "oral",
    "abstract": "We present a novel view on principal components analysis as a competitive game in which each approximate eigenvector is controlled by a player whose goal is to maximize their own utility function. We analyze the properties of this PCA game and the behavior of its gradient based updates. The resulting algorithm---which combines elements from Oja's rule with a generalized Gram-Schmidt orthogonalization---is naturally decentralized and hence parallelizable through message passing. We demonstrate the scalability of the algorithm with experiments on large image datasets and neural network activations. We discuss how this new view of PCA as a differentiable game can lead to further algorithmic developments and insights",
    "checked": true,
    "id": "17281d4de91cdd5c480f6f35a5e1a33b7abb18d2",
    "semantic_title": "eigengame: pca as a nash equilibrium",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=xpx9zj7CUlY": {
    "title": "Randomized Automatic Differentiation",
    "volume": "oral",
    "abstract": "The successes of deep learning, variational inference, and many other fields have been aided by specialized implementations of reverse-mode automatic differentiation (AD) to compute gradients of mega-dimensional objectives. The AD techniques underlying these tools were designed to compute exact gradients to numerical precision, but modern machine learning models are almost always trained with stochastic gradient descent. Why spend computation and memory on exact (minibatch) gradients only to use them for stochastic optimization? We develop a general framework and approach for randomized automatic differentiation (RAD), which can allow unbiased gradient estimates to be computed with reduced memory in return for variance. We examine limitations of the general approach, and argue that we must leverage problem specific structure to realize benefits. We develop RAD techniques for a variety of simple neural network architectures, and show that for a fixed memory budget, RAD converges in fewer iterations than using a small batch size for feedforward networks, and in a similar number for recurrent networks. We also show that RAD can be applied to scientific computing, and use it to develop a low-memory stochastic gradient method for optimizing the control parameters of a linear reaction-diffusion PDE representing a fission reactor",
    "checked": true,
    "id": "766bd9633ea0ed0ee2d8cc11ab083322666a3db9",
    "semantic_title": "randomized automatic differentiation",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=jWkw45-9AbL": {
    "title": "A Distributional Approach to Controlled Text Generation",
    "volume": "oral",
    "abstract": "We propose a Distributional Approach for addressing Controlled Text Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both \"pointwise'\" and \"distributional\" constraints over the target LM — to our knowledge, the first model with such generality —while minimizing KL divergence from the initial LM distribution. The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient. We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM. We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models. Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence. Code available at https://github.com/naver/gdc",
    "checked": true,
    "id": "07fd366a8ebdefe54cdb57d87c81dcd22de25a91",
    "semantic_title": "a distributional approach to controlled text generation",
    "citation_count": 120,
    "authors": []
  },
  "https://openreview.net/forum?id=YicbFdNTTy": {
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "volume": "oral",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train",
    "checked": true,
    "id": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
    "semantic_title": "an image is worth 16x16 words: transformers for image recognition at scale",
    "citation_count": 44497,
    "authors": []
  },
  "https://openreview.net/forum?id=XSLF1XFq5h": {
    "title": "Getting a CLUE: A Method for Explaining Uncertainty Estimates",
    "volume": "oral",
    "abstract": "Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input's prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty",
    "checked": true,
    "id": "aa9df1b1d7ca16ff3357a662714d15fd5340dce4",
    "semantic_title": "getting a clue: a method for explaining uncertainty estimates",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=PULSD5qI2N1": {
    "title": "Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime",
    "volume": "oral",
    "abstract": "We analyze the convergence of the averaged stochastic gradient descent for overparameterized two-layer neural networks for regression problems. It was recently found that a neural tangent kernel (NTK) plays an important role in showing the global convergence of gradient-based methods under the NTK regime, where the learning dynamics for overparameterized neural networks can be almost characterized by that for the associated reproducing kernel Hilbert space (RKHS). However, there is still room for a convergence rate analysis in the NTK regime. In this study, we show that the averaged stochastic gradient descent can achieve the minimax optimal convergence rate, with the global convergence guarantee, by exploiting the complexities of the target function and the RKHS associated with the NTK. Moreover, we show that the target function specified by the NTK of a ReLU network can be learned at the optimal convergence rate through a smooth approximation of a ReLU network under certain conditions",
    "checked": true,
    "id": "54e35cfa780f6444cfb3750ec9bb0c4d3e83ee16",
    "semantic_title": "optimal rates for averaged stochastic gradient descent under neural tangent kernel regime",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=-2FCwDKRREu": {
    "title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction",
    "volume": "oral",
    "abstract": "We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference",
    "checked": true,
    "id": "518b827e340c26582b5093401283a4f5cff605b9",
    "semantic_title": "learning invariant representations for reinforcement learning without reconstruction",
    "citation_count": 495,
    "authors": []
  },
  "https://openreview.net/forum?id=dYeAHXnpWJ4": {
    "title": "Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability",
    "volume": "oral",
    "abstract": "Current methods for the interpretability of discriminative deep neural networks commonly rely on the model's input-gradients, i.e., the gradients of the output logits w.r.t. the inputs. The common assumption is that these input-gradients contain information regarding $p_{\\theta} ( y\\mid \\mathbf{x} )$, the model's discriminative capabilities, thus justifying their use for interpretability. However, in this work, we show that these input-gradients can be arbitrarily manipulated as a consequence of the shift-invariance of softmax without changing the discriminative function. This leaves an open question: given that input-gradients can be arbitrary, why are they highly structured and explanatory in standard models? In this work, we re-interpret the logits of standard softmax-based classifiers as unnormalized log-densities of the data distribution and show that input-gradients can be viewed as gradients of a class-conditional generative model $p_{\\theta}(\\mathbf{x} \\mid y)$ implicit in the discriminative model. This leads us to hypothesize that the highly structured and explanatory nature of input-gradients may be due to the alignment of this class-conditional model $p_{\\theta}(\\mathbf{x} \\mid y)$ with that of the ground truth data distribution $p_{\\text{data}} (\\mathbf{x} \\mid y)$. We test this hypothesis by studying the effect of density alignment on gradient explanations. To achieve this density alignment, we use an algorithm called score-matching, and propose novel approximations to this algorithm to enable training large-scale models. Our experiments show that improving the alignment of the implicit density model with the data distribution enhances gradient structure and explanatory power while reducing this alignment has the opposite effect. This also leads us to conjecture that unintended density alignment in standard neural network training may explain the highly structured nature of input-gradients observed in practice. Overall, our finding that input-gradients capture information regarding an implicit generative model implies that we need to re-think their use for interpreting discriminative models",
    "checked": true,
    "id": "d8a9a7c7b051453b04f6d52769b21ab62ac91628",
    "semantic_title": "rethinking the role of gradient-based attribution methods for model interpretability",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=cPZOyoDloxl": {
    "title": "SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "c4c30bd9fc777f675e9a3cb8b5bb76d44a5d3d6b",
    "semantic_title": "smirl: surprise minimizing reinforcement learning in unstable environments",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=rALA0Xo6yNJ": {
    "title": "Learning to Reach Goals via Iterated Supervised Learning",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "831e7cbafed2dca05db1e7f5ef16d1a7614f44ec",
    "semantic_title": "learning to reach goals via iterated supervised learning",
    "citation_count": 188,
    "authors": []
  },
  "https://openreview.net/forum?id=O3Y56aqpChA": {
    "title": "Self-training For Few-shot Transfer Across Extreme Task Differences",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "1d16d4cdc3fcce26e2c2097d13896ec09683eee3",
    "semantic_title": "self-training for few-shot transfer across extreme task differences",
    "citation_count": 112,
    "authors": []
  },
  "https://openreview.net/forum?id=B7v4QMR6Z9w": {
    "title": "Federated Learning Based on Dynamic Regularization",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "5a3d70689925df014c46d1cd50dfc8a368cb4c86",
    "semantic_title": "federated learning based on dynamic regularization",
    "citation_count": 826,
    "authors": []
  },
  "https://openreview.net/forum?id=Mk6PZtgAgfq": {
    "title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "e61db54fe46bd13e1ef7a6ce293f4db202e3317e",
    "semantic_title": "rao-blackwellizing the straight-through gumbel-softmax gradient estimator",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=F3s69XzWOia": {
    "title": "Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "fac144db777ddc3d85bb314087889689293affa0",
    "semantic_title": "coupled oscillatory recurrent neural network (cornn): an accurate and (gradient) stable architecture for learning long time dependencies",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=a-xFK8Ymz5J": {
    "title": "DiffWave: A Versatile Diffusion Model for Audio Synthesis",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "34bf13e58c7226d615afead0c0f679432502940e",
    "semantic_title": "diffwave: a versatile diffusion model for audio synthesis",
    "citation_count": 1534,
    "authors": []
  },
  "https://openreview.net/forum?id=QIRlze3I6hX": {
    "title": "Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "0003b8fef7d5e048a9870cdafdec27af129ae990",
    "semantic_title": "learning cross-domain correspondence for control with dynamics cycle-consistency",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=gZ9hCDWe6ke": {
    "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504",
    "semantic_title": "deformable detr: deformable transformers for end-to-end object detection",
    "citation_count": 5423,
    "authors": []
  },
  "https://openreview.net/forum?id=UuchYL8wSZo": {
    "title": "Learning Generalizable Visual Representations via Interactive Gameplay",
    "volume": "oral",
    "abstract": "",
    "checked": false,
    "id": "f79af91cb0f8d0968c2b8dbea48a4d142764e798",
    "semantic_title": "learning generalized transformation equivariant representations via autoencoding transformations",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=3AOj0RCNC2": {
    "title": "Gradient Projection Memory for Continual Learning",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "dd41d2d8fd2d3ea2c6ec74d07b456db942bfa8de",
    "semantic_title": "gradient projection memory for continual learning",
    "citation_count": 314,
    "authors": []
  },
  "https://openreview.net/forum?id=kmG8vRXTFv": {
    "title": "Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "757a3c0e2a43f3d15b91c52fae32b081a6a66e3a",
    "semantic_title": "augmenting physical models with deep networks for complex dynamics forecasting",
    "citation_count": 143,
    "authors": []
  },
  "https://openreview.net/forum?id=0-uUGPbIjD": {
    "title": "Human-Level Performance in No-Press Diplomacy via Equilibrium Search",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "7f79ac114d30c2c7dae91075210fbfda90c9d76f",
    "semantic_title": "human-level performance in no-press diplomacy via equilibrium search",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=UH-cmocLJC": {
    "title": "How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "6b989b8327db3a7212141c59c1569f0219775058",
    "semantic_title": "how neural networks extrapolate: from feedforward to graph neural networks",
    "citation_count": 320,
    "authors": []
  },
  "https://openreview.net/forum?id=Ua6zuk0WRH": {
    "title": "Rethinking Attention with Performers",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
    "semantic_title": "rethinking attention with performers",
    "citation_count": 1673,
    "authors": []
  },
  "https://openreview.net/forum?id=nIAxjsniDzg": {
    "title": "What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "f6264b11ec0dd9133f1d88a5288a3267a93182f8",
    "semantic_title": "what matters for on-policy deep actor-critic methods? a large-scale study",
    "citation_count": 188,
    "authors": []
  },
  "https://openreview.net/forum?id=uAX8q61EVRu": {
    "title": "Neural Synthesis of Binaural Speech From Mono Audio",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "3a1285aa98156839271347c2b8ead71975ba3ebc",
    "semantic_title": "neural synthesis of binaural speech from mono audio",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=mSAKhLYLSsl": {
    "title": "Dataset Condensation with Gradient Matching",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "5a94bcc168330318d3020aa4d41bd73cf68ab285",
    "semantic_title": "dataset condensation with gradient matching",
    "citation_count": 540,
    "authors": []
  },
  "https://openreview.net/forum?id=HajQFbx_yB": {
    "title": "Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes",
    "volume": "oral",
    "abstract": "Determinantal point processes (DPPs) have attracted significant attention in machine learning for their ability to model subsets drawn from a large item collection. Recent work shows that nonsymmetric DPP (NDPP) kernels have significant advantages over symmetric kernels in terms of modeling power and predictive performance. However, for an item collection of size $M$, existing NDPP learning and inference algorithms require memory quadratic in $M$ and runtime cubic (for learning) or quadratic (for inference) in $M$, making them impractical for many typical subset selection tasks. In this work, we develop a learning algorithm with space and time requirements linear in $M$ by introducing a new NDPP kernel decomposition. We also derive a linear-complexity NDPP maximum a posteriori (MAP) inference algorithm that applies not only to our new kernel but also to that of prior work. Through evaluation on real-world datasets, we show that our algorithms scale significantly better, and can match the predictive performance of prior work",
    "checked": true,
    "id": "1a180e019dc2d194feb39a053ddfa60a780c248e",
    "semantic_title": "scalable learning and map inference for nonsymmetric determinantal point processes",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=iAX0l6Cz8ub": {
    "title": "Geometry-aware Instance-reweighted Adversarial Training",
    "volume": "oral",
    "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training",
    "checked": true,
    "id": "99a599d8fe56529f47e78243ed61250190f96196",
    "semantic_title": "geometry-aware instance-reweighted adversarial training",
    "citation_count": 284,
    "authors": []
  },
  "https://openreview.net/forum?id=JWOiYxMG92s": {
    "title": "Free Lunch for Few-shot Learning: Distribution Calibration",
    "volume": "oral",
    "abstract": "Learning from a limited number of samples is challenging since the learned model can easily become overfitted based on the biased distribution formed by only a few training examples. In this paper, we calibrate the distribution of these few-sample classes by transferring statistics from the classes with sufficient examples. Then an adequate number of examples can be sampled from the calibrated distribution to expand the inputs to the classifier. We assume every dimension in the feature representation follows a Gaussian distribution so that the mean and the variance of the distribution can borrow from that of similar classes whose statistics are better estimated with an adequate number of samples. Our method can be built on top of off-the-shelf pretrained feature extractors and classification models without extra parameters. We show that a simple logistic regression classifier trained using the features sampled from our calibrated distribution can outperform the state-of-the-art accuracy on three datasets (~5% improvement on miniImageNet compared to the next best). The visualization of these generated features demonstrates that our calibrated distribution is an accurate estimation",
    "checked": true,
    "id": "48c6d0d7e3fabf4692bd03fc8b7263e55ee1d584",
    "semantic_title": "free lunch for few-shot learning: distribution calibration",
    "citation_count": 335,
    "authors": []
  },
  "https://openreview.net/forum?id=EbIDjBynYJ8": {
    "title": "Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding",
    "volume": "oral",
    "abstract": "Disentangling the underlying generative factors from complex data has so far been limited to carefully constructed scenarios. We propose a path towards natural data by first showing that the statistics of natural data provide enough structure to enable disentanglement, both theoretically and empirically. Specifically, we provide evidence that objects in natural movies undergo transitions that are typically small in magnitude with occasional large jumps, which is characteristic of a temporally sparse distribution. To address this finding we provide a novel proof that relies on a sparse prior on temporally adjacent observations to recover the true latent variables up to permutations and sign flips, directly providing a stronger result than previous work. We show that equipping practical estimation methods with our prior often surpasses the current state-of-the-art on several established benchmark datasets without any impractical assumptions, such as knowledge of the number of changing generative factors. Furthermore, we contribute two new benchmarks, Natural Sprites and KITTI Masks, which integrate the measured natural dynamics to enable disentanglement evaluation with more realistic datasets. We leverage these benchmarks to test our theory, demonstrating improved performance. We also identify non-obvious challenges for current methods in scaling to more natural domains. Taken together our work addresses key issues in disentanglement research for moving towards more natural settings",
    "checked": true,
    "id": "0cc9a9ae5dd2fcb8039ba80d6cd561114c0791fa",
    "semantic_title": "towards nonlinear disentanglement in natural data with temporal sparse coding",
    "citation_count": 136,
    "authors": []
  },
  "https://openreview.net/forum?id=FGqiDsBUKL0": {
    "title": "Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs",
    "volume": "oral",
    "abstract": "Natural images are projections of 3D objects on a 2D image plane. While state-of-the-art 2D generative models like GANs show unprecedented quality in modeling the natural image manifold, it is unclear whether they implicitly capture the underlying 3D object structures. And if so, how could we exploit such knowledge to recover the 3D shapes of objects in the images? To answer these questions, in this work, we present the first attempt to directly mine 3D geometric cues from an off-the-shelf 2D GAN that is trained on RGB images only. Through our investigation, we found that such a pre-trained GAN indeed contains rich 3D knowledge and thus can be used to recover 3D shape from a single 2D image in an unsupervised manner. The core of our framework is an iterative strategy that explores and exploits diverse viewpoint and lighting variations in the GAN image manifold. The framework does not require 2D keypoint or 3D annotations, or strong assumptions on object shapes (e.g. shapes are symmetric), yet it successfully recovers 3D shapes with high precision for human faces, cats, cars, and buildings. The recovered 3D shapes immediately allow high-quality image editing like relighting and object rotation. We quantitatively demonstrate the effectiveness of our approach compared to previous methods in both 3D shape reconstruction and face rotation. Our code is available at https://github.com/XingangPan/GAN2Shape",
    "checked": true,
    "id": "7d7d189796efa8fbd3f516b183954bc36f262f3f",
    "semantic_title": "do 2d gans know 3d shape? unsupervised 3d shape reconstruction from 2d image gans",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=q3KSThy2GwB": {
    "title": "Practical Real Time Recurrent Learning with a Sparse Approximation",
    "volume": "spotlight",
    "abstract": "Recurrent neural networks are usually trained with backpropagation through time, which requires storing a complete history of network states, and prohibits updating the weights \"online\" (after every timestep). Real Time Recurrent Learning (RTRL) eliminates the need for history storage and allows for online weight updates, but does so at the expense of computational costs that are quartic in the state size. This renders RTRL training intractable for all but the smallest networks, even ones that are made highly sparse. We introduce the Sparse n-step Approximation (SnAp) to the RTRL influence matrix. SnAp only tracks the influence of a parameter on hidden units that are reached by the computation graph within $n$ timesteps of the recurrent core. SnAp with $n=1$ is no more expensive than backpropagation but allows training on arbitrarily long sequences. We find that it substantially outperforms other RTRL approximations with comparable costs such as Unbiased Online Recurrent Optimization. For highly sparse networks, SnAp with $n=2$ remains tractable and can outperform backpropagation through time in terms of learning speed when updates are done online",
    "checked": true,
    "id": "efbcb368bdc1f27ec13584d65034321837169ae1",
    "semantic_title": "practical real time recurrent learning with a sparse approximation",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=zv-typ1gPxA": {
    "title": "Retrieval-Augmented Generation for Code Summarization via Hybrid GNN",
    "volume": "spotlight",
    "abstract": "Source code summarization aims to generate natural language summaries from structured code snippets for better understanding code functionalities. However, automatic code summarization is challenging due to the complexity of the source code and the language gap between the source code and natural language summaries. Most previous approaches either rely on retrieval-based (which can take advantage of similar examples seen from the retrieval database, but have low generalization performance) or generation-based methods (which have better generalization performance, but cannot take advantage of similar examples). This paper proposes a novel retrieval-augmented mechanism to combine the benefits of both worlds. Furthermore, to mitigate the limitation of Graph Neural Networks (GNNs) on capturing global graph structure information of source code, we propose a novel attention-based dynamic graph to complement the static graph representation of the source code, and design a hybrid message passing GNN for capturing both the local and global structural information. To evaluate the proposed approach, we release a new challenging benchmark, crawled from diversified large-scale open-source C projects (total 95k+ unique functions in the dataset). Our method achieves the state-of-the-art performance, improving existing methods by 1.42, 2.44 and 1.29 in terms of BLEU-4, ROUGE-L and METEOR",
    "checked": true,
    "id": "7c0b558bf433c5aaaf774cd5d3c767bfd3dbe123",
    "semantic_title": "retrieval-augmented generation for code summarization via hybrid gnn",
    "citation_count": 170,
    "authors": []
  },
  "https://openreview.net/forum?id=2m0g1wEafh": {
    "title": "Benefit of deep learning with non-convex noisy gradient descent: Provable excess risk bound and superiority to kernel methods",
    "volume": "spotlight",
    "abstract": "Establishing a theoretical analysis that explains why deep learning can outperform shallow learning such as kernel methods is one of the biggest issues in the deep learning literature. Towards answering this question, we evaluate excess risk of a deep learning estimator trained by a noisy gradient descent with ridge regularization on a mildly overparameterized neural network, and discuss its superiority to a class of linear estimators that includes neural tangent kernel approach, random feature model, other kernel methods, $k$-NN estimator and so on. We consider a teacher-student regression model, and eventually show that {\\it any} linear estimator can be outperformed by deep learning in a sense of the minimax optimal rate especially for a high dimension setting. The obtained excess bounds are so-called fast learning rate which is faster than $O(1/\\sqrt{n})$ that is obtained by usual Rademacher complexity analysis. This discrepancy is induced by the non-convex geometry of the model and the noisy gradient descent used for neural network training provably reaches a near global optimal solution even though the loss landscape is highly non-convex. Although the noisy gradient descent does not employ any explicit or implicit sparsity inducing regularization, it shows a preferable generalization performance that dominates linear estimators",
    "checked": true,
    "id": "77c530612412c36c3d2b1ee0a1cea6b37a7f518f",
    "semantic_title": "benefit of deep learning with non-convex noisy gradient descent: provable excess risk bound and superiority to kernel methods",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=xppLmXCbOw1": {
    "title": "Self-supervised Visual Reinforcement Learning with Object-centric Representations",
    "volume": "spotlight",
    "abstract": "Autonomous agents need large repertoires of skills to act reasonably on new tasks that they have not seen before. However, acquiring these skills using only a stream of high-dimensional, unstructured, and unlabeled observations is a tricky challenge for any autonomous agent. Previous methods have used variational autoencoders to encode a scene into a low-dimensional vector that can be used as a goal for an agent to discover new skills. Nevertheless, in compositional/multi-object environments it is difficult to disentangle all the factors of variation into such a fixed-length representation of the whole scene. We propose to use object-centric representations as a modular and structured observation space, which is learned with a compositional generative world model. We show that the structure in the representations in combination with goal-conditioned attention policies helps the autonomous agent to discover and learn useful skills. These skills can be further combined to address compositional tasks like the manipulation of several different objects",
    "checked": true,
    "id": "7c1b75ab7bed79163d3d830a6a83a4023b496ebb",
    "semantic_title": "self-supervised visual reinforcement learning with object-centric representations",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=PUkhWz65dy5": {
    "title": "Discovering a set of policies for the worst case reward",
    "volume": "spotlight",
    "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite",
    "checked": true,
    "id": "7530e6bbe8c624c88c47e6bea4d58869428974e3",
    "semantic_title": "discovering a set of policies for the worst case reward",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=8PS8m9oYtNy": {
    "title": "Implicit Normalizing Flows",
    "volume": "spotlight",
    "abstract": "Normalizing flows define a probability distribution by an explicit invertible transformation $\\boldsymbol{\\mathbf{z}}=f(\\boldsymbol{\\mathbf{x}})$. In this work, we present implicit normalizing flows (ImpFlows), which generalize normalizing flows by allowing the mapping to be implicitly defined by the roots of an equation $F(\\boldsymbol{\\mathbf{z}}, \\boldsymbol{\\mathbf{x}})= \\boldsymbol{\\mathbf{0}}$. ImpFlows build on residual flows (ResFlows) with a proper balance between expressiveness and tractability. Through theoretical analysis, we show that the function space of ImpFlow is strictly richer than that of ResFlows. Furthermore, for any ResFlow with a fixed number of blocks, there exists some function that ResFlow has a non-negligible approximation error. However, the function is exactly representable by a single-block ImpFlow. We propose a scalable algorithm to train and draw samples from ImpFlows. Empirically, we evaluate ImpFlow on several classification and density modeling tasks, and ImpFlow outperforms ResFlow with a comparable amount of parameters on all the benchmarks",
    "checked": true,
    "id": "68e710c9f040ffb52a2d102c110ad238521b325c",
    "semantic_title": "implicit normalizing flows",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=Jnspzp-oIZE": {
    "title": "Gauge Equivariant Mesh CNNs: Anisotropic convolutions on geometric graphs",
    "volume": "spotlight",
    "abstract": "A common approach to define convolutions on meshes is to interpret them as a graph and apply graph convolutional networks (GCNs). Such GCNs utilize isotropic kernels and are therefore insensitive to the relative orientation of vertices and thus to the geometry of the mesh as a whole. We propose Gauge Equivariant Mesh CNNs which generalize GCNs to apply anisotropic gauge equivariant kernels. Since the resulting features carry orientation information, we introduce a geometric message passing scheme defined by parallel transporting features over mesh edges. Our experiments validate the significantly improved expressivity of the proposed model over conventional GCNs and other methods",
    "checked": true,
    "id": "b626e35f2d30ac54a77e6d80b6fa8d01fa68971f",
    "semantic_title": "gauge equivariant mesh cnns: anisotropic convolutions on geometric graphs",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=EGdFhBzmAwB": {
    "title": "Generalization bounds via distillation",
    "volume": "spotlight",
    "abstract": "This paper theoretically investigates the following empirical phenomenon: given a high-complexity network with poor generalization bounds, one can distill it into a network with nearly identical predictions but low complexity and vastly smaller generalization bounds. The main contribution is an analysis showing that the original network inherits this good generalization bound from its distillation, assuming the use of well-behaved data augmentation. This bound is presented both in an abstract and in a concrete form, the latter complemented by a reduction technique to handle modern computation graphs featuring convolutional layers, fully-connected layers, and skip connections, to name a few. To round out the story, a (looser) classical uniform convergence analysis of compression is also presented, as well as a variety of experiments on cifar and mnist demonstrating similar generalization performance between the original network and its distillation",
    "checked": true,
    "id": "1488259fc61220397eef585a857cb5b34cb7f2db",
    "semantic_title": "generalization bounds via distillation",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=roNqYL0_XP": {
    "title": "Learning Mesh-Based Simulation with Graph Networks",
    "volume": "spotlight",
    "abstract": "Mesh-based simulations are central to modeling complex physical systems in many disciplines across science and engineering. Mesh representations support powerful numerical integration methods and their resolution can be adapted to strike favorable trade-offs between accuracy and efficiency. However, high-dimensional scientific simulations are very expensive to run, and solvers and parameters must often be tuned individually to each system studied. Here we introduce MeshGraphNets, a framework for learning mesh-based simulations using graph neural networks. Our model can be trained to pass messages on a mesh graph and to adapt the mesh discretization during forward simulation. Our results show it can accurately predict the dynamics of a wide range of physical systems, including aerodynamics, structural mechanics, and cloth. The model's adaptivity supports learning resolution-independent dynamics and can scale to more complex state spaces at test time. Our method is also highly efficient, running 1-2 orders of magnitude faster than the simulation on which it is trained. Our approach broadens the range of problems on which neural network simulators can operate and promises to improve the efficiency of complex, scientific modeling tasks",
    "checked": true,
    "id": "9e20f6874feaaf7c9994f9875b1d9cab17a2fd59",
    "semantic_title": "learning mesh-based simulation with graph networks",
    "citation_count": 855,
    "authors": []
  },
  "https://openreview.net/forum?id=JiYq3eqTKY": {
    "title": "On Statistical Bias In Active Learning: How and When to Fix It",
    "volume": "spotlight",
    "abstract": "Active learning is a powerful tool when labelling data is expensive, but it introduces a bias because the training data no longer follows the population distribution. We formalize this bias and investigate the situations in which it can be harmful and sometimes even helpful. We further introduce novel corrective weights to remove bias when doing so is beneficial. Through this, our work not only provides a useful mechanism that can improve the active learning approach, but also an explanation for the empirical successes of various existing approaches which ignore this bias. In particular, we show that this bias can be actively helpful when training overparameterized models---like neural networks---with relatively modest dataset sizes",
    "checked": true,
    "id": "1920ed0e7799410009d11cd7584550b9a57d5c93",
    "semantic_title": "on statistical bias in active learning: how and when to fix it",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=LmUJqB1Cz8": {
    "title": "Winning the L2RPN Challenge: Power Grid Management via Semi-Markov Afterstate Actor-Critic",
    "volume": "spotlight",
    "abstract": "Safe and reliable electricity transmission in power grids is crucial for modern society. It is thus quite natural that there has been a growing interest in the automatic management of power grids, exempliﬁed by the Learning to Run a Power Network Challenge (L2RPN), modeling the problem as a reinforcement learning (RL) task. However, it is highly challenging to manage a real-world scale power grid, mostly due to the massive scale of its state and action space. In this paper, we present an off-policy actor-critic approach that effectively tackles the unique challenges in power grid management by RL, adopting the hierarchical policy together with the afterstate representation. Our agent ranked ﬁrst in the latest challenge (L2RPN WCCI 2020), being able to avoid disastrous situations while maintaining the highest level of operational efﬁciency in every test scenarios. This paper provides a formal description of the algorithmic aspect of our approach, as well as further experimental studies on diverse power grids",
    "checked": true,
    "id": "690c1ae40fef28f58d5ef35916e9c2693677ef13",
    "semantic_title": "winning the l2rpn challenge: power grid management via semi-markov afterstate actor-critic",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=lxHgXYN4bwl": {
    "title": "Expressive Power of Invariant and Equivariant Graph Neural Networks",
    "volume": "spotlight",
    "abstract": "Various classes of Graph Neural Networks (GNN) have been proposed and shown to be successful in a wide range of applications with graph structured data. In this paper, we propose a theoretical framework able to compare the expressive power of these GNN architectures. The current universality theorems only apply to intractable classes of GNNs. Here, we prove the first approximation guarantees for practical GNNs, paving the way for a better understanding of their generalization. Our theoretical results are proved for invariant GNNs computing a graph embedding (permutation of the nodes of the input graph does not affect the output) and equivariant GNNs computing an embedding of the nodes (permutation of the input permutes the output). We show that Folklore Graph Neural Networks (FGNN), which are tensor based GNNs augmented with matrix multiplication are the most expressive architectures proposed so far for a given tensor order. We illustrate our results on the Quadratic Assignment Problem (a NP-Hard combinatorial problem) by showing that FGNNs are able to learn how to solve the problem, leading to much better average performances than existing algorithms (based on spectral, SDP or other GNNs architectures). On a practical side, we also implement masked tensors to handle batches of graphs of varying sizes",
    "checked": true,
    "id": "1784a18105dafb85e2c53c86a9736fc7b8b71316",
    "semantic_title": "expressive power of invariant and equivariant graph neural networks",
    "citation_count": 113,
    "authors": []
  },
  "https://openreview.net/forum?id=kHSu4ebxFXY": {
    "title": "MARS: Markov Molecular Sampling for Multi-objective Drug Discovery",
    "volume": "spotlight",
    "abstract": "Searching for novel molecules with desired chemical properties is crucial in drug discovery. Existing work focuses on developing neural models to generate either molecular sequences or chemical graphs. However, it remains a big challenge to find novel and diverse compounds satisfying several properties. In this paper, we propose MARS, a method for multi-objective drug molecule discovery. MARS is based on the idea of generating the chemical candidates by iteratively editing fragments of molecular graphs. To search for high-quality candidates, it employs Markov chain Monte Carlo sampling (MCMC) on molecules with an annealing scheme and an adaptive proposal. To further improve sample efficiency, MARS uses a graph neural network (GNN) to represent and select candidate edits, where the GNN is trained on-the-fly with samples from MCMC. Experiments show that MARS achieves state-of-the-art performance in various multi-objective settings where molecular bio-activity, drug-likeness, and synthesizability are considered. Remarkably, in the most challenging setting where all four objectives are simultaneously optimized, our approach outperforms previous methods significantly in comprehensive evaluations. The code is available at https://github.com/yutxie/mars",
    "checked": true,
    "id": "73bc1497aca1a3cd0aa9c48e4d35be5cf40d2a9c",
    "semantic_title": "mars: markov molecular sampling for multi-objective drug discovery",
    "citation_count": 151,
    "authors": []
  },
  "https://openreview.net/forum?id=NeRdBeTionN": {
    "title": "On Self-Supervised Image Representations for GAN Evaluation",
    "volume": "spotlight",
    "abstract": "The embeddings from CNNs pretrained on Imagenet classification are de-facto standard image representations for assessing GANs via FID, Precision and Recall measures. Despite broad previous criticism of their usage for non-Imagenet domains, these embeddings are still the top choice in most of the GAN literature. In this paper, we advocate the usage of the state-of-the-art self-supervised representations to evaluate GANs on the established non-Imagenet benchmarks. These representations, typically obtained via contrastive learning, are shown to provide better transfer to new tasks and domains, therefore, can serve as more universal embeddings of natural images. With extensive comparison of the recent GANs on the common datasets, we show that self-supervised representations produce a more reasonable ranking of models in terms of FID/Precision/Recall, while the ranking with classification-pretrained embeddings often can be misleading",
    "checked": true,
    "id": "6fcf0d22745b1edbb46a1e86b68efda0523d5edf",
    "semantic_title": "on self-supervised image representations for gan evaluation",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=_XYzwxPIQu6": {
    "title": "Identifying nonlinear dynamical systems with multiple time scales and long-range dependencies",
    "volume": "spotlight",
    "abstract": "A main theoretical interest in biology and physics is to identify the nonlinear dynamical system (DS) that generated observed time series. Recurrent Neural Networks (RNN) are, in principle, powerful enough to approximate any underlying DS, but in their vanilla form suffer from the exploding vs. vanishing gradients problem. Previous attempts to alleviate this problem resulted either in more complicated, mathematically less tractable RNN architectures, or strongly limited the dynamical expressiveness of the RNN. Here we address this issue by suggesting a simple regularization scheme for vanilla RNN with ReLU activation which enables them to solve long-range dependency problems and express slow time scales, while retaining a simple mathematical structure which makes their DS properties partly analytically accessible. We prove two theorems that establish a tight connection between the regularized RNN dynamics and their gradients, illustrate on DS benchmarks that our regularization approach strongly eases the reconstruction of DS which harbor widely differing time scales, and show that our method is also en par with other long-range architectures like LSTMs on several tasks",
    "checked": true,
    "id": "be8e2098a004aa109f65966ee266ffaf431f8289",
    "semantic_title": "identifying nonlinear dynamical systems with multiple time scales and long-range dependencies",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=_WnwtieRHxM": {
    "title": "Understanding the role of importance weighting for deep learning",
    "volume": "spotlight",
    "abstract": "The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research",
    "checked": true,
    "id": "689d3394c674b8cb2906fa8ffb1c80ecc76e69d0",
    "semantic_title": "understanding the role of importance weighting for deep learning",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=3UDSdyIcBDA": {
    "title": "RMSprop converges with proper hyper-parameter",
    "volume": "spotlight",
    "abstract": "Despite the existence of divergence examples, RMSprop remains one of the most popular algorithms in machine learning. Towards closing the gap between theory and practice, we prove that RMSprop converges with proper choice of hyper-parameters under certain conditions. More specifically, we prove that when the hyper-parameter $\\beta_2$ is close enough to $1$, RMSprop and its random shuffling version converge to a bounded region in general, and to critical points in the interpolation regime. It is worth mentioning that our results do not depend on ``bounded gradient\" assumption, which is often the key assumption utilized by existing theoretical work for Adam-type adaptive gradient method. Removing this assumption allows us to establish a phase transition from divergence to non-divergence for RMSprop. Finally, based on our theory, we conjecture that in practice there is a critical threshold $\\sf{\\beta_2^*}$, such that RMSprop generates reasonably good results only if $1>\\beta_2\\ge \\sf{\\beta_2^*}$. We provide empirical evidence for such a phase transition in our numerical experiments",
    "checked": true,
    "id": "c7f93c7a6c8646b6df94ff7f5eaf230f3f285e1f",
    "semantic_title": "rmsprop converges with proper hyper-parameter",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=XJk19XzGq2J": {
    "title": "The Intrinsic Dimension of Images and Its Impact on Learning",
    "volume": "spotlight",
    "abstract": "It is widely believed that natural image data exhibits low-dimensional structure despite the high dimensionality of conventional pixel representations. This idea underlies a common intuition for the remarkable success of deep learning in computer vision. In this work, we apply dimension estimation tools to popular datasets and investigate the role of low-dimensional structure in deep learning. We find that common natural image datasets indeed have very low intrinsic dimension relative to the high number of pixels in the images. Additionally, we find that low dimensional datasets are easier for neural networks to learn, and models solving these tasks generalize better from training to test data. Along the way, we develop a technique for validating our dimension estimation tools on synthetic data generated by GANs allowing us to actively manipulate the intrinsic dimension by controlling the image generation process. Code for our experiments may be found \\href{https://github.com/ppope/dimensions}{here}",
    "checked": true,
    "id": "2841a41690bfbdeb552b845a85c3b76d586084ae",
    "semantic_title": "the intrinsic dimension of images and its impact on learning",
    "citation_count": 283,
    "authors": []
  },
  "https://openreview.net/forum?id=ZPa2SyGcbwh": {
    "title": "Learning with Feature-Dependent Label Noise: A Progressive Approach",
    "volume": "spotlight",
    "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels",
    "checked": true,
    "id": "80f76b959f6bb243ebb67bdb395ca38b4373d49b",
    "semantic_title": "learning with feature-dependent label noise: a progressive approach",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=O7ms4LFdsX": {
    "title": "Disentangled Recurrent Wasserstein Autoencoder",
    "volume": "spotlight",
    "abstract": "Learning disentangled representations leads to interpretable models and facilitates data generation with style transfer, which has been extensively studied on static data such as images in an unsupervised learning framework. However, only a few works have explored unsupervised disentangled sequential representation learning due to challenges of generating sequential data. In this paper, we propose recurrent Wasserstein Autoencoder (R-WAE), a new framework for generative modeling of sequential data. R-WAE disentangles the representation of an input sequence into static and dynamic factors (i.e., time-invariant and time-varying parts). Our theoretical analysis shows that, R-WAE minimizes an upper bound of a penalized form of the Wasserstein distance between model distribution and sequential data distribution, and simultaneously maximizes the mutual information between input data and different disentangled latent factors, respectively. This is superior to (recurrent) VAE which does not explicitly enforce mutual information maximization between input data and disentangled latent representations. When the number of actions in sequential data is available as weak supervision information, R-WAE is extended to learn a categorical latent representation of actions to improve its disentanglement. Experiments on a variety of datasets show that our models outperform other baselines with the same settings in terms of disentanglement and unconditional video generation both quantitatively and qualitatively",
    "checked": true,
    "id": "072ad179f7ca9e5ee232e8fadd05d193bca79a8c",
    "semantic_title": "disentangled recurrent wasserstein autoencoder",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=opHLcXxYTC_": {
    "title": "Influence Estimation for Generative Adversarial Networks",
    "volume": "spotlight",
    "abstract": "Identifying harmful instances, whose absence in a training dataset improves model performance, is important for building better machine learning models. Although previous studies have succeeded in estimating harmful instances under supervised settings, they cannot be trivially extended to generative adversarial networks (GANs). This is because previous approaches require that (i) the absence of a training instance directly affects the loss value and that (ii) the change in the loss directly measures the harmfulness of the instance for the performance of a model. In GAN training, however, neither of the requirements is satisfied. This is because, (i) the generator's loss is not directly affected by the training instances as they are not part of the generator's training steps, and (ii) the values of GAN's losses normally do not capture the generative performance of a model. To this end, (i) we propose an influence estimation method that uses the Jacobian of the gradient of the generator's loss with respect to the discriminator's parameters (and vice versa) to trace how the absence of an instance in the discriminator's training affects the generator's parameters, and (ii) we propose a novel evaluation scheme, in which we assess harmfulness of each training instance on the basis of how GAN evaluation metric (e.g., inception score) is expected to change due to the removal of the instance. We experimentally verified that our influence estimation method correctly inferred the changes in GAN evaluation metrics. We also demonstrated that the removal of the identified harmful instances effectively improved the model's generative performance with respect to various GAN evaluation metrics",
    "checked": true,
    "id": "a1bc580f9eaed7ab5f0505e7aa2d99f794606faf",
    "semantic_title": "influence estimation for generative adversarial networks",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=Pbj8H_jEHYv": {
    "title": "Orthogonalizing Convolutional Layers with the Cayley Transform",
    "volume": "spotlight",
    "abstract": "Recent work has highlighted several advantages of enforcing orthogonality in the weight layers of deep networks, such as maintaining the stability of activations, preserving gradient norms, and enhancing adversarial robustness by enforcing low Lipschitz constants. Although numerous methods exist for enforcing the orthogonality of fully-connected layers, those for convolutional layers are more heuristic in nature, often focusing on penalty methods or limited classes of convolutions. In this work, we propose and evaluate an alternative approach to directly parameterize convolutional layers that are constrained to be orthogonal. Specifically, we propose to apply the Cayley transform to a skew-symmetric convolution in the Fourier domain, so that the inverse convolution needed by the Cayley transform can be computed efficiently. We compare our method to previous Lipschitz-constrained and orthogonal convolutional layers and show that it indeed preserves orthogonality to a high degree even for large convolutions. Applied to the problem of certified adversarial robustness, we show that networks incorporating the layer outperform existing deterministic methods for certified defense against $\\ell_2$-norm-bounded adversaries, while scaling to larger architectures than previously investigated. Code is available at https://github.com/locuslab/orthogonal-convolutions",
    "checked": true,
    "id": "9f0e0a59a4b3d689df8470b1218d2574244c26d6",
    "semantic_title": "orthogonalizing convolutional layers with the cayley transform",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=lVgB2FUbzuQ": {
    "title": "Predicting Infectiousness for Proactive Contact Tracing",
    "volume": "spotlight",
    "abstract": "The COVID-19 pandemic has spread rapidly worldwide, overwhelming manual contact tracing in many countries and resulting in widespread lockdowns for emergency containment. Large-scale digital contact tracing (DCT) has emerged as a potential solution to resume economic and social activity while minimizing spread of the virus. Various DCT methods have been proposed, each making trade-offs be-tween privacy, mobility restrictions, and public health. The most common approach, binary contact tracing (BCT), models infection as a binary event, informed only by an individual's test results, with corresponding binary recommendations that either all or none of the individual's contacts quarantine. BCT ignores the inherent uncertainty in contacts and the infection process, which could be used to tailor messaging to high-risk individuals, and prompt proactive testing or earlier warnings. It also does not make use of observations such as symptoms or pre-existing medical conditions, which could be used to make more accurate infectiousness predictions. In this paper, we use a recently-proposed COVID-19 epidemiological simulator to develop and test methods that can be deployed to a smartphone to locally and proactively predict an individual's infectiousness (risk of infecting others) based on their contact history and other information, while respecting strong privacy constraints. Predictions are used to provide personalized recommendations to the individual via an app, as well as to send anonymized messages to the individual's contacts, who use this information to better predict their own infectiousness, an approach we call proactive contact tracing (PCT). Similarly to other works, we find that compared to no tracing, all DCT methods tested are able to reduce spread of the disease and thus save lives, even at low adoption rates, strongly supporting a role for DCT methods in managing the pandemic. Further, we find a deep-learning based PCT method which improves over BCT for equivalent average mobility, suggesting PCT could help in safe re-opening and second-wave prevention",
    "checked": true,
    "id": "59078a88b2a94636bd2bfb6b4186f6ab8a7813bc",
    "semantic_title": "predicting infectiousness for proactive contact tracing",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=04LZCAxMSco": {
    "title": "Learning a Latent Simplex in Input Sparsity Time",
    "volume": "spotlight",
    "abstract": "We consider the problem of learning a latent $k$-vertex simplex $K\\in\\mathbb{R}^d$, given $\\mathbf{A}\\in\\mathbb{R}^{d\\times n}$, which can be viewed as $n$ data points that are formed by randomly perturbing some latent points in $K$, possibly beyond $K$. A large class of latent variable models, such as adversarial clustering, mixed membership stochastic block models, and topic models can be cast in this view of learning a latent simplex. Bhattacharyya and Kannan (SODA 2020) give an algorithm for learning such a $k$-vertex latent simplex in time roughly $O(k\\cdot\\text{nnz}(\\mathbf{A}))$, where $\\text{nnz}(\\mathbf{A})$ is the number of non-zeros in $\\mathbf{A}$. We show that the dependence on $k$ in the running time is unnecessary given a natural assumption about the mass of the top $k$ singular values of $\\mathbf{A}$, which holds in many of these applications. Further, we show this assumption is necessary, as otherwise an algorithm for learning a latent simplex would imply a better low rank approximation algorithm than what is known. We obtain a spectral low-rank approximation to $\\mathbf{A}$ in input-sparsity time and show that the column space thus obtained has small $\\sin\\Theta$ (angular) distance to the right top-$k$ singular space of $\\mathbf{A}$. Our algorithm then selects $k$ points in the low-rank subspace with the largest inner product (in absolute value) with $k$ carefully chosen random vectors. By working in the low-rank subspace, we avoid reading the entire matrix in each iteration and thus circumvent the $\\Theta(k\\cdot\\text{nnz}(\\mathbf{A}))$ running time",
    "checked": true,
    "id": "71d0156086aa83f7c4249c17d6df23fe9abae30d",
    "semantic_title": "learning a latent simplex in input-sparsity time",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=87ZwsaQNHPZ": {
    "title": "CPT: Efficient Deep Neural Network Training via Cyclic Precision",
    "volume": "spotlight",
    "abstract": "Low-precision deep neural network (DNN) training has gained tremendous attention as reducing precision is one of the most effective knobs for boosting DNNs' training time/energy efficiency. In this paper, we attempt to explore low-precision training from a new perspective as inspired by recent findings in understanding DNN training: we conjecture that DNNs' precision might have a similar effect as the learning rate during DNN training, and advocate dynamic precision along the training trajectory for further boosting the time/energy efficiency of DNN training. Specifically, we propose Cyclic Precision Training (CPT) to cyclically vary the precision between two boundary values which can be identified using a simple precision range test within the first few training epochs. Extensive simulations and ablation studies on five datasets and eleven models demonstrate that CPT's effectiveness is consistent across various models/tasks (including classification and language modeling). Furthermore, through experiments and visualization we show that CPT helps to (1) converge to a wider minima with a lower generalization error and (2) reduce training variance which we believe opens up a new design knob for simultaneously improving the optimization and efficiency of DNN training",
    "checked": true,
    "id": "badefa14a1b7bc192dfe899ce80e934b716106b6",
    "semantic_title": "cpt: efficient deep neural network training via cyclic precision",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=w_7JMpGZRh0": {
    "title": "Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "c562477737cc35e08d5a84aef01163ee4652d796",
    "semantic_title": "watch-and-help: a challenge for social perception and human-ai collaboration",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=F1vEjWK-lH_": {
    "title": "Gradient Vaccine: Investigating and Improving Multi-task Optimization in Massively Multilingual Models",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "78d9f4a6bc0f53d0594a752f0934a3689b4460d8",
    "semantic_title": "gradient vaccine: investigating and improving multi-task optimization in massively multilingual models",
    "citation_count": 204,
    "authors": []
  },
  "https://openreview.net/forum?id=xCcdBRQEDW": {
    "title": "PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable Physics",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "15c59c5cd0e393dda0b411995310510719d03a0f",
    "semantic_title": "plasticinelab: a soft-body manipulation benchmark with differentiable physics",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=QYjO70ACDK": {
    "title": "Distributional Sliced-Wasserstein and Applications to Generative Modeling",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a27b1f09cc31bfa8b313f5e31d529be6d75bbd34",
    "semantic_title": "distributional sliced-wasserstein and applications to generative modeling",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=b9PoimzZFJ": {
    "title": "Systematic generalisation with group invariant predictions",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "359c56a068e4a84a9f3b78f43b53fe3b333c0ba0",
    "semantic_title": "systematic generalisation with group invariant predictions",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=bnY0jm4l59": {
    "title": "Memory Optimization for Deep Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "b5697ed0c72b318d291545871fbd3a7a6e59e3cf",
    "semantic_title": "memory optimization for deep networks",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=_0kaDkv3dVf": {
    "title": "HW-NAS-Bench: Hardware-Aware Neural Architecture Search Benchmark",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a10daed04b387cdb6b9c71a623994bc083599c84",
    "semantic_title": "hw-nas-bench: hardware-aware neural architecture search benchmark",
    "citation_count": 113,
    "authors": []
  },
  "https://openreview.net/forum?id=HgLO8yalfwc": {
    "title": "Regularized Inverse Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "973fbe99e9b32f9da7e976e11141eec05ba577d3",
    "semantic_title": "regularized inverse reinforcement learning",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=30EvkP2aQLD": {
    "title": "What are the Statistical Limits of Offline RL with Linear Function Approximation?",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "bf53ddef241c58303b509faad4ec7514d3a6fc14",
    "semantic_title": "what are the statistical limits of offline rl with linear function approximation?",
    "citation_count": 165,
    "authors": []
  },
  "https://openreview.net/forum?id=zrT3HcsWSAt": {
    "title": "Behavioral Cloning from Noisy Demonstrations",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f27fee37a1388d083315713155dee1b46e78ad48",
    "semantic_title": "behavioral cloning from noisy demonstrations",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=8yKEo06dKNo": {
    "title": "How Does Mixup Help With Robustness and Generalization?",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "92ed2e34501903d922d74f28a012d6e337418fa4",
    "semantic_title": "how does mixup help with robustness and generalization?",
    "citation_count": 256,
    "authors": []
  },
  "https://openreview.net/forum?id=LSFCEb3GYU7": {
    "title": "Emergent Symbols through Binding in External Memory",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "c4791fe1dc9c17bb939c18ac6d9b3ab819661a96",
    "semantic_title": "emergent symbols through binding in external memory",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=JBAa9we1AL": {
    "title": "Individually Fair Gradient Boosting",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "44f20764dbed5a309f992f4e651c5980e606ebb4",
    "semantic_title": "individually fair gradient boosting",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=US-TP-xnXI": {
    "title": "Structured Prediction as Translation between Augmented Natural Languages",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1cb3f6d545b68db3e7fc6055dcf44099c3ac4672",
    "semantic_title": "structured prediction as translation between augmented natural languages",
    "citation_count": 297,
    "authors": []
  },
  "https://openreview.net/forum?id=O-XJwyoIF-k": {
    "title": "Minimum Width for Universal Approximation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f63405f53db3b1016f565f555fc8fa409f02fdbd",
    "semantic_title": "minimum width for universal approximation",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=LGgdb4TS4Z": {
    "title": "Topology-Aware Segmentation Using Discrete Morse Theory",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "7d3b1cdc8b9fe6347feebfec7175f231ce158330",
    "semantic_title": "topology-aware segmentation using discrete morse theory",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=WznmQa42ZAx": {
    "title": "Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "dcb5f0c180e138e3dae95d6e65237ef10063474c",
    "semantic_title": "interpreting graph neural networks for nlp with differentiable edge masking",
    "citation_count": 234,
    "authors": []
  },
  "https://openreview.net/forum?id=yr1mzrH3IC": {
    "title": "Regularization Matters in Policy Optimization - An Empirical Study on Continuous Control",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "975e33347fd841ded9ebcd77d928b243ea786a8c",
    "semantic_title": "regularization matters in policy optimization - an empirical study on continuous control",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=mLcmdlEUxy-": {
    "title": "Recurrent Independent Mechanisms",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "67a9dde04f367efc903b6d06097df9bdd9887ae7",
    "semantic_title": "recurrent independent mechanisms",
    "citation_count": 340,
    "authors": []
  },
  "https://openreview.net/forum?id=hvdKKV2yt7T": {
    "title": "Dataset Inference: Ownership Resolution in Machine Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "b3086fbbc678a7616ac390a41945f45e0d0ab001",
    "semantic_title": "dataset inference: ownership resolution in machine learning",
    "citation_count": 121,
    "authors": []
  },
  "https://openreview.net/forum?id=1YLJDvSx6J4": {
    "title": "Learning from Protein Structure with Geometric Vector Perceptrons",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "5e7047851d05b2ecef5de451dda5404acda726de",
    "semantic_title": "learning from protein structure with geometric vector perceptrons",
    "citation_count": 511,
    "authors": []
  },
  "https://openreview.net/forum?id=GJwMHetHc73": {
    "title": "Unsupervised Object Keypoint Learning using Local Spatial Predictability",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "c34a5507fb6123cf4130538c518e7562f1572ff2",
    "semantic_title": "unsupervised object keypoint learning using local spatial predictability",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=zWy1uxjDdZJ": {
    "title": "Fast Geometric Projections for Local Robustness Certification",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "df1ff25efac423a37527349044630614bf4f8396",
    "semantic_title": "fast geometric projections for local robustness certification",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=QtTKTdVrFBB": {
    "title": "Random Feature Attention",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "9ed25f101f19ea735ca300848948ed64064b97ca",
    "semantic_title": "random feature attention",
    "citation_count": 371,
    "authors": []
  },
  "https://openreview.net/forum?id=6Tm1mposlrM": {
    "title": "Sharpness-aware Minimization for Efficiently Improving Generalization",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a2cd073b57be744533152202989228cb4122270a",
    "semantic_title": "sharpness-aware minimization for efficiently improving generalization",
    "citation_count": 1441,
    "authors": []
  },
  "https://openreview.net/forum?id=3Aoft6NWFej": {
    "title": "PMI-Masking: Principled masking of correlated spans",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "518cb6d4247bdebf21e2811f296b0c7372602a0a",
    "semantic_title": "pmi-masking: principled masking of correlated spans",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=KUDUoRsEphu": {
    "title": "Learning Incompressible Fluid Dynamics from Scratch - Towards Fast, Differentiable Fluid Models that Generalize",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "e6443acbc6ea9568ecbca57d1763f84681f7af9a",
    "semantic_title": "learning incompressible fluid dynamics from scratch - towards fast, differentiable fluid models that generalize",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=rumv7QmLUue": {
    "title": "A Gradient Flow Framework For Analyzing Network Pruning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "77f6796b250eac27432d7c36449c57b7d689db49",
    "semantic_title": "a gradient flow framework for analyzing network pruning",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=ks5nebunVn_": {
    "title": "Towards Robustness Against Natural Language Word Substitutions",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f659031ceb7bbdcb7b0690742f35e2924fd1ed75",
    "semantic_title": "towards robustness against natural language word substitutions",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=R4aWTjmrEKM": {
    "title": "Iterative Empirical Game Solving via Single Policy Best Response",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "0debec877210ece112da2ede80681605c77d1887",
    "semantic_title": "iterative empirical game solving via single policy best response",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=o_V-MjyyGV_": {
    "title": "Self-Supervised Policy Adaptation during Deployment",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "798786f2d7f31b5361d700d3891a72e1096e5c8e",
    "semantic_title": "self-supervised policy adaptation during deployment",
    "citation_count": 169,
    "authors": []
  },
  "https://openreview.net/forum?id=YTWGvpFOQD-": {
    "title": "Differentially Private Learning Needs Better Features (or Much More Data)",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f864273db01ce9b728c3c16b08a5f7b22b917efb",
    "semantic_title": "differentially private learning needs better features (or much more data)",
    "citation_count": 279,
    "authors": []
  },
  "https://openreview.net/forum?id=uCQfPZwRaUu": {
    "title": "Data-Efficient Reinforcement Learning with Self-Predictive Representations",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "7c4356ec0dca6e6df0af7a882e2cd1571c8bf3dc",
    "semantic_title": "data-efficient reinforcement learning with self-predictive representations",
    "citation_count": 332,
    "authors": []
  },
  "https://openreview.net/forum?id=wS0UFjsNYjn": {
    "title": "Meta-GMVAE: Mixture of Gaussian VAE for Unsupervised Meta-Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "c073ec91babb4c0384f4914a8ecc7774aa902aaf",
    "semantic_title": "meta-gmvae: mixture of gaussian vae for unsupervised meta-learning",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=0N8jUH4JMv6": {
    "title": "Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time",
    "volume": "spotlight",
    "abstract": "We study training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers",
    "checked": true,
    "id": "1fde90108b8984c5d7c7f6d06fa4919a2a5bbd36",
    "semantic_title": "implicit convex regularizers of cnn architectures: convex optimization of two- and three-layer networks in polynomial time",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=GY6-6sTvGaf": {
    "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels",
    "volume": "spotlight",
    "abstract": "We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to transform input examples, as well as regularizing the value function and policy. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Hafner et al., 2019; Lee et al., 2019; Hafner et al., 2018) methods and recently proposed contrastive learning (Srinivas et al., 2020). Our approach, which we dub DrQ: Data-regularized Q, can be combined with any model-free reinforcement learning algorithm. We further demonstrate this by applying it to DQN and significantly improve its data-efficiency on the Atari 100k benchmark",
    "checked": true,
    "id": "6568423cfaca7e24c88ea208cb0e67129e43aa9b",
    "semantic_title": "image augmentation is all you need: regularizing deep reinforcement learning from pixels",
    "citation_count": 813,
    "authors": []
  },
  "https://openreview.net/forum?id=Vfs_2RnOD0H": {
    "title": "Dynamic Tensor Rematerialization",
    "volume": "spotlight",
    "abstract": "Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an $N$-layer linear feedforward network on an $\\Omega(\\sqrt{N})$ memory budget with only $\\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors",
    "checked": true,
    "id": "3c55dd7b8da5c7b47e91b2e749c264f50d007cd4",
    "semantic_title": "dynamic tensor rematerialization",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=VqzVhqxkjH1": {
    "title": "Deep Neural Network Fingerprinting by Conferrable Adversarial Examples",
    "volume": "spotlight",
    "abstract": "In Machine Learning as a Service, a provider trains a deep neural network and gives many users access. The hosted (source) model is susceptible to model stealing attacks, where an adversary derives a surrogate model from API access to the source model. For post hoc detection of such attacks, the provider needs a robust method to determine whether a suspect model is a surrogate of their model. We propose a fingerprinting method for deep neural network classifiers that extracts a set of inputs from the source model so that only surrogates agree with the source model on the classification of such inputs. These inputs are a subclass of transferable adversarial examples which we call conferrable adversarial examples that exclusively transfer with a target label from a source model to its surrogates. We propose a new method to generate these conferrable adversarial examples. We present an extensive study on the irremovability of our fingerprint against fine-tuning, weight pruning, retraining, retraining with different architectures, three model extraction attacks from related work, transfer learning, adversarial training, and two new adaptive attacks. Our fingerprint is robust against distillation, related model extraction attacks, and even transfer learning when the attacker has no access to the model provider's dataset. Our fingerprint is the first method that reaches a ROC AUC of 1.0 in verifying surrogates, compared to a ROC AUC of 0.63 by previous fingerprints",
    "checked": true,
    "id": "5effd428e3012061ba3d8b32f1952c2938c7ab7b",
    "semantic_title": "deep neural network fingerprinting by conferrable adversarial examples",
    "citation_count": 147,
    "authors": []
  },
  "https://openreview.net/forum?id=UcoXdfrORC": {
    "title": "Model-Based Visual Planning with Self-Supervised Functional Distances",
    "volume": "spotlight",
    "abstract": "A generalist robot must be able to complete a variety of tasks in its environment. One appealing way to specify each task is in terms of a goal observation. However, learning goal-reaching policies with reinforcement learning remains a challenging problem, particularly when hand-engineered reward functions are not available. Learned dynamics models are a promising approach for learning about the environment without rewards or task-directed data, but planning to reach goals with such a model requires a notion of functional similarity between observations and goal states. We present a self-supervised method for model-based visual goal reaching, which uses both a visual dynamics model as well as a dynamical distance function learned using model-free reinforcement learning. Our approach learns entirely using offline, unlabeled data, making it practical to scale to large and diverse datasets. In our experiments, we find that our method can successfully learn models that perform a variety of tasks at test-time, moving objects amid distractors with a simulated robotic arm and even learning to open and close a drawer using a real-world robot. In comparisons, we find that this approach substantially outperforms both model-free and model-based prior methods",
    "checked": true,
    "id": "9a689727d040a6bf122f16ea50884d5cd5258321",
    "semantic_title": "model-based visual planning with self-supervised functional distances",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=YmqAnY0CMEy": {
    "title": "Mathematical Reasoning via Self-supervised Skip-tree Training",
    "volume": "spotlight",
    "abstract": "We demonstrate that self-supervised language modeling applied to mathematical formulas enables logical reasoning. To measure the logical reasoning abilities of language models, we formulate several evaluation (downstream) tasks, such as inferring types, suggesting missing assumptions and completing equalities. For training language models for formal mathematics, we propose a novel skip-tree task. We find that models trained on the skip-tree task show surprisingly strong mathematical reasoning abilities, and outperform models trained on standard skip-sequence tasks. We also analyze the models' ability to formulate new conjectures by measuring how often the predictions are provable and useful in other proofs",
    "checked": true,
    "id": "45bb43cdc35324fea4350ed335c500d4a5fd6ef5",
    "semantic_title": "mathematical reasoning via self-supervised skip-tree training",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=eMP1j9efXtX": {
    "title": "DeepAveragers: Offline Reinforcement Learning By Solving Derived Non-Parametric MDPs",
    "volume": "spotlight",
    "abstract": "We study an approach to offline reinforcement learning (RL) based on optimally solving finitely-represented MDPs derived from a static dataset of experience. This approach can be applied on top of any learned representation and has the potential to easily support multiple solution objectives as well as zero-shot adjustment to changing environments and goals. Our main contribution is to introduce the Deep Averagers with Costs MDP (DAC-MDP) and to investigate its solutions for offline RL. DAC-MDPs are a non-parametric model that can leverage deep representations and account for limited data by introducing costs for exploiting under-represented parts of the model. In theory, we show conditions that allow for lower-bounding the performance of DAC-MDP solutions. We also investigate the empirical behavior in a number of environments, including those with image-based observations. Overall, the experiments demonstrate that the framework can work in practice and scale to large complex offline RL problems",
    "checked": true,
    "id": "14e6502e4a63635ef01790471f45f369de72a1c0",
    "semantic_title": "deepaveragers: offline reinforcement learning by solving derived non-parametric mdps",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=p-NZIuwqhI4": {
    "title": "On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers",
    "volume": "spotlight",
    "abstract": "A deep equilibrium model uses implicit layers, which are implicitly defined through an equilibrium point of an infinite sequence of computation. It avoids any explicit computation of the infinite sequence by finding an equilibrium point directly via root-finding and by computing gradients via implicit differentiation. In this paper, we analyze the gradient dynamics of deep equilibrium models with nonlinearity only on weight matrices and non-convex objective functions of weights for regression and classification. Despite non-convexity, convergence to global optimum at a linear rate is guaranteed without any assumption on the width of the models, allowing the width to be smaller than the output dimension and the number of data points. Moreover, we prove a relation between the gradient dynamics of the deep implicit layer and the dynamics of trust region Newton method of a shallow explicit layer. This mathematically proven relation along with our numerical observation suggests the importance of understanding implicit bias of implicit layers and an open problem on the topic. Our proofs deal with implicit layers, weight tying and nonlinearity on weights, and differ from those in the related literature",
    "checked": true,
    "id": "6ad0c5c675d562c6855927b6b3279de4e68dc0ea",
    "semantic_title": "on the theory of implicit deep learning: global convergence with implicit layers",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=yHeg4PbFHh": {
    "title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration",
    "volume": "spotlight",
    "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation",
    "checked": true,
    "id": "30a156f17ca8f54aa14d01d32c2315c11fcbe723",
    "semantic_title": "bustle: bottom-up program-synthesis through learning-guided exploration",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=qYda4oLEc1": {
    "title": "The Traveling Observer Model: Multi-task Learning Through Spatial Variable Embeddings",
    "volume": "spotlight",
    "abstract": "This paper frames a general prediction system as an observer traveling around a continuous space, measuring values at some locations, and predicting them at others. The observer is completely agnostic about any particular task being solved; it cares only about measurement locations and their values. This perspective leads to a machine learning framework in which seemingly unrelated tasks can be solved by a single model, by embedding their input and output variables into a shared space. An implementation of the framework is developed in which these variable embeddings are learned jointly with internal model parameters. In experiments, the approach is shown to (1) recover intuitive locations of variables in space and time, (2) exploit regularities across related datasets with completely disjoint input and output spaces, and (3) exploit regularities across seemingly unrelated tasks, outperforming task-specific single-task models and multi-task learning alternatives. The results suggest that even seemingly unrelated tasks may originate from similar underlying processes, a fact that the traveling observer model can use to make better predictions",
    "checked": true,
    "id": "090e15ad813a07b00215dcb74ac57a947716188c",
    "semantic_title": "the traveling observer model: multi-task learning through spatial variable embeddings",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=NECTfffOvn1": {
    "title": "Fidelity-based Deep Adiabatic Scheduling",
    "volume": "spotlight",
    "abstract": "Adiabatic quantum computation is a form of computation that acts by slowly interpolating a quantum system between an easy to prepare initial state and a final state that represents a solution to a given computational problem. The choice of the interpolation schedule is critical to the performance: if at a certain time point, the evolution is too rapid, the system has a high probability to transfer to a higher energy state, which does not represent a solution to the problem. On the other hand, an evolution that is too slow leads to a loss of computation time and increases the probability of failure due to decoherence. In this work, we train deep neural models to produce optimal schedules that are conditioned on the problem at hand. We consider two types of problem representation: the Hamiltonian form, and the Quadratic Unconstrained Binary Optimization (QUBO) form. A novel loss function that scores schedules according to their approximated success probability is introduced. We benchmark our approach on random QUBO problems, Grover search, 3-SAT, and MAX-CUT problems and show that our approach outperforms, by a sizable margin, the linear schedules as well as alternative approaches that were very recently proposed",
    "checked": true,
    "id": "8a6df3c492e0ba13e64ea0b8ac97a961bf863c32",
    "semantic_title": "fidelity-based deep adiabatic scheduling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cri3xz59ga": {
    "title": "Deciphering and Optimizing Multi-Task Learning: a Random Matrix Approach",
    "volume": "spotlight",
    "abstract": "This article provides theoretical insights into the inner workings of multi-task and transfer learning methods, by studying the tractable least-square support vector machine multi-task learning (LS-SVM MTL) method, in the limit of large ($p$) and numerous ($n$) data. By a random matrix analysis applied to a Gaussian mixture data model, the performance of MTL LS-SVM is shown to converge, as $n,p\\to\\infty$, to a deterministic limit involving simple (small-dimensional) statistics of the data. We prove (i) that the standard MTL LS-SVM algorithm is in general strongly biased and may dramatically fail (to the point that individual single-task LS-SVMs may outperform the MTL approach, even for quite resembling tasks): our analysis provides a simple method to correct these biases, and that we reveal (ii) the sufficient statistics at play in the method, which can be efficiently estimated, even for quite small datasets. The latter result is exploited to automatically optimize the hyperparameters without resorting to any cross-validation procedure. Experiments on popular datasets demonstrate that our improved MTL LS-SVM method is computationally-efficient and outperforms sometimes much more elaborate state-of-the-art multi-task and transfer learning techniques",
    "checked": true,
    "id": "4283e256359f0364eed4c5c866bdab0f864fbbc5",
    "semantic_title": "deciphering and optimizing multi-task learning: a random matrix approach",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=tilovEHA3YS": {
    "title": "Learning-based Support Estimation in Sublinear Time",
    "volume": "spotlight",
    "abstract": "We consider the problem of estimating the number of distinct elements in a large data set (or, equivalently, the support size of the distribution induced by the data set) from a random sample of its elements. The problem occurs in many applications, including biology, genomics, computer systems and linguistics. A line of research spanning the last decade resulted in algorithms that estimate the support up to $ \\pm \\varepsilon n$ from a sample of size $O(\\log^2(1/\\varepsilon) \\cdot n/\\log n)$, where $n$ is the data set size. Unfortunately, this bound is known to be tight, limiting further improvements to the complexity of this problem. In this paper we consider estimation algorithms augmented with a machine-learning-based predictor that, given any element, returns an estimation of its frequency. We show that if the predictor is correct up to a constant approximation factor, then the sample complexity can be reduced significantly, to $$ \\ \\log (1/\\varepsilon) \\cdot n^{1-\\Theta(1/\\log(1/\\varepsilon))}. $$ We evaluate the proposed algorithms on a collection of data sets, using the neural-network based estimators from {Hsu et al, ICLR'19} as predictors. Our experiments demonstrate substantial (up to 3x) improvements in the estimation accuracy compared to the state of the art algorithm",
    "checked": true,
    "id": "bd5c32f003c670366e5d62c59fe02e747a470647",
    "semantic_title": "learning-based support estimation in sublinear time",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=iAmZUo0DxC0": {
    "title": "Unlearnable Examples: Making Personal Data Unexploitable",
    "volume": "spotlight",
    "abstract": "The volume of \"free\" data on the internet has been key to the current success of deep learning. However, it also raises privacy concerns about the unauthorized exploitation of personal data for training commercial models. It is thus crucial to develop methods to prevent unauthorized data exploitation. This paper raises the question: can data be made unlearnable for deep learning models? We present a type of error-minimizing noise that can indeed make training examples unlearnable. Error-minimizing noise is intentionally generated to reduce the error of one or more of the training example(s) close to zero, which can trick the model into believing there is \"nothing\" to learn from these example(s). The noise is restricted to be imperceptible to human eyes, and thus does not affect normal data utility. We empirically verify the effectiveness of error-minimizing noise in both sample-wise and class-wise forms. We also demonstrate its flexibility under extensive experimental settings and practicability in a case study of face recognition. Our work establishes an important ﬁrst step towards making personal data unexploitable to deep learning models",
    "checked": true,
    "id": "9a1090c590474190df976bf5a91c3e5cc1a30864",
    "semantic_title": "unlearnable examples: making personal data unexploitable",
    "citation_count": 209,
    "authors": []
  },
  "https://openreview.net/forum?id=g-wu9TMPODo": {
    "title": "How Benign is Benign Overfitting ?",
    "volume": "spotlight",
    "abstract": "We investigate two causes for adversarial vulnerability in deep neural networks: bad data and (poorly) trained models. When trained with SGD, deep neural networks essentially achieve zero training error, even in the presence of label noise, while also exhibiting good generalization on natural test data, something referred to as benign overfitting (Bartlett et al., 2020; Chatterji & Long, 2020). However, these models are vulnerable to adversarial attacks. We identify label noise as one of the causes for adversarial vulnerability, and provide theoretical and empirical evidence in support of this. Surprisingly, we find several instances of label noise in datasets such as MNIST and CIFAR, and that robustly trained models incur training error on some of these, i.e. they don't fit the noise. However, removing noisy labels alone does not suffice to achieve adversarial robustness. We conjecture that in part sub-optimal representation learning is also responsible for adversarial vulnerability. By means of simple theoretical setups, we show how the choice of representation can drastically affect adversarial robustness",
    "checked": true,
    "id": "dfb6250ae1c8f4d0ec3e28ed84596f77704485ab",
    "semantic_title": "how benign is benign overfitting?",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=5k8F6UU39V": {
    "title": "Autoregressive Entity Retrieval",
    "volume": "spotlight",
    "abstract": "Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. This enables us to mitigate the aforementioned technical issues since: (i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at https://github.com/facebookresearch/GENRE",
    "checked": true,
    "id": "572c12e81319ccd47cc0c637c82efadd03fd05ab",
    "semantic_title": "autoregressive entity retrieval",
    "citation_count": 466,
    "authors": []
  },
  "https://openreview.net/forum?id=SRDuJssQud": {
    "title": "Neural Approximate Sufficient Statistics for Implicit Models",
    "volume": "spotlight",
    "abstract": "We consider the fundamental problem of how to automatically construct summary statistics for implicit generative models where the evaluation of the likelihood function is intractable but sampling data from the model is possible. The idea is to frame the task of constructing sufficient statistics as learning mutual information maximizing representations of the data with the help of deep neural networks. The infomax learning procedure does not need to estimate any density or density ratio. We apply our approach to both traditional approximate Bayesian computation and recent neural likelihood methods, boosting their performance on a range of tasks",
    "checked": true,
    "id": "9dfe5d4fe11d5a708e9988e9a1464fd8afe3b797",
    "semantic_title": "neural approximate sufficient statistics for implicit models",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=sSjqmfsk95O": {
    "title": "Large Scale Image Completion via Co-Modulated Generative Adversarial Networks",
    "volume": "spotlight",
    "abstract": "Numerous task-specific variants of conditional generative adversarial networks have been developed for image completion. Yet, a serious limitation remains that all existing algorithms tend to fail when handling large-scale missing regions. To overcome this challenge, we propose a generic new approach that bridges the gap between image-conditional and recent modulated unconditional generative architectures via co-modulation of both conditional and stochastic style representations. Also, due to the lack of good quantitative metrics for image completion, we propose the new Paired/Unpaired Inception Discriminative Score (P-IDS/U-IDS), which robustly measures the perceptual fidelity of inpainted images compared to real images via linear separability in a feature space. Experiments demonstrate superior performance in terms of both quality and diversity over state-of-the-art methods in free-form image completion and easy generalization to image-to-image translation. Code is available at https://github.com/zsyzzsoft/co-mod-gan",
    "checked": true,
    "id": "39c2ae603902ea664adf9e74914117f79df2e612",
    "semantic_title": "large scale image completion via co-modulated generative adversarial networks",
    "citation_count": 302,
    "authors": []
  },
  "https://openreview.net/forum?id=6s7ME_X5_Un": {
    "title": "DDPNOpt: Differential Dynamic Programming Neural Optimizer",
    "volume": "spotlight",
    "abstract": "Interpretation of Deep Neural Networks (DNNs) training as an optimal control problem with nonlinear dynamical systems has received considerable attention recently, yet the algorithmic development remains relatively limited. In this work, we make an attempt along this line by reformulating the training procedure from the trajectory optimization perspective. We first show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order method rooted in the Approximate Dynamic Programming. In this vein, we propose a new class of optimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and convolution networks. DDPNOpt features layer-wise feedback policies which improve convergence and reduce sensitivity to hyper-parameter over existing methods. It outperforms other optimal-control inspired training methods in both convergence and complexity, and is competitive against state-of-the-art first and second order methods. We also observe DDPNOpt has surprising benefit in preventing gradient vanishing. Our work opens up new avenues for principled algorithmic design built upon the optimal control theory",
    "checked": true,
    "id": "925e6e1e9dec14569c3fe06d238f961443b3b48e",
    "semantic_title": "differential dynamic programming neural optimizer",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=MuSYkd1hxRP": {
    "title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search",
    "volume": "spotlight",
    "abstract": "Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces",
    "checked": true,
    "id": "301c48248ba3a4b6d726aac8ec4c3f335e3712e9",
    "semantic_title": "geometry-aware gradient algorithms for neural architecture search",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=rcQdycl0zyk": {
    "title": "Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with 1 / n Parameters",
    "volume": "spotlight",
    "abstract": "Recent works have demonstrated reasonable success of representation learning in hypercomplex space. Specifically, \"fully-connected layers with quaternions\" (quaternions are 4D hypercomplex numbers), which replace real-valued matrix multiplications in fully-connected layers with Hamilton products of quaternions, both enjoy parameter savings with only 1/4 learnable parameters and achieve comparable performance in various applications. However, one key caveat is that hypercomplex space only exists at very few predefined dimensions (4D, 8D, and 16D). This restricts the flexibility of models that leverage hypercomplex multiplications. To this end, we propose parameterizing hypercomplex multiplications, allowing models to learn multiplication rules from data regardless of whether such rules are predefined. As a result, our method not only subsumes the Hamilton product, but also learns to operate on any arbitrary $n$D hypercomplex space, providing more architectural flexibility using arbitrarily $1/n$ learnable parameters compared with the fully-connected layer counterpart. Experiments of applications to the LSTM and transformer models on natural language inference, machine translation, text style transfer, and subject verb agreement demonstrate architectural flexibility and effectiveness of the proposed approach",
    "checked": true,
    "id": "1d5f5df837139d4ae8af23e3634295594c5b85db",
    "semantic_title": "beyond fully-connected layers with quaternions: parameterization of hypercomplex multiplications with 1/n parameters",
    "citation_count": 86,
    "authors": []
  },
  "https://openreview.net/forum?id=uXl3bZLkr3c": {
    "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
    "volume": "spotlight",
    "abstract": "A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent): we optimize the model for confidence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training",
    "checked": true,
    "id": "180c78b132f6369a384d22a9529551d86c8788d3",
    "semantic_title": "tent: fully test-time adaptation by entropy minimization",
    "citation_count": 1218,
    "authors": []
  },
  "https://openreview.net/forum?id=Oos98K9Lv-k": {
    "title": "Neural Topic Model via Optimal Transport",
    "volume": "spotlight",
    "abstract": "Recently, Neural Topic Models (NTMs) inspired by variational autoencoders have obtained increasingly research interest due to their promising results on text analysis. However, it is usually hard for existing NTMs to achieve good document representation and coherent/diverse topics at the same time. Moreover, they often degrade their performance severely on short documents. The requirement of reparameterisation could also comprise their training quality and model flexibility. To address these shortcomings, we present a new neural topic model via the theory of optimal transport (OT). Specifically, we propose to learn the topic distribution of a document by directly minimising its OT distance to the document's word distributions. Importantly, the cost matrix of the OT distance models the weights between topics and words, which is constructed by the distances between topics and words in an embedding space. Our proposed model can be trained efficiently with a differentiable loss. Extensive experiments show that our framework significantly outperforms the state-of-the-art NTMs on discovering more coherent and diverse topics and deriving better document representations for both regular and short texts",
    "checked": true,
    "id": "a40b66c7650ea38e27ed5885585a10e696bbfcc9",
    "semantic_title": "neural topic model via optimal transport",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=9xC2tWEwBD": {
    "title": "A Panda? No, It's a Sloth: Slowdown Attacks on Adaptive Multi-Exit Neural Network Inference",
    "volume": "spotlight",
    "abstract": "Recent increases in the computational demands of deep neural networks (DNNs), combined with the observation that most input samples require only simple models, have sparked interest in input-adaptive multi-exit architectures, such as MSDNets or Shallow-Deep Networks. These architectures enable faster inferences and could bring DNNs to low-power devices, e.g., in the Internet of Things (IoT). However, it is unknown if the computational savings provided by this approach are robust against adversarial pressure. In particular, an adversary may aim to slowdown adaptive DNNs by increasing their average inference time—a threat analogous to the denial-of-service attacks from the Internet. In this paper, we conduct a systematic evaluation of this threat by experimenting with three generic multi-exit DNNs (based on VGG16, MobileNet, and ResNet56) and a custom multi-exit architecture, on two popular image classification benchmarks (CIFAR-10 and Tiny ImageNet). To this end, we show that adversarial example-crafting techniques can be modified to cause slowdown, and we propose a metric for comparing their impact on different architectures. We show that a slowdown attack reduces the efficacy of multi-exit DNNs by 90–100%, and it amplifies the latency by 1.5–5× in a typical IoT deployment. We also show that it is possible to craft universal, reusable perturbations and that the attack can be effective in realistic black-box scenarios, where the attacker has limited knowledge about the victim. Finally, we show that adversarial training provides limited protection against slowdowns. These results suggest that further research is needed for defending multi-exit architectures against this emerging threat. Our code is available at https://github.com/sanghyun-hong/deepsloth",
    "checked": true,
    "id": "14392f4d79d97a53670f9755eef7b275fc8224c0",
    "semantic_title": "a panda? no, it's a sloth: slowdown attacks on adaptive multi-exit neural network inference",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=Ut1vF_q_vC": {
    "title": "Are Neural Rankers still Outperformed by Gradient Boosted Decision Trees?",
    "volume": "spotlight",
    "abstract": "Despite the success of neural models on many major machine learning problems, their effectiveness on traditional Learning-to-Rank (LTR) problems is still not widely acknowledged. We first validate this concern by showing that most recent neural LTR models are, by a large margin, inferior to the best publicly available Gradient Boosted Decision Trees (GBDT) in terms of their reported ranking accuracy on benchmark datasets. This unfortunately was somehow overlooked in recent neural LTR papers. We then investigate why existing neural LTR models under-perform and identify several of their weaknesses. Furthermore, we propose a unified framework comprising of counter strategies to ameliorate the existing weaknesses of neural models. Our models are the first to be able to perform equally well, comparing with the best tree-based baseline, while outperforming recently published neural LTR models by a large margin. Our results can also serve as a benchmark to facilitate future improvement of neural LTR models",
    "checked": true,
    "id": "ed91feefa0b3de7155d510fbd527527068a71c8e",
    "semantic_title": "are neural rankers still outperformed by gradient boosted decision trees?",
    "citation_count": 120,
    "authors": []
  },
  "https://openreview.net/forum?id=qda7-sVg84": {
    "title": "Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Reinforcement learning methods trained on few environments rarely learn policies that generalize to unseen environments. To improve generalization, we incorporate the inherent sequential structure in reinforcement learning into the representation learning process. This approach is orthogonal to recent approaches, which rarely exploit this structure explicitly. Specifically, we introduce a theoretically motivated policy similarity metric (PSM) for measuring behavioral similarity between states. PSM assigns high similarity to states for which the optimal policies in those states as well as in future states are similar. We also present a contrastive representation learning procedure to embed any state similarity metric, which we instantiate with PSM to obtain policy similarity embeddings (PSEs). We demonstrate that PSEs improve generalization on diverse benchmarks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite",
    "checked": true,
    "id": "7428f65393c19a6ca6381693767cb4f643a49a5c",
    "semantic_title": "contrastive behavioral similarity embeddings for generalization in reinforcement learning",
    "citation_count": 173,
    "authors": []
  },
  "https://openreview.net/forum?id=RLRXCV6DbEJ": {
    "title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images",
    "volume": "spotlight",
    "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae",
    "checked": true,
    "id": "3e577c9bdc82cb7fed337a74f90bbc4505fdfb69",
    "semantic_title": "very deep vaes generalize autoregressive models and can outperform them on images",
    "citation_count": 361,
    "authors": []
  },
  "https://openreview.net/forum?id=9EsrXMzlFQY": {
    "title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors",
    "volume": "spotlight",
    "abstract": "Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new{asynchronous RED (Async-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of Async-RED is further reduced by using a random subset of measurements at every iteration. We present a complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate Async-RED on image recovery using pre-trained deep denoisers as priors",
    "checked": true,
    "id": "13df91191a54e860d24a86fac76920e6a22505c3",
    "semantic_title": "async-red: a provably convergent asynchronous block parallel stochastic method using deep denoising priors",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=6puCSjH3hwA": {
    "title": "A Good Image Generator Is What You Need for High-Resolution Video Synthesis",
    "volume": "spotlight",
    "abstract": "Image and video synthesis are closely related areas aiming at generating content from noise. While rapid progress has been demonstrated in improving image-based models to handle large resolutions, high-quality renderings, and wide variations in image content, achieving comparable video generation results remains problematic. We present a framework that leverages contemporary image generators to render high-resolution videos. We frame the video synthesis problem as discovering a trajectory in the latent space of a pre-trained and fixed image generator. Not only does such a framework render high-resolution videos, but it also is an order of magnitude more computationally efficient. We introduce a motion generator that discovers the desired trajectory, in which content and motion are disentangled. With such a representation, our framework allows for a broad range of applications, including content and motion manipulation. Furthermore, we introduce a new task, which we call cross-domain video synthesis, in which the image and motion generators are trained on disjoint datasets belonging to different domains. This allows for generating moving objects for which the desired video data is not available. Extensive experiments on various datasets demonstrate the advantages of our methods over existing video generation techniques. Code will be released at https://github.com/snap-research/MoCoGAN-HD",
    "checked": true,
    "id": "3618e503068e5f0e4f17ad1557a9bd6692daea79",
    "semantic_title": "a good image generator is what you need for high-resolution video synthesis",
    "citation_count": 191,
    "authors": []
  },
  "https://openreview.net/forum?id=0zvfm-nZqQs": {
    "title": "Undistillable: Making A Nasty Teacher That CANNOT teach students",
    "volume": "spotlight",
    "abstract": "Knowledge Distillation (KD) is a widely used technique to transfer knowledge from pre-trained teacher models to (usually more lightweight) student models. However, in certain situations, this technique is more of a curse than a blessing. For instance, KD poses a potential risk of exposing intellectual properties (IPs): even if a trained machine learning model is released in ``black boxes'' (e.g., as executable software or APIs without open-sourcing code), it can still be replicated by KD through imitating input-output behaviors. To prevent this unwanted effect of KD, this paper introduces and investigates a concept called $\\textit{Nasty Teacher}$: a specially trained teacher network that yields nearly the same performance as a normal one, but would significantly degrade the performance of student models learned by imitating it. We propose a simple yet effective algorithm to build the nasty teacher, called $\\textit{self-undermining knowledge distillation}$. Specifically, we aim to maximize the difference between the output of the nasty teacher and a normal pre-trained network. Extensive experiments on several datasets demonstrate that our method is effective on both standard KD and data-free KD, providing the desirable KD-immunity to model owners for the first time. We hope our preliminary study can draw more awareness and interest in this new practical problem of both social and legal importance. Our codes and pre-trained models can be found at: $\\url{https://github.com/VITA-Group/Nasty-Teacher}$",
    "checked": true,
    "id": "55da1c1b1c5fd16c91a26dce223f94c2592122e9",
    "semantic_title": "undistillable: making a nasty teacher that cannot teach students",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=EqoXe2zmhrh": {
    "title": "Support-set bottlenecks for video-text representation learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "78bc767ebd02c0cc690fdb334c37bf64cfaf0115",
    "semantic_title": "support-set bottlenecks for video-text representation learning",
    "citation_count": 253,
    "authors": []
  },
  "https://openreview.net/forum?id=wpSWuz_hyqA": {
    "title": "Grounded Language Learning Fast and Slow",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1c39625ed65389cfb9d268f93f406455665f201b",
    "semantic_title": "grounded language learning fast and slow",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=zDy_nQCXiIj": {
    "title": "GAN \"Steerability\" without optimization",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "bfbb573504d446d32a85c9182db4a9d2edbbcdd6",
    "semantic_title": "gan steerability without optimization",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=80FMcTSZ6J0": {
    "title": "Noise against noise: stochastic label noise helps combat inherent label noise",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "29891cba979efd7a5a332f2210a3a9aa5192a805",
    "semantic_title": "noise against noise: stochastic label noise helps combat inherent label noise",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=5m3SEczOV8L": {
    "title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "0d79c6737849bea78d5bd96c0894d9ec61190089",
    "semantic_title": "vaebm: a symbiosis between variational autoencoders and energy-based models",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=HHSEKOnPvaO": {
    "title": "Graph-Based Continual Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "45a8b9a22a9794f2a586a4ffc006058625b7327e",
    "semantic_title": "graph-based continual learning",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=pBqLS-7KYAF": {
    "title": "Sparse Quantized Spectral Clustering",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "6ce863bb510b204edb49aa90bf764da73810768a",
    "semantic_title": "sparse quantized spectral clustering",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=xTJEN-ggl1b": {
    "title": "LambdaNetworks: Modeling long-range Interactions without Attention",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "cec7872b194aadf54140578b9be52939eb1112e9",
    "semantic_title": "lambdanetworks: modeling long-range interactions without attention",
    "citation_count": 181,
    "authors": []
  },
  "https://openreview.net/forum?id=MLSvqIHRidA": {
    "title": "Contrastive Divergence Learning is a Time Reversal Adversarial Game",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1a8104680a2b3c03109cbe33fda666876fc90e37",
    "semantic_title": "contrastive divergence learning is a time reversal adversarial game",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=LwEQnp6CYev": {
    "title": "Quantifying Differences in Reward Functions",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "18f44e605082388ffc0a59b895ff70b01cfc0034",
    "semantic_title": "quantifying differences in reward functions",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=37nvvqkCo5": {
    "title": "Long-tail learning via logit adjustment",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "4f65f604d3bf5fa91634484a3232b426267b71ef",
    "semantic_title": "long-tail learning via logit adjustment",
    "citation_count": 736,
    "authors": []
  },
  "https://openreview.net/forum?id=S0UdquAnr9k": {
    "title": "Locally Free Weight Sharing for Network Width Search",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "3fd03da05802b1a714cbafeec43b1b178eeb6672",
    "semantic_title": "locally free weight sharing for network width search",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=OthEq8I5v1": {
    "title": "Mutual Information State Intrinsic Control",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "4d12862b0c6daee864f6f5537270d95b88560ebc",
    "semantic_title": "mutual information state intrinsic control",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=WiGQBFuVRv": {
    "title": "Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "be2a43bfd092781058e2a1597335061d3dc5d5ce",
    "semantic_title": "multi-variate probabilistic time series forecasting via conditioned normalizing flows",
    "citation_count": 187,
    "authors": []
  },
  "https://openreview.net/forum?id=dyaIRud1zXg": {
    "title": "Information Laundering for Model Privacy",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "d2212a1c37fad937fe807dbbb44fe78396ddfd3a",
    "semantic_title": "information laundering for model privacy",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=v9c7hr9ADKx": {
    "title": "UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "21ba210633380911c1e6941d798f7eb2a67f620e",
    "semantic_title": "updet: universal multi-agent rl via policy decoupling with transformers",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=xvxPuCkCNPO": {
    "title": "Correcting experience replay for multi-agent communication",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "e6283a93c69cd1583bba0d54ff0a86b643f09900",
    "semantic_title": "correcting experience replay for multi-agent communication",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=zQTezqCCtNx": {
    "title": "Improving Adversarial Robustness via Channel-wise Activation Suppressing",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "df872e72e87a85f9b5cd28da06ace46386462fde",
    "semantic_title": "improving adversarial robustness via channel-wise activation suppressing",
    "citation_count": 133,
    "authors": []
  },
  "https://openreview.net/forum?id=D9I3drBz4UC": {
    "title": "Long-tailed Recognition by Routing Diverse Distribution-Aware Experts",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "d618752d2e666d7b25f1bd6c7c3bd7c056e19d96",
    "semantic_title": "long-tailed recognition by routing diverse distribution-aware experts",
    "citation_count": 405,
    "authors": []
  },
  "https://openreview.net/forum?id=Tp7kI90Htd": {
    "title": "Generalization in data-driven models of primary visual cortex",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "10c8f1586cbf19d6aa16e86897bb558df7132a8b",
    "semantic_title": "generalization in data-driven models of primary visual cortex",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=Rhsu5qD36cL": {
    "title": "Sequential Density Ratio Estimation for Simultaneous Optimization of Speed and Accuracy",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "b894cb4685fb62b68ed01204dbe68ad8f4af59dc",
    "semantic_title": "sequential density ratio estimation for simultaneous optimization of speed and accuracy",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=eNdiU_DbM9": {
    "title": "Uncertainty Sets for Image Classifiers using Conformal Prediction",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "9cd7c6d19dedc5395155492e0193f399bbabec49",
    "semantic_title": "uncertainty sets for image classifiers using conformal prediction",
    "citation_count": 359,
    "authors": []
  },
  "https://openreview.net/forum?id=9OHFhefeB86": {
    "title": "Graph Convolution with Low-rank Learnable Local Filters",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "c81531d46ca6b01e4881a216fa8079d52392d7c8",
    "semantic_title": "graph convolution with low-rank learnable local filters",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=m1CD7tPubNy": {
    "title": "Mind the Pad -- CNNs Can Develop Blind Spots",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f24f62fabec6bca02658c320ff9b43c84947c5de",
    "semantic_title": "mind the pad - cnns can develop blind spots",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=QfTXQiGYudJ": {
    "title": "Stabilized Medical Image Attacks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "b7ac3263c9acd53f2699e5872cba156c353e9772",
    "semantic_title": "stabilized medical image attacks",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=73WTGs96kho": {
    "title": "Net-DNF: Effective Deep Modeling of Tabular Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "af5151a0b22be3cb9a107c6af563b3603156246b",
    "semantic_title": "net-dnf: effective deep modeling of tabular data",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=mNtmhaDkAr": {
    "title": "Predicting Inductive Biases of Pre-Trained Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a33b4a2002161a18bc7eb929566d77fd5178c2e9",
    "semantic_title": "predicting inductive biases of pre-trained models",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=CBmJwzneppz": {
    "title": "Optimism in Reinforcement Learning with Generalized Linear Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "741ebf91225327bbd11e0e4fa08e329d45637b13",
    "semantic_title": "optimism in reinforcement learning with generalized linear function approximation",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=oyZxhRI2RiE": {
    "title": "SCoRe: Pre-Training for Context Representation in Conversational Semantic Parsing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ff1d3698b8d5f942e6a0775e173720210429b8ae",
    "semantic_title": "score: pre-training for context representation in conversational semantic parsing",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=ECuvULjFQia": {
    "title": "A teacher-student framework to distill future trajectories",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "72e67cf701a59dbdcec58ee61548267985f00acb",
    "semantic_title": "a teacher-student framework to distill future trajectories",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=USCNapootw": {
    "title": "Certify or Predict: Boosting Certified Robustness with Compositional Architectures",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "48b7c4785693ba582f65b266756ba1946cef4bcb",
    "semantic_title": "certify or predict: boosting certified robustness with compositional architectures",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=8VXvj1QNRl1": {
    "title": "On the Transfer of Disentangled Representations in Realistic Settings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4f3ec3b817189927171e26b3232910609a5a33a1",
    "semantic_title": "on the transfer of disentangled representations in realistic settings",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=sCZbhBvqQaU": {
    "title": "Robust Reinforcement Learning on State Observations with Learned Optimal Adversary",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1a627d2a169d71563109546da590a7cceb0b349a",
    "semantic_title": "robust reinforcement learning on state observations with learned optimal adversary",
    "citation_count": 177,
    "authors": []
  },
  "https://openreview.net/forum?id=Hf3qXoiNkR": {
    "title": "Learning from others' mistakes: Avoiding dataset biases without modeling them",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "734f85727161f27bc7b295f0140a905363202d3f",
    "semantic_title": "learning from others' mistakes: avoiding dataset biases without modeling them",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=nVZtXBI6LNn": {
    "title": "Fast and Complete: Enabling Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers",
    "volume": "poster",
    "abstract": "Formal verification of neural networks (NNs) is a challenging and important problem. Existing efficient complete solvers typically require the branch-and-bound (BaB) process, which splits the problem domain into sub-domains and solves each sub-domain using faster but weaker incomplete verifiers, such as Linear Programming (LP) on linearly relaxed sub-domains. In this paper, we propose to use the backward mode linear relaxation based perturbation analysis (LiRPA) to replace LP during the BaB process, which can be efficiently implemented on the typical machine learning accelerators such as GPUs and TPUs. However, unlike LP, LiRPA when applied naively can produce much weaker bounds and even cannot check certain conflicts of sub-domains during splitting, making the entire procedure incomplete after BaB. To address these challenges, we apply a fast gradient based bound tightening procedure combined with batch splits and the design of minimal usage of LP bound procedure, enabling us to effectively use LiRPA on the accelerator hardware for the challenging complete NN verification problem and significantly outperform LP-based approaches. On a single GPU, we demonstrate an order of magnitude speedup compared to existing LP-based approaches",
    "checked": true,
    "id": "01fe33d7147cfb08d4542402089535ed911b4024",
    "semantic_title": "fast and complete: enabling complete neural network verification with rapid and massively parallel incomplete verifiers",
    "citation_count": 197,
    "authors": []
  },
  "https://openreview.net/forum?id=bgQek2O63w": {
    "title": "Self-supervised Adversarial Robustness for the Low-label, High-data Regime",
    "volume": "poster",
    "abstract": "Recent work discovered that training models to be invariant to adversarial perturbations requires substantially larger datasets than those required for standard classification. Perhaps more surprisingly, these larger datasets can be \"mostly\" unlabeled. Pseudo-labeling, a technique simultaneously pioneered by four separate and simultaneous works in 2019, has been proposed as a competitive alternative to labeled data for training adversarially robust models. However, when the amount of labeled data decreases, the performance of pseudo-labeling catastrophically drops, thus questioning the theoretical insights put forward by Uesato et al. (2019), which suggest that the sample complexity for learning an adversarially robust model from unlabeled data should match the fully supervised case. We introduce Bootstrap Your Own Robust Latents (BYORL), a self-supervised learning technique based on BYOL for training adversarially robust models. Our method enables us to train robust representations without any labels (reconciling practice with theory). Most notably, this robust representation can be leveraged by a linear classifier to train adversarially robust models, even when the linear classifier is not trained adversarially. We evaluate BYORL and pseudo-labeling on CIFAR-10 and ImageNet and demonstrate that BYORL achieves significantly higher robustness (i.e., models resulting from BYORL are up to two times more accurate). Experiments on CIFAR-10 against $\\ell_2$ and $\\ell_\\infty$ norm-bounded perturbations demonstrate that BYORL achieves near state-of-the-art robustness with as little as 500 labeled examples. We also note that against $\\ell_2$ norm-bounded perturbations of size $\\epsilon = 128/255$, BYORL surpasses the known state-of-the-art with an accuracy under attack of 77.61% (against 72.91% for the prior art)",
    "checked": true,
    "id": "a8e5c059d2acc2030663a088cd21cd198641974f",
    "semantic_title": "self-supervised adversarial robustness for the low-label, high-data regime",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=ZDnzZrTqU9N": {
    "title": "Modeling the Second Player in Distributionally Robust Optimization",
    "volume": "poster",
    "abstract": "Distributionally robust optimization (DRO) provides a framework for training machine learning models that are able to perform well on a collection of related data distributions (the \"uncertainty set\"). This is done by solving a min-max game: the model is trained to minimize its maximum expected loss among all distributions in the uncertainty set. While careful design of the uncertainty set is critical to the success of the DRO procedure, previous work has been limited to relatively simple alternatives that keep the min-max optimization problem exactly tractable, such as $f$-divergence balls. In this paper, we argue instead for the use of neural generative models to characterize the worst-case distribution, allowing for more flexible and problem-specific selection of the uncertainty set. However, while simple conceptually, this approach poses a number of implementation and optimization challenges. To circumvent these issues, we propose a relaxation of the KL-constrained inner maximization objective that makes the DRO problem more amenable to gradient-based optimization of large scale generative models, and develop model selection heuristics to guide hyper-parameter search. On both toy settings and realistic NLP tasks, we find that the proposed approach yields models that are more robust than comparable baselines",
    "checked": true,
    "id": "5ede529879d162d2779d410a5775d3f6cd6be3f4",
    "semantic_title": "modeling the second player in distributionally robust optimization",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=JFKR3WqwyXR": {
    "title": "Neural Jump Ordinary Differential Equations: Consistent Continuous-Time Prediction and Filtering",
    "volume": "poster",
    "abstract": "Combinations of neural ODEs with recurrent neural networks (RNN), like GRU-ODE-Bayes or ODE-RNN are well suited to model irregularly observed time series. While those models outperform existing discrete-time approaches, no theoretical guarantees for their predictive capabilities are available. Assuming that the irregularly-sampled time series data originates from a continuous stochastic process, the $L^2$-optimal online prediction is the conditional expectation given the currently available information. We introduce the Neural Jump ODE (NJ-ODE) that provides a data-driven approach to learn, continuously in time, the conditional expectation of a stochastic process. Our approach models the conditional expectation between two observations with a neural ODE and jumps whenever a new observation is made. We define a novel training framework, which allows us to prove theoretical guarantees for the first time. In particular, we show that the output of our model converges to the $L^2$-optimal prediction. This can be interpreted as solution to a special filtering problem. We provide experiments showing that the theoretical results also hold empirically. Moreover, we experimentally show that our model outperforms the baselines in more complex learning tasks and give comparisons on real-world datasets",
    "checked": true,
    "id": "cb6937dc0968baa7484455468e6972ab660663cd",
    "semantic_title": "neural jump ordinary differential equations: consistent continuous-time prediction and filtering",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=0O_cQfw6uEh": {
    "title": "Gradient Origin Networks",
    "volume": "poster",
    "abstract": "This paper proposes a new type of generative model that is able to quickly learn a latent representation without an encoder. This is achieved using empirical Bayes to calculate the expectation of the posterior, which is implemented by initialising a latent vector with zeros, then using the gradient of the log-likelihood of the data with respect to this zero vector as new latent points. The approach has similar characteristics to autoencoders, but with a simpler architecture, and is demonstrated in a variational autoencoder equivalent that permits sampling. This also allows implicit representation networks to learn a space of implicit functions without requiring a hypernetwork, retaining their representation advantages across datasets. The experiments show that the proposed method converges faster, with significantly lower reconstruction error than autoencoders, while requiring half the parameters",
    "checked": true,
    "id": "0cf5e829a745058a84ea29b8827ac7acb3f66e61",
    "semantic_title": "gradient origin networks",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=rWZz3sJfCkm": {
    "title": "Efficient Generalized Spherical CNNs",
    "volume": "poster",
    "abstract": "Many problems across computer vision and the natural sciences require the analysis of spherical data, for which representations may be learned efficiently by encoding equivariance to rotational symmetries. We present a generalized spherical CNN framework that encompasses various existing approaches and allows them to be leveraged alongside each other. The only existing non-linear spherical CNN layer that is strictly equivariant has complexity $\\mathcal{O}(C^2L^5)$, where $C$ is a measure of representational capacity and $L$ the spherical harmonic bandlimit. Such a high computational cost often prohibits the use of strictly equivariant spherical CNNs. We develop two new strictly equivariant layers with reduced complexity $\\mathcal{O}(CL^4)$ and $\\mathcal{O}(CL^3 \\log L)$, making larger, more expressive models computationally feasible. Moreover, we adopt efficient sampling theory to achieve further computational savings. We show that these developments allow the construction of more expressive hybrid models that achieve state-of-the-art accuracy and parameter efficiency on spherical benchmark problems",
    "checked": true,
    "id": "59b418a99f915e8bb44b642531e2c2b76a8f8a4b",
    "semantic_title": "efficient generalized spherical cnns",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=XPZIaotutsD": {
    "title": "DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION",
    "volume": "poster",
    "abstract": "Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understand(NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, outperforming the human baseline by a decent margin (90.3 versus 89.8). The pre-trained DeBERTa models and the source code were released at: https://github.com/microsoft/DeBERTa",
    "checked": true,
    "id": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583",
    "semantic_title": "deberta: decoding-enhanced bert with disentangled attention",
    "citation_count": 2875,
    "authors": []
  },
  "https://openreview.net/forum?id=-6vS_4Kfz0": {
    "title": "Optimizing Memory Placement using Evolutionary Graph Reinforcement Learning",
    "volume": "poster",
    "abstract": "For deep neural network accelerators, memory movement is both energetically expensive and can bound computation. Therefore, optimal mapping of tensors to memory hierarchies is critical to performance. The growing complexity of neural networks calls for automated memory mapping instead of manual heuristic approaches; yet the search space of neural network computational graphs have previously been prohibitively large. We introduce Evolutionary Graph Reinforcement Learning (EGRL), a method designed for large search spaces, that combines graph neural networks, reinforcement learning, and evolutionary search. A set of fast, stateless policies guide the evolutionary search to improve its sample-efficiency. We train and validate our approach directly on the Intel NNP-I chip for inference. EGRL outperforms policy-gradient, evolutionary search and dynamic programming baselines on BERT, ResNet-101 and ResNet-50. We additionally achieve 28-78% speed-up compared to the native NNP-I compiler on all three workloads",
    "checked": true,
    "id": "95cb5128f2cb9fb7fb94f2ce62cf1fb62361cc77",
    "semantic_title": "optimizing memory placement using evolutionary graph reinforcement learning",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=V8jrrnwGbuc": {
    "title": "On the geometry of generalization and memorization in deep neural networks",
    "volume": "poster",
    "abstract": "Understanding how large neural networks avoid memorizing training data is key to explaining their high generalization performance. To examine the structure of when and where memorization occurs in a deep network, we use a recently developed replica-based mean field theoretic geometric analysis method. We find that all layers preferentially learn from examples which share features, and link this behavior to generalization performance. Memorization predominately occurs in the deeper layers, due to decreasing object manifolds' radius and dimension, whereas early layers are minimally affected. This predicts that generalization can be restored by reverting the final few layer weights to earlier epochs before significant memorization occurred, which is confirmed by the experiments. Additionally, by studying generalization under different model sizes, we reveal the connection between the double descent phenomenon and the underlying model geometry. Finally, analytical analysis shows that networks avoid memorization early in training because close to initialization, the gradient contribution from permuted examples are small. These findings provide quantitative evidence for the structure of memorization across layers of a deep neural network, the drivers for such structure, and its connection to manifold geometric properties",
    "checked": true,
    "id": "5a41802f417aa7e55d76bfe5a61dae2141a7131b",
    "semantic_title": "on the geometry of generalization and memorization in deep neural networks",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=8xeBUgD8u9": {
    "title": "Continual learning in recurrent neural networks",
    "volume": "poster",
    "abstract": "While a diverse collection of continual learning (CL) methods has been proposed to prevent catastrophic forgetting, a thorough investigation of their effectiveness for processing sequential data with recurrent neural networks (RNNs) is lacking. Here, we provide the first comprehensive evaluation of established CL methods on a variety of sequential data benchmarks. Specifically, we shed light on the particularities that arise when applying weight-importance methods, such as elastic weight consolidation, to RNNs. In contrast to feedforward networks, RNNs iteratively reuse a shared set of weights and require working memory to process input samples. We show that the performance of weight-importance methods is not directly affected by the length of the processed sequences, but rather by high working memory requirements, which lead to an increased need for stability at the cost of decreased plasticity for learning subsequent tasks. We additionally provide theoretical arguments supporting this interpretation by studying linear RNNs. Our study shows that established CL methods can be successfully ported to the recurrent case, and that a recent regularization approach based on hypernetworks outperforms weight-importance methods, thus emerging as a promising candidate for CL in RNNs. Overall, we provide insights on the differences between CL in feedforward networks and RNNs, while guiding towards effective solutions to tackle CL on sequential data",
    "checked": true,
    "id": "a935d162c604b19b32b61f0b9f05f954498d5407",
    "semantic_title": "continual learning in recurrent neural networks",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=01olnfLIbD": {
    "title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching",
    "volume": "poster",
    "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data. In this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a particularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. Previous poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets. The central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset. Finally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems",
    "checked": true,
    "id": "8dc712493df0a46fef830a6c3be64899880100c4",
    "semantic_title": "witches' brew: industrial scale data poisoning via gradient matching",
    "citation_count": 235,
    "authors": []
  },
  "https://openreview.net/forum?id=oFp8Mx_V5FL": {
    "title": "Overfitting for Fun and Profit: Instance-Adaptive Data Compression",
    "volume": "poster",
    "abstract": "Neural data compression has been shown to outperform classical methods in terms of $RD$ performance, with results still improving rapidly. At a high level, neural compression is based on an autoencoder that tries to reconstruct the input instance from a (quantized) latent representation, coupled with a prior that is used to losslessly compress these latents. Due to limitations on model capacity and imperfect optimization and generalization, such models will suboptimally compress test data in general. However, one of the great strengths of learned compression is that if the test-time data distribution is known and relatively low-entropy (e.g. a camera watching a static scene, a dash cam in an autonomous car, etc.), the model can easily be finetuned or adapted to this distribution, leading to improved $RD$ performance. In this paper we take this concept to the extreme, adapting the full model to a single video, and sending model updates (quantized and compressed using a parameter-space prior) along with the latent representation. Unlike previous work, we finetune not only the encoder/latents but the entire model, and - during finetuning - take into account both the effect of model quantization and the additional costs incurred by sending the model updates. We evaluate an image compression model on I-frames (sampled at 2 fps) from videos of the Xiph dataset, and demonstrate that full-model adaptation improves $RD$ performance by ~1 dB, with respect to encoder-only finetuning",
    "checked": true,
    "id": "df80288c17b75cf9417cbf9c8e6b278deb81a0b8",
    "semantic_title": "overfitting for fun and profit: instance-adaptive data compression",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=6zaTwpNSsQ2": {
    "title": "A Block Minifloat Representation for Training Deep Neural Networks",
    "volume": "poster",
    "abstract": "Training Deep Neural Networks (DNN) with high efficiency can be difficult to achieve with native floating-point representations and commercially available hardware. Specialized arithmetic with custom acceleration offers perhaps the most promising alternative. Ongoing research is trending towards narrow floating-point representations, called minifloats, that pack more operations for a given silicon area and consume less power. In this paper, we introduce Block Minifloat (BM), a new spectrum of minifloat formats capable of training DNNs end-to-end with only 4-8 bit weight, activation and gradient tensors. While standard floating-point representations have two degrees of freedom, via the exponent and mantissa, BM exposes the exponent bias as an additional field for optimization. Crucially, this enables training with fewer exponent bits, yielding dense integer-like hardware for fused multiply-add (FMA) operations. For ResNet trained on ImageNet, 6-bit BM achieves almost no degradation in floating-point accuracy with FMA units that are $4.1\\times(23.9\\times)$ smaller and consume $2.3\\times(16.1\\times)$ less energy than FP8 (FP32). Furthermore, our 8-bit BM format matches floating-point accuracy while delivering a higher computational density and faster expected training times",
    "checked": true,
    "id": "37b2417957088d20300a05fecc595c18ba926408",
    "semantic_title": "a block minifloat representation for training deep neural networks",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=9p2ekP904Rs": {
    "title": "Representation Learning via Invariant Causal Mechanisms",
    "volume": "poster",
    "abstract": "Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signal by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework. We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method, and provide an alternative theoretical explanation for the success of these methods. Empirically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on 51 out of 57 games",
    "checked": true,
    "id": "57835c5ad5424f94ee75901c3113730f3900e656",
    "semantic_title": "representation learning via invariant causal mechanisms",
    "citation_count": 256,
    "authors": []
  },
  "https://openreview.net/forum?id=D_KeYoqCYC": {
    "title": "Sparse encoding for more-interpretable feature-selecting representations in probabilistic matrix factorization",
    "volume": "poster",
    "abstract": "Dimensionality reduction methods for count data are critical to a wide range of applications in medical informatics and other fields where model interpretability is paramount. For such data, hierarchical Poisson matrix factorization (HPF) and other sparse probabilistic non-negative matrix factorization (NMF) methods are considered to be interpretable generative models. They consist of sparse transformations for decoding their learned representations into predictions. However, sparsity in representation decoding does not necessarily imply sparsity in the encoding of representations from the original data features. HPF is often incorrectly interpreted in the literature as if it possesses encoder sparsity. The distinction between decoder sparsity and encoder sparsity is subtle but important. Due to the lack of encoder sparsity, HPF does not possess the column-clustering property of classical NMF -- the factor loading matrix does not sufficiently define how each factor is formed from the original features. We address this deficiency by self-consistently enforcing encoder sparsity, using a generalized additive model (GAM), thereby allowing one to relate each representation coordinate to a subset of the original data features. In doing so, the method also gains the ability to perform feature selection. We demonstrate our method on simulated data and give an example of how encoder sparsity is of practical use in a concrete application of representing inpatient comorbidities in Medicare patients",
    "checked": true,
    "id": "302c5388dfc37671ce109d65349a3c8cf0746788",
    "semantic_title": "sparse encoding for more-interpretable feature-selecting representations in probabilistic matrix factorization",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=J3OUycKwz-": {
    "title": "Mapping the Timescale Organization of Neural Language Models",
    "volume": "poster",
    "abstract": "In the human brain, sequences of language input are processed within a distributed and hierarchical architecture, in which higher stages of processing encode contextual information over longer timescales. In contrast, in recurrent neural networks which perform natural language processing, we know little about how the multiple timescales of contextual information are functionally organized. Therefore, we applied tools developed in neuroscience to map the \"processing timescales\" of individual units within a word-level LSTM language model. This timescale-mapping method assigned long timescales to units previously found to track long-range syntactic dependencies. Additionally, the mapping revealed a small subset of the network (less than 15% of units) with long timescales and whose function had not previously been explored. We next probed the functional organization of the network by examining the relationship between the processing timescale of units and their network connectivity. We identified two classes of long-timescale units: \"controller\" units composed a densely interconnected subnetwork and strongly projected to the rest of the network, while \"integrator\" units showed the longest timescales in the network, and expressed projection profiles closer to the mean projection profile. Ablating integrator and controller units affected model performance at different positions within a sentence, suggesting distinctive functions of these two sets of units. Finally, we tested the generalization of these results to a character-level LSTM model and models with different architectures. In summary, we demonstrated a model-free technique for mapping the timescale organization in recurrent neural networks, and we applied this method to reveal the timescale and functional organization of neural language models",
    "checked": true,
    "id": "7c12489708184df6f6a8d9b9f3d8e100b6684500",
    "semantic_title": "mapping the timescale organization of neural language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=C0qJUx5dxFb": {
    "title": "Neural networks with late-phase weights",
    "volume": "poster",
    "abstract": "The largely successful method of training neural networks is to learn their weights using some variant of stochastic gradient descent (SGD). Here, we show that the solutions found by SGD can be further improved by ensembling a subset of the weights in late stages of learning. At the end of learning, we obtain back a single model by taking a spatial average in weight space. To avoid incurring increased computational costs, we investigate a family of low-dimensional late-phase weight models which interact multiplicatively with the remaining parameters. Our results show that augmenting standard models with late-phase weights improves generalization in established benchmarks such as CIFAR-10/100, ImageNet and enwik8. These findings are complemented with a theoretical analysis of a noisy quadratic problem which provides a simplified picture of the late phases of neural network learning",
    "checked": true,
    "id": "b2344edc5f46d7a7cf6f72549ef9fd9a32585f17",
    "semantic_title": "neural networks with late-phase weights",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=Mu2ZxFctAI": {
    "title": "Uncertainty-aware Active Learning for Optimal Bayesian Classifier",
    "volume": "poster",
    "abstract": "For pool-based active learning, in each iteration a candidate training sample is chosen for labeling by optimizing an acquisition function. In Bayesian classification, expected Loss Reduction~(ELR) methods maximize the expected reduction in the classification error given a new labeled candidate based on a one-step-look-ahead strategy. ELR is the optimal strategy with a single query; however, since such myopic strategies cannot identify the long-term effect of a query on the classification error, ELR may get stuck before reaching the optimal classifier. In this paper, inspired by the mean objective cost of uncertainty (MOCU), a metric quantifying the uncertainty directly affecting the classification error, we propose an acquisition function based on a weighted form of MOCU. Similar to ELR, the proposed method focuses on the reduction of the uncertainty that pertains to the classification error. But unlike any other existing scheme, it provides the critical advantage that the resulting Bayesian active learning algorithm guarantees convergence to the optimal classifier of the true model. We demonstrate its performance with both synthetic and real-world datasets",
    "checked": true,
    "id": "7499293ac5a03c966464bfae06ef44c1b5a7f3b9",
    "semantic_title": "uncertainty-aware active learning for optimal bayesian classifier",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=HxzSxSxLOJZ": {
    "title": "ResNet After All: Neural ODEs and Their Numerical Solution",
    "volume": "poster",
    "abstract": "A key appeal of the recently proposed Neural Ordinary Differential Equation (ODE) framework is that it seems to provide a continuous-time extension of discrete residual neural networks. As we show herein, though, trained Neural ODE models actually depend on the specific numerical method used during training. If the trained model is supposed to be a flow generated from an ODE, it should be possible to choose another numerical solver with equal or smaller numerical error without loss of performance. We observe that if training relies on a solver with overly coarse discretization, then testing with another solver of equal or smaller numerical error results in a sharp drop in accuracy. In such cases, the combination of vector field and numerical method cannot be interpreted as a flow generated from an ODE, which arguably poses a fatal breakdown of the Neural ODE concept. We observe, however, that there exists a critical step size beyond which the training yields a valid ODE vector field. We propose a method that monitors the behavior of the ODE solver during training to adapt its step size, aiming to ensure a valid ODE without unnecessarily increasing computational cost. We verify this adaption algorithm on a common bench mark dataset as well as a synthetic dataset",
    "checked": true,
    "id": "59d9b7073424d2bb8f923e5c167fec21959448c8",
    "semantic_title": "resnet after all: neural odes and their numerical solution",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=_IM-AfFhna9": {
    "title": "Generalized Variational Continual Learning",
    "volume": "poster",
    "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration",
    "checked": true,
    "id": "6b41ca9988f832737430b24bdcf37000e1af62ea",
    "semantic_title": "generalized variational continual learning",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=a2gqxKDvYys": {
    "title": "Mind the Gap when Conditioning Amortised Inference in Sequential Latent-Variable Models",
    "volume": "poster",
    "abstract": "Amortised inference enables scalable learning of sequential latent-variable models (LVMs) with the evidence lower bound (ELBO). In this setting, variational posteriors are often only partially conditioned. While the true posteriors depend, e.g., on the entire sequence of observations, approximate posteriors are only informed by past observations. This mimics the Bayesian filter---a mixture of smoothing posteriors. Yet, we show that the ELBO objective forces partially-conditioned amortised posteriors to approximate products of smoothing posteriors instead. Consequently, the learned generative model is compromised. We demonstrate these theoretical findings in three scenarios: traffic flow, handwritten digits, and aerial vehicle dynamics. Using fully-conditioned approximate posteriors, performance improves in terms of generative modelling and multi-step prediction",
    "checked": true,
    "id": "a86bbc845782a316b25137c45d9bd18c6ea05734",
    "semantic_title": "mind the gap when conditioning amortised inference in sequential latent-variable models",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=SK7A5pdrgov": {
    "title": "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning",
    "volume": "poster",
    "abstract": "Despite recent successes of reinforcement learning (RL), it remains a challenge for agents to transfer learned skills to related environments. To facilitate research addressing this problem, we proposeCausalWorld, a benchmark for causal structure and transfer learning in a robotic manipulation environment. The environment is a simulation of an open-source robotic platform, hence offering the possibility of sim-to-real transfer. Tasks consist of constructing 3D shapes from a set of blocks - inspired by how children learn to build complex structures. The key strength of CausalWorld is that it provides a combinatorial family of such tasks with common causal structure and underlying factors (including, e.g., robot and object masses, colors, sizes). The user (or the agent) may intervene on all causal variables, which allows for fine-grained control over how similar different tasks (or task distributions) are. One can thus easily define training and evaluation distributions of a desired difficulty level, targeting a specific form of generalization (e.g., only changes in appearance or object mass). Further, this common parametrization facilitates defining curricula by interpolating between an initial and a target task. While users may define their own task distributions, we present eight meaningful distributions as concrete benchmarks, ranging from simple to very challenging, all of which require long-horizon planning as well as precise low-level motor control. Finally, we provide baseline results for a subset of these tasks on distinct training curricula and corresponding evaluation protocols, verifying the feasibility of the tasks in this benchmark",
    "checked": true,
    "id": "2342b32e245989103dbc56d6f07f1400f4fd2e06",
    "semantic_title": "causalworld: a robotic manipulation benchmark for causal structure and transfer learning",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=fylclEqgvgd": {
    "title": "Transformer protein language models are unsupervised structure learners",
    "volume": "poster",
    "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model",
    "checked": true,
    "id": "043e5e0cf3129284683260976c10d98c7f121f35",
    "semantic_title": "transformer protein language models are unsupervised structure learners",
    "citation_count": 309,
    "authors": []
  },
  "https://openreview.net/forum?id=27acGyyI1BY": {
    "title": "Neural ODE Processes",
    "volume": "poster",
    "abstract": "Neural Ordinary Differential Equations (NODEs) use a neural network to model the instantaneous rate of change in the state of a system. However, despite their apparent suitability for dynamics-governed time-series, NODEs present a few disadvantages. First, they are unable to adapt to incoming data-points, a fundamental requirement for real-time applications imposed by the natural direction of time. Second, time-series are often composed of a sparse set of measurements that could be explained by many possible underlying dynamics. NODEs do not capture this uncertainty. In contrast, Neural Processes (NPs) are a new class of stochastic processes providing uncertainty estimation and fast data-adaptation, but lack an explicit treatment of the flow of time. To address these problems, we introduce Neural ODE Processes (NDPs), a new class of stochastic processes determined by a distribution over Neural ODEs. By maintaining an adaptive data-dependent distribution over the underlying ODE, we show that our model can successfully capture the dynamics of low-dimensional systems from just a few data-points. At the same time, we demonstrate that NDPs scale up to challenging high-dimensional time-series with unknown latent dynamics such as rotating MNIST digits",
    "checked": true,
    "id": "f9dc6861bfe07c0db3d3bcb9d40c47efc816392a",
    "semantic_title": "neural ode processes",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=qbH974jKUVy": {
    "title": "The role of Disentanglement in Generalisation",
    "volume": "poster",
    "abstract": "Combinatorial generalisation — the ability to understand and produce novel combinations of familiar elements — is a core capacity of human intelligence that current AI systems struggle with. Recently, it has been suggested that learning disentangled representations may help address this problem. It is claimed that such representations should be able to capture the compositional structure of the world which can then be combined to support combinatorial generalisation. In this study, we systematically tested how the degree of disentanglement affects various forms of generalisation, including two forms of combinatorial generalisation that varied in difficulty. We trained three classes of variational autoencoders (VAEs) on two datasets on an unsupervised task by excluding combinations of generative factors during training. At test time we ask the models to reconstruct the missing combinations in order to measure generalisation performance. Irrespective of the degree of disentanglement, we found that the models supported only weak combinatorial generalisation. We obtained the same outcome when we directly input perfectly disentangled representations as the latents, and when we tested a model on a more complex task that explicitly required independent generative factors to be controlled. While learning disentangled representations does improve interpretability and sample efficiency in some downstream tasks, our results suggest that they are not sufficient for supporting more difficult forms of generalisation",
    "checked": true,
    "id": "f654d364231122738e8e3cd016da91cfbe3899a9",
    "semantic_title": "the role of disentanglement in generalisation",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=eU776ZYxEpz": {
    "title": "Learning to live with Dale's principle: ANNs with separate excitatory and inhibitory units",
    "volume": "poster",
    "abstract": "The units in artificial neural networks (ANNs) can be thought of as abstractions of biological neurons, and ANNs are increasingly used in neuroscience research. However, there are many important differences between ANN units and real neurons. One of the most notable is the absence of Dale's principle, which ensures that biological neurons are either exclusively excitatory or inhibitory. Dale's principle is typically left out of ANNs because its inclusion impairs learning. This is problematic, because one of the great advantages of ANNs for neuroscience research is their ability to learn complicated, realistic tasks. Here, by taking inspiration from feedforward inhibitory interneurons in the brain we show that we can develop ANNs with separate populations of excitatory and inhibitory units that learn just as well as standard ANNs. We call these networks Dale's ANNs (DANNs). We present two insights that enable DANNs to learn well: (1) DANNs are related to normalization schemes, and can be initialized such that the inhibition centres and standardizes the excitatory activity, (2) updates to inhibitory neuron parameters should be scaled using corrections based on the Fisher Information matrix. These results demonstrate how ANNs that respect Dale's principle can be built without sacrificing learning performance, which is important for future work using ANNs as models of the brain. The results may also have interesting implications for how inhibitory plasticity in the real brain operates",
    "checked": true,
    "id": "29f6a33bf15f44f3ecd3bb3a7aacc0d2da16b7b2",
    "semantic_title": "learning to live with dale's principle: anns with separate excitatory and inhibitory units",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=7EDgLu9reQD": {
    "title": "SALD: Sign Agnostic Learning with Derivatives",
    "volume": "poster",
    "abstract": "Learning 3D geometry directly from raw data, such as point clouds, triangle soups, or unoriented meshes is still a challenging task that feeds many downstream computer vision and graphics applications. In this paper, we introduce SALD: a method for learning implicit neural representations of shapes directly from raw data. We generalize sign agnostic learning (SAL) to include derivatives: given an unsigned distance function to the input raw data, we advocate a novel sign agnostic regression loss, incorporating both pointwise values and gradients of the unsigned distance function. Optimizing this loss leads to a signed implicit function solution, the zero level set of which is a high quality and valid manifold approximation to the input 3D data. The motivation behind SALD is that incorporating derivatives in a regression loss leads to a lower sample complexity, and consequently better fitting. In addition, we provide empirical evidence, as well as theoretical motivation in 2D that SAL enjoys a minimal surface property, favoring minimal area solutions. More importantly, we are able to show that this property still holds for SALD, i.e., with derivatives included. We demonstrate the efficacy of SALD for shape space learning on two challenging datasets: ShapeNet that contains inconsistent orientation and non-manifold meshes, and D-Faust that contains raw 3D scans (triangle soups). On both these datasets, we present state-of-the-art results",
    "checked": false,
    "id": "7e91d3e42fafcb5c3c1f411b30f5c10a3ad2bd6b",
    "semantic_title": "sal++: sign agnostic learning with derivatives",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=TaYhv-q1Xit": {
    "title": "Ringing ReLUs: Harmonic Distortion Analysis of Nonlinear Feedforward Networks",
    "volume": "poster",
    "abstract": "In this paper, we apply harmonic distortion analysis to understand the effect of nonlinearities in the spectral domain. Each nonlinear layer creates higher-frequency harmonics, which we call \"blueshift\", whose magnitude increases with network depth, thereby increasing the \"roughness\" of the output landscape. Unlike differential models (such as vanishing gradients, sharpness), this provides a more global view of how network architectures behave across larger areas of their parameter domain. For example, the model predicts that residual connections are able to counter the effect by dampening corresponding higher frequency modes. We empirically verify the connection between blueshift and architectural choices, and provide evidence for a connection with trainability",
    "checked": true,
    "id": "f2cf28e9e5d51129476007a64c6b6547be5aed6b",
    "semantic_title": "ringing relus: harmonic distortion analysis of nonlinear feedforward networks",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=VD_ozqvBy4W": {
    "title": "CoCon: A Self-Supervised Approach for Controlled Text Generation",
    "volume": "poster",
    "abstract": "Pretrained Transformer-based language models (LMs) display remarkable natural language generation capabilities. With their immense potential, controlling text generation of such LMs is getting attention. While there are studies that seek to control high-level attributes (such as sentiment and topic) of generated text, there is still a lack of more precise control over its content at the word- and phrase-level. Here, we propose Content-Conditioner (CoCon) to control an LM's output text with a content input, at a fine-grained level. In our self-supervised approach, the CoCon block learns to help the LM complete a partially-observed text sequence by conditioning with content inputs that are withheld from the LM. Through experiments, we show that CoCon can naturally incorporate target content into generated texts and control high-level text attributes in a zero-shot manner",
    "checked": true,
    "id": "9b975cd0e9cb330300062916c72df3d63e1db207",
    "semantic_title": "cocon: a self-supervised approach for controlled text generation",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=o3iritJHLfO": {
    "title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech",
    "volume": "poster",
    "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters",
    "checked": true,
    "id": "29e87771996a94d3c0aac39eaf6614a19dbcf5ca",
    "semantic_title": "bidirectional variational inference for non-autoregressive text-to-speech",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=aUX5Plaq7Oy": {
    "title": "Learning continuous-time PDEs from sparse data with graph neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "82977309d976e8146a5376df06b35394b02d47f3",
    "semantic_title": "learning continuous-time pdes from sparse data with graph neural networks",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=CU0APx9LMaL": {
    "title": "NAS-Bench-ASR: Reproducible Neural Architecture Search for Speech Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d0cf9923b9f9f58522b1407df60e6f96d4588f29",
    "semantic_title": "nas-bench-asr: reproducible neural architecture search for speech recognition",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=ULQdiUTHe3y": {
    "title": "Collective Robustness Certificates: Exploiting Interdependence in Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "46e95e212e74fd2b5f9f090dcf7f646651f04f76",
    "semantic_title": "collective robustness certificates: exploiting interdependence in graph neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_mQp5cr_iNy": {
    "title": "Adversarially Guided Actor-Critic",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "60056a89051545aae35d721adc62677f2ea3ee05",
    "semantic_title": "adversarially guided actor-critic",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=OGg9XnKxFAH": {
    "title": "Training independent subnetworks for robust prediction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4ea5678069a6c4213f53972872a211d780f9f42b",
    "semantic_title": "training independent subnetworks for robust prediction",
    "citation_count": 214,
    "authors": []
  },
  "https://openreview.net/forum?id=chPj_I5KMHG": {
    "title": "Grounding Language to Autonomously-Acquired Skills via Goal Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0b22f92be890b720eaa97fa75a49560f11ff2ab2",
    "semantic_title": "grounding language to autonomously-acquired skills via goal generation",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=tL89RnzIiCd": {
    "title": "Hopfield Networks is All You Need",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "804a6d7c23335bbca6eec3b7d3c8366dcbe395a5",
    "semantic_title": "hopfield networks is all you need",
    "citation_count": 464,
    "authors": []
  },
  "https://openreview.net/forum?id=qYZD-AO1Vn": {
    "title": "Differentiable Trust Region Layers for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0efa2efa49a1923954d69eaa9f898af22f63a983",
    "semantic_title": "differentiable trust region layers for deep reinforcement learning",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=ONBPHFZ7zG4": {
    "title": "Temporally-Extended ε-Greedy Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5de6d8b4436f6598f5bcba00bc07d864e962f1fb",
    "semantic_title": "temporally-extended {\\epsilon}-greedy exploration",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=TuK6agbdt27": {
    "title": "Learning Associative Inference Using Fast Weight Memory",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5e11e806d24dd80ecf0f91e7aacedbba8d9fd6fc",
    "semantic_title": "learning associative inference using fast weight memory",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=xoHdgbQJohv": {
    "title": "Multiscale Score Matching for Out-of-Distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "703ffd7ab0bdfcb091400ebb9c7b92446204831f",
    "semantic_title": "multiscale score matching for out-of-distribution detection",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=gJYlaqL8i8": {
    "title": "Learning to Sample with Local and Global Contexts in Experience Replay Buffer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "49f54e261633cd53034d85f777b393f41001c289",
    "semantic_title": "learning to sample with local and global contexts in experience replay buffer",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=tV6oBfuyLTQ": {
    "title": "Parameter-Based Value Functions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "071f7a580657c000dc911f933d0386b3493fbb0c",
    "semantic_title": "parameter-based value functions",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=t86MwoUCCNe": {
    "title": "New Bounds For Distributed Mean Estimation and Variance Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a50307a3a567966368c3e2b4c4b374b9d75766d3",
    "semantic_title": "new bounds for distributed mean estimation and variance reduction",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=cR91FAodFMe": {
    "title": "Learning to Set Waypoints for Audio-Visual Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8170b0ee2d609a4a7a0f3d5816fea70851b032b9",
    "semantic_title": "learning to set waypoints for audio-visual navigation",
    "citation_count": 110,
    "authors": []
  },
  "https://openreview.net/forum?id=K5j7D81ABvt": {
    "title": "Disambiguating Symbolic Expressions in Informal Documents",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e7b959c2e2bc3e823ba9d51213f604530c242912",
    "semantic_title": "disambiguating symbolic expressions in informal documents",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=5NA1PinlGFu": {
    "title": "Colorization Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "30f326353dfeed21216c1cf98d3c42d794fa054e",
    "semantic_title": "colorization transformer",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=SZ3wtsXfzQR": {
    "title": "Theoretical bounds on estimation error for meta-learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3aa54ef5d0bc888f69c92edaacc090049eea1922",
    "semantic_title": "theoretical bounds on estimation error for meta-learning",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=kvhzKz-_DMF": {
    "title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a4b9530c820b5fdb9edd869555cc7a2c2a1d21ad",
    "semantic_title": "variational information bottleneck for effective low-resource fine-tuning",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=IqtonxWI0V3": {
    "title": "TropEx: An Algorithm for Extracting Linear Terms in Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "54f35ae5c2d38d0150ddf601470ed80792e136fa",
    "semantic_title": "tropex: an algorithm for extracting linear terms in deep neural networks",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=dx4b7lm8jMM": {
    "title": "Seq2Tens: An Efficient Representation of Sequences by Low-Rank Tensor Projections",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "79ed5d46d114f8b6de92329694907570b769f408",
    "semantic_title": "seq2tens: an efficient representation of sequences by low-rank tensor projections",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=TVjLza1t4hI": {
    "title": "Representation learning for improved interpretability and classification accuracy of clinical factors from EEG",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "723b7790853f03d13cdc5e2d4b18aef49d639dae",
    "semantic_title": "representation learning for improved interpretability and classification accuracy of clinical factors from eeg",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=Xh5eMZVONGF": {
    "title": "Language-Agnostic Representation Learning of Source Code from Structure and Context",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "05e396e79a2f88f0b8f8d99f5ab36ab3efa95c14",
    "semantic_title": "language-agnostic representation learning of source code from structure and context",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=5Y21V0RDBV": {
    "title": "Generalized Multimodal ELBO",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8de5f12826bd6726f541c8155191a3127eec3710",
    "semantic_title": "generalized multimodal elbo",
    "citation_count": 105,
    "authors": []
  },
  "https://openreview.net/forum?id=p5uylG94S68": {
    "title": "Model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2e274d55a5da52fa1c410f161982dab4a33466c7",
    "semantic_title": "model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose?",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=04ArenGOz3": {
    "title": "Set Prediction without Imposing Structure as Conditional Density Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9da628bd954e2250ec3a8aec9670eab096575c2d",
    "semantic_title": "set prediction without imposing structure as conditional density estimation",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=NX1He-aFO_F": {
    "title": "Learning Value Functions in Deep Policy Gradients using Residual Variance",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6374643b741622ae24b8c1a735e8be0a905df84c",
    "semantic_title": "learning value functions in deep policy gradients using residual variance",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=MBOyiNnYthd": {
    "title": "IDF++: Analyzing and Improving Integer Discrete Flows for Lossless Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "23a108d165feaca935b99432f883ce57f01ef8b6",
    "semantic_title": "idf++: analyzing and improving integer discrete flows for lossless compression",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=agHLCOBM5jP": {
    "title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "331e7a89345464efd53dbefdddf7cb3c4f47187a",
    "semantic_title": "fully unsupervised diversity denoising with convolutional variational autoencoders",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=1FvkSpWosOl": {
    "title": "Is Attention Better Than Matrix Decomposition?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f829a355de02c08567927154d3045a6eb5425c91",
    "semantic_title": "is attention better than matrix decomposition?",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=NomEDgIEBwE": {
    "title": "Improving Transformation Invariance in Contrastive Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "43a160b76a38d2aa913bc78fea2873e2b1bebc7d",
    "semantic_title": "improving transformation invariance in contrastive representation learning",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=rq_Qr0c1Hyo": {
    "title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4fa32fec61c50f8339a05e097dacebe71cf9ab8e",
    "semantic_title": "on the origin of implicit regularization in stochastic gradient descent",
    "citation_count": 211,
    "authors": []
  },
  "https://openreview.net/forum?id=Qun8fv4qSby": {
    "title": "Transient Non-stationarity and Generalisation in Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "90974d9e0df8466a50338601e839fa0ea69c9872",
    "semantic_title": "transient non-stationarity and generalisation in deep reinforcement learning",
    "citation_count": 92,
    "authors": []
  },
  "https://openreview.net/forum?id=oxnp2q-PGL4": {
    "title": "Lossless Compression of Structured Convolutional Models via Lifting",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0b99559725062338ec1a56cd9d0d8371932f9f14",
    "semantic_title": "lossless compression of structured convolutional models via lifting",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=-qh0M9XWxnv": {
    "title": "Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective",
    "volume": "poster",
    "abstract": "In the recent literature of Graph Neural Networks (GNN), the expressive power of models has been studied through their capability to distinguish if two given graphs are isomorphic or not. Since the graph isomorphism problem is NP-intermediate, and Weisfeiler-Lehman (WL) test can give sufficient but not enough evidence in polynomial time, the theoretical power of GNNs is usually evaluated by the equivalence of WL-test order, followed by an empirical analysis of the models on some reference inductive and transductive datasets. However, such analysis does not account the signal processing pipeline, whose capability is generally evaluated in the spectral domain. In this paper, we argue that a spectral analysis of GNNs behavior can provide a complementary point of view to go one step further in the understanding of GNNs. By bridging the gap between the spectral and spatial design of graph convolutions, we theoretically demonstrate some equivalence of the graph convolution process regardless it is designed in the spatial or the spectral domain. Using this connection, we managed to re-formulate most of the state-of-the-art graph neural networks into one common framework. This general framework allows to lead a spectral analysis of the most popular GNNs, explaining their performance and showing their limits according to spectral point of view. Our theoretical spectral analysis is confirmed by experiments on various graph databases. Furthermore, we demonstrate the necessity of high and/or band-pass filters on a graph dataset, while the majority of GNN is limited to only low-pass and inevitably it fails",
    "checked": true,
    "id": "064c71e4bf43e49e3a7cefc29b570c60cbfcfc4f",
    "semantic_title": "analyzing the expressive power of graph neural networks in a spectral perspective",
    "citation_count": 168,
    "authors": []
  },
  "https://openreview.net/forum?id=ZsZM-4iMQkH": {
    "title": "A unifying view on implicit bias in training linear neural networks",
    "volume": "poster",
    "abstract": "We study the implicit bias of gradient flow (i.e., gradient descent with infinitesimal step size) on linear neural network training. We propose a tensor formulation of neural networks that includes fully-connected, diagonal, and convolutional networks as special cases, and investigate the linear version of the formulation called linear tensor networks. With this formulation, we can characterize the convergence direction of the network parameters as singular vectors of a tensor defined by the network. For $L$-layer linear tensor networks that are orthogonally decomposable, we show that gradient flow on separable classification finds a stationary point of the $\\ell_{2/L}$ max-margin problem in a \"transformed\" input space defined by the network. For underdetermined regression, we prove that gradient flow finds a global minimum which minimizes a norm-like function that interpolates between weighted $\\ell_1$ and $\\ell_2$ norms in the transformed input space. Our theorems subsume existing results in the literature while removing standard convergence assumptions. We also provide experiments that corroborate our analysis",
    "checked": true,
    "id": "6010bf788eef9825682ea09068ead4617bf46c26",
    "semantic_title": "a unifying view on implicit bias in training linear neural networks",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=TQt98Ya7UMP": {
    "title": "Balancing Constraints and Rewards with Meta-Gradient D4PG",
    "volume": "poster",
    "abstract": "Deploying Reinforcement Learning (RL) agents to solve real-world applications often requires satisfying complex system constraints. Often the constraint thresholds are incorrectly set due to the complex nature of a system or the inability to verify the thresholds offline (e.g, no simulator or reasonable offline evaluation procedure exists). This results in solutions where a task cannot be solved without violating the constraints. However, in many real-world cases, constraint violations are undesirable yet they are not catastrophic, motivating the need for soft-constrained RL approaches. We present two soft-constrained RL approaches that utilize meta-gradients to find a good trade-off between expected return and minimizing constraint violations. We demonstrate the effectiveness of these approaches by showing that they consistently outperform the baselines across four different Mujoco domains",
    "checked": true,
    "id": "eecc04e4751ef623ecd9f9e69e9601c9431152d2",
    "semantic_title": "balancing constraints and rewards with meta-gradient d4pg",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=lmTWnm3coJJ": {
    "title": "Robust Curriculum Learning: from clean label detection to noisy label self-correction",
    "volume": "poster",
    "abstract": "Neural network training can easily overfit noisy labels resulting in poor generalization performance. Existing methods address this problem by (1) filtering out the noisy data and only using the clean data for training or (2) relabeling the noisy data by the model during training or by another model trained only on a clean dataset. However, the former does not leverage the features' information of wrongly-labeled data, while the latter may produce wrong pseudo-labels for some data and introduce extra noises. In this paper, we propose a smooth transition and interplay between these two strategies as a curriculum that selects training samples dynamically. In particular, we start with learning from clean data and then gradually move to learn noisy-labeled data with pseudo labels produced by a time-ensemble of the model and data augmentations. Instead of using the instantaneous loss computed at the current step, our data selection is based on the dynamics of both the loss and output consistency for each sample across historical steps and different data augmentations, resulting in more precise detection of both clean labels and correct pseudo labels. On multiple benchmarks of noisy labels, we show that our curriculum learning strategy can significantly improve the test accuracy without any auxiliary model or extra clean data",
    "checked": true,
    "id": "66d9cb93003f61e56f825d4bc62023ceceacace4",
    "semantic_title": "robust curriculum learning: from clean label detection to noisy label self-correction",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=xnC8YwKUE3k": {
    "title": "Clairvoyance: A Pipeline Toolkit for Medical Time Series",
    "volume": "poster",
    "abstract": "Time-series learning is the bread and butter of data-driven *clinical decision support*, and the recent explosion in ML research has demonstrated great potential in various healthcare settings. At the same time, medical time-series problems in the wild are challenging due to their highly *composite* nature: They entail design choices and interactions among components that preprocess data, impute missing values, select features, issue predictions, estimate uncertainty, and interpret models. Despite exponential growth in electronic patient data, there is a remarkable gap between the potential and realized utilization of ML for clinical research and decision support. In particular, orchestrating a real-world project lifecycle poses challenges in engineering (i.e. hard to build), evaluation (i.e. hard to assess), and efficiency (i.e. hard to optimize). Designed to address these issues simultaneously, Clairvoyance proposes a unified, end-to-end, autoML-friendly pipeline that serves as a (i) software toolkit, (ii) empirical standard, and (iii) interface for optimization. Our ultimate goal lies in facilitating transparent and reproducible experimentation with complex inference workflows, providing integrated pathways for (1) personalized prediction, (2) treatment-effect estimation, and (3) information acquisition. Through illustrative examples on real-world data in outpatient, general wards, and intensive-care settings, we illustrate the applicability of the pipeline paradigm on core tasks in the healthcare journey. To the best of our knowledge, Clairvoyance is the first to demonstrate viability of a comprehensive and automatable pipeline for clinical time-series ML",
    "checked": true,
    "id": "b628204e8718c49b26f7d3bb58692595c378aa31",
    "semantic_title": "clairvoyance: a pipeline toolkit for medical time series",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=w2Z2OwVNeK": {
    "title": "Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks",
    "volume": "poster",
    "abstract": "In high-dimensional state spaces, the usefulness of Reinforcement Learning (RL) is limited by the problem of exploration. This issue has been addressed using potential-based reward shaping (PB-RS) previously. In the present work, we introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the strict optimality guarantees of PB-RS to a guarantee of preserved long-term behavior. Being less restrictive, FV-RS allows for reward shaping functions that are even better suited for improving the sample efficiency of RL algorithms. In particular, we consider settings in which the agent has access to an approximate plan. Here, we use examples of simulated robotic manipulation tasks to demonstrate that plan-based FV-RS can indeed significantly improve the sample efficiency of RL over plan-based PB-RS",
    "checked": true,
    "id": "0203baf1af219fad578f055ee5c9481ca4123b2c",
    "semantic_title": "plan-based relaxed reward shaping for goal-directed tasks",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=-Hs_otp2RB": {
    "title": "Improving VAEs' Robustness to Adversarial Attack",
    "volume": "poster",
    "abstract": "Variational autoencoders (VAEs) have recently been shown to be vulnerable to adversarial attacks, wherein they are fooled into reconstructing a chosen target image. However, how to defend against such attacks remains an open problem. We make significant advances in addressing this issue by introducing methods for producing adversarially robust VAEs. Namely, we first demonstrate that methods proposed to obtain disentangled latent representations produce VAEs that are more robust to these attacks. However, this robustness comes at the cost of reducing the quality of the reconstructions. We ameliorate this by applying disentangling methods to hierarchical VAEs. The resulting models produce high--fidelity autoencoders that are also adversarially robust. We confirm their capabilities on several different datasets and with current state-of-the-art VAE adversarial attacks, and also show that they increase the robustness of downstream tasks to attack",
    "checked": true,
    "id": "7ab3551d85efbc3c844dcf5714f2a5d3e8bafcf0",
    "semantic_title": "improving vaes' robustness to adversarial attack",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=4T489T4yav": {
    "title": "Differentiable Segmentation of Sequences",
    "volume": "poster",
    "abstract": "Segmented models are widely used to describe non-stationary sequential data with discrete change points. Their estimation usually requires solving a mixed discrete-continuous optimization problem, where the segmentation is the discrete part and all other model parameters are continuous. A number of estimation algorithms have been developed that are highly specialized for their specific model assumptions. The dependence on non-standard algorithms makes it hard to integrate segmented models in state-of-the-art deep learning architectures that critically depend on gradient-based optimization techniques. In this work, we formulate a relaxed variant of segmented models that enables joint estimation of all model parameters, including the segmentation, with gradient descent. We build on recent advances in learning continuous warping functions and propose a novel family of warping functions based on the two-sided power (TSP) distribution. TSP-based warping functions are differentiable, have simple closed-form expressions, and can represent segmentation functions exactly. Our formulation includes the important class of segmented generalized linear models as a special case, which makes it highly versatile. We use our approach to model the spread of COVID-19 with Poisson regression, apply it on a change point detection task, and learn classification models with concept drift. The experiments show that our approach effectively learns all these tasks with standard algorithms for gradient descent",
    "checked": true,
    "id": "1ab5db66bea0f628ca8b0c70e53b6971ecab695a",
    "semantic_title": "differentiable segmentation of sequences",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=kyaIeYj4zZ": {
    "title": "GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing",
    "volume": "poster",
    "abstract": "We present GraPPa, an effective pre-training approach for table semantic parsing that learns a compositional inductive bias in the joint representations of textual and tabular data. We construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG). We pre-train our model on the synthetic data to inject important structural properties commonly found in semantic parsing into the pre-training language model. To maintain the model's ability to represent real-world data, we also include masked language modeling (MLM) on several existing table-related datasets to regularize our pre-training process. Our proposed pre-training strategy is much data-efficient. When incorporated with strong base semantic parsers, GraPPa achieves new state-of-the-art results on four popular fully supervised and weakly supervised table semantic parsing tasks",
    "checked": true,
    "id": "8b2cbb2f101b025c16e12d0d7628f65e5378e10d",
    "semantic_title": "grappa: grammar-augmented pre-training for table semantic parsing",
    "citation_count": 262,
    "authors": []
  },
  "https://openreview.net/forum?id=t0TaKv0Gx6Z": {
    "title": "Sliced Kernelized Stein Discrepancy",
    "volume": "poster",
    "abstract": "Kernelized Stein discrepancy (KSD), though being extensively used in goodness-of-fit tests and model learning, suffers from the curse-of-dimensionality. We address this issue by proposing the sliced Stein discrepancy and its scalable and kernelized variants, which employs kernel-based test functions defined on the optimal one-dimensional projections. When applied to goodness-of-fit tests, extensive experiments show the proposed discrepancy significantly outperforms KSD and various baselines in high dimensions. For model learning, we show its advantages by training an independent component analysis when compared with existing Stein discrepancy baselines. We further propose a novel particle inference method called sliced Stein variational gradient descent (S-SVGD) which alleviates the mode-collapse issue of SVGD in training variational autoencoders",
    "checked": true,
    "id": "90a36c56d3e763f2336dfe3f3ec932b3f41b4c18",
    "semantic_title": "sliced kernelized stein discrepancy",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=P0p33rgyoE": {
    "title": "Variational Intrinsic Control Revisited",
    "volume": "poster",
    "abstract": "In this paper, we revisit variational intrinsic control (VIC), an unsupervised reinforcement learning method for finding the largest set of intrinsic options available to an agent. In the original work by Gregor et al. (2016), two VIC algorithms were proposed: one that represents the options explicitly, and the other that does it implicitly. We show that the intrinsic reward used in the latter is subject to bias in stochastic environments, causing convergence to suboptimal solutions. To correct this behavior, we propose two methods respectively based on the transitional probability model and Gaussian Mixture Model. We substantiate our claims through rigorous mathematical derivations and experimental analyses",
    "checked": true,
    "id": "c092f4d29e3d9dcc92750d845a9b9978182643c1",
    "semantic_title": "variational intrinsic control revisited",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=pHXfe1cOmA": {
    "title": "HyperDynamics: Meta-Learning Object and Agent Dynamics with Hypernetworks",
    "volume": "poster",
    "abstract": "We propose HyperDynamics, a dynamics meta-learning framework that conditions on an agent's interactions with the environment and optionally its visual observations, and generates the parameters of neural dynamics models based on inferred properties of the dynamical system. Physical and visual properties of the environment that are not part of the low-dimensional state yet affect its temporal dynamics are inferred from the interaction history and visual observations, and are implicitly captured in the generated parameters. We test HyperDynamics on a set of object pushing and locomotion tasks. It outperforms existing dynamics models in the literature that adapt to environment variations by learning dynamics over high dimensional visual observations, capturing the interactions of the agent in recurrent state representations, or using gradient-based meta-optimization. We also show our method matches the performance of an ensemble of separately trained experts, while also being able to generalize well to unseen environment variations at test time. We attribute its good performance to the multiplicative interactions between the inferred system properties—captured in the generated parameters—and the low-dimensional state representation of the dynamical system",
    "checked": true,
    "id": "116a9a94df0213b11aa0585af81109e853f9deb3",
    "semantic_title": "hyperdynamics: meta-learning object and agent dynamics with hypernetworks",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=AHOs7Sm5H7R": {
    "title": "Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning",
    "volume": "poster",
    "abstract": "Matrix factorization is a simple and natural test-bed to investigate the implicit regularization of gradient descent. Gunasekar et al. (2017) conjectured that gradient flow with infinitesimal initialization converges to the solution that minimizes the nuclear norm, but a series of recent papers argued that the language of norm minimization is not sufficient to give a full characterization for the implicit regularization. In this work, we provide theoretical and empirical evidence that for depth-2 matrix factorization, gradient flow with infinitesimal initialization is mathematically equivalent to a simple heuristic rank minimization algorithm, Greedy Low-Rank Learning, under some reasonable assumptions. This generalizes the rank minimization view from previous works to a much broader setting and enables us to construct counter-examples to refute the conjecture from Gunasekar et al. (2017). We also extend the results to the case where depth >= 3, and we show that the benefit of being deeper is that the above convergence has a much weaker dependence over initialization magnitude so that this rank minimization is more likely to take effect for initialization with practical scale",
    "checked": true,
    "id": "27558603527494688876cbd0cf5af53af5127f4a",
    "semantic_title": "towards resolving the implicit bias of gradient descent for matrix factorization: greedy low-rank learning",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=6isfR3JCbi": {
    "title": "Private Post-GAN Boosting",
    "volume": "poster",
    "abstract": "Differentially private GANs have proven to be a promising approach for generating realistic synthetic data without compromising the privacy of individuals. Due to the privacy-protective noise introduced in the training, the convergence of GANs becomes even more elusive, which often leads to poor utility in the output generator at the end of training. We propose Private post-GAN boosting (Private PGB), a differentially private method that combines samples produced by the sequence of generators obtained during GAN training to create a high-quality synthetic dataset. To that end, our method leverages the Private Multiplicative Weights method (Hardt and Rothblum, 2010) to reweight generated samples. We evaluate Private PGB on two dimensional toy data, MNIST images, US Census data and a standard machine learning prediction task. Our experiments show that Private PGB improves upon a standard private GAN approach across a collection of quality measures. We also provide a non-private variant of PGB that improves the data quality of standard GAN training",
    "checked": true,
    "id": "34ceeaef8f0569ca107b72622c14cbec15bf778f",
    "semantic_title": "private post-gan boosting",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=IX3Nnir2omJ": {
    "title": "Characterizing signal propagation to close the performance gap in unnormalized ResNets",
    "volume": "poster",
    "abstract": "Batch Normalization is a key component in almost all state-of-the-art image classifiers, but it also introduces practical challenges: it breaks the independence between training examples within a batch, can incur compute and memory overhead, and often results in unexpected bugs. Building on recent theoretical analyses of deep ResNets at initialization, we propose a simple set of analysis tools to characterize signal propagation on the forward pass, and leverage these tools to design highly performant ResNets without activation normalization layers. Crucial to our success is an adapted version of the recently proposed Weight Standardization. Our analysis tools show how this technique preserves the signal in ReLU networks by ensuring that the per-channel activation means do not grow with depth. Across a range of FLOP budgets, our networks attain performance competitive with state-of-the-art EfficientNets on ImageNet",
    "checked": true,
    "id": "feeae38fd404fdc17cad19d80461843059216fde",
    "semantic_title": "characterizing signal propagation to close the performance gap in unnormalized resnets",
    "citation_count": 125,
    "authors": []
  },
  "https://openreview.net/forum?id=KmykpuSrjcq": {
    "title": "Prototypical Contrastive Learning of Unsupervised Representations",
    "volume": "poster",
    "abstract": "This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL",
    "checked": true,
    "id": "8bf6c69bae0956db13aa9129fedc69fdc1256dce",
    "semantic_title": "prototypical contrastive learning of unsupervised representations",
    "citation_count": 1001,
    "authors": []
  },
  "https://openreview.net/forum?id=Ec85b0tUwbA": {
    "title": "Hyperbolic Neural Networks++",
    "volume": "poster",
    "abstract": "Hyperbolic spaces, which have the capacity to embed tree structures without distortion owing to their exponential volume growth, have recently been applied to machine learning to better capture the hierarchical nature of data. In this study, we generalize the fundamental components of neural networks in a single hyperbolic geometry model, namely, the Poincaré ball model. This novel methodology constructs a multinomial logistic regression, fully-connected layers, convolutional layers, and attention mechanisms under a unified mathematical interpretation, without increasing the parameters. Experiments show the superior parameter efficiency of our methods compared to conventional hyperbolic components, and stability and outperformance over their Euclidean counterparts",
    "checked": false,
    "id": "8c315c669a9e5f37ffcfb68b060e39d05de02d9f",
    "semantic_title": "hyperbolic neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-N7PBXqOUJZ": {
    "title": "Lipschitz Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Viewing recurrent neural networks (RNNs) as continuous-time dynamical systems, we propose a recurrent unit that describes the hidden state's evolution with two parts: a well-understood linear component plus a Lipschitz nonlinearity. This particular functional form facilitates stability analysis of the long-term behavior of the recurrent unit using tools from nonlinear systems theory. In turn, this enables architectural design decisions before experimentation. Sufficient conditions for global stability of the recurrent unit are obtained, motivating a novel scheme for constructing hidden-to-hidden matrices. Our experiments demonstrate that the Lipschitz RNN can outperform existing recurrent units on a range of benchmark tasks, including computer vision, language modeling and speech prediction tasks. Finally, through Hessian-based analysis we demonstrate that our Lipschitz recurrent unit is more robust with respect to input and parameter perturbations as compared to other continuous-time RNNs",
    "checked": true,
    "id": "bbc89fa342c06cf2216884238c531b1f6434e61d",
    "semantic_title": "lipschitz recurrent neural networks",
    "citation_count": 115,
    "authors": []
  },
  "https://openreview.net/forum?id=Rd138pWXMvG": {
    "title": "A statistical theory of cold posteriors in deep neural networks",
    "volume": "poster",
    "abstract": "To get Bayesian neural networks to perform comparably to standard neural networks it is usually necessary to artificially reduce uncertainty using a tempered or cold posterior. This is extremely concerning: if the prior is accurate, Bayes inference/decision theory is optimal, and any artificial changes to the posterior should harm performance. While this suggests that the prior may be at fault, here we argue that in fact, BNNs for image classification use the wrong likelihood. In particular, standard image benchmark datasets such as CIFAR-10 are carefully curated. We develop a generative model describing curation which gives a principled Bayesian account of cold posteriors, because the likelihood under this new generative model closely matches the tempered likelihoods used in past work",
    "checked": true,
    "id": "6f5a417658bfa3ef86e787e837e2c47b10a2a699",
    "semantic_title": "a statistical theory of cold posteriors in deep neural networks",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=ebS5NUfoMKL": {
    "title": "Boost then Convolve: Gradient Boosting Meets Graph Neural Networks",
    "volume": "poster",
    "abstract": "Graph neural networks (GNNs) are powerful models that have been successful in various graph representation learning tasks. Whereas gradient boosted decision trees (GBDT) often outperform other machine learning methods when faced with heterogeneous tabular data. But what approach should be used for graphs with tabular node features? Previous GNN models have mostly focused on networks with homogeneous sparse features and, as we show, are suboptimal in the heterogeneous setting. In this work, we propose a novel architecture that trains GBDT and GNN jointly to get the best of both worlds: the GBDT model deals with heterogeneous features, while GNN accounts for the graph structure. Our model benefits from end-to-end optimization by allowing new trees to fit the gradient updates of GNN. With an extensive experimental comparison to the leading GBDT and GNN models, we demonstrate a significant increase in performance on a variety of graphs with tabular features. The code is available: https://github.com/nd7141/bgnn",
    "checked": true,
    "id": "07d38f062da2f13e3ff532d630aacc3e8dcaccca",
    "semantic_title": "boost then convolve: gradient boosting meets graph neural networks",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=TGFO0DbD_pk": {
    "title": "Genetic Soft Updates for Policy Evolution in Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "The combination of Evolutionary Algorithms (EAs) and Deep Reinforcement Learning (DRL) has been recently proposed to merge the benefits of both solutions. Existing mixed approaches, however, have been successfully applied only to actor-critic methods and present significant overhead. We address these issues by introducing a novel mixed framework that exploits a periodical genetic evaluation to soft update the weights of a DRL agent. The resulting approach is applicable with any DRL method and, in a worst-case scenario, it does not exhibit detrimental behaviours. Experiments in robotic applications and continuous control benchmarks demonstrate the versatility of our approach that significantly outperforms prior DRL, EAs, and mixed approaches. Finally, we employ formal verification to confirm the policy improvement, mitigating the inefficient exploration and hyper-parameter sensitivity of DRL.ment, mitigating the inefficient exploration and hyper-parameter sensitivity of DRL",
    "checked": true,
    "id": "f667c3a0c904db9d55c225c0411833abbc38a561",
    "semantic_title": "genetic soft updates for policy evolution in deep reinforcement learning",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=5l9zj5G7vDY": {
    "title": "Spatially Structured Recurrent Modules",
    "volume": "poster",
    "abstract": "Capturing the structure of a data-generating process by means of appropriate inductive biases can help in learning models that generalise well and are robust to changes in the input distribution. While methods that harness spatial and temporal structures find broad application, recent work has demonstrated the potential of models that leverage sparse and modular structure using an ensemble of sparingly interacting modules. In this work, we take a step towards dynamic models that are capable of simultaneously exploiting both modular and spatiotemporal structures. To this end, we model the dynamical system as a collection of autonomous but sparsely interacting sub-systems that interact according to a learned topology which is informed by the spatial structure of the underlying system. This gives rise to a class of models that are well suited for capturing the dynamics of systems that only offer local views into their state, along with corresponding spatial locations of those views. On the tasks of video prediction from cropped frames and multi-agent world modelling from partial observations in the challenging Starcraft2 domain, we find our models to be more robust to the number of available views and better capable of generalisation to novel tasks without additional training than strong baselines that perform equally well or better on the training distribution",
    "checked": true,
    "id": "ffcea46f57ef1f5f65686e2424976e1af8689e78",
    "semantic_title": "s2rms: spatially structured recurrent modules",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=nzpLWnVAyah": {
    "title": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines",
    "volume": "poster",
    "abstract": "Fine-tuning pre-trained transformer-based language models such as BERT has become a common practice dominating leaderboards across various NLP benchmarks. Despite the strong empirical performance of fine-tuned models, fine-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Previous literature (Devlin et al., 2019; Lee et al., 2020; Dodge et al., 2020) identified two potential reasons for the observed instability: catastrophic forgetting and small size of the fine-tuning datasets. In this paper, we show that both hypotheses fail to explain the fine-tuning instability. We analyze BERT, RoBERTa, and ALBERT, fine-tuned on commonly used datasets from the GLUE benchmark, and show that the observed instability is caused by optimization difficulties that lead to vanishing gradients. Additionally, we show that the remaining variance of the downstream task performance can be attributed to differences in generalization where fine-tuned models with the same training loss exhibit noticeably different test performance. Based on our analysis, we present a simple but strong baseline that makes fine-tuning BERT-based models significantly more stable than the previously proposed approaches. Code to reproduce our results is available online: https://github.com/uds-lsv/bert-stable-fine-tuning",
    "checked": true,
    "id": "8b9d77d5e52a70af37451d3db3d32781b83ea054",
    "semantic_title": "on the stability of fine-tuning bert: misconceptions, explanations, and strong baselines",
    "citation_count": 367,
    "authors": []
  },
  "https://openreview.net/forum?id=rRFIni1CYmy": {
    "title": "End-to-End Egospheric Spatial Memory",
    "volume": "poster",
    "abstract": "Spatial memory, or the ability to remember and recall specific locations and objects, is central to autonomous agents' ability to carry out tasks in real environments. However, most existing artificial memory modules are not very adept at storing spatial information. We propose a parameter-free module, Egospheric Spatial Memory (ESM), which encodes the memory in an ego-sphere around the agent, enabling expressive 3D representations. ESM can be trained end-to-end via either imitation or reinforcement learning, and improves both training efficiency and final performance against other memory baselines on both drone and manipulator visuomotor control tasks. The explicit egocentric geometry also enables us to seamlessly combine the learned controller with other non-learned modalities, such as local obstacle avoidance. We further show applications to semantic segmentation on the ScanNet dataset, where ESM naturally combines image-level and map-level inference modalities. Through our broad set of experiments, we show that ESM provides a general computation graph for embodied spatial reasoning, and the module forms a bridge between real-time mapping systems and differentiable memory architectures. Implementation at: https://github.com/ivy-dl/memory",
    "checked": true,
    "id": "26926181a8e0a4d2f2fd1478aee63201974de91c",
    "semantic_title": "end-to-end egospheric spatial memory",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=jM76BCb6F9m": {
    "title": "LEAF: A Learnable Frontend for Audio Classification",
    "volume": "poster",
    "abstract": "Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters",
    "checked": true,
    "id": "1192660d960d44ae7416a3b7562e87c5f338d691",
    "semantic_title": "leaf: a learnable frontend for audio classification",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=Qr0aRliE_Hb": {
    "title": "Simple Augmentation Goes a Long Way: ADRL for DNN Quantization",
    "volume": "poster",
    "abstract": "Mixed precision quantization improves DNN performance by assigning different layers with different bit-width values. Searching for the optimal bit-width for each layer, however, remains a challenge. Deep Reinforcement Learning (DRL) shows some recent promise. It however suffers instability due to function approximation errors, causing large variances in the early training stages, slow convergence, and suboptimal policies in the mixed-precision quantization problem. This paper proposes augmented DRL (ADRL) as a way to alleviate these issues. This new strategy augments the neural networks in DRL with a complementary scheme to boost the performance of learning. The paper examines the effectiveness of ADRL both analytically and empirically, showing that it can produce more accurate quantized models than the state of the art DRL-based quantization while improving the learning speed by 4.5-64 times",
    "checked": true,
    "id": "7533636f292f37c90acd7241aae6719454ece1dd",
    "semantic_title": "simple augmentation goes a long way: adrl for dnn quantization",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=krz7T0xU9Z_": {
    "title": "The inductive bias of ReLU networks on orthogonally separable data",
    "volume": "poster",
    "abstract": "We study the inductive bias of two-layer ReLU networks trained by gradient flow. We identify a class of easy-to-learn (`orthogonally separable') datasets, and characterise the solution that ReLU networks trained on such datasets converge to. Irrespective of network width, the solution turns out to be a combination of two max-margin classifiers: one corresponding to the positive data subset and one corresponding to the negative data subset. The proof is based on the recently introduced concept of extremal sectors, for which we prove a number of properties in the context of orthogonal separability. In particular, we prove stationarity of activation patterns from some time $T$ onwards, which enables a reduction of the ReLU network to an ensemble of linear subnetworks",
    "checked": true,
    "id": "71452416e60234dc6fa32109af2fef7a684e06f1",
    "semantic_title": "the inductive bias of relu networks on orthogonally separable data",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=7_G8JySGecm": {
    "title": "Monte-Carlo Planning and Learning with Language Action Value Estimates",
    "volume": "poster",
    "abstract": "Interactive Fiction (IF) games provide a useful testbed for language-based reinforcement learning agents, posing significant challenges of natural language understanding, commonsense reasoning, and non-myopic planning in the combinatorial search space. Agents based on standard planning algorithms struggle to play IF games due to the massive search space of language actions. Thus, language-grounded planning is a key ability of such agents, since inferring the consequence of language action based on semantic understanding can drastically improve search. In this paper, we introduce Monte-Carlo planning with Language Action Value Estimates (MC-LAVE) that combines a Monte-Carlo tree search with language-driven exploration. MC-LAVE invests more search effort into semantically promising language actions using locally optimistic language value estimates, yielding a significant reduction in the effective search space of language actions. We then present a reinforcement learning approach via MC-LAVE, which alternates between MC-LAVE planning and supervised learning of the self-generated language actions. In the experiments, we demonstrate that our method achieves new high scores in various IF games",
    "checked": true,
    "id": "09e28122994e7879ebf4b4d38b9c80c47fe6be3c",
    "semantic_title": "monte-carlo planning and learning with language action value estimates",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=v_1Soh8QUNc": {
    "title": "Learning Energy-Based Models by Diffusion Recovery Likelihood",
    "volume": "poster",
    "abstract": "While energy-based models (EBMs) exhibit a number of desirable properties, training and sampling on high-dimensional datasets remains challenging. Inspired by recent progress on diffusion probabilistic models, we present a diffusion recovery likelihood method to tractably learn and sample from a sequence of EBMs trained on increasingly noisy versions of a dataset. Each EBM is trained with recovery likelihood, which maximizes the conditional probability of the data at a certain noise level given their noisy versions at a higher noise level. Optimizing recovery likelihood is more tractable than marginal likelihood, as sampling from the conditional distributions is much easier than sampling from the marginal distributions. After training, synthesized images can be generated by the sampling process that initializes from Gaussian white noise distribution and progressively samples the conditional distributions at decreasingly lower noise levels. Our method generates high fidelity samples on various image datasets. On unconditional CIFAR-10 our method achieves FID 9.58 and inception score 8.30, superior to the majority of GANs. Moreover, we demonstrate that unlike previous work on EBMs, our long-run MCMC samples from the conditional distributions do not diverge and still represent realistic images, allowing us to accurately estimate the normalized density of data even for high-dimensional datasets. Our implementation is available at \\url{https://github.com/ruiqigao/recovery_likelihood}",
    "checked": true,
    "id": "90695f261c12265fb2694fe89cf390aad029a7dc",
    "semantic_title": "learning energy-based models by diffusion recovery likelihood",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=wQRlSUZ5V7B": {
    "title": "Capturing Label Characteristics in VAEs",
    "volume": "poster",
    "abstract": "We present a principled approach to incorporating labels in variational autoencoders (VAEs) that captures the rich characteristic information associated with those labels. While prior work has typically conflated these by learning latent variables that directly correspond to label values, we argue this is contrary to the intended effect of supervision in VAEs—capturing rich label characteristics with the latents. For example, we may want to capture the characteristics of a face that make it look young, rather than just the age of the person. To this end, we develop a novel VAE model, the characteristic capturing VAE (CCVAE), which \"reparameterizes\" supervision through auxiliary variables and a concomitant variational objective. Through judicious structuring of mappings between latent and auxiliary variables, we show that the CCVAE can effectively learn meaningful representations of the characteristics of interest across a variety of supervision schemes. In particular, we show that the CCVAE allows for more effective and more general interventions to be performed, such as smooth traversals within the characteristics for a given label, diverse conditional generation, and transferring characteristics across datapoints",
    "checked": true,
    "id": "5c6c4dac75e8579f2748c58f7ed970a1750068eb",
    "semantic_title": "capturing label characteristics in vaes",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=Fmg_fQYUejf": {
    "title": "Linear Mode Connectivity in Multitask and Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9e2084eda22a62be53f9d70ab6628fae7b400e1b",
    "semantic_title": "linear mode connectivity in multitask and continual learning",
    "citation_count": 150,
    "authors": []
  },
  "https://openreview.net/forum?id=hkMoYYEkBoI": {
    "title": "Computational Separation Between Convolutional and Fully-Connected Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1e24ddc732a5a0187783b4a221a44e26def6d434",
    "semantic_title": "computational separation between convolutional and fully-connected networks",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=xpFFI_NtgpW": {
    "title": "Rethinking Embedding Coupling in Pre-trained Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bc87279d4b32a425377ff18ab63f7ecf95ff228c",
    "semantic_title": "rethinking embedding coupling in pre-trained language models",
    "citation_count": 144,
    "authors": []
  },
  "https://openreview.net/forum?id=vyY0jnWG-tK": {
    "title": "Physics-aware, probabilistic model order reduction with guaranteed stability",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7869928bc7c5809b05760367acc2eb5fd2a0e8c7",
    "semantic_title": "physics-aware, probabilistic model order reduction with guaranteed stability",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=-Lr-u0b42he": {
    "title": "Disentangling 3D Prototypical Networks for Few-Shot Concept Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9e9be84ee3793f9bc951b7680368d1b8ec9113a2",
    "semantic_title": "disentangling 3d prototypical networks for few-shot concept learning",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=kE3vd639uRW": {
    "title": "LiftPool: Bidirectional ConvNet Pooling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b7815c4a743ba26e10904647fc5916aa45cbf701",
    "semantic_title": "liftpool: bidirectional convnet pooling",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=4TSiOTkKe5P": {
    "title": "Latent Convergent Cross Mapping",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b0397ad6df0e67b1c8373edbe0a3b7d215b77b57",
    "semantic_title": "latent convergent cross mapping",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=yvQKLaqNE6M": {
    "title": "You Only Need Adversarial Supervision for Semantic Image Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8f7d4d61292886c111b1fe11f524ecaa8101de27",
    "semantic_title": "you only need adversarial supervision for semantic image synthesis",
    "citation_count": 193,
    "authors": []
  },
  "https://openreview.net/forum?id=wXgk_iCiYGo": {
    "title": "A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0d6b4c7bb6246ad6ddca0fa44b19f1bda43e91fa",
    "semantic_title": "a diffusion theory for deep learning dynamics: stochastic gradient descent exponentially favors flat minima",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=euDnVs0Ynts": {
    "title": "Robust Learning of Fixed-Structure Bayesian Networks in Nearly-Linear Time",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7427493f3233580b28c949baace2461a92f37706",
    "semantic_title": "robust learning of fixed-structure bayesian networks in nearly-linear time",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UvBPbpvHRj-": {
    "title": "Activation-level uncertainty in deep neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a4e645c2adebb01c25f2229bac69bbfb6e049bab",
    "semantic_title": "activation-level uncertainty in deep neural networks",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=2CjEVW-RGOJ": {
    "title": "SkipW: Resource Adaptable RNN with Strict Upper Computational Limit",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3edffde19f424327315956402f3aa181b66c435b",
    "semantic_title": "skipw: resource adaptable rnn with strict upper computational limit",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bEoxzW_EXsa": {
    "title": "Wasserstein-2 Generative Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "445ba554b6668ccd4a5c75f148430f7b99d5cefa",
    "semantic_title": "wasserstein-2 generative networks",
    "citation_count": 112,
    "authors": []
  },
  "https://openreview.net/forum?id=JkfYjnOEo6M": {
    "title": "Group Equivariant Stand-Alone Self-Attention For Vision",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7a5d1a7646ce1884ad76a0e177f956ae4d77c722",
    "semantic_title": "group equivariant stand-alone self-attention for vision",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=3tFAs5E-Pe": {
    "title": "Continuous Wasserstein-2 Barycenter Estimation without Minimax Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e2ac06efafb020e5815bd95620632b6340de7a3d",
    "semantic_title": "continuous wasserstein-2 barycenter estimation without minimax optimization",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=tGZu6DlbreV": {
    "title": "RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "32783724e06cca7328ddcc61e59b026cf624e102",
    "semantic_title": "rnnlogic: learning logic rules for reasoning on knowledge graphs",
    "citation_count": 189,
    "authors": []
  },
  "https://openreview.net/forum?id=N0M_4BkQ05i": {
    "title": "Selective Classification Can Magnify Disparities Across Groups",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "35610aa2be06c4af4ab944cda51de6fbad14b527",
    "semantic_title": "selective classification can magnify disparities across groups",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=Ogga20D2HO-": {
    "title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e1e17e17abc51b99a95c9ae8d67a6909924f3986",
    "semantic_title": "fedmix: approximation of mixup under mean augmented federated learning",
    "citation_count": 175,
    "authors": []
  },
  "https://openreview.net/forum?id=jznizqvr15J": {
    "title": "In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0f4374e62ae889dd2a35dc4c97b9a0510146fa87",
    "semantic_title": "in-n-out: pre-training and self-training using auxiliary information for out-of-distribution robustness",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=hSjxQ3B7GWq": {
    "title": "Sample-Efficient Automated Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "283f975e221f56974f30a3b12c7fb015c0c77377",
    "semantic_title": "sample-efficient automated deep reinforcement learning",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=whE31dn74cL": {
    "title": "A Temporal Kernel Approach for Deep Learning with Continuous-time Information",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ad594fcaf59b705180466c14df1aefcdab009dbd",
    "semantic_title": "a temporal kernel approach for deep learning with continuous-time information",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=VErQxgyrbfn": {
    "title": "Convex Regularization behind Neural Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a0886a4e611b211c4baf7b2fa5971efcb505c9de",
    "semantic_title": "convex regularization behind neural reconstruction",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=fGF8qAqpXXG": {
    "title": "Vector-output ReLU Neural Network Problems are Copositive Programs: Convex Analysis of Two Layer Networks and Polynomial-time Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "49d4a9e019b19b7a294d5590fcf43154663f2524",
    "semantic_title": "vector-output relu neural network problems are copositive programs: convex analysis of two layer networks and polynomial-time algorithms",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=5NsEIflpbSv": {
    "title": "Learning Better Structured Representations Using Low-rank Adaptive Label Smoothing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "394b10cfc658efbf69ec22f3f20dc5fde4affedf",
    "semantic_title": "learning better structured representations using low-rank adaptive label smoothing",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=eo6U4CAwVmg": {
    "title": "Training GANs with Stronger Augmentations via Contrastive Discriminator",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f3f1c2bc4cfb4c86c418ea312cacefaf6db0065e",
    "semantic_title": "training gans with stronger augmentations via contrastive discriminator",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=y06VOYLcQXa": {
    "title": "Private Image Reconstruction from System Side Channels Using Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "777d11340b9279e78a872f6373aac11b8834e6b4",
    "semantic_title": "private image reconstruction from system side channels using generative models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=ac288vnG_7U": {
    "title": "Learning to Make Decisions via Submodular Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "239eadb202a429d7bd64c69a492c5eccf83b2002",
    "semantic_title": "learning to make decisions via submodular regularization",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=3T9iFICe0Y9": {
    "title": "The Recurrent Neural Tangent Kernel",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "65d6365ca79fd78b966a794c05c7148317b9dee1",
    "semantic_title": "the recurrent neural tangent kernel",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=9uvhpyQwzM_": {
    "title": "Evaluation of Similarity-based Explanations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9782870723374d3dfb49a8042dd73800a71b7601",
    "semantic_title": "evaluation of similarity-based explanations",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=8xLkv08d70T": {
    "title": "Adaptive Procedural Task Generation for Hard-Exploration Problems",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e423c07b36936ddce137bce009b318f2c2741be5",
    "semantic_title": "adaptive procedural task generation for hard-exploration problems",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=dx11_7vm5_r": {
    "title": "Linear Last-iterate Convergence in Constrained Saddle-point Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "954931aa9a4b17e60d534f017ef600f7a87d31e2",
    "semantic_title": "linear last-iterate convergence in constrained saddle-point optimization",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=tiqI7w64JG2": {
    "title": "On Graph Neural Networks versus Graph-Augmented MLPs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "74d14abc4c9abd78415d6ae75d3313a5636ac8bc",
    "semantic_title": "on graph neural networks versus graph-augmented mlps",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=9SS69KwomAM": {
    "title": "Solving Compositional Reinforcement Learning Problems via Task Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "761427520e163f79869813122f4ca6eacbe27cbe",
    "semantic_title": "solving compositional reinforcement learning problems via task reduction",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=VJnrYcnRc6": {
    "title": "Conditional Generative Modeling via Learning the Latent Space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "09d7708c4460c74212d09ad7ea9293b210133207",
    "semantic_title": "conditional generative modeling via learning the latent space",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=kDnal_bbb-E": {
    "title": "DialoGraph: Incorporating Interpretable Strategy-Graph Networks into Negotiation Dialogues",
    "volume": "poster",
    "abstract": "To successfully negotiate a deal, it is not enough to communicate fluently: pragmatic planning of persuasive negotiation strategies is essential. While modern dialogue agents excel at generating fluent sentences, they still lack pragmatic grounding and cannot reason strategically. We present DialoGraph, a negotiation system that incorporates pragmatic strategies in a negotiation dialogue using graph neural networks. DialoGraph explicitly incorporates dependencies between sequences of strategies to enable improved and interpretable prediction of next optimal strategies, given the dialogue context. Our graph-based method outperforms prior state-of-the-art negotiation models both in the accuracy of strategy/dialogue act prediction and in the quality of downstream dialogue response generation. We qualitatively show further benefits of learned strategy-graphs in providing explicit associations between effective negotiation strategies over the course of the dialogue, leading to interpretable and strategic dialogues",
    "checked": true,
    "id": "2785ec102c3d2888ae9d29c8d3fba2a666b5b3af",
    "semantic_title": "dialograph: incorporating interpretable strategy-graph networks into negotiation dialogues",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=eEn8KTtJOx": {
    "title": "WaNet - Imperceptible Warping-based Backdoor Attack",
    "volume": "poster",
    "abstract": "With the thriving of deep learning and the widespread practice of using pre-trained networks, backdoor attacks have become an increasing security threat drawing many research interests in recent years. A third-party model can be poisoned in training to work well in normal conditions but behave maliciously when a trigger pattern appears. However, the existing backdoor attacks are all built on noise perturbation triggers, making them noticeable to humans. In this paper, we instead propose using warping-based triggers. The proposed backdoor outperforms the previous methods in a human inspection test by a wide margin, proving its stealthiness. To make such models undetectable by machine defenders, we propose a novel training mode, called the ``noise mode. The trained networks successfully attack and bypass the state-ofthe art defense methods on standard classification datasets, including MNIST, CIFAR-10, GTSRB, and CelebA. Behavior analyses show that our backdoors are transparent to network inspection, further proving this novel attack mechanism's efficiency",
    "checked": false,
    "id": "b5b0bbbe50f10d77138f089f96e157b5cb37d99f",
    "semantic_title": "l-red: efficient post-training detection of imperceptible backdoor attacks without access to the training set",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=B5VvQrI49Pa": {
    "title": "Nonseparable Symplectic Neural Networks",
    "volume": "poster",
    "abstract": "Predicting the behaviors of Hamiltonian systems has been drawing increasing attention in scientific machine learning. However, the vast majority of the literature was focused on predicting separable Hamiltonian systems with their kinematic and potential energy terms being explicitly decoupled, while building data-driven paradigms to predict nonseparable Hamiltonian systems that are ubiquitous in fluid dynamics and quantum mechanics were rarely explored. The main computational challenge lies in the effective embedding of symplectic priors to describe the inherently coupled evolution of position and momentum, which typically exhibits intricate dynamics. To solve the problem, we propose a novel neural network architecture, Nonseparable Symplectic Neural Networks (NSSNNs), to uncover and embed the symplectic structure of a nonseparable Hamiltonian system from limited observation data. The enabling mechanics of our approach is an augmented symplectic time integrator to decouple the position and momentum energy terms and facilitate their evolution. We demonstrated the efficacy and versatility of our method by predicting a wide range of Hamiltonian systems, both separable and nonseparable, including chaotic vortical flows. We showed the unique computational merits of our approach to yield long-term, accurate, and robust predictions for large-scale Hamiltonian systems by rigorously enforcing symplectomorphism",
    "checked": true,
    "id": "1ab8bae9fa2df31c81aac4226e12094c616f8a22",
    "semantic_title": "nonseparable symplectic neural networks",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=lvRTC669EY_": {
    "title": "Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization",
    "volume": "poster",
    "abstract": "We propose a simple, general and effective technique, Reward Randomization for discovering diverse strategic policies in complex multi-agent games. Combining reward randomization and policy gradient, we derive a new algorithm, Reward-Randomized Policy Gradient (RPG). RPG is able to discover a set of multiple distinctive human-interpretable strategies in challenging temporal trust dilemmas, including grid-world games and a real-world game Agar.io, where multiple equilibria exist but standard multi-agent policy gradient algorithms always converge to a fixed one with a sub-optimal payoff for every player even using state-of-the-art exploration techniques. Furthermore, with the set of diverse strategies from RPG, we can (1) achieve higher payoffs by fine-tuning the best policy from the set; and (2) obtain an adaptive agent by using this set of strategies as its training opponents",
    "checked": true,
    "id": "78d4285fbcb06fcf3289e099c19abae7e7ed95a3",
    "semantic_title": "discovering diverse multi-agent strategic behavior via reward randomization",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=9ITXiTrAoT": {
    "title": "Multi-timescale Representation Learning in LSTM Language Models",
    "volume": "poster",
    "abstract": "Language models must capture statistical dependencies between words at timescales ranging from very short to very long. Earlier work has demonstrated that dependencies in natural language tend to decay with distance between words according to a power law. However, it is unclear how this knowledge can be used for analyzing or designing neural network language models. In this work, we derived a theory for how the memory gating mechanism in long short-term memory (LSTM) language models can capture power law decay. We found that unit timescales within an LSTM, which are determined by the forget gate bias, should follow an Inverse Gamma distribution. Experiments then showed that LSTM language models trained on natural English text learn to approximate this theoretical distribution. Further, we found that explicitly imposing the theoretical distribution upon the model during training yielded better language model perplexity overall, with particular improvements for predicting low-frequency (rare) words. Moreover, the explicit multi-timescale model selectively routes information about different types of words through units with different timescales, potentially improving model interpretability. These results demonstrate the importance of careful, theoretically-motivated analysis of memory and timescale in language models",
    "checked": true,
    "id": "a833db6a27527fe5b85a3b161fc4317397e6b065",
    "semantic_title": "multi-timescale representation learning in lstm language models",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=HHiiQKWsOcV": {
    "title": "Explaining the Efficacy of Counterfactually Augmented Data",
    "volume": "poster",
    "abstract": "In attempts to produce machine learning models less reliant on spurious patterns in NLP datasets, researchers have recently proposed curating counterfactually augmented data (CAD) via a human-in-the-loop process in which given some documents and their (initial) labels, humans must revise the text to make a counterfactual label applicable. Importantly, edits that are not necessary to flip the applicable label are prohibited. Models trained on the augmented (original and revised) data appear, empirically, to rely less on semantically irrelevant words and to generalize better out of domain. While this work draws loosely on causal thinking, the underlying causal model (even at an abstract level) and the principles underlying the observed out-of-domain improvements remain unclear. In this paper, we introduce a toy analog based on linear Gaussian models, observing interesting relationships between causal models, measurement noise, out-of-domain generalization, and reliance on spurious signals. Our analysis provides some insights that help to explain the efficacy of CAD. Moreover, we develop the hypothesis that while adding noise to causal features should degrade both in-domain and out-of-domain performance, adding noise to non-causal features should lead to relative improvements in out-of-domain performance. This idea inspires a speculative test for determining whether a feature attribution technique has identified the causal spans. If adding noise (e.g., by random word flips) to the highlighted spans degrades both in-domain and out-of-domain performance on a battery of challenge datasets, but adding noise to the complement gives improvements out-of-domain, this suggests we have identified causal spans. Thus, we present a large scale empirical study comparing spans edited to create CAD to those selected by attention and saliency maps. Across numerous challenge domains and models, we find that the hypothesized phenomenon is pronounced for CAD",
    "checked": true,
    "id": "24fcdaf969089e6a411f7cebc9274bbc53c25e42",
    "semantic_title": "explaining the efficacy of counterfactually-augmented data",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=fAbkE6ant2": {
    "title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end Training",
    "volume": "poster",
    "abstract": "Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration",
    "checked": true,
    "id": "32a81aef8063274b1d8cc770a7f6dcfd8efe5336",
    "semantic_title": "revisiting locally supervised learning: an alternative to end-to-end training",
    "citation_count": 92,
    "authors": []
  },
  "https://openreview.net/forum?id=fgd7we_uZa6": {
    "title": "How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?",
    "volume": "poster",
    "abstract": "A recent line of research on deep learning focuses on the extremely over-parameterized setting, and shows that when the network width is larger than a high degree polynomial of the training sample size $n$ and the inverse of the target error $\\epsilon^{-1}$, deep neural networks learned by (stochastic) gradient descent enjoy nice optimization and generalization guarantees. Very recently, it is shown that under certain margin assumptions on the training data, a polylogarithmic width condition suffices for two-layer ReLU networks to converge and generalize (Ji and Telgarsky, 2020). However, whether deep neural networks can be learned with such a mild over-parameterization is still an open question. In this work, we answer this question affirmatively and establish sharper learning guarantees for deep ReLU networks trained by (stochastic) gradient descent. In specific, under certain assumptions made in previous work, our optimization and generalization guarantees hold with network width polylogarithmic in $n$ and $\\epsilon^{-1}$. Our results push the study of over-parameterized deep neural networks towards more practical settings",
    "checked": true,
    "id": "8f1b6eca0bdeb8cf0173d950b1157f290439cead",
    "semantic_title": "how much over-parameterization is sufficient to learn deep relu networks?",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=RqCC_00Bg7V": {
    "title": "Blending MPC & Value Function Approximation for Efficient Reinforcement Learning",
    "volume": "poster",
    "abstract": "Model-Predictive Control (MPC) is a powerful tool for controlling complex, real-world systems that uses a model to make predictions about future behavior. For each state encountered, MPC solves an online optimization problem to choose a control action that will minimize future cost. This is a surprisingly effective strategy, but real-time performance requirements warrant the use of simple models. If the model is not sufficiently accurate, then the resulting controller can be biased, limiting performance. We present a framework for improving on MPC with model-free reinforcement learning (RL). The key insight is to view MPC as constructing a series of local Q-function approximations. We show that by using a parameter $\\lambda$, similar to the trace decay parameter in TD($\\lambda$), we can systematically trade-off learned value estimates against the local Q-function approximations. We present a theoretical analysis that shows how error from inaccurate models in MPC and value function estimation in RL can be balanced. We further propose an algorithm that changes $\\lambda$ over time to reduce the dependence on MPC as our estimates of the value function improve, and test the efficacy our approach on challenging high-dimensional manipulation tasks with biased models in simulation. We demonstrate that our approach can obtain performance comparable with MPC with access to true dynamics even under severe model bias and is more sample efficient as compared to model-free RL",
    "checked": true,
    "id": "d4ce35cc18f37c41fdb89013d9ec0c5fd59bf1b7",
    "semantic_title": "blending mpc & value function approximation for efficient reinforcement learning",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=T1XmO8ScKim": {
    "title": "Probabilistic Numeric Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "Continuous input signals like images and time series that are irregularly sampled or have missing values are challenging for existing deep learning methods. Coherently defined feature representations must depend on the values in unobserved regions of the input. Drawing from the work in probabilistic numerics, we propose Probabilistic Numeric Convolutional Neural Networks which represent features as Gaussian processes, providing a probabilistic description of discretization error. We then define a convolutional layer as the evolution of a PDE defined on this GP, followed by a nonlinearity. This approach also naturally admits steerable equivariant convolutions under e.g. the rotation group. In experiments we show that our approach yields a $3\\times$ reduction of error from the previous state of the art on the SuperPixel-MNIST dataset and competitive performance on the medical time series dataset PhysioNet2012",
    "checked": true,
    "id": "1f5a66f818d286c48d2c5e84f8ae35be4cb0ac47",
    "semantic_title": "probabilistic numeric convolutional neural networks",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=TNkPBBYFkXg": {
    "title": "HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients",
    "volume": "poster",
    "abstract": "Federated Learning (FL) is a method of training machine learning models on private data distributed over a large number of possibly heterogeneous clients such as mobile phones and IoT devices. In this work, we propose a new federated learning framework named HeteroFL to address heterogeneous clients equipped with very different computation and communication capabilities. Our solution can enable the training of heterogeneous local models with varying computation complexities and still produce a single global inference model. For the first time, our method challenges the underlying assumption of existing work that local models have to share the same architecture as the global model. We demonstrate several strategies to enhance FL training and conduct extensive empirical evaluations, including five computation complexity levels of three model architecture on three datasets. We show that adaptively distributing subnetworks according to clients' capabilities is both computation and communication efficient",
    "checked": true,
    "id": "f9b3ed20d6da7dfbfbd8a58e2bde173e5e9c768c",
    "semantic_title": "heterofl: computation and communication efficient federated learning for heterogeneous clients",
    "citation_count": 585,
    "authors": []
  },
  "https://openreview.net/forum?id=Ov_sMNau-PF": {
    "title": "Semantic Re-tuning with Contrastive Tension",
    "volume": "poster",
    "abstract": "Extracting semantically useful natural language sentence representations from pre-trained deep neural networks such as Transformers remains a challenge. We first demonstrate that pre-training objectives impose a significant task bias onto the final layers of models with a layer-wise survey of the Semantic Textual Similarity (STS) correlations for multiple common Transformer language models. We then propose a new self-supervised method called Contrastive Tension (CT) to counter such biases. CT frames the training objective as a noise-contrastive task between the final layer representations of two independent models, in turn making the final layer representations suitable for feature extraction. Results from multiple common unsupervised and supervised STS tasks indicate that CT outperforms previous State Of The Art (SOTA), and when combining CT with supervised data we improve upon previous SOTA results with large margins",
    "checked": true,
    "id": "cbd78779af4e83fe101ba3f7ba4d4786388d12d8",
    "semantic_title": "semantic re-tuning with contrastive tension",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=l-PrrQrK0QR": {
    "title": "Dataset Meta-Learning from Kernel Ridge-Regression",
    "volume": "poster",
    "abstract": "One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. We introduce the novel concept of $\\epsilon$-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar performance. We introduce a meta-learning algorithm Kernel Inducing Points (KIP) for obtaining such remarkable datasets, drawing inspiration from recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime. Consequently, we obtain state of the art results for neural network dataset distillation with potential applications to privacy-preservation",
    "checked": true,
    "id": "8212605d274d5e68bcedf990728f4f5c26f88168",
    "semantic_title": "dataset meta-learning from kernel ridge-regression",
    "citation_count": 264,
    "authors": []
  },
  "https://openreview.net/forum?id=1GTma8HwlYp": {
    "title": "AUXILIARY TASK UPDATE DECOMPOSITION: THE GOOD, THE BAD AND THE NEUTRAL",
    "volume": "poster",
    "abstract": "While deep learning has been very beneficial in data-rich settings, tasks with smaller training set often resort to pre-training or multitask learning to leverage data from other tasks. In this case, careful consideration is needed to select tasks and model parameterizations such that updates from the auxiliary tasks actually help the primary task. We seek to alleviate this burden by formulating a model-agnostic framework that performs fine-grained manipulation of the auxiliary task gradients. We propose to decompose auxiliary updates into directions which help, damage or leave the primary task loss unchanged. This allows weighting the update directions differently depending on their impact on the problem of interest. We present a novel and efficient algorithm for that purpose and show its advantage in practice. Our method leverages efficient automatic differentiation procedures and randomized singular value decomposition for scalability. We show that our framework is generic and encompasses some prior work as particular cases. Our approach consistently outperforms strong and widely used baselines when leveraging out-of-distribution data for Text and Image classification tasks",
    "checked": true,
    "id": "06da85d0191857d0e48a3565e84e017ff76aa8dc",
    "semantic_title": "auxiliary task update decomposition: the good, the bad and the neutral",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=Lc28QAB4ypz": {
    "title": "Fast And Slow Learning Of Recurrent Independent Mechanisms",
    "volume": "poster",
    "abstract": "Decomposing knowledge into interchangeable pieces promises a generalization advantage when there are changes in distribution. A learning agent interacting with its environment is likely to be faced with situations requiring novel combinations of existing pieces of knowledge. We hypothesize that such a decomposition of knowledge is particularly relevant for being able to generalize in a systematic way to out-of-distribution changes. To study these ideas, we propose a particular training framework in which we assume that the pieces of knowledge an agent needs and its reward function are stationary and can be re-used across tasks. An attention mechanism dynamically selects which modules can be adapted to the current task, and the parameters of the \\textit{selected} modules are allowed to change quickly as the learner is confronted with variations in what it experiences, while the parameters of the attention mechanisms act as stable, slowly changing, meta-parameters. We focus on pieces of knowledge captured by an ensemble of modules sparsely communicating with each other via a bottleneck of attention. We find that meta-learning the modular aspects of the proposed system greatly helps in achieving faster adaptation in a reinforcement learning setup involving navigation in a partially observed grid world with image-level input. We also find that reversing the role of parameters and meta-parameters does not work nearly as well, suggesting a particular role for fast adaptation of the dynamically selected modules",
    "checked": true,
    "id": "b124114c0f9a4145892e328f19df7850333f207e",
    "semantic_title": "fast and slow learning of recurrent independent mechanisms",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=YHdeAO61l6T": {
    "title": "Auction Learning as a Two-Player Game",
    "volume": "poster",
    "abstract": "Designing an incentive compatible auction that maximizes expected revenue is a central problem in Auction Design. While theoretical approaches to the problem have hit some limits, a recent research direction initiated by Duetting et al. (2019) consists in building neural network architectures to find optimal auctions. We propose two conceptual deviations from their approach which result in enhanced performance. First, we use recent results in theoretical auction design to introduce a time-independent Lagrangian. This not only circumvents the need for an expensive hyper-parameter search (as in prior work), but also provides a single metric to compare the performance of two auctions (absent from prior work). Second, the optimization procedure in previous work uses an inner maximization loop to compute optimal misreports. We amortize this process through the introduction of an additional neural network. We demonstrate the effectiveness of our approach by learning competitive or strictly improved auctions compared to prior work. Both results together further imply a novel formulation of Auction Design as a two-player game with stationary utility functions",
    "checked": true,
    "id": "1db7a459b6062beb3ce81b564e4df16935c717f2",
    "semantic_title": "auction learning as a two-player game",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=TR-Nj6nFx42": {
    "title": "A PAC-Bayesian Approach to Generalization Bounds for Graph Neural Networks",
    "volume": "poster",
    "abstract": "In this paper, we derive generalization bounds for two primary classes of graph neural networks (GNNs), namely graph convolutional networks (GCNs) and message passing GNNs (MPGNNs), via a PAC-Bayesian approach. Our result reveals that the maximum node degree and the spectral norm of the weights govern the generalization bounds of both models. We also show that our bound for GCNs is a natural generalization of the results developed in \\citep{neyshabur2017pac} for fully-connected and convolutional neural networks. For MPGNNs, our PAC-Bayes bound improves over the Rademacher complexity based bound \\citep{garg2020generalization}, showing a tighter dependency on the maximum node degree and the maximum hidden dimension. The key ingredients of our proofs are a perturbation analysis of GNNs and the generalization of PAC-Bayes analysis to non-homogeneous GNNs. We perform an empirical study on several synthetic and real-world graph datasets and verify that our PAC-Bayes bound is tighter than others",
    "checked": true,
    "id": "bb681868f002199ac29fef0102173e61fc56825d",
    "semantic_title": "a pac-bayesian approach to generalization bounds for graph neural networks",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=zx_uX-BO7CH": {
    "title": "Contextual Transformation Networks for Online Continual Learning",
    "volume": "poster",
    "abstract": "Continual learning methods with fixed architectures rely on a single network to learn models that can perform well on all tasks. As a result, they often only accommodate common features of those tasks but neglect each task's specific features. On the other hand, dynamic architecture methods can have a separate network for each task, but they are too expensive to train and not scalable in practice, especially in online settings. To address this problem, we propose a novel online continual learning method named ``Contextual Transformation Networks\" (CTN) to efficiently model the \\emph{task-specific features} while enjoying neglectable complexity overhead compared to other fixed architecture methods. Moreover, inspired by the Complementary Learning Systems (CLS) theory, we propose a novel dual memory design and an objective to train CTN that can address both catastrophic forgetting and knowledge transfer simultaneously. Our extensive experiments show that CTN is competitive with a large scale dynamic architecture network and consistently outperforms other fixed architecture methods under the same standard backbone. Our implementation can be found at \\url{https://github.com/phquang/Contextual-Transformation-Network}",
    "checked": true,
    "id": "93e9cb14bccb3c7a11ca77fcd4d2e98e71195b58",
    "semantic_title": "contextual transformation networks for online continual learning",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=ahAUv8TI2Mz": {
    "title": "Adaptive and Generative Zero-Shot Learning",
    "volume": "poster",
    "abstract": "We address the problem of generalized zero-shot learning (GZSL) where the task is to predict the class label of a target image whether its label belongs to the seen or unseen category. Similar to ZSL, the learning setting assumes that all class-level semantic features are given, while only the images of seen classes are available for training. By exploring the correlation between image features and the corresponding semantic features, the main idea of the proposed approach is to enrich the semantic-to-visual (S2V) embeddings via a seamless fusion of adaptive and generative learning. To this end, we extend the semantic features of each class by supplementing image-adaptive attention so that the learned S2V embedding can account for not only inter-class but also intra-class variations. In addition, to break the limit of training with images only from seen classes, we design a generative scheme to simultaneously generate virtual class labels and their visual features by sampling and interpolating over seen counterparts. In inference, a testing image will give rise to two different S2V embeddings, seen and virtual. The former is used to decide whether the underlying label is of the unseen category or otherwise a specific seen class; the latter is to predict an unseen class label. To demonstrate the effectiveness of our method, we report state-of-the-art results on four standard GZSL datasets, including an ablation study of the proposed modules",
    "checked": true,
    "id": "931cbe1dac39e8dd1378b5752fd0ed128199e08f",
    "semantic_title": "adaptive and generative zero-shot learning",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=_i3ASPp12WS": {
    "title": "Online Adversarial Purification based on Self-supervised Learning",
    "volume": "poster",
    "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification",
    "checked": true,
    "id": "2338d9517a47e6fee1ff1f3b8998bdaa622bbfcb",
    "semantic_title": "online adversarial purification based on self-supervised learning",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=N6JECD-PI5w": {
    "title": "FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders",
    "volume": "poster",
    "abstract": "Pretrained text encoders, such as BERT, have been applied increasingly in various natural language processing (NLP) tasks, and have recently demonstrated significant performance gains. However, recent studies have demonstrated the existence of social bias in these pretrained NLP models. Although prior works have made progress on word-level debiasing, improved sentence-level fairness of pretrained encoders still lacks exploration. In this paper, we proposed the first neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, we introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoders, while continuously showing desirable performance on downstream tasks. Moreover, our post hoc method does not require any retraining of the text encoders, further enlarging FairFil's application space",
    "checked": true,
    "id": "df157cb42b574c3f46b269504c18375bfa5bc5b1",
    "semantic_title": "fairfil: contrastive neural debiasing method for pretrained text encoders",
    "citation_count": 106,
    "authors": []
  },
  "https://openreview.net/forum?id=HIGSa_3kOx3": {
    "title": "Reset-Free Lifelong Learning with Skill-Space Planning",
    "volume": "poster",
    "abstract": "The objective of \\textit{lifelong} reinforcement learning (RL) is to optimize agents which can continuously adapt and interact in changing environments. However, current RL approaches fail drastically when environments are non-stationary and interactions are non-episodic. We propose \\textit{Lifelong Skill Planning} (LiSP), an algorithmic framework for lifelong RL based on planning in an abstract space of higher-order skills. We learn the skills in an unsupervised manner using intrinsic rewards and plan over the learned skills using a learned dynamics model. Moreover, our framework permits skill discovery even from offline data, thereby reducing the need for excessive real-world interactions. We demonstrate empirically that LiSP successfully enables long-horizon planning and learns agents that can avoid catastrophic failures even in challenging non-stationary and non-episodic environments derived from gridworld and MuJoCo benchmarks",
    "checked": true,
    "id": "25c310d4f7bc581a94455a1e96373d924ec736ae",
    "semantic_title": "reset-free lifelong learning with skill-space planning",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=u2YNJPcQlwq": {
    "title": "Efficient Empowerment Estimation for Unsupervised Stabilization",
    "volume": "poster",
    "abstract": "Intrinsically motivated artificial agents learn advantageous behavior without externally-provided rewards. Previously, it was shown that maximizing mutual information between agent actuators and future states, known as the empowerment principle, enables unsupervised stabilization of dynamical systems at upright positions, which is a prototypical intrinsically motivated behavior for upright standing and walking. This follows from the coincidence between the objective of stabilization and the objective of empowerment. Unfortunately, sample-based estimation of this kind of mutual information is challenging. Recently, various variational lower bounds (VLBs) on empowerment have been proposed as solutions; however, they are often biased, unstable in training, and have high sample complexity. In this work, we propose an alternative solution based on a trainable representation of a dynamical system as a Gaussian channel, which allows us to efficiently calculate an unbiased estimator of empowerment by convex optimization. We demonstrate our solution for sample-based unsupervised stabilization on different dynamical control systems and show the advantages of our method by comparing it to the existing VLB approaches. Specifically, we show that our method has a lower sample complexity, is more stable in training, possesses the essential properties of the empowerment function, and allows estimation of empowerment from images. Consequently, our method opens a path to wider and easier adoption of empowerment for various applications",
    "checked": true,
    "id": "532b7a2cd054465f66ec582b55e128bdb842c28f",
    "semantic_title": "efficient empowerment estimation for unsupervised stabilization",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=UFGEelJkLu5": {
    "title": "MixKD: Towards Efficient Distillation of Large-scale Language Models",
    "volume": "poster",
    "abstract": "Large-scale language models have recently demonstrated impressive empirical performance. Nevertheless, the improved results are attained at the price of bigger models, more power consumption, and slower inference, which hinder their applicability to low-resource (both memory and computation) platforms. Knowledge distillation (KD) has been demonstrated as an effective framework for compressing such big models. However, large-scale neural network systems are prone to memorize training instances, and thus tend to make inconsistent predictions when the data distribution is altered slightly. Moreover, the student model has few opportunities to request useful information from the teacher model when there is limited task-specific data available. To address these issues, we propose MixKD, a data-agnostic distillation framework that leverages mixup, a simple yet efficient data augmentation approach, to endow the resulting model with stronger generalization ability. Concretely, in addition to the original training examples, the student model is encouraged to mimic the teacher's behavior on the linear interpolation of example pairs as well. We prove from a theoretical perspective that under reasonable conditions MixKD gives rise to a smaller gap between the generalization error and the empirical error. To verify its effectiveness, we conduct experiments on the GLUE benchmark, where MixKD consistently leads to significant gains over the standard KD training, and outperforms several competitive baselines. Experiments under a limited-data setting and ablation studies further demonstrate the advantages of the proposed approach",
    "checked": true,
    "id": "e8f6eb89897c5880a99748f23c3d3763346eea45",
    "semantic_title": "mixkd: towards efficient distillation of large-scale language models",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=h2EbJ4_wMVq": {
    "title": "CaPC Learning: Confidential and Private Collaborative Learning",
    "volume": "poster",
    "abstract": "Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties",
    "checked": true,
    "id": "12bc3df669b64666c9fac71e918c9761f6ed5b71",
    "semantic_title": "capc learning: confidential and private collaborative learning",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=OmtmcPkkhT": {
    "title": "Multiplicative Filter Networks",
    "volume": "poster",
    "abstract": "Although deep networks are typically used to approximate functions over high dimensional inputs, recent work has increased interest in neural networks as function approximators for low-dimensional-but-complex functions, such as representing images as a function of pixel coordinates, solving differential equations, or representing signed distance fields or neural radiance fields. Key to these recent successes has been the use of new elements such as sinusoidal nonlinearities, or Fourier features in positional encodings, which vastly outperform simple ReLU networks. In this paper, we propose and empirically demonstrate that an arguably simpler class of function approximators can work just as well for such problems: multiplicative filter networks. In these networks, we avoid traditional compositional depth altogether, and simply multiply together (linear functions of) sinusoidal or Gabor wavelet functions applied to the input. This representation has the notable advantage that the entire function can simply be viewed as a linear function approximator over an exponential number of Fourier or Gabor basis functions, respectively. Despite this simplicity, when compared to recent approaches that use Fourier features with ReLU networks or sinusoidal activation networks, we show that these multiplicative filter networks largely outperform or match the performance of these recent approaches on the domains highlighted in these past works",
    "checked": true,
    "id": "4a1c3bf359b9b654a23f340f28c9b73c8050371c",
    "semantic_title": "multiplicative filter networks",
    "citation_count": 156,
    "authors": []
  },
  "https://openreview.net/forum?id=V6BjBgku7Ro": {
    "title": "Planning from Pixels using Inverse Dynamics Models",
    "volume": "poster",
    "abstract": "Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches",
    "checked": true,
    "id": "0bf1a78aeefa158a80b23fd5b57a5586a32eb7c1",
    "semantic_title": "planning from pixels using inverse dynamics models",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=yFJ67zTeI2": {
    "title": "Semi-supervised Keypoint Localization",
    "volume": "poster",
    "abstract": "Knowledge about the locations of keypoints of an object in an image can assist in fine-grained classification and identification tasks, particularly for the case of objects that exhibit large variations in poses that greatly influence their visual appearance, such as wild animals. However, supervised training of a keypoint detection network requires annotating a large image dataset for each animal species, which is a labor-intensive task. To reduce the need for labeled data, we propose to learn simultaneously keypoint heatmaps and pose invariant keypoint representations in a semi-supervised manner using a small set of labeled images along with a larger set of unlabeled images. Keypoint representations are learnt with a semantic keypoint consistency constraint that forces the keypoint detection network to learn similar features for the same keypoint across the dataset. Pose invariance is achieved by making keypoint representations for the image and its augmented copies closer together in feature space. Our semi-supervised approach significantly outperforms previous methods on several benchmarks for human and animal body landmark localization",
    "checked": true,
    "id": "ed8a5583f874c19fed7759ee51675e41c53e7438",
    "semantic_title": "semi-supervised keypoint localization",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=d8Q1mt2Ghw": {
    "title": "Emergent Road Rules In Multi-Agent Driving Environments",
    "volume": "poster",
    "abstract": "For autonomous vehicles to safely share the road with human drivers, autonomous vehicles must abide by specific \"road rules\" that human drivers have agreed to follow. \"Road rules\" include rules that drivers are required to follow by law – such as the requirement that vehicles stop at red lights – as well as more subtle social rules – such as the implicit designation of fast lanes on the highway. In this paper, we provide empirical evidence that suggests that – instead of hard-coding road rules into self-driving algorithms – a scalable alternative may be to design multi-agent environments in which road rules emerge as optimal solutions to the problem of maximizing traffic flow. We analyze what ingredients in driving environments cause the emergence of these road rules and find that two crucial factors are noisy perception and agents' spatial density. We provide qualitative and quantitative evidence of the emergence of seven social driving behaviors, ranging from obeying traffic signals to following lanes, all of which emerge from training agents to drive quickly to destinations without colliding. Our results add empirical support for the social road rules that countries worldwide have agreed on for safe, efficient driving",
    "checked": true,
    "id": "95978530dc134f879533efcda05f16f6d5d1c29a",
    "semantic_title": "emergent road rules in multi-agent driving environments",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=v5gjXpmR8J": {
    "title": "SSD: A Unified Framework for Self-Supervised Outlier Detection",
    "volume": "poster",
    "abstract": "We ask the following question: what training information is required to design an effective outlier/out-of-distribution (OOD) detector, i.e., detecting samples that lie far away from training distribution? Since unlabeled data is easily accessible for many applications, the most compelling approach is to develop detectors based on only unlabeled in-distribution data. However, we observe that most existing detectors based on unlabeled data perform poorly, often equivalent to a random prediction. In contrast, existing state-of-the-art OOD detectors achieve impressive performance but require access to fine-grained data labels for supervised training. We propose SSD, an outlier detector based on only unlabeled in-distribution data. We use self-supervised representation learning followed by a Mahalanobis distance based detection in the feature space. We demonstrate that SSD outperforms most existing detectors based on unlabeled data by a large margin. Additionally, SSD even achieves performance on par, and sometimes even better, with supervised training based detectors. Finally, we expand our detection framework with two key extensions. First, we formulate few-shot OOD detection, in which the detector has access to only one to five samples from each class of the targeted OOD dataset. Second, we extend our framework to incorporate training data labels, if available. We find that our novel detection framework based on SSD displays enhanced performance with these extensions, and achieves state-of-the-art performance. Our code is publicly available at https://github.com/inspire-group/SSD",
    "checked": true,
    "id": "0fe615dc0a422100e85cfb7e26c9306c481f6c75",
    "semantic_title": "ssd: a unified framework for self-supervised outlier detection",
    "citation_count": 356,
    "authors": []
  },
  "https://openreview.net/forum?id=VbLH04pRA3": {
    "title": "ECONOMIC HYPERPARAMETER OPTIMIZATION WITH BLENDED SEARCH STRATEGY",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "8e4127163441ac9e5e7467eca0980a4e0eb693d7",
    "semantic_title": "optimization of convolutional neural network hyperparameters for automatic classification of adult mosquitoes",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=KYPz4YsCPj": {
    "title": "Inductive Representation Learning in Temporal Networks via Causal Anonymous Walks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b67d88a20c82c635fdeb9e7e39b7f24a4991be6f",
    "semantic_title": "inductive representation learning in temporal networks via causal anonymous walks",
    "citation_count": 255,
    "authors": []
  },
  "https://openreview.net/forum?id=qZzy5urZw9": {
    "title": "Robust Overfitting may be mitigated by properly learned smoothening",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e3f41f4b6b4e3ab740176f022bcad522ad4c38ec",
    "semantic_title": "robust overfitting may be mitigated by properly learned smoothening",
    "citation_count": 190,
    "authors": []
  },
  "https://openreview.net/forum?id=tH6_VWZjoq": {
    "title": "Local Search Algorithms for Rank-Constrained Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "39dce697171b3f03575de54e970e48b50c96bbb9",
    "semantic_title": "local search algorithms for rank-constrained convex optimization",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=vcopnwZ7bC": {
    "title": "Learning Task Decomposition with Ordered Memory Policy Network",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0b33c826480ab88116bd33a6c21d9665e466ccad",
    "semantic_title": "learning task decomposition with ordered memory policy network",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=tYxG_OMs9WE": {
    "title": "Property Controllable Variational Autoencoder via Invertible Mutual Dependence",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1d8a6ab7db213d427098195a4ef08d45fd3dfc35",
    "semantic_title": "property controllable variational autoencoder via invertible mutual dependence",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=bhCDO_cEGCz": {
    "title": "Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7c399773001dc47de23038f4eb1f822bb6a11131",
    "semantic_title": "grounding physical concepts of objects and events through dynamic visual reasoning",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=c_E8kFWfhp0": {
    "title": "gradSim: Differentiable simulation for system identification and visuomotor control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "92c89db048fb825d2c81b086d7bd82ed230f685b",
    "semantic_title": "gradsim: differentiable simulation for system identification and visuomotor control",
    "citation_count": 109,
    "authors": []
  },
  "https://openreview.net/forum?id=RmcPm9m3tnk": {
    "title": "Generative Scene Graph Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b1a400b5c09a653da2269e459219ecc5705cba25",
    "semantic_title": "generative scene graph networks",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=_kxlwvhOodK": {
    "title": "Decentralized Attribution of Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "38805607d448927c8e7d5fb59e3ff58c0c1cc73e",
    "semantic_title": "decentralized attribution of generative models",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=71zCSP_HuBN": {
    "title": "Individually Fair Rankings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "eb083948b91bbbc0e4bb80dd39883edbd1af64f1",
    "semantic_title": "individually fair rankings",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=LkFG3lB13U5": {
    "title": "Adaptive Federated Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "47c528344fedb6cb67a38e43d095b41c34715330",
    "semantic_title": "adaptive federated optimization",
    "citation_count": 1521,
    "authors": []
  },
  "https://openreview.net/forum?id=1AoMhc_9jER": {
    "title": "GANs Can Play Lottery Tickets Too",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "53c3ac3d3f454bf355fb1a1931e40da2fe1d650e",
    "semantic_title": "gans can play lottery tickets too",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=DiQD7FWL233": {
    "title": "Improving Relational Regularized Autoencoders with Spherical Sliced Fused Gromov Wasserstein",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "129fed5e7c229b20efb9a0a07a4f7b403e147eb8",
    "semantic_title": "improving relational regularized autoencoders with spherical sliced fused gromov wasserstein",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=hPWj1qduVw8": {
    "title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "608605f307363796dd959507b4bb96a777fda8cf",
    "semantic_title": "learning reasoning paths over semantic graphs for video-grounded dialogues",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=Z4R1vxLbRLO": {
    "title": "Extreme Memorization via Scale of Initialization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "18c330bc388a53630074ec0473bfbdb460dabeb0",
    "semantic_title": "extreme memorization via scale of initialization",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=4RbdgBh9gE": {
    "title": "Teaching with Commentaries",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1ac1d3eb086c42d72a0509a42f744f626e8b5711",
    "semantic_title": "teaching with commentaries",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=-ODN6SbiUU": {
    "title": "In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a21792db1c8d80c1d1f8525dab4959cc60b8e0ea",
    "semantic_title": "in defense of pseudo-labeling: an uncertainty-aware pseudo-label selection framework for semi-supervised learning",
    "citation_count": 545,
    "authors": []
  },
  "https://openreview.net/forum?id=MDsQkFP1Aw": {
    "title": "Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of On-Screen Sounds",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "04e4d11bda6da95070e4c406b0400ea00161c6d2",
    "semantic_title": "into the wild with audioscope: unsupervised audio-visual separation of on-screen sounds",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=bjkX6Kzb5H": {
    "title": "Cut out the annotator, keep the cutout: better segmentation with weak supervision",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "641f26f1f1656e346ff6dc6e5d07428148b34b44",
    "semantic_title": "cut out the annotator, keep the cutout: better segmentation with weak supervision",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=Ozk9MrX1hvA": {
    "title": "CoDA: Contrast-enhanced and Diversity-promoting Data Augmentation for Natural Language Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "77f08ec1fc1a26d5e2c493be06a305d1480ad1c0",
    "semantic_title": "coda: contrast-enhanced and diversity-promoting data augmentation for natural language understanding",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=EQfpYwF3-b": {
    "title": "Deep Learning meets Projective Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a80029ac0fd89bd0296d3d1f440ad8247f027b72",
    "semantic_title": "deep learning meets projective clustering",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=b7g3_ZMHnT0": {
    "title": "Learning to Deceive Knowledge Graph Augmented Models via Targeted Perturbation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1076e085c973ff4eacf7a840cc09191d7e7ed3ed",
    "semantic_title": "learning to deceive knowledge graph augmented models via targeted perturbation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m4UCf24r0Y": {
    "title": "Knowledge Distillation as Semiparametric Inference",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6dde2751467e286e72cf6a49ae90e8bbdcd3e91f",
    "semantic_title": "knowledge distillation as semiparametric inference",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=Ti87Pv5Oc8": {
    "title": "Meta-Learning with Neural Tangent Kernels",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "86e1e0fbb4388d7216973c6385428c11a505e7b5",
    "semantic_title": "meta-learning with neural tangent kernels",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=9r30XCjf5Dt": {
    "title": "Vulnerability-Aware Poisoning Mechanism for Online RL with Unknown Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9f00a4f3b0628f861e1e7b929db0befd564d7bae",
    "semantic_title": "vulnerability-aware poisoning mechanism for online rl with unknown dynamics",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=ZTFeSBIX9C": {
    "title": "Understanding and Improving Lexical Choice in Non-Autoregressive Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "24a1767f6731abaeb21f8fa745b7e02fd4bbf39f",
    "semantic_title": "understanding and improving lexical choice in non-autoregressive translation",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=H6ATjJ0TKdf": {
    "title": "Layer-adaptive Sparsity for the Magnitude-based Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9227d5897abbf297a34d447e94a802a714b8eab2",
    "semantic_title": "layer-adaptive sparsity for the magnitude-based pruning",
    "citation_count": 214,
    "authors": []
  },
  "https://openreview.net/forum?id=n1HD8M6WGn": {
    "title": "Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9ee27d4df0a0e7032c520cb5f26567fb06769ad1",
    "semantic_title": "understanding and improving encoder layer fusion in sequence-to-sequence learning",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=-M0QkvBGTTq": {
    "title": "SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "29e34b721a72357ab83717643c1566f977e6a761",
    "semantic_title": "saliencymix: a saliency guided data augmentation strategy for better regularization",
    "citation_count": 233,
    "authors": []
  },
  "https://openreview.net/forum?id=_zx8Oka09eF": {
    "title": "Are wider nets better given the same number of parameters?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9e104d440540d2ffc9caaa0952a9e5f7f9344ba9",
    "semantic_title": "are wider nets better given the same number of parameters?",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=jP1vTH3inC": {
    "title": "Discovering Non-monotonic Autoregressive Orderings with Variational Inference",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "063eee315e864f0842d3074629dccc4bb36d19e7",
    "semantic_title": "discovering non-monotonic autoregressive orderings with variational inference",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=IgIk8RRT-Z": {
    "title": "CompOFA – Compound Once-For-All Networks for Faster Multi-Platform Deployment",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "17820d356a6441183c954d1b0d5ca5ceb1ff0045",
    "semantic_title": "compofa: compound once-for-all networks for faster multi-platform deployment",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=mCtadqIxOJ": {
    "title": "Representing Partial Programs with Blended Abstract Semantics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4116e65ef8a05c82c0fd739d98ca72e50802cf83",
    "semantic_title": "representing partial programs with blended abstract semantics",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=TYXs_y84xRj": {
    "title": "PolarNet: Learning to Optimize Polar Keypoints for Keypoint Based Object Detection",
    "volume": "poster",
    "abstract": "A variety of anchor-free object detectors have been actively proposed as possible alternatives to the mainstream anchor-based detectors that often rely on complicated design of anchor boxes. Despite achieving promising performance on par with anchor-based detectors, the existing anchor-free detectors such as FCOS or CenterNet predict objects based on standard Cartesian coordinates, which often yield poor quality keypoints. Further, the feature representation is also scale-sensitive. In this paper, we propose a new anchor-free keypoint based detector ``PolarNet\", where keypoints are represented as a set of Polar coordinates instead of Cartesian coordinates. The ``PolarNet\" detector learns offsets pointing to the corners of objects in order to learn high quality keypoints. Additionally, PolarNet uses features of corner points to localize objects, making the localization scale-insensitive. Finally in our experiments, we show that PolarNet, an anchor-free detector, outperforms the existing anchor-free detectors, and it is able to achieve highly competitive result on COCO test-dev benchmark ($47.8\\%$ and $50.3\\%$ AP under the single-model single-scale and multi-scale testing) which is on par with the state-of-the-art two-stage anchor-based object detectors. The code and the models are available at https://github.com/XiongweiWu/PolarNetV1",
    "checked": true,
    "id": "58ace7a2cd38838e8b6c21a1789a18d7e1c5101f",
    "semantic_title": "polarnet: learning to optimize polar keypoints for keypoint based object detection",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=Cnon5ezMHtu": {
    "title": "Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective",
    "volume": "poster",
    "abstract": "Neural Architecture Search (NAS) has been explosively studied to automate the discovery of top-performer neural networks. Current works require heavy training of supernet or intensive architecture evaluations, thus suffering from heavy resource consumption and often incurring search bias due to truncated training or approximations. Can we select the best neural architectures without involving any training and eliminate a drastic portion of the search cost? We provide an affirmative answer, by proposing a novel framework called \\textit{training-free neural architecture search} ($\\textbf{TE-NAS}$). TE-NAS ranks architectures by analyzing the spectrum of the neural tangent kernel (NTK), and the number of linear regions in the input space. Both are motivated by recent theory advances in deep networks, and can be computed without any training. We show that: (1) these two measurements imply the $\\textit{trainability}$ and $\\textit{expressivity}$ of a neural network; and (2) they strongly correlate with the network's actual test accuracy. Further on, we design a pruning-based NAS mechanism to achieve a more flexible and superior trade-off between the trainability and expressivity during the search. In NAS-Bench-201 and DARTS search spaces, TE-NAS completes high-quality search but only costs $\\textbf{0.5}$ and $\\textbf{4}$ GPU hours with one 1080Ti on CIFAR-10 and ImageNet, respectively. We hope our work to inspire more attempts in bridging between the theoretic findings of deep networks and practical impacts in real NAS applications",
    "checked": true,
    "id": "8d84c38f5fce1bd1b4ae1d55400c8fb7fa5d19c8",
    "semantic_title": "neural architecture search on imagenet in four gpu hours: a theoretically inspired perspective",
    "citation_count": 247,
    "authors": []
  },
  "https://openreview.net/forum?id=8qDwejCuCN": {
    "title": "Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding",
    "volume": "poster",
    "abstract": "Time series are often complex and rich in information but sparsely labeled and therefore challenging to model. In this paper, we propose a self-supervised framework for learning robust and generalizable representations for time series. Our approach, called Temporal Neighborhood Coding (TNC), takes advantage of the local smoothness of a signal's generative process to define neighborhoods in time with stationary properties. Using a debiased contrastive objective, our framework learns time series representations by ensuring that in the encoding space, the distribution of signals from within a neighborhood is distinguishable from the distribution of non-neighboring signals. Our motivation stems from the medical field, where the ability to model the dynamic nature of time series data is especially valuable for identifying, tracking, and predicting the underlying patients' latent states in settings where labeling data is practically impossible. We compare our method to recently developed unsupervised representation learning approaches and demonstrate superior performance on clustering and classification tasks for multiple datasets",
    "checked": true,
    "id": "e19ca3c11fd45dfc9bc6e43ddf9b03b6c798e66d",
    "semantic_title": "unsupervised representation learning for time series with temporal neighborhood coding",
    "citation_count": 309,
    "authors": []
  },
  "https://openreview.net/forum?id=KJNcAkY8tY4": {
    "title": "Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth",
    "volume": "poster",
    "abstract": "A key factor in the success of deep neural networks is the ability to scale models to improve performance by varying the architecture depth and width. This simple property of neural network design has resulted in highly effective architectures for a variety of tasks. Nevertheless, there is limited understanding of effects of depth and width on the learned representations. In this paper, we study this fundamental question. We begin by investigating how varying depth and width affects model hidden representations, finding a characteristic block structure in the hidden representations of larger capacity (wider or deeper) models. We demonstrate that this block structure arises when model capacity is large relative to the size of the training set, and is indicative of the underlying layers preserving and propagating the dominant principal component of their representations. This discovery has important ramifications for features learned by different models, namely, representations outside the block structure are often similar across architectures with varying widths and depths, but the block structure is unique to each model. We analyze the output predictions of different model architectures, finding that even when the overall accuracy is similar, wide and deep models exhibit distinctive error patterns and variations across classes",
    "checked": true,
    "id": "d21806115a79c960298cfca45a49b24682cac71a",
    "semantic_title": "do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth",
    "citation_count": 286,
    "authors": []
  },
  "https://openreview.net/forum?id=cu7IUiOhujH": {
    "title": "Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning",
    "volume": "poster",
    "abstract": "State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. However, the cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, our proposed SCL loss obtains significant improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in few-shot learning settings, without requiring specialized architecture, data augmentations, memory banks, or additional unsupervised data. Our proposed fine-tuning objective leads to models that are more robust to different levels of noise in the fine-tuning training data, and can generalize better to related tasks with limited labeled data",
    "checked": true,
    "id": "96c22a88ec3b9d3799daa41098555ab665c24ea8",
    "semantic_title": "supervised contrastive learning for pre-trained language model fine-tuning",
    "citation_count": 517,
    "authors": []
  },
  "https://openreview.net/forum?id=tlV90jvZbw": {
    "title": "Early Stopping in Deep Networks: Double Descent and How to Eliminate it",
    "volume": "poster",
    "abstract": "Over-parameterized models, such as large deep networks, often exhibit a double descent phenomenon, whereas a function of model size, error first decreases, increases, and decreases at last. This intriguing double descent behavior also occurs as a function of training epochs and has been conjectured to arise because training epochs control the model complexity. In this paper, we show that such epoch-wise double descent occurs for a different reason: It is caused by a superposition of two or more bias-variance tradeoffs that arise because different parts of the network are learned at different epochs, and mitigating this by proper scaling of stepsizes can significantly improve the early stopping performance. We show this analytically for i) linear regression, where differently scaled features give rise to a superposition of bias-variance tradeoffs, and for ii) a wide two-layer neural network, where the first and second layers govern bias-variance tradeoffs. Inspired by this theory, we study two standard convolutional networks empirically and show that eliminating epoch-wise double descent through adjusting stepsizes of different layers improves the early stopping performance",
    "checked": true,
    "id": "002614a13b02f331be5e739a5a4a31e19ca28a60",
    "semantic_title": "early stopping in deep networks: double descent and how to eliminate it",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=F8whUO8HNbP": {
    "title": "Contrastive Syn-to-Real Generalization",
    "volume": "poster",
    "abstract": "Training on synthetic data can be beneficial for label or data-scarce scenarios. However, synthetically trained models often suffer from poor generalization in real domains due to domain gaps. In this work, we make a key observation that the diversity of the learned feature embeddings plays an important role in the generalization performance. To this end, we propose contrastive synthetic-to-real generalization (CSG), a novel framework that leverage the pre-trained ImageNet knowledge to prevent overfitting to the synthetic domain, while promoting the diversity of feature embeddings as an inductive bias to improve generalization. In addition, we enhance the proposed CSG framework with attentional pooling (A-pool) to let the model focus on semantically important regions and further improve its generalization. We demonstrate the effectiveness of CSG on various synthetic training tasks, exhibiting state-of-the-art performance on zero-shot domain generalization",
    "checked": true,
    "id": "732f76b0a2aa5d255d60cb6e29257d9fb292a6f4",
    "semantic_title": "contrastive syn-to-real generalization",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=kWSeGEeHvF8": {
    "title": "Benchmarks for Deep Off-Policy Evaluation",
    "volume": "poster",
    "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area",
    "checked": true,
    "id": "d86dfdbb8eab91cf23b81c541b4f741f88b7d756",
    "semantic_title": "benchmarks for deep off-policy evaluation",
    "citation_count": 106,
    "authors": []
  },
  "https://openreview.net/forum?id=3k20LAiHYL2": {
    "title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense",
    "volume": "poster",
    "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective. Our proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin",
    "checked": true,
    "id": "abaadb4c6affc4d874c4f59bfac60686e851cb5e",
    "semantic_title": "pre-training text-to-text transformers for concept-centric common sense",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=8E1-f3VhX1o": {
    "title": "Combining Label Propagation and Simple Models out-performs Graph Neural Networks",
    "volume": "poster",
    "abstract": "Graph Neural Networks (GNNs) are a predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an \"error correlation\" that spreads residual errors in training data to correct errors in test data and (ii) a \"prediction correlation\" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C&S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques that have long been used in graph-based semi-supervised learning. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best-known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as is common in traditional methods) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains in some cases",
    "checked": true,
    "id": "f1e5e65941617604923225cc4bf464e370fcae67",
    "semantic_title": "combining label propagation and simple models out-performs graph neural networks",
    "citation_count": 288,
    "authors": []
  },
  "https://openreview.net/forum?id=_X_4Akcd8Re": {
    "title": "Learning Long-term Visual Dynamics with Region Proposal Interaction Networks",
    "volume": "poster",
    "abstract": "Learning long-term dynamics models is the key to understanding physical common sense. Most existing approaches on learning dynamics from visual input sidestep long-term predictions by resorting to rapid re-planning with short-term models. This not only requires such models to be super accurate but also limits them only to tasks where an agent can continuously obtain feedback and take action at each step until completion. In this paper, we aim to leverage the ideas from success stories in visual recognition tasks to build object representations that can capture inter-object and object-environment interactions over a long range. To this end, we propose Region Proposal Interaction Networks (RPIN), which reason about each object's trajectory in a latent region-proposal feature space. Thanks to the simple yet effective object representation, our approach outperforms prior methods by a significant margin both in terms of prediction quality and their ability to plan for downstream tasks, and also generalize well to novel environments. Code, pre-trained models, and more visualization results are available at https://haozhi.io/RPIN",
    "checked": true,
    "id": "4053a225b189852df0ef5d24bc5c400987778ac4",
    "semantic_title": "learning long-term visual dynamics with region proposal interaction networks",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=a3wKPZpGtCF": {
    "title": "Chaos of Learning Beyond Zero-sum and Coordination via Game Decompositions",
    "volume": "poster",
    "abstract": "It is of primary interest for ML to understand how agents learn and interact dynamically in competitive environments and games (e.g. GANs). But this has been a difficult task, as irregular behaviors are commonly observed in such systems. This can be explained theoretically, for instance, by the works of Cheung and Piliouras (COLT 2019; NeurIPS 2020), which showed that in two-person zero-sum games, if agents employ one of the most well-known learning algorithms, Multiplicative Weights Update (MWU), then Lyapunov chaos occurs everywhere in the payoff space. In this paper, we study how persistent chaos can occur in the more general normal game settings, where the agents might have the motivation to coordinate (which is not true for zero-sum games) and the number of agents can be arbitrary. We characterize bimatrix games where MWU, its optimistic variant (OMWU) or Follow-the-Regularized-Leader (FTRL) algorithms are Lyapunov chaotic almost everywhere in the payoff space. Technically, our characterization is derived by extending the volume-expansion argument of Cheung and Piliouras via the canonical game decomposition into zero-sum and coordination components. Interestingly, the two components induce opposite volume-changing behaviors, so the overall behavior can be analyzed by comparing the strengths of the components against each other. The comparison is done via our new notion of \"matrix domination\" or via a linear program. For multi-player games, we present a local equivalence of volume change between general games and graphical games, which is used to perform volume and chaos analyses of MWU and OMWU in potential games",
    "checked": true,
    "id": "181bc029f499b4876f9903a9eac57df5f6be109c",
    "semantic_title": "chaos of learning beyond zero-sum and coordination via game decompositions",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=dgd4EJqsbW5": {
    "title": "Control-Aware Representations for Model-based Reinforcement Learning",
    "volume": "poster",
    "abstract": "A major challenge in modern reinforcement learning (RL) is efficient control of dynamical systems from high-dimensional sensory observations. Learning controllable embedding (LCE) is a promising approach that addresses this challenge by embedding the observations into a lower-dimensional latent space, estimating the latent dynamics, and utilizing it to perform control in the latent space. Two important questions in this area are how to learn a representation that is amenable to the control problem at hand, and how to achieve an end-to-end framework for representation learning and control. In this paper, we take a few steps towards addressing these questions. We first formulate a LCE model to learn representations that are suitable to be used by a policy iteration style algorithm in the latent space.We call this model control-aware representation learning(CARL). We derive a loss function and three implementations for CARL. In the offline implementation, we replace the locally-linear control algorithm (e.g., iLQR) used by the existing LCE methods with a RL algorithm, namely model-based soft actor-critic, and show that it results in significant improvement. In online CARL, we interleave representation learning and control, and demonstrate further gain in performance. Finally, we propose value-guided CARL, a variation in which we optimize a weighted version of the CARL loss function, where the weights depend on the TD-error of the current policy. We evaluate the proposed algorithms by extensive experiments on benchmark tasks and compare them with several LCE baselines",
    "checked": true,
    "id": "b50da2e0bf200bb481725d92e5e3c80f8273dacc",
    "semantic_title": "control-aware representations for model-based reinforcement learning",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=sRA5rLNpmQc": {
    "title": "Provably robust classification of adversarial examples with detection",
    "volume": "poster",
    "abstract": "Adversarial attacks against deep networks can be defended against either by building robust classifiers or, by creating classifiers that can \\emph{detect} the presence of adversarial perturbations. Although it may intuitively seem easier to simply detect attacks rather than build a robust classifier, this has not bourne out in practice even empirically, as most detection methods have subsequently been broken by adaptive attacks, thus necessitating \\emph{verifiable} performance for detection mechanisms. In this paper, we propose a new method for jointly training a provably robust classifier and detector. Specifically, we show that by introducing an additional \"abstain/detection\" into a classifier, we can modify existing certified defense mechanisms to allow the classifier to either robustly classify \\emph{or} detect adversarial attacks. We extend the common interval bound propagation (IBP) method for certified robustness under $\\ell_\\infty$ perturbations to account for our new robust objective, and show that the method outperforms traditional IBP used in isolation, especially for large perturbation sizes. Specifically, tests on MNIST and CIFAR-10 datasets exhibit promising results, for example with provable robust error less than $63.63\\%$ and $67.92\\%$, for $55.6\\%$ and $66.37\\%$ natural error, for $\\epsilon=8/255$ and $16/255$ on the CIFAR-10 dataset, respectively",
    "checked": true,
    "id": "73dd7ff60ba551fcf1b7b13fdf65ae29d6e2f37c",
    "semantic_title": "provably robust classification of adversarial examples with detection",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=_TM6rT7tXke": {
    "title": "Return-Based Contrastive Representation Learning for Reinforcement Learning",
    "volume": "poster",
    "abstract": "Recently, various auxiliary tasks have been proposed to accelerate representation learning and improve sample efficiency in deep reinforcement learning (RL). However, existing auxiliary tasks do not take the characteristics of RL problems into consideration and are unsupervised. By leveraging returns, the most important feedback signals in RL, we propose a novel auxiliary task that forces the learnt representations to discriminate state-action pairs with different returns. Our auxiliary loss is theoretically justified to learn representations that capture the structure of a new form of state-action abstraction, under which state-action pairs with similar return distributions are aggregated together. Empirically, our algorithm outperforms strong baselines on complex tasks in Atari games and DeepMind Control suite, and achieves even better performance when combined with existing auxiliary tasks",
    "checked": true,
    "id": "f37a60d5c352b322e0a1a1e852e980a1e9e903b3",
    "semantic_title": "return-based contrastive representation learning for reinforcement learning",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=ijJZbomCJIm": {
    "title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification",
    "volume": "poster",
    "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better",
    "checked": true,
    "id": "eba3ad34d897c6c6a9c875585f05f93d0db8ec15",
    "semantic_title": "adversarially-trained deep nets transfer better: illustration on image classification",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=v9hAX77--cZ": {
    "title": "Learning Structural Edits via Incremental Tree Transformations",
    "volume": "poster",
    "abstract": "While most neural generative models generate outputs in a single pass, the human creative process is usually one of iterative building and refinement. Recent work has proposed models of editing processes, but these mostly focus on editing sequential data and/or only model a single editing pass. In this paper, we present a generic model for incremental editing of structured data (i.e. ''structural edits''). Particularly, we focus on tree-structured data, taking abstract syntax trees of computer programs as our canonical example. Our editor learns to iteratively generate tree edits (e.g. deleting or adding a subtree) and applies them to the partially edited data, thereby the entire editing process can be formulated as consecutive, incremental tree transformations. To show the unique benefits of modeling tree edits directly, we further propose a novel edit encoder for learning to represent edits, as well as an imitation learning method that allows the editor to be more robust. We evaluate our proposed editor on two source code edit datasets, where results show that, with the proposed edit encoder, our editor significantly improves accuracy over previous approaches that generate the edited program directly in one pass. Finally, we demonstrate that training our editor to imitate experts and correct its mistakes dynamically can further improve its performance",
    "checked": true,
    "id": "1756376bf7cf0d0a7bec881d663b57907a361ecf",
    "semantic_title": "learning structural edits via incremental tree transformations",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=hWr3e3r-oH5": {
    "title": "Cross-Attentional Audio-Visual Fusion for Weakly-Supervised Action Localization",
    "volume": "poster",
    "abstract": "Temporally localizing actions in videos is one of the key components for video understanding. Learning from weakly-labeled data is seen as a potential solution towards avoiding expensive frame-level annotations. Different from other works which only depend on visual-modality, we propose to learn richer audiovisual representation for weakly-supervised action localization. First, we propose a multi-stage cross-attention mechanism to collaboratively fuse audio and visual features, which preserves the intra-modal characteristics. Second, to model both foreground and background frames, we construct an open-max classifier that treats the background class as an open-set. Third, for precise action localization, we design consistency losses to enforce temporal continuity for the action class prediction, and also help with foreground-prediction reliability. Extensive experiments on two publicly available video-datasets (AVE and ActivityNet1.2) show that the proposed method effectively fuses audio and visual modalities, and achieves the state-of-the-art results for weakly-supervised action localization",
    "checked": true,
    "id": "da016a4ab022122635419d782b9513653bcd152a",
    "semantic_title": "cross-attentional audio-visual fusion for weakly-supervised action localization",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=BUlyHkzjgmA": {
    "title": "Improved Estimation of Concentration Under ℓ p -Norm Distance Metrics Using Half Spaces",
    "volume": "poster",
    "abstract": "Concentration of measure has been argued to be the fundamental cause of adversarial vulnerability. Mahloujifar et al. (2019) presented an empirical way to measure the concentration of a data distribution using samples, and employed it to find lower bounds on intrinsic robustness for several benchmark datasets. However, it remains unclear whether these lower bounds are tight enough to provide a useful approximation for the intrinsic robustness of a dataset. To gain a deeper understanding of the concentration of measure phenomenon, we first extend the Gaussian Isoperimetric Inequality to non-spherical Gaussian measures and arbitrary $\\ell_p$-norms ($p \\geq 2$). We leverage these theoretical insights to design a method that uses half-spaces to estimate the concentration of any empirical dataset under $\\ell_p$-norm distance metrics. Our proposed algorithm is more efficient than Mahloujifar et al. (2019)'s, and experiments on synthetic datasets and image benchmarks demonstrate that it is able to find much tighter intrinsic robustness bounds. These tighter estimates provide further evidence that rules out intrinsic dataset concentration as a possible explanation for the adversarial vulnerability of state-of-the-art classifiers",
    "checked": true,
    "id": "ba8d84deb3633076a135e6b885da609eb7b1c7e0",
    "semantic_title": "improved estimation of concentration under $\\ell_p$-norm distance metrics using half spaces",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=MyHwDabUHZm": {
    "title": "Beyond Categorical Label Representations for Image Classification",
    "volume": "poster",
    "abstract": "We find that the way we choose to represent data labels can have a profound effect on the quality of trained models. For example, training an image classifier to regress audio labels rather than traditional categorical probabilities produces a more reliable classification. This result is surprising, considering that audio labels are more complex than simpler numerical probabilities or text. We hypothesize that high dimensional, high entropy label representations are generally more useful because they provide a stronger error signal. We support this hypothesis with evidence from various label representations including constant matrices, spectrograms, shuffled spectrograms, Gaussian mixtures, and uniform random matrices of various dimensionalities. Our experiments reveal that high dimensional, high entropy labels achieve comparable accuracy to text (categorical) labels on standard image classification tasks, but features learned through our label representations exhibit more robustness under various adversarial attacks and better effectiveness with a limited amount of training data. These results suggest that label representation may play a more important role than previously thought",
    "checked": true,
    "id": "18d02ad0cf4bc9184a38aaacb72642b7a2d2bf18",
    "semantic_title": "beyond categorical label representations for image classification",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=JCRblSgs34Z": {
    "title": "Fantastic Four: Differentiable and Efficient Bounds on Singular Values of Convolution Layers",
    "volume": "poster",
    "abstract": "In deep neural networks, the spectral norm of the Jacobian of a layer bounds the factor by which the norm of a signal changes during forward/backward propagation. Spectral norm regularizations have been shown to improve generalization, robustness and optimization of deep learning methods. Existing methods to compute the spectral norm of convolution layers either rely on heuristics that are efficient in computation but lack guarantees or are theoretically-sound but computationally expensive. In this work, we obtain the best of both worlds by deriving {\\it four} provable upper bounds on the spectral norm of a standard 2D multi-channel convolution layer. These bounds are differentiable and can be computed efficiently during training with negligible overhead. One of these bounds is in fact the popular heuristic method of Miyato et al. (multiplied by a constant factor depending on filter sizes). Each of these four bounds can achieve the tightest gap depending on convolution filters. Thus, we propose to use the minimum of these four bounds as a tight, differentiable and efficient upper bound on the spectral norm of convolution layers. Moreover, our spectral bound is an effective regularizer and can be used to bound either the lipschitz constant or curvature values (eigenvalues of the Hessian) of neural networks. Through experiments on MNIST and CIFAR-10, we demonstrate the effectiveness of our spectral bound in improving generalization and robustness of deep networks",
    "checked": true,
    "id": "efb0782e46159be5184040811ab5a93fdf78a746",
    "semantic_title": "fantastic four: differentiable and efficient bounds on singular values of convolution layers",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=iOnhIy-a-0n": {
    "title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction",
    "volume": "poster",
    "abstract": "Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown promise in accelerating the convergence in non-convex learning; however, an excessively large correction for avoiding biases from noisy energy estimators has limited the potential of the acceleration. To address this issue, we study the variance reduction for noisy energy estimators, which promotes much more effective swaps. Theoretically, we provide a non-asymptotic analysis on the exponential convergence for the underlying continuous-time Markov jump process; moreover, we consider a generalized Girsanov theorem which includes the change of Poisson measure to overcome the crude discretization based on the Gr\\\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein ($\\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and obtain state-of-the-art results in optimization and uncertainty estimates for synthetic experiments and image data",
    "checked": true,
    "id": "45d17f397fe04cc25aeeee5e0872bcbf85e9aedc",
    "semantic_title": "accelerating convergence of replica exchange stochastic gradient mcmc via variance reduction",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=Pzj6fzU6wkj": {
    "title": "IsarStep: a Benchmark for High-level Mathematical Reasoning",
    "volume": "poster",
    "abstract": "A well-defined benchmark is essential for measuring and accelerating research progress of machine learning models. In this paper, we present a benchmark for high-level mathematical reasoning and study the reasoning capabilities of neural sequence-to-sequence models. We build a non-synthetic dataset from the largest repository of proofs written by human experts in a theorem prover. The dataset has a broad coverage of undergraduate and research-level mathematical and computer science theorems. In our defined task, a model is required to fill in a missing intermediate proposition given surrounding proofs. This task provides a starting point for the long-term goal of having machines generate human-readable proofs automatically. Our experiments and analysis reveal that while the task is challenging, neural models can capture non-trivial mathematical reasoning. We further design a hierarchical transformer that outperforms the transformer baseline",
    "checked": true,
    "id": "593499b654360101682edec1dd711fa7c09f6971",
    "semantic_title": "isarstep: a benchmark for high-level mathematical reasoning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=VVdmjgu7pKM": {
    "title": "Factorizing Declarative and Procedural Knowledge in Structured, Dynamical Environments",
    "volume": "poster",
    "abstract": "Modeling a structured, dynamic environment like a video game requires keeping track of the objects and their states (declarative knowledge) as well as predicting how objects behave (procedural knowledge). Black-box models with a monolithic hidden state often fail to apply procedural knowledge consistently and uniformly, i.e., they lack systematicity. For example, in a video game, correct prediction of one enemy's trajectory does not ensure correct prediction of another's. We address this issue via an architecture that factorizes declarative and procedural knowledge and that imposes modularity within each form of knowledge. The architecture consists of active modules called object files that maintain the state of a single object and invoke passive external knowledge sources called schemata that prescribe state updates. To use a video game as an illustration, two enemies of the same type will share schemata but will have separate object files to encode their distinct state (e.g., health, position). We propose to use attention to determine which object files to update, the selection of schemata, and the propagation of information between object files. The resulting architecture is a drop-in replacement conforming to the same input-output interface as normal recurrent networks (e.g., LSTM, GRU) yet achieves substantially better generalization on environments that have multiple object tokens of the same type, including a challenging intuitive physics benchmark",
    "checked": true,
    "id": "336f5ebe4a0e03697e87df9c5cb519fa4db39aa5",
    "semantic_title": "factorizing declarative and procedural knowledge in structured, dynamical environments",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=hx1IXFHAw7R": {
    "title": "Provable Rich Observation Reinforcement Learning with Combinatorial Latent States",
    "volume": "poster",
    "abstract": "We propose a novel setting for reinforcement learning that combines two common real-world difficulties: presence of observations (such as camera images) and factored states (such as location of objects). In our setting, the agent receives observations generated stochastically from a \"latent\" factored state. These observations are \"rich enough\" to enable decoding of the latent state and remove partial observability concerns. Since the latent state is combinatorial, the size of state space is exponential in the number of latent factors. We create a learning algorithm FactoRL (Fact-o-Rel) for this setting, which uses noise-contrastive learning to identify latent structures in emission processes and discover a factorized state space. We derive polynomial sample complexity guarantees for FactoRL which polynomially depend upon the number factors, and very weakly depend on the size of the observation space. We also provide a guarantee of polynomial time complexity when given access to an efficient planning algorithm",
    "checked": true,
    "id": "c2ebce87f856fada7ac735fdc9fe7549bfd5b583",
    "semantic_title": "provable rich observation reinforcement learning with combinatorial latent states",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=hJmtwocEqzc": {
    "title": "LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition",
    "volume": "poster",
    "abstract": "Facial recognition systems are increasingly deployed by private corporations, government agencies, and contractors for consumer services and mass surveillance programs alike. These systems are typically built by scraping social media profiles for user images. Adversarial perturbations have been proposed for bypassing facial recognition systems. However, existing methods fail on full-scale systems and commercial APIs. We develop our own adversarial filter that accounts for the entire image processing pipeline and is demonstrably effective against industrial-grade pipelines that include face detection and large scale databases. Additionally, we release an easy-to-use webtool that significantly degrades the accuracy of Amazon Rekognition and the Microsoft Azure Face Recognition API, reducing the accuracy of each to below 1%",
    "checked": true,
    "id": "73263d9c1ce3e0a105aa9d79d6a87d111d0465e3",
    "semantic_title": "lowkey: leveraging adversarial attacks to protect social media users from facial recognition",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=7t1FcJUWhi3": {
    "title": "Neural Networks for Learning Counterfactual G-Invariances from Single Environments",
    "volume": "poster",
    "abstract": "Despite —or maybe because of— their astonishing capacity to fit data, neural networks are believed to have difficulties extrapolating beyond training data distribution. This work shows that, for extrapolations based on finite transformation groups, a model's inability to extrapolate is unrelated to its capacity. Rather, the shortcoming is inherited from a learning hypothesis: Examples not explicitly observed with infinitely many training examples have underspecified outcomes in the learner's model. In order to endow neural networks with the ability to extrapolate over group transformations, we introduce a learning framework counterfactually-guided by the learning hypothesis that any group invariance to (known) transformation groups is mandatory even without evidence, unless the learner deems it inconsistent with the training data. Unlike existing invariance-driven methods for (counterfactual) extrapolations, this framework allows extrapolations from a single environment. Finally, we introduce sequence and image extrapolation tasks that validate our framework and showcase the shortcomings of traditional approaches",
    "checked": true,
    "id": "98602b3c90f0b6659faf974eb1c1144e42e6d042",
    "semantic_title": "neural networks for learning counterfactual g-invariances from single environments",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=CYO5T-YjWZV": {
    "title": "Simple Spectral Graph Convolution",
    "volume": "poster",
    "abstract": "Graph Convolutional Networks (GCNs) are leading methods for learning graph representations. However, without specially designed architectures, the performance of GCNs degrades quickly with increased depth. As the aggregated neighborhood size and neural network depth are two completely orthogonal aspects of graph representation, several methods focus on summarizing the neighborhood by aggregating K-hop neighborhoods of nodes while using shallow neural networks. However, these methods still encounter oversmoothing, and suffer from high computation and storage costs. In this paper, we use a modified Markov Diffusion Kernel to derive a variant of GCN called Simple Spectral Graph Convolution (SSGC). Our spectral analysis shows that our simple spectral graph convolution used in SSGC is a trade-off of low- and high-pass filter bands which capture the global and local contexts of each node. We provide two theoretical claims which demonstrate that we can aggregate over a sequence of increasingly larger neighborhoods compared to competitors while limiting severe oversmoothing. Our experimental evaluations show that SSGC with a linear learner is competitive in text and node classification tasks. Moreover, SSGC is comparable to other state-of-the-art methods for node clustering and community prediction tasks",
    "checked": true,
    "id": "69f1bb893516207b2a02bf3673edc46fb5cf1f94",
    "semantic_title": "simple spectral graph convolution",
    "citation_count": 304,
    "authors": []
  },
  "https://openreview.net/forum?id=LhY8QdUGSuw": {
    "title": "Anatomy of Catastrophic Forgetting: Hidden Representations and Task Semantics",
    "volume": "poster",
    "abstract": "Catastrophic forgetting is a recurring challenge to developing versatile deep learning models. Despite its ubiquity, there is limited understanding of its connections to neural network (hidden) representations and task semantics. In this paper, we address this important knowledge gap. Through quantitative analysis of neural representations, we find that deeper layers are disproportionately responsible for forgetting, with sequential training resulting in an erasure of earlier task representational subspaces. Methods to mitigate forgetting stabilize these deeper layers, but show diversity on precise effects, with some increasing feature reuse while others store task representations orthogonally, preventing interference. These insights also enable the development of an analytic argument and empirical picture relating forgetting to task semantic similarity, where we find that maximal forgetting occurs for task sequences with intermediate similarity",
    "checked": true,
    "id": "1dddfe2c8c3cce6ae7c18f7ecb89fbe664057269",
    "semantic_title": "anatomy of catastrophic forgetting: hidden representations and task semantics",
    "citation_count": 184,
    "authors": []
  },
  "https://openreview.net/forum?id=o81ZyBCojoA": {
    "title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning",
    "volume": "poster",
    "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\\textit{meta-initialization}$ of model parameters (that we call $\\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\\textit{how}$ robust regularization can $\\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning",
    "checked": true,
    "id": "118a605ad954c8f8e1ad65941429d0fd2c14c918",
    "semantic_title": "on fast adversarial robustness adaptation in model-agnostic meta-learning",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=42kiJ7n_8xO": {
    "title": "The geometry of integration in text classification RNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b895ec37d0bc703b155c617ebb9f3e61d826d237",
    "semantic_title": "the geometry of integration in text classification rnns",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=2AL06y9cDE-": {
    "title": "Towards Robust Neural Networks via Close-loop Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1a452afbaf41306dbc9cbeb5cdedb85b85eacb0f",
    "semantic_title": "towards robust neural networks via close-loop control",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=MBpHUFrcG2x": {
    "title": "Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3d04a6572e19642881dd9b20d070e8b33800e519",
    "semantic_title": "projected latent markov chain monte carlo: conditional sampling of normalizing flows",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=fSTD6NFIW_b": {
    "title": "Understanding the failure modes of out-of-distribution generalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1b4a54670bb4fe15bcb0d06de0391d5b6d10ace2",
    "semantic_title": "understanding the failure modes of out-of-distribution generalization",
    "citation_count": 181,
    "authors": []
  },
  "https://openreview.net/forum?id=p8agn6bmTbr": {
    "title": "Usable Information and Evolution of Optimal Representations During Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7a2628958ed2e3dd60e84d3ac380cda66109935e",
    "semantic_title": "usable information and evolution of optimal representations during training",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=R0a0kFI3dJx": {
    "title": "Adaptive Extra-Gradient Methods for Min-Max Optimization and Games",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "93d715bb46005880ebc9d32cb64344cf1697be07",
    "semantic_title": "adaptive extra-gradient methods for min-max optimization and games",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=OPyWRrcjVQw": {
    "title": "Shapley explainability on the data manifold",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4c35e7cb5731c6f281e98b524d4e922c9fb88b3b",
    "semantic_title": "shapley explainability on the data manifold",
    "citation_count": 104,
    "authors": []
  },
  "https://openreview.net/forum?id=QFYnKlBJYR": {
    "title": "Reinforcement Learning with Random Delays",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "10efeca4a3237504985963fff25ef34fb4808737",
    "semantic_title": "reinforcement learning with random delays",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=NcFEZOi-rLa": {
    "title": "Shape or Texture: Understanding Discriminative Features in CNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fa3d0599f8a082add349b5b09a208136489dae34",
    "semantic_title": "shape or texture: understanding discriminative features in cnns",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=Iw4ZGwenbXf": {
    "title": "NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "625052a8c9df7c69f0325e22a0b88ec080b2b7b0",
    "semantic_title": "non-convex optimization via adaptive stochastic search for end-to-end learning and control",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=Ovp8dvB8IBH": {
    "title": "Negative Data Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b4beb15b524c583cd828300605bab66dc3caf386",
    "semantic_title": "negative data augmentation",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=jHefDGsorp5": {
    "title": "Molecule Optimization by Explainable Evolution",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8b1f79bc91be1768b73e7df78cf113036f2a30b7",
    "semantic_title": "molecule optimization by explainable evolution",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=VcB4QkSfyO": {
    "title": "Estimating Lipschitz constants of monotone deep equilibrium models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ebc596f8f17c67923c1382916c2687f5d6d12c08",
    "semantic_title": "estimating lipschitz constants of monotone deep equilibrium models",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=3q5IqUrkcF": {
    "title": "Implicit Gradient Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "060eb1ad5da6a059320c244532ad5c9c0ab52485",
    "semantic_title": "implicit gradient regularization",
    "citation_count": 158,
    "authors": []
  },
  "https://openreview.net/forum?id=YCXrx6rRCXO": {
    "title": "Faster Binary Embeddings for Preserving Euclidean Distances",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "03c5afa7595fb7eba7ecd268889cf92355e47ba5",
    "semantic_title": "faster binary embeddings for preserving euclidean distances",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=23ZjUGpjcc": {
    "title": "Scalable Transfer Learning with Expert Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "beffe798cff58416e06372904de204dcf9fb9157",
    "semantic_title": "scalable transfer learning with expert models",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=lqU2cs3Zca": {
    "title": "Signatory: differentiable computations of the signature and logsignature transforms, on both CPU and GPU",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "eb284f2c2d8611e2a5cbfe9bdc6e070d700a6e0f",
    "semantic_title": "signatory: differentiable computations of the signature and logsignature transforms, on both cpu and gpu",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=IFqrg1p5Bc": {
    "title": "Distance-Based Regularisation of Deep Networks for Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3a807ae90aceba10b0d3150506084dc4fae69def",
    "semantic_title": "distance-based regularisation of deep networks for fine-tuning",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=iWLByfvUhN": {
    "title": "Decoupling Global and Local Representations via Invertible Generative Flows",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3e384108a9cb09acfaa96c38aa1502fbf08ab771",
    "semantic_title": "decoupling global and local representations via invertible generative flows",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=C3qvk5IQIJY": {
    "title": "Understanding Over-parameterization in Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "6f585a16bf7744f0788f803da0549bd555c87ea3",
    "semantic_title": "understanding estimation and generalization error of generative adversarial networks",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=A2gNouoXE7": {
    "title": "Filtered Inner Product Projection for Crosslingual Embedding Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d36ab43c91f29e3e4979a74fc74a03c18cb52953",
    "semantic_title": "filtered inner product projection for crosslingual embedding alignment",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=YUGG2tFuPM": {
    "title": "Deep Partition Aggregation: Provable Defenses against General Poisoning Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6cf1116dc8b431b2471bb5407d9a8be0eb067c2d",
    "semantic_title": "deep partition aggregation: provable defense against general poisoning attacks",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=9GBZBPn0Jx": {
    "title": "HalentNet: Multimodal Trajectory Forecasting with Hallucinative Intents",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "84b6c6923127e01f820636273bfc00b06c998f0b",
    "semantic_title": "halentnet: multimodal trajectory forecasting with hallucinative intents",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=O6LPudowNQm": {
    "title": "INT: An Inequality Benchmark for Evaluating Generalization in Theorem Proving",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "016863a86189c4e8ccecf9a36c4406c439a8a84c",
    "semantic_title": "int: an inequality benchmark for evaluating generalization in theorem proving",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=lgNx56yZh8a": {
    "title": "Bayesian Few-Shot Classification with One-vs-Each Pólya-Gamma Augmented Gaussian Processes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2aeada5a75c3e5a6eb2e1589d6cd5da06215af8b",
    "semantic_title": "bayesian few-shot classification with one-vs-each pólya-gamma augmented gaussian processes",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=l35SB-_raSQ": {
    "title": "A Hypergradient Approach to Robust Regression without Correspondence",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e01eeb3fc6e681cd2edac0be9f6ef6493b485574",
    "semantic_title": "a hypergradient approach to robust regression without correspondence",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=W3Wf_wKmqm9": {
    "title": "C-Learning: Horizon-Aware Cumulative Accessibility Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d6a02799e3b71f137b15ffb0a460cc7b3080ec56",
    "semantic_title": "c-learning: horizon-aware cumulative accessibility estimation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=W1G1JZEIy5_": {
    "title": "MIROSTAT: A NEURAL TEXT DECODING ALGORITHM THAT DIRECTLY CONTROLS PERPLEXITY",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e8f1d69cdf4d92350ce237e2d6a615ebe2e52e43",
    "semantic_title": "mirostat: a neural text decoding algorithm that directly controls perplexity",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=bB2drc7DPuB": {
    "title": "Global optimality of softmax policy gradient with single hidden layer neural networks in the mean-field regime",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "262851f264c6a0b10bad077289c52c4668156378",
    "semantic_title": "global optimality of softmax policy gradient with single hidden layer neural networks in the mean-field regime",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=guetrIHLFGI": {
    "title": "The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "339b5d3316d13062d936b335aab06e9da48a5c17",
    "semantic_title": "the deep bootstrap framework: good online learners are good offline generalizers",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=vYVI1CHPaQg": {
    "title": "A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0ffe2dde5dc78775bcf0c116661664845937b499",
    "semantic_title": "a better alternative to error feedback for communication-efficient distributed learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=9FWas6YbmB3": {
    "title": "DrNAS: Dirichlet Neural Architecture Search",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "11b430ae91c5b7a3e51f86235818f74fd8b449ef",
    "semantic_title": "drnas: dirichlet neural architecture search",
    "citation_count": 106,
    "authors": []
  },
  "https://openreview.net/forum?id=FmMKSO4e8JK": {
    "title": "Offline Model-Based Optimization via Normalized Maximum Likelihood Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3a6a35fe9871a702add7c9396267b64efc773234",
    "semantic_title": "offline model-based optimization via normalized maximum likelihood estimation",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=enoVQWLsfyL": {
    "title": "Viewmaker Networks: Learning Views for Unsupervised Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "600881eb8a24caee65633f796d5a7ef40c1cc9b0",
    "semantic_title": "viewmaker networks: learning views for unsupervised representation learning",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=vXj_ucZQ4hA": {
    "title": "Robust Pruning at Initialization",
    "volume": "poster",
    "abstract": "Overparameterized Neural Networks (NN) display state-of-the-art performance. However, there is a growing need for smaller, energy-efficient, neural networks to be able to use machine learning applications on devices with limited computational resources. A popular approach consists of using pruning techniques. While these techniques have traditionally focused on pruning pre-trained NN (LeCun et al.,1990; Hassibi et al., 1993), recent work by Lee et al. (2018) has shown promising results when pruning at initialization. However, for Deep NNs, such procedures remain unsatisfactory as the resulting pruned networks can be difficult to train and, for instance, they do not prevent one layer from being fully pruned. In this paper, we provide a comprehensive theoretical analysis of Magnitude and Gradient based pruning at initialization and training of sparse architectures. This allows us to propose novel principled approaches which we validate experimentally on a variety of NN architectures",
    "checked": true,
    "id": "6d60d0dd613fb1bc9b6a52f6b3e8b65599cade5a",
    "semantic_title": "robust pruning at initialization",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=CHLhSw9pSw8": {
    "title": "Single-Photon Image Classification",
    "volume": "poster",
    "abstract": "Quantum Computing based Machine Learning mainly focuses on quantum computing hardware that is experimentally challenging to realize due to requiring quantum gates that operate at very low temperature. We demonstrate the existence of a \"quantum computing toy model\" that illustrates key aspects of quantum information processing while being experimentally accessible with room temperature optics. Pondering the question of the theoretical classification accuracy performance limit for MNIST (respectively \"Fashion-MNIST\") classifiers, subject to the constraint that a decision has to be made after detection of the very first photon that passed through an image-filter, we show that a machine learning system that is permitted to use quantum interference on the photon's state can substantially outperform any machine learning system that can not. Specifically, we prove that a \"classical\" MNIST (respectively \"Fashion-MNIST\") classifier cannot achieve an accuracy of better than $21.28\\%$ (respectively $18.28\\%$ for \"Fashion-MNIST\") if it must make a decision after seeing a single photon falling on one of the $28\\times 28$ image pixels of a detector array. We further demonstrate that a classifier that is permitted to employ quantum interference by optically transforming the photon state prior to detection can achieve a classification accuracy of at least $41.27\\%$ for MNIST (respectively $36.14\\%$ for \"Fashion-MNIST\"). We show in detail how to train the corresponding quantum state transformation with TensorFlow and also explain how this example can serve as a teaching tool for the measurement process in quantum mechanics",
    "checked": true,
    "id": "e86099c24d7825fa36a484981eff11474debe3b3",
    "semantic_title": "single-photon image classification",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=xfmSoxdxFCG": {
    "title": "Can a Fruit Fly Learn Word Embeddings?",
    "volume": "poster",
    "abstract": "The mushroom body of the fruit fly brain is one of the best studied systems in neuroscience. At its core it consists of a population of Kenyon cells, which receive inputs from multiple sensory modalities. These cells are inhibited by the anterior paired lateral neuron, thus creating a sparse high dimensional representation of the inputs. In this work we study a mathematical formalization of this network motif and apply it to learning the correlational structure between words and their context in a corpus of unstructured text, a common natural language processing (NLP) task. We show that this network can learn semantic representations of words and can generate both static and context-dependent word embeddings. Unlike conventional methods (e.g., BERT, GloVe) that use dense representations for word embedding, our algorithm encodes semantic meaning of words and their context in the form of sparse binary hash codes. The quality of the learned representations is evaluated on word similarity analysis, word-sense disambiguation, and document classification. It is shown that not only can the fruit fly network motif achieve performance comparable to existing methods in NLP, but, additionally, it uses only a fraction of the computational resources (shorter training time and smaller memory footprint)",
    "checked": true,
    "id": "65d77663124f850bc5a87efb286cffd2e31ba8c1",
    "semantic_title": "can a fruit fly learn word embeddings?",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=ajOrOhQOsYx": {
    "title": "A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels",
    "volume": "poster",
    "abstract": "Group equivariant convolutional networks (GCNNs) endow classical convolutional networks with additional symmetry priors, which can lead to a considerably improved performance. Recent advances in the theoretical description of GCNNs revealed that such models can generally be understood as performing convolutions with $G$-steerable kernels, that is, kernels that satisfy an equivariance constraint themselves. While the $G$-steerability constraint has been derived, it has to date only been solved for specific use cases - a general characterization of $G$-steerable kernel spaces is still missing. This work provides such a characterization for the practically relevant case of $G$ being any compact group. Our investigation is motivated by a striking analogy between the constraints underlying steerable kernels on the one hand and spherical tensor operators from quantum mechanics on the other hand. By generalizing the famous Wigner-Eckart theorem for spherical tensor operators, we prove that steerable kernel spaces are fully understood and parameterized in terms of 1) generalized reduced matrix elements, 2) Clebsch-Gordan coefficients, and 3) harmonic basis functions on homogeneous spaces",
    "checked": true,
    "id": "7379045f1abe69ec6f89c53293ab1bb41fa156ec",
    "semantic_title": "a wigner-eckart theorem for group equivariant convolution kernels",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=dKg5D1Z1Lm": {
    "title": "Non-asymptotic Confidence Intervals of Off-policy Evaluation: Primal and Dual Bounds",
    "volume": "poster",
    "abstract": "Off-policy evaluation (OPE) is the task of estimating the expected reward of a given policy based on offline data previously collected under different policies. Therefore, OPE is a key step in applying reinforcement learning to real-world domains such as medical treatment, where interactive data collection is expensive or even unsafe. As the observed data tends to be noisy and limited, it is essential to provide rigorous uncertainty quantification, not just a point estimation, when applying OPE to make high stakes decisions. This work considers the problem of constructing non-asymptotic confidence intervals in infinite-horizon off-policy evaluation, which remains a challenging open question. We develop a practical algorithm through a primal-dual optimization-based approach, which leverages the kernel Bellman loss (KBL) of Feng et al. 2019 and a new martingale concentration inequality of KBL applicable to time-dependent data with unknown mixing conditions. Our algorithm makes minimum assumptions on the data and the function class of the Q-function, and works for the behavior-agnostic settings where the data is collected under a mix of arbitrary unknown behavior policies. We present empirical results that clearly demonstrate the advantages of our approach over existing methods",
    "checked": true,
    "id": "8990c62f00fc31c3dabdba049eb0a1d888c3ee40",
    "semantic_title": "non-asymptotic confidence intervals of off-policy evaluation: primal and dual bounds",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=9YlaeLfuhJF": {
    "title": "Model Patching: Closing the Subgroup Performance Gap with Data Augmentation",
    "volume": "poster",
    "abstract": "Classifiers in machine learning are often brittle when deployed. Particularly concerning are models with inconsistent performance on specific subgroups of a class, e.g., exhibiting disparities in skin cancer classification in the presence or absence of a spurious bandage. To mitigate these performance differences, we introduce model patching, a two-stage framework for improving robustness that encourages the model to be invariant to subgroup differences, and focus on class information shared by subgroups. Model patching first models subgroup features within a class and learns semantic transformations between them, and then trains a classifier with data augmentations that deliberately manipulate subgroup features. We instantiate model patching with CAMEL, which (1) uses a CycleGAN to learn the intra-class, inter-subgroup augmentations, and (2) balances subgroup performance using a theoretically-motivated subgroup consistency regularizer, accompanied by a new robust objective. We demonstrate CAMEL's effectiveness on 3 benchmark datasets, with reductions in robust error of up to 33% relative to the best baseline. Lastly, CAMEL successfully patches a model that fails due to spurious features on a real-world skin cancer dataset",
    "checked": true,
    "id": "9207480a5cd071a3e85f408082b09283413cbfa5",
    "semantic_title": "model patching: closing the subgroup performance gap with data augmentation",
    "citation_count": 122,
    "authors": []
  },
  "https://openreview.net/forum?id=fw-BHZ1KjxJ": {
    "title": "SOLAR: Sparse Orthogonal Learned and Random Embeddings",
    "volume": "poster",
    "abstract": "Dense embedding models are commonly deployed in commercial search engines, wherein all the document vectors are pre-computed, and near-neighbor search (NNS) is performed with the query vector to find relevant documents. However, the bottleneck of indexing a large number of dense vectors and performing an NNS hurts the query time and accuracy of these models. In this paper, we argue that high-dimensional and ultra-sparse embedding is a significantly superior alternative to dense low-dimensional embedding for both query efficiency and accuracy. Extreme sparsity eliminates the need for NNS by replacing them with simple lookups, while its high dimensionality ensures that the embeddings are informative even when sparse. However, learning extremely high dimensional embeddings leads to blow up in the model size. To make the training feasible, we propose a partitioning algorithm that learns such high dimensional embeddings across multiple GPUs without any communication. This is facilitated by our novel asymmetric mixture of Sparse, Orthogonal, Learned and Random (SOLAR) Embeddings. The label vectors are random, sparse, and near-orthogonal by design, while the query vectors are learned and sparse. We theoretically prove that our way of one-sided learning is equivalent to learning both query and label embeddings. With these unique properties, we can successfully train 500K dimensional SOLAR embeddings for the tasks of searching through 1.6M books and multi-label classification on the three largest public datasets. We achieve superior precision and recall compared to the respective state-of-the-art baselines for each task with up to 10 times faster speed",
    "checked": true,
    "id": "b2881852a6b072c731c0a54001d2908b488cd86e",
    "semantic_title": "solar: sparse orthogonal learned and random embeddings",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=snOgiCYZgJ7": {
    "title": "Neural representation and generation for RNA secondary structures",
    "volume": "poster",
    "abstract": "Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules",
    "checked": true,
    "id": "590b227307e0c33ac6128005234b783e8ec6584b",
    "semantic_title": "neural representation and generation for rna secondary structures",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=vVjIW3sEc1s": {
    "title": "A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks",
    "volume": "poster",
    "abstract": "Autoregressive language models, pretrained using large text corpora to do well on next word prediction, have been successful at solving many downstream tasks, even with zero-shot usage. However, there is little theoretical understanding of this success. This paper initiates a mathematical study of this phenomenon for the downstream task of text classification by considering the following questions: (1) What is the intuitive connection between the pretraining task of next word prediction and text classification? (2) How can we mathematically formalize this connection and quantify the benefit of language modeling? For (1), we hypothesize, and verify empirically, that classification tasks of interest can be reformulated as sentence completion tasks, thus making language modeling a meaningful pretraining task. With a mathematical formalization of this hypothesis, we make progress towards (2) and show that language models that are $\\epsilon$-optimal in cross-entropy (log-perplexity) learn features that can linearly solve such classification tasks with $\\mathcal{O}(\\sqrt{\\epsilon})$ error, thus demonstrating that doing well on language modeling can be beneficial for downstream tasks. We experimentally verify various assumptions and theoretical findings, and also use insights from the analysis to design a new objective function that performs well on some classification tasks",
    "checked": true,
    "id": "01400290c7db96c4d665d1c29519c42ba47401e0",
    "semantic_title": "a mathematical exploration of why language models help solve downstream tasks",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=gl3D-xY7wLq": {
    "title": "Noise or Signal: The Role of Image Backgrounds in Object Recognition",
    "volume": "poster",
    "abstract": "We assess the tendency of state-of-the-art object recognition models to depend on signals from image backgrounds. We create a toolkit for disentangling foreground and background signal on ImageNet images, and find that (a) models can achieve non-trivial accuracy by relying on the background alone, (b) models often misclassify images even in the presence of correctly classified foregrounds--up to 88% of the time with adversarially chosen backgrounds, and (c) more accurate models tend to depend on backgrounds less. Our analysis of backgrounds brings us closer to understanding which correlations machine learning models use, and how they determine models' out of distribution performance",
    "checked": true,
    "id": "5c63fc87400a4d3afea63ab8a068a47249f815c2",
    "semantic_title": "noise or signal: the role of image backgrounds in object recognition",
    "citation_count": 398,
    "authors": []
  },
  "https://openreview.net/forum?id=yUxUNaj2Sl": {
    "title": "Does enhanced shape bias improve neural network robustness to common corruptions?",
    "volume": "poster",
    "abstract": "Convolutional neural networks (CNNs) learn to extract representations of complex features, such as object shapes and textures to solve image recognition tasks. Recent work indicates that CNNs trained on ImageNet are biased towards features that encode textures and that these alone are sufficient to generalize to unseen test data from the same distribution as the training data but often fail to generalize to out-of-distribution data. It has been shown that augmenting the training data with different image styles decreases this texture bias in favor of increased shape bias while at the same time improving robustness to common corruptions, such as noise and blur. Commonly, this is interpreted as shape bias increasing corruption robustness. However, this relationship is only hypothesized. We perform a systematic study of different ways of composing inputs based on natural images, explicit edge information, and stylization. While stylization is essential for achieving high corruption robustness, we do not find a clear correlation between shape bias and robustness. We conclude that the data augmentation caused by style-variation accounts for the improved corruption robustness and increased shape bias is only a byproduct",
    "checked": true,
    "id": "3797437ea7990e99cbba6e94d402650a842ba738",
    "semantic_title": "does enhanced shape bias improve neural network robustness to common corruptions?",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=LXMSvPmsm0g": {
    "title": "Long Live the Lottery: The Existence of Winning Tickets in Lifelong Learning",
    "volume": "poster",
    "abstract": "The lottery ticket hypothesis states that a highly sparsified sub-network can be trained in isolation, given the appropriate weight initialization. This paper extends that hypothesis from one-shot task learning, and demonstrates for the first time that such extremely compact and independently trainable sub-networks can be also identified in the lifelong learning scenario, which we call lifelong tickets. We show that the resulting lifelong ticket can further be leveraged to improve the performance of learning over continual tasks. However, it is highly non-trivial to conduct network pruning in the lifelong setting. Two critical roadblocks arise: i) As many tasks now arrive sequentially, finding tickets in a greedy weight pruning fashion will inevitably suffer from the intrinsic bias, that the earlier emerging tasks impact more; ii) As lifelong learning is consistently challenged by catastrophic forgetting, the compact network capacity of tickets might amplify the risk of forgetting. In view of those, we introduce two pruning options, e.g., top-down and bottom-up, for finding lifelong tickets. Compared to the top-down pruning that extends vanilla (iterative) pruning over sequential tasks, we show that the bottom-up one, which can dynamically shrink and (re-)expand model capacity, effectively avoids the undesirable excessive pruning in the early stage. We additionally introduce lottery teaching that further overcomes forgetting via knowledge distillation aided by external unlabeled data. Unifying those ingredients, we demonstrate the existence of very competitive lifelong tickets, e.g., achieving 3-8% of the dense model size with even higher accuracy, compared to strong class-incremental learning baselines on CIFAR-10/CIFAR-100/Tiny-ImageNet datasets. Codes available at https://github.com/VITA-Group/Lifelong-Learning-LTH",
    "checked": true,
    "id": "05e2ca9357bcf542a33b3f97310d9f477cd0776f",
    "semantic_title": "long live the lottery: the existence of winning tickets in lifelong learning",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=MjvduJCsE4": {
    "title": "Exploring the Uncertainty Properties of Neural Networks' Implicit Priors in the Infinite-Width Limit",
    "volume": "poster",
    "abstract": "Modern deep learning models have achieved great success in predictive accuracy for many data modalities. However, their application to many real-world tasks is restricted by poor uncertainty estimates, such as overconfidence on out-of-distribution (OOD) data and ungraceful failing under distributional shift. Previous benchmarks have found that ensembles of neural networks (NNs) are typically the best calibrated models on OOD data. Inspired by this, we leverage recent theoretical advances that characterize the function-space prior of an infinitely-wide NN as a Gaussian process, termed the neural network Gaussian process (NNGP). We use the NNGP with a softmax link function to build a probabilistic model for multi-class classification and marginalize over the latent Gaussian outputs to sample from the posterior. This gives us a better understanding of the implicit prior NNs place on function space and allows a direct comparison of the calibration of the NNGP and its finite-width analogue. We also examine the calibration of previous approaches to classification with the NNGP, which treat classification problems as regression to the one-hot labels. In this case the Bayesian posterior is exact, and we compare several heuristics to generate a categorical distribution over classes. We find these methods are well calibrated under distributional shift. Finally, we consider an infinite-width final layer in conjunction with a pre-trained embedding. This replicates the important practical use case of transfer learning and allows scaling to significantly larger datasets. As well as achieving competitive predictive accuracy, this approach is better calibrated than its finite width analogue",
    "checked": true,
    "id": "7edf5e8e02301d3826d634faeda547b6cb28e8fe",
    "semantic_title": "exploring the uncertainty properties of neural networks' implicit priors in the infinite-width limit",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=TK_6nNb_C7q": {
    "title": "Hierarchical Autoregressive Modeling for Neural Video Compression",
    "volume": "poster",
    "abstract": "Recent work by Marino et al. (2020) showed improved performance in sequential density estimation by combining masked autoregressive flows with hierarchical latent variable models. We draw a connection between such autoregressive generative models and the task of lossy video compression. Specifically, we view recent neural video compression methods (Lu et al., 2019; Yang et al., 2020b; Agustssonet al., 2020) as instances of a generalized stochastic temporal autoregressive transform, and propose avenues for enhancement based on this insight. Comprehensive evaluations on large-scale video data show improved rate-distortion performance over both state-of-the-art neural and conventional video compression methods",
    "checked": true,
    "id": "8cd7485c4065832cfdfe63e8b53542463b7a3106",
    "semantic_title": "hierarchical autoregressive modeling for neural video compression",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=ujmgfuxSLrO": {
    "title": "DeLighT: Deep and Light-weight Transformer",
    "volume": "poster",
    "abstract": "We introduce a deep and light-weight transformer, DeLighT, that delivers similar or better performance than standard transformer-based models with significantly fewer parameters. DeLighT more efficiently allocates parameters both (1) within each Transformer block using the DeLighT transformation, a deep and light-weight transformation and (2) across blocks using block-wise scaling, that allows for shallower and narrower DeLighT blocks near the input and wider and deeper DeLighT blocks near the output. Overall, DeLighT networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on benchmark machine translation and language modeling tasks show that DeLighT matches or improves the performance of baseline Transformers with 2 to 3 times fewer parameters on average",
    "checked": true,
    "id": "b08c360ddf899923aebf25913706b4f03e54eccd",
    "semantic_title": "delight: deep and light-weight transformer",
    "citation_count": 86,
    "authors": []
  },
  "https://openreview.net/forum?id=nkIDwI6oO4_": {
    "title": "Learning A Minimax Optimizer: A Pilot Study",
    "volume": "poster",
    "abstract": "Solving continuous minimax optimization is of extensive practical interest, yet notoriously unstable and difficult. This paper introduces the learning to optimize(L2O) methodology to the minimax problems for the first time and addresses its accompanying unique challenges. We first present Twin-L2O, the first dedicated minimax L2O method consisting of two LSTMs for updating min and max variables separately. The decoupled design is found to facilitate learning, particularly when the min and max variables are highly asymmetric. Empirical experiments on a variety of minimax problems corroborate the effectiveness of Twin-L2O. We then discuss a crucial concern of Twin-L2O, i.e., its inevitably limited generalizability to unseen optimizees. To address this issue, we present two complementary strategies. Our first solution, Enhanced Twin-L2O, is empirically applicable for general minimax problems, by improving L2O training via leveraging curriculum learning. Our second alternative, called Safeguarded Twin-L2O, is a preliminary theoretical exploration stating that under some strong assumptions, it is possible to theoretically establish the convergence of Twin-L2O. We benchmark our algorithms on several testbed problems and compare against state-of-the-art minimax solvers. The code is available at: https://github.com/VITA-Group/L2O-Minimax",
    "checked": true,
    "id": "36f6f7eba02b03a65bd3005893ec1e488c726d8e",
    "semantic_title": "learning a minimax optimizer: a pilot study",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=zElset1Klrp": {
    "title": "Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online",
    "volume": "poster",
    "abstract": "Recent work has shown that sparse representations---where only a small percentage of units are active---can significantly reduce interference. Those works, however, relied on relatively complex regularization or meta-learning approaches, that have only been used offline in a pre-training phase. In this work, we pursue a direction that achieves sparsity by design, rather than by learning. Specifically, we design an activation function that produces sparse representations deterministically by construction, and so is more amenable to online training. The idea relies on the simple approach of binning, but overcomes the two key limitations of binning: zero gradients for the flat regions almost everywhere, and lost precision---reduced discrimination---due to coarse aggregation. We introduce a Fuzzy Tiling Activation (FTA) that provides non-negligible gradients and produces overlap between bins that improves discrimination. We first show that FTA is robust under covariate shift in a synthetic online supervised learning problem, where we can vary the level of correlation and drift. Then we move to the deep reinforcement learning setting and investigate both value-based and policy gradient algorithms that use neural networks with FTAs, in classic discrete control and Mujoco continuous control environments. We show that algorithms equipped with FTAs are able to learn a stable policy faster without needing target networks on most domains",
    "checked": true,
    "id": "c65467138600a36577045a8dac0cbe3287092f6b",
    "semantic_title": "fuzzy tiling activations: a simple approach to learning sparse representations online",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=dgtpE6gKjHn": {
    "title": "FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning",
    "volume": "poster",
    "abstract": "Federated learning aims to collaboratively train a strong global model by accessing users' locally trained models but not their own data. A crucial step is therefore to aggregate local models into a global model, which has been shown challenging when users have non-i.i.d. data. In this paper, we propose a novel aggregation algorithm named FedBE, which takes a Bayesian inference perspective by sampling higher-quality global models and combining them via Bayesian model Ensemble, leading to much robust aggregation. We show that an effective model distribution can be constructed by simply fitting a Gaussian or Dirichlet distribution to the local models. Our empirical studies validate FedBE's superior performance, especially when users' data are not i.i.d. and when the neural networks go deeper. Moreover, FedBE is compatible with recent efforts in regularizing users' model training, making it an easily applicable module: you only need to replace the aggregation method but leave other parts of your federated learning algorithm intact",
    "checked": true,
    "id": "60ba0a0c600df527b87525469dcde42f1925cdcc",
    "semantic_title": "fedbe: making bayesian model ensemble applicable to federated learning",
    "citation_count": 273,
    "authors": []
  },
  "https://openreview.net/forum?id=7uVcpu-gMD": {
    "title": "Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks",
    "volume": "poster",
    "abstract": "Neural networks (NNs) whose subnetworks implement reusable functions are expected to offer numerous advantages, including compositionality through efficient recombination of functional building blocks, interpretability, preventing catastrophic interference, etc. Understanding if and how NNs are modular could provide insights into how to improve them. Current inspection methods, however, fail to link modules to their functionality. In this paper, we present a novel method based on learning binary weight masks to identify individual weights and subnets responsible for specific functions. Using this powerful tool, we contribute an extensive study of emerging modularity in NNs that covers several standard architectures and datasets. We demonstrate how common NNs fail to reuse submodules and offer new insights into the related issue of systematic generalization on language tasks",
    "checked": true,
    "id": "649c758b0e59ddedaae37a3757e8eabdba664e5a",
    "semantic_title": "are neural nets modular? inspecting functional modularity through differentiable weight masks",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=-bxf89v3Nx": {
    "title": "Calibration tests beyond classification",
    "volume": "poster",
    "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression problems",
    "checked": true,
    "id": "b54d5ea54cfe0f0f8ccdcb564717f19d857f522f",
    "semantic_title": "calibration tests beyond classification",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=3jjmdp7Hha": {
    "title": "Meta Back-Translation",
    "volume": "poster",
    "abstract": "Back-translation is an effective strategy to improve the performance of Neural Machine Translation~(NMT) by generating pseudo-parallel data. However, several recent works have found that better translation quality in the pseudo-parallel data does not necessarily lead to a better final translation model, while lower-quality but diverse data often yields stronger results instead. In this paper we propose a new way to generate pseudo-parallel data for back-translation that directly optimizes the final model performance. Specifically, we propose a meta-learning framework where the back-translation model learns to match the forward-translation model's gradients on the development data with those on the pseudo-parallel data. In our evaluations in both the standard datasets WMT En-De'14 and WMT En-Fr'14, as well as a multilingual translation setting, our method leads to significant improvements over strong baselines",
    "checked": true,
    "id": "fcdac45272543b4f8b8eaa59d66044d1b7018494",
    "semantic_title": "meta back-translation",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=vujTf_I8Kmc": {
    "title": "Attentional Constellation Nets for Few-Shot Learning",
    "volume": "poster",
    "abstract": "The success of deep convolutional neural networks builds on top of the learning of effective convolution operations, capturing a hierarchy of structured features via filtering, activation, and pooling. However, the explicit structured features, e.g. object parts, are not expressive in the existing CNN frameworks. In this paper, we tackle the few-shot learning problem and make an effort to enhance structured features by expanding CNNs with a constellation model, which performs cell feature clustering and encoding with a dense part representation; the relationships among the cell features are further modeled by an attention mechanism. With the additional constellation branch to increase the awareness of object parts, our method is able to attain the advantages of the CNNs while making the overall internal representations more robust in the few-shot learning setting. Our approach attains a significant improvement over the existing methods in few-shot learning on the CIFAR-FS, FC100, and mini-ImageNet benchmarks",
    "checked": true,
    "id": "ab6413f67becd89dd7676c5cf5e260e13b42ddfd",
    "semantic_title": "attentional constellation nets for few-shot learning",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=jN5y-zb5Q7m": {
    "title": "Uncertainty Estimation in Autoregressive Structured Prediction",
    "volume": "poster",
    "abstract": "Uncertainty estimation is important for ensuring safety and robustness of AI systems. While most research in the area has focused on un-structured prediction tasks, limited work has investigated general uncertainty estimation approaches for structured prediction. Thus, this work aims to investigate uncertainty estimation for structured prediction tasks within a single unified and interpretable probabilistic ensemble-based framework. We consider: uncertainty estimation for sequence data at the token-level and complete sequence-level; interpretations for, and applications of, various measures of uncertainty; and discuss both the theoretical and practical challenges associated with obtaining them. This work also provides baselines for token-level and sequence-level error detection, and sequence-level out-of-domain input detection on the WMT'14 English-French and WMT'17 English-German translation and LibriSpeech speech recognition datasets",
    "checked": true,
    "id": "0921322cf6ea34d1852f13cb67eeac9d1f863518",
    "semantic_title": "uncertainty estimation in autoregressive structured prediction",
    "citation_count": 280,
    "authors": []
  },
  "https://openreview.net/forum?id=d7KBjmI3GmQ": {
    "title": "Measuring Massive Multitask Language Understanding",
    "volume": "poster",
    "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings",
    "checked": true,
    "id": "814a4f680b9ba6baba23b93499f4b48af1a27678",
    "semantic_title": "measuring massive multitask language understanding",
    "citation_count": 5048,
    "authors": []
  },
  "https://openreview.net/forum?id=ixpSxO9flk3": {
    "title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models",
    "volume": "poster",
    "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains",
    "checked": true,
    "id": "13b154ca78b245e3125ed67b90438ec50b859df3",
    "semantic_title": "no mcmc for me: amortized sampling for fast and stable training of energy-based models",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=dNy_RKzJacY": {
    "title": "Aligning AI With Shared Human Values",
    "volume": "poster",
    "abstract": "We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values",
    "checked": true,
    "id": "65906e6027246ae9e4ecd18d6e019a24505c842e",
    "semantic_title": "aligning ai with shared human values",
    "citation_count": 610,
    "authors": []
  },
  "https://openreview.net/forum?id=7pgFL2Dkyyy": {
    "title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning",
    "volume": "poster",
    "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem — continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm",
    "checked": true,
    "id": "2c1572ff4a708397405210f780006b7dd2274ec8",
    "semantic_title": "class normalization for (continual)? generalized zero-shot learning",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=hb1sDDSLbV": {
    "title": "Learning explanations that are hard to vary",
    "volume": "poster",
    "abstract": "In this paper, we investigate the principle that good explanations are hard to vary in the context of deep learning. We show that averaging gradients across examples -- akin to a logical OR of patterns -- can favor memorization and `patchwork' solutions that sew together different strategies, instead of identifying invariances. To inspect this, we first formalize a notion of consistency for minima of the loss surface, which measures to what extent a minimum appears only when examples are pooled. We then propose and experimentally validate a simple alternative algorithm based on a logical AND, that focuses on invariances and prevents memorization in a set of real-world tasks. Finally, using a synthetic dataset with a clear distinction between invariant and spurious mechanisms, we dissect learning signals and compare this approach to well-established regularizers",
    "checked": true,
    "id": "6972010d883e9746ecdf8147168caeccef6dcdb3",
    "semantic_title": "learning explanations that are hard to vary",
    "citation_count": 191,
    "authors": []
  },
  "https://openreview.net/forum?id=NSBrFgJAHg": {
    "title": "Degree-Quant: Quantization-Aware Training for Graph Neural Networks",
    "volume": "poster",
    "abstract": "Graph neural networks (GNNs) have demonstrated strong performance on a wide variety of tasks due to their ability to model non-uniform structured data. Despite their promise, there exists little research exploring methods to make them more efficient at inference time. In this work, we explore the viability of training quantized GNNs, enabling the usage of low precision integer arithmetic during inference. For GNNs seemingly unimportant choices in quantization implementation cause dramatic changes in performance. We identify the sources of error that uniquely arise when attempting to quantize GNNs, and propose an architecturally-agnostic and stable method, Degree-Quant, to improve performance over existing quantization-aware training baselines commonly used on other architectures, such as CNNs. We validate our method on six datasets and show, unlike previous quantization attempts, that models generalize to unseen graphs. Models trained with Degree-Quant for INT8 quantization perform as well as FP32 models in most cases; for INT4 models, we obtain up to 26% gains over the baselines. Our work enables up to 4.7x speedups on CPU when using INT8 arithmetic",
    "checked": true,
    "id": "9c0e855382de7e708c8eea7b4d5cf792bcd4a326",
    "semantic_title": "degree-quant: quantization-aware training for graph neural networks",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=PbEHqvFtcS": {
    "title": "Byzantine-Resilient Non-Convex Stochastic Gradient Descent",
    "volume": "poster",
    "abstract": "We study adversary-resilient stochastic distributed optimization, in which $m$ machines can independently compute stochastic gradients, and cooperate to jointly optimize over their local objective functions. However, an $\\alpha$-fraction of the machines are Byzantine, in that they may behave in arbitrary, adversarial ways. We consider a variant of this procedure in the challenging non-convex case. Our main result is a new algorithm SafeguardSGD, which can provably escape saddle points and find approximate local minima of the non-convex objective. The algorithm is based on a new concentration filtering technique, and its sample and time complexity bounds match the best known theoretical bounds in the stochastic, distributed setting when no Byzantine machines are present. Our algorithm is very practical: it improves upon the performance of all prior methods when training deep neural networks, it is relatively lightweight, and it is the first method to withstand two recently-proposed Byzantine attacks",
    "checked": true,
    "id": "029c33e39236f01e83a952c4203d86e07a6c1532",
    "semantic_title": "byzantine-resilient non-convex stochastic gradient descent",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=dOcQK-f4byz": {
    "title": "Teaching Temporal Logics to Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b32e631ae7779d93b9979c61c5b920a76342063e",
    "semantic_title": "teaching temporal logics to neural networks",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=7dpmlkBuJFC": {
    "title": "Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "465c52d9aa7b451a6ced3fafb377bac1b7da1ca1",
    "semantic_title": "bypassing the ambient dimension: private sgd with gradient subspace identification",
    "citation_count": 113,
    "authors": []
  },
  "https://openreview.net/forum?id=PpshD0AXfA": {
    "title": "Generative Time-series Modeling with Fourier Flows",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "da21bb9843e1bcd0c50a0652585d2906c70abb18",
    "semantic_title": "generative time-series modeling with fourier flows",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=jphnJNOwe36": {
    "title": "Overparameterisation and worst-case generalisation: friend or foe?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0aae10ade8fc9e58e177e034b794fce45c32fde8",
    "semantic_title": "overparameterisation and worst-case generalisation: friend or foe?",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=TgSVWXw22FQ": {
    "title": "Improving Zero-Shot Voice Style Transfer via Disentangled Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1e3476b4f6f8b85d7e1a891dbb94caf80ee5fe35",
    "semantic_title": "improving zero-shot voice style transfer via disentangled representation learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=60j5LygnmD": {
    "title": "Meta-learning with negative learning rates",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4454a763c891afb3fb8fa6567a367d05b1938e97",
    "semantic_title": "meta-learning with negative learning rates",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=PS3IMnScugk": {
    "title": "Learning to Recombine and Resample Data For Compositional Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5cc8ea815bd05be3b28519b489afe6de278a4209",
    "semantic_title": "learning to recombine and resample data for compositional generalization",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=4IwieFS44l": {
    "title": "Fooling a Complete Neural Network Verifier",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1f4229f0cafc65c2a5f8b7a7f0ae0fc2c79017b9",
    "semantic_title": "fooling a complete neural network verifier",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=BM---bH_RSh": {
    "title": "UMEC: Unified model and embedding compression for efficient recommendation systems",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "37cd69f55703f1bd3448016cfe30ee2e32252b27",
    "semantic_title": "umec: unified model and embedding compression for efficient recommendation systems",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=4dXmpCDGNp7": {
    "title": "Evaluations and Methods for Explanation through Robustness Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "20e0abdfed000269d3eabe1313223701030c2ed9",
    "semantic_title": "evaluations and methods for explanation through robustness analysis",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=HCSgyPUfeDj": {
    "title": "Learning and Evaluating Representations for Deep One-Class Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "498e003901f8287e89e5064477cd22dd47e49d61",
    "semantic_title": "learning and evaluating representations for deep one-class classification",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=v8b3e5jN66j": {
    "title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3a1dea12746651884fa1c4349aa71327d789ae6b",
    "semantic_title": "conditional negative sampling for contrastive learning of visual representations",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=qpsl2dR9twy": {
    "title": "Communication in Multi-Agent Reinforcement Learning: Intention Sharing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9ed9ff62749f450247d1f6be9d1c14142fa63f05",
    "semantic_title": "communication in multi-agent reinforcement learning: intention sharing",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=tkAtoZkcUnm": {
    "title": "Neural Thompson Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "60dad5057a90669907aa2dbdfe4d66a9fb05d2c4",
    "semantic_title": "batched thompson sampling",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=7R7fAoUygoa": {
    "title": "Optimal Regularization can Mitigate Double Descent",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4df60eaad8933ae16eb8744fe2cc7229fbe4879a",
    "semantic_title": "optimal regularization can mitigate double descent",
    "citation_count": 134,
    "authors": []
  },
  "https://openreview.net/forum?id=8HhkbjrWLdE": {
    "title": "Separation and Concentration in Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d13d7c36fa3e49ff70fef052756563f024a87f1b",
    "semantic_title": "separation and concentration in deep networks",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=rgFNuJHHXv": {
    "title": "Group Equivariant Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "67b3930ff102f7f4055eee61b01c94e371532c77",
    "semantic_title": "group equivariant generative adversarial networks",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=hpH98mK5Puk": {
    "title": "InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a79c6fd3da1c3eafc228a0e429846ff048027689",
    "semantic_title": "infobert: improving robustness of language models from an information theoretic perspective",
    "citation_count": 120,
    "authors": []
  },
  "https://openreview.net/forum?id=sjuuTm4vj0": {
    "title": "Using latent space regression to analyze and leverage compositionality in GANs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "16beac3066280e8895f8fa4b9ff66203e1a2f629",
    "semantic_title": "using latent space regression to analyze and leverage compositionality in gans",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=giit4HdDNa": {
    "title": "Go with the flow: Adaptive control for Neural ODEs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dccf736fe0884ebac4f9ad187cb8249b3122efe2",
    "semantic_title": "go with the flow: adaptive control for neural odes",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=7aL-OtQrBWD": {
    "title": "A Learning Theoretic Perspective on Local Explainability",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1df7fa194f3cc72ba6fb9541fe413defbaf4cfb4",
    "semantic_title": "a learning theoretic perspective on local explainability",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=XI-OJ5yyse": {
    "title": "CopulaGNN: Towards Integrating Representational and Correlational Roles of Graphs in Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7e2cffd6341939869a3e48c78e06c738d18f7d60",
    "semantic_title": "copulagnn: towards integrating representational and correlational roles of graphs in graph neural networks",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=jh-rTtvkGeM": {
    "title": "Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "026bb8a1066f50ddc8797e1341353603149a8cb8",
    "semantic_title": "gradient descent on neural networks typically occurs at the edge of stability",
    "citation_count": 294,
    "authors": []
  },
  "https://openreview.net/forum?id=SlrqM9_lyju": {
    "title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "76488b0743c9553b7b1d7ec46afe107ea60a67ca",
    "semantic_title": "autolrs: automatic learning-rate schedule by bayesian optimization on the fly",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=kmqjgSNXby": {
    "title": "Autoregressive Dynamics Models for Offline Policy Evaluation and Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7e687699b6c2e075091cebe4b7b8dbd4dc3a7406",
    "semantic_title": "autoregressive dynamics models for offline policy evaluation and optimization",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=IrM64DGB21": {
    "title": "On the role of planning in model-based deep reinforcement learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8b5bc6b5e42b2ea39e65cb14c06d8a819a03a496",
    "semantic_title": "on the role of planning in model-based deep reinforcement learning",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=0cmMMy8J5q": {
    "title": "Zero-Cost Proxies for Lightweight NAS",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6dc84d38f6d8d964456b127d6f45c5b4de73bb86",
    "semantic_title": "zero-cost proxies for lightweight nas",
    "citation_count": 267,
    "authors": []
  },
  "https://openreview.net/forum?id=ehJqJQk9cw": {
    "title": "Personalized Federated Learning with First Order Model Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "30dba214afa23aa38340d9035ebfdbd77a135411",
    "semantic_title": "personalized federated learning with first order model optimization",
    "citation_count": 321,
    "authors": []
  },
  "https://openreview.net/forum?id=d-XzF81Wg1": {
    "title": "Deconstructing the Regularization of BatchNorm",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5b4aae4cbeae599ecc5c91e7ef58a758389f695d",
    "semantic_title": "deconstructing the regularization of batchnorm",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=193sEnKY1ij": {
    "title": "No Cost Likelihood Manipulation at Test Time for Making Better Mistakes in Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "776321e72310f9a8fd37e9cb8ad27935366e08ba",
    "semantic_title": "no cost likelihood manipulation at test time for making better mistakes in deep networks",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=l0mSUROpwY": {
    "title": "Intrinsic-Extrinsic Convolution and Pooling for Learning on 3D Protein Structures",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "18939e167782868f9d5c63e1c7908c1bf70eb284",
    "semantic_title": "intrinsic-extrinsic convolution and pooling for learning on 3d protein structures",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=45uOPa46Kh": {
    "title": "Generative Language-Grounded Policy in Vision-and-Language Navigation with Bayes' Rule",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3435e193998ec4118f51bbb608a843b0e123661b",
    "semantic_title": "generative language-grounded policy in vision-and-language navigation with bayes' rule",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=90JprVrJBO": {
    "title": "Learning a Latent Search Space for Routing Problems using Variational Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "168526097c6c1e8a2010ce9d2ec434319a5fb948",
    "semantic_title": "learning a latent search space for routing problems using variational autoencoders",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=0oabwyZbOu": {
    "title": "Mastering Atari with Discrete World Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b44bb1762640ed72091fd5f5fdc20719a6dc24af",
    "semantic_title": "mastering atari with discrete world models",
    "citation_count": 918,
    "authors": []
  },
  "https://openreview.net/forum?id=I4c4K9vBNny": {
    "title": "Spatial Dependency Networks: Neural Layers for Improved Generative Image Modeling",
    "volume": "poster",
    "abstract": "How to improve generative modeling by better exploiting spatial regularities and coherence in images? We introduce a novel neural network for building image generators (decoders) and apply it to variational autoencoders (VAEs). In our spatial dependency networks (SDNs), feature maps at each level of a deep neural net are computed in a spatially coherent way, using a sequential gating-based mechanism that distributes contextual information across 2-D space. We show that augmenting the decoder of a hierarchical VAE by spatial dependency layers considerably improves density estimation over baseline convolutional architectures and the state-of-the-art among the models within the same class. Furthermore, we demonstrate that SDN can be applied to large images by synthesizing samples of high quality and coherence. In a vanilla VAE setting, we find that a powerful SDN decoder also improves learning disentangled representations, indicating that neural architectures play an important role in this task. Our results suggest favoring spatial dependency over convolutional layers in various VAE settings. The accompanying source code is given at https://github.com/djordjemila/sdn",
    "checked": true,
    "id": "0019f027ef77749d95fa2751185046302dc6b264",
    "semantic_title": "spatial dependency networks: neural layers for improved generative image modeling",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=uR9LaO_QxF": {
    "title": "Efficient Transformers in Reinforcement Learning using Actor-Learner Distillation",
    "volume": "poster",
    "abstract": "Many real-world applications such as robotics provide hard constraints on power and compute that limit the viable model complexity of Reinforcement Learning (RL) agents. Similarly, in many distributed RL settings, acting is done on un-accelerated hardware such as CPUs, which likewise restricts model size to prevent intractable experiment run times. These \"actor-latency\" constrained settings present a major obstruction to the scaling up of model complexity that has recently been extremely successful in supervised learning. To be able to utilize large model capacity while still operating within the limits imposed by the system during acting, we develop an \"Actor-Learner Distillation\" (ALD) procedure that leverages a continual form of distillation that transfers learning progress from a large capacity learner model to a small capacity actor model. As a case study, we develop this procedure in the context of partially-observable environments, where transformer models have had large improvements over LSTMs recently, at the cost of significantly higher computational complexity. With transformer models as the learner and LSTMs as the actor, we demonstrate in several challenging memory environments that using Actor-Learner Distillation largely recovers the clear sample-efficiency gains of the transformer learner model while maintaining the fast inference and reduced total training time of the LSTM actor model",
    "checked": true,
    "id": "cd37fee4da0d4483322d6fa3cc67af9ed8c07be6",
    "semantic_title": "efficient transformers in reinforcement learning using actor-learner distillation",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=ipUPfYxWZvM": {
    "title": "IOT: Instance-wise Layer Reordering for Transformer Structures",
    "volume": "poster",
    "abstract": "With sequentially stacked self-attention, (optional) encoder-decoder attention, and feed-forward layers, Transformer achieves big success in natural language processing (NLP), and many variants have been proposed. Currently, almost all these models assume that the \\emph{layer order} is fixed and kept the same across data samples. We observe that different data samples actually favor different orders of the layers. Based on this observation, in this work, we break the assumption of the fixed layer order in Transformer and introduce instance-wise layer reordering into model structure. Our Instance-wise Ordered Transformer (IOT) can model variant functions by reordered layers, which enables each sample to select the better one to improve the model performance under the constraint of almost same number of parameters. To achieve this, we introduce a light predictor with negligible parameter and inference cost to decide the most capable and favorable layer order for any input sequence. Experiments on $3$ tasks (neural machine translation, abstractive summarization, and code generation) and $9$ datasets demonstrate consistent improvements of our method. We further show that our method can also be applied to other architectures beyond Transformer. Our code is released at Github\\footnote{\\url{https://github.com/instance-wise-ordered-transformer/IOT}}",
    "checked": true,
    "id": "f77d9e0b14598df68d6cfb64c5dd70916ee29aea",
    "semantic_title": "iot: instance-wise layer reordering for transformer structures",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=GFsU8a0sGB": {
    "title": "Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms",
    "volume": "poster",
    "abstract": "Federated learning is typically approached as an optimization problem, where the goal is to minimize a global loss function by distributing computation across client devices that possess local data and specify different parts of the global objective. We present an alternative perspective and formulate federated learning as a posterior inference problem, where the goal is to infer a global posterior distribution by having client devices each infer the posterior of their local data. While exact inference is often intractable, this perspective provides a principled way to search for global optima in federated settings. Further, starting with the analysis of federated quadratic objectives, we develop a computation- and communication-efficient approximate posterior inference algorithm—federated posterior averaging (FedPA). Our algorithm uses MCMC for approximate inference of local posteriors on the clients and efficiently communicates their statistics to the server, where the latter uses them to refine a global estimate of the posterior mode. Finally, we show that FedPA generalizes federated averaging (FedAvg), can similarly benefit from adaptive optimizers, and yields state-of-the-art results on four realistic and challenging benchmarks, converging faster, to better optima",
    "checked": true,
    "id": "59477007095a68ba551a588df756fb9873a14b72",
    "semantic_title": "federated learning via posterior averaging: a new perspective and practical algorithms",
    "citation_count": 115,
    "authors": []
  },
  "https://openreview.net/forum?id=Nc3TJqbcl3": {
    "title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers",
    "volume": "poster",
    "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping",
    "checked": true,
    "id": "7d2ea736f9827cc8652dbaf5eafcbd5c6c7af82b",
    "semantic_title": "extracting strong policies for robotics tasks from zero-order trajectory optimizers",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=mQPBmvyAuk": {
    "title": "BREEDS: Benchmarks for Subpopulation Shift",
    "volume": "poster",
    "abstract": "We develop a methodology for assessing the robustness of models to subpopulation shift---specifically, their ability to generalize to novel data subpopulations that were not observed during training. Our approach leverages the class structure underlying existing datasets to control the data subpopulations that comprise the training and test distributions. This enables us to synthesize realistic distribution shifts whose sources can be precisely controlled and characterized, within existing large-scale datasets. Applying this methodology to the ImageNet dataset, we create a suite of subpopulation shift benchmarks of varying granularity. We then validate that the corresponding shifts are tractable by obtaining human baselines. Finally, we utilize these benchmarks to measure the sensitivity of standard model architectures as well as the effectiveness of existing train-time robustness interventions",
    "checked": true,
    "id": "767c6702045f2290012a259744db9edb4d55bcb8",
    "semantic_title": "breeds: benchmarks for subpopulation shift",
    "citation_count": 176,
    "authors": []
  },
  "https://openreview.net/forum?id=NQbnPjPYaG6": {
    "title": "On the Impossibility of Global Convergence in Multi-Loss Optimization",
    "volume": "poster",
    "abstract": "Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any 'reasonable' algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum. Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL",
    "checked": true,
    "id": "71065d3de6d962596915d3b91d981d7871a3fc68",
    "semantic_title": "on the impossibility of global convergence in multi-loss optimization",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=OHgnfSrn2jv": {
    "title": "Efficient Wasserstein Natural Gradients for Reinforcement Learning",
    "volume": "poster",
    "abstract": "A novel optimization approach is proposed for application to policy gradient methods and evolution strategies for reinforcement learning (RL). The procedure uses a computationally efficient \\emph{Wasserstein natural gradient} (WNG) descent that takes advantage of the geometry induced by a Wasserstein penalty to speed optimization. This method follows the recent theme in RL of including divergence penalties in the objective to establish trust regions. Experiments on challenging tasks demonstrate improvements in both computational cost and performance over advanced baselines",
    "checked": true,
    "id": "6e1ee4042e627e17128ff38adc550c305e539a85",
    "semantic_title": "efficient wasserstein natural gradients for reinforcement learning",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=C70cp4Cn32": {
    "title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks",
    "volume": "poster",
    "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments",
    "checked": true,
    "id": "b97d07c6dc6132dbcbddf203d4a3f2d2a803f414",
    "semantic_title": "multi-level local sgd for heterogeneous hierarchical networks",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=QoWatN-b8T": {
    "title": "Kanerva++: Extending the Kanerva Machine With Differentiable, Locally Block Allocated Latent Memory",
    "volume": "poster",
    "abstract": "Episodic and semantic memory are critical components of the human memory model. The theory of complementary learning systems (McClelland et al., 1995) suggests that the compressed representation produced by a serial event (episodic memory) is later restructured to build a more generalized form of reusable knowledge (semantic memory). In this work, we develop a new principled Bayesian memory allocation scheme that bridges the gap between episodic and semantic memory via a hierarchical latent variable model. We take inspiration from traditional heap allocation and extend the idea of locally contiguous memory to the Kanerva Machine, enabling a novel differentiable block allocated latent memory. In contrast to the Kanerva Machine, we simplify the process of memory writing by treating it as a fully feed forward deterministic process, relying on the stochasticity of the read key distribution to disperse information within the memory. We demonstrate that this allocation scheme improves performance in memory conditional image generation, resulting in new state-of-the-art conditional likelihood values on binarized MNIST (≤41.58 nats/image) , binarized Omniglot (≤66.24 nats/image), as well as presenting competitive performance on CIFAR10, DMLab Mazes, Celeb-A and ImageNet32×32",
    "checked": true,
    "id": "c04dca87cc9330f57a1de3a3cc7c0d2085910772",
    "semantic_title": "kanerva++: extending the kanerva machine with differentiable, locally block allocated latent memory",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=hsFN92eQEla": {
    "title": "EVALUATION OF NEURAL ARCHITECTURES TRAINED WITH SQUARE LOSS VS CROSS-ENTROPY IN CLASSIFICATION TASKS",
    "volume": "poster",
    "abstract": "Modern neural architectures for classification tasks are trained using the cross-entropy loss, which is widely believed to be empirically superior to the square loss. In this work we provide evidence indicating that this belief may not be well-founded. We explore several major neural architectures and a range of standard benchmark datasets for NLP, automatic speech recognition (ASR) and computer vision tasks to show that these architectures, with the same hyper-parameter settings as reported in the literature, perform comparably or better when trained with the square loss, even after equalizing computational resources. Indeed, we observe that the square loss produces better results in the dominant majority of NLP and ASR experiments. Cross-entropy appears to have a slight edge on computer vision tasks. We argue that there is little compelling empirical or theoretical evidence indicating a clear-cut advantage to the cross-entropy loss. Indeed, in our experiments, performance on nearly all non-vision tasks can be improved, sometimes significantly, by switching to the square loss. Furthermore, training with square loss appears to be less sensitive to the randomness in initialization. We posit that training using the square loss for classification needs to be a part of best practices of modern deep learning on equal footing with cross-entropy",
    "checked": true,
    "id": "bcc48c5f68a387c89c75cd80d52ef52284db3c3a",
    "semantic_title": "evaluation of neural architectures trained with square loss vs cross-entropy in classification tasks",
    "citation_count": 176,
    "authors": []
  },
  "https://openreview.net/forum?id=-bdp_8Itjwp": {
    "title": "Self-supervised Learning from a Multi-view Perspective",
    "volume": "poster",
    "abstract": "As a subset of unsupervised representation learning, self-supervised representation learning adopts self-defined signals as supervision and uses the learned representation for downstream tasks, such as object detection and image captioning. Many proposed approaches for self-supervised learning follow naturally a multi-view perspective, where the input (e.g., original images) and the self-supervised signals (e.g., augmented images) can be seen as two redundant views of the data. Building from this multi-view perspective, this paper provides an information-theoretical framework to better understand the properties that encourage successful self-supervised learning. Specifically, we demonstrate that self-supervised learned representations can extract task-relevant information and discard task-irrelevant information. Our theoretical framework paves the way to a larger space of self-supervised learning objective design. In particular, we propose a composite objective that bridges the gap between prior contrastive and predictive learning objectives, and introduce an additional objective term to discard task-irrelevant information. To verify our analysis, we conduct controlled experiments to evaluate the impact of the composite objectives. We also explore our framework's empirical generalization beyond the multi-view perspective, where the cross-view redundancy may not be clearly observed",
    "checked": true,
    "id": "ce88a95ec5f226b66cfce6770fcecbd1dc5748b3",
    "semantic_title": "self-supervised learning from a multi-view perspective",
    "citation_count": 193,
    "authors": []
  },
  "https://openreview.net/forum?id=j9Rv7qdXjd": {
    "title": "Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels",
    "volume": "poster",
    "abstract": "Current neural architecture search (NAS) strategies focus only on finding a single, good, architecture. They offer little insight into why a specific network is performing well, or how we should modify the architecture if we want further improvements. We propose a Bayesian optimisation (BO) approach for NAS that combines the Weisfeiler-Lehman graph kernel with a Gaussian process surrogate. Our method not only optimises the architecture in a highly data-efficient manner, but also affords interpretability by discovering useful network features and their corresponding impact on the network performance. Moreover, our method is capable of capturing the topological structures of the architectures and is scalable to large graphs, thus making the high-dimensional and graph-like search spaces amenable to BO. We demonstrate empirically that our surrogate model is capable of identifying useful motifs which can guide the generation of new architectures. We finally show that our method outperforms existing NAS approaches to achieve the state of the art on both closed- and open-domain search spaces",
    "checked": true,
    "id": "2764af5084331e91e386d498ed53a2f4ce0eb354",
    "semantic_title": "interpretable neural architecture search via bayesian optimisation with weisfeiler-lehman kernels",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=UoaQUQREMOs": {
    "title": "CT-Net: Channel Tensorization Network for Video Classification",
    "volume": "poster",
    "abstract": "3D convolution is powerful for video classification but often computationally expensive, recent studies mainly focus on decomposing it on spatial-temporal and/or channel dimensions. Unfortunately, most approaches fail to achieve a preferable balance between convolutional efficiency and feature-interaction sufficiency. For this reason, we propose a concise and novel Channel Tensorization Network (CT-Net), by treating the channel dimension of input feature as a multiplication of K sub-dimensions. On one hand, it naturally factorizes convolution in a multiple dimension way, leading to a light computation burden. On the other hand, it can effectively enhance feature interaction from different channels, and progressively enlarge the 3D receptive field of such interaction to boost classification accuracy. Furthermore, we equip our CT-Module with a Tensor Excitation (TE) mechanism. It can learn to exploit spatial, temporal and channel attention in a high-dimensional manner, to improve the cooperative power of all the feature dimensions in our CT-Module. Finally, we flexibly adapt ResNet as our CT-Net. Extensive experiments are conducted on several challenging video benchmarks, e.g., Kinetics-400, Something-Something V1 and V2. Our CT-Net outperforms a number of recent SOTA approaches, in terms of accuracy and/or efficiency",
    "checked": true,
    "id": "780bd9eaa032be199ae5e6b7c239e5a4ae8f42ed",
    "semantic_title": "ct-net: channel tensorization network for video classification",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=tqOvYpjPax2": {
    "title": "Intraclass clustering: an implicit learning ability that regularizes DNNs",
    "volume": "poster",
    "abstract": "Several works have shown that the regularization mechanisms underlying deep neural networks' generalization performances are still poorly understood. In this paper, we hypothesize that deep neural networks are regularized through their ability to extract meaningful clusters among the samples of a class. This constitutes an implicit form of regularization, as no explicit training mechanisms or supervision target such behaviour. To support our hypothesis, we design four different measures of intraclass clustering, based on the neuron- and layer-level representations of the training data. We then show that these measures constitute accurate predictors of generalization performance across variations of a large set of hyperparameters (learning rate, batch size, optimizer, weight decay, dropout rate, data augmentation, network depth and width)",
    "checked": true,
    "id": "4b4df14b4b4b7cd1dd69093cc9cbccfe35a709ec",
    "semantic_title": "intraclass clustering: an implicit learning ability that regularizes dnns",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=fmOOI2a3tQP": {
    "title": "Learning Robust State Abstractions for Hidden-Parameter Block MDPs",
    "volume": "poster",
    "abstract": "Many control tasks exhibit similar dynamics that can be modeled as having common latent structure. Hidden-Parameter Markov Decision Processes (HiP-MDPs) explicitly model this structure to improve sample efficiency in multi-task settings. However, this setting makes strong assumptions on the observability of the state that limit its application in real-world scenarios with rich observation spaces. In this work, we leverage ideas of common structure from the HiP-MDP setting, and extend it to enable robust state abstractions inspired by Block MDPs. We derive instantiations of this new framework for both multi-task reinforcement learning (MTRL) and meta-reinforcement learning (Meta-RL) settings. Further, we provide transfer and generalization bounds based on task and state similarity, along with sample complexity bounds that depend on the aggregate number of samples across tasks, rather than the number of tasks, a significant improvement over prior work. To further demonstrate efficacy of the proposed method, we empirically compare and show improvement over multi-task and meta-reinforcement learning baselines",
    "checked": true,
    "id": "9835d0d85faa36d2c27bea806487b988935e92a2",
    "semantic_title": "learning robust state abstractions for hidden-parameter block mdps",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=FX0vR39SJ5q": {
    "title": "Isometric Transformation Invariant and Equivariant Graph Convolutional Networks",
    "volume": "poster",
    "abstract": "Graphs are one of the most important data structures for representing pairwise relations between objects. Specifically, a graph embedded in a Euclidean space is essential to solving real problems, such as physical simulations. A crucial requirement for applying graphs in Euclidean spaces to physical simulations is learning and inferring the isometric transformation invariant and equivariant features in a computationally efficient manner. In this paper, we propose a set of transformation invariant and equivariant models based on graph convolutional networks, called IsoGCNs. We demonstrate that the proposed model has a competitive performance compared to state-of-the-art methods on tasks related to geometrical and physical simulation data. Moreover, the proposed model can scale up to graphs with 1M vertices and conduct an inference faster than a conventional finite element analysis, which the existing equivariant models cannot achieve",
    "checked": true,
    "id": "a66aef5f4d60aa6faee9011d8fc42af722fb94d3",
    "semantic_title": "isometric transformation invariant and equivariant graph convolutional networks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=P6_q1BRxY8Q": {
    "title": "Learning Safe Multi-agent Control with Decentralized Neural Barrier Certificates",
    "volume": "poster",
    "abstract": "We study the multi-agent safe control problem where agents should avoid collisions to static obstacles and collisions with each other while reaching their goals. Our core idea is to learn the multi-agent control policy jointly with learning the control barrier functions as safety certificates. We propose a new joint-learning framework that can be implemented in a decentralized fashion, which can adapt to an arbitrarily large number of agents. Building upon this framework, we further improve the scalability by incorporating neural network architectures that are invariant to the quantity and permutation of neighboring agents. In addition, we propose a new spontaneous policy refinement method to further enforce the certificate condition during testing. We provide extensive experiments to demonstrate that our method significantly outperforms other leading multi-agent control approaches in terms of maintaining safety and completing original tasks. Our approach also shows substantial generalization capability in that the control policy can be trained with 8 agents in one scenario, while being used on other scenarios with up to 1024 agents in complex multi-agent environments and dynamics. Videos and source code can be found at https://realm.mit.edu/blog/learning-safe-multi-agent-control-decentralized-neural-barrier-certificates",
    "checked": true,
    "id": "133dd31096fb36803e65001ba106767b2be65bd0",
    "semantic_title": "learning safe multi-agent control with decentralized neural barrier certificates",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=xCxXwTzx4L1": {
    "title": "ChipNet: Budget-Aware Pruning with Heaviside Continuous Approximations",
    "volume": "poster",
    "abstract": "Structured pruning methods are among the effective strategies for extracting small resource-efficient convolutional neural networks from their dense counterparts with minimal loss in accuracy. However, most existing methods still suffer from one or more limitations, that include 1) the need for training the dense model from scratch with pruning-related parameters embedded in the architecture, 2) requiring model-specific hyperparameter settings, 3) inability to include budget-related constraint in the training process, and 4) instability under scenarios of extreme pruning. In this paper, we present ChipNet, a deterministic pruning strategy that employs continuous Heaviside function and a novel crispness loss to identify a highly sparse network out of an existing dense network. Our choice of continuous Heaviside function is inspired by the field of design optimization, where the material distribution task is posed as a continuous optimization problem, but only discrete values (0 or 1) are practically feasible and expected as final outcomes. Our approach's flexible design facilitates its use with different choices of budget constraints while maintaining stability for very low target budgets. Experimental results show that ChipNet outperforms state-of-the-art structured pruning methods by remarkable margins of up to 16.1% in terms of accuracy. Further, we show that the masks obtained with ChipNet are transferable across datasets. For certain cases, it was observed that masks transferred from a model trained on feature-rich teacher dataset provide better performance on the student dataset than those obtained by directly pruning on the student data itself",
    "checked": true,
    "id": "1e11c7a586298625113f1bb0cdff505fe767a3a0",
    "semantic_title": "chipnet: budget-aware pruning with heaviside continuous approximations",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=Drynvt7gg4L": {
    "title": "AdaSpeech: Adaptive Text to Speech for Custom Voice",
    "volume": "poster",
    "abstract": "Custom voice, a specific text to speech (TTS) service in commercial speech platforms, aims to adapt a source TTS model to synthesize personal voice for a target speaker using few speech from her/him. Custom voice presents two unique challenges for TTS adaptation: 1) to support diverse customers, the adaptation model needs to handle diverse acoustic conditions which could be very different from source speech data, and 2) to support a large number of customers, the adaptation parameters need to be small enough for each target speaker to reduce memory usage while maintaining high voice quality. In this work, we propose AdaSpeech, an adaptive TTS system for high-quality and efficient customization of new voices. We design several techniques in AdaSpeech to address the two challenges in custom voice: 1) To handle different acoustic conditions, we model the acoustic information in both utterance and phoneme level. Specifically, we use one acoustic encoder to extract an utterance-level vector and another one to extract a sequence of phoneme-level vectors from the target speech during pre-training and fine-tuning; in inference, we extract the utterance-level vector from a reference speech and use an acoustic predictor to predict the phoneme-level vectors. 2) To better trade off the adaptation parameters and voice quality, we introduce conditional layer normalization in the mel-spectrogram decoder of AdaSpeech, and fine-tune this part in addition to speaker embedding for adaptation. We pre-train the source TTS model on LibriTTS datasets and fine-tune it on VCTK and LJSpeech datasets (with different acoustic conditions from LibriTTS) with few adaptation data, e.g., 20 sentences, about 1 minute speech. Experiment results show that AdaSpeech achieves much better adaptation quality than baseline methods, with only about 5K specific parameters for each speaker, which demonstrates its effectiveness for custom voice. The audio samples are available at https://speechresearch.github.io/adaspeech/",
    "checked": true,
    "id": "4c055698c68b21c4e8f25a5c1b3439f9f7f3be62",
    "semantic_title": "adaspeech: adaptive text to speech for custom voice",
    "citation_count": 197,
    "authors": []
  },
  "https://openreview.net/forum?id=YLewtnvKgR7": {
    "title": "Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors",
    "volume": "poster",
    "abstract": "Predictive uncertainty estimation is an essential next step for the reliable deployment of deep object detectors in safety-critical tasks. In this work, we focus on estimating predictive distributions for bounding box regression output with variance networks. We show that in the context of object detection, training variance networks with negative log likelihood (NLL) can lead to high entropy predictive distributions regardless of the correctness of the output mean. We propose to use the energy score as a non-local proper scoring rule and find that when used for training, the energy score leads to better calibrated and lower entropy predictive distributions than NLL. We also address the widespread use of non-proper scoring metrics for evaluating predictive distributions from deep object detectors by proposing an alternate evaluation approach founded on proper scoring rules. Using the proposed evaluation tools, we show that although variance networks can be used to produce high quality predictive distributions, ad-hoc approaches used by seminal object detectors for choosing regression targets during training do not provide wide enough data support for reliable variance learning. We hope that our work helps shift evaluation in probabilistic object detection to better align with predictive uncertainty evaluation in other machine learning domains. Code for all models, evaluation, and datasets is available at: https://github.com/asharakeh/probdet.git",
    "checked": true,
    "id": "bc03b69ade305dada3024e580a387de57b75a894",
    "semantic_title": "estimating and evaluating regression predictive uncertainty in deep object detectors",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=QubpWYfdNry": {
    "title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints",
    "volume": "poster",
    "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment",
    "checked": true,
    "id": "42a37d42369c08f15f55a57e522d0a177da20ab7",
    "semantic_title": "domain-robust visual imitation learning with mutual information constraints",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=e12NDM7wkEY": {
    "title": "Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation",
    "volume": "poster",
    "abstract": "Clustering is one of the most fundamental tasks in machine learning. Recently, deep clustering has become a major trend in clustering techniques. Representation learning often plays an important role in the effectiveness of deep clustering, and thus can be a principal cause of performance degradation. In this paper, we propose a clustering-friendly representation learning method using instance discrimination and feature decorrelation. Our deep-learning-based representation learning method is motivated by the properties of classical spectral clustering. Instance discrimination learns similarities among data and feature decorrelation removes redundant correlation among features. We utilize an instance discrimination method in which learning individual instance classes leads to learning similarity among instances. Through detailed experiments and examination, we show that the approach can be adapted to learning a latent space for clustering. We design novel softmax-formulated decorrelation constraints for learning. In evaluations of image clustering using CIFAR-10 and ImageNet-10, our method achieves accuracy of 81.5% and 95.4%, respectively. We also show that the softmax-formulated constraints are compatible with various neural networks",
    "checked": true,
    "id": "5085d0386c77c0e25335b8f169e609509e302b85",
    "semantic_title": "clustering-friendly representation learning via instance discrimination and feature decorrelation",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=9GsFOUyUPi": {
    "title": "Progressive Skeletonization: Trimming more fat from a network at initialization",
    "volume": "poster",
    "abstract": "Recent studies have shown that skeletonization (pruning parameters) of networks at initialization provides all the practical benefits of sparsity both at inference and training time, while only marginally degrading their performance. However, we observe that beyond a certain level of sparsity (approx 95%), these approaches fail to preserve the network performance, and to our surprise, in many cases perform even worse than trivial random pruning. To this end, we propose an objective to find a skeletonized network with maximum foresight connection sensitivity (FORCE) whereby the trainability, in terms of connection sensitivity, of a pruned network is taken into consideration. We then propose two approximate procedures to maximize our objective (1) Iterative SNIP: allows parameters that were unimportant at earlier stages of skeletonization to become important at later stages; and (2) FORCE: iterative process that allows exploration by allowing already pruned parameters to resurrect at later stages of skeletonization. Empirical analysis on a large suite of experiments show that our approach, while providing at least as good performance as other recent approaches on moderate pruning levels, provide remarkably improved performance on high pruning levels (could remove up to 99.5% parameters while keeping the networks trainable)",
    "checked": true,
    "id": "0b5c5571ab9dd88ebf010e8dc6898d8078d8e877",
    "semantic_title": "progressive skeletonization: trimming more fat from a network at initialization",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=8Sqhl-nF50": {
    "title": "On the Curse of Memory in Recurrent Neural Networks: Approximation and Optimization Analysis",
    "volume": "poster",
    "abstract": "We study the approximation properties and optimization dynamics of recurrent neural networks (RNNs) when applied to learn input-output relationships in temporal data. We consider the simple but representative setting of using continuous-time linear RNNs to learn from data generated by linear relationships. Mathematically, the latter can be understood as a sequence of linear functionals. We prove a universal approximation theorem of such linear functionals and characterize the approximation rate. Moreover, we perform a fine-grained dynamical analysis of training linear RNNs by gradient methods. A unifying theme uncovered is the non-trivial effect of memory, a notion that can be made precise in our framework, on both approximation and optimization: when there is long-term memory in the target, it takes a large number of neurons to approximate it. Moreover, the training process will suffer from slow downs. In particular, both of these effects become exponentially more pronounced with increasing memory - a phenomenon we call the \"curse of memory\". These analyses represent a basic step towards a concrete mathematical understanding of new phenomenons that may arise in learning temporal relationships using recurrent architectures",
    "checked": true,
    "id": "d4d20bcbb002f575a6f8fd9db9a2822b23f3a0bd",
    "semantic_title": "on the curse of memory in recurrent neural networks: approximation and optimization analysis",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=7aogOj_VYO0": {
    "title": "Do not Let Privacy Overbill Utility: Gradient Embedding Perturbation for Private Learning",
    "volume": "poster",
    "abstract": "The privacy leakage of the model about the training data can be bounded in the differential privacy mechanism. However, for meaningful privacy parameters, a differentially private model degrades the utility drastically when the model comprises a large number of trainable parameters. In this paper, we propose an algorithm \\emph{Gradient Embedding Perturbation (GEP)} towards training differentially private deep models with decent accuracy. Specifically, in each gradient descent step, GEP first projects individual private gradient into a non-sensitive anchor subspace, producing a low-dimensional gradient embedding and a small-norm residual gradient. Then, GEP perturbs the low-dimensional embedding and the residual gradient separately according to the privacy budget. Such a decomposition permits a small perturbation variance, which greatly helps to break the dimensional barrier of private learning. With GEP, we achieve decent accuracy with low computational cost and modest privacy guarantee for deep models. Especially, with privacy bound $\\epsilon=8$, we achieve $74.9\\%$ test accuracy on CIFAR10 and $95.1\\%$ test accuracy on SVHN, significantly improving over existing results",
    "checked": true,
    "id": "ee500b621cc59f76b3396c6c5f136be8e9c44726",
    "semantic_title": "do not let privacy overbill utility: gradient embedding perturbation for private learning",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=z5Z023VBmDZ": {
    "title": "More or Less: When and How to Build Convolutional Neural Network Ensembles",
    "volume": "poster",
    "abstract": "Convolutional neural networks are utilized to solve increasingly more complex problems and with more data. As a result, researchers and practitioners seek to scale the representational power of such models by adding more parameters. However, increasing parameters requires additional critical resources in terms of memory and compute, leading to increased training and inference cost. Thus a consistent challenge is to obtain as high as possible accuracy within a parameter budget. As neural network designers navigate this complex landscape, they are guided by conventional wisdom that is informed from past empirical studies. We identify a critical part of this design space that is not well-understood: How to decide between the alternatives of expanding a single convolutional network model or increasing the number of networks in the form of an ensemble. We study this question in detail across various network architectures and data sets. We build an extensive experimental framework that captures numerous angles of the possible design space in terms of how a new set of parameters can be used in a model. We consider a holistic set of metrics such as training time, inference time, and memory usage. The framework provides a robust assessment by making sure it controls for the number of parameters. Contrary to conventional wisdom, we show that when we perform a holistic and robust assessment, we uncover a wide design space, where ensembles provide better accuracy, train faster, and deploy at speed comparable to single convolutional networks with the same total number of parameters",
    "checked": true,
    "id": "46d4c96a2952ede68b8907de5aad1f3c9f5f48df",
    "semantic_title": "more or less: when and how to build convolutional neural network ensembles",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=3RLN4EPMdYd": {
    "title": "Revisiting Hierarchical Approach for Persistent Long-Term Video Prediction",
    "volume": "poster",
    "abstract": "Learning to predict the long-term future of video frames is notoriously challenging due to the inherent ambiguities in a distant future and dramatic amplification of prediction error over time. Despite the recent advances in the literature, existing approaches are limited to moderately short-term prediction (less than a few seconds), while extrapolating it to a longer future quickly leads to destruction in structure and content. In this work, we revisit the hierarchical models in video prediction. Our method generates future frames by first estimating a sequence of dense semantic structures and subsequently translating the estimated structures to pixels by video-to-video translation model. Despite the simplicity, we show that modeling structures and their dynamics in categorical structure space with stochastic sequential estimator leads to surprisingly successful long-term prediction. We evaluate our method on two challenging video prediction scenarios, \\emph{car driving} and \\emph{human dancing}, and demonstrate that it can generate complicated scene structures and motions over a very long time horizon (\\ie~thousands frames), setting a new standard of video prediction with orders of magnitude longer prediction time than existing approaches. Video results are available at https://1konny.github.io/HVP/",
    "checked": true,
    "id": "8e9f455548d6f5088647c504cbe675a78971622c",
    "semantic_title": "revisiting hierarchical approach for persistent long-term video prediction",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=jEYKjPE1xYN": {
    "title": "Symmetry-Aware Actor-Critic for 3D Molecular Design",
    "volume": "poster",
    "abstract": "Automating molecular design using deep reinforcement learning (RL) has the potential to greatly accelerate the search for novel materials. Despite recent progress on leveraging graph representations to design molecules, such methods are fundamentally limited by the lack of three-dimensional (3D) information. In light of this, we propose a novel actor-critic architecture for 3D molecular design that can generate molecular structures unattainable with previous approaches. This is achieved by exploiting the symmetries of the design process through a rotationally covariant state-action representation based on a spherical harmonics series expansion. We demonstrate the benefits of our approach on several 3D molecular design tasks, where we find that building in such symmetries significantly improves generalization and the quality of generated molecules",
    "checked": true,
    "id": "d11ac1bbfb9148ff177a9443b3700695fb5995e1",
    "semantic_title": "symmetry-aware actor-critic for 3d molecular design",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=vQzcqQWIS0q": {
    "title": "Learnable Embedding sizes for Recommender Systems",
    "volume": "poster",
    "abstract": "The embedding-based representation learning is commonly used in deep learning recommendation models to map the raw sparse features to dense vectors. The traditional embedding manner that assigns a uniform size to all features has two issues. First, the numerous features inevitably lead to a gigantic embedding table that causes a high memory usage cost. Second, it is likely to cause the over-fitting problem for those features that do not require too large representation capacity. Existing works that try to address the problem always cause a significant drop in recommendation performance or suffers from the limitation of unaffordable training time cost. In this paper, we proposed a novel approach, named PEP (short for Plug-in Embedding Pruning), to reduce the size of the embedding table while avoiding the drop of recommendation accuracy. PEP prunes embedding parameter where the pruning threshold(s) can be adaptively learned from data. Therefore we can automatically obtain a mixed-dimension embedding-scheme by pruning redundant parameters for each feature. PEP is a general framework that can plug in various base recommendation models. Extensive experiments demonstrate it can efficiently cut down embedding parameters and boost the base model's performance. Specifically, it achieves strong recommendation performance while reducing 97-99% parameters. As for the computation cost, PEP only brings an additional 20-30% time cost compare with base models",
    "checked": true,
    "id": "7a7e23b1973c6555958a3e4bcf1bcc96b9065c31",
    "semantic_title": "learnable embedding sizes for recommender systems",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=pAbm1qfheGk": {
    "title": "Learning Neural Generative Dynamics for Molecular Conformation Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bf8483f233ab19f600de940a1bfce6cc323cb91d",
    "semantic_title": "learning neural generative dynamics for molecular conformation generation",
    "citation_count": 121,
    "authors": []
  },
  "https://openreview.net/forum?id=NjF772F4ZZR": {
    "title": "Learning the Pareto Front with Hypernetworks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d96711ced185b92a011f3e51191497e8ae4b8559",
    "semantic_title": "learning the pareto front with hypernetworks",
    "citation_count": 154,
    "authors": []
  },
  "https://openreview.net/forum?id=Y87Ri-GNHYu": {
    "title": "Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0d6a4e45acde6f47d704ed0752f17f7ab52223af",
    "semantic_title": "ask your humans: using human instructions to improve generalization in reinforcement learning",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=ZW0yXJyNmoG": {
    "title": "Taming GANs with Lookahead-Minmax",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0e50e8ff4bd85f6dfe1708010ed0e56348f60dfe",
    "semantic_title": "taming gans with lookahead-minmax",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=1Jv6b0Zq3qi": {
    "title": "Uncertainty in Gradient Boosting via Ensembles",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4528b9f6eb57cb7f180060150877616c956a63fd",
    "semantic_title": "uncertainty in gradient boosting via ensembles",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=eLfqMl3z3lq": {
    "title": "Adversarial score matching and improved sampling for image generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "22c3badd79d4ee60892705b34c59807a6e828850",
    "semantic_title": "adversarial score matching and improved sampling for image generation",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=piLPYqxtWuA": {
    "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1623d6ffb6efd94d21537db2b96b91a196842aef",
    "semantic_title": "fastspeech 2: fast and high-quality end-to-end text to speech",
    "citation_count": 1463,
    "authors": []
  },
  "https://openreview.net/forum?id=DEa4JdMWRHp": {
    "title": "Interpretable Models for Granger Causality Using Self-explaining Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "732fe890e5fa6e7378fb936aaa3192f2d98d4af2",
    "semantic_title": "interpretable models for granger causality using self-explaining neural networks",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=sy4Kg_ZQmS7": {
    "title": "Learning Deep Features in Instrumental Variable Regression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bbf19a76e51cf5c2d2ed853b48f470554bf4ee39",
    "semantic_title": "learning deep features in instrumental variable regression",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=z9k8BWL-_2u": {
    "title": "Statistical inference for individual fairness",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "de2eecf29162c9505d68bc04ab0529b20b2dcc5b",
    "semantic_title": "statistical inference for individual fairness",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=AY8zfZm0tDd": {
    "title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "736590f70e7f2dc464c1c62491cfa8adb4d718f3",
    "semantic_title": "randomized ensembled double q-learning: learning fast without a model",
    "citation_count": 295,
    "authors": []
  },
  "https://openreview.net/forum?id=ADWd4TJO13G": {
    "title": "Lifelong Learning of Compositional Structures",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a714514a989a943d08f8ad25e307df2bf7150c6c",
    "semantic_title": "lifelong learning of compositional structures",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=Rcmk0xxIQV": {
    "title": "QPLEX: Duplex Dueling Multi-Agent Q-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "052c100d45f949c06e8419b504e319b442cb3f0a",
    "semantic_title": "qplex: duplex dueling multi-agent q-learning",
    "citation_count": 475,
    "authors": []
  },
  "https://openreview.net/forum?id=--gvHfE3Xf5": {
    "title": "Meta-Learning of Structured Task Distributions in Humans and Machines",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15ad6f868acb05e836e88774aa2b71b0e082e5aa",
    "semantic_title": "meta-learning of structured task distributions in humans and machines",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=8X2eaSZxTP": {
    "title": "PC2WF: 3D Wireframe Reconstruction from Raw Point Clouds",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "75f25a80d5ea871de855f09aa5d558a6fcc993d8",
    "semantic_title": "pc2wf: 3d wireframe reconstruction from raw point clouds",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=9l0K4OM-oXE": {
    "title": "Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4d4e5c0c691e42b7078208598d585caacc2e34c2",
    "semantic_title": "neural attention distillation: erasing backdoor triggers from deep neural networks",
    "citation_count": 454,
    "authors": []
  },
  "https://openreview.net/forum?id=hiq1rHO8pNT": {
    "title": "HyperGrid Transformers: Towards A Single Model for Multiple Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "942aec6e9dddcaa6700e14a2f6e1b77164a092cb",
    "semantic_title": "hypergrid transformers: towards a single model for multiple tasks",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=qVyeW-grC2k": {
    "title": "Long Range Arena : A Benchmark for Efficient Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7e9ff94476f41041c75e253e84f487db00e9c861",
    "semantic_title": "long range arena: a benchmark for efficient transformers",
    "citation_count": 757,
    "authors": []
  },
  "https://openreview.net/forum?id=j1RMMKeP2gR": {
    "title": "Acting in Delayed Environments with Non-Stationary Markov Policies",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8aa6248fa36de344c3cd6b3e19ae95c5bc3fa87a",
    "semantic_title": "acting in delayed environments with non-stationary markov policies",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=q_S44KLQ_Aa": {
    "title": "Neurally Augmented ALISTA",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "68e4b45d99ec0ffa075d540a91fc27385d8f6707",
    "semantic_title": "neurally augmented alista",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=vhKe9UFbrJo": {
    "title": "Relating by Contrasting: A Data-efficient Framework for Multimodal Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f2f3c91fad164c8919699159e8eab511f03d0fe2",
    "semantic_title": "relating by contrasting: a data-efficient framework for multimodal generative models",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=-GLNZeVDuik": {
    "title": "Categorical Normalizing Flows via Continuous Transformations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6670ce9fa7dbc271612078f2b3f05c12e0281aeb",
    "semantic_title": "categorical normalizing flows via continuous transformations",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=aCgLmfhIy_f": {
    "title": "Prototypical Representation Learning for Relation Extraction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5d8e85d12c218f56f3c50351fa2ddc3da098b31c",
    "semantic_title": "prototypical representation learning for relation extraction",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=0PtUPB9z6qK": {
    "title": "Generalized Energy Based Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "131d51ce9a3f32266a7e85b2f32a659edc649745",
    "semantic_title": "generalized energy based models",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=Y9McSeEaqUh": {
    "title": "Predicting Classification Accuracy When Adding New Unobserved Classes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "33391f5117b1e93548b0f297de1e36ee4a61157a",
    "semantic_title": "predicting classification accuracy when adding new unobserved classes",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=lQdXeXDoWtI": {
    "title": "In Search of Lost Domain Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6a5efb990b6558c21d9fdded4884c00ba152cb7c",
    "semantic_title": "in search of lost domain generalization",
    "citation_count": 1196,
    "authors": []
  },
  "https://openreview.net/forum?id=kEnBH98BGs5": {
    "title": "Estimating informativeness of samples with Smooth Unique Information",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1b53e8652676a1806d0d2abc5d7abcd212709046",
    "semantic_title": "estimating informativeness of samples with smooth unique information",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=45NZvF1UHam": {
    "title": "Identifying Physical Law of Hamiltonian Systems via Meta-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "87174a4df7567e52771d15f949d459791145379b",
    "semantic_title": "identifying physical law of hamiltonian systems via meta-learning",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=dyjPVUc2KB": {
    "title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "616ef21345e4063c2f1227fc64225e7e65a66473",
    "semantic_title": "adapting to reward progressivity via spectral reinforcement learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=rsogjAnYs4z": {
    "title": "Understanding the effects of data parallelism and sparsity on neural network training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9713b351df45932a6c496c283e1bf5f664c87c69",
    "semantic_title": "understanding the effects of data parallelism and sparsity on neural network training",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=TtYSU29zgR": {
    "title": "Primal Wasserstein Imitation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "25287b593d2642b1627b96a79c5ff8d3c8ec1f5c",
    "semantic_title": "primal wasserstein imitation learning",
    "citation_count": 133,
    "authors": []
  },
  "https://openreview.net/forum?id=Ptaz_zIFbX": {
    "title": "Prediction and generalisation over directed actions by grid cells",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "085a8cd9324c12da35d66e6164de3e250683eb68",
    "semantic_title": "prediction and generalisation over directed actions by grid cells",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=1rxHOBjeDUW": {
    "title": "Drop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "05ad9fc4ba9fad5aade4b700e8505d37eefd0519",
    "semantic_title": "drop-bottleneck: learning discrete compressed representation for noise-robust exploration",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=umIdUL8rMH": {
    "title": "BOIL: Towards Representation Change for Few-shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a925908789343e12b9ec62e2b4f7681c248866c4",
    "semantic_title": "boil: towards representation change for few-shot learning",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=ee6W5UgQLa": {
    "title": "MultiModalQA: complex question answering over text, tables and images",
    "volume": "poster",
    "abstract": "When answering complex questions, people can seamlessly combine information from visual, textual and tabular sources. While interest in models that reason over multiple pieces of evidence has surged in recent years, there has been relatively little work on question answering models that reason across multiple modalities. In this paper, we present MultiModalQA (MMQA): a challenging question answering dataset that requires joint reasoning over text, tables and images. We create MMQA using a new framework for generating complex multi-modal questions at scale, harvesting tables from Wikipedia, and attaching images and text paragraphs using entities that appear in each table. We then define a formal language that allows us to take questions that can be answered from a single modality, and combine them to generate cross-modal questions. Last, crowdsourcing workers take these automatically generated questions and rephrase them into more fluent language. We create 29,918 questions through this procedure, and empirically demonstrate the necessity of a multi-modal multi-hop approach to solve our task: our multi-hop model, ImplicitDecomp, achieves an average F1 of 51.7 over cross-modal questions, substantially outperforming a strong baseline that achieves 38.2 F1, but still lags significantly behind human performance, which is at 90.1 F1",
    "checked": true,
    "id": "d365978adf0a5c9c6028820857e015617856256b",
    "semantic_title": "multimodalqa: complex question answering over text, tables and images",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=EoFNy62JGd": {
    "title": "Neural gradients are near-lognormal: improved quantized and sparse training",
    "volume": "poster",
    "abstract": "While training can mostly be accelerated by reducing the time needed to propagate neural gradients (loss gradients with respect to the intermediate neural layer outputs) back throughout the model, most previous works focus on the quantization/pruning of weights and activations. These methods are often not applicable to neural gradients, which have very different statistical properties. Distinguished from weights and activations, we find that the distribution of neural gradients is approximately lognormal. Considering this, we suggest two closed-form analytical methods to reduce the computational and memory burdens of neural gradients. The first method optimizes the floating-point format and scale of the gradients. The second method accurately sets sparsity thresholds for gradient pruning. Each method achieves state-of-the-art results on ImageNet. To the best of our knowledge, this paper is the first to (1) quantize the gradients to 6-bit floating-point formats, or (2) achieve up to 85% gradient sparsity --- in each case without accuracy degradation. Reference implementation accompanies the paper in the supplementary material",
    "checked": true,
    "id": "9df893d2b8ac20271c0317a72e97cc9720954f0b",
    "semantic_title": "neural gradients are near-lognormal: improved quantized and sparse training",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=e8W-hsu_q5": {
    "title": "Group Equivariant Conditional Neural Processes",
    "volume": "poster",
    "abstract": "We present the group equivariant conditional neural process (EquivCNP), a meta-learning method with permutation invariance in a data set as in conventional conditional neural processes (CNPs), and it also has transformation equivariance in data space. Incorporating group equivariance, such as rotation and scaling equivariance, provides a way to consider the symmetry of real-world data. We give a decomposition theorem for permutation-invariant and group-equivariant maps, which leads us to construct EquivCNPs with an infinite-dimensional latent space to handle group symmetries. In this paper, we build architecture using Lie group convolutional layers for practical implementation. We show that EquivCNP with translation equivariance achieves comparable performance to conventional CNPs in a 1D regression task. Moreover, we demonstrate that incorporating an appropriate Lie group equivariance, EquivCNP is capable of zero-shot generalization for an image-completion task by selecting an appropriate Lie group equivariance",
    "checked": true,
    "id": "bc716f31b93eac93f627e04039da7f6c4d126165",
    "semantic_title": "group equivariant conditional neural processes",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=3hGNqpI4WS": {
    "title": "Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization",
    "volume": "poster",
    "abstract": "Most reinforcement learning (RL) algorithms assume online access to the environment, in which one may readily interleave updates to the policy with experience collection using that policy. However, in many real-world applications such as health, education, dialogue agents, and robotics, the cost or potential risk of deploying a new data-collection policy is high, to the point that it can become prohibitive to update the data-collection policy more than a few times during learning. With this view, we propose a novel concept of deployment efficiency, measuring the number of distinct data-collection policies that are used during policy learning. We observe that naïvely applying existing model-free offline RL algorithms recursively does not lead to a practical deployment-efficient and sample-efficient algorithm. We propose a novel model-based algorithm, Behavior-Regularized Model-ENsemble (BREMEN), that not only performs better than or comparably as the state-of-the-art dynamic-programming-based and concurrently-proposed model-based offline approaches on existing benchmarks, but can also effectively optimize a policy offline using 10-20 times fewer data than prior works. Furthermore, the recursive application of BREMEN achieves impressive deployment efficiency while maintaining the same or better sample efficiency, learning successful policies from scratch on simulated robotic environments with only 5-10 deployments, compared to typical values of hundreds to millions in standard RL baselines",
    "checked": true,
    "id": "79ebde314ab90d066cee3b82193ef05666323394",
    "semantic_title": "deployment-efficient reinforcement learning via model-based offline optimization",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=tnSo6VRLmT": {
    "title": "Efficient Conformal Prediction via Cascaded Inference with Expanded Admission",
    "volume": "poster",
    "abstract": "In this paper, we present a novel approach for conformal prediction (CP), in which we aim to identify a set of promising prediction candidates---in place of a single prediction. This set is guaranteed to contain a correct answer with high probability, and is well-suited for many open-ended classification tasks. In the standard CP paradigm, the predicted set can often be unusably large and also costly to obtain. This is particularly pervasive in settings where the correct answer is not unique, and the number of total possible answers is high. We first expand the CP correctness criterion to allow for additional, inferred \"admissible\" answers, which can substantially reduce the size of the predicted set while still providing valid performance guarantees. Second, we amortize costs by conformalizing prediction cascades, in which we aggressively prune implausible labels early on by using progressively stronger classifiers---again, while still providing valid performance guarantees. We demonstrate the empirical effectiveness of our approach for multiple applications in natural language processing and computational chemistry for drug discovery",
    "checked": true,
    "id": "5c5683527a738c3051eaa8dc02322632fea4aa5c",
    "semantic_title": "efficient conformal prediction via cascaded inference with expanded admission",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=sTeoJiB4uR": {
    "title": "Reducing the Computational Cost of Deep Generative Models with Binary Neural Networks",
    "volume": "poster",
    "abstract": "Deep generative models provide a powerful set of tools to understand real-world data. But as these models improve, they increase in size and complexity, so their computational cost in memory and execution time grows. Using binary weights in neural networks is one method which has shown promise in reducing this cost. However, whether binary neural networks can be used in generative models is an open problem. In this work we show, for the first time, that we can successfully train generative models which utilize binary neural networks. This reduces the computational cost of the models massively. We develop a new class of binary weight normalization, and provide insights for architecture designs of these binarized generative models. We demonstrate that two state-of-the-art deep generative models, the ResNet VAE and Flow++ models, can be binarized effectively using these techniques. We train binary models that achieve loss values close to those of the regular models but are 90%-94% smaller in size, and also allow significant speed-ups in execution time",
    "checked": true,
    "id": "0daf387e612363a3746778326aaaa49467b5e9d6",
    "semantic_title": "reducing the computational cost of deep generative models with binary neural networks",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=0aW6lYOYB7d": {
    "title": "Large-width functional asymptotics for deep Gaussian neural networks",
    "volume": "poster",
    "abstract": "In this paper, we consider fully connected feed-forward deep neural networks where weights and biases are independent and identically distributed according to Gaussian distributions. Extending previous results (Matthews et al., 2018a;b;Yang, 2019) we adopt a function-space perspective, i.e. we look at neural networks as infinite-dimensional random elements on the input space $\\mathbb{R}^I$. Under suitable assumptions on the activation function we show that: i) a network defines a continuous Gaussian process on the input space $\\mathbb{R}^I$; ii) a network with re-scaled weights converges weakly to a continuous Gaussian process in the large-width limit; iii) the limiting Gaussian process has almost surely locally $\\gamma$-Hölder continuous paths, for $0 < \\gamma <1$. Our results contribute to recent theoretical studies on the interplay between infinitely wide deep neural networks and Gaussian processes by establishing weak convergence in function-space with respect to a stronger metric",
    "checked": true,
    "id": "e7174887d624d6b2533061861bba18815f314cf0",
    "semantic_title": "large-width functional asymptotics for deep gaussian neural networks",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=9z_dNsC4B5t": {
    "title": "MetaNorm: Learning to Normalize Few-Shot Batches Across Domains",
    "volume": "poster",
    "abstract": "Batch normalization plays a crucial role when training deep neural networks. However, batch statistics become unstable with small batch sizes and are unreliable in the presence of distribution shifts. We propose MetaNorm, a simple yet effective meta-learning normalization. It tackles the aforementioned issues in a unified way by leveraging the meta-learning setting and learns to infer adaptive statistics for batch normalization. MetaNorm is generic, flexible and model-agnostic, making it a simple plug-and-play module that is seamlessly embedded into existing meta-learning approaches. It can be efficiently implemented by lightweight hypernetworks with low computational cost. We verify its effectiveness by extensive evaluation on representative tasks suffering from the small batch and domain shift problems: few-shot learning and domain generalization. We further introduce an even more challenging setting: few-shot domain generalization. Results demonstrate that MetaNorm consistently achieves better, or at least competitive, accuracy compared to existing batch normalization methods",
    "checked": true,
    "id": "e3db1323a61cf02daa93d936bd6c02426df82b07",
    "semantic_title": "metanorm: learning to normalize few-shot batches across domains",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=QpNz8r_Ri2Y": {
    "title": "Representation Balancing Offline Model-based Reinforcement Learning",
    "volume": "poster",
    "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks",
    "checked": true,
    "id": "61ce91bd0a925bb24c4d236fd14a5f27c3f5b43f",
    "semantic_title": "representation balancing offline model-based reinforcement learning",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=AWOSz_mMAPx": {
    "title": "Local Convergence Analysis of Gradient Descent Ascent with Finite Timescale Separation",
    "volume": "poster",
    "abstract": "We study the role that a finite timescale separation parameter $\\tau$ has on gradient descent-ascent in non-convex, non-concave zero-sum games where the learning rate of player 1 is denoted by $\\gamma_1$ and the learning rate of player 2 is defined to be $\\gamma_2=\\tau\\gamma_1$. We provide a non-asymptotic construction of the finite timescale separation parameter $\\tau^{\\ast}$ such that gradient descent-ascent locally converges to $x^{\\ast}$ for all $\\tau \\in (\\tau^{\\ast}, \\infty)$ if and only if it is a strict local minmax equilibrium. Moreover, we provide explicit local convergence rates given the finite timescale separation. The convergence results we present are complemented by a non-convergence result: given a critical point $x^{\\ast}$ that is not a strict local minmax equilibrium, we present a non-asymptotic construction of a finite timescale separation $\\tau_{0}$ such that gradient descent-ascent with timescale separation $\\tau\\in (\\tau_0, \\infty)$ does not converge to $x^{\\ast}$. Finally, we extend the results to gradient penalty regularization methods for generative adversarial networks and empirically demonstrate on CIFAR-10 and CelebA the significant impact timescale separation has on training performance",
    "checked": true,
    "id": "f4465442a9b850a2c5b71a63fff0d24396b15f2c",
    "semantic_title": "local convergence analysis of gradient descent ascent with finite timescale separation",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=E3Ys6a1NTGT": {
    "title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization",
    "volume": "poster",
    "abstract": "We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments",
    "checked": true,
    "id": "86fa0f350b0ede3a86e31aa2900af551531ee570",
    "semantic_title": "the importance of pessimism in fixed-dataset policy optimization",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=9EKHN1jOlA": {
    "title": "Uncertainty Estimation and Calibration with Finite-State Probabilistic RNNs",
    "volume": "poster",
    "abstract": "Uncertainty quantification is crucial for building reliable and trustable machine learning systems. We propose to estimate uncertainty in recurrent neural networks (RNNs) via stochastic discrete state transitions over recurrent timesteps. The uncertainty of the model can be quantified by running a prediction several times, each time sampling from the recurrent state transition distribution, leading to potentially different results if the model is uncertain. Alongside uncertainty quantification, our proposed method offers several advantages in different settings. The proposed method can (1) learn deterministic and probabilistic automata from data, (2) learn well-calibrated models on real-world classification tasks, (3) improve the performance of out-of-distribution detection, and (4) control the exploration-exploitation trade-off in reinforcement learning. An implementation is available",
    "checked": true,
    "id": "a83e36e203a4e90c2af814320092fc8f78f4ce28",
    "semantic_title": "uncertainty estimation and calibration with finite-state probabilistic rnns",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=jrA5GAccy_": {
    "title": "Empirical or Invariant Risk Minimization? A Sample Complexity Perspective",
    "volume": "poster",
    "abstract": "Recently, invariant risk minimization (IRM) was proposed as a promising solution to address out-of-distribution (OOD) generalization. However, it is unclear when IRM should be preferred over the widely-employed empirical risk minimization (ERM) framework. In this work, we analyze both these frameworks from the perspective of sample complexity, thus taking a firm step towards answering this important question. We find that depending on the type of data generation mechanism, the two approaches might have very different finite sample and asymptotic behavior. For example, in the covariate shift setting we see that the two approaches not only arrive at the same asymptotic solution, but also have similar finite sample behavior with no clear winner. For other distribution shifts such as those involving confounders or anti-causal variables, however, the two approaches arrive at different asymptotic solutions where IRM is guaranteed to be close to the desired OOD solutions in the finite sample regime, while ERM is biased even asymptotically. We further investigate how different factors --- the number of environments, complexity of the model, and IRM penalty weight --- impact the sample complexity of IRM in relation to its distance from the OOD solutions",
    "checked": true,
    "id": "fbf50ed8e09b3268770029af30b01d31973f77d0",
    "semantic_title": "empirical or invariant risk minimization? a sample complexity perspective",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=zeFrfgyZln": {
    "title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval",
    "volume": "poster",
    "abstract": "Conducting text retrieval in a learned dense representation space has many intriguing advantages. Yet dense retrieval (DR) often underperforms word-based sparse retrieval. In this paper, we first theoretically show the bottleneck of dense retrieval is the domination of uninformative negatives sampled in mini-batch training, which yield diminishing gradient norms, large gradient variances, and slow convergence. We then propose Approximate nearest neighbor Negative Contrastive Learning (ANCE), which selects hard training negatives globally from the entire corpus. Our experiments demonstrate the effectiveness of ANCE on web search, question answering, and in a commercial search engine, showing ANCE dot-product retrieval nearly matches the accuracy of BERT-based cascade IR pipeline. We also empirically validate our theory that negative sampling with ANCE better approximates the oracle importance sampling procedure and improves learning convergence",
    "checked": true,
    "id": "c9b8593db099869fe7254aa1fa53f3c9073b0176",
    "semantic_title": "approximate nearest neighbor negative contrastive learning for dense text retrieval",
    "citation_count": 1281,
    "authors": []
  },
  "https://openreview.net/forum?id=YNnpaAKeCfx": {
    "title": "FairBatch: Batch Selection for Model Fairness",
    "volume": "poster",
    "abstract": "Training a fair machine learning model is essential to prevent demographic disparity. Existing techniques for improving model fairness require broad changes in either data preprocessing or model training, rendering themselves difficult-to-adopt for potentially already complex machine learning systems. We address this problem via the lens of bilevel optimization. While keeping the standard training algorithm as an inner optimizer, we incorporate an outer optimizer so as to equip the inner problem with an additional functionality: Adaptively selecting minibatch sizes for the purpose of improving model fairness. Our batch selection algorithm, which we call FairBatch, implements this optimization and supports prominent fairness measures: equal opportunity, equalized odds, and demographic parity. FairBatch comes with a significant implementation benefit -- it does not require any modification to data preprocessing or model training. For instance, a single-line change of PyTorch code for replacing batch selection part of model training suffices to employ FairBatch. Our experiments conducted both on synthetic and benchmark real data demonstrate that FairBatch can provide such functionalities while achieving comparable (or even greater) performances against the state of the arts. Furthermore, FairBatch can readily improve fairness of any pre-trained model simply via fine-tuning. It is also compatible with existing batch selection techniques intended for different purposes, such as faster convergence, thus gracefully achieving multiple purposes",
    "checked": true,
    "id": "d688594d2aac080f657b7be251e89cca6a7df165",
    "semantic_title": "fairbatch: batch selection for model fairness",
    "citation_count": 134,
    "authors": []
  },
  "https://openreview.net/forum?id=tIjRAiFmU3y": {
    "title": "An Unsupervised Deep Learning Approach for Real-World Image Denoising",
    "volume": "poster",
    "abstract": "Designing an unsupervised image denoising approach in practical applications is a challenging task due to the complicated data acquisition process. In the real-world case, the noise distribution is so complex that the simplified additive white Gaussian (AWGN) assumption rarely holds, which significantly deteriorates the Gaussian denoisers' performance. To address this problem, we apply a deep neural network that maps the noisy image into a latent space in which the AWGN assumption holds, and thus any existing Gaussian denoiser is applicable. More specifically, the proposed neural network consists of the encoder-decoder structure and approximates the likelihood term in the Bayesian framework. Together with a Gaussian denoiser, the neural network can be trained with the input image itself and does not require any pre-training in other datasets. Extensive experiments on real-world noisy image datasets have shown that the combination of neural networks and Gaussian denoisers improves the performance of the original Gaussian denoisers by a large margin. In particular, the neural network+BM3D method significantly outperforms other unsupervised denoising approaches and is competitive with supervised networks such as DnCNN, FFDNet, and CBDNet",
    "checked": true,
    "id": "51da598da3fc475884299ff498e224abd333a87b",
    "semantic_title": "an unsupervised deep learning approach for real-world image denoising",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=WesiCoRVQ15": {
    "title": "When Optimizing f -Divergence is Robust with Label Noise",
    "volume": "poster",
    "abstract": "We show when maximizing a properly defined $f$-divergence measure with respect to a classifier's predictions and the supervised labels is robust with label noise. Leveraging its variational form, we derive a nice decoupling property for a family of $f$-divergence measures when label noise presents, where the divergence is shown to be a linear combination of the variational difference defined on the clean distribution and a bias term introduced due to the noise. The above derivation helps us analyze the robustness of different $f$-divergence functions. With established robustness, this family of $f$-divergence functions arises as useful metrics for the problem of learning with noisy labels, which do not require the specification of the labels' noise rate. When they are possibly not robust, we propose fixes to make them so. In addition to the analytical results, we present thorough experimental evidence. Our code is available at https://github.com/UCSC-REAL/Robust-f-divergence-measures",
    "checked": true,
    "id": "0abff557bf1c38a1fbbbeaa01dea66cd1ac5e988",
    "semantic_title": "when optimizing f-divergence is robust with label noise",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=-QxT4mJdijq": {
    "title": "Meta-learning Symmetries by Reparameterization",
    "volume": "poster",
    "abstract": "Many successful deep learning architectures are equivariant to certain transformations in order to conserve parameters and improve generalization: most famously, convolution layers are equivariant to shifts of the input. This approach only works when practitioners know the symmetries of the task and can manually construct an architecture with the corresponding equivariances. Our goal is an approach for learning equivariances from data, without needing to design custom task-specific architectures. We present a method for learning and encoding equivariances into networks by learning corresponding parameter sharing patterns from data. Our method can provably represent equivariance-inducing parameter sharing for any finite group of symmetry transformations. Our experiments suggest that it can automatically learn to encode equivariances to common transformations used in image processing tasks",
    "checked": true,
    "id": "2e072998dd7b40e9e514b2bb43c8074bd5aa43d2",
    "semantic_title": "meta-learning symmetries by reparameterization",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=GH7QRzUDdXG": {
    "title": "A Geometric Analysis of Deep Generative Image Models and Its Applications",
    "volume": "poster",
    "abstract": "Generative adversarial networks (GANs) have emerged as a powerful unsupervised method to model the statistical patterns of real-world data sets, such as natural images. These networks are trained to map random inputs in their latent space to new samples representative of the learned data. However, the structure of the latent space is hard to intuit due to its high dimensionality and the non-linearity of the generator, which limits the usefulness of the models. Understanding the latent space requires a way to identify input codes for existing real-world images (inversion), and a way to identify directions with known image transformations (interpretability). Here, we use a geometric framework to address both issues simultaneously. We develop an architecture-agnostic method to compute the Riemannian metric of the image manifold created by GANs. The eigen-decomposition of the metric isolates axes that account for different levels of image variability. An empirical analysis of several pretrained GANs shows that image variation around each position is concentrated along surprisingly few major axes (the space is highly anisotropic) and the directions that create this large variation are similar at different positions in the space (the space is homogeneous). We show that many of the top eigenvectors correspond to interpretable transforms in the image space, with a substantial part of eigenspace corresponding to minor transforms which could be compressed out. This geometric understanding unifies key previous results related to GAN interpretability. We show that the use of this metric allows for more efficient optimization in the latent space (e.g. GAN inversion) and facilitates unsupervised discovery of interpretable axes. Our results illustrate that defining the geometry of the GAN image manifold can serve as a general framework for understanding GANs",
    "checked": true,
    "id": "c4c57c766bdfab05f3fe5b135073cb188e7fcf63",
    "semantic_title": "a geometric analysis of deep generative image models and its applications",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=tC6iW2UUbJf": {
    "title": "What Makes Instance Discrimination Good for Transfer Learning?",
    "volume": "poster",
    "abstract": "Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation. It comes as a surprise that image annotations would be better left unused for transfer learning. In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models? From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations. Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category",
    "checked": true,
    "id": "199d88fb9ec7430ca653f4c066b02aa7c3b4dd98",
    "semantic_title": "what makes instance discrimination good for transfer learning?",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=EMHoBG0avc1": {
    "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval",
    "volume": "poster",
    "abstract": "We propose a simple and efficient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previous work, our method does not require access to any corpus-specific information, such as inter-document hyperlinks or human-annotated entity markers, and can be applied to any unstructured text corpus. Our system also yields a much better efficiency-accuracy trade-off, matching the best published accuracy on HotpotQA while being 10 times faster at inference time",
    "checked": true,
    "id": "3684491d62db5c3e5602375271e4b339bbf416ee",
    "semantic_title": "answering complex open-domain questions with multi-hop dense retrieval",
    "citation_count": 198,
    "authors": []
  },
  "https://openreview.net/forum?id=xgGS6PmzNq6": {
    "title": "On Dyadic Fairness: Exploring and Mitigating Bias in Graph Connections",
    "volume": "poster",
    "abstract": "Disparate impact has raised serious concerns in machine learning applications and its societal impacts. In response to the need of mitigating discrimination, fairness has been regarded as a crucial property in algorithmic design. In this work, we study the problem of disparate impact on graph-structured data. Specifically, we focus on dyadic fairness, which articulates a fairness concept that a predictive relationship between two instances should be independent of the sensitive attributes. Based on this, we theoretically relate the graph connections to dyadic fairness on link predictive scores in learning graph neural networks, and reveal that regulating weights on existing edges in a graph contributes to dyadic fairness conditionally. Subsequently, we propose our algorithm, \\textbf{FairAdj}, to empirically learn a fair adjacency matrix with proper graph structural constraints for fair link prediction, and in the meanwhile preserve predictive accuracy as much as possible. Empirical validation demonstrates that our method delivers effective dyadic fairness in terms of various statistics, and at the same time enjoys a favorable fairness-utility tradeoff",
    "checked": true,
    "id": "4742859e6e27fe5eda18dbe14752ed6a4871d356",
    "semantic_title": "on dyadic fairness: exploring and mitigating bias in graph connections",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=CF-ZIuSMXRz": {
    "title": "Spatio-Temporal Graph Scattering Transform",
    "volume": "poster",
    "abstract": "Although spatio-temporal graph neural networks have achieved great empirical success in handling multiple correlated time series, they may be impractical in some real-world scenarios due to a lack of sufficient high-quality training data. Furthermore, spatio-temporal graph neural networks lack theoretical interpretation. To address these issues, we put forth a novel mathematically designed framework to analyze spatio-temporal data. Our proposed spatio-temporal graph scattering transform (ST-GST) extends traditional scattering transform to the spatio-temporal domain. It performs iterative applications of spatio-temporal graph wavelets and nonlinear activation functions, which can be viewed as a forward pass of spatio-temporal graph convolutional networks without training. Since all the filter coefficients in ST-GST are mathematically designed, it is promising for the real-world scenarios with limited training data, and also allows for a theoretical analysis, which shows that the proposed ST-GST is stable to small perturbations of input signals and structures. Finally, our experiments show that i) ST-GST outperforms spatio-temporal graph convolutional networks by an increase of 35% in accuracy for MSR Action3D dataset; ii) it is better and computationally more efficient to design the transform based on separable spatio-temporal graphs than the joint ones; and iii) nonlinearity in ST-GST is critical to empirical performance",
    "checked": true,
    "id": "29d22a4939e3497d5a9b6a3e1c47584854a46000",
    "semantic_title": "spatio-temporal graph scattering transform",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=XjYgR6gbCEc": {
    "title": "MODALS: Modality-agnostic Automated Data Augmentation in the Latent Space",
    "volume": "poster",
    "abstract": "Data augmentation is an efficient way to expand a training dataset by creating additional artificial data. While data augmentation is found to be effective in improving the generalization capabilities of models for various machine learning tasks, the underlying augmentation methods are usually manually designed and carefully evaluated for each data modality separately, like image processing functions for image data and word-replacing rules for text data. In this work, we propose an automated data augmentation approach called MODALS (Modality-agnostic Automated Data Augmentation in the Latent Space) to augment data for any modality in a generic way. MODALS exploits automated data augmentation to fine-tune four universal data transformation operations in the latent space to adapt the transform to data of different modalities. Through comprehensive experiments, we demonstrate the effectiveness of MODALS on multiple datasets for text, tabular, time-series and image modalities",
    "checked": true,
    "id": "d1b11e6c58e4a66ecb4f0dfd20a21b4099c4ede5",
    "semantic_title": "modals: modality-agnostic automated data augmentation in the latent space",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=0IOX0YcCdTn": {
    "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
    "volume": "poster",
    "abstract": "Given a simple request like Put a washed apple in the kitchen fridge, humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text-based policies in TextWorld (Côté et al., 2018) and then execute goals from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, and visual scene understanding)",
    "checked": true,
    "id": "398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
    "semantic_title": "alfworld: aligning text and embodied environments for interactive learning",
    "citation_count": 490,
    "authors": []
  },
  "https://openreview.net/forum?id=jXe91kq3jAq": {
    "title": "Latent Skill Planning for Exploration and Transfer",
    "volume": "poster",
    "abstract": "To quickly solve new tasks in complex environments, intelligent agents need to build up reusable knowledge. For example, a learned world model captures knowledge about the environment that applies to new tasks. Similarly, skills capture general behaviors that can apply to new tasks. In this paper, we investigate how these two approaches can be integrated into a single reinforcement learning agent. Specifically, we leverage the idea of partial amortization for fast adaptation at test time. For this, actions are produced by a policy that is learned over time while the skills it conditions on are chosen using online planning. We demonstrate the benefits of our design decisions across a suite of challenging locomotion tasks and demonstrate improved sample efficiency in single tasks as well as in transfer from one task to another, as compared to competitive baselines. Videos are available at: https://sites.google.com/view/latent-skill-planning/",
    "checked": true,
    "id": "22a8ab2f4cd0777ebc93d8e414535c03d4d57615",
    "semantic_title": "latent skill planning for exploration and transfer",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=djwS0m4Ft_A": {
    "title": "Evaluating the Disentanglement of Deep Generative Models through Manifold Topology",
    "volume": "poster",
    "abstract": "Learning disentangled representations is regarded as a fundamental task for improving the generalization, robustness, and interpretability of generative models. However, measuring disentanglement has been challenging and inconsistent, often dependent on an ad-hoc external model or specific to a certain dataset. To address this, we present a method for quantifying disentanglement that only uses the generative model, by measuring the topological similarity of conditional submanifolds in the learned representation. This method showcases both unsupervised and supervised variants. To illustrate the effectiveness and applicability of our method, we empirically evaluate several state-of-the-art models across multiple datasets. We find that our method ranks models similarly to existing methods. We make our code publicly available at https://github.com/stanfordmlgroup/disentanglement",
    "checked": true,
    "id": "6363dd8404cad824f293ecc7cf67fe89330733ad",
    "semantic_title": "evaluating the disentanglement of deep generative models through manifold topology",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=g11CZSghXyY": {
    "title": "Combining Ensembles and Data Augmentation Can Harm Your Calibration",
    "volume": "poster",
    "abstract": "Ensemble methods which average over multiple neural network predictions are a simple approach to improve a model's calibration and robustness. Similarly, data augmentation techniques, which encode prior information in the form of invariant feature transformations, are effective for improving calibration and robustness. In this paper, we show a surprising pathology: combining ensembles and data augmentation can harm model calibration. This leads to a trade-off in practice, whereby improved accuracy by combining the two techniques comes at the expense of calibration. On the other hand, selecting only one of the techniques ensures good uncertainty estimates at the expense of accuracy. We investigate this pathology and identify a compounding under-confidence among methods which marginalize over sets of weights and data augmentation techniques which soften labels. Finally, we propose a simple correction, achieving the best of both worlds with significant accuracy and calibration gains over using only ensembles or data augmentation individually. Applying the correction produces new state-of-the art in uncertainty calibration and robustness across CIFAR-10, CIFAR-100, and ImageNet",
    "checked": true,
    "id": "8bc63c8bd96b40d8d0f5329e06e0a96eaf214c7f",
    "semantic_title": "combining ensembles and data augmentation can harm your calibration",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=eom0IUrF__F": {
    "title": "CoCo: Controllable Counterfactuals for Evaluating Dialogue State Trackers",
    "volume": "poster",
    "abstract": "Dialogue state trackers have made significant progress on benchmark datasets, but their generalization capability to novel and realistic scenarios beyond the held- out conversations is less understood. We propose controllable counterfactuals (COCO) to bridge this gap and evaluate dialogue state tracking (DST) models on novel scenarios, i.e., would the system successfully tackle the request if the user responded differently but still consistently with the dialogue flow? COCO leverages turn-level belief states as counterfactual conditionals to produce novel conversation scenarios in two steps: (i) counterfactual goal generation at turn- level by dropping and adding slots followed by replacing slot values, (ii) counterfactual conversation generation that is conditioned on (i) and consistent with the dialogue flow. Evaluating state-of-the-art DST models on MultiWOZ dataset with COCO-generated counterfactuals results in a significant performance drop of up to 30.8% (from 49.4% to 18.6%) in absolute joint goal accuracy. In comparison, widely used techniques like paraphrasing only affect the accuracy by at most 2%. Human evaluations show that COCO-generated conversations perfectly reflect the underlying user goal with more than 95% accuracy and are as human-like as the original conversations, further strengthening its reliability and promise to be adopted as part of the robustness evaluation of DST models",
    "checked": true,
    "id": "16e4a6b20f1d8bff37fcfd5671e21a13b41d242f",
    "semantic_title": "coco: controllable counterfactuals for evaluating dialogue state trackers",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=QkRbdiiEjM": {
    "title": "AdaGCN: Adaboosting Graph Convolutional Networks into Deep Models",
    "volume": "poster",
    "abstract": "The design of deep graph models still remains to be investigated and the crucial part is how to explore and exploit the knowledge from different hops of neighbors in an efficient way. In this paper, we propose a novel RNN-like deep graph neural network architecture by incorporating AdaBoost into the computation of network; and the proposed graph convolutional network called AdaGCN~(Adaboosting Graph Convolutional Network) has the ability to efficiently extract knowledge from high-order neighbors of current nodes and then integrates knowledge from different hops of neighbors into the network in an Adaboost way. Different from other graph neural networks that directly stack many graph convolution layers, AdaGCN shares the same base neural network architecture among all ``layers'' and is recursively optimized, which is similar to an RNN. Besides, We also theoretically established the connection between AdaGCN and existing graph convolutional methods, presenting the benefits of our proposal. Finally, extensive experiments demonstrate the consistent state-of-the-art prediction performance on graphs across different label rates and the computational advantage of our approach AdaGCN~\\footnote{Code is available at \\url{https://github.com/datake/AdaGCN}.}",
    "checked": true,
    "id": "6c6c265c6d1de08f03fed0da0604bef5307fbcec",
    "semantic_title": "adagcn: adaboosting graph convolutional networks into deep models",
    "citation_count": 86,
    "authors": []
  },
  "https://openreview.net/forum?id=kBVJ2NtiY-": {
    "title": "Learning What To Do by Simulating the Past",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4a78cd72b184bec55e05db0e2df25d63765de068",
    "semantic_title": "learning what to do by simulating the past",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=PrzjugOsDeE": {
    "title": "CcGAN: Continuous Conditional Generative Adversarial Networks for Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f70a9bcd4cc1dffdcb2611441db2a70047a750b1",
    "semantic_title": "ccgan: continuous conditional generative adversarial networks for image generation",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=7I12hXRi8F": {
    "title": "ANOCE: Analysis of Causal Effects with Multiple Mediators via Constrained Structural Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d22037765e1aae7beaf6909562e29971fc240844",
    "semantic_title": "anoce: analysis of causal effects with multiple mediators via constrained structural learning",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=3X64RLgzY6O": {
    "title": "Direction Matters: On the Implicit Bias of Stochastic Gradient Descent with Moderate Learning Rate",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "89cd1a37012317393ad230ef3b93a286978649f0",
    "semantic_title": "direction matters: on the implicit regularization effect of stochastic gradient descent with moderate learning rate",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=Eql5b1_hTE4": {
    "title": "Robust early-learning: Hindering the memorization of noisy labels",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bafb088c459188fd22fc20eb3af6b731d4856629",
    "semantic_title": "robust early-learning: hindering the memorization of noisy labels",
    "citation_count": 263,
    "authors": []
  },
  "https://openreview.net/forum?id=GMgHyUPrXa": {
    "title": "A Design Space Study for LISTA and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fd5b0172edfbc55f8ca8e165aa1e9e29a541c6e2",
    "semantic_title": "a design space study for lista and beyond",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Qk-Wq5AIjpq": {
    "title": "PAC Confidence Predictions for Deep Neural Network Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3c3d007b159a74a08f75c1190722790d4930ddee",
    "semantic_title": "pac confidence predictions for deep neural network classifiers",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=LiX3ECzDPHZ": {
    "title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8c6dc25db6c2e5e90b03d0ecd328b74e3f4cd0c4",
    "semantic_title": "x2t: training an x-to-text typing interface with online learning from user feedback",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=WEHSlH5mOk": {
    "title": "Discrete Graph Structure Learning for Forecasting Multiple Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "401930b2dc738a5a67d136bc9a2d04461c5bf93a",
    "semantic_title": "discrete graph structure learning for forecasting multiple time series",
    "citation_count": 253,
    "authors": []
  },
  "https://openreview.net/forum?id=CGQ6ENUMX6": {
    "title": "Task-Agnostic Morphology Evolution",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f89af36631d45126faf2d23b81c4a767d4f91d56",
    "semantic_title": "task-agnostic morphology evolution",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=aDjoksTpXOP": {
    "title": "Deep Equals Shallow for ReLU Networks in Kernel Regimes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "959c999a48125ddcceba3cc1b0fa62bbbf89c745",
    "semantic_title": "deep equals shallow for relu networks in kernel regimes",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=5jzlpHvvRk": {
    "title": "Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3c49b14137703eb2fd777bdc4896918c0db38d4f",
    "semantic_title": "loss function discovery for object detection via convergence-simulation driven search",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=jxdXSW9Doc": {
    "title": "Effective Distributed Learning with Random Features: Improved Bounds and Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "18c1ebb93ef9625c80c1324760840028c1185c34",
    "semantic_title": "effective distributed learning with random features: improved bounds and algorithms",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=AhElGnhU2BV": {
    "title": "On InstaHide, Phase Retrieval, and Sparse Matrix Factorization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5acca7c62ef943d46ff298426d5af310aa5b02de",
    "semantic_title": "on instahide, phase retrieval, and sparse matrix factorization",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=Vd7lCMvtLqg": {
    "title": "Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "39d5aec7ccd7d03037118ce6ae08e2b9318ba0b3",
    "semantic_title": "anchor & transform: learning sparse embeddings for large vocabularies",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=jDdzh5ul-d": {
    "title": "Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "433000baf18bb4403681fde5740bccd1fa2034a9",
    "semantic_title": "achieving linear speedup with partial worker participation in non-iid federated learning",
    "citation_count": 268,
    "authors": []
  },
  "https://openreview.net/forum?id=vK9WrZ0QYQ": {
    "title": "Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4ddc0757d280e51524fadcfec150be11f9b48fde",
    "semantic_title": "deep neural tangent kernel and laplace kernel have the same rkhs",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=w2mYg3d0eot": {
    "title": "Fast convergence of stochastic subgradient method under interpolation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fcc7a7305a1b502bc3f40462c06bd5c61c5cd0a7",
    "semantic_title": "fast convergence of stochastic subgradient method under interpolation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=PH5PH9ZO_4": {
    "title": "Generating Adversarial Computer Programs using Optimized Obfuscations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2e05413a737a7fe823c97e12c2ddc10d4a4c9dc0",
    "semantic_title": "generating adversarial computer programs using optimized obfuscations",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=tc5qisoB-C": {
    "title": "C-Learning: Learning to Achieve Goals via Recursive Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f0901642e339d17b3eb66daae112f5d62556c637",
    "semantic_title": "c-learning: learning to achieve goals via recursive classification",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=eqBwg3AcIAK": {
    "title": "Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ae712addbdd969c3d1f207fcaec1ce9f8b8741e3",
    "semantic_title": "off-dynamics reinforcement learning: training for transfer with domain classifiers",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=KtH8W3S_RE": {
    "title": "Multi-resolution modeling of a discrete stochastic process identifies causes of cancer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3d9a3486ca5ed6ab332f361f5440e147c8cc5e2f",
    "semantic_title": "multi-resolution modeling of a discrete stochastic process identifies causes of cancer",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=8Ln-Bq0mZcy": {
    "title": "On the Critical Role of Conventions in Adaptive Human-AI Collaboration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "53d035f903070b8b8ea47be79f9957738f63373b",
    "semantic_title": "on the critical role of conventions in adaptive human-ai collaboration",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=NsMLjcFaO8O": {
    "title": "WaveGrad: Estimating Gradients for Waveform Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "685af6d2bcdff7170574643b2c5ab4fbcc36f597",
    "semantic_title": "wavegrad: estimating gradients for waveform generation",
    "citation_count": 810,
    "authors": []
  },
  "https://openreview.net/forum?id=FOyuZ26emy": {
    "title": "A Critique of Self-Expressive Deep Subspace Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0f49c53693d0c4a6cd9e1a20ff6b8d58491d2858",
    "semantic_title": "a critique of self-expressive deep subspace clustering",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=unI5ucw_Jk": {
    "title": "Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "31189438c9f40905e0d87ecc3a88e88c2165c015",
    "semantic_title": "explaining by imitating: understanding decisions by interpretable policy learning",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=Srmggo3b3X6": {
    "title": "For self-supervised learning, Rationality implies generalization, provably",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e2c80fd059ba9986da37e498330979dc5976beb7",
    "semantic_title": "for self-supervised learning, rationality implies generalization, provably",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=JbuYF437WB6": {
    "title": "Directed Acyclic Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c6337dc83db09c9648ae850c71937eb8e5fd7a43",
    "semantic_title": "directed acyclic graph neural networks",
    "citation_count": 112,
    "authors": []
  },
  "https://openreview.net/forum?id=N3zUDGN5lO": {
    "title": "My Body is a Cage: the Role of Morphology in Graph-Based Incompatible Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "85cb1d20ae5ba3f547dc76d2cc73677652900d9b",
    "semantic_title": "my body is a cage: the role of morphology in graph-based incompatible control",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=3SV-ZePhnZM": {
    "title": "Incremental few-shot learning via vector quantization in deep embedded space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4b7a308de0bb4ed36584e7bc1bf7b15b07d5e40e",
    "semantic_title": "incremental few-shot learning via vector quantization in deep embedded space",
    "citation_count": 106,
    "authors": []
  },
  "https://openreview.net/forum?id=cO1IH43yUF": {
    "title": "Revisiting Few-sample BERT Fine-tuning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "056935031bc5cf0aeeaa0946320de26e14a1817e",
    "semantic_title": "revisiting few-sample bert fine-tuning",
    "citation_count": 450,
    "authors": []
  },
  "https://openreview.net/forum?id=84gjULz1t5": {
    "title": "Linear Convergent Decentralized Optimization with Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2502dc4523fe70b02857cb58174dda314eb0a8bf",
    "semantic_title": "linear convergent decentralized optimization with compression",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=Ldau9eHU-qO": {
    "title": "Learning from Demonstration with Weakly Supervised Disentanglement",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "31727b59155cd883106a18a57c631a958e394a35",
    "semantic_title": "learning from demonstration with weakly supervised disentanglement",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=wta_8Hx2KD": {
    "title": "Incorporating Symmetry into Deep Dynamics Models for Improved Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "412d0b5aa97e0713259d203e76f8a57b0357e0c9",
    "semantic_title": "incorporating symmetry into deep dynamics models for improved generalization",
    "citation_count": 181,
    "authors": []
  },
  "https://openreview.net/forum?id=BbNIbVPJ-42": {
    "title": "The Risks of Invariant Risk Minimization",
    "volume": "poster",
    "abstract": "Invariant Causal Prediction (Peters et al., 2016) is a technique for out-of-distribution generalization which assumes that some aspects of the data distribution vary across the training set but that the underlying causal mechanisms remain constant. Recently, Arjovsky et al. (2019) proposed Invariant Risk Minimization (IRM), an objective based on this idea for learning deep, invariant features of data which are a complex function of latent variables; many alternatives have subsequently been suggested. However, formal guarantees for all of these works are severely lacking. In this paper, we present the first analysis of classification under the IRM objective—as well as these recently proposed alternatives—under a fairly natural and general model. In the linear case, we show simple conditions under which the optimal solution succeeds or, more often, fails to recover the optimal invariant predictor. We furthermore present the very first results in the non-linear regime: we demonstrate that IRM can fail catastrophically unless the test data is sufficiently similar to the training distribution—this is precisely the issue that it was intended to solve. Thus, in this setting we find that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization",
    "checked": true,
    "id": "1e76e2fbf27198986271a672f462dc38d790d00f",
    "semantic_title": "the risks of invariant risk minimization",
    "citation_count": 319,
    "authors": []
  },
  "https://openreview.net/forum?id=V5j-jdoDDP": {
    "title": "Scaling Symbolic Methods using Gradients for Neural Model Explanation",
    "volume": "poster",
    "abstract": "Symbolic techniques based on Satisfiability Modulo Theory (SMT) solvers have been proposed for analyzing and verifying neural network properties, but their usage has been fairly limited owing to their poor scalability with larger networks. In this work, we propose a technique for combining gradient-based methods with symbolic techniques to scale such analyses and demonstrate its application for model explanation. In particular, we apply this technique to identify minimal regions in an input that are most relevant for a neural network's prediction. Our approach uses gradient information (based on Integrated Gradients) to focus on a subset of neurons in the first layer, which allows our technique to scale to large networks. The corresponding SMT constraints encode the minimal input mask discovery problem such that after masking the input, the activations of the selected neurons are still above a threshold. After solving for the minimal masks, our approach scores the mask regions to generate a relative ordering of the features within the mask. This produces a saliency map which explains\" where a model is looking\" when making a prediction. We evaluate our technique on three datasets-MNIST, ImageNet, and Beer Reviews, and demonstrate both quantitatively and qualitatively that the regions generated by our approach are sparser and achieve higher saliency scores compared to the gradient-based methods alone. Code and examples are at - https://github.com/google-research/google-research/tree/master/smug_saliency",
    "checked": true,
    "id": "04362f8cad633c9a71b99024c8c577a542fa5ea4",
    "semantic_title": "scaling symbolic methods using gradients for neural model explanation",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=ct8_a9h1M": {
    "title": "Contextual Dropout: An Efficient Sample-Dependent Dropout Module",
    "volume": "poster",
    "abstract": "Dropout has been demonstrated as a simple and effective module to not only regularize the training process of deep neural networks, but also provide the uncertainty estimation for prediction. However, the quality of uncertainty estimation is highly dependent on the dropout probabilities. Most current models use the same dropout distributions across all data samples due to its simplicity. Despite the potential gains in the flexibility of modeling uncertainty, sample-dependent dropout, on the other hand, is less explored as it often encounters scalability issues or involves non-trivial model changes. In this paper, we propose contextual dropout with an efficient structural design as a simple and scalable sample-dependent dropout module, which can be applied to a wide range of models at the expense of only slightly increased memory and computational cost. We learn the dropout probabilities with a variational objective, compatible with both Bernoulli dropout and Gaussian dropout. We apply the contextual dropout module to various models with applications to image classification and visual question answering and demonstrate the scalability of the method with large-scale datasets, such as ImageNet and VQA 2.0. Our experimental results show that the proposed method outperforms baseline methods in terms of both accuracy and quality of uncertainty estimation",
    "checked": true,
    "id": "0c6b8ec01311daba0cdf7c9ea79291329f7de3bf",
    "semantic_title": "contextual dropout: an efficient sample-dependent dropout module",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=CR1XOQ0UTh-": {
    "title": "Contrastive Learning with Hard Negative Samples",
    "volume": "poster",
    "abstract": "We consider the question: how can you sample good negative examples for contrastive learning? We argue that, as with metric learning, learning contrastive representations benefits from hard negative samples (i.e., points that are difficult to distinguish from an anchor point). The key challenge toward using hard negatives is that contrastive methods must remain unsupervised, making it infeasible to adopt existing negative sampling strategies that use label information. In response, we develop a new class of unsupervised methods for selecting hard negative samples where the user can control the amount of hardness. A limiting case of this sampling results in a representation that tightly clusters each class, and pushes different classes as far apart as possible. The proposed method improves downstream performance across multiple modalities, requires only few additional lines of code to implement, and introduces no computational overhead",
    "checked": true,
    "id": "7097137596f6755675f6aafcdd80969a747322ae",
    "semantic_title": "contrastive learning with hard negative samples",
    "citation_count": 823,
    "authors": []
  },
  "https://openreview.net/forum?id=6puUoArESGp": {
    "title": "Debiasing Concept-based Explanations with Causal Analysis",
    "volume": "poster",
    "abstract": "Concept-based explanation approach is a popular model interpertability tool because it expresses the reasons for a model's predictions in terms of concepts that are meaningful for the domain experts. In this work, we study the problem of the concepts being correlated with confounding information in the features. We propose a new causal prior graph for modeling the impacts of unobserved variables and a method to remove the impact of confounding information and noise using a two-stage regression technique borrowed from the instrumental variable literature. We also model the completeness of the concepts set and show that our debiasing method works when the concepts are not complete. Our synthetic and real-world experiments demonstrate the success of our method in removing biases and improving the ranking of the concepts in terms of their contribution to the explanation of the predictions",
    "checked": true,
    "id": "42c061c1e329b157e6dc4f27f40bed534bddc871",
    "semantic_title": "debiasing concept-based explanations with causal analysis",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=TBIzh9b5eaz": {
    "title": "Risk-Averse Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance",
    "checked": true,
    "id": "7e4520c972c88bbbe3bb3d15a89208f77756aae2",
    "semantic_title": "risk-averse offline reinforcement learning",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=6UdQLhqJyFD": {
    "title": "Parameter Efficient Multimodal Transformers for Video Representation Learning",
    "volume": "poster",
    "abstract": "The recent success of Transformers in the language domain has motivated adapting it to a multimodal setting, where a new visual model is trained in tandem with an already pretrained language model. However, due to the excessive memory requirements from Transformers, existing work typically fixes the language model and train only the vision module, which limits its ability to learn cross-modal information in an end-to-end manner. In this work, we focus on reducing the parameters of multimodal Transformers in the context of audio-visual video representation learning. We alleviate the high memory requirement by sharing the parameters of Transformers across layers and modalities; we decompose the Transformer into modality-specific and modality-shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. We show that our approach reduces parameters of the Transformers up to 97%, allowing us to train our model end-to-end from scratch. We also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns together with the Transformers. To demonstrate our approach, we pretrain our model on 30-second clips (480 frames) from Kinetics-700 and transfer it to audio-visual classification tasks",
    "checked": true,
    "id": "aed0f85b14e2c52e0c1850c93503bd637ff8119b",
    "semantic_title": "parameter efficient multimodal transformers for video representation learning",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=ZcKPWuhG6wy": {
    "title": "Tradeoffs in Data Augmentation: An Empirical Study",
    "volume": "poster",
    "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two",
    "checked": true,
    "id": "9da85492d28671cd9e513ab1a2c8ea3c772786ae",
    "semantic_title": "tradeoffs in data augmentation: an empirical study",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=eJIJF3-LoZO": {
    "title": "Concept Learners for Few-Shot Learning",
    "volume": "poster",
    "abstract": "Developing algorithms that are able to generalize to a novel task given only a few labeled examples represents a fundamental challenge in closing the gap between machine- and human-level performance. The core of human cognition lies in the structured, reusable concepts that help us to rapidly adapt to new tasks and provide reasoning behind our decisions. However, existing meta-learning methods learn complex representations across prior labeled tasks without imposing any structure on the learned representations. Here we propose COMET, a meta-learning method that improves generalization ability by learning to learn along human-interpretable concept dimensions. Instead of learning a joint unstructured metric space, COMET learns mappings of high-level concepts into semi-structured metric spaces, and effectively combines the outputs of independent concept learners. We evaluate our model on few-shot tasks from diverse domains, including fine-grained image classification, document categorization and cell type annotation on a novel dataset from a biological domain developed in our work. COMET significantly outperforms strong meta-learning baselines, achieving 6-15% relative improvement on the most challenging 1-shot learning tasks, while unlike existing methods providing interpretations behind the model's predictions",
    "checked": true,
    "id": "9483d411aac8741e89fb4ad5313adbfa652c3a5a",
    "semantic_title": "concept learners for few-shot learning",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=q8qLAbQBupm": {
    "title": "Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics",
    "volume": "poster",
    "abstract": "Understanding the dynamics of neural network parameters during training is one of the key challenges in building a theoretical foundation for deep learning. A central obstacle is that the motion of a network in high-dimensional parameter space undergoes discrete finite steps along complex stochastic gradients derived from real-world datasets. We circumvent this obstacle through a unifying theoretical framework based on intrinsic symmetries embedded in a network's architecture that are present for any dataset. We show that any such symmetry imposes stringent geometric constraints on gradients and Hessians, leading to an associated conservation law in the continuous-time limit of stochastic gradient descent (SGD), akin to Noether's theorem in physics. We further show that finite learning rates used in practice can actually break these symmetry induced conservation laws. We apply tools from finite difference methods to derive modified gradient flow, a differential equation that better approximates the numerical trajectory taken by SGD at finite learning rates. We combine modified gradient flow with our framework of symmetries to derive exact integral expressions for the dynamics of certain parameter combinations. We empirically validate our analytic expressions for learning dynamics on VGG-16 trained on Tiny ImageNet. Overall, by exploiting symmetry, our work demonstrates that we can analytically describe the learning dynamics of various parameter combinations at finite learning rates and batch sizes for state of the art architectures trained on any dataset",
    "checked": true,
    "id": "b93de30557605a4d7fe688524dc38dd52abc59e0",
    "semantic_title": "neural mechanics: symmetry and broken conservation laws in deep learning dynamics",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=DNl5s5BXeBn": {
    "title": "Fair Mixup: Fairness via Interpolation",
    "volume": "poster",
    "abstract": "Training classifiers under fairness constraints such as group fairness, regularizes the disparities of predictions between the groups. Nevertheless, even though the constraints are satisfied during training, they might not generalize at evaluation time. To improve the generalizability of fair classifiers, we propose fair mixup, a new data augmentation strategy for imposing the fairness constraint. In particular, we show that fairness can be achieved by regularizing the models on paths of interpolated samples between the groups. We use mixup, a powerful data augmentation strategy to generate these interpolates. We analyze fair mixup and empirically show that it ensures a better generalization for both accuracy and fairness measurement in tabular, vision, and language benchmarks",
    "checked": true,
    "id": "3470256133fe185031791e600f84376262bd9015",
    "semantic_title": "fair mixup: fairness via interpolation",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=UwGY2qjqoLD": {
    "title": "Heating up decision boundaries: isocapacitory saturation, adversarial scenarios and generalization bounds",
    "volume": "poster",
    "abstract": "In the present work we study classifiers' decision boundaries via Brownian motion processes in ambient data space and associated probabilistic techniques. Intuitively, our ideas correspond to placing a heat source at the decision boundary and observing how effectively the sample points warm up. We are largely motivated by the search for a soft measure that sheds further light on the decision boundary's geometry. En route, we bridge aspects of potential theory and geometric analysis (Maz'ya 2011, Grigor'Yan and Saloff-Coste 2002) with active fields of ML research such as adversarial examples and generalization bounds. First, we focus on the geometric behavior of decision boundaries in the light of adversarial attack/defense mechanisms. Experimentally, we observe a certain capacitory trend over different adversarial defense strategies: decision boundaries locally become flatter as measured by isoperimetric inequalities (Ford et al 2019); however, our more sensitive heat-diffusion metrics extend this analysis and further reveal that some non-trivial geometry invisible to plain distance-based methods is still preserved. Intuitively, we provide evidence that the decision boundaries nevertheless retain many persistent \"wiggly and fuzzy\" regions on a finer scale. Second, we show how Brownian hitting probabilities translate to soft generalization bounds which are in turn connected to compression and noise stability (Arora et al 2018), and these bounds are significantly stronger if the decision boundary has controlled geometric features",
    "checked": true,
    "id": "329c246b5916c6d0fa066fbfd3586fa02a9c4ab7",
    "semantic_title": "heating up decision boundaries: isocapacitory saturation, adversarial scenarios and generalization bounds",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=xHKVVHGDOEk": {
    "title": "Influence Functions in Deep Learning Are Fragile",
    "volume": "poster",
    "abstract": "Influence functions approximate the effect of training samples in test-time predictions and have a wide variety of applications in machine learning interpretability and uncertainty estimation. A commonly-used (first-order) influence function can be implemented efficiently as a post-hoc method requiring access only to the gradients and Hessian of the model. For linear models, influence functions are well-defined due to the convexity of the underlying loss function and are generally accurate even across difficult settings where model changes are fairly large such as estimating group influences. Influence functions, however, are not well-understood in the context of deep learning with non-convex loss functions. In this paper, we provide a comprehensive and large-scale empirical study of successes and failures of influence functions in neural network models trained on datasets such as Iris, MNIST, CIFAR-10 and ImageNet. Through our extensive experiments, we show that the network architecture, its depth and width, as well as the extent of model parameterization and regularization techniques have strong effects in the accuracy of influence functions. In particular, we find that (i) influence estimates are fairly accurate for shallow networks, while for deeper networks the estimates are often erroneous; (ii) for certain network architectures and datasets, training with weight-decay regularization is important to get high-quality influence estimates; and (iii) the accuracy of influence estimates can vary significantly depending on the examined test points. These results suggest that in general influence functions in deep learning are fragile and call for developing improved influence estimation methods to mitigate these issues in non-convex setups",
    "checked": true,
    "id": "098076a2c90e42c81b843bf339446427c2ff02ed",
    "semantic_title": "influence functions in deep learning are fragile",
    "citation_count": 246,
    "authors": []
  },
  "https://openreview.net/forum?id=pW2Q2xLwIMD": {
    "title": "Few-Shot Learning via Learning the Representation, Provably",
    "volume": "poster",
    "abstract": "This paper studies few-shot learning via representation learning, where one uses $T$ source tasks with $n_1$ data per task to learn a representation in order to reduce the sample complexity of a target task for which there is only $n_2 (\\ll n_1)$ data. Specifically, we focus on the setting where there exists a good common representation between source and target, and our goal is to understand how much a sample size reduction is possible. First, we study the setting where this common representation is low-dimensional and provide a risk bound of $\\tilde{O}(\\frac{dk}{n_1T} + \\frac{k}{n_2})$ on the target task for the linear representation class; here $d$ is the ambient input dimension and $k (\\ll d)$ is the dimension of the representation. This result bypasses the $\\Omega(\\frac{1}{T})$ barrier under the i.i.d. task assumption, and can capture the desired property that all $n_1T$ samples from source tasks can be \\emph{pooled} together for representation learning. We further extend this result to handle a general representation function class and obtain a similar result. Next, we consider the setting where the common representation may be high-dimensional but is capacity-constrained (say in norm); here, we again demonstrate the advantage of representation learning in both high-dimensional linear regression and neural networks, and show that representation learning can fully utilize all $n_1T$ samples from source tasks",
    "checked": true,
    "id": "3217f8d996b873410d720987f79790321cb2f877",
    "semantic_title": "few-shot learning via learning the representation, provably",
    "citation_count": 266,
    "authors": []
  },
  "https://openreview.net/forum?id=jMPcEkJpdD": {
    "title": "Self-Supervised Learning of Compressed Video Representations",
    "volume": "poster",
    "abstract": "Self-supervised learning of video representations has received great attention. Existing methods typically require frames to be decoded before being processed, which increases compute and storage requirements and ultimately hinders large-scale training. In this work, we propose an efficient self-supervised approach to learn video representations by eliminating the expensive decoding step. We use a three-stream video architecture that encodes I-frames and P-frames of a compressed video. Unlike existing approaches that encode I-frames and P-frames individually, we propose to jointly encode them by establishing bidirectional dynamic connections across streams. To enable self-supervised learning, we propose two pretext tasks that leverage the multimodal nature (RGB, motion vector, residuals) and the internal GOP structure of compressed videos. The first task asks our network to predict zeroth-order motion statistics in a spatio-temporal pyramid; the second task asks correspondence types between I-frames and P-frames after applying temporal transformations. We show that our approach achieves competitive performance on compressed video recognition both in supervised and self-supervised regimes",
    "checked": true,
    "id": "35c80e047de3cf45ede8092ff3827135d7ccd4fc",
    "semantic_title": "self-supervised learning of compressed video representations",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=Ig53hpHxS4": {
    "title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis",
    "volume": "poster",
    "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech variation. Flowtron borrows insights from Autoregressive Flows and revamps Tacotron 2 in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be used to modulate many aspects of speech synthesis (timbre, expressivity, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. We provide results on speech variation, interpolation over time between samples and style transfer between seen and unseen speakers. Code and pre-trained models are publicly available at \\href{https://github.com/NVIDIA/flowtron}{https://github.com/NVIDIA/flowtron}",
    "checked": true,
    "id": "e9495ee15e9dcc844095ef5071dd663470d4d564",
    "semantic_title": "flowtron: an autoregressive flow-based generative network for text-to-speech synthesis",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=RSU17UoKfJF": {
    "title": "R-GAP: Recursive Gradient Attack on Privacy",
    "volume": "poster",
    "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network's security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP",
    "checked": true,
    "id": "2ae3f6b82dd8503da380b373520e0d0713627cf2",
    "semantic_title": "r-gap: recursive gradient attack on privacy",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=iQQK02mxVIT": {
    "title": "Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients",
    "volume": "poster",
    "abstract": "A data set sampled from a certain population is biased if the subgroups of the population are sampled at proportions that are significantly different from their underlying proportions. Training machine learning models on biased data sets requires correction techniques to compensate for the bias. We consider two commonly-used techniques, resampling and reweighting, that rebalance the proportions of the subgroups to maintain the desired objective function. Though statistically equivalent, it has been observed that resampling outperforms reweighting when combined with stochastic gradient algorithms. By analyzing illustrative examples, we explain the reason behind this phenomenon using tools from dynamical stability and stochastic asymptotics. We also present experiments from regression, classification, and off-policy prediction to demonstrate that this is a general phenomenon. We argue that it is imperative to consider the objective function design and the optimization algorithm together while addressing the sampling bias",
    "checked": false,
    "id": "5e2c1a7e662d1a23bf3ec90b91c366810600e8af",
    "semantic_title": "why resampling outperforms reweighting for correcting sampling bias",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=2VXyy9mIyU3": {
    "title": "Learning with Instance-Dependent Label Noise: A Sample Sieve Approach",
    "volume": "poster",
    "abstract": "Human-annotated labels are often prone to noise, and the presence of such noise will degrade the performance of the resulting deep neural network (DNN) models. Much of the literature (with several recent exceptions) of learning with noisy labels focuses on the case when the label noise is independent of features. Practically, annotations errors tend to be instance-dependent and often depend on the difficulty levels of recognizing a certain task. Applying existing results from instance-independent settings would require a significant amount of estimation of noise rates. Therefore, providing theoretically rigorous solutions for learning with instance-dependent label noise remains a challenge. In this paper, we propose CORES$^{2}$ (COnfidence REgularized Sample Sieve), which progressively sieves out corrupted examples. The implementation of CORES$^{2}$ does not require specifying noise rates and yet we are able to provide theoretical guarantees of CORES$^{2}$ in filtering out the corrupted examples. This high-quality sample sieve allows us to treat clean examples and the corrupted ones separately in training a DNN solution, and such a separation is shown to be advantageous in the instance-dependent noise setting. We demonstrate the performance of CORES$^{2}$ on CIFAR10 and CIFAR100 datasets with synthetic instance-dependent label noise and Clothing1M with real-world human noise. As of independent interests, our sample sieve provides a generic machinery for anatomizing noisy datasets and provides a flexible interface for various robust training techniques to further improve the performance. Code is available at https://github.com/UCSC-REAL/cores",
    "checked": true,
    "id": "599ed9357448d8c55e2dc7f4f12224d5c6dd1fcc",
    "semantic_title": "learning with instance-dependent label noise: a sample sieve approach",
    "citation_count": 216,
    "authors": []
  },
  "https://openreview.net/forum?id=43VKWxg_Sqr": {
    "title": "Unsupervised Audiovisual Synthesis via Exemplar Autoencoders",
    "volume": "poster",
    "abstract": "We present an unsupervised approach that converts the input speech of any individual into audiovisual streams of potentially-infinitely many output speakers. Our approach builds on simple autoencoders that project out-of-sample data onto the distribution of the training set. We use exemplar autoencoders to learn the voice, stylistic prosody, and visual appearance of a specific target exemplar speech. In contrast to existing methods, the proposed approach can be easily extended to an arbitrarily large number of speakers and styles using only 3 minutes of target audio-video data, without requiring any training data for the input speaker. To do so, we learn audiovisual bottleneck representations that capture the structured linguistic content of speech. We outperform prior approaches on both audio and video synthesis",
    "checked": true,
    "id": "25163583e46d42879c457f1b6f3eed95302d3364",
    "semantic_title": "unsupervised audiovisual synthesis via exemplar autoencoders",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=pqZV_srUVmK": {
    "title": "Single-Timescale Actor-Critic Provably Finds Globally Optimal Policy",
    "volume": "poster",
    "abstract": "We study the global convergence and global optimality of actor-critic, one of the most popular families of reinforcement learning algorithms. While most existing works on actor-critic employ bi-level or two-timescale updates, we focus on the more practical single-timescale setting, where the actor and critic are updated simultaneously. Specifically, in each iteration, the critic update is obtained by applying the Bellman evaluation operator only once while the actor is updated in the policy gradient direction computed using the critic. Moreover, we consider two function approximation settings where both the actor and critic are represented by linear or deep neural networks. For both cases, we prove that the actor sequence converges to a globally optimal policy at a sublinear $O(K^{-1/2})$ rate, where $K$ is the number of iterations. To the best of our knowledge, we establish the rate of convergence and global optimality of single-timescale actor-critic with linear function approximation for the first time. Moreover, under the broader scope of policy optimization with nonlinear function approximation, we prove that actor-critic with deep neural network finds the globally optimal policy at a sublinear rate for the first time",
    "checked": true,
    "id": "9c17c98b40028c4239639f41b08d58353b75acc1",
    "semantic_title": "single-timescale actor-critic provably finds globally optimal policy",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=oZIvHV04XgC": {
    "title": "Wandering within a world: Online contextualized few-shot learning",
    "volume": "poster",
    "abstract": "We aim to bridge the gap between typical human and machine-learning environments by extending the standard framework of few-shot learning to an online, continual setting. In this setting, episodes do not have separate training and testing phases, and instead models are evaluated online while learning novel classes. As in the real world, where the presence of spatiotemporal context helps us retrieve learned skills in the past, our online few-shot learning setting also features an underlying context that changes throughout time. Object classes are correlated within a context and inferring the correct context can lead to better performance. Building upon this setting, we propose a new few-shot learning dataset based on large scale indoor imagery that mimics the visual experience of an agent wandering within a world. Furthermore, we convert popular few-shot learning approaches into online versions and we also propose a new model that can make use of spatiotemporal contextual information from the recent past",
    "checked": true,
    "id": "9f774c0710b887064f309f5a639c0679eccbb6f2",
    "semantic_title": "wandering within a world: online contextualized few-shot learning",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=Q1jmmQz72M2": {
    "title": "Neural Delay Differential Equations",
    "volume": "poster",
    "abstract": "Neural Ordinary Differential Equations (NODEs), a framework of continuous-depth neural networks, have been widely applied, showing exceptional efficacy in coping with some representative datasets. Recently, an augmented framework has been successfully developed for conquering some limitations emergent in application of the original framework. Here we propose a new class of continuous-depth neural networks with delay, named as Neural Delay Differential Equations (NDDEs), and, for computing the corresponding gradients, we use the adjoint sensitivity method to obtain the delayed dynamics of the adjoint. Since the differential equations with delays are usually seen as dynamical systems of infinite dimension possessing more fruitful dynamics, the NDDEs, compared to the NODEs, own a stronger capacity of nonlinear representations. Indeed, we analytically validate that the NDDEs are of universal approximators, and further articulate an extension of the NDDEs, where the initial function of the NDDEs is supposed to satisfy ODEs. More importantly, we use several illustrative examples to demonstrate the outstanding capacities of the NDDEs and the NDDEs with ODEs' initial value. More precisely, (1) we successfully model the delayed dynamics where the trajectories in the lower-dimensional phase space could be mutually intersected, while the traditional NODEs without any argumentation are not directly applicable for such modeling, and (2) we achieve lower loss and higher accuracy not only for the data produced synthetically by complex models but also for the real-world image datasets, i.e., CIFAR10, MNIST and SVHN. Our results on the NDDEs reveal that appropriately articulating the elements of dynamical systems into the network design is truly beneficial to promoting the network performance",
    "checked": true,
    "id": "7500229c7160547b11d8dc1144f1567e33546332",
    "semantic_title": "neural delay differential equations",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=0OlrLvrsHwQ": {
    "title": "Learning Parametrised Graph Shift Operators",
    "volume": "poster",
    "abstract": "In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in both node- and graph-classification tasks",
    "checked": true,
    "id": "1c295507aa5c724b1b30156c514e43e4c457790b",
    "semantic_title": "learning parametrised graph shift operators",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=6FqKiVAdI3Y": {
    "title": "DOP: Off-Policy Multi-Agent Decomposed Policy Gradients",
    "volume": "poster",
    "abstract": "Multi-agent policy gradient (MAPG) methods recently witness vigorous progress. However, there is a significant performance discrepancy between MAPG methods and state-of-the-art multi-agent value-based approaches. In this paper, we investigate causes that hinder the performance of MAPG algorithms and present a multi-agent decomposed policy gradient method (DOP). This method introduces the idea of value function decomposition into the multi-agent actor-critic framework. Based on this idea, DOP supports efficient off-policy learning and addresses the issue of centralized-decentralized mismatch and credit assignment in both discrete and continuous action spaces. We formally show that DOP critics have sufficient representational capability to guarantee convergence. In addition, empirical evaluations on the StarCraft II micromanagement benchmark and multi-agent particle environments demonstrate that DOP outperforms both state-of-the-art value-based and policy-based multi-agent reinforcement learning algorithms. Demonstrative videos are available at https://sites.google.com/view/dop-mapg/",
    "checked": false,
    "id": "4b3fbe0194e7bacc6ce7b5e9a0e826e04b1548b3",
    "semantic_title": "off-policy multi-agent decomposed policy gradients",
    "citation_count": 183,
    "authors": []
  },
  "https://openreview.net/forum?id=lf7st0bJIA5": {
    "title": "Unsupervised Discovery of 3D Physical Objects from Video",
    "volume": "poster",
    "abstract": "We study the problem of unsupervised physical object discovery. While existing frameworks aim to decompose scenes into 2D segments based off each object's appearance, we explore how physics, especially object interactions, facilitates disentangling of 3D geometry and position of objects from video, in an unsupervised manner. Drawing inspiration from developmental psychology, our Physical Object Discovery Network (POD-Net) uses both multi-scale pixel cues and physical motion cues to accurately segment observable and partially occluded objects of varying sizes, and infer properties of those objects. Our model reliably segments objects on both synthetic and real scenes. The discovered object properties can also be used to reason about physical events",
    "checked": true,
    "id": "a5a28056d84506f9bc4f3245e961e9d2f943e915",
    "semantic_title": "unsupervised discovery of 3d physical objects from video",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=OqtLIabPTit": {
    "title": "Exploring Balanced Feature Spaces for Representation Learning",
    "volume": "poster",
    "abstract": "Existing self-supervised learning (SSL) methods are mostly applied for training representation models from artificially balanced datasets (e.g., ImageNet). It is unclear how well they will perform in the practical scenarios where datasets are often imbalanced w.r.t. the classes. Motivated by this question, we conduct a series of studies on the performance of self-supervised contrastive learning and supervised learning methods over multiple datasets where training instance distributions vary from a balanced one to a long-tailed one. Our findings are quite intriguing. Different from supervised methods with large performance drop, the self-supervised contrastive learning methods perform stably well even when the datasets are heavily imbalanced. This motivates us to explore the balanced feature spaces learned by contrastive learning, where the feature representations present similar linear separability w.r.t. all the classes. Our further experiments reveal that a representation model generating a balanced feature space can generalize better than that yielding an imbalanced one across multiple settings. Inspired by these insights, we develop a novel representation learning method, called $k$-positive contrastive learning. It effectively combines strengths of the supervised method and the contrastive learning method to learn representations that are both discriminative and balanced. Extensive experiments demonstrate its superiority on multiple recognition tasks. Remarkably, it achieves new state-of-the-art on challenging long-tailed recognition benchmarks. Code and models will be released",
    "checked": true,
    "id": "b65bc8bd4e8900c38eb09bc1cbccea2499a86627",
    "semantic_title": "exploring balanced feature spaces for representation learning",
    "citation_count": 238,
    "authors": []
  },
  "https://openreview.net/forum?id=n7wIfYPdVet": {
    "title": "Auxiliary Learning by Implicit Differentiation",
    "volume": "poster",
    "abstract": "Training neural networks with auxiliary tasks is a common practice for improving the performance on a main task of interest. Two main challenges arise in this multi-task learning setting: (i) designing useful auxiliary tasks; and (ii) combining auxiliary tasks into a single coherent loss. Here, we propose a novel framework, AuxiLearn, that targets both challenges based on implicit differentiation. First, when useful auxiliaries are known, we propose learning a network that combines all losses into a single coherent objective function. This network can learn non-linear interactions between tasks. Second, when no useful auxiliary task is known, we describe how to learn a network that generates a meaningful, novel auxiliary task. We evaluate AuxiLearn in a series of tasks and domains, including image segmentation and learning with attributes in the low data regime, and find that it consistently outperforms competing methods",
    "checked": true,
    "id": "5043e91d4f36ada5504e1f7fa8b534c614a8ad98",
    "semantic_title": "auxiliary learning by implicit differentiation",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=i80OPhOCVH2": {
    "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications",
    "volume": "poster",
    "abstract": "Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008), one of the major problems in training GNNs was their struggle to propagate information between distant nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. In this paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffers from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights. Our code is available at https://github.com/tech-srl/bottleneck/",
    "checked": true,
    "id": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
    "semantic_title": "on the bottleneck of graph neural networks and its practical implications",
    "citation_count": 726,
    "authors": []
  },
  "https://openreview.net/forum?id=L7WD8ZdscQ5": {
    "title": "The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods",
    "volume": "poster",
    "abstract": "The adaptive stochastic gradient descent (SGD) with momentum has been widely adopted in deep learning as well as convex optimization. In practice, the last iterate is commonly used as the final solution. However, the available regret analysis and the setting of constant momentum parameters only guarantee the optimal convergence of the averaged solution. In this paper, we fill this theory-practice gap by investigating the convergence of the last iterate (referred to as {\\it individual convergence}), which is a more difficult task than convergence analysis of the averaged solution. Specifically, in the constrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB) method, in which the step size is only updated using the exponential moving average strategy, attains an individual convergence rate of $O(\\frac{1}{\\sqrt{t}})$, as opposed to that of $O(\\frac{\\log t}{\\sqrt {t}})$ of SGD, where $t$ is the number of iterations. Our new analysis not only shows how the HB momentum and its time-varying weight help us to achieve the acceleration in convex optimization but also gives valuable hints how the momentum parameters should be scheduled in deep learning. Empirical results validate the correctness of our convergence analysis in optimizing convex functions and demonstrate the improved performance of the adaptive HB methods in training deep networks",
    "checked": true,
    "id": "55d0c4c556f5298930111b7e968912738c5a7f3c",
    "semantic_title": "the role of momentum parameters in the optimal convergence of adaptive polyak's heavy-ball methods",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=xjXg0bnoDmS": {
    "title": "Entropic gradient descent algorithms and wide flat minima",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7b8e694b5032a2ab2b40993c5cd20d31d8f5b28a",
    "semantic_title": "entropic gradient descent algorithms and wide flat minima",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=XLfdzwNKzch": {
    "title": "SEDONA: Search for Decoupled Neural Networks toward Greedy Block-wise Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b57d4098fa63a330e201df09fe088d7cc0027251",
    "semantic_title": "sedona: search for decoupled neural networks toward greedy block-wise learning",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=tu29GQT0JFy": {
    "title": "not-MIWAE: Deep Generative Modelling with Missing not at Random Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a6bf259a29fbe2593720d1b62697edc825292c14",
    "semantic_title": "not-miwae: deep generative modelling with missing not at random data",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=9G5MIc-goqB": {
    "title": "Reweighting Augmented Samples by Minimizing the Maximal Expected Loss",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "601e490881aa6baac15640b0dfcc0e7ad2731475",
    "semantic_title": "reweighting augmented samples by minimizing the maximal expected loss",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=Xv_s64FiXTv": {
    "title": "Learning to Represent Action Values as a Hypergraph on the Action Vertices",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "467658037d60be494126f8c00974fb30593403f1",
    "semantic_title": "learning to represent action values as a hypergraph on the action vertices",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=vsU0efpivw": {
    "title": "Shapley Explanation Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "899e489944063f0313ef01b33a8d4ce6ca386321",
    "semantic_title": "shapley explanation networks",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=mCLVeEpplNE": {
    "title": "NBDT: Neural-Backed Decision Tree",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "511c19acb526452aedc6ba6d4f738f9e916ea24d",
    "semantic_title": "nbdt: neural-backed decision tree",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=St1giarCHLP": {
    "title": "Denoising Diffusion Implicit Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "014576b866078524286802b1d0e18628520aa886",
    "semantic_title": "denoising diffusion implicit models",
    "citation_count": 8276,
    "authors": []
  },
  "https://openreview.net/forum?id=qrwe7XHTmYb": {
    "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1882f194cb43828852cc052887671e55a80f945a",
    "semantic_title": "gshard: scaling giant models with conditional computation and automatic sharding",
    "citation_count": 1294,
    "authors": []
  },
  "https://openreview.net/forum?id=CZ8Y3NzuVzO": {
    "title": "What Should Not Be Contrastive in Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c3a0059e69a10c8fe70efab423761e378fdab74b",
    "semantic_title": "what should not be contrastive in contrastive learning",
    "citation_count": 309,
    "authors": []
  },
  "https://openreview.net/forum?id=7FNqrcPtieT": {
    "title": "On Data-Augmentation and Consistency-Based Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b4cc976bd6fd45aa8ad91859a962d4b90ba580c3",
    "semantic_title": "on data-augmentation and consistency-based semi-supervised learning",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=Qm7R_SdqTpT": {
    "title": "Diverse Video Generation using a Gaussian Process Trigger",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f286529cf63202f927a4ff7c450b918b42b128a9",
    "semantic_title": "diverse video generation using a gaussian process trigger",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=0pxiMpCyBtr": {
    "title": "Monotonic Kronecker-Factored Lattice",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f33956ca6730433e35bd4a24b9b6194524473a4e",
    "semantic_title": "monotonic kronecker-factored lattice",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=MJAqnaC2vO1": {
    "title": "Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "98f2cb7e6baa140e1b5e72fd5af9357c678c176b",
    "semantic_title": "auto seg-loss: searching metric surrogates for semantic segmentation",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=l0V53bErniB": {
    "title": "Combining Physics and Machine Learning for Network Flow Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "998c3b6ac109da36df480d9868806f52c60e8487",
    "semantic_title": "combining physics and machine learning for network flow estimation",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=pmj131uIL9H": {
    "title": "NeMo: Neural Mesh Models of Contrastive Features for Robust 3D Pose Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "74c61e2d2f12e56015741bcc2340f14a776e8606",
    "semantic_title": "nemo: neural mesh models of contrastive features for robust 3d pose estimation",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=Wi5KUNlqWty": {
    "title": "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dc9b9c5a76df331409890eea0d01e3cf3459a526",
    "semantic_title": "how to find your friendly neighborhood: graph attention design with self-supervision",
    "citation_count": 269,
    "authors": []
  },
  "https://openreview.net/forum?id=gV3wdEOGy_V": {
    "title": "MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "158bb664b0b7d7dec5a2f8cb3ae20c4b67fd1d6e",
    "semantic_title": "mice: mixture of contrastive experts for unsupervised image clustering",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=TSRTzJnuEBS": {
    "title": "Anytime Sampling for Autoregressive Models via Ordered Autoencoding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "222066c35e9be2a161521bf022623e38d03c514b",
    "semantic_title": "anytime sampling for autoregressive models via ordered autoencoding",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=KTlJT1nof6d": {
    "title": "Initialization and Regularization of Factorized Neural Layers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f93f2476972228de142fde13913bccbec76859b8",
    "semantic_title": "initialization and regularization of factorized neural layers",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=U4XLJhqwNF1": {
    "title": "CO2: Consistent Contrast for Unsupervised Visual Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b6f54f6d3a0cb9d3f1244c63773c40b0f5a1e224",
    "semantic_title": "co2: consistent contrast for unsupervised visual representation learning",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=V1ZHVxJ6dSS": {
    "title": "DC3: A learning method for optimization with hard constraints",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ac879e53927cdf7c3b8b5c00bc3ff13959f11d97",
    "semantic_title": "dc3: a learning method for optimization with hard constraints",
    "citation_count": 206,
    "authors": []
  },
  "https://openreview.net/forum?id=5lhWG3Hj2By": {
    "title": "Enforcing robust control guarantees within neural network policies",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7915568a86a196c06bb70fb98b3bc076a5a18b8d",
    "semantic_title": "enforcing robust control guarantees within neural network policies",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=BVSM0x3EDK6": {
    "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b27ad18e20d27efe8a9fbc54b1c2dcef8b2da19f",
    "semantic_title": "robust and generalizable visual representation learning via random convolutions",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=3SqrRe8FWQ-": {
    "title": "WrapNet: Neural Net Inference with Ultra-Low-Precision Arithmetic",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "91df6b1c9c11d55593f90ff28b098402a98ac2a3",
    "semantic_title": "wrapnet: neural net inference with ultra-low-precision arithmetic",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=MtEE0CktZht": {
    "title": "Rank the Episodes: A Simple Approach for Exploration in Procedurally-Generated Environments",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a188c3b58e71657ecfcddc37359aca51213e2187",
    "semantic_title": "rank the episodes: a simple approach for exploration in procedurally-generated environments",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=gwFTuzxJW0": {
    "title": "Stochastic Security: Adversarial Defense Using Long-Run Dynamics of Energy-Based Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d966edcc545f2b6a8ee2403da237eafc2330e048",
    "semantic_title": "stochastic security: adversarial defense using long-run dynamics of energy-based models",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=7wCBOfJ8hJM": {
    "title": "Nearest Neighbor Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "20d51f8e449b59c7e140f7a7eec9ab4d4d6f80ea",
    "semantic_title": "nearest neighbor machine translation",
    "citation_count": 291,
    "authors": []
  },
  "https://openreview.net/forum?id=yqPnIRhHtZv": {
    "title": "Learning Hyperbolic Representations of Topological Features",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5ac187d38060044c0add3df95e3cc05c9030d600",
    "semantic_title": "learning hyperbolic representations of topological features",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=8nl0k08uMi": {
    "title": "Selectivity considered harmful: evaluating the causal impact of class selectivity in DNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f60ce46af07db12e1540200ec0193ce1d8915d4e",
    "semantic_title": "selectivity considered harmful: evaluating the causal impact of class selectivity in dnns",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=IMPA6MndSXU": {
    "title": "Integrating Categorical Semantics into Unsupervised Domain Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "09b09a624cd126b78b3416c8839bc4e4325e1cfe",
    "semantic_title": "integrating categorical semantics into unsupervised domain translation",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=8wqCDnBmnrT": {
    "title": "Zero-shot Synthesis with Group-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1bf443a6a98dd0cbc22be3b8e82da4b60093e41d",
    "semantic_title": "zero-shot synthesis with group-supervised learning",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=g21u6nlbPzn": {
    "title": "VA-RED 2 : Video Adaptive Redundancy Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "e30b1ca378d245ec14a3dfcf09a493273882405c",
    "semantic_title": "va-red2: video adaptive redundancy reduction",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=YWtLZvLmud7": {
    "title": "BERTology Meets Biology: Interpreting Attention in Protein Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2b364917b0c51e91fcf2ab9c1d66a14ed4b44c03",
    "semantic_title": "bertology meets biology: interpreting attention in protein language models",
    "citation_count": 299,
    "authors": []
  },
  "https://openreview.net/forum?id=J8_GttYLFgr": {
    "title": "Trajectory Prediction using Equivariant Continuous Convolution",
    "volume": "poster",
    "abstract": "Trajectory prediction is a critical part of many AI applications, for example, the safe operation of autonomous vehicles. However, current methods are prone to making inconsistent and physically unrealistic predictions. We leverage insights from fluid dynamics to overcome this limitation by considering internal symmetry in real-world trajectories. We propose a novel model, Equivariant Continous COnvolution (ECCO) for improved trajectory prediction. ECCO uses rotationally-equivariant continuous convolutions to embed the symmetries of the system. On both vehicle and pedestrian trajectory datasets, ECCO attains competitive accuracy with significantly fewer parameters. It is also more sample efficient, generalizing automatically from few data points in any orientation. Lastly, ECCO improves generalization with equivariance, resulting in more physically consistent predictions. Our method provides a fresh perspective towards increasing trust and transparency in deep learning models. Our code and data can be found at https://github.com/Rose-STL-Lab/ECCO",
    "checked": true,
    "id": "38e52bbcfb67029822c4284e53d15225b478b90f",
    "semantic_title": "trajectory prediction using equivariant continuous convolution",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=XOjv2HxIF6i": {
    "title": "Unsupervised Meta-Learning through Latent-Space Interpolation in Generative Models",
    "volume": "poster",
    "abstract": "Several recently proposed unsupervised meta-learning approaches rely on synthetic meta-tasks created using techniques such as random selection, clustering and/or augmentation. In this work, we describe a novel approach that generates meta-tasks using generative models. The proposed family of algorithms generate pairs of in-class and out-of-class samples from the latent space in a principled way, allowing us to create synthetic classes forming the training and validation data of a meta-task. We find that the proposed approach, LAtent Space Interpolation Unsupervised Meta-learning (LASIUM), outperforms or is competitive with current unsupervised learning baselines on few-shot classification tasks on the most widely used benchmark datasets",
    "checked": true,
    "id": "64bc52c8349cd62cc561f11c78cbb9b914064819",
    "semantic_title": "unsupervised meta-learning through latent-space interpolation in generative models",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=mEdwVCRJuX4": {
    "title": "Heteroskedastic and Imbalanced Deep Learning with Adaptive Regularization",
    "volume": "poster",
    "abstract": "Real-world large-scale datasets are heteroskedastic and imbalanced --- labels have varying levels of uncertainty and label distributions are long-tailed. Heteroskedasticity and imbalance challenge deep learning algorithms due to the difficulty of distinguishing among mislabeled, ambiguous, and rare examples. Addressing heteroskedasticity and imbalance simultaneously is under-explored. We propose a data-dependent regularization technique for heteroskedastic datasets that regularizes different regions of the input space differently. Inspired by the theoretical derivation of the optimal regularization strength in a one-dimensional nonparametric classification setting, our approach adaptively regularizes the data points in higher-uncertainty, lower-density regions more heavily. We test our method on several benchmark tasks, including a real-world heteroskedastic and imbalanced dataset, WebVision. Our experiments corroborate our theory and demonstrate a significant improvement over other methods in noise-robust deep learning",
    "checked": true,
    "id": "92b46eeed09d9ff130901b97af38b55e4f4bdc2d",
    "semantic_title": "heteroskedastic and imbalanced deep learning with adaptive regularization",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=O9bnihsFfXU": {
    "title": "Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "We identify an implicit under-parameterization phenomenon in value-based deep RL methods that use bootstrapping: when value functions, approximated using deep neural networks, are trained with gradient descent using iterated regression onto target values generated by previous instances of the value network, more gradient updates decrease the expressivity of the current value network. We char- acterize this loss of expressivity via a drop in the rank of the learned value net- work features, and show that this typically corresponds to a performance drop. We demonstrate this phenomenon on Atari and Gym benchmarks, in both offline and online RL settings. We formally analyze this phenomenon and show that it results from a pathological interaction between bootstrapping and gradient-based optimization. We further show that mitigating implicit under-parameterization by controlling rank collapse can improve performance",
    "checked": true,
    "id": "63b4843f3789838331168315ea6fa406f0f973b1",
    "semantic_title": "implicit under-parameterization inhibits data-efficient deep reinforcement learning",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=ESG-DMKQKsD": {
    "title": "Bowtie Networks: Generative Modeling for Joint Few-Shot Recognition and Novel-View Synthesis",
    "volume": "poster",
    "abstract": "We propose a novel task of joint few-shot recognition and novel-view synthesis: given only one or few images of a novel object from arbitrary views with only category annotation, we aim to simultaneously learn an object classifier and generate images of that type of object from new viewpoints. While existing work copes with two or more tasks mainly by multi-task learning of shareable feature representations, we take a different perspective. We focus on the interaction and cooperation between a generative model and a discriminative model, in a way that facilitates knowledge to flow across tasks in complementary directions. To this end, we propose bowtie networks that jointly learn 3D geometric and semantic representations with a feedback loop. Experimental evaluation on challenging fine-grained recognition datasets demonstrates that our synthesized images are realistic from multiple viewpoints and significantly improve recognition performance as ways of data augmentation, especially in the low-data regime",
    "checked": true,
    "id": "98297bece690c0460f6d01fbb1131389a36df049",
    "semantic_title": "bowtie networks: generative modeling for joint few-shot recognition and novel-view synthesis",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=c9-WeM-ceB": {
    "title": "Saliency is a Possible Red Herring When Diagnosing Poor Generalization",
    "volume": "poster",
    "abstract": "Poor generalization is one symptom of models that learn to predict target variables using spuriously-correlated image features present only in the training distribution instead of the true image features that denote a class. It is often thought that this can be diagnosed visually using attribution (aka saliency) maps. We study if this assumption is correct. In some prediction tasks, such as for medical images, one may have some images with masks drawn by a human expert, indicating a region of the image containing relevant information to make the prediction. We study multiple methods that take advantage of such auxiliary labels, by training networks to ignore distracting features which may be found outside of the region of interest. This mask information is only used during training and has an impact on generalization accuracy depending on the severity of the shift between the training and test distributions. Surprisingly, while these methods improve generalization performance in the presence of a covariate shift, there is no strong correspondence between the correction of attribution towards the features a human expert have labelled as important and generalization performance. These results suggest that the root cause of poor generalization may not always be spatially defined, and raise questions about the utility of masks as 'attribution priors' as well as saliency maps for explainable predictions",
    "checked": true,
    "id": "23b87f50e600efae796c7737a5c57ac1fe93afd3",
    "semantic_title": "saliency is a possible red herring when diagnosing poor generalization",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=Qm8UNVCFdh": {
    "title": "What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions",
    "volume": "poster",
    "abstract": "Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ``\"muscly-supervised\" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch",
    "checked": true,
    "id": "0944c6bd883bf3cf2d7d4f765ab5266dd1482595",
    "semantic_title": "what can you learn from your muscles? learning visual representation from human interactions",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=iaO86DUuKi": {
    "title": "Conservative Safety Critics for Exploration",
    "volume": "poster",
    "abstract": "Safe exploration presents a major challenge in reinforcement learning (RL): when active data collection requires deploying partially trained policies, we must ensure that these policies avoid catastrophically unsafe regions, while still enabling trial and error learning. In this paper, we target the problem of safe exploration in RL, by learning a conservative safety estimate of environment states through a critic, and provably upper bound the likelihood of catastrophic failures at every training iteration. We theoretically characterize the tradeoff between safety and policy improvement, show that the safety constraints are satisfied with high probability during training, derive provable convergence guarantees for our approach which is no worse asymptotically then standard RL, and empirically demonstrate the efficacy of the proposed approach on a suite of challenging navigation, manipulation, and locomotion tasks. Our results demonstrate that the proposed approach can achieve competitive task performance, while incurring significantly lower catastrophic failure rates during training as compared to prior methods. Videos are at this URL https://sites.google.com/view/conservative-safety-critics/",
    "checked": true,
    "id": "a334f9897a330abddf99cfec0b5a70f751e9497b",
    "semantic_title": "conservative safety critics for exploration",
    "citation_count": 143,
    "authors": []
  },
  "https://openreview.net/forum?id=4c0J6lwQ4_": {
    "title": "Multi-Time Attention Networks for Irregularly Sampled Time Series",
    "volume": "poster",
    "abstract": "Irregular sampling occurs in many time series modeling applications where it presents a significant challenge to standard deep learning models. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. In this paper, we propose a new deep learning framework for this setting that we call Multi-Time Attention Networks. Multi-Time Attention Networks learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. We investigate the performance of this framework on interpolation and classification tasks using multiple datasets. Our results show that the proposed approach performs as well or better than a range of baseline and recently proposed models while offering significantly faster training times than current state-of-the-art methods",
    "checked": true,
    "id": "b940f6e62bb8bc64ea17331077e5c4d73191278c",
    "semantic_title": "multi-time attention networks for irregularly sampled time series",
    "citation_count": 211,
    "authors": []
  },
  "https://openreview.net/forum?id=6DOZ8XNNfGN": {
    "title": "Graph Traversal with Tensor Functionals: A Meta-Algorithm for Scalable Learning",
    "volume": "poster",
    "abstract": "Graph Representation Learning (GRL) methods have impacted fields from chemistry to social science. However, their algorithmic implementations are specialized to specific use-cases e.g. \"message passing\" methods are run differently from \"node embedding\" ones. Despite their apparent differences, all these methods utilize the graph structure, and therefore, their learning can be approximated with stochastic graph traversals. We propose Graph Traversal via Tensor Functionals (GTTF), a unifying meta-algorithm framework for easing the implementation of diverse graph algorithms and enabling transparent and efficient scaling to large graphs. GTTF is founded upon a data structure (stored as a sparse tensor) and a stochastic graph traversal algorithm (described using tensor operations). The algorithm is a functional that accept two functions, and can be specialized to obtain a variety of GRL models and objectives, simply by changing those two functions. We show for a wide class of methods, our algorithm learns in an unbiased fashion and, in expectation, approximates the learning as if the specialized implementations were run directly. With these capabilities, we scale otherwise non-scalable methods to set state-of-the-art on large graph datasets while being more efficient than existing GRL libraries -- with only a handful of lines of code for each method specialization",
    "checked": true,
    "id": "9fed4c8b88333f912ae217fbd0db0f864870d398",
    "semantic_title": "graph traversal with tensor functionals: a meta-algorithm for scalable learning",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=YmA86Zo-P_t": {
    "title": "What they do when in doubt: a study of inductive biases in seq2seq learners",
    "volume": "poster",
    "abstract": "Sequence-to-sequence (seq2seq) learners are widely used, but we still have only limited knowledge about what inductive biases shape the way they generalize. We address that by investigating how popular seq2seq learners generalize in tasks that have high ambiguity in the training data. We use four new tasks to study learners' preferences for memorization, arithmetic, hierarchical, and compositional reasoning. Further, we connect to Solomonoff's theory of induction and propose to use description length as a principled and sensitive measure of inductive biases. In our experimental study, we find that LSTM-based learners can learn to perform counting, addition, and multiplication by a constant from a single training example. Furthermore, Transformer and LSTM-based learners show a bias toward the hierarchical induction over the linear one, while CNN-based learners prefer the opposite. The latter also show a bias toward a compositional generalization over memorization. Finally, across all our experiments, description length proved to be a sensitive measure of inductive biases",
    "checked": true,
    "id": "032399b7fc693a9fc12bb26d6be8c02d77dd397a",
    "semantic_title": "what they do when in doubt: a study of inductive biases in seq2seq learners",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=YtMG5ex0ou": {
    "title": "Tomographic Auto-Encoder: Unsupervised Bayesian Recovery of Corrupted Data",
    "volume": "poster",
    "abstract": "We propose a new probabilistic method for unsupervised recovery of corrupted data. Given a large ensemble of degraded samples, our method recovers accurate posteriors of clean values, allowing the exploration of the manifold of possible reconstructed data and hence characterising the underlying uncertainty. In this set-ting, direct application of classical variational methods often gives rise to collapsed densities that do not adequately explore the solution space. Instead, we derive our novel reduced entropy condition approximate inference method that results in rich posteriors. We test our model in a data recovery task under the common setting of missing values and noise, demonstrating superior performance to existing variational methods for imputation and de-noising with different real data sets. We further show higher classification accuracy after imputation, proving the advantage of propagating uncertainty to downstream tasks with our model",
    "checked": true,
    "id": "f8d73165f7c27e36283621117fb6ecaa51f73d06",
    "semantic_title": "tomographic auto-encoder: unsupervised bayesian recovery of corrupted data",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=V69LGwJ0lIN": {
    "title": "OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "Reinforcement learning (RL) has achieved impressive performance in a variety of online settings in which an agent's ability to query the environment for transitions and rewards is effectively unlimited. However, in many practical applications, the situation is reversed: an agent may have access to large amounts of undirected offline experience data, while access to the online environment is severely limited. In this work, we focus on this offline setting. Our main insight is that, when presented with offline data composed of a variety of behaviors, an effective way to leverage this data is to extract a continuous space of recurring and temporally extended primitive behaviors before using these primitives for downstream task learning. Primitives extracted in this way serve two purposes: they delineate the behaviors that are supported by the data from those that are not, making them useful for avoiding distributional shift in offline RL; and they provide a degree of temporal abstraction, which reduces the effective horizon yielding better learning in theory, and improved offline RL in practice. In addition to benefiting offline policy optimization, we show that performing offline primitive learning in this way can also be leveraged for improving few-shot imitation learning as well as exploration and transfer in online RL on a variety of benchmark domains. Visualizations and code are available at https://sites.google.com/view/opal-iclr",
    "checked": true,
    "id": "0a321a38ba98499f17a2423f84972de29a5b2e7f",
    "semantic_title": "opal: offline primitive discovery for accelerating offline reinforcement learning",
    "citation_count": 164,
    "authors": []
  },
  "https://openreview.net/forum?id=ZzwDy_wiWv": {
    "title": "Knowledge distillation via softmax regression representation learning",
    "volume": "poster",
    "abstract": "This paper addresses the problem of model compression via knowledge distillation. We advocate for a method that optimizes the output feature of the penultimate layer of the student network and hence is directly related to representation learning. Previous distillation methods which typically impose direct feature matching between the student and the teacher do not take into account the classification problem at hand. On the contrary, our distillation method decouples representation learning and classification and utilizes the teacher's pre-trained classifier to train the student's penultimate layer feature. In particular, for the same input image, we wish the teacher's and student's feature to produce the same output when passed through the teacher's classifier which is achieved with a simple $L_2$ loss. Our method is extremely simple to implement and straightforward to train and is shown to consistently outperform previous state-of-the-art methods over a large set of experimental settings including different (a) network architectures, (b) teacher-student capacities, (c) datasets, and (d) domains. The code will be available at \\url{https://github.com/jingyang2017/KD_SRRL}",
    "checked": true,
    "id": "3555f47e781e25cf6e1b80d0daf0c39ad1a2d705",
    "semantic_title": "knowledge distillation via softmax regression representation learning",
    "citation_count": 151,
    "authors": []
  },
  "https://openreview.net/forum?id=X4y_10OX-hX": {
    "title": "Large Associative Memory Problem in Neurobiology and Machine Learning",
    "volume": "poster",
    "abstract": "Dense Associative Memories or modern Hopfield networks permit storage and reliable retrieval of an exponentially large (in the dimension of feature space) number of memories. At the same time, their naive implementation is non-biological, since it seemingly requires the existence of many-body synaptic junctions between the neurons. We show that these models are effective descriptions of a more microscopic (written in terms of biological degrees of freedom) theory that has additional (hidden) neurons and only requires two-body interactions between them. For this reason our proposed microscopic theory is a valid model of large associative memory with a degree of biological plausibility. The dynamics of our network and its reduced dimensional equivalent both minimize energy (Lyapunov) functions. When certain dynamical variables (hidden neurons) are integrated out from our microscopic theory, one can recover many of the models that were previously discussed in the literature, e.g. the model presented in \"Hopfield Networks is All You Need\" paper. We also provide an alternative derivation of the energy function and the update rule proposed in the aforementioned paper and clarify the relationships between various models of this class",
    "checked": true,
    "id": "6a8092b98771b8157437e71e351ae1231bdd8259",
    "semantic_title": "large associative memory problem in neurobiology and machine learning",
    "citation_count": 151,
    "authors": []
  },
  "https://openreview.net/forum?id=tHgJoMfy6nI": {
    "title": "Remembering for the Right Reasons: Explanations Reduce Catastrophic Forgetting",
    "volume": "poster",
    "abstract": "The goal of continual learning (CL) is to learn a sequence of tasks without suffering from the phenomenon of catastrophic forgetting. Previous work has shown that leveraging memory in the form of a replay buffer can reduce performance degradation on prior tasks. We hypothesize that forgetting can be further reduced when the model is encouraged to remember the \\textit{evidence} for previously made decisions. As a first step towards exploring this hypothesis, we propose a simple novel training paradigm, called Remembering for the Right Reasons (RRR), that additionally stores visual model explanations for each example in the buffer and ensures the model has ``the right reasons'' for its predictions by encouraging its explanations to remain consistent with those used to make decisions at training time. Without this constraint, there is a drift in explanations and increase in forgetting as conventional continual learning algorithms learn new tasks. We demonstrate how RRR can be easily added to any memory or regularization-based approach and results in reduced forgetting, and more importantly, improved model explanations. We have evaluated our approach in the standard and few-shot settings and observed a consistent improvement across various CL approaches using different architectures and techniques to generate model explanations and demonstrated our approach showing a promising connection between explainability and continual learning. Our code is available at \\url{https://github.com/SaynaEbrahimi/Remembering-for-the-Right-Reasons}",
    "checked": true,
    "id": "2c826ded75942bf6e0cd9b6ae1f6db372ee92f27",
    "semantic_title": "remembering for the right reasons: explanations reduce catastrophic forgetting",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=O-6Pm_d_Q-": {
    "title": "Deep Networks and the Multiple Manifold Problem",
    "volume": "poster",
    "abstract": "We study the multiple manifold problem, a binary classification task modeled on applications in machine vision, in which a deep fully-connected neural network is trained to separate two low-dimensional submanifolds of the unit sphere. We provide an analysis of the one-dimensional case, proving for a simple manifold configuration that when the network depth $L$ is large relative to certain geometric and statistical properties of the data, the network width $n$ grows as a sufficiently large polynomial in $L$, and the number of i.i.d. samples from the manifolds is polynomial in $L$, randomly-initialized gradient descent rapidly learns to classify the two manifolds perfectly with high probability. Our analysis demonstrates concrete benefits of depth and width in the context of a practically-motivated model problem: the depth acts as a fitting resource, with larger depths corresponding to smoother networks that can more readily separate the class manifolds, and the width acts as a statistical resource, enabling concentration of the randomly-initialized network and its gradients. The argument centers around the \"neural tangent kernel\" of Jacot et al. and its role in the nonasymptotic analysis of training overparameterized neural networks; to this literature, we contribute essentially optimal rates of concentration for the neural tangent kernel of deep fully-connected ReLU networks, requiring width $n \\geq L\\,\\mathrm{poly}(d_0)$ to achieve uniform concentration of the initial kernel over a $d_0$-dimensional submanifold of the unit sphere $\\mathbb{S}^{n_0-1}$, and a nonasymptotic framework for establishing generalization of networks trained in the \"NTK regime\" with structured data. The proof makes heavy use of martingale concentration to optimally treat statistical dependencies across layers of the initial random network. This approach should be of use in establishing similar results for other network architectures",
    "checked": true,
    "id": "4d5bb81da39b39b8158f63a0babc3c8e24c730cc",
    "semantic_title": "deep networks and the multiple manifold problem",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=gLWj29369lW": {
    "title": "Interpreting Knowledge Graph Relation Representation from Word Embeddings",
    "volume": "poster",
    "abstract": "Many models learn representations of knowledge graph data by exploiting its low-rank latent structure, encoding known relations between entities and enabling unknown facts to be inferred. To predict whether a relation holds between entities, embeddings are typically compared in the latent space following a relation-specific mapping. Whilst their predictive performance has steadily improved, how such models capture the underlying latent structure of semantic information remains unexplained. Building on recent theoretical understanding of word embeddings, we categorise knowledge graph relations into three types and for each derive explicit requirements of their representations. We show that empirical properties of relation representations and the relative performance of leading knowledge graph representation methods are justified by our analysis",
    "checked": true,
    "id": "366e9d016ec238d965f3c3c4674c6cba86760636",
    "semantic_title": "interpreting knowledge graph relation representation from word embeddings",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=MIDckA56aD": {
    "title": "Learning perturbation sets for robust machine learning",
    "volume": "poster",
    "abstract": "Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. In this paper, we aim to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, we use a conditional generator that defines the perturbation set over a constrained region of the latent space. We formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, our approach can generate a variety of perturbations at different complexities and scales, ranging from baseline spatial transformations, through common image corruptions, to lighting variations. We measure the quality of our learned perturbation sets both quantitatively and qualitatively, finding that our models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, we leverage our learned perturbation sets to train models which are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations, while improving generalization on non-adversarial data. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found at https://github.com/locuslab/perturbation_learning",
    "checked": true,
    "id": "857b1c3f171afb3cdf9df23d23e5d0cdfaa83efb",
    "semantic_title": "learning perturbation sets for robust machine learning",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=ZK6vTvb84s": {
    "title": "A Trainable Optimal Transport Embedding for Feature Aggregation and its Relationship to Attention",
    "volume": "poster",
    "abstract": "We address the problem of learning on sets of features, motivated by the need of performing pooling operations in long biological sequences of varying sizes, with long-range dependencies, and possibly few labeled data. To address this challenging task, we introduce a parametrized representation of fixed size, which embeds and then aggregates elements from a given input set according to the optimal transport plan between the set and a trainable reference. Our approach scales to large datasets and allows end-to-end training of the reference, while also providing a simple unsupervised learning mechanism with small computational cost. Our aggregation technique admits two useful interpretations: it may be seen as a mechanism related to attention layers in neural networks, or it may be seen as a scalable surrogate of a classical optimal transport-based kernel. We experimentally demonstrate the effectiveness of our approach on biological sequences, achieving state-of-the-art results for protein fold recognition and detection of chromatin profiles tasks, and, as a proof of concept, we show promising results for processing natural language sequences. We provide an open-source implementation of our embedding that can be used alone or as a module in larger learning models at https://github.com/claying/OTK",
    "checked": true,
    "id": "b8ca07fc7e7e70b6fd0f0b3b4921e4dabe11cd23",
    "semantic_title": "a trainable optimal transport embedding for feature aggregation and its relationship to attention",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=TTUVg6vkNjK": {
    "title": "RODE: Learning Roles to Decompose Multi-Agent Tasks",
    "volume": "poster",
    "abstract": "Role-based learning holds the promise of achieving scalable multi-agent learning by decomposing complex tasks using roles. However, it is largely unclear how to efficiently discover such a set of roles. To solve this problem, we propose to first decompose joint action spaces into restricted role action spaces by clustering actions according to their effects on the environment and other agents. Learning a role selector based on action effects makes role discovery much easier because it forms a bi-level learning hierarchy: the role selector searches in a smaller role space and at a lower temporal resolution, while role policies learn in significantly reduced primitive action-observation spaces. We further integrate information about action effects into the role policies to boost learning efficiency and policy generalization. By virtue of these advances, our method (1) outperforms the current state-of-the-art MARL algorithms on 9 of the 14 scenarios that comprise the challenging StarCraft II micromanagement benchmark and (2) achieves rapid transfer to new environments with three times the number of agents. Demonstrative videos can be viewed at https://sites.google.com/view/rode-marl",
    "checked": true,
    "id": "5764095b0186a3fc3832c1052aa14996a5927edc",
    "semantic_title": "rode: learning roles to decompose multi-agent tasks",
    "citation_count": 222,
    "authors": []
  },
  "https://openreview.net/forum?id=4qR3coiNaIv": {
    "title": "Scalable Bayesian Inverse Reinforcement Learning",
    "volume": "poster",
    "abstract": "Bayesian inference over the reward presents an ideal solution to the ill-posed nature of the inverse reinforcement learning problem. Unfortunately current methods generally do not scale well beyond the small tabular setting due to the need for an inner-loop MDP solver, and even non-Bayesian methods that do themselves scale often require extensive interaction with the environment to perform well, being inappropriate for high stakes or costly applications such as healthcare. In this paper we introduce our method, Approximate Variational Reward Imitation Learning (AVRIL), that addresses both of these issues by jointly learning an approximate posterior distribution over the reward that scales to arbitrarily complicated state spaces alongside an appropriate policy in a completely offline manner through a variational approach to said latent reward. Applying our method to real medical data alongside classic control simulations, we demonstrate Bayesian reward inference in environments beyond the scope of current methods, as well as task performance competitive with focused offline imitation learning algorithms",
    "checked": true,
    "id": "03308bd92c5993f6828950aaea2f38f5f7d9cd8d",
    "semantic_title": "scalable bayesian inverse reinforcement learning",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=h0de3QWtGG": {
    "title": "Learning \"What-if\" Explanations for Sequential Decision-Making",
    "volume": "poster",
    "abstract": "Building interpretable parameterizations of real-world decision-making on the basis of demonstrated behavior--i.e. trajectories of observations and actions made by an expert maximizing some unknown reward function--is essential for introspecting and auditing policies in different institutions. In this paper, we propose learning explanations of expert decisions by modeling their reward function in terms of preferences with respect to ``\"what if'' outcomes: Given the current history of observations, what would happen if we took a particular action? To learn these cost-benefit tradeoffs associated with the expert's actions, we integrate counterfactual reasoning into batch inverse reinforcement learning. This offers a principled way of defining reward functions and explaining expert behavior, and also satisfies the constraints of real-world decision-making---where active experimentation is often impossible (e.g. in healthcare). Additionally, by estimating the effects of different actions, counterfactuals readily tackle the off-policy nature of policy evaluation in the batch setting, and can naturally accommodate settings where the expert policies depend on histories of observations rather than just current states. Through illustrative experiments in both real and simulated medical environments, we highlight the effectiveness of our batch, counterfactual inverse reinforcement learning approach in recovering accurate and interpretable descriptions of behavior",
    "checked": true,
    "id": "d96fe15cfff06c4661996e5f0bcc45618e759ef7",
    "semantic_title": "learning \"what-if\" explanations for sequential decision-making",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=-gfhS00XfKj": {
    "title": "Learning advanced mathematical computations from examples",
    "volume": "poster",
    "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge",
    "checked": true,
    "id": "d9610589189e0821500516994dcee543a558b70c",
    "semantic_title": "learning advanced mathematical computations from examples",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=qkLMTphG5-h": {
    "title": "Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning",
    "volume": "poster",
    "abstract": "Model-agnostic meta-learning (MAML) is a popular method for few-shot learning but assumes that we have access to the meta-training set. In practice, training on the meta-training set may not always be an option due to data privacy concerns, intellectual property issues, or merely lack of computing resources. In this paper, we consider the novel problem of repurposing pretrained MAML checkpoints to solve new few-shot classification tasks. Because of the potential distribution mismatch, the original MAML steps may no longer be optimal. Therefore we propose an alternative meta-testing procedure and combine MAML gradient steps with adversarial training and uncertainty-based stepsize adaptation. Our method outperforms \"vanilla\" MAML on same-domain and cross-domains benchmarks using both SGD and Adam optimizers and shows improved robustness to the choice of base stepsize",
    "checked": true,
    "id": "b6c4a3f913ee819afa49e115f9739fe31ecf13f5",
    "semantic_title": "repurposing pretrained models for robust out-of-domain few-shot learning",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=5jRVa89sZk": {
    "title": "Empirical Analysis of Unlabeled Entity Problem in Named Entity Recognition",
    "volume": "poster",
    "abstract": "In many scenarios, named entity recognition (NER) models severely suffer from unlabeled entity problem, where the entities of a sentence may not be fully annotated. Through empirical studies performed on synthetic datasets, we find two causes of performance degradation. One is the reduction of annotated entities and the other is treating unlabeled entities as negative instances. The first cause has less impact than the second one and can be mitigated by adopting pretraining language models. The second cause seriously misguides a model in training and greatly affects its performances. Based on the above observations, we propose a general approach, which can almost eliminate the misguidance brought by unlabeled entities. The key idea is to use negative sampling that, to a large extent, avoids training NER models with unlabeled entities. Experiments on synthetic datasets and real-world datasets show that our model is robust to unlabeled entity problem and surpasses prior baselines. On well-annotated datasets, our model is competitive with the state-of-the-art method",
    "checked": true,
    "id": "95fceae74f4dbe582675a5bd6457702349939e72",
    "semantic_title": "empirical analysis of unlabeled entity problem in named entity recognition",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=eQe8DEWNN2W": {
    "title": "Calibration of Neural Networks using Splines",
    "volume": "poster",
    "abstract": "Calibrating neural networks is of utmost importance when employing them in safety-critical applications where the downstream decision making depends on the predicted probabilities. Measuring calibration error amounts to comparing two empirical distributions. In this work, we introduce a binning-free calibration measure inspired by the classical Kolmogorov-Smirnov (KS) statistical test in which the main idea is to compare the respective cumulative probability distributions. From this, by approximating the empirical cumulative distribution using a differentiable function via splines, we obtain a recalibration function, which maps the network outputs to actual (calibrated) class assignment probabilities. The spline-fitting is performed using a held-out calibration set and the obtained recalibration function is evaluated on an unseen test set. We tested our method against existing calibration approaches on various image classification datasets and our spline-based recalibration approach consistently outperforms existing methods on KS error as well as other commonly used calibration measures. Code is available online at https://github.com/kartikgupta-at-anu/spline-calibration",
    "checked": true,
    "id": "af2549305c2839e85367a07db10366670161afba",
    "semantic_title": "calibration of neural networks using splines",
    "citation_count": 112,
    "authors": []
  },
  "https://openreview.net/forum?id=17VnwXYZyhH": {
    "title": "Probing BERT in Hyperbolic Spaces",
    "volume": "poster",
    "abstract": "Recently, a variety of probing tasks are proposed to discover linguistic properties learned in contextualized word embeddings. Many of these works implicitly assume these embeddings lay in certain metric spaces, typically the Euclidean space. This work considers a family of geometrically special spaces, the hyperbolic spaces, that exhibit better inductive biases for hierarchical structures and may better reveal linguistic hierarchies encoded in contextualized representations. We introduce a $\\textit{Poincaré probe}$, a structural probe projecting these embeddings into a Poincaré subspace with explicitly defined hierarchies. We focus on two probing objectives: (a) dependency trees where the hierarchy is defined as head-dependent structures; (b) lexical sentiments where the hierarchy is defined as the polarity of words (positivity and negativity). We argue that a key desideratum of a probe is its sensitivity to the existence of linguistic structures. We apply our probes on BERT, a typical contextualized embedding model. In a syntactic subspace, our probe better recovers tree structures than Euclidean probes, revealing the possibility that the geometry of BERT syntax may not necessarily be Euclidean. In a sentiment subspace, we reveal two possible meta-embeddings for positive and negative sentiments and show how lexically-controlled contextualization would change the geometric localization of embeddings. We demonstrate the findings with our Poincaré probe via extensive experiments and visualization. Our results can be reproduced at https://github.com/FranxYao/PoincareProbe",
    "checked": true,
    "id": "4072a2333682941d23755e9b7e1e3a6d899683c6",
    "semantic_title": "probing bert in hyperbolic spaces",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=Zbc-ue9p_rE": {
    "title": "Refining Deep Generative Models via Discriminator Gradient Flow",
    "volume": "poster",
    "abstract": "Deep generative modeling has seen impressive advances in recent years, to the point where it is now commonplace to see simulated samples (e.g., images) that closely resemble real-world data. However, generation quality is generally inconsistent for any given model and can vary dramatically between samples. We introduce Discriminator Gradient $f$low (DG$f$low), a new technique that improves generated samples via the gradient flow of entropy-regularized $f$-divergences between the real and the generated data distributions. The gradient flow takes the form of a non-linear Fokker-Plank equation, which can be easily simulated by sampling from the equivalent McKean-Vlasov process. By refining inferior samples, our technique avoids wasteful sample rejection used by previous methods (DRS & MH-GAN). Compared to existing works that focus on specific GAN variants, we show our refinement approach can be applied to GANs with vector-valued critics and even other deep generative models such as VAEs and Normalizing Flows. Empirical results on multiple synthetic, image, and text datasets demonstrate that DG$f$low leads to significant improvement in the quality of generated samples for a variety of generative models, outperforming the state-of-the-art Discriminator Optimal Transport (DOT) and Discriminator Driven Latent Sampling (DDLS) methods",
    "checked": true,
    "id": "38a60a6fde1769f92062cf2295d3809c223abdd8",
    "semantic_title": "refining deep generative models via discriminator gradient flow",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=BtZhsSGNRNi": {
    "title": "Coping with Label Shift via Distributionally Robust Optimisation",
    "volume": "poster",
    "abstract": "The label shift problem refers to the supervised learning setting where the train and test label distributions do not match. Existing work addressing label shift usually assumes access to an unlabelled test sample. This sample may be used to estimate the test label distribution, and to then train a suitably re-weighted classifier. While approaches using this idea have proven effective, their scope is limited as it is not always feasible to access the target domain; further, they require repeated retraining if the model is to be deployed in multiple test environments. Can one instead learn a single classifier that is robust to arbitrary label shifts from a broad family? In this paper, we answer this question by proposing a model that minimises an objective based on distributionally robust optimisation (DRO). We then design and analyse a gradient descent-proximal mirror ascent algorithm tailored for large-scale problems to optimise the proposed objective. Finally, through experiments on CIFAR-100 and ImageNet, we show that our technique can significantly improve performance over a number of baselines in settings where label shift is present",
    "checked": true,
    "id": "8246f0d3799290be5ed47254f6b88b601fa98230",
    "semantic_title": "coping with label shift via distributionally robust optimisation",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=XAS3uKeFWj": {
    "title": "Variational State-Space Models for Localisation and Dense 3D Mapping in 6 DoF",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "32d88170812ea8295536a4ecb1976b94ce5622ec",
    "semantic_title": "variational state-space models for localisation and dense 3d mapping in 6 dof",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=bJxgv5C3sYc": {
    "title": "Few-Shot Bayesian Optimization with Deep Kernel Surrogates",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "aea3f03299ff0cfea9b394f5559aa1c173f9876f",
    "semantic_title": "few-shot bayesian optimization with deep kernel surrogates",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=T6AxtOaWydQ": {
    "title": "i -Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c709a3ac669aa334d6a0e9544f9191c5516da8a2",
    "semantic_title": "i-mix: a domain-agnostic strategy for contrastive representation learning",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=bM4Iqfg8M2k": {
    "title": "Graph Information Bottleneck for Subgraph Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1a901533b09269bafa1222ad5f2f5eb0279915ac",
    "semantic_title": "graph information bottleneck for subgraph recognition",
    "citation_count": 164,
    "authors": []
  },
  "https://openreview.net/forum?id=09-528y2Fgf": {
    "title": "Rethinking Positional Encoding in Language Pre-training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8256f48f759cf85044db251cc512f965834945b3",
    "semantic_title": "rethinking positional encoding in language pre-training",
    "citation_count": 305,
    "authors": []
  },
  "https://openreview.net/forum?id=6k7VdojAIK": {
    "title": "Practical Massively Parallel Monte-Carlo Tree Search Applied to Molecular Design",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "89b26dec9c4acb601cc9ae99737a998d8cae955e",
    "semantic_title": "practical massively parallel monte-carlo tree search applied to molecular design",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=S724o4_WB3": {
    "title": "When does preconditioning help or hurt generalization?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "af4fe7104ef94619920df684b5731ebfd58ddca9",
    "semantic_title": "when does preconditioning help or hurt generalization?",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=JoCR4h9O3Ew": {
    "title": "ARMOURED: Adversarially Robust MOdels using Unlabeled data by REgularizing Diversity",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2bc747f61085bcd4c0cfa868ac6cfc92b0c1ce37",
    "semantic_title": "armoured: adversarially robust models using unlabeled data by regularizing diversity",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=aD1_5zowqV": {
    "title": "Learning Energy-Based Generative Models via Coarse-to-Fine Expanding and Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4f2f0869c4698afc71fe983e457aacc2cc234501",
    "semantic_title": "learning energy-based generative models via coarse-to-fine expanding and sampling",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=U_mat0b9iv": {
    "title": "Multi-Prize Lottery Ticket Hypothesis: Finding Accurate Binary Neural Networks by Pruning A Randomly Weighted Network",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a52d17eac54b145cbc2b2c823f32b9e76be2595d",
    "semantic_title": "multi-prize lottery ticket hypothesis: finding accurate binary neural networks by pruning a randomly weighted network",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=cTbIjyrUVwJ": {
    "title": "Learning Accurate Entropy Model with Global Reference for Image Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "68a0d2855416a95cac648d63424c493ed1fb8b1a",
    "semantic_title": "learning accurate entropy model with global reference for image compression",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=te7PVH1sPxJ": {
    "title": "Convex Potential Flows: Universal Probability Distributions with Optimal Transport and Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7156cef2a8f1425928224bd7ff39b6958b5fda8c",
    "semantic_title": "convex potential flows: universal probability distributions with optimal transport and convex optimization",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=6t_dLShIUyZ": {
    "title": "Greedy-GQ with Variance Reduction: Finite-time Analysis and Improved Complexity",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "10b7c0e3fb4174fb4400c0e49e6e73908e3c8a45",
    "semantic_title": "greedy-gq with variance reduction: finite-time analysis and improved complexity",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=cP5IcoAkfKa": {
    "title": "Large Batch Simulation for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "543cd616f7f43d76b18cd6beae4f0a2ba5da9b61",
    "semantic_title": "large batch simulation for deep reinforcement learning",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=MaZFq7bJif7": {
    "title": "Hopper: Multi-hop Transformer for Spatiotemporal Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cab65b1c4b3dbddee7be6a7a9f764629b020882e",
    "semantic_title": "hopper: multi-hop transformer for spatiotemporal reasoning",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=fmtSg8591Q": {
    "title": "Efficient Reinforcement Learning in Factored MDPs with Application to Constrained RL",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e4c30f8e2fc6577fc04efbe8f14a557869a19fd4",
    "semantic_title": "efficient reinforcement learning in factored mdps with application to constrained rl",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=MJIve1zgR_": {
    "title": "Unbiased Teacher for Semi-Supervised Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fd9d3f4de3e340137011ae470aab7591a62b6e38",
    "semantic_title": "unbiased teacher for semi-supervised object detection",
    "citation_count": 506,
    "authors": []
  },
  "https://openreview.net/forum?id=D3PcGLdMx0": {
    "title": "MELR: Meta-Learning via Modeling Episode-Level Relationships for Few-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "53bd17596ecad58a85708cc1775bbbd4ec62836d",
    "semantic_title": "melr: meta-learning via modeling episode-level relationships for few-shot learning",
    "citation_count": 104,
    "authors": []
  },
  "https://openreview.net/forum?id=6BRLOfrMhW": {
    "title": "Partitioned Learned Bloom Filters",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e7a7ce163a286bc826bd9a10cdbca3777b4fe149",
    "semantic_title": "partitioned learned bloom filter",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=AAes_3W-2z": {
    "title": "Wasserstein Embedding for Graph Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "463f490d3bded6e527b0838da8495ed6441da25a",
    "semantic_title": "wasserstein embedding for graph learning",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=MxaY4FzOTa": {
    "title": "High-Capacity Expert Binary Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "06c4ba737d5bae10de2476ccc902bec6fb76cfc2",
    "semantic_title": "high-capacity expert binary networks",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=Cz3dbFm5u-": {
    "title": "SAFENet: A Secure, Accurate and Fast Neural Network Inference",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "be942d0c5640ceab1b7e72e106af413b0f842a28",
    "semantic_title": "safenet: a secure, accurate and fast neural network inference",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=Gu5WqN9J3Fn": {
    "title": "Learning Manifold Patch-Based Representations of Man-Made Shapes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "35c72580d82e7b8a16d4f452a6e44f401570c1d3",
    "semantic_title": "learning manifold patch-based representations of man-made shapes",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=-IXhmY16R3M": {
    "title": "Universal approximation power of deep residual neural networks via nonlinear control theory",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ac4d9ff742c5b778934e3eaee1c3cf9ed6c82232",
    "semantic_title": "universal approximation power of deep residual neural networks via nonlinear control theory",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=kW_zpEmMLdP": {
    "title": "Learning Neural Event Functions for Ordinary Differential Equations",
    "volume": "poster",
    "abstract": "The existing Neural ODE formulation relies on an explicit knowledge of the termination time. We extend Neural ODEs to implicitly defined termination criteria modeled by neural event functions, which can be chained together and differentiated through. Neural Event ODEs are capable of modeling discrete and instantaneous changes in a continuous-time system, without prior knowledge of when these changes should occur or how many such changes should exist. We test our approach in modeling hybrid discrete- and continuous- systems such as switching dynamical systems and collision in multi-body systems, and we propose simulation-based training of point processes with applications in discrete control",
    "checked": false,
    "id": "2fdcad1cfaca5f42634cc3f5c0bfd4ec0fa3716d",
    "semantic_title": "transfer learning using neural ordinary differential equations",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=XQQA6-So14": {
    "title": "Neural Spatio-Temporal Point Processes",
    "volume": "poster",
    "abstract": "We propose a new class of parameterizations for spatio-temporal point processes which leverage Neural ODEs as a computational method and enable flexible, high-fidelity models of discrete events that are localized in continuous time and space. Central to our approach is a combination of continuous-time neural networks with two novel neural architectures, \\ie, Jump and Attentive Continuous-time Normalizing Flows. This approach allows us to learn complex distributions for both the spatial and temporal domain and to condition non-trivially on the observed event history. We validate our models on data sets from a wide variety of contexts such as seismology, epidemiology, urban mobility, and neuroscience",
    "checked": true,
    "id": "289322a72c5693f5942054019a8e90bd3adebd94",
    "semantic_title": "neural spatio-temporal point processes",
    "citation_count": 106,
    "authors": []
  },
  "https://openreview.net/forum?id=LVotkZmYyDi": {
    "title": "Proximal Gradient Descent-Ascent: Variable Convergence under KŁ Geometry",
    "volume": "poster",
    "abstract": "The gradient descent-ascent (GDA) algorithm has been widely applied to solve minimax optimization problems. In order to achieve convergent policy parameters for minimax optimization, it is important that GDA generates convergent variable sequences rather than convergent sequences of function value or gradient norm. However, the variable convergence of GDA has been proved only under convexity geometries, and it is lack of understanding in general nonconvex minimax optimization. This paper fills such a gap by studying the convergence of a more general proximal-GDA for regularized nonconvex-strongly-concave minimax optimization. Specifically, we show that proximal-GDA admits a novel Lyapunov function, which monotonically decreases in the minimax optimization process and drives the variable sequences to a critical point. By leveraging this Lyapunov function and the KL geometry that parameterizes the local geometries of general nonconvex functions, we formally establish the variable convergence of proximal-GDA to a certain critical point $x^*$, i.e., $x_t\\to x^*, y_t\\to y^*(x^*)$. Furthermore, over the full spectrum of the KL-parameterized geometry, we show that proximal-GDA achieves different types of convergence rates ranging from sublinear convergence up to finite-step convergence, depending on the geometry associated with the KL parameter. This is the first theoretical result on the variable convergence for nonconvex minimax optimization",
    "checked": true,
    "id": "619713fd242ad771e788a183b77285cba9dd143b",
    "semantic_title": "proximal gradient descent-ascent: variable convergence under kł geometry",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=n6jl7fLxrP": {
    "title": "Adaptive Universal Generalized PageRank Graph Neural Network",
    "volume": "poster",
    "abstract": "In many important graph data processing applications the acquired information includes both node features and observations of the graph topology. Graph neural networks (GNNs) are designed to exploit both sources of evidence but they do not optimally trade-off their utility and integrate them in a manner that is also universal. Here, universality refers to independence on homophily or heterophily graph assumptions. We address these issues by introducing a new Generalized PageRank (GPR) GNN architecture that adaptively learns the GPR weights so as to jointly optimize node feature and topological information extraction, regardless of the extent to which the node labels are homophilic or heterophilic. Learned GPR weights automatically adjust to the node label pattern, irrelevant on the type of initialization, and thereby guarantee excellent learning performance for label patterns that are usually hard to handle. Furthermore, they allow one to avoid feature over-smoothing, a process which renders feature information nondiscriminative, without requiring the network to be shallow. Our accompanying theoretical analysis of the GPR-GNN method is facilitated by novel synthetic benchmark datasets generated by the so-called contextual stochastic block model. We also compare the performance of our GNN architecture with that of several state-of-the-art GNNs on the problem of node-classification, using well-known benchmark homophilic and heterophilic datasets. The results demonstrate that GPR-GNN offers significant performance improvement compared to existing techniques on both synthetic and benchmark data",
    "checked": true,
    "id": "0ee0801ba010a441403f9ed666ef9bf006b3aa07",
    "semantic_title": "adaptive universal generalized pagerank graph neural network",
    "citation_count": 770,
    "authors": []
  },
  "https://openreview.net/forum?id=MmCRswl1UYl": {
    "title": "Open Question Answering over Tables and Text",
    "volume": "poster",
    "abstract": "In open question answering (QA), the answer to a question is produced by retrieving and then analyzing documents that might contain answers to the question. Most open QA systems have considered only retrieving information from unstructured text. Here we consider for the first time open QA over {\\em both} tabular and textual data and present a new large-scale dataset \\emph{Open Table-and-Text Question Answering} (OTT-QA) to evaluate performance on this task. Most questions in OTT-QA require multi-hop inference across tabular data and unstructured text, and the evidence required to answer a question can be distributed in different ways over these two types of input, making evidence retrieval challenging---our baseline model using an iterative retriever and BERT-based reader achieves an exact match score less than 10\\%. We then propose two novel techniques to address the challenge of retrieving and aggregating evidence for OTT-QA. The first technique is to use ``early fusion'' to group multiple highly relevant tabular and textual units into a fused block, which provides more context for the retriever to search for. The second technique is to use a cross-block reader to model the cross-dependency between multiple retrieved evidence with global-local sparse attention. Combining these two techniques improves the score significantly, to above 27\\%",
    "checked": true,
    "id": "93d3e45395117e21214d404c8753b578c29266d1",
    "semantic_title": "open question answering over tables and text",
    "citation_count": 205,
    "authors": []
  },
  "https://openreview.net/forum?id=RovX-uQ1Hua": {
    "title": "Text Generation by Learning from Demonstrations",
    "volume": "poster",
    "abstract": "Current approaches to text generation largely rely on autoregressive models and maximum likelihood estimation. This paradigm leads to (i) diverse but low-quality samples due to mismatched learning objective and evaluation metric (likelihood vs. quality) and (ii) exposure bias due to mismatched history distributions (gold vs. model-generated). To alleviate these problems, we frame text generation as an offline reinforcement learning (RL) problem with expert demonstrations (i.e., the reference), where the goal is to maximize quality given model-generated histories. We propose GOLD (generation by off-policy learning from demonstrations): an easy-to-optimize algorithm that learns from the demonstrations by importance weighting. Intuitively, GOLD upweights confident tokens and downweights unconfident ones in the reference during training, avoiding optimization issues faced by prior RL approaches that rely on online data collection. According to both automatic and human evaluation, models trained by GOLD outperform those trained by MLE and policy gradient on summarization, question generation, and machine translation. Further, our models are less sensitive to decoding algorithms and alleviate exposure bias",
    "checked": true,
    "id": "7b20d003dcde3106ed8f178e0083963775d7c15a",
    "semantic_title": "text generation by learning from demonstrations",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=K5YasWXZT3O": {
    "title": "Tilted Empirical Risk Minimization",
    "volume": "poster",
    "abstract": "Empirical risk minimization (ERM) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly. While many methods aim to address these problems individually, in this work, we explore them through a unified framework---tilted empirical risk minimization (TERM). In particular, we show that it is possible to flexibly tune the impact of individual losses through a straightforward extension to ERM using a hyperparameter called the tilt. We provide several interpretations of the resulting framework: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to a superquantile method. We develop batch and stochastic first-order optimization methods for solving TERM, and show that the problem can be efficiently solved relative to common alternatives. Finally, we demonstrate that TERM can be used for a multitude of applications, such as enforcing fairness between subgroups, mitigating the effect of outliers, and handling class imbalance. TERM is not only competitive with existing solutions tailored to these individual problems, but can also enable entirely new applications, such as simultaneously addressing outliers and promoting fairness",
    "checked": true,
    "id": "1f6de95137e96872274eedae1beb1bd55f03c57a",
    "semantic_title": "tilted empirical risk minimization",
    "citation_count": 137,
    "authors": []
  },
  "https://openreview.net/forum?id=pGIHq1m7PU": {
    "title": "Explainable Subgraph Reasoning for Forecasting on Temporal Knowledge Graphs",
    "volume": "poster",
    "abstract": "Modeling time-evolving knowledge graphs (KGs) has recently gained increasing interest. Here, graph representation learning has become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to interpret their predictions. This paper provides a link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the structural dependencies and the temporal dynamics. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and by attention propagation. Our approach provides human-understandable evidence explaining the forecast. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model obtains a relative improvement of up to 20 $\\%$ on Hits@1 compared to the previous best temporal KG forecasting method. We also conduct a survey with 53 respondents, and the results show that the evidence extracted by the model for link forecasting is aligned with human understanding",
    "checked": true,
    "id": "08ede1cbadd5c631f93c0f952ac7ca99605d8a21",
    "semantic_title": "explainable subgraph reasoning for forecasting on temporal knowledge graphs",
    "citation_count": 166,
    "authors": []
  },
  "https://openreview.net/forum?id=ufZN2-aehFa": {
    "title": "Bayesian Context Aggregation for Neural Processes",
    "volume": "poster",
    "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research. Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models",
    "checked": true,
    "id": "5d2726260d6e52f3d9443b94cec3c116d5dec3cb",
    "semantic_title": "bayesian context aggregation for neural processes",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=q-cnWaaoUTH": {
    "title": "Conformation-Guided Molecular Representation with Hamiltonian Neural Networks",
    "volume": "poster",
    "abstract": "Well-designed molecular representations (fingerprints) are vital to combine medical chemistry and deep learning. Whereas incorporating 3D geometry of molecules (i.e. conformations) in their representations seems beneficial, current 3D algorithms are still in infancy. In this paper, we propose a novel molecular representation algorithm which preserves 3D conformations of molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and momentums of atoms in a molecule interact in the Hamiltonian Engine following the discretized Hamiltonian equations. These implicit coordinations are supervised with real conformations with translation- & rotation-invariant losses, and further used as inputs to the Fingerprint Generator, a message-passing neural network. Experiments show that the Hamiltonian Engine can well preserve molecular conformations, and that the fingerprints generated by HamNet achieve state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark",
    "checked": true,
    "id": "69bab843432803a8f69ffa7c33779f205e843841",
    "semantic_title": "hamnet: conformation-guided molecular representation with hamiltonian neural networks",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=ETBc_MIMgoX": {
    "title": "Learning with AMIGo: Adversarially Motivated Intrinsic Goals",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "df8a0bed6d685fee9c5ad418f4834e9537878de2",
    "semantic_title": "learning with amigo: adversarially motivated intrinsic goals",
    "citation_count": 129,
    "authors": []
  },
  "https://openreview.net/forum?id=dV19Yyi1fS3": {
    "title": "Training with Quantization Noise for Extreme Model Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0171ad4cc87cc7db25b4ec3169e293eed9a13b39",
    "semantic_title": "training with quantization noise for extreme model compression",
    "citation_count": 248,
    "authors": []
  },
  "https://openreview.net/forum?id=Jacdvfjicf7": {
    "title": "Interpreting and Boosting Dropout from a Game-Theoretic View",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9017fba9c4268dabc15e4bd8a99c6992625f9585",
    "semantic_title": "interpreting and boosting dropout from a game-theoretic view",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=DILxQP08O3B": {
    "title": "VTNet: Visual Transformer Network for Object Goal Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bcc16c38e479a2351c1ce4ec982b97353cb0d5d0",
    "semantic_title": "vtnet: visual transformer network for object goal navigation",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=QO9-y8also-": {
    "title": "Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ad2dc971f3712b055c65e3f93cf15cabf2b6c951",
    "semantic_title": "exemplary natural images explain cnn activations better than state-of-the-art feature visualization",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=AICNpd8ke-m": {
    "title": "Multi-Class Uncertainty Calibration via Mutual Information Maximization-based Binning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ecafa739f42dea74c23d7a3bb4ab613aeb86d1aa",
    "semantic_title": "multi-class uncertainty calibration via mutual information maximization-based binning",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=-_Zp7r2-cGK": {
    "title": "A Discriminative Gaussian Mixture Model with Sparsity",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fb2efb2b6a074fedc6ac54594b28689da17a72fb",
    "semantic_title": "a discriminative gaussian mixture model with sparsity",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=OOsR8BzCnl5": {
    "title": "Trusted Multi-View Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0fa224f1de6fe38cdeae3cd40de5249475cddf60",
    "semantic_title": "trusted multi-view classification",
    "citation_count": 179,
    "authors": []
  },
  "https://openreview.net/forum?id=xzqLpqRzxLq": {
    "title": "IEPT: Instance-Level and Episode-Level Pretext Tasks for Few-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6d79b85981aca97363a46fc7ee1a0c3f3ff3130d",
    "semantic_title": "iept: instance-level and episode-level pretext tasks for few-shot learning",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=KpfasTaLUpq": {
    "title": "Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ac6535d096fc79dde2d9ce0329e0626b79ede7f0",
    "semantic_title": "deep encoder, shallow decoder: reevaluating non-autoregressive machine translation",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=ldxlzGYWDmW": {
    "title": "Effective Abstract Reasoning with Dual-Contrast Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "7c102ad4766787ff7cd98b649bba024278e16051",
    "semantic_title": "uk stroke forum 2019 abstract supplement",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=onxoVA9FxMw": {
    "title": "On Position Embeddings in BERT",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dc35daba3fb34b2e6a5b12530badb7b799262bbf",
    "semantic_title": "on position embeddings in bert",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=o966_Is_nPA": {
    "title": "Neural Pruning via Growing Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "400080a724d55bbdd3cfc1c54f0aae6af4ec7879",
    "semantic_title": "neural pruning via growing regularization",
    "citation_count": 156,
    "authors": []
  },
  "https://openreview.net/forum?id=l-LGlk4Yl6G": {
    "title": "Mixed-Features Vectors and Subspace Splitting",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "75bfdf15cc615a69159065f928bf7ecfbb23f0dd",
    "semantic_title": "mixed-features vectors and subspace splitting",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r-gPPHEjpmw": {
    "title": "Hierarchical Reinforcement Learning by Discovering Intrinsic Options",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "005acb881061eb8137e9d36a05a6a0bdf0026b61",
    "semantic_title": "hierarchical reinforcement learning by discovering intrinsic options",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=r28GdiQF7vM": {
    "title": "Sharper Generalization Bounds for Learning with Gradient-dominated Objective Functions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8da78b1be38bb14c706c49405ec728e617ed936c",
    "semantic_title": "sharper generalization bounds for learning with gradient-dominated objective functions",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=Naqw7EHIfrv": {
    "title": "Representation Learning for Sequence Data with Deep Autoencoding Predictive Components",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0665091220e2c22ac2e6f6d63a97ae8a092d51b4",
    "semantic_title": "representation learning for sequence data with deep autoencoding predictive components",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=H0syOoy3Ash": {
    "title": "Average-case Acceleration for Bilinear Games and Normal Matrices",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "edc845f87b4ccdc97568410d157ed103c0a493d1",
    "semantic_title": "average-case acceleration for bilinear games and normal matrices",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=qzBUIzq5XR2": {
    "title": "Learning Task-General Representations with Generative Neuro-Symbolic Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "27ad6a5a17a75d1879f47a21a6d07f56ce87cab9",
    "semantic_title": "learning task-general representations with generative neuro-symbolic modeling",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=PObuuGVrGaZ": {
    "title": "Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1c38fb9ee7a86755c184a37bcb654786150038c5",
    "semantic_title": "is label smoothing truly incompatible with knowledge distillation: an empirical study",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=aYuZO9DIdnn": {
    "title": "The Unreasonable Effectiveness of Patches in Deep Convolutional Kernels Methods",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d6cd672bd7c186b6bb74b9f0ca29d877902bbf32",
    "semantic_title": "the unreasonable effectiveness of patches in deep convolutional kernels methods",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=uxpzitPEooJ": {
    "title": "Graph Coarsening with Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bf4209bf37f86a1fe77153fcb10eb541c18e23b2",
    "semantic_title": "graph coarsening with neural networks",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=0IO5VdnSAaH": {
    "title": "On the Universality of the Double Descent Peak in Ridgeless Regression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "55fc38656da779216eedf8bbbfd30878f5593635",
    "semantic_title": "on the universality of the double descent peak in ridgeless regression",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=Yz-XtK5RBxB": {
    "title": "Deep Repulsive Clustering of Ordered Data Based on Order-Identity Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e2d8017a34bff9e3e9c517f11b89246e1f7f683e",
    "semantic_title": "deep repulsive clustering of ordered data based on order-identity decomposition",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=F-mvpFpn_0q": {
    "title": "Rapid Task-Solving in Novel Environments",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6f505f9b8611ea5d1fcf4405a6abb42e0c0c27f1",
    "semantic_title": "rapid task-solving in novel environments",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=WAISmwsqDsb": {
    "title": "DINO: A Conditional Energy-Based GAN for Domain Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "25b02f72bd767accb132e5d9b68242bf69f40f7d",
    "semantic_title": "dino: a conditional energy-based gan for domain translation",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=eIHYL6fpbkA": {
    "title": "Removing Undesirable Feature Contributions Using Out-of-Distribution Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2be5c2f23b997e1cb22479dc1d5d1735997ad4be",
    "semantic_title": "removing undesirable feature contributions using out-of-distribution data",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=JHcqXGaqiGn": {
    "title": "Accurate Learning of Graph Representations with Graph Multiset Pooling",
    "volume": "poster",
    "abstract": "Graph neural networks have been widely used on modeling graph data, achieving impressive results on node classification and link prediction tasks. Yet, obtaining an accurate representation for a graph further requires a pooling function that maps a set of node representations into a compact form. A simple sum or average over all node representations considers all node features equally without consideration of their task relevance, and any structural dependencies among them. Recently proposed hierarchical graph pooling methods, on the other hand, may yield the same representation for two different graphs that are distinguished by the Weisfeiler-Lehman test, as they suboptimally preserve information from the node features. To tackle these limitations of existing graph pooling methods, we first formulate the graph pooling problem as a multiset encoding problem with auxiliary information about the graph structure, and propose a Graph Multiset Transformer (GMT) which is a multi-head attention based global pooling layer that captures the interaction between nodes according to their structural dependencies. We show that GMT satisfies both injectiveness and permutation invariance, such that it is at most as powerful as the Weisfeiler-Lehman graph isomorphism test. Moreover, our methods can be easily extended to the previous node clustering approaches for hierarchical graph pooling. Our experimental results show that GMT significantly outperforms state-of-the-art graph pooling methods on graph classification benchmarks with high memory and time efficiency, and obtains even larger performance gain on graph reconstruction and generation tasks",
    "checked": true,
    "id": "12b03b224a737c3cb8399b3d3b318d94bc04a0b5",
    "semantic_title": "accurate learning of graph representations with graph multiset pooling",
    "citation_count": 181,
    "authors": []
  },
  "https://openreview.net/forum?id=ce6CFXBh30h": {
    "title": "Federated Semi-Supervised Learning with Inter-Client Consistency & Disjoint Learning",
    "volume": "poster",
    "abstract": "While existing federated learning approaches mostly require that clients have fully-labeled data to train on, in realistic settings, data obtained at the client-side often comes without any accompanying labels. Such deficiency of labels may result from either high labeling cost, or difficulty of annotation due to the requirement of expert knowledge. Thus the private data at each client may be either partly labeled, or completely unlabeled with labeled data being available only at the server, which leads us to a new practical federated learning problem, namely Federated Semi-Supervised Learning (FSSL). In this work, we study two essential scenarios of FSSL based on the location of the labeled data. The first scenario considers a conventional case where clients have both labeled and unlabeled data (labels-at-client), and the second scenario considers a more challenging case, where the labeled data is only available at the server (labels-at-server). We then propose a novel method to tackle the problems, which we refer to as Federated Matching (FedMatch). FedMatch improves upon naive combinations of federated learning and semi-supervised learning approaches with a new inter-client consistency loss and decomposition of the parameters for disjoint learning on labeled and unlabeled data. Through extensive experimental validation of our method in the two different scenarios, we show that our method outperforms both local semi-supervised learning and baselines which naively combine federated learning with semi-supervised learning",
    "checked": false,
    "id": "51b61fbf3339433c7ffdbfa9c946185fb49317da",
    "semantic_title": "federated semi-supervised learning with inter-client consistency",
    "citation_count": 217,
    "authors": []
  },
  "https://openreview.net/forum?id=Wga_hrCa3P3": {
    "title": "Contrastive Learning with Adversarial Perturbations for Conditional Text Generation",
    "volume": "poster",
    "abstract": "Recently, sequence-to-sequence (seq2seq) models with the Transformer architecture have achieved remarkable performance on various conditional text generation tasks, such as machine translation. However, most of them are trained with teacher forcing with the ground truth label given at each time step, without being exposed to incorrectly generated tokens during training, which hurts its generalization to unseen inputs, that is known as the \"exposure bias\" problem. In this work, we propose to solve the conditional text generation problem by contrasting positive pairs with negative pairs, such that the model is exposed to various valid or incorrect perturbations of the inputs, for improved generalization. However, training the model with naïve contrastive learning framework using random non-target sequences as negative examples is suboptimal, since they are easily distinguishable from the correct output, especially so with models pretrained with large text corpora. Also, generating positive examples requires domain-specific augmentation heuristics which may not generalize over diverse domains. To tackle this problem, we propose a principled method to generate positive and negative samples for contrastive learning of seq2seq models. Specifically, we generate negative examples by adding small perturbations to the input sequence to minimize its conditional likelihood, and positive examples by adding large perturbations while enforcing it to have a high conditional likelihood. Such `\"hard'' positive and negative pairs generated using our method guides the model to better distinguish correct outputs from incorrect ones. We empirically show that our proposed method significantly improves the generalization of the seq2seq on three text generation tasks --- machine translation, text summarization, and question generation",
    "checked": true,
    "id": "7b00dcca92337be90a4c4c100a5704dc1932c303",
    "semantic_title": "contrastive learning with adversarial perturbations for conditional text generation",
    "citation_count": 109,
    "authors": []
  },
  "https://openreview.net/forum?id=FZ1oTwcXchK": {
    "title": "Optimal Conversion of Conventional Artificial Neural Networks to Spiking Neural Networks",
    "volume": "poster",
    "abstract": "Spiking neural networks (SNNs) are biology-inspired artificial neural networks (ANNs) that comprise of spiking neurons to process asynchronous discrete signals. While more efficient in power consumption and inference speed on the neuromorphic hardware, SNNs are usually difficult to train directly from scratch with spikes due to the discreteness. As an alternative, many efforts have been devoted to converting conventional ANNs into SNNs by copying the weights from ANNs and adjusting the spiking threshold potential of neurons in SNNs. Researchers have designed new SNN architectures and conversion algorithms to diminish the conversion error. However, an effective conversion should address the difference between the SNN and ANN architectures with an efficient approximation of the loss function, which is missing in the field. In this work, we analyze the conversion error by recursive reduction to layer-wise summation and propose a novel strategic pipeline that transfers the weights to the target SNN by combining threshold balance and soft-reset mechanisms. This pipeline enables almost no accuracy loss between the converted SNNs and conventional ANNs with only $\\sim1/10$ of the typical SNN simulation time. Our method is promising to get implanted onto embedded platforms with better support of SNNs with limited energy and memory. Codes are available at https://github.com/Jackn0/snn_optimal_conversion_pipeline",
    "checked": true,
    "id": "05711a79bf9e00bc7ebf8cbbcdb2d0d2f2c6679b",
    "semantic_title": "optimal conversion of conventional artificial neural networks to spiking neural networks",
    "citation_count": 213,
    "authors": []
  },
  "https://openreview.net/forum?id=EKV158tSfwv": {
    "title": "Efficient Continual Learning with Modular Networks and Task-Driven Priors",
    "volume": "poster",
    "abstract": "Existing literature in Continual Learning (CL) has focused on overcoming catastrophic forgetting, the inability of the learner to recall how to perform tasks observed in the past. There are however other desirable properties of a CL system, such as the ability to transfer knowledge from previous tasks and to scale memory and compute sub-linearly with the number of tasks. Since most current benchmarks focus only on forgetting using short streams of tasks, we first propose a new suite of benchmarks to probe CL algorithms across these new axes. Finally, we introduce a new modular architecture, whose modules represent atomic skills that can be composed to perform a certain task. Learning a task reduces to figuring out which past modules to re-use, and which new modules to instantiate to solve the current task. Our learning algorithm leverages a task-driven prior over the exponential search space of all possible ways to combine modules, enabling efficient learning on long streams of tasks. Our experiments show that this modular architecture and learning algorithm perform competitively on widely used CL benchmarks while yielding superior performance on the more challenging benchmarks we introduce in this work. The Benchmark is publicly available at https://github.com/facebookresearch/CTrLBenchmark",
    "checked": true,
    "id": "56912f12c35af9579999b45fe6ab7d5b9f090df6",
    "semantic_title": "efficient continual learning with modular networks and task-driven priors",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=6NFBvWlRXaG": {
    "title": "On the Universality of Rotation Equivariant Point Cloud Networks",
    "volume": "poster",
    "abstract": "Learning functions on point clouds has applications in many fields, including computer vision, computer graphics, physics, and chemistry. Recently, there has been a growing interest in neural architectures that are invariant or equivariant to all three shape-preserving transformations of point clouds: translation, rotation, and permutation. In this paper, we present a first study of the approximation power of these architectures. We first derive two sufficient conditions for an equivariant architecture to have the universal approximation property, based on a novel characterization of the space of equivariant polynomials. We then use these conditions to show that two recently suggested models, Tensor field Networks and SE3-Transformers, are universal, and for devising two other novel universal architectures",
    "checked": true,
    "id": "30827cfdabbb8e1f02c6fd7c188d5a7e6b05c6da",
    "semantic_title": "on the universality of rotation equivariant point cloud networks",
    "citation_count": 86,
    "authors": []
  },
  "https://openreview.net/forum?id=ATp1nW2FuZL": {
    "title": "Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces",
    "volume": "poster",
    "abstract": "Recent research has proposed neural architectures for solving combinatorial problems in structured output spaces. In many such problems, there may exist multiple solutions for a given input, e.g. a partially filled Sudoku puzzle may have many completions satisfying all constraints. Further, we are often interested in finding any \"one\" of the possible solutions, without any preference between them. Existing approaches completely ignore this solution multiplicity. In this paper, we argue that being oblivious to the presence of multiple solutions can severely hamper their training ability. Our contribution is two-fold. First, we formally define the task of learning one-of-many solutions for combinatorial problems in structured output spaces, which is applicable for solving several problems of interest such as N-Queens, and Sudoku. Second, we present a generic learning framework that adapts an existing prediction network for a combinatorial problem to handle solution multiplicity. Our framework uses a selection module, whose goal is to dynamically determine, for every input, the solution that is most effective for training the network parameters in any given learning iteration. We propose an RL based approach to jointly train the selection module with the prediction network. Experiments on three different domains, and using two different prediction networks, demonstrate that our framework significantly improves the accuracy in our setting, obtaining up to 21 pt gain over the baselines",
    "checked": true,
    "id": "51c62d63c6204deecb24a1d3f9ea8e0a42d23817",
    "semantic_title": "neural learning of one-of-many solutions for combinatorial problems in structured output spaces",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=SHvF5xaueVn": {
    "title": "GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images",
    "volume": "poster",
    "abstract": "We tackle a challenging blind image denoising problem, in which only single distinct noisy images are available for training a denoiser, and no information about noise is known, except for it being zero-mean, additive, and independent of the clean image. In such a setting, which often occurs in practice, it is not possible to train a denoiser with the standard discriminative training or with the recently developed Noise2Noise (N2N) training; the former requires the underlying clean image for the given noisy image, and the latter requires two independently realized noisy image pair for a clean image. To that end, we propose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise) method that first learns a generative model that can 1) simulate the noise in the given noisy images and 2) generate a rough, noisy estimates of the clean images, then 3) iteratively trains a denoiser with subsequently synthesized noisy image pairs (as in N2N), obtained from the generative model. In results, we show the denoiser trained with our GAN2GAN achieves an impressive denoising performance on both synthetic and real-world datasets for the blind denoising setting; it almost approaches the performance of the standard discriminatively-trained or N2N-trained models that have more information than ours, and it significantly outperforms the recent baseline for the same setting, \\textit{e.g.}, Noise2Void, and a more conventional yet strong one, BM3D. The official code of our method is available at https://github.com/csm9493/GAN2GAN",
    "checked": true,
    "id": "a40a41206214e7c400a7bc67867af95b6328e2f6",
    "semantic_title": "gan2gan: generative noise learning for blind denoising with single noisy images",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=F2v4aqEL6ze": {
    "title": "CPR: Classifier-Projection Regularization for Continual Learning",
    "volume": "poster",
    "abstract": "We propose a general, yet simple patch that can be applied to existing regularization-based continual learning methods called classifier-projection regularization (CPR). Inspired by both recent results on neural networks with wide local minima and information theory, CPR adds an additional regularization term that maximizes the entropy of a classifier's output probability. We demonstrate that this additional term can be interpreted as a projection of the conditional probability given by a classifier's output to the uniform distribution. By applying the Pythagorean theorem for KL divergence, we then prove that this projection may (in theory) improve the performance of continual learning methods. In our extensive experimental results, we apply CPR to several state-of-the-art regularization-based continual learning methods and benchmark performance on popular image recognition datasets. Our results demonstrate that CPR indeed promotes a wide local minima and significantly improves both accuracy and plasticity while simultaneously mitigating the catastrophic forgetting of baseline continual learning methods. The codes and scripts for this work are available at https://github.com/csm9493/CPR_CL",
    "checked": true,
    "id": "4a339a899a48d74545ad056e71e3b0107987c7f3",
    "semantic_title": "cpr: classifier-projection regularization for continual learning",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=1OCTOShAmqB": {
    "title": "On the Dynamics of Training Attention Models",
    "volume": "poster",
    "abstract": "The attention mechanism has been widely used in deep neural networks as a model component. By now, it has become a critical building block in many state-of-the-art natural language models. Despite its great success established empirically, the working mechanism of attention has not been investigated at a sufficient theoretical depth to date. In this paper, we set up a simple text classification task and study the dynamics of training a simple attention-based classification model using gradient descent. In this setting, we show that, for the discriminative words that the model should attend to, a persisting identity exists relating its embedding and the inner product of its key and the query. This allows us to prove that training must converge to attending to the discriminative words when the attention output is classified by a linear classifier. Experiments are performed, which validate our theoretical analysis and provide further insights",
    "checked": true,
    "id": "4d5c9a2a55b64bcf2cd01c5aa954776575c98500",
    "semantic_title": "on the dynamics of training attention models",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=OMNB1G5xzd4": {
    "title": "Model-Based Offline Planning",
    "volume": "poster",
    "abstract": "Offline learning is a key part of making reinforcement learning (RL) useable in real systems. Offline RL looks at scenarios where there is data from a system's operation, but no direct access to the system when learning a policy. Recent work on training RL policies from offline data has shown results both with model-free policies learned directly from the data, or with planning on top of learnt models of the data. Model-free policies tend to be more performant, but are more opaque, harder to command externally, and less easy to integrate into larger systems. We propose an offline learner that generates a model that can be used to control the system directly through planning. This allows us to have easily controllable policies directly from data, without ever interacting with the system. We show the performance of our algorithm, Model-Based Offline Planning (MBOP) on a series of robotics-inspired tasks, and demonstrate its ability leverage planning to respect environmental constraints. We are able to find near-optimal polices for certain simulated systems from as little as 50 seconds of real-time system interaction, and create zero-shot goal-conditioned policies on a series of environments",
    "checked": true,
    "id": "3cb8e96faba73efa027fa858e2a78cd1fc3c6e4d",
    "semantic_title": "model-based offline planning",
    "citation_count": 155,
    "authors": []
  },
  "https://openreview.net/forum?id=kLbhLJ8OT12": {
    "title": "Modelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System",
    "volume": "poster",
    "abstract": "Designing task-oriented dialogue systems is a challenging research topic, since it needs not only to generate utterances fulfilling user requests but also to guarantee the comprehensibility. Many previous works trained end-to-end (E2E) models with supervised learning (SL), however, the bias in annotated system utterances remains as a bottleneck. Reinforcement learning (RL) deals with the problem through using non-differentiable evaluation metrics (e.g., the success rate) as rewards. Nonetheless, existing works with RL showed that the comprehensibility of generated system utterances could be corrupted when improving the performance on fulfilling user requests. In our work, we (1) propose modelling the hierarchical structure between dialogue policy and natural language generator (NLG) with the option framework, called HDNO, where the latent dialogue act is applied to avoid designing specific dialogue act representations; (2) train HDNO via hierarchical reinforcement learning (HRL), as well as suggest the asynchronous updates between dialogue policy and NLG during training to theoretically guarantee their convergence to a local maximizer; and (3) propose using a discriminator modelled with language models as an additional reward to further improve the comprehensibility. We test HDNO on MultiWoz 2.0 and MultiWoz 2.1, the datasets on multi-domain dialogues, in comparison with word-level E2E model trained with RL, LaRL and HDSA, showing improvements on the performance evaluated by automatic evaluation metrics and human evaluation. Finally, we demonstrate the semantic meanings of latent dialogue acts to show the explanability for HDNO",
    "checked": true,
    "id": "cf4ce485d4971b17c8733188da65065d153a68a5",
    "semantic_title": "modelling hierarchical structure between dialogue policy and natural language generator with option framework for task-oriented dialogue system",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=NTEz-6wysdb": {
    "title": "Distilling Knowledge from Reader to Retriever for Question Answering",
    "volume": "poster",
    "abstract": "The task of information retrieval is an important component of many natural language processing systems, such as open domain question answering. While traditional methods were based on hand-crafted features, continuous representations based on neural networks recently obtained competitive results. A challenge of using such methods is to obtain supervised data to train the retriever model, corresponding to pairs of query and support documents. In this paper, we propose a technique to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever. We evaluate our method on question answering, obtaining state-of-the-art results",
    "checked": true,
    "id": "2b4bc49a3b23229a060609380752666b24b435fb",
    "semantic_title": "distilling knowledge from reader to retriever for question answering",
    "citation_count": 270,
    "authors": []
  },
  "https://openreview.net/forum?id=hr-3PMvDpil": {
    "title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers",
    "volume": "poster",
    "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches",
    "checked": true,
    "id": "0427939b1539c681147b97e3ce5ddb57633b8281",
    "semantic_title": "efficient certified defenses against patch attacks on image classifiers",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=lU5Rs_wCweN": {
    "title": "Taking Notes on the Fly Helps Language Pre-Training",
    "volume": "poster",
    "abstract": "How to make unsupervised language pre-training more efficient and less resource-intensive is an important research direction in NLP. In this paper, we focus on improving the efficiency of language pre-training methods through providing better data utilization. It is well-known that in language data corpus, words follow a heavy-tail distribution. A large proportion of words appear only very few times and the embeddings of rare words are usually poorly optimized. We argue that such embeddings carry inadequate semantic signals, which could make the data utilization inefficient and slow down the pre-training of the entire model. To mitigate this problem, we propose Taking Notes on the Fly (TNF), which takes notes for rare words on the fly during pre-training to help the model understand them when they occur next time. Specifically, TNF maintains a note dictionary and saves a rare word's contextual information in it as notes when the rare word occurs in a sentence. When the same rare word occurs again during training, the note information saved beforehand can be employed to enhance the semantics of the current sentence. By doing so, TNF provides a better data utilization since cross-sentence information is employed to cover the inadequate semantics caused by rare words in the sentences. We implement TNF on both BERT and ELECTRA to check its efficiency and effectiveness. Experimental results show that TNF's training time is 60% less than its backbone pre-training models when reaching the same performance. When trained with same number of iterations, TNF outperforms its backbone methods on most of downstream tasks and the average GLUE score. Code is attached in the supplementary material",
    "checked": true,
    "id": "8988742cb5658634e2a173d0d4baaaab17304229",
    "semantic_title": "taking notes on the fly helps language pre-training",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=dlEJsyHGeaL": {
    "title": "Graph Edit Networks",
    "volume": "poster",
    "abstract": "While graph neural networks have made impressive progress in classification and regression, few approaches to date perform time series prediction on graphs, and those that do are mostly limited to edge changes. We suggest that graph edits are a more natural interface for graph-to-graph learning. In particular, graph edits are general enough to describe any graph-to-graph change, not only edge changes; they are sparse, making them easier to understand for humans and more efficient computationally; and they are local, avoiding the need for pooling layers in graph neural networks. In this paper, we propose a novel output layer - the graph edit network - which takes node embeddings as input and generates a sequence of graph edits that transform the input graph to the output graph. We prove that a mapping between the node sets of two graphs is sufficient to construct training data for a graph edit network and that an optimal mapping yields edit scripts that are almost as short as the graph edit distance between the graphs. We further provide a proof-of-concept empirical evaluation on several graph dynamical systems, which are difficult to learn for baselines from the literature",
    "checked": true,
    "id": "f45e441b34c8b6825926b027fd440cedfa64f9df",
    "semantic_title": "graph edit networks",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=8cpHIfgY4Dj": {
    "title": "FOCAL: Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization",
    "volume": "poster",
    "abstract": "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks",
    "checked": true,
    "id": "2cef71e693c3c1e6cc48e9387d9e3bec1ad1b5bb",
    "semantic_title": "focal: efficient fully-offline meta-reinforcement learning via distance metric learning and behavior regularization",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=33rtZ4Sjwjn": {
    "title": "Effective and Efficient Vote Attack on Capsule Networks",
    "volume": "poster",
    "abstract": "Standard Convolutional Neural Networks (CNNs) can be easily fooled by images with small quasi-imperceptible artificial perturbations. As alternatives to CNNs, the recently proposed Capsule Networks (CapsNets) are shown to be more robust to white-box attack than CNNs under popular attack protocols. Besides, the class-conditional reconstruction part of CapsNets is also used to detect adversarial examples. In this work, we investigate the adversarial robustness of CapsNets, especially how the inner workings of CapsNets change when the output capsules are attacked. The first observation is that adversarial examples misled CapsNets by manipulating the votes from primary capsules. Another observation is the high computational cost, when we directly apply multi-step attack methods designed for CNNs to attack CapsNets, due to the computationally expensive routing mechanism. Motivated by these two observations, we propose a novel vote attack where we attack votes of CapsNets directly. Our vote attack is not only effective, but also efficient by circumventing the routing process. Furthermore, we integrate our vote attack into the detection-aware attack paradigm, which can successfully bypass the class-conditional reconstruction based detection method. Extensive experiments demonstrate the superior attack performance of our vote attack on CapsNets",
    "checked": true,
    "id": "a385fe3de8679357cdb1985f4dc46b3d6af4130d",
    "semantic_title": "effective and efficient vote attack on capsule networks",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=rkQuFUmUOg3": {
    "title": "Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets",
    "volume": "poster",
    "abstract": "Despite the success of recent Neural Architecture Search (NAS) methods on various tasks which have shown to output networks that largely outperform human-designed networks, conventional NAS methods have mostly tackled the optimization of searching for the network architecture for a single task (dataset), which does not generalize well across multiple tasks (datasets). Moreover, since such task-specific methods search for a neural architecture from scratch for every given task, they incur a large computational cost, which is problematic when the time and monetary budget are limited. In this paper, we propose an efficient NAS framework that is trained once on a database consisting of datasets and pretrained networks and can rapidly search for a neural architecture for a novel dataset. The proposed MetaD2A (Meta Dataset-to-Architecture) model can stochastically generate graphs (architectures) from a given set (dataset) via a cross-modal latent space learned with amortized meta-learning. Moreover, we also propose a meta-performance predictor to estimate and select the best architecture without direct training on target datasets. The experimental results demonstrate that our model meta-learned on subsets of ImageNet-1K and architectures from NAS-Bench 201 search space successfully generalizes to multiple unseen datasets including CIFAR-10 and CIFAR-100, with an average search time of 33 GPU seconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than NSGANetV2, a transferable NAS method, with comparable performance. We believe that the MetaD2A proposes a new research direction for rapid NAS as well as ways to utilize the knowledge from rich databases of datasets and architectures accumulated over the past years. Code is available at https://github.com/HayeonLee/MetaD2A",
    "checked": true,
    "id": "067df3c59a57ab8a03dd1ede1ee6ee0af3adc491",
    "semantic_title": "rapid neural architecture search by learning to generate graphs from datasets",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=edJ_HipawCa": {
    "title": "Impact of Representation Learning in Linear Bandits",
    "volume": "poster",
    "abstract": "We study how representation learning can improve the efficiency of bandit problems. We study the setting where we play $T$ linear bandits with dimension $d$ concurrently, and these $T$ bandit tasks share a common $k (\\ll d)$ dimensional linear representation. For the finite-action setting, we present a new algorithm which achieves $\\widetilde{O}(T\\sqrt{kN} + \\sqrt{dkNT})$ regret, where $N$ is the number of rounds we play for each bandit. When $T$ is sufficiently large, our algorithm significantly outperforms the naive algorithm (playing $T$ bandits independently) that achieves $\\widetilde{O}(T\\sqrt{d N})$ regret. We also provide an $\\Omega(T\\sqrt{kN} + \\sqrt{dkNT})$ regret lower bound, showing that our algorithm is minimax-optimal up to poly-logarithmic factors. Furthermore, we extend our algorithm to the infinite-action setting and obtain a corresponding regret bound which demonstrates the benefit of representation learning in certain regimes. We also present experiments on synthetic and real-world data to illustrate our theoretical findings and demonstrate the effectiveness of our proposed algorithms",
    "checked": true,
    "id": "401084c502524652ea335023406606d72857ea34",
    "semantic_title": "impact of representation learning in linear bandits",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=gwnoVHIES05": {
    "title": "Creative Sketch Generation",
    "volume": "poster",
    "abstract": "Sketching or doodling is a popular creative activity that people engage in. However, most existing work in automatic sketch understanding or generation has focused on sketches that are quite mundane. In this work, we introduce two datasets of creative sketches -- Creative Birds and Creative Creatures -- containing 10k sketches each along with part annotations. We propose DoodlerGAN -- a part-based Generative Adversarial Network (GAN) -- to generate unseen compositions of novel part appearances. Quantitative evaluations as well as human studies demonstrate that sketches generated by our approach are more creative and of higher quality than existing approaches. In fact, in Creative Birds, subjects prefer sketches generated by DoodlerGAN over those drawn by humans!",
    "checked": true,
    "id": "9cc1c31cf4c399d7749815037479844b9d8b151c",
    "semantic_title": "creative sketch generation",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=068E_JSq9O": {
    "title": "Self-supervised Representation Learning with Relative Predictive Coding",
    "volume": "poster",
    "abstract": "This paper introduces Relative Predictive Coding (RPC), a new contrastive representation learning objective that maintains a good balance among training stability, minibatch size sensitivity, and downstream task performance. The key to the success of RPC is two-fold. First, RPC introduces the relative parameters to regularize the objective for boundedness and low variance. Second, RPC contains no logarithm and exponential score functions, which are the main cause of training instability in prior contrastive objectives. We empirically verify the effectiveness of RPC on benchmark vision and speech self-supervised learning tasks. Lastly, we relate RPC with mutual information (MI) estimation, showing RPC can be used to estimate MI with low variance",
    "checked": true,
    "id": "c47a26236cbd5446c4b1897ec5eafa8a9bfbce54",
    "semantic_title": "self-supervised representation learning with relative predictive coding",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=uz5uw6gM0m": {
    "title": "One Network Fits All? Modular versus Monolithic Task Formulations in Neural Networks",
    "volume": "poster",
    "abstract": "Can deep learning solve multiple, very different tasks simultaneously? We investigate how the representations of the underlying tasks affect the ability of a single neural network to learn them jointly. We present theoretical and empirical findings that a single neural network is capable of simultaneously learning multiple tasks from a combined data set, for a variety of methods for representing tasks---for example, when the distinct tasks are encoded by well-separated clusters or decision trees over some task-code attributes. Indeed, more strongly, we present a novel analysis that shows that families of simple programming-like constructs for the codes encoding the tasks are learnable by two-layer neural networks with standard training. We study more generally how the complexity of learning such combined tasks grows with the complexity of the task codes; we find that learning many tasks can be provably hard, even though the individual tasks are easy to learn. We provide empirical support for the usefulness of the learning bounds by training networks on clusters, decision trees, and SQL-style aggregation",
    "checked": true,
    "id": "d67c96c7948b9b0e6c9f9ea553ea0e6186e2b9dc",
    "semantic_title": "one network fits all? modular versus monolithic task formulations in neural networks",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=de11dbHzAMF": {
    "title": "Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data",
    "volume": "poster",
    "abstract": "Multi-Task Learning (MTL) networks have emerged as a promising method for transferring learned knowledge across different tasks. However, MTL must deal with challenges such as: overfitting to low resource tasks, catastrophic forgetting, and negative task transfer, or learning interference. Often, in Natural Language Processing (NLP), a separate model per task is needed to obtain the best performance. However, many fine-tuning approaches are both parameter inefficient, i.e., potentially involving one new model per task, and highly susceptible to losing knowledge acquired during pretraining. We propose a novel Transformer based Hypernetwork Adapter consisting of a new conditional attention mechanism as well as a set of task-conditioned modules that facilitate weight sharing. Through this construction, we achieve more efficient parameter sharing and mitigate forgetting by keeping half of the weights of a pretrained model fixed. We also use a new multi-task data sampling strategy to mitigate the negative effects of data imbalance across tasks. Using this approach, we are able to surpass single task fine-tuning methods while being parameter and data efficient (using around 66% of the data). Compared to other BERT Large methods on GLUE, our 8-task model surpasses other Adapter methods by 2.8% and our 24-task model outperforms by 0.7-1.0% models that use MTL and single task fine-tuning. We show that a larger variant of our single multi-task model approach performs competitively across 26 NLP tasks and yields state-of-the-art results on a number of test and development sets",
    "checked": true,
    "id": "55c4a747855c74210919c45f7899e1f79e4c97f5",
    "semantic_title": "conditionally adaptive multi-task learning: improving transfer learning in nlp using fewer parameters & less data",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=04cII6MumYV": {
    "title": "A Universal Representation Transformer Layer for Few-Shot Image Classification",
    "volume": "poster",
    "abstract": "Few-shot classification aims to recognize unseen classes when presented with only a small number of samples. We consider the problem of multi-domain few-shot image classification, where unseen classes and examples come from diverse data sources. This problem has seen growing interest and has inspired the development of benchmarks such as Meta-Dataset. A key challenge in this multi-domain setting is to effectively integrate the feature representations from the diverse set of training domains. Here, we propose a Universal Representation Transformer (URT) layer, that meta-learns to leverage universal features for few-shot classification by dynamically re-weighting and composing the most appropriate domain-specific representations. In experiments, we show that URT sets a new state-of-the-art result on Meta-Dataset. Specifically, it achieves top-performance on the highest number of data sources compared to competing methods. We analyze variants of URT and present a visualization of the attention score heatmaps that sheds light on how the model performs cross-domain generalization",
    "checked": true,
    "id": "6e1efe22d5696269aff7addcb438f77ff6cc2508",
    "semantic_title": "a universal representation transformer layer for few-shot image classification",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=-mWcQVLPSPy": {
    "title": "Isometric Propagation Network for Generalized Zero-shot Learning",
    "volume": "poster",
    "abstract": "Zero-shot learning (ZSL) aims to classify images of an unseen class only based on a few attributes describing that class but no access to any training sample. A popular strategy is to learn a mapping between the semantic space of class attributes and the visual space of images based on the seen classes and their data. Thus, an unseen class image can be ideally mapped to its corresponding class attributes. The key challenge is how to align the representations in the two spaces. For most ZSL settings, the attributes for each seen/unseen class are only represented by a vector while the seen-class data provide much more information. Thus, the imbalanced supervision from the semantic and the visual space can make the learned mapping easily overfitting to the seen classes. To resolve this problem, we propose Isometric Propagation Network (IPN), which learns to strengthen the relation between classes within each space and align the class dependency in the two spaces. Specifically, IPN learns to propagate the class representations on an auto-generated graph within each space. In contrast to only aligning the resulted static representation, we regularize the two dynamic propagation procedures to be isometric in terms of the two graphs' edge weights per step by minimizing a consistency loss between them. IPN achieves state-of-the-art performance on three popular ZSL benchmarks. To evaluate the generalization capability of IPN, we further build two larger benchmarks with more diverse unseen classes and demonstrate the advantages of IPN on them",
    "checked": true,
    "id": "26147e2b3f82417b56f61ee01b02979396b7e1b7",
    "semantic_title": "isometric propagation network for generalized zero-shot learning",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=IMPnRXEWpvr": {
    "title": "Towards Impartial Multi-task Learning",
    "volume": "poster",
    "abstract": "Multi-task learning (MTL) has been widely used in representation learning. However, naively training all tasks simultaneously may lead to the partial training issue, where specific tasks are trained more adequately than others. In this paper, we propose to learn multiple tasks impartially. Specifically, for the task-shared parameters, we optimize the scaling factors via a closed-form solution, such that the aggregated gradient (sum of raw gradients weighted by the scaling factors) has equal projections onto individual tasks. For the task-specific parameters, we dynamically weigh the task losses so that all of them are kept at a comparable scale. Further, we find the above gradient balance and loss balance are complementary and thus propose a hybrid balance method to further improve the performance. Our impartial multi-task learning (IMTL) can be end-to-end trained without any heuristic hyper-parameter tuning, and is general to be applied on all kinds of losses without any distribution assumption. Moreover, our IMTL can converge to similar results even when the task losses are designed to have different scales, and thus it is scale-invariant. We extensively evaluate our IMTL on the standard MTL benchmarks including Cityscapes, NYUv2 and CelebA. It outperforms existing loss weighting methods under the same experimental settings",
    "checked": true,
    "id": "45c0828baec1dd53b81f1b2635788fdf27d0792d",
    "semantic_title": "towards impartial multi-task learning",
    "citation_count": 186,
    "authors": []
  },
  "https://openreview.net/forum?id=Uu1Nw-eeTxJ": {
    "title": "On Learning Universal Representations Across Languages",
    "volume": "poster",
    "abstract": "Recent studies have demonstrated the overwhelming advantage of cross-lingual pre-trained models (PTMs), such as multilingual BERT and XLM, on cross-lingual NLP tasks. However, existing approaches essentially capture the co-occurrence among tokens through involving the masked language model (MLM) objective with token-level cross entropy. In this work, we extend these approaches to learn sentence-level representations and show the effectiveness on cross-lingual understanding and generation. Specifically, we propose a Hierarchical Contrastive Learning (HiCTL) method to (1) learn universal representations for parallel sentences distributed in one or multiple languages and (2) distinguish the semantically-related words from a shared cross-lingual vocabulary for each sentence. We conduct evaluations on two challenging cross-lingual tasks, XTREME and machine translation. Experimental results show that the HiCTL outperforms the state-of-the-art XLM-R by an absolute gain of 4.2% accuracy on the XTREME benchmark as well as achieves substantial improvements on both of the high resource and low-resource English$\\rightarrow$X translation tasks over strong baselines",
    "checked": true,
    "id": "311909621177c397c6b7099beff32332124f7d46",
    "semantic_title": "on learning universal representations across languages",
    "citation_count": 86,
    "authors": []
  },
  "https://openreview.net/forum?id=xYGNO86OWDH": {
    "title": "Isotropy in the Contextual Embedding Space: Clusters and Manifolds",
    "volume": "poster",
    "abstract": "The geometric properties of contextual embedding spaces for deep language models such as BERT and ERNIE, have attracted considerable attention in recent years. Investigations on the contextual embeddings demonstrate a strong anisotropic space such that most of the vectors fall within a narrow cone, leading to high cosine similarities. It is surprising that these LMs are as successful as they are, given that most of their embedding vectors are as similar to one another as they are. In this paper, we argue that the isotropy indeed exists in the space, from a different but more constructive perspective. We identify isolated clusters and low dimensional manifolds in the contextual embedding space, and introduce tools to both qualitatively and quantitatively analyze them. We hope the study in this paper could provide insights towards a better understanding of the deep language models",
    "checked": true,
    "id": "2a8bb26654c015f663670d1e0745f870071d5507",
    "semantic_title": "isotropy in the contextual embedding space: clusters and manifolds",
    "citation_count": 117,
    "authors": []
  },
  "https://openreview.net/forum?id=0-EYBhgw80y": {
    "title": "MoPro: Webly Supervised Learning with Momentum Prototypes",
    "volume": "poster",
    "abstract": "We propose a webly-supervised representation learning method that does not suffer from the annotation unscalability of supervised learning, nor the computation unscalability of self-supervised learning. Most existing works on webly-supervised representation learning adopt a vanilla supervised learning method without accounting for the prevalent noise in the training data, whereas most prior methods in learning with label noise are less effective for real-world large-scale noisy data. We propose momentum prototypes (MoPro), a simple contrastive learning method that achieves online label noise correction, out-of-distribution sample removal, and representation learning. MoPro achieves state-of-the-art performance on WebVision, a weakly-labeled noisy dataset. MoPro also shows superior performance when the pretrained model is transferred to down-stream image classification and detection tasks. It outperforms the ImageNet supervised pretrained model by +10.5 on 1-shot classification on VOC, and outperforms the best self-supervised pretrained model by +17.3 when finetuned on 1% of ImageNet labeled samples. Furthermore, MoPro is more robust to distribution shifts. Code and pretrained models are available at https://github.com/salesforce/MoPro",
    "checked": true,
    "id": "1cb29798801b315d6287aae1093f5432f54673dc",
    "semantic_title": "mopro: webly supervised learning with momentum prototypes",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=jLoC4ez43PZ": {
    "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4083958684292f6fa2f5c7fd4f9be975e80145b6",
    "semantic_title": "graphcodebert: pre-training code representations with data flow",
    "citation_count": 1205,
    "authors": []
  },
  "https://openreview.net/forum?id=HOFxeCutxZR": {
    "title": "Enjoy Your Editing: Controllable GANs for Image Editing via Latent Space Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3cdac7e1a3904a9458f55694d1dc6e6374659a02",
    "semantic_title": "enjoy your editing: controllable gans for image editing via latent space navigation",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=c8P9NQVtmnO": {
    "title": "Fourier Neural Operator for Parametric Partial Differential Equations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2f7dc1ee85e9f6a97810c66016e09ffeed684f03",
    "semantic_title": "fourier neural operator for parametric partial differential equations",
    "citation_count": 2621,
    "authors": []
  },
  "https://openreview.net/forum?id=vYeQQ29Tbvx": {
    "title": "Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "84ef2cf4f73bb4eae7ae63fbca04a4d774b75ac7",
    "semantic_title": "training batchnorm and only batchnorm: on the expressive power of random features in cnns",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=1Fqg133qRaI": {
    "title": "Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6c4fe31504d47b8547e47267c0cb4efa464f022b",
    "semantic_title": "towards faster and stabilized gan training for high-fidelity few-shot image synthesis",
    "citation_count": 244,
    "authors": []
  },
  "https://openreview.net/forum?id=iKQAk8a2kM0": {
    "title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1045b77db15e56cc1c87d35fe90332164f6ac5a8",
    "semantic_title": "targeted attack against deep neural networks via flipping limited weight bits",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=6YEQUn0QICG": {
    "title": "FedBN: Federated Learning on Non-IID Features via Local Batch Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2c0f4711c9c124a8dc056eaee82a2ca5ef276da8",
    "semantic_title": "fedbn: federated learning on non-iid features via local batch normalization",
    "citation_count": 860,
    "authors": []
  },
  "https://openreview.net/forum?id=Iz3zU3M316D": {
    "title": "AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e11e8d81c68cc782f564ed78e595b66790719804",
    "semantic_title": "adamp: slowing down the slowdown for momentum optimizers on scale-invariant weights",
    "citation_count": 137,
    "authors": []
  },
  "https://openreview.net/forum?id=wxRwhSdORKG": {
    "title": "Learning Subgoal Representations with Slow Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "01e80034058d471e3e15ced46b54647aa443af15",
    "semantic_title": "learning subgoal representations with slow dynamics",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=X76iqnUbBjz": {
    "title": "A Unified Approach to Interpreting and Boosting Adversarial Transferability",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "47b4744162537f40572cdd723f8f37fb489a3e75",
    "semantic_title": "a unified approach to interpreting and boosting adversarial transferability",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=dFwBosAcJkN": {
    "title": "Perceptual Adversarial Robustness: Defense Against Unseen Threat Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "473a854a939eca4bf39420ff496f8e24e223d460",
    "semantic_title": "perceptual adversarial robustness: defense against unseen threat models",
    "citation_count": 193,
    "authors": []
  },
  "https://openreview.net/forum?id=OQ08SN70M1V": {
    "title": "Better Fine-Tuning by Reducing Representational Collapse",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b88c11922cac84e5ea902f82d27ae21c3dda2e04",
    "semantic_title": "better fine-tuning by reducing representational collapse",
    "citation_count": 212,
    "authors": []
  },
  "https://openreview.net/forum?id=K9bw7vqp_s": {
    "title": "Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "5e38dc1ccf33ac1df09b8eb6476f110cb3d1966f",
    "semantic_title": "learning n: m fine-grained structured sparse neural networks from scratch",
    "citation_count": 253,
    "authors": []
  },
  "https://openreview.net/forum?id=OMizHuea_HB": {
    "title": "Active Contrastive Learning of Audio-Visual Video Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3505a5ab5fdc55a37479e8a1610862ad3314884b",
    "semantic_title": "active contrastive learning of audio-visual video representations",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=-TwO99rbVRu": {
    "title": "PseudoSeg: Designing Pseudo Labels for Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "217dd8145b5fd7ae7eaa921fe1e98b03039904ad",
    "semantic_title": "pseudoseg: designing pseudo labels for semantic segmentation",
    "citation_count": 310,
    "authors": []
  },
  "https://openreview.net/forum?id=BXewfAYMmJw": {
    "title": "Counterfactual Generative Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "011fbfe79c738b84c1bfcdabcd752977524b1363",
    "semantic_title": "counterfactual generative networks",
    "citation_count": 129,
    "authors": []
  },
  "https://openreview.net/forum?id=Q4EUywJIkqr": {
    "title": "Contemplating Real-World Object Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "149a9180d1a6c140d267a2cda133448b12b81321",
    "semantic_title": "contemplating real-world object classification",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=A5VV3UyIQz": {
    "title": "Explainable Deep One-Class Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "16a67491ed4bdb6293d1c2be35b0e8bae962cdeb",
    "semantic_title": "explainable deep one-class classification",
    "citation_count": 202,
    "authors": []
  },
  "https://openreview.net/forum?id=po-DLlBuAuz": {
    "title": "Batch Reinforcement Learning Through Continuation Method",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "69e40b090c15a426957f2a8879fdfaa7f45052ff",
    "semantic_title": "batch reinforcement learning through continuation method",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=LucJxySuJcE": {
    "title": "Protecting DNNs from Theft using an Ensemble of Diverse Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2c68b364549724dc2cbb479243e6dc4c33bb26e7",
    "semantic_title": "protecting dnns from theft using an ensemble of diverse models",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=6xHJ37MVxxp": {
    "title": "Domain Generalization with MixStyle",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4f6eafafc9563a5b904535078df7e74afe39ef59",
    "semantic_title": "domain generalization with mixstyle",
    "citation_count": 793,
    "authors": []
  },
  "https://openreview.net/forum?id=aGfU_xziEX8": {
    "title": "Efficient Inference of Flexible Interaction in Spiking-neuron Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7f4e7e8f4a8d6f32e4cfbd35c4f3892da1b90c05",
    "semantic_title": "efficient inference of flexible interaction in spiking-neuron networks",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=R2ZlTVPx0Gk": {
    "title": "DICE: Diversity in Deep Ensembles via Conditional Redundancy Adversarial Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a3ec0076139420a34d2079a029b0836b890e5d3c",
    "semantic_title": "dice: diversity in deep ensembles via conditional redundancy adversarial estimation",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=N33d7wjgzde": {
    "title": "Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d49c5ca5fd043f00b8400e4390d57d55a3c9daf8",
    "semantic_title": "universal weakly supervised segmentation by pixel-to-segment contrastive learning",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=Db4yerZTYkz": {
    "title": "Shape-Texture Debiased Neural Network Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b7826eb413218602e152e9affc96e14833bbb360",
    "semantic_title": "shape-texture debiased neural network training",
    "citation_count": 109,
    "authors": []
  },
  "https://openreview.net/forum?id=IDFQI9OY6K": {
    "title": "Interactive Weak Supervision: Learning Useful Heuristics for Data Labeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c50ac3bb4fbed9c92de1afb88bdb889716b6d469",
    "semantic_title": "interactive weak supervision: learning useful heuristics for data labeling",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=8e6BrwU6AjQ": {
    "title": "MoVie: Revisiting Modulated Convolutions for Visual Counting and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ef7a22c4eca639e2f2f035f0a95e8184aa9cfff1",
    "semantic_title": "movie: revisiting modulated convolutions for visual counting and beyond",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=uKhGRvM8QNH": {
    "title": "Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ba28c939b3e1f66d60e212457b2d76973ec7846f",
    "semantic_title": "improve object detection with feature-based knowledge distillation: towards accurate and efficient detectors",
    "citation_count": 216,
    "authors": []
  },
  "https://openreview.net/forum?id=YwpZmcAehZ": {
    "title": "Revisiting Dynamic Convolution via Matrix Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7cda4cc432db2797758d8fd1f2c4703523fd36d0",
    "semantic_title": "revisiting dynamic convolution via matrix decomposition",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=GTGb3M_KcUl": {
    "title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b8a4e778549f89da6352ae18208654f32621d0db",
    "semantic_title": "dynatune: dynamic tensor program optimization in deep neural network compilation",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=blfSjHeFM_e": {
    "title": "MALI: A memory efficient and reverse accurate integrator for Neural ODEs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "57afbedf7e3f51f952ce28f92dccf8349133a523",
    "semantic_title": "mali: a memory efficient and reverse accurate integrator for neural odes",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=Xb8xvrtB8Ce": {
    "title": "Bag of Tricks for Adversarial Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "67f74fe9d46f88661573003f8f1f12967ae49fa3",
    "semantic_title": "bag of tricks for adversarial training",
    "citation_count": 229,
    "authors": []
  },
  "https://openreview.net/forum?id=rABUmU3ulQh": {
    "title": "Learning to Generate 3D Shapes with Generative Cellular Automata",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "98ec1203d439de0f7d3fd65d215852c3d6ebadad",
    "semantic_title": "learning to generate 3d shapes with generative cellular automata",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=AHm3dbp7D1D": {
    "title": "SEED: Self-supervised Distillation For Visual Representation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2ff7d8d79c1ab50c1826d965475a1eb32db0c133",
    "semantic_title": "seed: self-supervised distillation for visual representation",
    "citation_count": 197,
    "authors": []
  },
  "https://openreview.net/forum?id=vLaHRtHvfFp": {
    "title": "PDE-Driven Spatiotemporal Disentanglement",
    "volume": "poster",
    "abstract": "A recent line of work in the machine learning community addresses the problem of predicting high-dimensional spatiotemporal phenomena by leveraging specific tools from the differential equations theory. Following this direction, we propose in this article a novel and general paradigm for this task based on a resolution method for partial differential equations: the separation of variables. This inspiration allows us to introduce a dynamical interpretation of spatiotemporal disentanglement. It induces a principled model based on learning disentangled spatial and temporal representations of a phenomenon to accurately predict future observations. We experimentally demonstrate the performance and broad applicability of our method against prior state-of-the-art models on physical and synthetic video datasets",
    "checked": true,
    "id": "fef00d097117b48129f0925e9c0165496cc024b1",
    "semantic_title": "pde-driven spatiotemporal disentanglement",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=gIHd-5X324": {
    "title": "Rethinking Soft Labels for Knowledge Distillation: A Bias–Variance Tradeoff Perspective",
    "volume": "poster",
    "abstract": "Knowledge distillation is an effective approach to leverage a well-trained network or an ensemble of them, named as the teacher, to guide the training of a student network. The outputs from the teacher network are used as soft labels for supervising the training of a new network. Recent studies (M ̈uller et al., 2019; Yuan et al., 2020) revealed an intriguing property of the soft labels that making labels soft serves as a good regularization to the student network. From the perspective of statistical learning, regularization aims to reduce the variance, however how bias and variance change is not clear for training with soft labels. In this paper, we investigate the bias-variance tradeoff brought by distillation with soft labels. Specifically, we observe that during training the bias-variance tradeoff varies sample-wisely. Further, under the same distillation temperature setting, we observe that the distillation performance is negatively associated with the number of some specific samples, which are named as regularization samples since these samples lead to bias increasing and variance decreasing. Nevertheless, we empirically find that completely filtering out regularization samples also deteriorates distillation performance. Our discoveries inspired us to propose the novel weighted soft labels to help the network adaptively handle the sample-wise bias-variance tradeoff. Experiments on standard evaluation benchmarks validate the effectiveness of our method. Our code is available in the supplementary",
    "checked": true,
    "id": "be51e9141ae2af4daf3a1ba745ad3ff66a5990f3",
    "semantic_title": "rethinking soft labels for knowledge distillation: a bias-variance tradeoff perspective",
    "citation_count": 178,
    "authors": []
  },
  "https://openreview.net/forum?id=H8UHdhWG6A3": {
    "title": "Distributed Momentum for Byzantine-resilient Stochastic Gradient Descent",
    "volume": "poster",
    "abstract": "Byzantine-resilient Stochastic Gradient Descent (SGD) aims at shielding model training from Byzantine faults, be they ill-labeled training datapoints, exploited software/hardware vulnerabilities, or malicious worker nodes in a distributed setting. Two recent attacks have been challenging state-of-the-art defenses though, often successfully precluding the model from even fitting the training set. The main identified weakness in current defenses is their requirement of a sufficiently low variance-norm ratio for the stochastic gradients. We propose a practical method which, despite increasing the variance, reduces the variance-norm ratio, mitigating the identified weakness. We assess the effectiveness of our method over 736 different training configurations, comprising the 2 state-of-the-art attacks and 6 defenses. For confidence and reproducibility purposes, each configuration is run 5 times with specified seeds (1 to 5), totalling 3680 runs. In our experiments, when the attack is effective enough to decrease the highest observed top-1 cross-accuracy by at least 20% compared to the unattacked run, our technique systematically increases back the highest observed accuracy, and is able to recover at least 20% in more than 60% of the cases",
    "checked": true,
    "id": "e175f23a38f7d589266b6e059ff8e2782d9a365b",
    "semantic_title": "distributed momentum for byzantine-resilient stochastic gradient descent",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=uQfOy7LrlTR": {
    "title": "Scaling the Convex Barrier with Active Sets",
    "volume": "poster",
    "abstract": "Tight and efficient neural network bounding is of critical importance for the scaling of neural network verification systems. A number of efficient specialised dual solvers for neural network bounds have been presented recently, but they are often too loose to verify more challenging properties. This lack of tightness is linked to the weakness of the employed relaxation, which is usually a linear program of size linear in the number of neurons. While a tighter linear relaxation for piecewise linear activations exists, it comes at the cost of exponentially many constraints and thus currently lacks an efficient customised solver. We alleviate this deficiency via a novel dual algorithm that realises the full potential of the new relaxation by operating on a small active set of dual variables. Our method recovers the strengths of the new relaxation in the dual space: tightness and a linear separation oracle. At the same time, it shares the benefits of previous dual approaches for weaker relaxations: massive parallelism, GPU implementation, low cost per iteration and valid bounds at any time. As a consequence, we obtain better bounds than off-the-shelf solvers in only a fraction of their running time and recover the speed-accuracy trade-offs of looser dual solvers if the computational budget is small. We demonstrate that this results in significant formal verification speed-ups",
    "checked": true,
    "id": "69e1fab8fb4d599420cea6803f02e06fe9e0d11b",
    "semantic_title": "scaling the convex barrier with active sets",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=TiXl51SCNw8": {
    "title": "BSQ: Exploring Bit-Level Sparsity for Mixed-Precision Neural Network Quantization",
    "volume": "poster",
    "abstract": "Mixed-precision quantization can potentially achieve the optimal tradeoff between performance and compression rate of deep neural networks, and thus, have been widely investigated. However, it lacks a systematic method to determine the exact quantization scheme. Previous methods either examine only a small manually-designed search space or utilize a cumbersome neural architecture search to explore the vast search space. These approaches cannot lead to an optimal quantization scheme efficiently. This work proposes bit-level sparsity quantization (BSQ) to tackle the mixed-precision quantization from a new angle of inducing bit-level sparsity. We consider each bit of quantized weights as an independent trainable variable and introduce a differentiable bit-sparsity regularizer. BSQ can induce all-zero bits across a group of weight elements and realize the dynamic precision reduction, leading to a mixed-precision quantization scheme of the original model. Our method enables the exploration of the full mixed-precision space with a single gradient-based optimization process, with only one hyperparameter to tradeoff the performance and compression. BSQ achieves both higher accuracy and higher bit reduction on various model architectures on the CIFAR-10 and ImageNet datasets comparing to previous methods",
    "checked": true,
    "id": "2f7741e0b19c5fd927bd0cae7de915c01c220563",
    "semantic_title": "bsq: exploring bit-level sparsity for mixed-precision neural network quantization",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=Pz_dcqfcKW8": {
    "title": "Dual-mode ASR: Unify and Improve Streaming ASR with Full-context Modeling",
    "volume": "poster",
    "abstract": "Streaming automatic speech recognition (ASR) aims to emit each hypothesized word as quickly and accurately as possible, while full-context ASR waits for the completion of a full speech utterance before emitting completed hypotheses. In this work, we propose a unified framework, Dual-mode ASR, to train a single end-to-end ASR model with shared weights for both streaming and full-context speech recognition. We show that the latency and accuracy of streaming ASR significantly benefit from weight sharing and joint training of full-context ASR, especially with inplace knowledge distillation during the training. The Dual-mode ASR framework can be applied to recent state-of-the-art convolution-based and transformer-based ASR networks. We present extensive experiments with two state-of-the-art ASR networks, ContextNet and Conformer, on two datasets, a widely used public dataset LibriSpeech and a large-scale dataset MultiDomain. Experiments and ablation studies demonstrate that Dual-mode ASR not only simplifies the workflow of training and deploying streaming and full-context ASR models, but also significantly improves both emission latency and recognition accuracy of streaming ASR. With Dual-mode ASR, we achieve new state-of-the-art streaming ASR results on both LibriSpeech and MultiDomain in terms of accuracy and latency",
    "checked": true,
    "id": "de9c16610ed1181710debba81d89a39dbde1fb50",
    "semantic_title": "dual-mode asr: unify and improve streaming asr with full-context modeling",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=pzpytjk3Xb2": {
    "title": "Policy-Driven Attack: Learning to Query for Hard-label Black-box Adversarial Examples",
    "volume": "poster",
    "abstract": "To craft black-box adversarial examples, adversaries need to query the victim model and take proper advantage of its feedback. Existing black-box attacks generally suffer from high query complexity, especially when only the top-1 decision (i.e., the hard-label prediction) of the victim model is available. In this paper, we propose a novel hard-label black-box attack named Policy-Driven Attack, to reduce the query complexity. Our core idea is to learn promising search directions of the adversarial examples using a well-designed policy network in a novel reinforcement learning formulation, in which the queries become more sensible. Experimental results demonstrate that our method can significantly reduce the query complexity in comparison with existing state-of-the-art hard-label black-box attacks on various image classification benchmark datasets. Code and models for reproducing our results are available at https://github.com/ZiangYan/pda.pytorch",
    "checked": true,
    "id": "e6902aee95f52edb6a38bd364b82e1ff68afd5ff",
    "semantic_title": "policy-driven attack: learning to query for hard-label black-box adversarial examples",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=O3bqkf_Puys": {
    "title": "PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences",
    "volume": "poster",
    "abstract": "Point cloud sequences are irregular and unordered in the spatial dimension while exhibiting regularities and order in the temporal dimension. Therefore, existing grid based convolutions for conventional video processing cannot be directly applied to spatio-temporal modeling of raw point cloud sequences. In this paper, we propose a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequences. Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution is used to model the dynamics of the spatial regions along the time dimension. Furthermore, we incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequences in a hierarchical manner. Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequences",
    "checked": true,
    "id": "1b10aa9cf95b41a19317dee2e8cc11c2edd73418",
    "semantic_title": "pstnet: point spatio-temporal convolution on point cloud sequences",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=Cb54AMqHQFP": {
    "title": "Network Pruning That Matters: A Case Study on Retraining Variants",
    "volume": "poster",
    "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning",
    "checked": true,
    "id": "dfe2efeea8889a1937be16538220f5e3477d42fb",
    "semantic_title": "network pruning that matters: a case study on retraining variants",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=lWaz5a9lcFU": {
    "title": "EEC: Learning to Encode and Regenerate Images for Continual Learning",
    "volume": "poster",
    "abstract": "The two main impediments to continual learning are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17% over state-of-the-art methods on benchmark datasets, while requiring 78% less storage space",
    "checked": true,
    "id": "557c15c4e3306676b8a4d053c7295880608d3fd0",
    "semantic_title": "eec: learning to encode and regenerate images for continual learning",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=KLH36ELmwIB": {
    "title": "DARTS-: Robustly Stepping out of Performance Collapse Without Indicators",
    "volume": "poster",
    "abstract": "Despite the fast development of differentiable architecture search (DARTS), it suffers from a standing instability issue regarding searching performance, which extremely limits its application. Existing robustifying methods draw clues from the outcome instead of finding out the causing factor. Various indicators such as Hessian eigenvalues are proposed as a signal of performance collapse, and the searching should be stopped once an indicator reaches a preset threshold. However, these methods tend to easily reject good architectures if thresholds are inappropriately set, let alone the searching is intrinsically noisy. In this paper, we undertake a more subtle and direct approach to resolve the collapse. We first demonstrate that skip connections with a learnable architectural coefficient can easily recover from a disadvantageous state and become dominant. We conjecture that skip connections profit too much from this privilege, hence causing the collapse for the derived model. Therefore, we propose to factor out this benefit with an auxiliary skip connection, ensuring a fairer competition for all operations. Extensive experiments on various datasets verify that our approach can substantially improve the robustness of DARTS. Our code is available at https://github.com/Meituan-AutoML/DARTS-",
    "checked": true,
    "id": "10daddaa1e1ed27e88b08b1c124d800de865c5e3",
    "semantic_title": "darts-: robustly stepping out of performance collapse without indicators",
    "citation_count": 170,
    "authors": []
  },
  "https://openreview.net/forum?id=9QLRCVysdlO": {
    "title": "BiPointNet: Binary Neural Network for Point Clouds",
    "volume": "poster",
    "abstract": "To alleviate the resource constraint for real-time point cloud applications that run on edge devices, in this paper we present BiPointNet, the first model binarization approach for efficient deep learning on point clouds. We discover that the immense performance drop of binarized models for point clouds mainly stems from two challenges: aggregation-induced feature homogenization that leads to a degradation of information entropy, and scale distortion that hinders optimization and invalidates scale-sensitive structures. With theoretical justifications and in-depth analysis, our BiPointNet introduces Entropy-Maximizing Aggregation (EMA) to modulate the distribution before aggregation for the maximum information entropy, and Layer-wise Scale Recovery (LSR) to efficiently restore feature representation capacity. Extensive experiments show that BiPointNet outperforms existing binarization methods by convincing margins, at the level even comparable with the full precision counterpart. We highlight that our techniques are generic, guaranteeing significant improvements on various fundamental tasks and mainstream backbones. Moreover, BiPointNet gives an impressive 14.7× speedup and 18.9× storage saving on real-world resource-constrained devices",
    "checked": true,
    "id": "404bf88bd0194e2ad1de79b4730c70bff5c841d4",
    "semantic_title": "bipointnet: binary neural network for point clouds",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=bM3L3I_853": {
    "title": "AdaFuse: Adaptive Temporal Fusion Network for Efficient Action Recognition",
    "volume": "poster",
    "abstract": "Temporal modelling is the key for efficient video action recognition. While understanding temporal information can improve recognition accuracy for dynamic actions, removing temporal redundancy and reusing past features can significantly save computation leading to efficient action recognition. In this paper, we introduce an adaptive temporal fusion network, called AdaFuse, that dynamically fuses channels from current and past feature maps for strong temporal modelling. Specifically, the necessary information from the historical convolution feature maps is fused with current pruned feature maps with the goal of improving both recognition accuracy and efficiency. In addition, we use a skipping operation to further reduce the computation cost of action recognition. Extensive experiments on SomethingV1 & V2, Jester and Mini-Kinetics show that our approach can achieve about 40% computation savings with comparable accuracy to state-of-the-art methods. The project page can be found at https://mengyuest.github.io/AdaFuse/",
    "checked": true,
    "id": "0b4f9d1c5dbf8e8f4735a5eb0434f264116b47ba",
    "semantic_title": "adafuse: adaptive temporal fusion network for efficient action recognition",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=M88oFvqp_9": {
    "title": "Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains",
    "volume": "poster",
    "abstract": "We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars). The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively. This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains. Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains",
    "checked": true,
    "id": "5e2b2ca2a088b86307e00b4534b18dc8d8ed7809",
    "semantic_title": "generating furry cars: disentangling object shape and appearance across multiple domains",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Ig-VyQc-MLK": {
    "title": "Pruning Neural Networks at Initialization: Why Are We Missing the Mark?",
    "volume": "poster",
    "abstract": "Recent work has explored the possibility of pruning neural networks at initialization. We assess proposals for doing so: SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), and magnitude pruning. Although these methods surpass the trivial baseline of random pruning, they remain below the accuracy of magnitude pruning after training, and we endeavor to understand why. We show that, unlike pruning after training, randomly shuffling the weights these methods prune within each layer or sampling new initial values preserves or improves accuracy. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune. This property suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both",
    "checked": true,
    "id": "0932abfd0fb90e8a28f7bd195633c9891bfd7ecb",
    "semantic_title": "pruning neural networks at initialization: why are we missing the mark?",
    "citation_count": 242,
    "authors": []
  },
  "https://openreview.net/forum?id=POWv6hDd9XH": {
    "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction",
    "volume": "poster",
    "abstract": "We study the challenging task of neural network quantization without end-to-end retraining, called Post-training Quantization (PTQ). PTQ usually requires a small subset of training data but produces less powerful quantized models than Quantization-Aware Training (QAT). In this work, we propose a novel PTQ framework, dubbed BRECQ, which pushes the limits of bitwidth in PTQ down to INT2 for the first time. BRECQ leverages the basic building blocks in neural networks and reconstructs them one-by-one. In a comprehensive theoretical study of the second-order error, we show that BRECQ achieves a good balance between cross-layer dependency and generalization error. To further employ the power of quantization, the mixed precision technique is incorporated in our framework by approximating the inter-layer and intra-layer sensitivity. Extensive experiments on various handcrafted and searched neural architectures are conducted for both image classification and object detection tasks. And for the first time we prove that, without bells and whistles, PTQ can attain 4-bit ResNet and MobileNetV2 comparable with QAT and enjoy 240 times faster production of quantized models. Codes are available at https://github.com/yhhhli/BRECQ",
    "checked": true,
    "id": "6dffdeb81ebc761a8cce1e36b8d140d5b8d2a0e3",
    "semantic_title": "brecq: pushing the limit of post-training quantization by block reconstruction",
    "citation_count": 464,
    "authors": []
  }
}