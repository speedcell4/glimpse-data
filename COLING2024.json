{
  "https://aclanthology.org/2024.lrec-main.1": {
    "title": "3AM: An Ambiguity-Aware Multi-Modal Machine Translation Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Ma",
      "Xuebo Liu",
      "Derek F. Wong",
      "Jun Rao",
      "Bei Li",
      "Liang Ding",
      "Lidia S. Chao",
      "Dacheng Tao",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.2": {
    "title": "A Benchmark Evaluation of Clinical Named Entity Recognition in French",
    "volume": "main",
    "abstract": "Background: Transformer-based language models have shown strong performance on many Natural Language Processing (NLP) tasks. Masked Language Models (MLMs) attract sustained interest because they can be adapted to different languages and sub-domains through training or fine-tuning on specific corpora while remaining lighter than modern Large Language Models (MLMs). Recently, several MLMs have been released for the biomedical domain in French, and experiments suggest that they outperform standard French counterparts. However, no systematic evaluation comparing all models on the same corpora is available. Objective: This paper presents an evaluation of masked language models for biomedical French on the task of clinical named entity recognition. Material and methods: We evaluate biomedical models CamemBERT-bio and DrBERT and compare them to standard French models CamemBERT, FlauBERT and FrAlBERT as well as multilingual mBERT using three publically available corpora for clinical named entity recognition in French. The evaluation set-up relies on gold-standard corpora as released by the corpus developers. Results: Results suggest that CamemBERT-bio outperforms DrBERT consistently while FlauBERT offers competitive performance and FrAlBERT achieves the lowest carbon footprint. Conclusion: This is the first benchmark evaluation of biomedical masked language models for French clinical entity recognition that compares model performance consistently on nested entity recognition using metrics covering performance and environmental impact",
    "checked": true,
    "id": "523388fecab9d7a744928bf329cbc308811c0ad7",
    "semantic_title": "a benchmark evaluation of clinical named entity recognition in french",
    "citation_count": 0,
    "authors": [
      "Nesrine Bannour",
      "Christophe Servan",
      "Aurélie Névéol",
      "Xavier Tannier"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.3": {
    "title": "A Benchmark for Recipe Understanding in Artificial Agents",
    "volume": "main",
    "abstract": "This paper introduces a novel benchmark that has been designed as a test bed for evaluating whether artificial agents are able to understand how to perform everyday activities, with a focus on the cooking domain. Understanding how to cook recipes is a highly challenging endeavour due to the underspecified and grounded nature of recipe texts, combined with the fact that recipe execution is a knowledge-intensive and precise activity. The benchmark comprises a corpus of recipes, a procedural semantic representation language of cooking actions, qualitative and quantitative kitchen simulators, and a standardised evaluation procedure. Concretely, the benchmark task consists in mapping a recipe formulated in natural language to a set of cooking actions that is precise enough to be executed in the simulated kitchen and yields the desired dish. To overcome the challenges inherent to recipe execution, this mapping process needs to incorporate reasoning over the recipe text, the state of the simulated kitchen environment, common-sense knowledge, knowledge of the cooking domain, and the action space of a virtual or robotic chef. This benchmark thereby addresses the growing interest in human-centric systems that combine natural language processing and situated reasoning to perform everyday activities",
    "checked": true,
    "id": "3ea2d14a565208c6cee460ebf2259145969c0efd",
    "semantic_title": "a benchmark for recipe understanding in artificial agents",
    "citation_count": 0,
    "authors": [
      "Jens Nevens",
      "Robin de Haes",
      "Rachel Ringe",
      "Mihai Pomarlan",
      "Robert Porzel",
      "Katrien Beuls",
      "Paul van Eecke"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.4": {
    "title": "ABLE: Agency-BeLiefs Embedding to Address Stereotypical Bias through Awareness Instead of Obliviousness",
    "volume": "main",
    "abstract": "Natural Language Processing (NLP) models tend to inherit and amplify stereotypical biases present in their training data, leading to harmful societal consequences. Current efforts to rectify these biases typically revolve around making models oblivious to bias, which is at odds with the idea that humans require increased awareness to tackle these biases better. This prompts a fundamental research question: are bias-oblivious models the only viable solution to combat stereotypical biases? This paper answers this question by proposing the Agency-BeLiefs Embedding (ABLE) model, a novel approach that actively encodes stereotypical biases into the embedding space. ABLE draws upon social psychological theory to acquire and represent stereotypical biases in the form of agency and belief scores rather than directly representing stereotyped groups. Our experimental results showcase ABLE's effectiveness in learning agency and belief stereotypes while preserving the language model's proficiency. Furthermore, we underscore the practical significance of incorporating stereotypes within the ABLE model by demonstrating its utility in various downstream tasks. Our approach exemplifies the potential benefits of addressing bias through awareness, as opposed to the prevailing approach of mitigating bias through obliviousness",
    "checked": true,
    "id": "bf46c77417ce45babd303eb397a78a83020ef319",
    "semantic_title": "able: agency-beliefs embedding to address stereotypical bias through awareness instead of obliviousness",
    "citation_count": 0,
    "authors": [
      "Michelle YoungJin Kim",
      "Junghwan Kim",
      "Kristen Johnson"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.5": {
    "title": "Abstractive Multi-Video Captioning: Benchmark Dataset Construction and Extensive Evaluation",
    "volume": "main",
    "abstract": "This paper introduces a new task, abstractive multi-video captioning, which focuses on abstracting multiple videos with natural language. Unlike conventional video captioning tasks generating a specific caption for a video, our task generates an abstract caption of the shared content in a video group containing multiple videos. To address our task, models must learn to understand each video in detail and have strong abstraction abilities to find commonalities among videos. We construct a benchmark dataset for abstractive multi-video captioning named AbstrActs. AbstrActs contains 13.5k video groups and corresponding abstract captions. As abstractive multi-video captioning models, we explore two approaches: end-to-end and cascade. For evaluation, we proposed a new metric, CocoA, which can evaluate the model performance based on the abstractness of the generated captions. In experiments, we report the impact of the way of combining multiple video features, the overall model architecture, and the number of input videos",
    "checked": true,
    "id": "8b445f45989fc090b29944db992b3c19d68eb130",
    "semantic_title": "abstractive multi-video captioning: benchmark dataset construction and extensive evaluation",
    "citation_count": 0,
    "authors": [
      "Rikito Takahashi",
      "Hirokazu Kiyomaru",
      "Chenhui Chu",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.6": {
    "title": "Abstract-level Deductive Reasoning for Pre-trained Language Models",
    "volume": "main",
    "abstract": "Pre-trained Language Models have been shown to be able to emulate deductive reasoning in natural language. However, PLMs are easily affected by irrelevant information (e.g., entity) in instance-level proofs when learning deductive reasoning. To address this limitation, we propose an Abstract-level Deductive Reasoner (ADR). ADR is trained to predict the abstract reasoning proof of each sample, which guides PLMs to learn general reasoning patterns rather than instance-level knowledge. Experimental results demonstrate that ADR significantly reduces the impact of PLMs learning instance-level knowledge (over 70%)",
    "checked": true,
    "id": "dd6634cfaec83a453beafb240a000ac27bcc3bad",
    "semantic_title": "abstract-level deductive reasoning for pre-trained language models",
    "citation_count": 0,
    "authors": [
      "Xin Wu",
      "Yi Cai",
      "Ho-fung Leung"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.7": {
    "title": "A Call for Clarity in Beam Search: How It Works and When It Stops",
    "volume": "main",
    "abstract": "Text generation with beam search has proven successful in a wide range of applications. We point out that, though largely overlooked in the literature, the commonly-used implementation of beam decoding (e.g., Hugging Face Transformers and fairseq) uses a first come, first served heuristic: it keeps a set of already completed sequences over time steps and stops when the size of this set reaches the beam size. Based on this finding, we introduce a patience factor, a simple modification to this beam decoding implementation, that generalizes the stopping criterion and provides flexibility to the depth of search. Empirical results demonstrate that adjusting this patience factor improves decoding performance of strong pretrained models on news text summarization and machine translation over diverse language pairs, with a negligible inference slowdown. Our approach only modifies one line of code and can be thus readily incorporated in any implementation. Further, we find that different versions of beam decoding result in large performance differences in summarization, demonstrating the need for clarity in specifying the beam search implementation in research work. Our code will be available upon publication",
    "checked": true,
    "id": "f290bc1c70688b7985b7cca863e4670398875595",
    "semantic_title": "a call for clarity in beam search: how it works and when it stops",
    "citation_count": 5,
    "authors": [
      "Jungo Kasai",
      "Keisuke Sakaguchi",
      "Ronan Le Bras",
      "Dragomir Radev",
      "Yejin Choi",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.8": {
    "title": "A Canonical Form for Flexible Multiword Expressions",
    "volume": "main",
    "abstract": "This paper proposes a canonical form for Multiword Expressions (MWEs), in particular for the Dutch language. The canonical form can be enriched with all kinds of annotations that can be used to describe the properties of the MWE and its components. It also introduces the DUCAME (DUtch CAnonical Multiword Expressions) lexical resource with more than 11k MWEs in canonical form. DUCAME is used in MWE-Finder to automatically generate queries for searching for flexible MWEs in large text corpora",
    "checked": true,
    "id": "528733889ef0bfe69ba1863e7187c1ed487a7e13",
    "semantic_title": "a canonical form for flexible multiword expressions",
    "citation_count": 1,
    "authors": [
      "Jan Odijk"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.9": {
    "title": "A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation",
    "volume": "main",
    "abstract": "Empowered by the large-scale pretrained language models, existing dialogue systems have demonstrated impressive performance conducting fluent and natural-sounding conversations. However, they are still plagued by the <b>hallucination</b> problem, causing unpredictable factual errors in the generated responses. Recently, knowledge-grounded dialogue generation models, that intentionally invoke external knowledge resources to more informative responses, are also proven to be effective in reducing hallucination. Following the idea of getting high-quality knowledge, a few efforts have achieved pretty good performance on this issue. As some inevitable knowledge noises may also lead to hallucinations, it is emergent to investigate the reason and future directions for building noise-tolerant methods in KGD tasks. In this paper, we analyze the causal story behind this problem with counterfactual reasoning methods. Based on the causal effect analysis, we propose a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction. Experimental results of our example implementation show that this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models. We hope our efforts can support and call for more attention to developing lightweight techniques towards robust and trusty dialogue systems",
    "checked": true,
    "id": "afdeef9585232642d18e7c6a7942b2395e94ede1",
    "semantic_title": "a cause-effect look at alleviating hallucination of knowledge-grounded dialogue generation",
    "citation_count": 0,
    "authors": [
      "Jifan Yu",
      "Xiaohan Zhang",
      "Yifan Xu",
      "Xuanyu Lei",
      "Zijun Yao",
      "Jing Zhang",
      "Lei Hou",
      "Juanzi Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.10": {
    "title": "Access Control Framework for Language Collections",
    "volume": "main",
    "abstract": "This paper introduces the licence-based access control framework developed by the Language Data Commons of Australia (LDaCA) for a range of language collections, with examples given of implementation for significant Indigenous and Australian English collections. Language collections may be curated for many reasons, such as documentation for language revival, for research, security or commercial purposes. Some language collections are created with the intention of being \"Open Access\"; publicly available with no restriction. Other collections require that access be limited to individuals or groups of people, either at the collection level or at the level of individual items, such as a recording. To facilitate access, while respecting the intended access conditions for a collection, or collection items, some form of user identification and authorisation process is typically required. The access control framework described in this paper is based upon descriptions of access conditions in easy-to-read licences which are stored alongside data files in the collections; and is implemented using identity-based authentication and authorisation systems where required. The framework accommodates accessibility needs from unrestricted to extremely limited access, is dynamic, and able to be modified in response to changes in access needs. Storing licences with the data is a significant development in separating language data and access requirements from access infrastructure",
    "checked": true,
    "id": "2c97588108de39b4313a42ab5954077b05314bdf",
    "semantic_title": "access control framework for language collections",
    "citation_count": 0,
    "authors": [
      "Ben Foley",
      "Peter Sefton",
      "Simon Musgrave",
      "Moises Sacal Bonequi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.11": {
    "title": "A Challenge Dataset and Effective Models for Conversational Stance Detection",
    "volume": "main",
    "abstract": "Previous stance detection studies typically concentrate on evaluating stances within individual instances, thereby exhibiting limitations in effectively modeling multi-party discussions concerning the same specific topic, as naturally transpire in authentic social media interactions. This constraint arises primarily due to the scarcity of datasets that authentically replicate real social media contexts, hindering the research progress of conversational stance detection. In this paper, we introduce a new multi-turn conversation stance detection dataset (called MT-CSD), which encompasses multiple targets for conversational stance detection. To derive stances from this challenging dataset, we propose a global-local attention network (GLAN) to address both long and short-range dependencies inherent in conversational data. Notably, even state-of-the-art stance detection methods, exemplified by GLAN, exhibit an accuracy of only 50.47%, highlighting the persistent challenges in conversational stance detection. Furthermore, our MT-CSD dataset serves as a valuable resource to catalyze advancements in cross-domain stance detection, where a classifier is adapted from a different yet related target. We believe that MT-CSD will contribute to advancing real-world applications of stance detection research. Our source code, data, and models are available at https://github.com/nfq729/MT-CSD",
    "checked": true,
    "id": "0f502817e5cd24e9ddbe0902db389f62f236a658",
    "semantic_title": "a challenge dataset and effective models for conversational stance detection",
    "citation_count": 0,
    "authors": [
      "Fuqiang Niu",
      "Min Yang",
      "Ang Li",
      "Baoquan Zhang",
      "Xiaojiang Peng",
      "Bowen Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.12": {
    "title": "A Closer Look at Clustering Bilingual Comparable Corpora",
    "volume": "main",
    "abstract": "We study in this paper the problem of clustering comparable corpora, building upon the observation that different types of clusters can be present in such corpora: monolingual clusters comprising documents in a single language, and bilingual or multilingual clusters comprising documents written in different languages. Based on a state-of-the-art deep variant of Kmeans, we propose new clustering models fully adapted to comparable corpora and illustrate their behavior on several bilingual collections (in English, French, German and Russian) created from Wikipedia",
    "checked": true,
    "id": "9c5ca7c9397502cad5871192ce0f500226ec22ec",
    "semantic_title": "a closer look at clustering bilingual comparable corpora",
    "citation_count": 0,
    "authors": [
      "Anna Laskina",
      "Eric Gaussier",
      "Gaelle Calvary"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.13": {
    "title": "AcnEmpathize: A Dataset for Understanding Empathy in Dermatology Conversations",
    "volume": "main",
    "abstract": "Empathy is critical for effective communication and mental health support, and in many online health communities people anonymously engage in conversations to seek and provide empathetic support. The ability to automatically recognize and detect empathy contributes to the understanding of human emotions expressed in text, therefore advancing natural language understanding across various domains. Existing empathy and mental health-related corpora focus on broader contexts and lack domain specificity, but similarly to other tasks (e.g., learning distinct patterns associated with COVID-19 versus skin allergies in clinical notes), observing empathy within different domains is crucial to providing tailored support. To address this need, we introduce AcnEmpathize, a dataset that captures empathy expressed in acne-related discussions from forum posts focused on its emotional and psychological effects. We find that transformer-based models trained on our dataset demonstrate excellent performance at empathy classification. Our dataset is publicly released to facilitate analysis of domain-specific empathy in online conversations and advance research in this challenging and intriguing domain",
    "checked": true,
    "id": "0695d6ec302c01608ccf500e35ed61727c859ac1",
    "semantic_title": "acnempathize: a dataset for understanding empathy in dermatology conversations",
    "citation_count": 0,
    "authors": [
      "Gyeongeun Lee",
      "Natalie Parde"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.14": {
    "title": "A Collection of Pragmatic-Similarity Judgments over Spoken Dialog Utterances",
    "volume": "main",
    "abstract": "Automatic measures of similarity between sentences or utterances are invaluable for training speech synthesizers, evaluating machine translation, and assessing learner productions. While there exist measures for semantic similarity and prosodic similarity, there are as yet none for pragmatic similarity. To enable the training of such measures, we developed the first collection of human judgments of pragmatic similarity between utterance pairs. 9 judges listened to 220 utterance pairs, each consisting of an utterance extracted from a recorded dialog and a re-enactment of that utterance under various conditions designed to create various degrees of similarity. Each pair was rated on a continuous scale. The average inter-judge correlation was 0.45. We make this data available at https://github.com/divettemarco/PragSim",
    "checked": true,
    "id": "ac8854220ca0177756b914ea5f6389a3dad614d1",
    "semantic_title": "a collection of pragmatic-similarity judgments over spoken dialog utterances",
    "citation_count": 0,
    "authors": [
      "Nigel Ward",
      "Divette Marco"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.15": {
    "title": "A Community-Driven Data-to-Text Platform for Football Match Summaries",
    "volume": "main",
    "abstract": "Data-to-text systems offer a transformative approach to generating textual content in data-rich environments. This paper describes the architecture and deployment of Prosebot, a community-driven data-to-text platform tailored for generating textual summaries of football matches derived from match statistics. The system enhances the visibility of lower-tier matches, traditionally accessible only through data tables. Prosebot uses a template-based Natural Language Generation (NLG) module to generate initial drafts, which are subsequently refined by the reading community. Comprehensive evaluations, encompassing both human-mediated and automated assessments, were conducted to assess the system's efficacy. Analysis of the community-edited texts reveals that significant segments of the initial automated drafts are retained, suggesting their high quality and acceptance by the collaborators. Preliminary surveys conducted among platform users highlight a predominantly positive reception within the community",
    "checked": true,
    "id": "c9003942c4173f2faf34206552f25d66312fdc6b",
    "semantic_title": "a community-driven data-to-text platform for football match summaries",
    "citation_count": 0,
    "authors": [
      "Pedro Fernandes",
      "Sérgio Nunes",
      "Luís Santos"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.16": {
    "title": "A Comparative Analysis of Word-Level Metric Differential Privacy: Benchmarking the Privacy-Utility Trade-off",
    "volume": "main",
    "abstract": "The application of Differential Privacy to Natural Language Processing techniques has emerged in relevance in recent years, with an increasing number of studies published in established NLP outlets. In particular, the adaptation of Differential Privacy for use in NLP tasks has first focused on the *word-level*, where calibrated noise is added to word embedding vectors to achieve \"noisy\" representations. To this end, several implementations have appeared in the literature, each presenting an alternative method of achieving word-level Differential Privacy. Although each of these includes its own evaluation, no comparative analysis has been performed to investigate the performance of such methods relative to each other. In this work, we conduct such an analysis, comparing seven different algorithms on two NLP tasks with varying hyperparameters, including the *epsilon* parameter, or privacy budget. In addition, we provide an in-depth analysis of the results with a focus on the privacy-utility trade-off, as well as open-source our implementation code for further reproduction. As a result of our analysis, we give insight into the benefits and challenges of word-level Differential Privacy, and accordingly, we suggest concrete steps forward for the research field",
    "checked": true,
    "id": "1718ef9e997f146ba8e91ffd63e538dce28b625c",
    "semantic_title": "a comparative analysis of word-level metric differential privacy: benchmarking the privacy-utility trade-off",
    "citation_count": 0,
    "authors": [
      "Stephen Joseph Meisenbacher",
      "Nihildev Nandakumar",
      "Alexandra Klymenko",
      "Florian Matthes"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.17": {
    "title": "A Comparative Study of Explicit and Implicit Gender Biases in Large Language Models via Self-evaluation",
    "volume": "main",
    "abstract": "While extensive work has examined the explicit and implicit biases in large language models (LLMs), little research explores the relation between these two types of biases. This paper presents a comparative study of the explicit and implicit biases in LLMs grounded in social psychology. Social psychology distinguishes between explicit and implicit biases by whether the bias can be self-recognized by individuals. Aligning with this conceptualization, we propose a self-evaluation-based two-stage measurement of explicit and implicit biases within LLMs. First, the LLM is prompted to automatically fill templates with social targets to measure implicit bias toward these targets, where the bias is less likely to be self-recognized by the LLM. Then, the LLM is prompted to self-evaluate the templates filled by itself to measure explicit bias toward the same targets, where the bias is more likely to be self-recognized by the LLM. Experiments conducted on state-of-the-art LLMs reveal human-like inconsistency between explicit and implicit occupational gender biases. This work bridges a critical gap where prior studies concentrate solely on either explicit or implicit bias. We advocate that future work highlight the relation between explicit and implicit biases in LLMs",
    "checked": true,
    "id": "5d66af87a27b1e5223e524b2d32f5bab608a1cc5",
    "semantic_title": "a comparative study of explicit and implicit gender biases in large language models via self-evaluation",
    "citation_count": 0,
    "authors": [
      "Yachao Zhao",
      "Bo Wang",
      "Yan Wang",
      "Dongming Zhao",
      "Xiaojia Jin",
      "Jijun Zhang",
      "Ruifang He",
      "Yuexian Hou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.18": {
    "title": "A Computational Analysis of the Dehumanisation of Migrants from Syria and Ukraine in Slovene News Media",
    "volume": "main",
    "abstract": "Dehumanisation involves the perception and/or treatment of a social group's members as less than human. This phenomenon is rarely addressed with computational linguistic techniques. We adapt a recently proposed approach for English, making it easier to transfer to other languages and to evaluate, introducing a new sentiment resource, the use of zero-shot cross-lingual valence and arousal detection, and a new method for statistical significance testing. We then apply it to study attitudes to migration expressed in Slovene newspapers, to examine changes in the Slovene discourse on migration between the 2015-16 migration crisis following the war in Syria and the 2022-23 period following the war in Ukraine. We find that while this discourse became more negative and more intense over time, it is less dehumanising when specifically addressing Ukrainian migrants compared to others",
    "checked": true,
    "id": "2e9d8ac4e575956601e2445ac27b03d0355dc78d",
    "semantic_title": "a computational analysis of the dehumanisation of migrants from syria and ukraine in slovene news media",
    "citation_count": 0,
    "authors": [
      "Jaya Caporusso",
      "Damar Hoogland",
      "Mojca Brglez",
      "Boshko Koloski",
      "Matthew Purver",
      "Senja Pollak"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.19": {
    "title": "A Computational Approach to Quantifying Grammaticization of English Deverbal Prepositions",
    "volume": "main",
    "abstract": "This paper explores grammaticization of deverbal prepositions by a computational approach based on corpus data. Deverbal prepositions are words or phrases that are derived from a verb and that behave as a preposition such as \"regarding\" and \"according to\". Linguistic studies have revealed important aspects of grammaticization of deverbal prepositions. This paper augments them by methods for measuring the degree of grammaticization of deverbal prepositions based on non-contextualized or contextualized word vectors. Experiments show that the methods correlate well with human judgements (as high as 0.69 in Spearman's rank correlation coefficient). Using the best-performing method, this paper further shows that the methods support previous findings in linguistics including (i) Deverbal prepositions are marginal in terms of prepositionality; and (ii) The process where verbs are grammaticized into prepositions is gradual. As a pilot study, it also conducts a diachronic analysis of grammaticization of deverbal preposition",
    "checked": true,
    "id": "564f2d5008f2cf15c7541b52d8b7a425d4e926e4",
    "semantic_title": "a computational approach to quantifying grammaticization of english deverbal prepositions",
    "citation_count": 0,
    "authors": [
      "Ryo Nagata",
      "Yoshifumi Kawasaki",
      "Naoki Otani",
      "Hiroya Takamura"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.20": {
    "title": "A Computational Model of Latvian Morphology",
    "volume": "main",
    "abstract": "In this paper we describe a computational model of Latvian morphology that provides a formal structure for Latvian word form inflection and has been implemented in software for generation, analysis and lemmatization of Latvian word forms. The work was motivated by the need for a NLP inflection model that can cover all the complexity of Latvian language and explicitly enumerate and handle the many exceptions to the general Latvian inflection principles. This is an evolution of earlier work, extending the initial proof of concept model to properly cover Latvian language. We provide a set of morphological paradigms that differ from current linguistic tradition, a set of systematic stem changes and combine it with an extensive lexicon that includes paradigm information and structured morphological attributes for 118 000 lexemes. This model has been applied on both dictionary and corpora data, demonstrating that it provides a good coverage for modern Latvian literary language. We also consider that there is a good potential to extend this also to the related Latgalian language",
    "checked": true,
    "id": "1f890adfedb9d4426c833573627c7ac1b0156e0f",
    "semantic_title": "a computational model of latvian morphology",
    "citation_count": 0,
    "authors": [
      "Peteris Paikens",
      "Lauma Pretkalniņa",
      "Laura Rituma"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.21": {
    "title": "A Concept Based Approach for Translation of Medical Dialogues into Pictographs",
    "volume": "main",
    "abstract": "Pictographs have been found to improve patient comprehension of medical information or instructions. However, tools to produce pictograph representations from natural language are still scarce. In this contribution we describe a system that automatically translates French speech into pictographs to enable diagnostic interviews in emergency settings, thereby providing a tool to overcome the language barrier or provide support in Augmentative and Alternative Communication (AAC) contexts. Our approach is based on a semantic gloss that serves as pivot between spontaneous language and pictographs, with medical concepts represented using the UMLS ontology. In this study we evaluate different available pre-trained models fine-tuned on artificial data to translate French into this semantic gloss. On unseen data collected in real settings, consisting of questions and instructions by physicians, the best model achieves an F0.5 score of 86.7. A complementary human evaluation of the semantic glosses differing from the reference shows that 71% of these would be usable to transmit the intended meaning. Finally, a human evaluation of the pictograph sequences derived from the gloss reveals very few additions, omissions or order issues (<3%), suggesting that the gloss as designed is well suited as a pivot for translation into pictographs",
    "checked": true,
    "id": "0717b012b589622f384495979bf4770621d96fd1",
    "semantic_title": "a concept based approach for translation of medical dialogues into pictographs",
    "citation_count": 0,
    "authors": [
      "Johanna Gerlach",
      "Pierrette Bouillon",
      "Jonathan Mutal",
      "Hervé Spechbach"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.22": {
    "title": "A Construction Grammar Corpus of Varying Schematicity: A Dataset for the Evaluation of Abstractions in Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have been developed without a theoretical framework, yet we posit that evaluating and improving LLMs will benefit from the development of theoretical frameworks that enable comparison of the structures of human language and the model of language built up by LLMs through the processing of text. In service of this goal, we develop the Construction Grammar Schematicity (\"CoGS\") corpus of 10 distinct English constructions, where the constructions vary with respect to schematicity, or in other words the level to which constructional slots require specific, fixed lexical items, or can be filled with a variety of elements that fulfill a particular semantic role of the slot. Our corpus constructions are carefully curated to range from substantive, frozen constructions (e.g., Let-alone) to entirely schematic constructions (e.g., Resultative). The corpus was collected to allow us to probe LLMs for constructional information at varying levels of abstraction. We present our own probing experiments using this corpus, which clearly demonstrate that even the largest LLMs are limited to more substantive constructions and do not exhibit recognition of the similarity of purely schematic constructions. We publicly release our dataset, prompts, and associated model responses",
    "checked": true,
    "id": "d66c90945f5fa8575d81b55b12de956845b584a9",
    "semantic_title": "a construction grammar corpus of varying schematicity: a dataset for the evaluation of abstractions in language models",
    "citation_count": 0,
    "authors": [
      "Claire Bonial",
      "Harish Tayyar Madabushi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.23": {
    "title": "A Controlled Reevaluation of Coreference Resolution Models",
    "volume": "main",
    "abstract": "All state-of-the-art coreference resolution (CR) models involve finetuning a pretrained language model. Whether the superior performance of one CR model over another is due to the choice of language model or other factors, such as the task-specific architecture, is difficult or impossible to determine due to lack of a standardized experimental setup. To resolve this ambiguity, we systematically evaluate five CR models and control for certain design decisions including the pretrained language model used by each. When controlling for language model size, encoder-based CR models outperform more recent decoder-based models in terms of both accuracy and inference speed. Surprisingly, among encoder-based CR models, more recent models are not always more accurate, and the oldest CR model that we test generalizes the best to out-of-domain textual genres. We conclude that controlling for the choice of language model reduces most, but not all, of the increase in F1 score reported in the past five years",
    "checked": true,
    "id": "2e622c0b71c8039d3c0d4ed2a60c3d4d6f471ff9",
    "semantic_title": "a controlled reevaluation of coreference resolution models",
    "citation_count": 0,
    "authors": [
      "Ian Porada",
      "Xiyuan Zou",
      "Jackie Chi Kit Cheung"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.24": {
    "title": "A Corpus and Method for Chinese Named Entity Recognition in Manufacturing",
    "volume": "main",
    "abstract": "Manufacturing specifications are documents entailing different techniques, processes, and components involved in manufacturing. There is a growing demand for named entity recognition (NER) resources and techniques for manufacturing-specific named entities, with the development of smart manufacturing. In this paper, we introduce a corpus of Chinese manufacturing specifications, named MS-NERC, including 4,424 sentences and 16,383 entities. We also propose an entity recognizer named Trainable State Transducer (TST), which is initialized with a finite state transducer describing the morphological patterns of entities. It can directly recognize entities based on prior morphological knowledge without training. Experimental results show that TST achieves an overall 82.05% F1 score for morphological-specific entities in zero-shot. TST can be improved through training, the result of which outperforms neural methods in few-shot and rich-resource. We believe that our corpus and model will be valuable resources for NER research not only in manufacturing but also in other low-resource domains",
    "checked": true,
    "id": "a5ff69fedeaffd066f0b7da38a2e9d281e03dc87",
    "semantic_title": "a corpus and method for chinese named entity recognition in manufacturing",
    "citation_count": 0,
    "authors": [
      "Ruiting Li",
      "Peiyan Wang",
      "Libang Wang",
      "Danqingxin Yang",
      "Dongfeng Cai"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.25": {
    "title": "A Corpus for Sentence-Level Subjectivity Detection on English News Articles",
    "volume": "main",
    "abstract": "We develop novel annotation guidelines for sentence-level subjectivity detection, which are not limited to language-specific cues. We use our guidelines to collect NewsSD-ENG, a corpus of 638 objective and 411 subjective sentences extracted from English news articles on controversial topics. Our corpus paves the way for subjectivity detection in English and across other languages without relying on language-specific tools, such as lexicons or machine translation. We evaluate state-of-the-art multilingual transformer-based models on the task in mono-, multi-, and cross-language settings. For this purpose, we re-annotate an existing Italian corpus. We observe that models trained in the multilingual setting achieve the best performance on the task",
    "checked": true,
    "id": "bd2ba05874900c1ebfca95d6d0498f653808f9b8",
    "semantic_title": "a corpus for sentence-level subjectivity detection on english news articles",
    "citation_count": 2,
    "authors": [
      "Francesco Antici",
      "Federico Ruggeri",
      "Andrea Galassi",
      "Katerina Korre",
      "Arianna Muti",
      "Alessandra Bardi",
      "Alice Fedotova",
      "Alberto Barrón-Cedeño"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.26": {
    "title": "A Corpus of German Abstract Meaning Representation (DeAMR)",
    "volume": "main",
    "abstract": "We present the first comprehensive set of guidelines for German Abstract Meaning Representation (Deutsche AMR, DeAMR) along with an annotated corpus of 400 DeAMR. Taking English AMR (EnAMR) as our starting point, we propose significant adaptations to faithfully represent the structure and semantics of German, focusing particularly on verb frames, compound words, and modality. We validate our annotation through inter-annotator agreement and further evaluate our corpus with a comparison of structural divergences between EnAMR and DeAMR on parallel sentences, replicating previous work that finds both cases of cross-lingual structural alignment and cases of meaningful linguistic divergence. Finally, we fine-tune state-of-the-art multi-lingual and cross-lingual AMR parsers on our corpus and find that, while our small corpus is insufficient to produce quality output, there is a need to continue develop and evaluate against gold non-English AMR data",
    "checked": true,
    "id": "ad6f1bc661eb0b923f519572e625e01088c5653f",
    "semantic_title": "a corpus of german abstract meaning representation (deamr)",
    "citation_count": 0,
    "authors": [
      "Christoph Otto",
      "Jonas Groschwitz",
      "Alexander Koller",
      "Xiulin Yang",
      "Lucia Donatelli"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.27": {
    "title": "A Corpus of Spontaneous L2 English Speech for Real-situation Speaking Assessment",
    "volume": "main",
    "abstract": "When assessing second language proficiency (L2), evaluation of spontaneous speech performance is crucial. This paper presents a corpus of spontaneous L2 English speech, focusing on the speech performance of B1 and B2 proficiency speakers. Two hundred and sixty university students were recorded during a speaking task as part of a French national certificate in English. This task entailed a 10-minute role-play among 2 or 3 candidates, arguing about a controversial topic, in order to reach a negotiated compromise. Each student's performance was evaluated by two experts, categorizing them into B2, B1 or below B1 speaking proficiency levels. Automatic diarization, transcription, and alignment at the word level were performed on the recorded conversations, in order to analyse lexical stress realisation in polysyllabic plain words of B1 and B2 proficiency students. Results showed that only 35.4% of the 6,350 targeted words had stress detected on the expected syllable, revealing a common stress shift to the final syllable. Besides a substantial inter-speaker variability (0% to 68.4%), B2 speakers demonstrated a slightly higher stress accuracy (36%) compared to B1 speakers (29.6%). Those with accurate stress placement utilized F0 and intensity to make syllable prominence, while speakers with lower accuracy tended to lengthen words on their last syllables, with minimal changes in other dimensions",
    "checked": true,
    "id": "1a4e12193a1a72eddc0431c2f1e36db68a174087",
    "semantic_title": "a corpus of spontaneous l2 english speech for real-situation speaking assessment",
    "citation_count": 0,
    "authors": [
      "Sylvain Coulange",
      "Marie-Hélène Fries",
      "Monica Masperi",
      "Solange Rossato"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.28": {
    "title": "Action and Reaction Go Hand in Hand! a Multi-modal Dialogue Act Aided Sarcasm Identification",
    "volume": "main",
    "abstract": "Sarcasm primarily involves saying something but \"meaning the opposite\" or \"meaning something completely different\" in order to convey a particular tone or mood. In both the above cases, the \"meaning\" is reflected by the communicative intention of the speaker, known as dialogue acts. In this paper, we seek to investigate a novel phenomenon of analyzing sarcasm in the context of dialogue acts with the hypothesis that the latter helps to understand the former better. Toward this aim, we extend the multi-modal MUStARD dataset to enclose dialogue acts for each dialogue. To demonstrate the utility of our hypothesis, we develop a dialogue act-aided multi-modal transformer network for sarcasm identification (MM-SARDAC), leveraging interrelation between these tasks. In addition, we introduce an order-infused, multi-modal infusion mechanism into our proposed model, which allows for a more intuitive combined modality representation by selectively focusing on relevant modalities in an ordered manner. Extensive empirical results indicate that dialogue act-aided sarcasm identification achieved better performance compared to performing sarcasm identification alone. The dataset and code are available at https://github.com/mohit2b/MM-SARDAC",
    "checked": true,
    "id": "4dd8a45f46f53337328d5a6de889a94e3793e2da",
    "semantic_title": "action and reaction go hand in hand! a multi-modal dialogue act aided sarcasm identification",
    "citation_count": 0,
    "authors": [
      "Mohit Singh Tomar",
      "Tulika Saha",
      "Abhisek Tiwari",
      "Sriparna Saha"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.29": {
    "title": "Action-Concentrated Embedding Framework: This Is Your Captain Sign-tokening",
    "volume": "main",
    "abstract": "Sign language is the primary communication medium for people who are deaf or have hearing loss. However, given the divergent range of sensory abilities of these individuals, there is a communication gap that needs to be addressed. In this paper, we present action-concentrated embedding (ACE), which is a novel sign token embedding framework. Additionally, to provide a more structured foundation for sign language analysis, we introduce a dedicated notation system tailored for sign language that endeavors to encapsulate the nuanced gestures and movements that are integral with sign communication. The proposed ACE approach tracks a signer's actions based on human posture estimation. Tokenizing these actions and capturing the token embedding using a short-time Fourier transform encapsulates the time-based behavioral changes. Hence, ACE offers input embedding to translate sign language into natural language sentences. When tested against a disaster sign language dataset using automated machine translation measures, ACE notably surpasses prior research in terms of translation capabilities, improving the performance by up to 5.79% for BLEU-4 and 5.46% for ROUGE-L metric",
    "checked": true,
    "id": "206d500b038d6827fd95b428ce586b88ce1901d0",
    "semantic_title": "action-concentrated embedding framework: this is your captain sign-tokening",
    "citation_count": 0,
    "authors": [
      "Hyunwook Yu",
      "Suhyeon Shin",
      "Junku Heo",
      "Hyuntaek Shin",
      "Hyosu Kim",
      "Mucheol Kim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.30": {
    "title": "Active Learning Design Choices for NER with Transformers",
    "volume": "main",
    "abstract": "We explore multiple important choices that have not been analyzed in conjunction regarding active learning for token classification using transformer networks. These choices are: (i) how to select what to annotate, (ii) decide whether to annotate entire sentences or smaller sentence fragments, (iii) how to train with incomplete annotations at token-level, and (iv) how to select the initial seed dataset. We explore whether annotating at sub-sentence level can translate to an improved downstream performance by considering two different sub-sentence annotation strategies: (i) entity-level, and (ii) token-level. These approaches result in some sentences being only partially annotated. To address this issue, we introduce and evaluate multiple strategies to deal with partially-annotated sentences during the training process. We show that annotating at the sub-sentence level achieves comparable or better performance than sentence-level annotations with a smaller number of annotated tokens. We then explore the extent to which the performance gap remains once accounting for the annotation time and found that both annotation schemes perform similarly",
    "checked": true,
    "id": "2f17c6b7a146fb47f4aeb5f07bd192d4f5e23171",
    "semantic_title": "active learning design choices for ner with transformers",
    "citation_count": 1,
    "authors": [
      "Robert Vacareanu",
      "Enrique Noriega-Atala",
      "Gus Hahn-Powell",
      "Marco A. Valenzuela-Escarcega",
      "Mihai Surdeanu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.31": {
    "title": "A CURATEd CATalog: Rethinking the Extraction of Pretraining Corpora for Mid-Resourced Languages",
    "volume": "main",
    "abstract": "We present and describe two language resources in this paper: CATalog 1.0, the largest text corpus in Catalan to date, and CURATE (Corpus Utility for RAting TExt), a modular, parallelizable pipeline used for processing and scoring documents based on text quality that we have optimised to run in High Performance Cluster (HPC) environments. In the coming sections we describe our data preprocessing pipeline at length; traditional pipelines usually implement a set of binary filters such that a given document is either in or out. In our experience with Catalan, in lower-resource settings it is more practical to instead assign a document a soft score to allow for more flexible decision-making. We describe how the document score is calculated and highlight its interpretability by showing that it is significantly correlated with human judgements as obtained from a comparative judgement experiment. We additionally describe the different subcorpora that make up CATalog 1.0",
    "checked": true,
    "id": "77a2e761ed2dc2360043280bb543c7add804983d",
    "semantic_title": "a curated catalog: rethinking the extraction of pretraining corpora for mid-resourced languages",
    "citation_count": 2,
    "authors": [
      "Jorge Palomar-Giner",
      "Jose Javier Saiz",
      "Ferran Espuña",
      "Mario Mina",
      "Severino Da Dalt",
      "Joan Llop",
      "Malte Ostendorff",
      "Pedro Ortiz Suarez",
      "Georg Rehm",
      "Aitor Gonzalez-Agirre",
      "Marta Villegas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.32": {
    "title": "AdaKron: An Adapter-based Parameter Efficient Model Tuning with Kronecker Product",
    "volume": "main",
    "abstract": "The fine-tuning paradigm has been widely adopted to train neural models tailored for specific tasks. However, the recent upsurge of Large Language Models (LLMs), characterized by billions of parameters, has introduced profound computational challenges to the fine-tuning process. This has fueled intensive research on Parameter-Efficient Fine-Tuning (PEFT) techniques, usually involving the training of a selective subset of the original model parameters. One of the most used approaches is Adapters, which add trainable lightweight layers to the existing pretrained weights. Within this context, we propose AdaKron, an Adapter-based fine-tuning with the Kronecker product. In particular, we leverage the Kronecker product to combine the output of two small networks, resulting in a final vector whose dimension is the product of the dimensions of the individual outputs, allowing us to train only 0.55% of the model's original parameters. We evaluate AdaKron performing a series of experiments on the General Language Understanding Evaluation (GLUE) benchmark, achieving results in the same ballpark as recent state-of-the-art PEFT methods, despite training fewer parameters",
    "checked": true,
    "id": "97ee0de7af778d1f7a6453bb7fdb59900e4111a8",
    "semantic_title": "adakron: an adapter-based parameter efficient model tuning with kronecker product",
    "citation_count": 0,
    "authors": [
      "Marco Braga",
      "Alessandro Raganato",
      "Gabriella Pasi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.33": {
    "title": "Adaptive Reinforcement Tuning Language Models as Hard Data Generators for Sentence Representation",
    "volume": "main",
    "abstract": "Sentence representation learning is a fundamental task in NLP. Existing methods use contrastive learning (CL) to learn effective sentence representations, which benefit from high-quality contrastive data but require extensive human annotation. Large language models (LLMs) like ChatGPT and GPT4 can automatically generate such data. However, this alternative strategy also encounters challenges: 1) obtaining high-quality generated data from small-parameter LLMs is difficult, and 2) inefficient utilization of the generated data. To address these challenges, we propose a novel adaptive reinforcement tuning (ART) framework. Specifically, to address the first challenge, we introduce a reinforcement learning approach for fine-tuning small-parameter LLMs, enabling the generation of high-quality hard contrastive data without human feedback. To address the second challenge, we propose an adaptive iterative framework to guide the small-parameter LLMs to generate progressively harder samples through multiple iterations, thereby maximizing the utility of generated data. Experiments conducted on seven semantic text similarity tasks demonstrate that the sentence representation models trained using the synthetic data generated by our proposed method achieve state-of-the-art performance. Our code is available at https://github.com/WuNein/AdaptCL",
    "checked": true,
    "id": "a7556918f473f3256be77f7d37642804f1bbd3ce",
    "semantic_title": "adaptive reinforcement tuning language models as hard data generators for sentence representation",
    "citation_count": 0,
    "authors": [
      "Bo Xu",
      "Yifei Wu",
      "Shouang Wei",
      "Ming Du",
      "Hongya Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.34": {
    "title": "Adaptive Simultaneous Sign Language Translation with Confident Translation Length Estimation",
    "volume": "main",
    "abstract": "Traditional non-simultaneous Sign Language Translation (SLT) methods, while effective for pre-recorded videos, face challenges in real-time scenarios due to inherent inference delays. The emerging field of simultaneous SLT aims to address this issue by progressively translating incrementally received sign video. However, the sole existing work in simultaneous SLT adopts a fixed gloss-based policy, which suffer from limitations in boundary prediction and contextual comprehension. In this paper, we delve deeper into this area and propose an adaptive policy for simultaneous SLT. Our approach introduces the concept of \"confident translation length\", denoting maximum accurate translation achievable from current input. An estimator measures this length for streaming sign video, enabling the model to make informed decisions on whether to wait for more input or proceed with translation. To train the estimator, we construct a training data of confident translation length based on the longest common prefix between translations of partial and complete inputs. Furthermore, we incorporate adaptive training, utilizing pseudo prefix pairs, to refine the offline translation model for optimal performance in simultaneous scenarios. Experimental results on PHOENIX2014T and CSL-Daily demonstrate the superiority of our adaptive policy over existing methods, particularly excelling in situations requiring extremely low latency",
    "checked": true,
    "id": "5534b7d197b531c720e76e5584841ec8d64f1151",
    "semantic_title": "adaptive simultaneous sign language translation with confident translation length estimation",
    "citation_count": 0,
    "authors": [
      "Tong Sun",
      "Biao Fu",
      "Cong Hu",
      "Liang Zhang",
      "Ruiquan Zhang",
      "Xiaodong Shi",
      "Jinsong Su",
      "Yidong Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.35": {
    "title": "A Dataset for Named Entity Recognition and Entity Linking in Chinese Historical Newspapers",
    "volume": "main",
    "abstract": "In this study, we present a novel historical Chinese dataset for named entity recognition, entity linking, coreference and entity relations. We use data from Chinese newspapers from 1872 to 1949 and multilingual bibliographic resources from the same period. The period and the language are the main strength of the present work, offering a resource which covers different styles and language uses, as well as the largest historical Chinese NER dataset with manual annotations from this transitional period. After detailing the selection and annotation process, we present the very first results that can be obtained from this dataset. Texts and annotations are freely downloadable from the GitHub repository",
    "checked": true,
    "id": "9901717b5e1746c2d542bc1cf8b1671d716809a2",
    "semantic_title": "a dataset for named entity recognition and entity linking in chinese historical newspapers",
    "citation_count": 0,
    "authors": [
      "Baptiste Blouin",
      "Cécile Armand",
      "Christian Henriot"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.36": {
    "title": "A Dataset for Pharmacovigilance in German, French, and Japanese: Annotating Adverse Drug Reactions across Languages",
    "volume": "main",
    "abstract": "User-generated data sources have gained significance in uncovering Adverse Drug Reactions (ADRs), with an increasing number of discussions occurring in the digital world. However, the existing clinical corpora predominantly revolve around scientific articles in English. This work presents a multilingual corpus of texts concerning ADRs gathered from diverse sources, including patient fora, social media, and clinical reports in German, French, and Japanese. Our corpus contains annotations covering 12 entity types, four attribute types, and 13 relation types. It contributes to the development of real-world multilingual language models for healthcare. We provide statistics to highlight certain challenges associated with the corpus and conduct preliminary experiments resulting in strong baselines for extracting entities and relations between these entities, both within and across languages",
    "checked": true,
    "id": "4bf51ac7e6f0819f5dae789b0d85559743cd17f4",
    "semantic_title": "a dataset for pharmacovigilance in german, french, and japanese: annotating adverse drug reactions across languages",
    "citation_count": 0,
    "authors": [
      "Lisa Raithel",
      "Hui-Syuan Yeh",
      "Shuntaro Yada",
      "Cyril Grouin",
      "Thomas Lavergne",
      "Aurélie Névéol",
      "Patrick Paroubek",
      "Philippe Thomas",
      "Tomohiro Nishiyama",
      "Sebastian Möller",
      "Eiji Aramaki",
      "Yuji Matsumoto",
      "Roland Roller",
      "Pierre Zweigenbaum"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.37": {
    "title": "Adding SPICE to Life: Speaker Profiling in Multiparty Conversations",
    "volume": "main",
    "abstract": "In the realm of conversational dynamics, individual idiosyncrasies challenge the suitability of a one-size-fits-all approach for dialogue agent responses. Prior studies often assumed the speaker's persona's immediate availability, a premise not universally applicable. To address this gap, we explore the Speaker Profiling in Conversations (SPC) task, aiming to synthesize persona attributes for each dialogue participant. SPC comprises three core subtasks: persona discovery, persona-type identification, and persona-value extraction. The first subtask identifies persona-related utterances, the second classifies specific attributes, and the third extracts precise values for the persona. To confront this multifaceted challenge, we've diligently compiled SPICE, an annotated dataset, underpinning our thorough evaluation of diverse baseline models. Additionally, we benchmark these findings against our innovative neural model, SPOT, presenting an exhaustive analysis encompassing a nuanced assessment of quantitative and qualitative merits and limitations",
    "checked": true,
    "id": "9639402ce19df2bf013a67ab18690454c02c0185",
    "semantic_title": "adding spice to life: speaker profiling in multiparty conversations",
    "citation_count": 0,
    "authors": [
      "Shivani Kumar",
      "Rishabh Gupta",
      "Md. Shad Akhtar",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.38": {
    "title": "ADEA: An Argumentative Dialogue Dataset on Ethical Issues Concerning Future A.I. Applications",
    "volume": "main",
    "abstract": "Introducing ADEA: a German dataset that captures online dialogues and focuses on ethical issues related to future AI applications. This dataset, which includes over 2800 labeled user utterances on four different topics, is specifically designed for the training of chatbots that can navigate the complexities of real-world ethical AI conversations. The creation of these dialogues is the result of two carefully conducted studies in which university students interacted with an argumentative dialogue system. A fundamental part of our methodology is the use of German argument graphs. These graphs not only form the knowledge base of the dialogue system but also serve as an effective annotation scheme for the dialogues. Apart from the introduction of the dataset and the argument graphs, we provide a preliminary benchmark using GPT-4 via the OpenAI API. This provides researchers with a concrete reference point while demonstrating the potential of our dataset. We make our dataset and argument graphs available at https://github.com/HaupChris/ADEA-Dialogue-Dataset",
    "checked": true,
    "id": "97a2c92688d165460327c581124e38edf0bbbd17",
    "semantic_title": "adea: an argumentative dialogue dataset on ethical issues concerning future a.i. applications",
    "citation_count": 0,
    "authors": [
      "Christian Hauptmann",
      "Adrian Krenzer",
      "Antonia Krause",
      "Frank Puppe"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.39": {
    "title": "A Decade of Scholarly Research on Open Knowledge Graphs",
    "volume": "main",
    "abstract": "The proliferation of open knowledge graphs has led to a surge in scholarly research on the topic over the past decade. This paper presents a bibliometric analysis of the scholarly literature on open knowledge graphs published between 2013 and 2023. The study aims to identify the trends, patterns, and impact of research in this field, as well as the key topics and research questions that have emerged. The work uses bibliometric techniques to analyze a sample of 4445 scholarly articles retrieved from Scopus. The findings reveal an ever-increasing number of publications on open knowledge graphs published every year, particularly in developed countries (+50 per year). These outputs are published in highly-referred scholarly journals and conferences. The study identifies three main research themes: (1) knowledge graph construction and enrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into NLP systems. Within these themes, the study identifies specific tasks that have received considerable attention, including entity linking, knowledge graph embedding, and graph neural networks",
    "checked": true,
    "id": "91a2d5085f191044dfa1bff5bb685e8f05b57896",
    "semantic_title": "a decade of scholarly research on open knowledge graphs",
    "citation_count": 0,
    "authors": [
      "Houcemeddine Turki",
      "Abraham Toluwase Owodunni",
      "Mohamed Ali Hadj Taieb",
      "René Fabrice Bile",
      "Mohamed Ben Aouicha"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.40": {
    "title": "A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference",
    "volume": "main",
    "abstract": "Integer Linear Programming (ILP) has been proposed as a formalism for encoding precise structural and semantic constraints for Natural Language Inference (NLI). However, traditional ILP frameworks are non-differentiable, posing critical challenges for the integration of continuous language representations based on deep learning. In this paper, we introduce a novel approach, named Diff-Comb Explainer, a neuro-symbolic architecture for explanation-based NLI based on Differentiable BlackBox Combinatorial Solvers (DBCS). Differently from existing neuro-symbolic solvers, Diff-Comb Explainer does not necessitate a continuous relaxation of the semantic constraints, enabling a direct, more precise, and efficient incorporation of neural representations into the ILP formulation. Our experiments demonstrate that Diff-Comb Explainer achieves superior performance when compared to conventional ILP solvers, neuro-symbolic black-box solvers, and Transformer-based encoders. Moreover, a deeper analysis reveals that Diff-Comb Explainer can significantly improve the precision, consistency, and faithfulness of the constructed explanations, opening new opportunities for research on neuro-symbolic architectures for explainable and transparent NLI in complex domains",
    "checked": true,
    "id": "353e3d7181d87f8b61ee0f20d973c22e3341add6",
    "semantic_title": "a differentiable integer linear programming solver for explanation-based natural language inference",
    "citation_count": 0,
    "authors": [
      "Mokanarangan Thayaparan",
      "Marco Valentino",
      "André Freitas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.41": {
    "title": "A Document-Level Text Simplification Dataset for Japanese",
    "volume": "main",
    "abstract": "Document-level text simplification, a task that combines single-document summarization and intra-sentence simplification, has garnered significant attention. However, studies have primarily focused on languages such as English and German, leaving Japanese and similar languages underexplored because of a scarcity of linguistic resources. In this study, we devised JADOS, the first Japanese document-level text simplification dataset based on newspaper articles and Wikipedia. Our dataset focuses on simplification, to enhance readability by reducing the number of sentences and tokens in a document. We conducted investigations using our dataset. Firstly, we analyzed the characteristics of Japanese simplification by comparing it across different domains and with English counterparts. Moreover, we experimentally evaluated the performances of text summarization methods, transformer-based text simplification models, and large language models. In terms of D-SARI scores, the transformer-based models performed best across all domains. Finally, we manually evaluated several model outputs and target articles, demonstrating the need for document-level text simplification models in Japanese",
    "checked": true,
    "id": "f677b557bad44250f840b9e9e6d4587cac7546ec",
    "semantic_title": "a document-level text simplification dataset for japanese",
    "citation_count": 0,
    "authors": [
      "Yoshinari Nagai",
      "Teruaki Oka",
      "Mamoru Komachi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.42": {
    "title": "A Dual-View Approach to Classifying Radiology Reports by Co-Training",
    "volume": "main",
    "abstract": "Radiology report analysis provides valuable information that can aid with public health initiatives, and has been attracting increasing attention from the research community. In this work, we present a novel insight that the structure of a radiology report (namely, the Findings and Impression sections) offers different views of a radiology scan. Based on this intuition, we further propose a co-training approach, where two machine learning models are built upon the Findings and Impression sections, respectively, and use each other's information to boost performance with massive unlabeled data in a semi-supervised manner. We conducted experiments in a public health surveillance study, and results show that our co-training approach is able to improve performance using the dual views and surpass competing supervised and semi-supervised methods",
    "checked": true,
    "id": "449a5e418f5a77f25b7fd03cbcbe960869d0737d",
    "semantic_title": "a dual-view approach to classifying radiology reports by co-training",
    "citation_count": 0,
    "authors": [
      "Yutong Han",
      "Yan Yuan",
      "Lili Mou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.43": {
    "title": "Advancing Semi-Supervised Learning for Automatic Post-Editing: Data-Synthesis by Mask-Infilling with Erroneous Terms",
    "volume": "main",
    "abstract": "Semi-supervised learning that leverages synthetic data for training has been widely adopted for developing automatic post-editing (APE) models due to the lack of training data. With this aim, we focus on data-synthesis methods to create high-quality synthetic data. Given that APE takes as input a machine-translation result that might include errors, we present a data-synthesis method by which the resulting synthetic data mimic the translation errors found in actual data. We introduce a noising-based data-synthesis method by adapting the masked language model approach, generating a noisy text from a clean text by infilling masked tokens with erroneous tokens. Moreover, we propose selective corpus interleaving that combines two separate synthetic datasets by taking only the advantageous samples to enhance the quality of the synthetic data further. Experimental results show that using the synthetic data created by our approach results in significantly better APE performance than other synthetic data created by existing methods",
    "checked": true,
    "id": "841f5bc6c9bcab7fb909a100e89c81f19afec4df",
    "semantic_title": "advancing semi-supervised learning for automatic post-editing: data-synthesis by mask-infilling with erroneous terms",
    "citation_count": 0,
    "authors": [
      "Wonkee Lee",
      "Seong-Hwan Heo",
      "Jong-Hyeok Lee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.44": {
    "title": "Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark",
    "volume": "main",
    "abstract": "Topic segmentation and outline generation strive to divide a document into coherent topic sections and generate corresponding subheadings, unveiling the discourse topic structure of a document. Compared with sentence-level topic structure, the paragraph-level topic structure can quickly grasp and understand the overall context of the document from a higher level, benefitting many downstream tasks such as summarization, discourse parsing, and information retrieval. However, the lack of large-scale, high-quality Chinese paragraph-level topic structure corpora restrained relative research and applications. To fill this gap, we build the Chinese paragraph-level topic representation, corpus, and benchmark in this paper. Firstly, we propose a hierarchical paragraph-level topic structure representation with three layers to guide the corpus construction. Then, we employ a two-stage man-machine collaborative annotation method to construct the largest Chinese Paragraph-level Topic Structure corpus (CPTS), achieving high quality. We also build several strong baselines, including ChatGPT, to validate the computability of CPTS on two fundamental tasks (topic segmentation and outline generation) and preliminarily verified its usefulness for the downstream task (discourse parsing)",
    "checked": true,
    "id": "c486807d157c003bf16f118c2d5c7c365a243718",
    "semantic_title": "advancing topic segmentation and outline generation in chinese texts: the paragraph-level topic representation, corpus, and benchmark",
    "citation_count": 0,
    "authors": [
      "Feng Jiang",
      "Weihao Liu",
      "Xiaomin Chu",
      "Peifeng Li",
      "Qiaoming Zhu",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.45": {
    "title": "A Family of Pretrained Transformer Language Models for Russian",
    "volume": "main",
    "abstract": "Transformer language models (LMs) are fundamental to NLP research methodologies and applications in various languages. However, developing such models specifically for the Russian language has received little attention. This paper introduces a collection of 13 Russian Transformer LMs, which spans encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder (ruT5, FRED-T5) architectures. We provide a report on the model architecture design and pretraining, and the results of evaluating their generalization abilities on Russian language understanding and generation datasets and benchmarks. By pretraining and releasing these specialized Transformer LMs, we aim to broaden the scope of the NLP research directions and enable the development of industrial solutions for the Russian language",
    "checked": true,
    "id": "062c191742a9413e3134c105469e8d2c81bc31da",
    "semantic_title": "a family of pretrained transformer language models for russian",
    "citation_count": 4,
    "authors": [
      "Dmitry Zmitrovich",
      "Aleksandr Abramov",
      "Andrey Kalmykov",
      "Vitaly Kadulin",
      "Maria Tikhonova",
      "Ekaterina Taktasheva",
      "Danil Astafurov",
      "Mark Baushenko",
      "Artem Snegirev",
      "Tatiana Shavrina",
      "Sergei S. Markov",
      "Vladislav Mikhailov",
      "Alena Fenogenova"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.46": {
    "title": "A Fast and High-quality Text-to-Speech Method with Compressed Auxiliary Corpus and Limited Target Speaker Corpus",
    "volume": "main",
    "abstract": "With an auxiliary corpus (non-target speaker corpus) for model pre-training, Text-to-Speech (TTS) methods can generate high-quality speech with a limited target speaker corpus. However, this approach comes with expensive training costs. To overcome the challenge, a high-quality TTS method is proposed, significantly reducing training costs while maintaining the naturalness of synthesized speech. In this paper, we propose an auxiliary corpus compression algorithm that reduces the training cost while the naturalness of the synthesized speech is not significantly degraded. We then use the compressed corpus to pre-train the proposed TTS model CMDTTS, which fuses phoneme and word multi-level prosody modeling components and denoises the generated mel-spectrograms using denoising diffusion probabilistic models (DDPMs). In addition, a fine-tuning step that the conditional generative adversarial network (cGAN) is introduced to embed the target speaker feature and improve speech quality using the target speaker corpus. Experiments are conducted on Chinese and English single speaker's corpora, and the results show that the method effectively balances the model training speed and the synthesized speech quality and outperforms the current models",
    "checked": true,
    "id": "7111cd9edd2c5e29a79d868288fe85eefed9fc48",
    "semantic_title": "a fast and high-quality text-to-speech method with compressed auxiliary corpus and limited target speaker corpus",
    "citation_count": 0,
    "authors": [
      "Ye Tao",
      "Chaofeng Lu",
      "Meng Liu",
      "Kai Xu",
      "Tianyu Liu",
      "Yunlong Tian",
      "Yongjie Du"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.47": {
    "title": "A Frustratingly Simple Decoding Method for Neural Text Generation",
    "volume": "main",
    "abstract": "We introduce a frustratingly simple, highly efficient, and surprisingly effective decoding method, termed Frustratingly Simple Decoding (FSD), for neural text generation. The idea behind FSD is straightforward: We construct an anti-language model (anti-LM) based on previously generated text, which is employed to penalize the future generation of repetitive content. The anti-LM can be implemented as simple as an n-gram language model or a vectorized variant. In this way, FSD incurs no additional model parameters and negligible computational overhead (FSD can be as fast as greedy search). Despite its simplicity, FSD is surprisingly effective and generalizes across different datasets, models, and languages. Extensive experiments show that FSD outperforms established strong baselines in terms of generation quality, decoding speed, and universality",
    "checked": true,
    "id": "bb6ae69945f0f90c5197f8ab7072dcc8eec02cd6",
    "semantic_title": "a frustratingly simple decoding method for neural text generation",
    "citation_count": 6,
    "authors": [
      "Haoran Yang",
      "Deng Cai",
      "Huayang Li",
      "Wei Bi",
      "Wai Lam",
      "Shuming Shi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.48": {
    "title": "A Gaze-grounded Visual Question Answering Dataset for Clarifying Ambiguous Japanese Questions",
    "volume": "main",
    "abstract": "Situated conversations, which refer to visual information as visual question answering (VQA), often contain ambiguities caused by reliance on directive information. This problem is exacerbated because some languages, such as Japanese, often omit subjective or objective terms. Such ambiguities in questions are often clarified by the contexts in conversational situations, such as joint attention with a user or user gaze information. In this study, we propose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous questions using gaze information by focusing on a clarification process complemented by gaze information. We also propose a method that utilizes gaze target estimation results to improve the accuracy of GazeVQA tasks. Our experimental results showed that the proposed method improved the performance in some cases of a VQA system on GazeVQA and identified some typical problems of GazeVQA tasks that need to be improved",
    "checked": true,
    "id": "9b3ea2cf5ce93f70def1d373f64c580b844b2bb3",
    "semantic_title": "a gaze-grounded visual question answering dataset for clarifying ambiguous japanese questions",
    "citation_count": 0,
    "authors": [
      "Shun Inadumi",
      "Seiya Kawano",
      "Akishige Yuguchi",
      "Yasutomo Kawanishi",
      "Koichiro Yoshino"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.49": {
    "title": "Agenda-Driven Question Generation: A Case Study in the Courtroom Domain",
    "volume": "main",
    "abstract": "This paper introduces a novel problem of automated question generation for courtroom examinations, CourtQG. While question generation has been studied in domains such as educational testing and product description, CourtQG poses several unique challenges owing to its non-cooperative and agenda-driven nature. Specifically, not only the generated questions need to be relevant to the case and underlying context, they also have to achieve certain objectives such as challenging the opponent's arguments and/or revealing potential inconsistencies in their answers. We propose to leverage large language models (LLM) for CourtQG by fine-tuning them on two auxiliary tasks, agenda explanation (i.e., uncovering the underlying intents) and question type prediction. We additionally propose cold-start generation of questions from background documents without relying on examination history. We construct a dataset to evaluate our proposed method and show that it generates better questions according to standard metrics when compared to several baselines",
    "checked": true,
    "id": "41c24ecf8b61ab80293126ba40ea41dbdfb9ffb5",
    "semantic_title": "agenda-driven question generation: a case study in the courtroom domain",
    "citation_count": 0,
    "authors": [
      "Yi Fung",
      "Anoop Kumar",
      "Aram Galstyan",
      "Heng Ji",
      "Prem Natarajan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.50": {
    "title": "A Generative Model for Lambek Categorial Sequents",
    "volume": "main",
    "abstract": "In this work, we introduce a generative model, PLC+, for generating Lambek Categorial Grammar(LCG) sequents. We also introduce a simple method to numerically estimate the model's parameters from an annotated corpus. Then we compare our model with probabilistic context-free grammars (PCFGs) and show that PLC+ simultaneously assigns a higher probability to a common corpus, and has greater coverage",
    "checked": true,
    "id": "51171f45c6d5470fefb216d5fc06b1f9c42810de",
    "semantic_title": "a generative model for lambek categorial sequents",
    "citation_count": 0,
    "authors": [
      "Jinman Zhao",
      "Gerald Penn"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.51": {
    "title": "Agent-based Modeling of Language Change in a Small-world Network",
    "volume": "main",
    "abstract": "Language change has been the subject of numerous studies in linguistics. However, due to the dynamic and complex nature of this phenomenon, and to the difficulty of obtaining extensive real data of language in use, some of its aspects remain obscure. In recent years, nonetheless, research has used computational modeling to simulate features related to variation, change, propagation, and evolution of languages in speech communities, finding compelling results. In this article, agent-based modeling and simulation is used to study language change. Drawing on previous studies, a speech community was modeled using Zachary's karate club network, a well-established small-world network model in the field of complex systems. Idiolects were assigned through numerical values for each agent. The results demonstrate that the centrality of each agent in the network, interpreted as social prestige, appears to be a factor influencing change. Additionally, the nature of idiolects also seems to impact the spread of linguistic variants in the language change process. These findings complement the theoretical understanding of the language change phenomenon with new simulation data and provide new avenues for research",
    "checked": true,
    "id": "e47d0a237505961e282c31eb08fd29c76117aa6a",
    "semantic_title": "agent-based modeling of language change in a small-world network",
    "citation_count": 0,
    "authors": [
      "Dalmo Buzato",
      "Evandro Cunha"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.52": {
    "title": "Agettivu, Aggitivu o Aghjettivu? POS Tagging Corsican Dialects",
    "volume": "main",
    "abstract": "In this paper we present a series of experiments towards POS tagging Corsican, a less-resourced language spoken in Corsica and linguistically related to Italian. The first contribution is Corsican-POS, the first gold standard POS-tagged corpus for Corsica, composed of 500 sentences manually annotated with the Universal POS tagset. Our second contribution is a set of experiments and evaluation of POS tagging models which starts with a baseline model for Italian and is aimed at finding the best training configuration, namely in terms of the size and combination strategy of the existing raw and annotated resources. These experiments result in (i) the first POS tagger for Corsican, reaching an accuracy of 93.38%, (ii) a quantification of the gain provided by the use of each available resource. We find that the optimal configuration uses Italian word embeddings further specialized with Corsican embeddings and trained on the largest gold corpus for Corsican available so far",
    "checked": true,
    "id": "024b1d9db05fc4be4ce4781bb67921b60f628ade",
    "semantic_title": "agettivu, aggitivu o aghjettivu? pos tagging corsican dialects",
    "citation_count": 0,
    "authors": [
      "Alice Millour",
      "Lorenza Brasile",
      "Alberto Ghia",
      "Laurent Kevers"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.53": {
    "title": "Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models",
    "volume": "main",
    "abstract": "Recent advancements in Chain-of-Thought prompting have facilitated significant breakthroughs for Large Language Models (LLMs) in complex reasoning tasks. Current research enhances the reasoning performance of LLMs by sampling multiple reasoning chains and ensembling based on the answer frequency. However, this approach fails in scenarios where the correct answers are in the minority. We identify this as a primary factor constraining the reasoning capabilities of LLMs, a limitation that cannot be resolved solely based on the predicted answers. To address this shortcoming, we introduce a hierarchical reasoning aggregation framework AoR (Aggregation of Reasoning), which selects answers based on the evaluation of reasoning chains. Additionally, AoR incorporates dynamic sampling, adjusting the number of reasoning chains in accordance with the complexity of the task. Experimental results on a series of complex reasoning tasks show that AoR outperforms prominent ensemble methods. Further analysis reveals that AoR not only adapts various LLMs but also achieves a superior performance ceiling when compared to current methods",
    "checked": true,
    "id": "d59e4560e3f3b8d53b0660e4ca2d6ea8d559787b",
    "semantic_title": "aggregation of reasoning: a hierarchical framework for enhancing answer selection in large language models",
    "citation_count": 0,
    "authors": [
      "Zhangyue Yin",
      "Qiushi Sun",
      "Qipeng Guo",
      "Zhiyuan Zeng",
      "Xiaonan Li",
      "Tianxiang Sun",
      "Cheng Chang",
      "Qinyuan Cheng",
      "Ding Wang",
      "Xiaofeng Mou",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.54": {
    "title": "A Hierarchical Sequence-to-Set Model with Coverage Mechanism for Aspect Category Sentiment Analysis",
    "volume": "main",
    "abstract": "Aspect category sentiment analysis (ACSA) aims to simultaneously detect aspect categories and their corresponding sentiment polarities (category-sentiment pairs). Some recent studies have used pre-trained generative models to complete ACSA and achieved good results. However, for ACSA, generative models still face three challenges. First, addressing the missing predictions in ACSA is crucial, which involves accurately predicting all category-sentiment pairs within a sentence. Second, category-sentiment pairs are inherently a disordered set. Consequently, the model incurs a penalty even when its predictions are correct, but the predicted order is inconsistent with the ground truths. Third, different aspect categories should focus on relevant sentiment words, and the polarity of the aspect category should be the aggregation of the polarities of these sentiment words. This paper proposes a hierarchical generative model with a coverage mechanism using sequence-to-set learning to tackle all three challenges simultaneously. Our model's superior performance is demonstrated through extensive experiments conducted on several datasets",
    "checked": true,
    "id": "5dcf17bba0f07a5ed979dc865738285f8229eb3d",
    "semantic_title": "a hierarchical sequence-to-set model with coverage mechanism for aspect category sentiment analysis",
    "citation_count": 0,
    "authors": [
      "Siyu Wang",
      "Jianhui Jiang",
      "Shengran Dai",
      "Jiangtao Qiu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.55": {
    "title": "A Hong Kong Sign Language Corpus Collected from Sign-interpreted TV News",
    "volume": "main",
    "abstract": "This paper introduces TVB-HKSL-News, a new Hong Kong sign language (HKSL) dataset collected from a TV news program over a period of 7 months. The dataset is collected to enrich resources for HKSL and support research in large-vocabulary continuous sign language recognition (SLR) and translation (SLT). It consists of 16.07 hours of sign videos of two signers with a vocabulary of 6,515 glosses (for SLR) and 2,850 Chinese characters or 18K Chinese words (for SLT). One signer has 11.66 hours of sign videos and the other has 4.41 hours. One objective in building the dataset is to support the investigation of how well large-vocabulary continuous sign language recognition/translation can be done for a single signer given a (relatively) large amount of his/her training data, which could potentially lead to the development of new modeling methods. Besides, most parts of the data collection pipeline are automated with little human intervention; we believe that our collection method can be scaled up to collect more sign language data easily for SLT in the future for any sign languages if such sign-interpreted videos are available. We also run a SOTA SLR/SLT model on the dataset and get a baseline SLR word error rate of 34.08% and a baseline SLT BLEU-4 score of 23.58 for benchmarking future research on the dataset",
    "checked": true,
    "id": "eabce17185aa25bdd86d675f8958d7cfabe3938b",
    "semantic_title": "a hong kong sign language corpus collected from sign-interpreted tv news",
    "citation_count": 0,
    "authors": [
      "Zhe Niu",
      "Ronglai Zuo",
      "Brian Mak",
      "Fangyun Wei"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.56": {
    "title": "A Hybrid Approach to Aspect Based Sentiment Analysis Using Transfer Learning",
    "volume": "main",
    "abstract": "Aspect-Based Sentiment Analysis ( ABSA) aims to identify terms or multiword expressions (MWEs) on which sentiments are expressed and the sentiment polarities associated with them. The development of supervised models has been at the forefront of research in this area. However, training these models requires the availability of manually annotated datasets which is both expensive and time-consuming. Furthermore, the available annotated datasets are tailored to a specific domain, language, and text type. In this work, we address this notable challenge in current state-of-the-art ABSA research. We propose a hybrid approach for Aspect Based Sentiment Analysis using transfer learning. The approach focuses on generating weakly-supervised annotations by exploiting the strengths of both large language models (LLM) and traditional syntactic dependencies. We utilise syntactic dependency structures of sentences to complement the annotations generated by LLMs, as they may overlook domain-specific aspect terms. Extensive experimentation on multiple datasets is performed to demonstrate the efficacy of our hybrid method for the tasks of aspect term extraction and aspect sentiment classification",
    "checked": true,
    "id": "ea23b1f298b5b50ffc105bb71adad53f65255be0",
    "semantic_title": "a hybrid approach to aspect based sentiment analysis using transfer learning",
    "citation_count": 1,
    "authors": [
      "Gaurav Negi",
      "Rajdeep Sarkar",
      "Omnia Zayed",
      "Paul Buitelaar"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.57": {
    "title": "A Japanese News Simplification Corpus with Faithfulness",
    "volume": "main",
    "abstract": "Text Simplification enhances the readability of texts for specific audiences. However, automated models may introduce unwanted content or omit essential details, necessitating a focus on maintaining faithfulness to the original input. Furthermore, existing simplified corpora contain instances of low faithfulness. Motivated by this issue, we present a new Japanese simplification corpus designed to prioritize faithfulness. Our collection comprises 7,075 paired sentences simplified from newspaper articles. This process involved collaboration with language education experts who followed guidelines balancing readability and faithfulness. Through corpus analysis, we confirmed that our dataset preserves the content of the original text, including personal names, dates, and city names. Manual evaluation showed that our corpus robustly maintains faithfulness to the original text, surpassing other existing corpora. Furthermore, evaluation by non-native readers confirmed its readability to the target audience. Through the experiment of fine-tuning and in-context learning, we demonstrated that our corpus enhances faithful sentence simplification",
    "checked": true,
    "id": "e03d6dfdc83b8c42dfeaed0b6be0e3a536579031",
    "semantic_title": "a japanese news simplification corpus with faithfulness",
    "citation_count": 0,
    "authors": [
      "Toru Urakawa",
      "Yuya Taguchi",
      "Takuro Niitsuma",
      "Hideaki Tamori"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.58": {
    "title": "A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation",
    "volume": "main",
    "abstract": "Knowledge-based, open-domain dialogue generation aims to build chit-chat systems that talk to humans using mined support knowledge. Many types and sources of knowledge have previously been shown to be useful as support knowledge. Even in the era of large language models, response generation grounded in knowledge retrieved from additional up-to-date sources remains a practically important approach. While prior work using single-source knowledge has shown a clear positive correlation between the performances of knowledge selection and response generation, there are no existing multi-source datasets for evaluating support knowledge retrieval. Further, prior work has assumed that the knowledge sources available at test time are the same as during training. This unrealistic assumption unnecessarily handicaps models, as new knowledge sources can become available after a model is trained. In this paper, we present a high-quality benchmark named multi-source Wizard of Wikipedia (Ms.WoW) for evaluating multi-source dialogue knowledge selection and response generation. Unlike existing datasets, it contains clean support knowledge, grounded at the utterance level and partitioned into multiple knowledge sources. We further propose a new challenge, dialogue knowledge plug-and-play, which aims to test an already trained dialogue model on using new support knowledge from previously unseen sources in a zero-shot fashion",
    "checked": true,
    "id": "10a88156909efbc31bd6c5d8a3da337920476eda",
    "semantic_title": "a knowledge plug-and-play test bed for open-domain dialogue generation",
    "citation_count": 0,
    "authors": [
      "Xiangci Li",
      "Linfeng Song",
      "Lifeng Jin",
      "Haitao Mi",
      "Jessica Ouyang",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.59": {
    "title": "A Large Annotated Reference Corpus of New High German Poetry",
    "volume": "main",
    "abstract": "This paper introduces a large annotated corpus of public domain German poetry, covering the time period from 1600 to the 1920s with 65k poems. We describe how the corpus was compiled, how it was cleaned (including duplicate detection), and how it looks now in terms of size, format, temporal distribution, and automatic annotation. Besides metadata, the corpus contains reliable annotation of tokens, syllables, part-of-speech, and meter and verse measure. Finally, we give some statistics on the annotation and an overview of other poetry corpora",
    "checked": true,
    "id": "398cdba624e0b08319c19e478b41e66f97a5a05c",
    "semantic_title": "a large annotated reference corpus of new high german poetry",
    "citation_count": 0,
    "authors": [
      "Thomas Haider"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.60": {
    "title": "A Lifelong Multilingual Multi-granularity Semantic Alignment Approach via Maximum Co-occurrence Probability",
    "volume": "main",
    "abstract": "Cross-lingual pre-training methods mask and predict tokens in multilingual text to generalize diverse multilingual information. However, due to the lack of sufficient aligned multilingual resources in the pre-training process, these methods may not fully explore the multilingual correlation of masked tokens, resulting in the limitation of multilingual information interaction. In this paper, we propose a lifelong multilingual multi-granularity semantic alignment approach, which continuously extracts massive aligned linguistic units from noisy data via a maximum co-occurrence probability algorithm. Then, the approach releases a version of the multilingual multi-granularity semantic alignment resource, supporting seven languages, namely English, Czech, German, Russian, Romanian, Hindi and Turkish. Finally, we propose how to use this resource to improve the translation performance on WMT14 18 benchmarks in twelve directions. Experimental results show an average of 0.3 1.1 BLEU improvements in all translation benchmarks. The analysis and discussion also demonstrate the superiority and potential of the proposed approach. The resource used in this work will be publicly available",
    "checked": true,
    "id": "6d7283b9d1c41803f94c7b10e04e84875bc44251",
    "semantic_title": "a lifelong multilingual multi-granularity semantic alignment approach via maximum co-occurrence probability",
    "citation_count": 0,
    "authors": [
      "Xin Liu",
      "Hongwei Sun",
      "Shaojie Dai",
      "Bo Lv",
      "Youcheng Pan",
      "Hui Wang",
      "Yue Yu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.61": {
    "title": "A Lightweight Approach to a Giga-Corpus of Historical Periodicals: The Story of a Slovenian Historical Newspaper Collection",
    "volume": "main",
    "abstract": "Preparing historical newspaper collections is a complicated endeavour, consisting of multiple steps that have to be carefully adapted to the specific content in question, including imaging, layout prediction, optical character recognition, and linguistic annotation. To address the high costs associated with the process, we present a lightweight approach to producing high-quality corpora and apply it to a massive collection of Slovenian historical newspapers from the 18th, 19th and 20th century resulting in a billion-word giga-corpus. We start with noisy OCR-ed data produced by different technologies in varying periods by the National and University Library of Slovenia. To address the inherent variability in the quality of textual data, a challenge commonly encountered in digital libraries globally, we perform a targeted post-digitisation correction procedure, coupled with a robust curation mechanism for noisy texts via language model inference. Subsequently, we subject the corrected and filtered output to comprehensive linguistic annotation, enriching the corpus with part-of-speech tags, lemmas, and named entity labels. Finally, we perform an analysis through topic modeling at the noun lemma level, along with a frequency analysis of the named entities, to confirm the viability of our corpus preparation method",
    "checked": true,
    "id": "eef234052cdcaf91eafc047cffecae28e5f62934",
    "semantic_title": "a lightweight approach to a giga-corpus of historical periodicals: the story of a slovenian historical newspaper collection",
    "citation_count": 0,
    "authors": [
      "Filip Dobranić",
      "Bojan Evkoski",
      "Nikola Ljubešić"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.62": {
    "title": "Aligning the Norwegian UD Treebank with Entity and Coreference Information",
    "volume": "main",
    "abstract": "This paper presents a merged collection of entity and coreference annotated data grounded in the Universal Dependencies (UD) treebanks for the two written forms of Norwegian: Bokmål and Nynorsk. The aligned and converted corpora are the Norwegian Named Entities (NorNE) and Norwegian Anaphora Resolution Corpus (NARC). While NorNE is aligned with an older version of the treebank, NARC is misaligned and requires extensive transformation from the original annotations to the UD structure and CoNLL-U format. Here, we demonstrate the conversion and alignment processes, along with an analysis of discovered issues and errors in the data, some of which include data split overlaps in the original treebank. These procedures and the developed system may prove helpful for future work on processing and aligning data from universal dependencies. The merged corpora comprise the first Norwegian UD treebank enriched with named entities and coreference information, supporting the standardized format for the CorefUD initiative",
    "checked": true,
    "id": "c13d0fa24e441b009c192942104bfb78938aac5e",
    "semantic_title": "aligning the norwegian ud treebank with entity and coreference information",
    "citation_count": 0,
    "authors": [
      "Tollef Emil Jørgensen",
      "Andre Kåsen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.63": {
    "title": "Alignment before Awareness: Towards Visual Question Localized-Answering in Robotic Surgery via Optimal Transport and Answer Semantics",
    "volume": "main",
    "abstract": "The visual question localized-answering (VQLA) system has garnered increasing attention due to its potential as a knowledgeable assistant in surgical education. Apart from providing text-based answers, VQLA can also pinpoint the specific region of interest for better surgical scene understanding. Although recent Transformer-based models for VQLA have obtained promising results, they (1) conduct vanilla text-to-image cross attention, leading to unidirectional and coarse-grained alignment; (2) ignore exploiting the semantics of answers to further boost performance. In this paper, we propose a novel model termed OTAS, which first introduces optimal transport to achieve bidirectional and fine-grained alignment between images and questions, enabling more precise localization. Besides, OTAS incorporates a set of learnable candidate answer embeddings to query the probability of each answer class for a given image-question pair. Through Transformer attention, the candidate answer embeddings interact with the fused features of the image-question pair to make the answer decision. Extensive experiments on two widely-used benchmark datasets demonstrate the superiority of our model over state-of-the-art methods",
    "checked": true,
    "id": "847bceb8c88cbfb7775caf8f15f1486ff9807f78",
    "semantic_title": "alignment before awareness: towards visual question localized-answering in robotic surgery via optimal transport and answer semantics",
    "citation_count": 1,
    "authors": [
      "Zhihong Zhu",
      "Yunyan Zhang",
      "Xuxin Cheng",
      "Zhiqi Huang",
      "Derong Xu",
      "Xian Wu",
      "Yefeng Zheng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.64": {
    "title": "Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation",
    "volume": "main",
    "abstract": "The advent of scalable deep models and large datasets has improved the performance of Neural Machine Translation (NMT). Knowledge Distillation (KD) enhances efficiency by transferring knowledge from a teacher model to a more compact student model. However, KD approaches to Transformer architecture often rely on heuristics, particularly when deciding which teacher layers to distill from. In this paper, we introduce the \"Align-to-Distill\" (A2D) strategy, designed to address the feature mapping problem by adaptively aligning student attention heads with their teacher counterparts during training. The Attention Alignment Module (AAM) in A2D performs a dense head-by-head comparison between student and teacher attention heads across layers, turning the combinatorial mapping heuristics into a learning problem. Our experiments show the efficacy of A2D, demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De→Dsb and WMT-2014 En→De, respectively, compared to Transformer baselines.The code and data are available at https://github.com/ncsoft/Align-to-Distill",
    "checked": true,
    "id": "6e33594cb0af7def21d805046be3957ab9461cf4",
    "semantic_title": "align-to-distill: trainable attention alignment for knowledge distillation in neural machine translation",
    "citation_count": 0,
    "authors": [
      "Heegon Jin",
      "Seonil Son",
      "Jemin Park",
      "Youngseok Kim",
      "Hyungjong Noh",
      "Yeonsoo Lee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.65": {
    "title": "A Linguistically-Informed Annotation Strategy for Korean Semantic Role Labeling",
    "volume": "main",
    "abstract": "Semantic role labeling is an essential component of semantic and syntactic processing of natural languages, which reveals the predicate-argument structure of the language. Despite its importance, semantic role labeling for the Korean language has not been studied extensively. One notable issue is the lack of uniformity among data annotation strategies across different datasets, which often lack thorough rationales. In this study, we suggest an annotation strategy for Korean semantic role labeling that is in line with the previously proposed linguistic theories as well as the distinct properties of the Korean language. We further propose a simple yet viable conversion strategy from the Sejong verb dictionary to a CoNLL-style dataset for Korean semantic role labeling. Experiment results using a transformer-based sequence labeling model demonstrate the reliability and trainability of the converted dataset",
    "checked": true,
    "id": "38ab20ea9d2318afaf319f6968713b7c44100a03",
    "semantic_title": "a linguistically-informed annotation strategy for korean semantic role labeling",
    "citation_count": 0,
    "authors": [
      "Yige Chen",
      "KyungTae Lim",
      "Jungyeul Park"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.66": {
    "title": "Alleviating Exposure Bias in Abstractive Summarization via Sequentially Generating and Revising",
    "volume": "main",
    "abstract": "Abstractive summarization commonly suffers from exposure bias caused by supervised teacher-force learning, that a model predicts the next token conditioned on the accurate pre-context during training while on its preceding outputs at inference. Existing solutions bridge this gap through un- or semi-supervised holistic learning yet still leave the risk of error accumulation while generating a summary. In this paper, we attribute this problem to the limitation of unidirectional autoregressive text generation and introduce post-processing steps to alleviate it. Specifically, we reformat abstractive summarization to sequential generation and revision (SeGRe), i.e., a model in the revision phase re-inputs the generated summary and refines it by contrasting it with the source document. This provides the model additional opportunities to assess the flawed summary from a global view and thereby modify inappropriate expressions. Moreover, we train the SeGRe model with a regularized minimum-risk policy to ensure effective generation and revision. A lot of comparative experiments are implemented on two well-known datasets, exhibiting the new or matched state-of-the-art performance of SeGRe",
    "checked": true,
    "id": "8525cc56df55f67a027703a96c01a21c4897f199",
    "semantic_title": "alleviating exposure bias in abstractive summarization via sequentially generating and revising",
    "citation_count": 0,
    "authors": [
      "Jiaxin Duan",
      "Fengyu Lu",
      "Junfei Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.67": {
    "title": "ALLIES: A Speech Corpus for Segmentation, Speaker Diarization, Speech Recognition and Speaker Change Detection",
    "volume": "main",
    "abstract": "This paper presents ALLIES, a meta corpus which gathers and extends existing French corpora collected from radio and TV shows. The corpus contains 1048 audio files for about 500 hours of speech. Agglomeration of data is always a difficult issue, as the guidelines used to collect, annotate and transcribe speech are generally different from one corpus to another. ALLIES intends to homogenize and correct speaker labels among the different files by integrated human feedback within a speaker verification system. The main contribution of this article is the design of a protocol in order to evaluate properly speech segmentation (including music and overlap detection), speaker diarization, speech transcription and speaker change detection. As part of it, a test partition has been carefully manually 1) segmented and annotated according to speech, music, noise, speaker labels with specific guidelines for overlap speech, 2) orthographically transcribed. This article also provides as a second contribution baseline results for several speech processing tasks",
    "checked": true,
    "id": "7f6cefe5136a7bd7dbd2fd5b49f10ddc0b2e4aca",
    "semantic_title": "allies: a speech corpus for segmentation, speaker diarization, speech recognition and speaker change detection",
    "citation_count": 0,
    "authors": [
      "Marie Tahon",
      "Anthony Larcher",
      "Martin Lebourdais",
      "Fethi Bougares",
      "Anna Silnova",
      "Pablo Gimeno"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.68": {
    "title": "A Logical Pattern Memory Pre-trained Model for Entailment Tree Generation",
    "volume": "main",
    "abstract": "Generating coherent and credible explanations remains a significant challenge in the field of AI. In recent years, researchers have delved into the utilization of entailment trees to depict explanations, which exhibit a reasoning process of how a hypothesis is deduced from the supporting facts. However, existing models often overlook the importance of generating intermediate conclusions with logical consistency from the given facts, leading to inaccurate conclusions and undermining the overall credibility of entailment trees. To address this limitation, we propose the logical pattern memory pre-trained model (LMPM). LMPM incorporates an external memory structure to learn and store the latent representations of logical patterns, which aids in generating logically consistent conclusions. Furthermore, to mitigate the influence of logically irrelevant domain knowledge in the Wikipedia-based data, we introduce an entity abstraction approach to construct the dataset for pre-training LMPM. The experimental results highlight the effectiveness of our approach in improving the quality of entailment tree generation. By leveraging logical entailment patterns, our model produces more coherent and reasonable conclusions that closely align with the underlying premises",
    "checked": true,
    "id": "d5d23834e7f25a5c5ef7a74ce01792281c1521fc",
    "semantic_title": "a logical pattern memory pre-trained model for entailment tree generation",
    "citation_count": 0,
    "authors": [
      "Li Yuan",
      "Yi Cai",
      "Haopeng Ren",
      "Jiexin Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.69": {
    "title": "AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework",
    "volume": "main",
    "abstract": "The task of financial analysis primarily encompasses two key areas: stock trend prediction and the corresponding financial question answering. Currently, machine learning and deep learning algorithms (ML&DL) have been widely applied for stock trend predictions, leading to significant progress. However, these methods fail to provide reasons for predictions, lacking interpretability and reasoning processes. Also, they can not integrate textual information such as financial news or reports. Meanwhile, large language models (LLM) have remarkable textual understanding and generation ability. But due to the scarcity of financial training datasets and limited integration with real-time knowledge, LLM still suffer from hallucinations and unable to keep up with the latest information. To tackle these challenges, we first release AlphaFin datasets, combining traditional research datasets, real-time financial data, and handwritten chain-of-thought (CoT) data. It has positive impact on training LLM for completing financial analysis. We then use AlphaFin datasets to benchmark a state-of-the-art method, called Stock-Chain, for effectively tackling the financial analysis task, which integrates retrieval-augmented generation (RAG) techniques. Extensive experiments are conducted to demonstrate the effectiveness of our framework on financial analysis",
    "checked": true,
    "id": "3a6bdf724da556ca534a9786b7a9f3f0adc567f7",
    "semantic_title": "alphafin: benchmarking financial analysis with retrieval-augmented stock-chain framework",
    "citation_count": 1,
    "authors": [
      "Xiang Li",
      "Zhenyu Li",
      "Chen Shi",
      "Yong Xu",
      "Qing Du",
      "Mingkui Tan",
      "Jun Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.70": {
    "title": "A Luxembourgish Corpus as a Gender Bias Evaluation Testset",
    "volume": "main",
    "abstract": "According to the United Nations Development Programme, gender inequality is a metric that is composed of three dimensions: reproductive health, empowerment, and the labour market. Gender inequality is an obstacle to equal opportunities in society as a whole. In this paper we present our work-in-progress of designing and playing a physical game with digital elements. We currently conduct Conversation Analysis of transcribed speech of 58567 words and documenting bias. We also test OpenAI's ChatGPT for bias in quiz-like gender-related questions",
    "checked": true,
    "id": "e4e165cecb65aeaf713705052516e24836e3a452",
    "semantic_title": "a luxembourgish corpus as a gender bias evaluation testset",
    "citation_count": 0,
    "authors": [
      "Dimitra Anastasiou",
      "Carole Blond-Hanten",
      "Marie Gallais"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.71": {
    "title": "A Matter of Perspective: Building a Multi-Perspective Annotated Dataset for the Study of Literary Quality",
    "volume": "main",
    "abstract": "Studies on literary quality have constantly stimulated the interest of critics, both in theoretical and empirical fields. To examine the perceived quality of literary works, some approaches have focused on data annotated through crowd-sourcing platforms, and others relied on available expert annotated data. In this work, we contribute to the debate by presenting a dataset collecting quality judgments on 9,000 19th and 20th century English-language literary novels by 3,150 predominantly Anglophone authors. We incorporate expert opinions and crowd-sourced annotations to allow comparative analyses between different literary quality evaluations. We also provide several textual metrics chosen for their potential connection with literary reception and engagement. While a large part of the texts is subjected to copyright, we release quality and reception measures together with stylometric and sentiment data for each of the 9,000 novels to promote future research and comparison",
    "checked": true,
    "id": "2f25bb37fdaaa2b202a0979a59b4fb995d9dddc0",
    "semantic_title": "a matter of perspective: building a multi-perspective annotated dataset for the study of literary quality",
    "citation_count": 0,
    "authors": [
      "Yuri Bizzoni",
      "Pascale Feldkamp Moreira",
      "Ida Marie S. Lassen",
      "Mads Rosendahl Thomsen",
      "Kristoffer Nielbo"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.72": {
    "title": "AMenDeD: Modelling Concepts by Aligning Mentions, Definitions and Decontextualised Embeddings",
    "volume": "main",
    "abstract": "Contextualised Language Models (LM) improve on traditional word embeddings by encoding the meaning of words in context. However, such models have also made it possible to learn high-quality decontextualised concept embeddings. Three main strategies for learning such embeddings have thus far been considered: (i) fine-tuning the LM to directly predict concept embeddings from the name of the concept itself, (ii) averaging contextualised representations of mentions of the concept in a corpus, and (iii) encoding definitions of the concept. As these strategies have complementary strengths and weaknesses, we propose to learn a unified embedding space in which all three types of representations can be integrated. We show that this allows us to outperform existing approaches in tasks such as ontology completion, which heavily depends on access to high-quality concept embeddings. We furthermore find that mentions and definitions are well-aligned in the resulting space, enabling tasks such as target sense verification, even without the need for any fine-tuning",
    "checked": true,
    "id": "859123cbf02aae6f597d625293d03a8896ed7abc",
    "semantic_title": "amended: modelling concepts by aligning mentions, definitions and decontextualised embeddings",
    "citation_count": 0,
    "authors": [
      "Amit Gajbhiye",
      "Zied Bouraoui",
      "Luis Espinosa Anke",
      "Steven Schockaert"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.73": {
    "title": "A Multi-Label Dataset of French Fake News: Human and Machine Insights",
    "volume": "main",
    "abstract": "We present a corpus of 100 documents, named OBSINFOX, selected from 17 sources of French press considered unreliable by expert agencies, annotated using 11 labels by 8 annotators. By collecting more labels than usual, by more annotators than is typically done, we can identify features that humans consider as characteristic of fake news, and compare them to the predictions of automated classifiers. We present a topic and genre analysis using Gate Cloud, indicative of the prevalence of satire-like text in the corpus. We then use the subjectivity analyzer VAGO, and a neural version of it, to clarify the link between ascriptions of the label Subjective and ascriptions of the label Fake News. The annotated dataset is available online at the following url: https://github.com/obs-info/obsinfox Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion, Exaggeration, French Press",
    "checked": true,
    "id": "d6a35b262f330b2f0a30f94f02df94b3ffd268fd",
    "semantic_title": "a multi-label dataset of french fake news: human and machine insights",
    "citation_count": 0,
    "authors": [
      "Benjamin Icard",
      "François Maine",
      "Morgane Casanova",
      "Géraud Faye",
      "Julien Chanson",
      "Guillaume Gadek",
      "Ghislain Atemezing",
      "François Bancilhon",
      "Paul Égré"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.74": {
    "title": "A Multi-layered Approach to Physical Commonsense Understanding: Creation and Evaluation of an Italian Dataset",
    "volume": "main",
    "abstract": "In this paper, we explore physical commonsense reasoning of large language models (LLMs) and propose a specific methodology to evaluate low-level understanding of the physical world. Specifically, the goal is to create a test set to analyze physical commonsense reasoning in large language models for Italian and focus on a trustworthy analysis of the results. To that end, we present a tiered Italian dataset, called Graded Italian Annotated dataset (GITA), written and thoroughly annotated by a professional linguist, which allows us to concentrate on three different levels of commonsense understanding. Moreover, we create a semi-automated system to complete the accurate annotation of the dataset. We also validate our dataset by carrying out three tasks with a multilingual model (XLM-RoBERTa) and propose a qualitative analysis of the results. We found out that, although the model may perform at high-level classification tasks, its easoning is inconsistent and unverifiable, since it does not capture intermediate evidence",
    "checked": true,
    "id": "29061cb5803943ad1230c1054d9dc77fede5ee23",
    "semantic_title": "a multi-layered approach to physical commonsense understanding: creation and evaluation of an italian dataset",
    "citation_count": 0,
    "authors": [
      "Giulia Pensa",
      "Begoña Altuna",
      "Itziar Gonzalez-Dios"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.75": {
    "title": "A Multilingual Parallel Corpus for Aromanian",
    "volume": "main",
    "abstract": "We report the creation of the first high-quality corpus of Aromanian - an endangered Romance language spoken in the Balkans - and the equivalent sentence-aligned translations into Romanian, English, and French. The corpus is released publicly using several orthographic standards and consists in short stories collected in the ‘70s in Romania. Additionally, we provide an corpus-based analysis of Aromanian linguistic particularities and the overall demographic and political context which impacts the contemporary development of the language",
    "checked": true,
    "id": "24e7eb787582162d93aa546b5026f33676a364b6",
    "semantic_title": "a multilingual parallel corpus for aromanian",
    "citation_count": 0,
    "authors": [
      "Iulia Petrariu",
      "Sergiu Nisioi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.76": {
    "title": "A Multimodal French Corpus of Aligned Speech, Text, and Pictogram Sequences for Speech-to-Pictogram Machine Translation",
    "volume": "main",
    "abstract": "The automatic translation of spoken language into pictogram units can facilitate communication involving individuals with language impairments. However, there is no established translation formalism or publicly available datasets for training end-to-end speech translation systems. This paper introduces the first aligned speech, text, and pictogram translation dataset ever created in any language. We provide a French dataset that contains 230 hours of speech resources. We create a rule-based pictogram grammar with a restricted vocabulary and include a discussion of the strategic decisions involved. It takes advantage of an in-depth linguistic study of resources taken from the ARASAAC website. We validate these rules through multiple post-editing phases by expert annotators. The constructed dataset is then used to experiment with a Speech-to-Pictogram cascade model, which employs state-of-the-art Automatic Speech Recognition models. The dataset is freely available under a non-commercial licence. This marks a starting point to conduct research into the automatic translation of speech into pictogram units",
    "checked": true,
    "id": "36dc1469e31ac8d9f47893a4f54bda769692a0bf",
    "semantic_title": "a multimodal french corpus of aligned speech, text, and pictogram sequences for speech-to-pictogram machine translation",
    "citation_count": 0,
    "authors": [
      "Cécile Macaire",
      "Chloé Dion",
      "Jordan Arrigo",
      "Claire Lemaire",
      "Emmanuelle Esperança-Rodier",
      "Benjamin Lecouteux",
      "Didier Schwab"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.77": {
    "title": "A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation",
    "volume": "main",
    "abstract": "In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to produce the description. During training, we keep the visual encoder and language model frozen, focusing on optimizing the modules responsible for creating multimodal in-context references and dynamic prompts. This approach preserves the language generation prowess of large language models (LLMs), facilitating a substantial increase in description diversity. To assess the effectiveness of ModICT across various language model scales and types, we collect data from three distinct product categories within the E-commerce domain. Extensive experiments demonstrate that ModICT significantly improves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4% on D-5) of generated results compared to conventional methods. Our findings underscore the potential of ModICT as a valuable tool for enhancing the automatic generation of product descriptions in a wide range of applications. Data and code are at https://github.com/HITsz-TMG/Multimodal-In-Context-Tuning",
    "checked": true,
    "id": "9a9a3f7bbed6cd6e6da97434b5cb0bf81bf94fdc",
    "semantic_title": "a multimodal in-context tuning approach for e-commerce product description generation",
    "citation_count": 0,
    "authors": [
      "Yunxin Li",
      "Baotian Hu",
      "Wenhan Luo",
      "Lin Ma",
      "Yuxin Ding",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.78": {
    "title": "A Multi-Task Transformer Model for Fine-grained Labelling of Chest X-Ray Reports",
    "volume": "main",
    "abstract": "Precise understanding of free-text radiology reports through localised extraction of clinical findings can enhance medical imaging applications like computer-aided diagnosis. We present a new task, that of segmenting radiology reports into topically meaningful passages (segments) and a transformer-based model that both segments reports into semantically coherent segments and classifies each segment using a set of 37 radiological abnormalities, thus enabling fine-grained analysis. This contrasts with prior work that performs classification on full reports without localisation. Trained on over 2.7 million unlabelled chest X-ray reports and over 28k segmented and labelled reports, our model achieves state-of-the-art performance on report segmentation (0.0442 WinDiff) and multi-label classification (0.84 report-level macro F1) over 37 radiological labels and 8 NLP-specific labels. This work establishes new benchmarks for fine-grained understanding of free-text radiology reports, with precise localisation of semantics unlocking new opportunities to improve computer vision model training and clinical decision support. We open-source our annotation tool, model code and pretrained weights to encourage future research",
    "checked": true,
    "id": "e52afc0fcb8beb759e5dfa7345e5cb2e14ebeb9f",
    "semantic_title": "a multi-task transformer model for fine-grained labelling of chest x-ray reports",
    "citation_count": 0,
    "authors": [
      "Yuanyi Zhu",
      "Maria Liakata",
      "Giovanni Montana"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.79": {
    "title": "Analysis of Sensation-transfer Dialogues in Motorsports",
    "volume": "main",
    "abstract": "Clarifying the effects of subjective ideas on group performance is essential for future dialogue systems to improve mutual understanding among humans and group creativity. However, there has been little focus on dialogue research on quantitatively analyzing the effects of the quality and quantity of subjective information contained in dialogues on group performance. We hypothesize that the more subjective information interlocutors exchange, the better the group performance in collaborative work. We collected dialogues between drivers and engineers in motorsports when deciding how the car should be tuned as a suitable case to verify this hypothesis. Our analysis suggests that the greater the amount of subjective information (which we defined as \"sensation\") in the driver's utterances, the greater the race performance and driver satisfaction with the car's tuning. The results indicate that it is essential for the development of dialogue research to create a corpus of situations that require high performance through collaboration among experts with different backgrounds but who have mastered their respective fields",
    "checked": true,
    "id": "e1842e00b530c913b2291870ab60eebcc9f396d6",
    "semantic_title": "analysis of sensation-transfer dialogues in motorsports",
    "citation_count": 0,
    "authors": [
      "Takeru Isaka",
      "Atsushi Otsuka",
      "Iwaki Toshima"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.80": {
    "title": "Analysis on Unsupervised Acquisition Process of Bilingual Vocabulary through Iterative Back-Translation",
    "volume": "main",
    "abstract": "In this paper, we investigate how new bilingual vocabulary is acquired through Iterative Back-Translation (IBT), which is known as a data augmentation method for machine translation from monolingual data of both source and target languages. To reveal the acquisition process, we first identify the word translation pairs in test data that do not exist in a bilingual data but do only in two monolingual data, then observe how many pairs are successfully translated by the translation model trained through IBT. We experimented on it with domain adaptation settings on two language pairs. Our experimental evaluation showed that more than 60% of the new bilingual vocabulary is successfully acquired through IBT along with the improvement in the translation quality in terms of BLEU. It also revealed that new bilingual vocabulary was gradually acquired by repeating IBT iterations. From the results, we present our hypothesis on the process of new bilingual vocabulary acquisition where the context of the words plays a critical role in the success of the acquisition",
    "checked": true,
    "id": "577be763c95d63238d78b286f456a4a70b947af4",
    "semantic_title": "analysis on unsupervised acquisition process of bilingual vocabulary through iterative back-translation",
    "citation_count": 0,
    "authors": [
      "Takuma Tanigawa",
      "Tomoyosi Akiba",
      "Hajime Tsukada"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.81": {
    "title": "Analyzing Chain-of-thought Prompting in Black-Box Large Language Models via Estimated V-information",
    "volume": "main",
    "abstract": "Chain-of-Thought (CoT) prompting combined with large language models (LLM) has shown great potential in improving performance on challenging reasoning tasks. While understanding why CoT prompting is effective is crucial for the application and improvement of CoT prompting, few studies have addressed this issue. Besides, almost no prior work has conducted theoretical analysis on CoT prompting in the context of black-box models. In this paper, we approach the analysis of CoT prompting in black-box LLMs from an information-theoretic perspective. Specifically, we propose a new metric, EPVI (Estimated Pointwise V-Information), which extends the concept of pointwise V-information to black-box models, quantifying the label-relevant new information introduced by CoT prompting beyond the pre-existing information in the input. Based on this, we conduct a series of experiments at both the task and instance levels to analyze CoT prompting, demonstrating that the effectiveness of CoT prompting can be attributed to its capacity to influence the difficulty of model inference by augmenting or reducing the model-usable information. Furthermore, we show that selecting high-quality demonstrations of CoT reasoning based on EPVI can improve the downstream performance of reasoning tasks",
    "checked": true,
    "id": "7458265fea70fe1cf94a086635578a0a4d5062c7",
    "semantic_title": "analyzing chain-of-thought prompting in black-box large language models via estimated v-information",
    "citation_count": 0,
    "authors": [
      "Zecheng Wang",
      "Chunshan Li",
      "Zhao Yang",
      "Qingbin Liu",
      "Yanchao Hao",
      "Xi Chen",
      "Dianhui Chu",
      "Dianbo Sui"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.82": {
    "title": "Analyzing Effects of Learning Downstream Tasks on Moral Bias in Large Language Models",
    "volume": "main",
    "abstract": "Pre-training and fine-tuning large language models (LMs) is currently the state-of-the-art methodology for enabling data-scarce downstream tasks. However, the derived models still tend to replicate and perpetuate social biases. To understand this process in more detail, this paper investigates the actual effects of learning downstream tasks on moral bias in LMs. We develop methods to assess the agreement of LMs to explicitly codified norms in both pre-training and fine-tuning stages. Even if a pre-trained foundation model exhibits consistent norms, we find that introducing downstream tasks may indeed lead to unexpected inconsistencies in norm representation. Specifically, we observe two phenomena during fine-tuning across both masked and causal LMs: (1) pre-existing moral bias may be mitigated or amplified even when presented with opposing views and (2) prompt sensitivity may be negatively impacted. We provide empirical evidence of models deteriorating into conflicting states, where contradictory answers can easily be triggered by slight modifications in the input sequence. Our findings thus raise concerns about the general ability of LMs to mitigate moral biases effectively",
    "checked": true,
    "id": "2857f84952ef4e1edb9e63027e75e015345e424b",
    "semantic_title": "analyzing effects of learning downstream tasks on moral bias in large language models",
    "citation_count": 0,
    "authors": [
      "Niklas Kiehne",
      "Alexander Ljapunov",
      "Marc Bätje",
      "Wolf-Tilo Balke"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.83": {
    "title": "Analyzing Homonymy Disambiguation Capabilities of Pretrained Language Models",
    "volume": "main",
    "abstract": "Word Sense Disambiguation (WSD) is a key task in Natural Language Processing (NLP), aiming to assign the correct meaning (sense) to a word in context. However, traditional WSD systems rely on WordNet as the underlying sense inventory, often differentiating meticulously between subtle nuances of word meanings, which may lead to excessive complexity and reduced practicality of WSD systems in today's NLP. Indeed, current Pretrained Language Models (PLMs) do seem to be able to perform disambiguation, but it is not clear to what extent, or to what level of granularity, they actually operate. In this paper, we address these points and, firstly, introduce a new large-scale resource that leverages homonymy relations to systematically cluster WordNet senses, effectively reducing the granularity of word senses to a very coarse-grained level; secondly, we use this resource to train Homonymy Disambiguation systems and investigate whether PLMs are inherently able to differentiate coarse-grained word senses. Our findings demonstrate that, while state-of-the-art models still struggle to choose the correct fine-grained meaning of a word in context, Homonymy Disambiguation systems are able to differentiate homonyms with up to 95% accuracy scores even without fine-tuning the underlying PLM. We release our data and code at https://github.com/SapienzaNLP/homonymy-wsd",
    "checked": true,
    "id": "e45920b82220eb044c7a57ba7d517d8766303595",
    "semantic_title": "analyzing homonymy disambiguation capabilities of pretrained language models",
    "citation_count": 0,
    "authors": [
      "Lorenzo Proietti",
      "Stefano Perrella",
      "Simone Tedeschi",
      "Giulia Vulpis",
      "Leonardo Lavalle",
      "Andrea Sanchietti",
      "Andrea Ferrari",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.84": {
    "title": "Analyzing Interpretability of Summarization Model with Eye-gaze Information",
    "volume": "main",
    "abstract": "Interpretation methods provide saliency scores indicating the importance of input words for neural summarization models. Prior work has analyzed models by comparing them to human behavior, often using eye-gaze as a proxy for human attention in reading tasks such as classification. This paper presents a framework to analyze the model behavior in summarization by comparing it to human summarization behavior using eye-gaze data. We examine two research questions: RQ1) whether model saliency conforms to human gaze during summarization and RQ2) how model saliency and human gaze affect summarization performance. For RQ1, we measure conformity by calculating the correlation between model saliency and human fixation counts. For RQ2, we conduct ablation experiments removing words/sentences considered important by models or humans. Experiments on two datasets with human eye-gaze during summarization partially confirm that model saliency aligns with human gaze (RQ1). However, ablation experiments show that removing highly-attended words/sentences from the human gaze does not significantly degrade performance compared with the removal by the model saliency (RQ2)",
    "checked": true,
    "id": "666a8fab22c06d75854f7f530cca7c20154d80ee",
    "semantic_title": "analyzing interpretability of summarization model with eye-gaze information",
    "citation_count": 0,
    "authors": [
      "Fariz Ikhwantri",
      "Hiroaki Yamada",
      "Takenobu Tokunaga"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.85": {
    "title": "Analyzing Large Language Models' Capability in Location Prediction",
    "volume": "main",
    "abstract": "In this paper, we investigate and evaluate large language models' capability in location prediction. We present experimental results with four models—FLAN-T5, FLAN-UL2, FLAN-Alpaca, and ChatGPT—in various instruction finetuning and exemplar settings. We analyze whether taking into account the context—tweets published before and after the tweet mentioning a location—is beneficial. Additionally, we conduct an ablation study to explore whether instruction modification is beneficial. Lastly, our qualitative analysis sheds light on the errors made by the best-performing model",
    "checked": true,
    "id": "0d23c64e3535b1567a5fa94ca04e2135df13c8f7",
    "semantic_title": "analyzing large language models' capability in location prediction",
    "citation_count": 0,
    "authors": [
      "Zhaomin Xiao",
      "Eduardo Blanco",
      "Yan Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.86": {
    "title": "Analyzing Occupational Distribution Representation in Japanese Language Models",
    "volume": "main",
    "abstract": "Recent advances in large language models (LLMs) have enabled users to generate fluent and seemingly convincing text. However, these models have uneven performance in different languages, which is also associated with undesirable societal biases toward marginalized populations. Specifically, there is relatively little work on Japanese models, despite it being the thirteenth most widely spoken language. In this work, we first develop three Japanese language prompts to probe LLMs' understanding of Japanese names and their association between gender and occupations. We then evaluate a variety of English, multilingual, and Japanese models, correlating the models' outputs with occupation statistics from the Japanese Census Bureau from the last 100 years. Our findings indicate that models can associate Japanese names with the correct gendered occupations when using constrained decoding. However, with sampling or greedy decoding, Japanese language models have a preference for a small set of stereotypically gendered occupations, and multilingual models, though trained on Japanese, are not always able to understand Japanese prompts",
    "checked": true,
    "id": "6a4eea166f69bf29f8d0cbcac71c938fc42f0f7d",
    "semantic_title": "analyzing occupational distribution representation in japanese language models",
    "citation_count": 0,
    "authors": [
      "Katsumi Ibaraki",
      "Winston Wu",
      "Lu Wang",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.87": {
    "title": "Analyzing Symptom-based Depression Level Estimation through the Prism of Psychiatric Expertise",
    "volume": "main",
    "abstract": "The ever-growing number of people suffering from mental distress has motivated significant research initiatives towards automated depression estimation. Despite the multidisciplinary nature of the task, very few of these approaches include medical professionals in their research process, thus ignoring a vital source of domain knowledge. In this paper, we propose to bring the domain experts back into the loop and incorporate their knowledge within the gold-standard DAIC-WOZ dataset. In particular, we define a novel transformer-based architecture and analyse its performance in light of our expert annotations. Overall findings demonstrate a strong correlation between the psychological tendencies of medical professionals and the behavior of the proposed model, which additionally provides new state-of-the-art results",
    "checked": true,
    "id": "5a1e1bf5029deb0801a70567590134b7da466f11",
    "semantic_title": "analyzing symptom-based depression level estimation through the prism of psychiatric expertise",
    "citation_count": 0,
    "authors": [
      "Navneet Agarwal",
      "Kirill Milintsevich",
      "Lucie Metivier",
      "Maud Rotharmel",
      "Gaël Dias",
      "Sonia Dollfus"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.88": {
    "title": "Analyzing the Dynamics of Climate Change Discourse on Twitter: A New Annotated Corpus and Multi-Aspect Classification",
    "volume": "main",
    "abstract": "The discourse surrounding climate change on social media platforms has emerged as a significant avenue for understanding public sentiments, perspectives, and engagement with this critical global issue. The unavailability of publicly available datasets, coupled with ignoring the multi-aspect analysis of climate discourse on social media platforms, has underscored the necessity for further advancement in this area. To address this gap, in this paper, we present an extensive exploration of the intricate realm of climate change discourse on Twitter, leveraging a meticulously annotated ClimaConvo dataset comprising 15,309 tweets. Our annotations encompass a rich spectrum, including aspects like relevance, stance, hate speech, the direction of hate, and humor, offering a nuanced understanding of the discourse dynamics. We address the challenges inherent in dissecting online climate discussions and detail our comprehensive annotation methodology. In addition to annotations, we conduct benchmarking assessments across various algorithms for six tasks: relevance detection, stance detection, hate speech identification, direction and target, and humor analysis. This assessment enhances our grasp of sentiment fluctuations and linguistic subtleties within the discourse. Our analysis extends to exploratory data examination, unveiling tweet distribution patterns, stance prevalence, and hate speech trends. Employing sophisticated topic modeling techniques uncovers underlying thematic clusters, providing insights into the diverse narrative threads woven within the discourse. The findings present a valuable resource for researchers, policymakers, and communicators seeking to navigate the intricacies of climate change discussions. The dataset and resources for this paper are available at https://github.com/shucoll/ClimaConvo",
    "checked": true,
    "id": "aacba5367cdac82e57e171c61db5f0a2cf8c899f",
    "semantic_title": "analyzing the dynamics of climate change discourse on twitter: a new annotated corpus and multi-aspect classification",
    "citation_count": 14,
    "authors": [
      "Shuvam Shiwakoti",
      "Surendrabikram Thapa",
      "Kritesh Rauniyar",
      "Akshyat Shah",
      "Aashish Bhandari",
      "Usman Naseem"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.89": {
    "title": "Analyzing the Performance of Large Language Models on Code Summarization",
    "volume": "main",
    "abstract": "Large language models (LLMs) such as Llama 2 perform very well on tasks that involve both natural language and source code, particularly code summarization and code generation. We show that for the task of code summarization, the performance of these models on individual examples often depends on the amount of (subword) token overlap between the code and the corresponding reference natural language descriptions in the dataset. This token overlap arises because the reference descriptions in standard datasets (corresponding to docstrings in large code bases) are often highly similar to the names of the functions they describe. We also show that this token overlap occurs largely in the function names of the code and compare the relative performance of these models after removing function names versus removing code structure. We also show that using multiple evaluation metrics like BLEU and BERTScore gives us very little additional insight since these metrics are highly correlated with each other",
    "checked": true,
    "id": "bc93eb1fe229bbf1da41d0c163b9306102a849a9",
    "semantic_title": "analyzing the performance of large language models on code summarization",
    "citation_count": 0,
    "authors": [
      "Rajarshi Haldar",
      "Julia Hockenmaier"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.90": {
    "title": "Analyzing the Understanding of Morphologically Complex Words in Large Language Models",
    "volume": "main",
    "abstract": "We empirically study the ability of a Large Language Model (gpt-3.5-turbo-instruct) to understand morphologically complex words. In our experiments, we looked at a variety of tasks to analyse German compounds with regard to compositional word formation and derivation, such as identifying the head noun of existing and novel compounds, identifying the shared verb stem between two words, or recognizing words constructed with inappropriately used derivation morphemes as invalid. Our results show that the language model is generally capable of solving most tasks, except for the task of identifying ill-formed word forms. While the model demonstrated a good overall understanding of complex words and their word-internal structure, the results also suggest that there is no formal knowledge of derivational rules, but rather an interpretation of the observed word parts to derive the meaning of a word",
    "checked": true,
    "id": "313e58abc9871cf1272eabb155add879a18da432",
    "semantic_title": "analyzing the understanding of morphologically complex words in large language models",
    "citation_count": 0,
    "authors": [
      "Marion Weller-Di Marco",
      "Alexander Fraser"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.91": {
    "title": "An Argument for Symmetric Coordination from Dependency Length Minimization: A Replication Study",
    "volume": "main",
    "abstract": "It is well known that left conjuncts tend to be shorter in English coordinate structures. On the basis of Penn Treebank, Przepiórkowski and Woźniak 2023 (in ACL 2023 proceedings) show that this tendency depends on the difference between lengths of conjuncts: the larger the difference, the stronger the tendency for the shorter conjunct to occur on the left. However, this dynamics is observed only when the governor of the coordinate structure is on the left of the coordination (e.g., \"Bring apples and oranges!\") or when it is absent (e.g., \"Come and sing!\"), and not when it is on the right (e.g., \"Apples and oranges fell\"). Given the principle of Dependency Length Minimization, this turns out to provide an argument for the symmetric structure of coordination. We replicate and sharpen this result on the basis of a much larger dataset: parts of the COCA corpus parsed with Stanza. We also investigate the dependence of this result on the assumed unit of length (word vs. character) and on genre",
    "checked": true,
    "id": "1d0b7eb72cdc38010e2d1f0d3a992dc001ca28d5",
    "semantic_title": "an argument for symmetric coordination from dependency length minimization: a replication study",
    "citation_count": 0,
    "authors": [
      "Adam Przepiórkowski",
      "Magdalena Borysiak",
      "Adam Głowacki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.92": {
    "title": "A Natural Approach for Synthetic Short-Form Text Analysis",
    "volume": "main",
    "abstract": "Detecting synthetically generated text in the wild has become increasingly difficult with advances in Natural Language Generation techniques and the proliferation of freely available Large Language Models (LLMs). Social media and news sites can be flooded with synthetically generated misinformation via tweets and posts while authentic users can inadvertently spread this text via shares and retweets. Most modern natural language processing techniques designed to detect synthetically generated text focus primarily on long-form content, such as news articles, or incorporate stylometric characteristics and metadata during their analysis. Unfortunately, for short form text like tweets, this information is often unavailable, usually detached from its original source, displayed out of context, and is often too short or informal to yield significant information from stylometry. This paper proposes a method of detecting synthetically generated tweets via a Transformer architecture and incorporating unique style-based features. Additionally, we have created a new dataset consisting of human-generated and Large Language Model generated tweets for 4 topics and another dataset consisting of tweets paraphrased by 3 different paraphrase models",
    "checked": true,
    "id": "f6d3f1ba63dbf179910532d3475d750fc1de6d9c",
    "semantic_title": "a natural approach for synthetic short-form text analysis",
    "citation_count": 0,
    "authors": [
      "Ruiting Shao",
      "Ryan Schwarz",
      "Christopher Clifton",
      "Edward Delp"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.93": {
    "title": "An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation",
    "volume": "main",
    "abstract": "Data availability is crucial for advancing artificial intelligence applications, including voice-based technologies. As content creation, particularly in social media, experiences increasing demand, translation and text-to-speech (TTS) technologies have become essential tools. Notably, the performance of these TTS technologies is highly dependent on the quality of the training data, emphasizing the mutual dependence of data availability and technological progress. This paper introduces an end-to-end tool to generate high-quality datasets for text-to-speech (TTS) models to address this critical need for high-quality data. The contributions of this work are manifold and include: the integration of language-specific phoneme distribution into sample selection, automation of the recording process, automated and human-in-the-loop quality assurance of recordings, and processing of recordings to meet specified formats. The proposed application aims to streamline the dataset creation process for TTS models through these features, thereby facilitating advancements in voice-based technologies",
    "checked": true,
    "id": "596fab32bc37e548e411a92b4ba6c6d00ac2435b",
    "semantic_title": "an automated end-to-end open-source software for high-quality text-to-speech dataset generation",
    "citation_count": 0,
    "authors": [
      "Ahmet Gunduz",
      "Kamer Ali Yuksel",
      "Kareem Darwish",
      "Golara Javadi",
      "Fabio Minazzi",
      "Nicola Sobieski",
      "Sébastien Bratières"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.94": {
    "title": "Anchor and Broadcast: An Efficient Concept Alignment Approach for Evaluation of Semantic Graphs",
    "volume": "main",
    "abstract": "In this paper, we present AnCast, an intuitive and efficient tool for evaluating graph-based meaning representations (MR). AnCast implements evaluation metrics that are well understood in the NLP community, and they include concept F1, unlabeled relation F1, labeled relation F1, and weighted relation F1. The efficiency of the tool comes from a novel anchor broadcast alignment algorithm that is not subject to the trappings of local maxima. We show through experimental results that the AnCast score is highly correlated with the widely used Smatch score, but its computation takes only about 40% the time",
    "checked": true,
    "id": "5414add7a4d5c49ea30fa329944b70feaf225990",
    "semantic_title": "anchor and broadcast: an efficient concept alignment approach for evaluation of semantic graphs",
    "citation_count": 1,
    "authors": [
      "Haibo Sun",
      "Nianwen Xue"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.95": {
    "title": "An Effective Span-based Multimodal Named Entity Recognition with Consistent Cross-Modal Alignment",
    "volume": "main",
    "abstract": "With the increasing availability of multimodal content on social media, consisting primarily of text and images, multimodal named entity recognition (MNER) has gained a wide-spread attention. A fundamental challenge of MNER lies in effectively aligning different modalities. However, the majority of current approaches rely on word-based sequence labeling framework and align the image and text at inconsistent semantic levels (whole image-words or regions-words). This misalignment may lead to inferior entity recognition performance. To address this issue, we propose an effective span-based method, named SMNER, which achieves a more consistent multimodal alignment from the perspectives of information-theoretic and cross-modal interaction, respectively. Specifically, we first introduce a cross-modal information bottleneck module for the global-level multimodal alignment (whole image-whole text). This module aims to encourage the semantic distribution of the image to be closer to the semantic distribution of the text, which can enable the filtering out of visual noise. Next, we introduce a cross-modal attention module for the local-level multimodal alignment (regions-spans), which captures the correlations between regions in the image and spans in the text, enabling a more precise alignment of the two modalities. Extensive ex- periments conducted on two benchmark datasets demonstrate that SMNER outperforms the state-of-the-art baselines",
    "checked": true,
    "id": "efedcbe664a0b69d144f4fa6849172f9abd018f3",
    "semantic_title": "an effective span-based multimodal named entity recognition with consistent cross-modal alignment",
    "citation_count": 0,
    "authors": [
      "Yongxiu Xu",
      "Hao Xu",
      "Heyan Huang",
      "Shiyao Cui",
      "Minghao Tang",
      "Longzheng Wang",
      "Hongbo Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.96": {
    "title": "An Empirical Study of Synthetic Data Generation for Implicit Discourse Relation Recognition",
    "volume": "main",
    "abstract": "Implicit Discourse Relation Recognition (IDRR), which is the task of recognizing the semantic relation between given text spans that do not contain overt clues, is a long-standing and challenging problem. In particular, the paucity of training data for some error-prone discourse relations makes the problem even more challenging. To address this issue, we propose a method of generating synthetic data for IDRR using a large language model. The proposed method is summarized as two folds: extraction of confusing discourse relation pairs based on false negative rate and synthesis of data focused on the confusion. The key points of our proposed method are utilizing a confusion matrix and adopting two-stage prompting to obtain effective synthetic data. According to the proposed method, we generated synthetic data several times larger than training examples for some error-prone discourse relations and incorporated it into training. As a result of experiments, we achieved state-of-the-art macro-F1 performance thanks to the synthetic data without sacrificing micro-F1 performance and demonstrated its positive effects especially on recognizing some infrequent discourse relations",
    "checked": true,
    "id": "a3a468e87661de42691490084f2b313f5d2733f8",
    "semantic_title": "an empirical study of synthetic data generation for implicit discourse relation recognition",
    "citation_count": 0,
    "authors": [
      "Kazumasa Omura",
      "Fei Cheng",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.97": {
    "title": "An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation",
    "volume": "main",
    "abstract": "Massively multilingual neural machine translation (MMNMT) has been proven to enhance the translation quality of low-resource languages. In this paper, we empirically investigate the translation robustness of Indonesian-Chinese translation in the face of various naturally occurring noise. To assess this, we create a robustness evaluation benchmark dataset for Indonesian-Chinese translation. This dataset is automatically translated into Chinese using four NLLB-200 models of different sizes. We conduct both automatic and human evaluations. Our in-depth analysis reveal the correlations between translation error types and the types of noise present, how these correlations change across different model sizes, and the relationships between automatic evaluation indicators and human evaluation indicators. The dataset is publicly available at https://github.com/tjunlp-lab/ID-ZH-MTRobustEval",
    "checked": true,
    "id": "c2f6a36d010db21de46c7f716628972215412a6e",
    "semantic_title": "an empirical study on the robustness of massively multilingual neural machine translation",
    "citation_count": 0,
    "authors": [
      "Supryadi Supryadi",
      "Leiyu Pan",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.98": {
    "title": "An Evaluation of Croatian ASR Models for Čakavian Transcription",
    "volume": "main",
    "abstract": "To assist in the documentation of Čakavian, an endangered language variety closely related to Croatian, we test four currently available ASR models that are trained with Croatian data and assess their performance in the transcription of Čakavian audio data. We compare the models' word error rates, analyze the word-level error types, and showcase the most frequent Deletion and Substitution errors. The evaluation results indicate that the best-performing system for transcribing Čakavian was a CTC-based variant of the Conformer model",
    "checked": true,
    "id": "7bb5b2f5a70715a76b770306e7f597d3175fd588",
    "semantic_title": "an evaluation of croatian asr models for čakavian transcription",
    "citation_count": 0,
    "authors": [
      "Shulin Zhang",
      "John Hale",
      "Margaret Renwick",
      "Zvjezdana Vrzić",
      "Keith Langston"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.99": {
    "title": "An Event-based Abductive Learning for Hard Time-sensitive Question Answering",
    "volume": "main",
    "abstract": "Time-Sensitive Question Answering (TSQA) is to answer questions qualified for a certain timestamp based on the given document. It is split into easy and hard modes depending on whether the document contain time qualifiers mentioned in the question. While existing models have performed well on easy mode, their performance is significant reduced for answering hard time-sensitive questions, whose time qualifiers are implicit in the document. An intuitive idea is to match temporal events in the given document by treating time-sensitive question as a temporal event of missing objects. However, not all temporal events extracted from the document have explicit time qualifiers. In this paper, we propose an Event-AL framework, in which a graph pruning model is designed to locate the timespan of implicit temporal events by capturing temporal relation between events. Moreover, we present an abductive reasoning module to determine proper objects while providing explanations. Besides, as the same relation may be scattered throughout the document in diverse expressions, a relation-based prompt is introduced to instructs LLMs in extracting candidate temporal events. We conduct extensive experiment and results show that Event-AL outperforms strong baselines for hard time-sensitive questions, with a 12.7% improvement in EM scores. In addition, it also exhibits great superiority for multi-answer and beyond hard time-sensitive questions",
    "checked": true,
    "id": "a31ffd9494c741b52335517d7277fb6a037c5ccd",
    "semantic_title": "an event-based abductive learning for hard time-sensitive question answering",
    "citation_count": 0,
    "authors": [
      "Shaojuan Wu",
      "Jitong Li",
      "Xiaowang Zhang",
      "Zhiyong Feng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.100": {
    "title": "A New Massive Multilingual Dataset for High-Performance Language Technologies",
    "volume": "main",
    "abstract": "We present the HPLT (High Performance Language Technologies) language resources, a new massive multilingual dataset including both monolingual and bilingual corpora extracted from CommonCrawl and previously unused web crawls from the Internet Archive. We describe our methods for data acquisition, management and processing of large corpora, which rely on open-source software tools and high-performance computing. Our monolingual collection focuses on low- to medium-resourced languages and covers 75 languages and a total of ≈ 5.6 trillion word tokens de-duplicated on the document level. Our English-centric parallel corpus is derived from its monolingual counterpart and covers 18 language pairs and more than 96 million aligned sentence pairs with roughly 1.4 billion English tokens. The HPLT language resources are one of the largest open text corpora ever released, providing a great resource for language modeling and machine translation training. We publicly release the corpora, the software, and the tools used in this work",
    "checked": true,
    "id": "428600c0586bd017199690277a1ba1ce8ffe351b",
    "semantic_title": "a new massive multilingual dataset for high-performance language technologies",
    "citation_count": 1,
    "authors": [
      "Ona de Gibert",
      "Graeme Nail",
      "Nikolay Arefyev",
      "Marta Bañón",
      "Jelmer van der Linde",
      "Shaoxiong Ji",
      "Jaume Zaragoza-Bernabeu",
      "Mikko Aulamo",
      "Gema Ramírez-Sánchez",
      "Andrey Kutuzov",
      "Sampo Pyysalo",
      "Stephan Oepen",
      "Jörg Tiedemann"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.101": {
    "title": "An LCF-IDF Document Representation Model Applied to Long Document Classification",
    "volume": "main",
    "abstract": "A document representation model that has been used for years in NLP and Text Mining tasks is TF-IDF (Term Frequency-Inverse Document Frequency). This model is indeed effective for various tasks like Information Retrieval and Document Classification. However, it may fall short when it comes to capturing the deeper semantic and contextual meaning of a text, which is where Transformer-based Pre-trained Language Models (PLMs) such as BERT have been gaining significant traction in recent years. Despite this, these models also face specific challenges related to Transformers and their attention mechanism limits, especially when dealing with long documents. Therefore, this paper proposes a novel approach to exploit the advantages of the TF-IDF representation while incorporating semantic context, by introducing a Latent Concept Frequency-Inverse Document Frequency (LCF-IDF) document representation model. Its effectiveness is tested with respect to the Long Document Classification task. The results obtained show promising performance of the proposed solution compared to TF-IDF and BERT-like representation models, including those specifically for long documents such as Longformer as well as those designed for particular domains, especially when it comes to Single Label Multi-Class (SLMC) classification",
    "checked": true,
    "id": "964651c8ee92885890e9c18fa26b9bddeeebfbb2",
    "semantic_title": "an lcf-idf document representation model applied to long document classification",
    "citation_count": 0,
    "authors": [
      "Renzo Arturo Alva Principe",
      "Nicola Chiarini",
      "Marco Viviani"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.102": {
    "title": "An LLM-Enhanced Adversarial Editing System for Lexical Simplification",
    "volume": "main",
    "abstract": "Lexical Simplification (LS) aims to simplify text at the lexical level. Existing methods rely heavily on annotated data, making it challenging to apply in low-resource scenarios. In this paper, we propose a novel LS method without parallel corpora. This method employs an Adversarial Editing System with guidance from a confusion loss and an invariance loss to predict lexical edits in the original sentences. Meanwhile, we introduce an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system. From that, complex words within sentences are masked and a Difficulty-aware Filling module is crafted to replace masked positions with simpler words. At last, extensive experimental results and analyses on three benchmark LS datasets demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "d24adcc7393b657d2542e2311cb680639a9cca66",
    "semantic_title": "an llm-enhanced adversarial editing system for lexical simplification",
    "citation_count": 0,
    "authors": [
      "Keren Tan",
      "Kangyang Luo",
      "Yunshi Lan",
      "Zheng Yuan",
      "Jinlong Shu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.103": {
    "title": "AnnoCTR: A Dataset for Detecting and Linking Entities, Tactics, and Techniques in Cyber Threat Reports",
    "volume": "main",
    "abstract": "Monitoring the threat landscape to be aware of actual or potential attacks is of utmost importance to cybersecurity professionals. Information about cyber threats is typically distributed using natural language reports. Natural language processing can help with managing this large amount of unstructured information, yet to date, the topic has received little attention. With this paper, we present AnnoCTR, a new CC-BY-SA-licensed dataset of cyber threat reports. The reports have been annotated by a domain expert with named entities, temporal expressions, and cybersecurity-specific concepts including implicitly mentioned techniques and tactics. Entities and concepts are linked to Wikipedia and the MITRE ATT&CK knowledge base, the most widely-used taxonomy for classifying types of attacks. Prior datasets linking to MITRE ATT&CK either provide a single label per document or annotate sentences out-of-context; our dataset annotates entire documents in a much finer-grained way. In an experimental study, we model the annotations of our dataset using state-of-the-art neural models. In our few-shot scenario, we find that for identifying the MITRE ATT&CK concepts that are mentioned explicitly or implicitly in a text, concept descriptions from MITRE ATT&CK are an effective source for training data augmentation",
    "checked": true,
    "id": "9b9a614d94bf3a198f0971a63264cf5de0543f53",
    "semantic_title": "annoctr: a dataset for detecting and linking entities, tactics, and techniques in cyber threat reports",
    "citation_count": 0,
    "authors": [
      "Lukas Lange",
      "Marc Müller",
      "Ghazaleh Haratinezhad Torbati",
      "Dragan Milchevski",
      "Patrick Grau",
      "Subhash Chandra Pujari",
      "Annemarie Friedrich"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.104": {
    "title": "Annotate Chinese Aspect with UMR——a Case Study on the Liitle Prince",
    "volume": "main",
    "abstract": "Aspect is a valuable tool for determining the perspective from which an event is observed, allowing for viewing both at the situation and viewpoint level. Uniform Meaning Representation (UMR) seeks to provide a standard, typologically-informed representation of aspects across languages. It employs an aspectual lattice to adapt to different languages and design values that encompass both viewpoint aspect and situation aspects. In the context of annotating the Chinese version of The Little Prince, we paid particular attention to the interactions between aspect values and aspect markers and we also want to know the annotation effectiveness and challenges under the UMR aspectual lattice. During our annotation process, we identified the relationships between aspectual markers and labels. We further categorized and analyzed complex examples that led to low inter-annotator agreement. The factors contributing to disagreement among annotators included the interpretations of lexical semantics, implications, and the influence of aspectual markers, which is related to the inclination of the situation aspect and the exclusivity between the two aspects' perspectives. Overall, our work sheds light on the challenges of aspect annotation in Chinese and highlights the need for more comprehensive guidelines",
    "checked": true,
    "id": "eeb2de371a2d49114a110028fbfb148b54889704",
    "semantic_title": "annotate chinese aspect with umr——a case study on the liitle prince",
    "citation_count": 0,
    "authors": [
      "Sijia Ge",
      "Zilong Li",
      "Alvin Po-Chun Chen",
      "Guanchao Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.105": {
    "title": "Annotate the Way You Think: An Incremental Note Generation Framework for the Summarization of Medical Conversations",
    "volume": "main",
    "abstract": "The scarcity of public datasets for the summarization of medical conversations has been a limiting factor for advancing NLP research in the healthcare domain, and the structure of the existing data is largely limited to the simple format of conversation-summary pairs. We therefore propose a novel Incremental Note Generation (ING) annotation framework capable of greatly enriching summarization datasets in the healthcare domain and beyond. Our framework is designed to capture the human summarization process via an annotation task by instructing the annotators to first incrementally create a draft note as they accumulate information through a conversation transcript (Generation) and then polish the draft note into a reference note (Rewriting). The annotation results include both the reference note and a comprehensive editing history of the draft note in tabular format. Our pilot study on the task of SOAP note generation showed reasonable consistency between four expert annotators, established a solid baseline for quantitative targets of inter-rater agreement, and demonstrated the ING framework as an improvement over the traditional annotation process for future modeling of summarization",
    "checked": true,
    "id": "d9273bba13948bf2a021dfb62ad67103713e682f",
    "semantic_title": "annotate the way you think: an incremental note generation framework for the summarization of medical conversations",
    "citation_count": 0,
    "authors": [
      "Longxiang Zhang",
      "Caleb D. Hart",
      "Susanne Burger",
      "Thomas Schaaf"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.106": {
    "title": "Annotating Chinese Word Senses with English WordNet: A Practice on OntoNotes Chinese Sense Inventories",
    "volume": "main",
    "abstract": "In this paper, we present our exploration of annotating Chinese word senses using English WordNet synsets, with examples extracted from OntoNotes Chinese sense inventories. Given a target word along with the example that contains it, the annotators select a WordNet synset that best describes the meaning of the target word in the context. The result demonstrates an inter-annotator agreement of 38% between two annotators. We delve into the instances of disagreement by comparing the two annotated synsets, including their positions within the WordNet hierarchy. The examination reveals intriguing patterns among closely related synsets, shedding light on similar concepts represented within the WordNet structure. The data offers as an indirect linking of Chinese word senses defined in OntoNotes Chinese sense inventories to WordNet sysnets, and thus promotes the value of the OntoNotes corpus. Compared to a direct linking of Chinese word senses to WordNet synsets, the example-based annotation has the merit of not being affected by inaccurate sense definitions and thus offers a new way of mapping WordNets of different languages. At the same time, the annotated data also serves as a valuable linguistic resource for exploring potential lexical differences between English and Chinese, with potential contributions to the broader understanding of cross-linguistic semantic mapping",
    "checked": true,
    "id": "7edf7645b70a3f516a7ad9986fd64379bc453d3b",
    "semantic_title": "annotating chinese word senses with english wordnet: a practice on ontonotes chinese sense inventories",
    "citation_count": 0,
    "authors": [
      "Hongzhi Xu",
      "Jingxia Lin",
      "Sameer Pradhan",
      "Mitchell Marcus",
      "Ming Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.107": {
    "title": "Annotating Customer-Oriented Behaviour in Call Centre Sales Dialogues",
    "volume": "main",
    "abstract": "Customer-oriented behaviour (COB) plays an important role in call centre interactions, particularly in the context of successful sales negotiation. However, the evaluation of COB in customer-agent conversations often lacks clarity in its definition and robust computational assessment methods. This paper addresses these challenges by presenting a comprehensive conceptual and empirical framework. We conducted multidimensional dialogue act annotations on authentic call centre interactions using the ISO 24617-2 taxonomy, capturing the multifaceted nature of these interactions. This process led to the identification of relevant dialogue act categories, proposed extensions concerning relationship-building aspects, and derived corpus statistics. The findings highlight specific facets of COB that positively impact on Customer Satisfaction (CS), as determined through correlation analysis. Additionally, we delved into the dependencies between COB and feedback acts, leveraging the hierarchical structure of the DIT++ model. This framework improves our understanding of the dynamics shaping sales strategies in call centres and holds promise for practical applications in optimising customer-agent interactions",
    "checked": true,
    "id": "c9a11c6934232d68b657935c46efc0d9aee5e65c",
    "semantic_title": "annotating customer-oriented behaviour in call centre sales dialogues",
    "citation_count": 0,
    "authors": [
      "Jutta Stock",
      "Volha Petukhova",
      "Dietrich Klakow"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.108": {
    "title": "Annotation and Classification of Relevant Clauses in Terms-and-Conditions Contracts",
    "volume": "main",
    "abstract": "In this paper, we propose a new annotation scheme to classify different types of clauses in Terms-and-Conditions contracts with the ultimate goal of supporting legal experts to quickly identify and assess problematic issues in this type of legal documents. To this end, we built a small corpus of Terms-and-Conditions contracts and finalized an annotation scheme of 14 categories, eventually reaching an inter-annotator agreement of 0.92. Then, for 11 of them, we experimented with binary classification tasks using few-shot prompting with a multilingual T5 and two fine-tuned versions of two BERT-based LLMs for Italian. Our experiments showed the feasibility of automatic classification of our categories by reaching accuracies ranging from .79 to .95 on validation tasks",
    "checked": true,
    "id": "eac3b79509eb158d4675553239751b5295b94987",
    "semantic_title": "annotation and classification of relevant clauses in terms-and-conditions contracts",
    "citation_count": 0,
    "authors": [
      "Pietro Giovanni Bizzaro",
      "Elena Della Valentina",
      "Maurizio Napolitano",
      "Nadia Mana",
      "Massimo Zancanaro"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.109": {
    "title": "Annotation of Japanese Discourse Relations Focusing on Concessive Inferences",
    "volume": "main",
    "abstract": "In this study, we focus on the inference presupposed in the concessive discourse relation and present the discourse relation annotation for the Japanese connectives ‘nagara' and ‘tsutsu', both of which have two usages: Synchronous and Concession, just like English while. We also present the annotation for ‘tokorode', which is ambiguous in three ways: Temporal, Location, and Concession. While corpora containing concessive discourse relations already exist, the distinctive feature of our study is that it aims to identify the concessive inferential relations by writing out the implicit presupposed inferences. In this paper, we report on the annotation methodology and its results, as well as the characteristics of concession that became apparent during annotation",
    "checked": true,
    "id": "2ce72a8910b68e03ce63d185564666520a3c82bb",
    "semantic_title": "annotation of japanese discourse relations focusing on concessive inferences",
    "citation_count": 0,
    "authors": [
      "Ai Kubota",
      "Takuma Sato",
      "Takayuki Amamoto",
      "Ryota Akiyoshi",
      "Koji Mineshima"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.110": {
    "title": "Annotation of Transition-Relevance Places and Interruptions for the Description of Turn-Taking in Conversations in French Media Content",
    "volume": "main",
    "abstract": "Few speech resources describe interruption phenomena, especially for TV and media content. The description of these phenomena may vary across authors: it thus leaves room for improved annotation protocols. We present an annotation of Transition-Relevance Places (TRP) and Floor-Taking event types on an existing French TV and Radio broadcast corpus to facilitate studies of interruptions and turn-taking. Each speaker change is annotated with the presence or absence of a TRP, and a classification of the next-speaker floor-taking as Smooth, Backchannel or different types of turn violations (cooperative or competitive, successful or attempted interruption). An inter-rater agreement analysis shows such annotations' moderate to substantial reliability. The inter-annotator agreement for TRP annotation reaches κ=0.75, κ=0.56 for Backchannel and κ=0.5 for the Interruption/non-interruption distinction. More precise differences linked to cooperative or competitive behaviors lead to lower agreements. These results underline the importance of low-level features like TRP to derive a classification of turn changes that would be less subject to interpretation. The analysis of the presence of overlapping speech highlights the existence of interruptions without overlaps and smooth transitions with overlaps. These annotations are available at https://lium.univ-lemans.fr/corpus-allies/",
    "checked": true,
    "id": "99fe25c4a4a0ae81398424cd8016d3a928d062bf",
    "semantic_title": "annotation of transition-relevance places and interruptions for the description of turn-taking in conversations in french media content",
    "citation_count": 0,
    "authors": [
      "Rémi Uro",
      "Marie Tahon",
      "Jane Wottawa",
      "David Doukhan",
      "Albert Rilliard",
      "Antoine Laurent"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.111": {
    "title": "Annotations for Exploring Food Tweets from Multiple Aspects",
    "volume": "main",
    "abstract": "This research builds upon the Latvian Twitter Eater Corpus (LTEC), which is focused on the narrow domain of tweets related to food, drinks, eating and drinking. LTEC has been collected for more than 12 years and reaching almost 3 million tweets with the basic information as well as extended automatically and manually annotated metadata. In this paper we supplement the LTEC with manually annotated subsets of evaluation data for machine translation, named entity recognition, timeline-balanced sentiment analysis, and text-image relation classification. We experiment with each of the data sets using baseline models and highlight future challenges for various modelling approaches",
    "checked": true,
    "id": "94e2709e768741d25857ecf25aaa7b8edd60a87b",
    "semantic_title": "annotations for exploring food tweets from multiple aspects",
    "citation_count": 0,
    "authors": [
      "Matiss Rikters",
      "Rinalds Vīksna",
      "Edison Marrese-Taylor"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.112": {
    "title": "Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost",
    "volume": "main",
    "abstract": "Current foundation models have shown impressive performance across various tasks. However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process. Most of this data comes from Western countries, leading to poor results for underrepresented countries. To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck. In this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs. Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models. Next, we identify countries with higher visual similarity for these topics and show that using data from these countries to supplement the training data improves model performance and reduces annotation costs. The resulting lists of countries and corresponding topics are made available at https://github.com/MichiganNLP/visual_diversity_budget",
    "checked": true,
    "id": "b27cee2aaa1e35f71faaefd8f4cba130c8400324",
    "semantic_title": "annotations on a budget: leveraging geo-data similarity to balance model performance and annotation cost",
    "citation_count": 0,
    "authors": [
      "Oana Ignat",
      "Longju Bai",
      "Joan C. Nwatu",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.113": {
    "title": "AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies",
    "volume": "main",
    "abstract": "More than 7,000 known languages are spoken around the world. However, due to the lack of annotated resources, only a small fraction of them are currently covered by speech technologies. Albeit self-supervised speech representations, recent massive speech corpora collections, as well as the organization of challenges, have alleviated this inequality, most studies are mainly benchmarked on English. This situation is aggravated when tasks involving both acoustic and visual speech modalities are addressed. In order to promote research on low-resource languages for audio-visual speech technologies, we present AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription. In addition, to show the complete process of preparing AnnoTheia for a language of interest, we also describe the adaptation of a pre-trained model for active speaker detection to Spanish, using a database not initially conceived for this type of task. Prior evaluations show that the toolkit is able to speed up to four times the annotation process. The AnnoTheia toolkit, tutorials, and pre-trained models are available at https://github.com/joactr/AnnoTheia/",
    "checked": true,
    "id": "9d0aafc36def8dc080344be3d66f2ff390d0b050",
    "semantic_title": "annotheia: a semi-automatic annotation toolkit for audio-visual speech technologies",
    "citation_count": 0,
    "authors": [
      "José-M. Acosta-Triana",
      "David Gimeno-Gómez",
      "Carlos-D. Martínez-Hinarejos"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.114": {
    "title": "Announcing the Prague Discourse Treebank 3.0",
    "volume": "main",
    "abstract": "We present the Prague Discourse Treebank 3.0 – a new version of the annotation of discourse relations marked by primary and secondary discourse connectives in the data of the Prague Dependency Treebank. Compared to the previous version (PDiT 2.0), the version 3.0 comes with three types of major updates: (i) it brings a largely revised annotation of discourse relations: pragmatic relations have been thoroughly reworked, many inconsistencies across all discourse types have been fixed and previously unclear cases marked in annotators' comments have been resolved, (ii) it achieves consistency with a Lexicon of Czech Discourse Connectives (CzeDLex), and (iii) it provides the data not only in its native format (Prague Markup Language, discourse relations annotated at the top of the dependency trees), but also in the Penn Discourse Treebank 3.0 format (plain text plus a stand-off discourse annotation) and sense taxonomy. PDiT 3.0 contains 21,662 discourse relations (plus 445 list relations) in 49 thousand sentences",
    "checked": true,
    "id": "28818b436344eee4fb9e849af96997809dbb51cc",
    "semantic_title": "announcing the prague discourse treebank 3.0",
    "citation_count": 0,
    "authors": [
      "Pavlína Synková",
      "Jiří Mírovský",
      "Lucie Poláková",
      "Magdaléna Rysová"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.115": {
    "title": "A Novel Corpus of Annotated Medical Imaging Reports and Information Extraction Results Using BERT-based Language Models",
    "volume": "main",
    "abstract": "Medical imaging is critical to the diagnosis, surveillance, and treatment of many health conditions, including oncological, neurological, cardiovascular, and musculoskeletal disorders, among others. Radiologists interpret these complex, unstructured images and articulate their assessments through narrative reports that remain largely unstructured. This unstructured narrative must be converted into a structured semantic representation to facilitate secondary applications such as retrospective analyses or clinical decision support. Here, we introduce the Corpus of Annotated Medical Imaging Reports (CAMIR), which includes 609 annotated radiology reports from three imaging modality types: Computed Tomography, Magnetic Resonance Imaging, and Positron Emission Tomography-Computed Tomography. Reports were annotated using an event-based schema that captures clinical indications, lesions, and medical problems. Each event consists of a trigger and multiple arguments, and a majority of the argument types, including anatomy, normalize the spans to pre-defined concepts to facilitate secondary use. CAMIR uniquely combines a granular event structure and concept normalization. To extract CAMIR events, we explored two BERT (Bi-directional Encoder Representation from Transformers)-based architectures, including an existing architecture (mSpERT) that jointly extracts all event information and a multi-step approach (PL-Marker++) that we augmented for the CAMIR schema",
    "checked": true,
    "id": "aa04f53a53b804eaba1cbe761a73bbf1a13f945c",
    "semantic_title": "a novel corpus of annotated medical imaging reports and information extraction results using bert-based language models",
    "citation_count": 0,
    "authors": [
      "Namu Park",
      "Kevin Lybarger",
      "Giridhar Kaushik Ramachandran",
      "Spencer Lewis",
      "Aashka Damani",
      "Özlem Uzuner",
      "Martin Gunn",
      "Meliha Yetisgen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.116": {
    "title": "A Novel Three-stage Framework for Few-shot Named Entity Recognition",
    "volume": "main",
    "abstract": "Different from most existing tasks relying on abundant labeled data, Few-shot Named Entity Recognition (NER) aims to develop NER systems that are capable of learning from a small set of labeled samples and then generalizing well to new, unseen data.In this paper, with the intention of obtaining a model that can better adapt to new domains, we design a novel three-stage framework for Few-shot NER, including teacher span recognizer, student span recognizer and entity classifier.We first train a teacher span recognizer which is based on a global boundary matrix to obtain soft boundary labels.Then we leverage the soft boundary labels learned by the teacher model to assist in training the student span recognizer,which can smooth the training process of span recognizer.Finally, we adopt the traditional prototypical network as entity classifier and incorporate the idea of prompt learning to construct a more generalizable semantic space.Extensive experiments on various benchmarks demonstrate that our approach surpasses prior methods",
    "checked": true,
    "id": "179612909d967c309427056e1c965f8cf9279d51",
    "semantic_title": "a novel three-stage framework for few-shot named entity recognition",
    "citation_count": 0,
    "authors": [
      "Shengjie Ji",
      "Fang Kong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.117": {
    "title": "AntCritic: Argument Mining for Free-Form and Visually-Rich Financial Comments",
    "volume": "main",
    "abstract": "Argument mining aims to detect all possible argumentative components and identify their relationships automatically. As a thriving task in natural language processing, there has been a large amount of corpus for academic study and application development in this field. However, the research in this area is still constrained by the inherent limitations of existing datasets. Specifically, all the publicly available datasets are relatively small in scale, and few of them provide information from other modalities to facilitate the learning process. Moreover, the statements and expressions in these corpora are usually in a compact form, which restricts the generalization ability of models. To this end, we collect a novel dataset AntCritic to serve as a helpful complement to this area, which consists of about 10k free-form and visually-rich financial comments and supports both argument component detection and argument relation prediction tasks. Besides, to cope with the challenges brought by scenario expansion, we thoroughly explore the fine-grained relation prediction and structure reconstruction scheme and discuss the encoding mechanism for visual styles and layouts. On this basis, we design two simple but effective model architectures and conduct various experiments on this dataset to provide benchmark performances as a reference and verify the practicability of our proposed architecture. We release our data and code in this link, and this dataset follows CC BY-NC-ND 4.0 license",
    "checked": true,
    "id": "0659469897f46946249a730e44ccbf50f55ad6b1",
    "semantic_title": "antcritic: argument mining for free-form and visually-rich financial comments",
    "citation_count": 0,
    "authors": [
      "Huadai Liu",
      "Xu Wenqiang",
      "Xuan Lin",
      "Jingjing Huo",
      "Hong Chen",
      "Zhou Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.118": {
    "title": "An Unsupervised Framework for Adaptive Context-aware Simplified-Traditional Chinese Character Conversion",
    "volume": "main",
    "abstract": "Traditional Chinese character is an important carrier of Chinese culture, and is still actively used in many areas. Automatic conversion between traditional and simplified Chinese characters can help modern people understand traditional culture and facilitate communication among different regions. Previous conversion methods rely on rule-based mapping or shallow feature-based machine learning models, which struggle to convert simplified characters with different origins and constructing training data is costly. In this study, we propose an unsupervised adaptive context-aware conversion model that learns to convert between simplified and traditional Chinese characters under a denoising auto-encoder framework requiring no labeled data. Our model includes a Latent Generative Adversarial Encoder that transforms vectors to a latent space with generative adversarial network, which adds noise as an inevitable side effect, Based on which a Context-aware Semantic Reconstruction Decoder restores the original input while considering a broader range of context with a pretrained language model. Additionally, we propose to apply early exit mechanism during inference to reduce the computation complexity and improve the generalization ability. To test the effectiveness of our model, we construct a high quality test dataset with simplified-traditional Chinese character text pairs. Experiment results and extensive analysis demonstrate that our model outperforms strong unsupervised baselines and yields better conversion result for one-to-many cases",
    "checked": true,
    "id": "a5687d963e5c8b51079ccab42f407b71f69ce99b",
    "semantic_title": "an unsupervised framework for adaptive context-aware simplified-traditional chinese character conversion",
    "citation_count": 0,
    "authors": [
      "Wei Li",
      "Shutan Huang",
      "Yanqiu Shao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.119": {
    "title": "An Untold Story of Preprocessing Task Evaluation: An Alignment-based Joint Evaluation Approach",
    "volume": "main",
    "abstract": "A preprocessing task such as tokenization and sentence boundary detection (SBD) has commonly been considered as NLP challenges that have already been solved. This perception is due to their generally good performance and the presence of pre-tokenized data. However, it's important to note that the low error rates of current methods are mainly specific to certain tasks, and rule-based tokenization can be difficult to use across different systems. Despite being subtle, these limitations are significant in the context of the NLP pipeline. In this paper, we introduce a novel evaluation algorithm for the preprocessing task, including both tokenization and SBD results. This algorithm aims to enhance the reliability of evaluations by reevaluating the counts of true positive cases for F1 measures in both preprocessing tasks jointly. It achieves this through an alignment-based approach inspired by sentence and word alignments used in machine translation. Our evaluation algorithm not only allows for precise counting of true positive tokens and sentence boundaries but also combines these two evaluation tasks into a single organized pipeline. To illustrate and clarify the intricacies of this calculation and integration, we provide detailed pseudo-code configurations for implementation. Additionally, we offer empirical evidence demonstrating how sentence and word alignment can improve evaluation reliability and present case studies to further support our approach",
    "checked": true,
    "id": "18f4d2e82d1a9c018b390e87ce4a7b9af4bc5977",
    "semantic_title": "an untold story of preprocessing task evaluation: an alignment-based joint evaluation approach",
    "citation_count": 0,
    "authors": [
      "Eunkyul Leah Jo",
      "Angela Yoonseo Park",
      "Grace Tianjiao Zhang",
      "Izia Xiaoxiao Wang",
      "Junrui Wang",
      "MingJia Mao",
      "Jungyeul Park"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.120": {
    "title": "A Paradigm Shift: The Future of Machine Translation Lies with Large Language Models",
    "volume": "main",
    "abstract": "Machine Translation (MT) has greatly advanced over the years due to the developments in deep neural networks. However, the emergence of Large Language Models (LLMs) like GPT-4 and ChatGPT is introducing a new phase in the MT domain. In this context, we believe that the future of MT is intricately tied to the capabilities of LLMs. These models not only offer vast linguistic understandings but also bring innovative methodologies, such as prompt-based techniques, that have the potential to further elevate MT. In this paper, we provide an overview of the significant enhancements in MT that are influenced by LLMs and advocate for their pivotal role in upcoming MT research and implementations. We highlight several new MT directions, emphasizing the benefits of LLMs in scenarios such as Long-Document Translation, Stylized Translation, and Interactive Translation. Additionally, we address the important concern of privacy in LLM-driven MT and suggest essential privacy-preserving strategies. By showcasing practical instances, we aim to demonstrate the advantages that LLMs offer, particularly in tasks like translating extended documents. We conclude by emphasizing the critical role of LLMs in guiding the future evolution of MT and offer a roadmap for future exploration in the sector",
    "checked": true,
    "id": "ffabde35e2437db93871801e7d733f411911813f",
    "semantic_title": "a paradigm shift: the future of machine translation lies with large language models",
    "citation_count": 17,
    "authors": [
      "Chenyang Lyu",
      "Zefeng Du",
      "Jitao Xu",
      "Yitao Duan",
      "Minghao Wu",
      "Teresa Lynn",
      "Alham Fikri Aji",
      "Derek F. Wong",
      "Longyue Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.121": {
    "title": "A Persona-Based Corpus in the Diabetes Self-Care Domain - Applying a Human-Centered Approach to a Low-Resource Context",
    "volume": "main",
    "abstract": "While Natural Language Processing (NLP) models have gained substantial attention, only in recent years has research opened new paths for tackling Human-Computer Design (HCD) from the perspective of natural language. We focus on developing a human-centered corpus, more specifically, a persona-based corpus in a particular healthcare domain (diabetes mellitus self-care). In order to follow an HCD approach, we created personas to model interpersonal interaction (expert and non-expert users) in that specific domain. We show that an HCD approach benefits language generation from different perspectives, from machines to humans - contributing with new directions for low-resource contexts (languages other than English and sensitive domains) where the need to promote effective communication is essential",
    "checked": true,
    "id": "2ba4b555467980ce070dadbf14d159f6b42de988",
    "semantic_title": "a persona-based corpus in the diabetes self-care domain - applying a human-centered approach to a low-resource context",
    "citation_count": 0,
    "authors": [
      "Rossana Cunha",
      "Thiago Castro Ferreira",
      "Adriana Pagano",
      "Fabio Alves"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.122": {
    "title": "APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning",
    "volume": "main",
    "abstract": "Long-form numerical reasoning aims to generate a reasoning program to calculate the answer for a given question. Previous work followed a retriever-generator framework, where the retriever selects key facts from a long-form document, and the generator generates a reasoning program based on the retrieved facts. However, they treated all facts equally without considering the different contributions of facts with and without numerical information. Furthermore, they ignored program consistency, leading to the wrong punishment of programs that differed from the ground truth. In order to address these issues, we proposed APOLLO (An optimized training aPproach fOr Long-form numericaL reasOning), to improve long-form numerical reasoning. APOLLO includes a number-aware negative sampling strategy for the retriever to discriminate key numerical facts, and a consistency-based reinforcement learning with target program augmentation for the generator to ultimately increase the execution accuracy. Experimental results on the FinQA and ConvFinQA leaderboards verify the effectiveness of our proposed methods, achieving the new state-of-the-art",
    "checked": true,
    "id": "efa92d27065501981f2ade15c1cd884fdf644f44",
    "semantic_title": "apollo: an optimized training approach for long-form numerical reasoning",
    "citation_count": 5,
    "authors": [
      "Jiashuo Sun",
      "Hang Zhang",
      "Chen Lin",
      "Xiangdong Su",
      "Yeyun Gong",
      "Jian Guo"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.123": {
    "title": "Applying Transfer Learning to German Metaphor Prediction",
    "volume": "main",
    "abstract": "This paper presents results in transfer-learning metaphor recognition in German. Starting from an English language corpus annotated for metaphor at the sentence level, and its machine-translation to German, we annotate 1000 sentences of the German part to use it as a Gold standard for two different metaphor prediction setups: i) a sequence labeling set-up (on the token-level), and ii) a classification (based on sentences) setup. We test two transfer leaning approaches: i) a group of transformer models, and ii) a technique that utilizes bilingual embeddings together with an RNN classifier. We find out that the transformer models do moderately in a zero-shot scenario (up to 61% F1 for classification) and the embeddings approaches do not even beat the guessing baseline (36% F1 for classification). We use our Gold data to fine-tune the classification tasks on target-language data achieving up to 90% F1 with both, the multilingual BERT and the bilingual embeddings. We also publish the annotated bilingual corpus",
    "checked": true,
    "id": "85fcdead5be1184f4408892db385a3548e07b23b",
    "semantic_title": "applying transfer learning to german metaphor prediction",
    "citation_count": 0,
    "authors": [
      "Maria Berger",
      "Sebastian Michael Reimann",
      "Nieke Marie Kiwitt"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.124": {
    "title": "Appraisal Framework for Clinical Empathy: A Novel Application to Breaking Bad News Conversations",
    "volume": "main",
    "abstract": "Empathy is essential in healthcare communication. We introduce an annotation approach that draws on well-established frameworks for clinical empathy and breaking bad news (BBN) conversations for considering the interactive dynamics of discourse relations. We construct Empathy in BBNs, a span-relation task dataset of simulated BBN conversations in German, using our annotation scheme, in collaboration with a large medical school to support research on educational tools for medical didactics. The annotation is based on 1) Pounds (2011)'s appraisal framework for clinical empathy, which is grounded in systemic functional linguistics, and 2) the SPIKES protocol for breaking bad news (Baile et al., 2000), commonly taught in medical didactics training. This approach presents novel opportunities to study clinical empathic behavior and enables the training of models to detect causal relations involving empathy, a highly desirable feature of systems that can provide feedback to medical professionals in training. We present illustrative examples, discuss applications of the annotation scheme, and insights we can draw from the framework",
    "checked": true,
    "id": "47f9ec4dbbbd12d280fcf219a815e3c2cf10219d",
    "semantic_title": "appraisal framework for clinical empathy: a novel application to breaking bad news conversations",
    "citation_count": 0,
    "authors": [
      "Allison Claire Lahnala",
      "Béla Neuendorf",
      "Alexander Thomin",
      "Charles Welch",
      "Tina Stibane",
      "Lucie Flek"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.125": {
    "title": "Approaches and Challenges for Resolving Different Representations of Fictional Characters for Chinese Novels",
    "volume": "main",
    "abstract": "Due to the huge scale of literary works, automatic text analysis technologies are urgently needed for literary studies such as Digital Humanities. However, the domain-generality of existing NLP technologies limits their effectiveness on in-depth literary studies. It is valuable to explore how to adapt NLP technologies to the literary-specific tasks. Fictional characters are the most essential elements of a novel, and thus crucial to understanding the content of novels. The prerequisite of collecting a character's information is to resolve its different representations. It is a specific problem of anaphora resolution which is a classical and open-domain NLP task. We adapt a state-of-the-art anaphora resolution model to resolve character representations in Chinese novels by making some modifications, and train a widely used BERT fine-tuned model for speaker extraction as assistance. We also analyze the challenges and potential solutions for character-resolution in Chinese novels according to the resolution results on a specific Chinese novel",
    "checked": true,
    "id": "0a85d44c13f8e8904bd9476ec53ac22f526204cb",
    "semantic_title": "approaches and challenges for resolving different representations of fictional characters for chinese novels",
    "citation_count": 0,
    "authors": [
      "Li Song",
      "Ying Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.126": {
    "title": "A Preliminary Study of ChatGPT for Spanish E2R Text Adaptation",
    "volume": "main",
    "abstract": "The process of adapting and creating Easy-to-Read (E2R) texts is very expensive and time-consuming. Due to the success of Large Language Models (LLMs) such as ChatGPT and their ability to generate written language, it is likely to think that such models can help in the adaptation or creation of text in E2R. In this paper, we explore the concept of E2R, its underlying principles and applications, and provides a preliminary study on the usefulness of ChatGPT-4 for E2R text adaptation. We focus on the Spanish language and its E2R variant, Lectura Fácil (LF). We consider a range of prompts that can be used and the differences in output that this produces. We then carry out a three-folded evaluation on 10 texts adapted by ChatGPT-4: (1) an automated evaluation to check values related to the readability of texts, (2) a checklist-based manual evaluation (for which we also propose three new capabilities) and (3) a users' evaluation with people with cognitive disabilities. We show that it is difficult to choose the best prompt to make ChatGPT-4 adapt texts to LF. Furthermore, the generated output does not follow the E2R text rules, so it is often not suitable for the target audience",
    "checked": true,
    "id": "a09f38845fd5b6a378c1c5cbc9b848144c63ce13",
    "semantic_title": "a preliminary study of chatgpt for spanish e2r text adaptation",
    "citation_count": 0,
    "authors": [
      "Margot Madina",
      "Itziar Gonzalez-Dios",
      "Melanie Siegel"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.127": {
    "title": "A Quantum-Inspired Matching Network with Linguistic Theories for Metaphor Detection",
    "volume": "main",
    "abstract": "Enabling machines with the capability to recognize and comprehend metaphors is a crucial step toward achieving artificial intelligence. In linguistic theories, metaphor can be identified through Metaphor Identification Procedure (MIP) or Selectional Preference Violation (SPV), both of which are typically considered as matching tasks in the field of natural language processing. However, the implementation of MIP poses a challenge due to the semantic uncertainty and ambiguity of literal meanings of words. Simultaneously, SPV often struggles to recognize conventional metaphors. Inspired by Quantum Language Model (QLM) for modeling semantic uncertainty and fine-grained feature matching, we propose a quantum-inspired matching network for metaphor detection. Specifically, we use the density matrix to explicitly characterize the literal meanings of the target word for MIP, in order to model the uncertainty and ambiguity of the literal meanings of words. This can make SPV effective even in the face of conventional metaphors. MIP and SPV are then achieved by fine-grained feature matching. The results of the experiment finally demonstrated our approach has strong competitiveness",
    "checked": true,
    "id": "b2cf25512354f69e2f933580a1a75388c3a4fa82",
    "semantic_title": "a quantum-inspired matching network with linguistic theories for metaphor detection",
    "citation_count": 0,
    "authors": [
      "Wenbo Qiao",
      "Peng Zhang",
      "ZengLai Ma"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.128": {
    "title": "Arabic Diacritization Using Morphologically Informed Character-Level Model",
    "volume": "main",
    "abstract": "Arabic diacritic recovery i.e. diacritization is necessary for proper vocalization and an enabler for downstream applications such as language learning and text to speech. Diacritics come in two varieties, namely: core-word diacritics and case endings. In this paper we introduce a highly effective morphologically informed character-level model that can recover both types of diacritics simultaneously. The model uses a Recurrent Neural Network (RNN) based architecture that takes in text as a sequence of characters, with markers for morphological segmentation, and outputs a sequence of diacritics. We also introduce a character-based morphological segmentation model that we train for Modern Standard Arabic (MSA) and dialectal Arabic. We demonstrate the efficacy of our diacritization model on Classical Arabic, MSA, and two dialectal (Moroccan and Tunisian) texts. We achieve the lowest reported word-level diacritization error rate for MSA (3.4%), match the best results for Classical Arabic (5.4%), and report competitive results for dialectal Arabic",
    "checked": true,
    "id": "061d80c21d1e7ceab3becd07a7e14966d191c381",
    "semantic_title": "arabic diacritization using morphologically informed character-level model",
    "citation_count": 0,
    "authors": [
      "Muhammad Morsy Elmallah",
      "Mahmoud Reda",
      "Kareem Darwish",
      "Abdelrahman El-Sheikh",
      "Ashraf Hatim Elneima",
      "Murtadha Aljubran",
      "Nouf Alsaeed",
      "Reem Mohammed",
      "Mohamed Al-Badrashiny"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.129": {
    "title": "Arbitrary Time Information Modeling via Polynomial Approximation for Temporal Knowledge Graph Embedding",
    "volume": "main",
    "abstract": "Distinguished from traditional knowledge graphs (KGs), temporal knowledge graphs (TKGs) must explore and reason over temporally evolving facts adequately. However, existing TKG approaches still face two main challenges, i.e., the limited capability to model arbitrary timestamps continuously and the lack of rich inference patterns under temporal constraints. In this paper, we propose an innovative TKGE method (PTBox) via polynomial decomposition-based temporal representation and box embedding-based entity representation to tackle the above-mentioned problems. Specifically, we decompose time information by polynomials and then enhance the model's capability to represent arbitrary timestamps flexibly by incorporating the learnable temporal basis tensor. In addition, we model every entity as a hyperrectangle box and define each relation as a transformation on the head and tail entity boxes. The entity boxes can capture complex geometric structures and learn robust representations, improving the model's inductive capability for rich inference patterns. Theoretically, our PTBox can encode arbitrary time information or even unseen timestamps while capturing rich inference patterns and higher-arity relations of the knowledge base. Extensive experiments on real-world datasets demonstrate the effectiveness of our method",
    "checked": true,
    "id": "66758d7e55af9b67985d85bfdd7f38bf2b1ec498",
    "semantic_title": "arbitrary time information modeling via polynomial approximation for temporal knowledge graph embedding",
    "citation_count": 0,
    "authors": [
      "Zhiyu Fang",
      "Jingyan Qin",
      "Xiaobin Zhu",
      "Chun Yang",
      "Xu-Cheng Yin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.130": {
    "title": "ARBRES Kenstur: A Breton-French Parallel Corpus Rooted in Field Linguistics",
    "volume": "main",
    "abstract": "ARBRES is an ongoing project of open science implemented as a platform (\"wikigrammar\") documenting both the Breton language itself and the state of research and engineering work in linguistics and NLP. Along its nearly 15 years of operation, it has aggregated a wealth of linguistic data in the form of interlinear glosses with translations illustrating lexical items, grammatical features, dialectal variations... While these glosses were primarily meant for human consumption, their volume and the regular format imposed by the wiki engine used for the website also make them suitable for machine processing. ARBRES Kenstur is a new parallel corpus derived from the glosses in ARBRES, including about 5k phrases and sentences in Breton along with translations in standard French. The nature of the original data — sourced from field linguistic inquiries meant to document the structure of Breton — leads to a resource that is mechanically more concerned with the internal variations of the language and rare phenomena than typical parallel corpora. Preliminaries experiments in using this corpus show that it can help improve machine translation for Breton, demonstrating that sourcing data from field linguistic documentation can be a way to help provide NLP tools for minority and low-resource languages",
    "checked": true,
    "id": "0e9e621174b70a2a47058905dc7af104d411e69f",
    "semantic_title": "arbres kenstur: a breton-french parallel corpus rooted in field linguistics",
    "citation_count": 0,
    "authors": [
      "Loïc Grobol",
      "Mélanie Jouitteau"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.131": {
    "title": "A Regularization-based Transfer Learning Method for Information Extraction via Instructed Graph Decoder",
    "volume": "main",
    "abstract": "Information extraction (IE) aims to extract complex structured information from the text. Numerous datasets have been constructed for various IE tasks, leading to time-consuming and labor-intensive data annotations. Nevertheless, most prevailing methods focus on training task-specific models, while the common knowledge among different IE tasks is not explicitly modeled. Moreover, the same phrase may have inconsistent labels in different tasks, which poses a big challenge for knowledge transfer using a unified model. In this study, we propose a regularization-based transfer learning method for IE (TIE) via an instructed graph decoder. Specifically, we first construct an instruction pool for datasets from all well-known IE tasks, and then present an instructed graph decoder, which decodes various complex structures into a graph uniformly based on corresponding instructions. In this way, the common knowledge shared with existing datasets can be learned and transferred to a new dataset with new labels. Furthermore, to alleviate the label inconsistency problem among various IE tasks, we introduce a task-specific regularization strategy, which does not update the gradients of two tasks with ‘opposite direction'. We conduct extensive experiments on 12 datasets spanning four IE tasks, and the results demonstrate the great advantages of our proposed method",
    "checked": true,
    "id": "e90fff62198da4224cf47b16b6ed62abdd4eb737",
    "semantic_title": "a regularization-based transfer learning method for information extraction via instructed graph decoder",
    "citation_count": 0,
    "authors": [
      "Kedi Chen",
      "Jie Zhou",
      "Qin Chen",
      "Shunyu Liu",
      "Liang He"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.132": {
    "title": "A Reinforcement Learning Approach to Improve Low-Resource Machine Translation Leveraging Domain Monolingual Data",
    "volume": "main",
    "abstract": "Due to the lack of parallel data, the mainstream fine-tuning-based domain adaptation methods have the overfitting problem in the translation of low-resource domains, and it is difficult for the model to learn the in-domain generalization knowledge. To address the above issue, in this work, we propose a novel Reinforcement Learning Domain Adaptation method for Neural Machine Translation (RLDA-NMT) in the low-resource domain. RLDA-NMT utilizes in-domain source monolingual data to make up for the lack of parallel data, and reinforces domain features learning to make the translation model learn the domain-specific knowledge more fully. Specifically, we first train a ranking-based model with a small-scale in-domain parallel corpus, and then adopt it as the reward model to select higher-quality generated translations for reinforcement when fine-tuning pre-trained NMT model using in-domain source monolingual data. We conduct experiments on Education, Laws, Thesis, and Patent domains of Chinese⇔English translation tasks. Experimental results demonstrate that RLDA-NMT can alleviate overfitting and reinforce the NMT model to learn domain-specific knowledge. Additionally, the results also show that RLDA-NMT and back-translation (BT) are nicely complementary to each other, where combining RLDA-NMT with BT can further improve translation quality",
    "checked": true,
    "id": "a26bd9be76bcd0ad31285e2d6cf8fd38a808add2",
    "semantic_title": "a reinforcement learning approach to improve low-resource machine translation leveraging domain monolingual data",
    "citation_count": 0,
    "authors": [
      "Hongxiao Zhang",
      "Mingtong Liu",
      "Chunyou Li",
      "Yufeng Chen",
      "Jinan Xu",
      "Ming Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.133": {
    "title": "Are Large Language Models Good at Lexical Semantics? A Case of Taxonomy Learning",
    "volume": "main",
    "abstract": "Recent studies on LLMs do not pay enough attention to linguistic and lexical semantic tasks, such as taxonomy learning. In this paper, we explore the capacities of Large Language Models featuring LLaMA-2 and Mistral for several Taxonomy-related tasks. We introduce a new methodology and algorithm for data collection via stochastic graph traversal leading to controllable data collection. Collected cases provide the ability to form nearly any type of graph operation. We test the collected dataset for learning taxonomy structure based on English WordNet and compare different input templates for fine-tuning LLMs. Moreover, we apply the fine-tuned models on such datasets on the downstream tasks achieving state-of-the-art results on the TexEval-2 dataset",
    "checked": true,
    "id": "96ec285dd85dd7e4fa9d1764a1e3c825fed1c2f2",
    "semantic_title": "are large language models good at lexical semantics? a case of taxonomy learning",
    "citation_count": 0,
    "authors": [
      "Viktor Moskvoretskii",
      "Alexander Panchenko",
      "Irina Nikishina"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.134": {
    "title": "Are Text Classifiers Xenophobic? A Country-Oriented Bias Detection Method with Least Confounding Variables",
    "volume": "main",
    "abstract": "Classical bias detection methods used in Machine Learning are themselves biased because of the different confounding variables implied in the assessment of the initial biases. First they are using templates that are syntactically simple and distant from the target data on which the model will deployed. Second, current methods are assessing biases in pre-trained language models or in dataset, but not directly on the fine-tuned classifier that can actually produce harms. We propose a simple method to detect the biases of a specific fine-tuned classifier on any type of unlabeled data. The idea is to study the classifier behavior by creating counterfactual examples directly on the target data distribution and quantify the amount of changes. In this work, we focus on named entity perturbations by applying a Named Entity Recognition on target-domain data and modifying them accordingly to most common names or location of a target group (gender and country), and this for several morphosynctactically different languages spoken in relation with the countries of the target groups. We used our method on two models available open-source that are likely to be deployed by industry, and on two tasks and domains. We first assess the bias of a multilingual sentiment analysis model trained over multiple-languages tweets and available open-source, and then a multilingual stance recognition model trained over several languages and assessed over English language. Finally we propose to link the perplexity of each example with the bias of the model, by looking at the change in label distribution with respect to the language of the target group. Our work offers a fine-grained analysis of the interactions between names and languages, revealing significant biases in multilingual models",
    "checked": true,
    "id": "061de7676a0cc0a8abf07362710719e203b3f0b9",
    "semantic_title": "are text classifiers xenophobic? a country-oriented bias detection method with least confounding variables",
    "citation_count": 0,
    "authors": [
      "Valentin Barriere",
      "Sebastian Cifuentes"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.135": {
    "title": "Argument Quality Assessment in the Age of Instruction-Following Large Language Models",
    "volume": "main",
    "abstract": "The computational treatment of arguments on controversial issues has been subject to extensive NLP research, due to its envisioned impact on opinion formation, decision making, writing education, and the like. A critical task in any such application is the assessment of an argument's quality - but it is also particularly challenging. In this position paper, we start from a brief survey of argument quality research, where we identify the diversity of quality notions and the subjectiveness of their perception as the main hurdles towards substantial progress on argument quality assessment. We argue that the capabilities of instruction-following large language models (LLMs) to leverage knowledge across contexts enable a much more reliable assessment. Rather than just fine-tuning LLMs towards leaderboard chasing on assessment tasks, they need to be instructed systematically with argumentation theories and scenarios as well as with ways to solve argument-related problems. We discuss the real-world opportunities and ethical issues emerging thereby",
    "checked": true,
    "id": "b563e9a945dc51d52ffeff0bcf93e4154f04655d",
    "semantic_title": "argument quality assessment in the age of instruction-following large language models",
    "citation_count": 0,
    "authors": [
      "Henning Wachsmuth",
      "Gabriella Lapesa",
      "Elena Cabrio",
      "Anne Lauscher",
      "Joonsuk Park",
      "Eva Maria Vecchi",
      "Serena Villata",
      "Timon Ziegenbein"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.136": {
    "title": "Article Classification with Graph Neural Networks and Multigraphs",
    "volume": "main",
    "abstract": "Classifying research output into context-specific label taxonomies is a challenging and relevant downstream task, given the volume of existing and newly published articles. We propose a method to enhance the performance of article classification by enriching simple Graph Neural Network (GNN) pipelines with multi-graph representations that simultaneously encode multiple signals of article relatedness, e.g. references, co-authorship, shared publication source, shared subject headings, as distinct edge types. Fully supervised transductive node classification experiments are conducted on the Open Graph Benchmark OGBN-arXiv dataset and the PubMed diabetes dataset, augmented with additional metadata from Microsoft Academic Graph and PubMed Central, respectively. The results demonstrate that multi-graphs consistently improve the performance of a variety of GNN models compared to the default graphs. When deployed with SOTA textual node embedding methods, the transformed multi-graphs enable simple and shallow 2-layer GNN pipelines to achieve results on par with more complex architectures",
    "checked": true,
    "id": "e51fba259a859fc102cbe7fb4a2e6b325cca7f76",
    "semantic_title": "article classification with graph neural networks and multigraphs",
    "citation_count": 0,
    "authors": [
      "Khang Ly",
      "Yury Kashnitsky",
      "Savvas Chamezopoulos",
      "Valeria Krzhizhanovskaya"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.137": {
    "title": "ART: The Alternating Reading Task Corpus for Speech Entrainment and Imitation",
    "volume": "main",
    "abstract": "We introduce the Alternating Reading Task (ART) Corpus, a collection of dyadic sentence reading for studying the entrainment and imitation behaviour in speech communication. The ART corpus features three experimental conditions - solo reading, alternating reading, and deliberate imitation - as well as three subcorpora encompassing French-, Italian-, and Slovak-accented English. This design allows systematic investigation of speech entrainment in a controlled and less spontaneous setting. Alongside detailed transcriptions, it includes English proficiency scores, demographics, and in-experiment questionnaires for probing linguistic, personal and interpersonal influences on entrainment. Our presentation covers its design, collection, annotation processes, initial analysis, and future research prospects",
    "checked": true,
    "id": "e8fe2b6bd79ffcf956f50d73869b8179f2b22680",
    "semantic_title": "art: the alternating reading task corpus for speech entrainment and imitation",
    "citation_count": 1,
    "authors": [
      "Zheng Byron Yuan",
      "Dorina de Jong",
      "Ruitao Feng",
      "Štefan Beňuš",
      "Noël Nguyen",
      "Róbert Sabo",
      "Luciano Fadiga",
      "Alessandro D’Ausilio"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.138": {
    "title": "A Self-verified Method for Exploring Simile Knowledge from Pre-trained Language Models",
    "volume": "main",
    "abstract": "Simile tasks are challenging in natural language processing (NLP) because models require adequate world knowledge to produce predictions. In recent years, pre-trained language models (PLMs) have succeeded in NLP since they learn generic knowledge from a large corpus. The knowledge embedded in PLMs can be used for different kinds of Simile tasks. However, previous work usually explored one type of simile knowledge for a specific simile task, how to fully utilize different types of knowledge embedded in the PLMs requires further exploration. This paper proposes a self-verified method for exploring simile knowledge from PLMs, which allows the PLMs to leverage one type of simile knowledge to self-validate another. To this end, we first enhance PLMs with a novel multi-level simile recognition (MLSR) task that trains PLMs to evaluate the quality of similes. Then the PLMs leverage this evaluation score to assist the simile interpretation and generation tasks. In this way, we connect different types of simile knowledge in PLMs and make better use of them. Experiments on different pre-trained models and multiple publicly available datasets show that our method works for different kinds of PLMs and can explore more accurate simile knowledge for PLMs. Our code/data will be released on GitHub",
    "checked": true,
    "id": "a3990c8c32f9a6dbf1032640311fc236bed7309f",
    "semantic_title": "a self-verified method for exploring simile knowledge from pre-trained language models",
    "citation_count": 0,
    "authors": [
      "Longxuan Ma",
      "Changxin Ke",
      "Shuhan Zhou",
      "Churui Sun",
      "Wei-Nan Zhang",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.139": {
    "title": "A Semantic Mention Graph Augmented Model for Document-Level Event Argument Extraction",
    "volume": "main",
    "abstract": "Document-level Event Argument Extraction (DEAE) aims to identify arguments and their specific roles from an unstructured document. The advanced approaches on DEAE utilize prompt-based methods to guide pre-trained language models (PLMs) in extracting arguments from input documents. They mainly concentrate on establishing relations between triggers and entity mentions within documents, leaving two unresolved problems: a) independent modeling of entity mentions; b) document-prompt isolation. To this end, we propose a semantic mention Graph Augmented Model (GAM) to address these two problems in this paper. Firstly, GAM constructs a semantic mention graph that captures relations within and between documents and prompts, encompassing co-existence, co-reference and co-type relations. Furthermore, we introduce an ensemble graph transformer module to address mentions and their three semantic relations effectively. Later, the graph-augmented encoder-decoder module incorporates the relation-specific graph into the input embedding of PLMs and optimizes the encoder section with topology information, enhancing the relations comprehensively. Extensive experiments on the RAMS and WikiEvents datasets demonstrate the effectiveness of our approach, surpassing baseline methods and achieving a new state-of-the-art performance",
    "checked": true,
    "id": "0cd00c87f7d39afd347ce1f5d211e9e704220c83",
    "semantic_title": "a semantic mention graph augmented model for document-level event argument extraction",
    "citation_count": 0,
    "authors": [
      "Jian Zhang",
      "Changlin Yang",
      "Haiping Zhu",
      "Qika Lin",
      "Fangzhi Xu",
      "Jun Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.140": {
    "title": "ASEM: Enhancing Empathy in Chatbot through Attention-based Sentiment and Emotion Modeling",
    "volume": "main",
    "abstract": "Effective feature representations play a critical role in enhancing the performance of text generation models that rely on deep neural networks. However, current approaches suffer from several drawbacks, such as the inability to capture the deep semantics of language and sensitivity to minor input variations, resulting in significant changes in the generated text. In this paper, we present a novel solution to these challenges by employing a mixture of experts, multiple encoders, to offer distinct perspectives on the emotional state of the user's utterance while simultaneously enhancing performance. We propose an end-to-end model architecture called ASEM that performs emotion analysis on top of sentiment analysis for open-domain chatbots, enabling the generation of empathetic responses that are fluent and relevant. In contrast to traditional attention mechanisms, the proposed model employs a specialized attention strategy that uniquely zeroes in on sentiment and emotion nuances within the user's utterance. This ensures the generation of context-rich representations tailored to the underlying emotional tone and sentiment intricacies of the text. Our approach outperforms existing methods for generating empathetic embeddings, providing empathetic and diverse responses. The performance of our proposed model significantly exceeds that of existing models, enhancing emotion detection accuracy by 6.2% and lexical diversity by 1.4%. ASEM code is released at https://github.com/MIRAH-Official/Empathetic-Chatbot-ASEM.git",
    "checked": true,
    "id": "b9d00f197cba6cdb0d926e72a1eb688c1d9d6f07",
    "semantic_title": "asem: enhancing empathy in chatbot through attention-based sentiment and emotion modeling",
    "citation_count": 0,
    "authors": [
      "Omama Hamad",
      "Khaled Shaban",
      "Ali Hamdi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.141": {
    "title": "A Single Linear Layer Yields Task-Adapted Low-Rank Matrices",
    "volume": "main",
    "abstract": "Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning (PEFT) method that updates an initial weight matrix W0 with a delta matrix 𝛥 W consisted by two low-rank matrices A and B. A previous study suggested that there is correlation between W0 and 𝛥 W. In this study, we aim to delve deeper into relationships between W0 and low-rank matrices A and B to further comprehend the behavior of LoRA. In particular, we analyze a conversion matrix that transform W0 into low-rank matrices, which encapsulates information about the relationships. Our analysis reveals that the conversion matrices are similar across each layer. Inspired by these findings, we hypothesize that a single linear layer, which takes each layer's W0 as input, can yield task-adapted low-rank matrices. To confirm this hypothesis, we devise a method named Conditionally Parameterized LoRA (CondLoRA) that updates initial weight matrices with low-rank matrices derived from a single linear layer. Our empirical results show that CondLoRA maintains a performance on par with LoRA, despite the fact that the trainable parameters of CondLoRA are fewer than those of LoRA. Therefore, we conclude that \"a single linear layer yields task-adapted low-rank matrices.\" The code used in our experiments is available at https://github.com/CyberAgentAILab/CondLoRA",
    "checked": true,
    "id": "0a2f994976920bb95a1b1e20e5335384ba38fb1d",
    "semantic_title": "a single linear layer yields task-adapted low-rank matrices",
    "citation_count": 0,
    "authors": [
      "Hwichan Kim",
      "Shota Sasaki",
      "Sho Hoshino",
      "Ukyo Honda"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.142": {
    "title": "Asking and Answering Questions to Extract Event-Argument Structures",
    "volume": "main",
    "abstract": "This paper presents a question-answering approach to extract document-level event-argument structures. We automatically ask and answer questions for each argument type an event may have. Questions are generated using manually defined templates and generative transformers. Template-based questions are generated using predefined role-specific wh-words and event triggers from the context document. Transformer-based questions are generated using large language models trained to formulate questions based on a passage and the expected answer. Additionally, we develop novel data augmentation strategies specialized in inter-sentential event-argument relations. We use a simple span-swapping technique, coreference resolution, and large language models to augment the training instances. Our approach enables transfer learning without any corpora-specific modifications and yields competitive results with the RAMS dataset. It outperforms previous work, and it is especially beneficial to extract arguments that appear in different sentences than the event trigger. We also present detailed quantitative and qualitative analyses shedding light on the most common errors made by our best model",
    "checked": true,
    "id": "6a5abb17944918569281fb2bf1bc2ec0a64f45ed",
    "semantic_title": "asking and answering questions to extract event-argument structures",
    "citation_count": 1,
    "authors": [
      "Md Nayem Uddin",
      "Enfa Rose George",
      "Eduardo Blanco",
      "Steven R. Corman"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.143": {
    "title": "AssameseBackTranslit: Back Transliteration of Romanized Assamese Social Media Text",
    "volume": "main",
    "abstract": "This paper presents a novel back transliteration dataset capturing native language text originally composed in the Roman/Latin script, harvested from popular social media platforms, along with its corresponding representation in the native Assamese script. Assamese, categorized as a low-resource language within the Indo-Aryan language family, predominantly spoken in the north-east Indian state of Assam, faces a scarcity of linguistic resources. The dataset comprises a total of 60,312 Roman-native parallel transliterated sentences. This paper diverges from conventional forward transliteration datasets consisting mainly of named entities and technical terms, instead presenting a novel transliteration dataset cultivated from three prominent social media platforms, Facebook, Twitter(currently X), and YouTube, in the backward transliteration direction. The paper offers a comprehensive examination of ten state-of-the-art word-level transliteration models within the context of this dataset, encompassing transliteration evaluation benchmarks, extensive performance assessments, and a discussion of the unique chal- lenges encountered during the processing of transliterated social media content. Our approach involves the initial use of two statistical transliteration models, followed by the training of two state-of-the-art neural network-based transliteration models, evaluation of three publicly available pre-trained models, and ultimately fine-tuning one existing state-of-the-art multilingual transliteration model along with two pre-trained large language models using the collected datasets. Notably, the Neural Transformer model outperforms all other baseline transliteration models, achieving the lowest Word Error Rate (WER) and Character Error Rate (CER), and the highest BLEU (up to 4 gram) score of 55.05, 19.44, and 69.15, respectively",
    "checked": true,
    "id": "d24095183fc5cc9337d7a5677e0ad2fd6cf62277",
    "semantic_title": "assamesebacktranslit: back transliteration of romanized assamese social media text",
    "citation_count": 0,
    "authors": [
      "Hemanta Baruah",
      "Sanasam Ranbir Singh",
      "Priyankoo Sarmah"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.144": {
    "title": "Assessing Online Writing Feedback Resources: Generative AI vs. Good Samaritans",
    "volume": "main",
    "abstract": "Providing constructive feedback on student essays is a critical factor in improving educational results; however, it presents notable difficulties and may demand substantial time investments, especially when aiming to deliver individualized and informative guidance. This study undertakes a comparative analysis of two readily available online resources for students seeking to hone their skills in essay writing for English proficiency tests: 1) essayforum.com, a widely used platform where students can submit their essays and receive feedback from volunteer educators at no cost, and 2) Large Language Models (LLMs) such as ChatGPT. By contrasting the feedback obtained from these two resources, we posit that they can mutually reinforce each other and are more helpful if employed in conjunction when seeking no-cost online assistance. The findings of this research shed light on the challenges of providing personalized feedback and highlight the potential of AI in advancing the field of automated essay evaluation",
    "checked": true,
    "id": "baecc6afd6f0899107e7f1a0d151efe8754da051",
    "semantic_title": "assessing online writing feedback resources: generative ai vs. good samaritans",
    "citation_count": 0,
    "authors": [
      "Shabnam Behzad",
      "Omid Kashefi",
      "Swapna Somasundaran"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.145": {
    "title": "Assessing the Capabilities of Large Language Models in Coreference: An Evaluation",
    "volume": "main",
    "abstract": "This paper offers a nuanced examination of the role Large Language Models (LLMs) play in coreference resolution, aimed at guiding the future direction in the era of LLMs. We carried out both manual and automatic analyses of different LLMs' abilities, employing different prompts to examine the performance of different LLMs, obtaining a comprehensive view of their strengths and weaknesses. We found that LLMs show exceptional ability in understanding coreference. However, harnessing this ability to achieve state of the art results on traditional datasets and benchmarks isn't straightforward. Given these findings, we propose that future efforts should: (1) Improve the scope, data, and evaluation methods of traditional coreference research to adapt to the development of LLMs. (2) Enhance the fine-grained language understanding capabilities of LLMs",
    "checked": true,
    "id": "a6afdf49e32e641d6716143852f5ac8b81bd5947",
    "semantic_title": "assessing the capabilities of large language models in coreference: an evaluation",
    "citation_count": 0,
    "authors": [
      "Yujian Gan",
      "Massimo Poesio",
      "Juntao Yu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.146": {
    "title": "Assessing the Efficacy of Grammar Error Correction: A Human Evaluation Approach in the Japanese Context",
    "volume": "main",
    "abstract": "In this study, we evaluated the performance of the state-of-the-art sequence tagging grammar error detection and correction model (SeqTagger) using Japanese university students' writing samples. With an automatic annotation toolkit, ERRANT, we first evaluated SeqTagger's performance on error correction with human expert correction as the benchmark. Then a human-annotated approach was adopted to evaluate Seqtagger's performance in error detection using a subset of the writing dataset. Results indicated a precision of 63.66% and a recall of 20.19% for error correction in the full dataset. For the subset, after manual exclusion of irrelevant errors such as semantic and mechanical ones, the model shows an adjusted precision of 97.98% and an adjusted recall of 42.98% for error detection, indicating the model's high accuracy but also its conservativeness. Thematic analysis on errors undetected by the model revealed that determiners and articles, especially the latter, were predominant. Specifically, in terms of context-independent errors, the model occasionally overlooked basic ones and faced challenges with overly erroneous or complex structures. Meanwhile, context-dependent errors, notably those related to tense and noun number, as well as those possibly influenced by the students' first language (L1), remained particularly challenging",
    "checked": true,
    "id": "93c1e590fe8ea88e4472d1727d4a844d1449e388",
    "semantic_title": "assessing the efficacy of grammar error correction: a human evaluation approach in the japanese context",
    "citation_count": 0,
    "authors": [
      "Qiao Wang",
      "Zheng Yuan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.147": {
    "title": "A Streamlined Span-based Factorization Method for Few Shot Named Entity Recognition",
    "volume": "main",
    "abstract": "Few-shot named entity recognition (NER) is a challenging task that aims to recognize new named entities with only a limited amount of labeled examples. In this paper, we introduce SSF, which is a streamlined span-based factorization method that addresses the problem of few-shot NER. Our approach formulates few-shot NER as a span-level alignment problem between query and support instances. To achieve this goal, SSF decomposes the span-level alignment problem into several refined span-level procedures. The proposed approach encompasses several key modules such as the Span Boosting Module, Span Prototypical Module, Span Alignment Module, and Span Optimization Module. Our experimental results demonstrate a significant improvement over the previous state-of-the-art performance. Specifically, compared to previous methods, our proposed approach achieves an average F1 score improvement of 12 points on the FewNERD dataset and 10 points on the SNIPS dataset. Moreover, our approach has surpassed the latest state-of-the-art performance on both datasets",
    "checked": true,
    "id": "1524ec3b1785908b7d67ee7c03b9f7b83b44489b",
    "semantic_title": "a streamlined span-based factorization method for few shot named entity recognition",
    "citation_count": 0,
    "authors": [
      "Wenjie Xu",
      "Yidan Chen",
      "Jianquan Ouyang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.148": {
    "title": "A Study on How Attention Scores in the BERT Model Are Aware of Lexical Categories in Syntactic and Semantic Tasks on the GLUE Benchmark",
    "volume": "main",
    "abstract": "This study examines whether the attention scores between tokens in the BERT model significantly vary based on lexical categories during the fine-tuning process for downstream tasks. Drawing inspiration from the notion that in human language processing, syntactic and semantic information is parsed differently, we categorize tokens in sentences according to their lexical categories and focus on changes in attention scores among these categories. Our hypothesis posits that in downstream tasks that prioritize semantic information, attention scores centered on content words are enhanced, while in cases emphasizing syntactic information, attention scores centered on function words are intensified. Through experimentation conducted on six tasks from the GLUE benchmark dataset, we substantiate our hypothesis regarding the fine-tuning process. Furthermore, our additional investigations reveal the presence of BERT layers that consistently assign more bias to specific lexical categories, irrespective of the task, highlighting the existence of task-agnostic lexical category preferences",
    "checked": true,
    "id": "e5ecf52d3a7d0d5e79017d1cf5a178e37bd0f7dc",
    "semantic_title": "a study on how attention scores in the bert model are aware of lexical categories in syntactic and semantic tasks on the glue benchmark",
    "citation_count": 0,
    "authors": [
      "Dongjun Jang",
      "Sungjoo Byun",
      "Hyopil Shin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.149": {
    "title": "A Survey on Natural Language Processing for Programming",
    "volume": "main",
    "abstract": "Natural language processing for programming aims to use NLP techniques to assist programming. It is increasingly prevalent for its effectiveness in improving productivity. Distinct from natural language, a programming language is highly structured and functional. Constructing a structure-based representation and a functionality-oriented algorithm is at the heart of program understanding and generation. In this paper, we conduct a systematic review covering tasks, datasets, evaluation methods, techniques, and models from the perspective of the structure-based and functionality-oriented property, aiming to understand the role of the two properties in each component. Based on the analysis, we illustrate unexplored areas and suggest potential directions for future work",
    "checked": true,
    "id": "044bff0b129d4fcfafb6aae5a134d072d4d9318b",
    "semantic_title": "a survey on natural language processing for programming",
    "citation_count": 0,
    "authors": [
      "Qingfu Zhu",
      "Xianzhen Luo",
      "Fang Liu",
      "Cuiyun Gao",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.150": {
    "title": "A Tool for Determining Distances and Overlaps between Multimodal Annotations",
    "volume": "main",
    "abstract": "Comparing annotations is a constant and necessary step in corpus analysis. Although the nature of these annotations is normally research-specific, the tools used for this purpose do not have to be. Here, we present a tool for extracting and comparing annotations from ELAN, despite their idiosyncrasies. The intention behind this tool is to provide a handy way to analyze ELAN annotated files, by comparing tiers to a reference unit. Using the presented tool, it is possible to see how tiers overlap (even if they are of symbolic type), to which ratio, and the displacement regarding a reference unit. We present an example of multimodal corpus analysis, regarding the coordination between speech and gesture units based on a pragmatic reference. We argue that looking into overlap ratios can be more informative of the association between speech and gestures, and that considering a time buffer between speech and gestural events can be misleading",
    "checked": true,
    "id": "f5b1d246a196dad7645c3af5e23c61d16ebb3a7e",
    "semantic_title": "a tool for determining distances and overlaps between multimodal annotations",
    "citation_count": 0,
    "authors": [
      "Camila Antonio Barros",
      "Jorge Francisco Ciprián-Sánchez",
      "Saulo Mendes Santos"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.151": {
    "title": "A Treebank of Asia Minor Greek",
    "volume": "main",
    "abstract": "Asia Minor Greek (AMG) dialects are endangered dialects rich in history and cultAsia Minor Greek (AMG) dialects are endangered dialects rich in history and cultAsia Minor Greek (AMG) dialects are endangered dialects rich in history and cultAsia Minor Greek (AMG) dialects are endangered dialects rich in history and cultAsia Minor Greek (AMG) dialects are endangered dialects rich in history and culture that face a dire struggle for preservation due to declining speaker base and scarce linguistic resources. To address this need, we introduce a Universal Dependencies treebank of Pharasiot Greek, one of the severly endangerd AMG dialects. The present treebank is fully manually annotated and currently consists of 350 sentences from six fairy tales in Pharasiot dialect. Besides describing the treebank and the annotation process, we provide and discuss interesting phenomena we observed in the treebank. Most phenomena we discuss are related to contact-induced linguistic changes that these dialects are well known for. Beyond linguistic inquiry, like other treebanks for truly low-resource languages, the AMG treebank we present offers potentials for diverse applications, such as language preservation and revitalization, as well as NLP tools that have to be developed with scarce resources",
    "checked": true,
    "id": "d5d6f3a639ee3490d0ae6518a18086b33506d25b",
    "semantic_title": "a treebank of asia minor greek",
    "citation_count": 0,
    "authors": [
      "Eleni Vligouridou",
      "Inessa Iliadou",
      "Çağrı Çöltekin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.152": {
    "title": "A Trusted Multi-View Evidential Fusion Framework for Commonsense Reasoning",
    "volume": "main",
    "abstract": "While deep learning models are powerful, they have limitations in tasks that require commonsense reasoning, as these tasks often involve interpreting information that may not be directly available in the input. Providing evidence has been proven to significantly enhance performance in commonsense reasoning tasks. However, there are various perspectives on evidence, including natural language explanations generated by pre-trained language models, facts derived from world knowledge like text corpora and knowledge bases, and rationales extracted from the input context. Hence, it is crucial to determine how to estimate the confidence degree of different evidence and how to combine them reliably. To address these challenges, this study proposes a trusted multi-view evidential fusion framework for reliable commonsense reasoning tasks that dynamically assesses the confidence of evidence and combines different views of evidence in a trustworthy manner. The proposed method is applied to three commonsense question-answering benchmarks, demonstrating that this approach can effectively reason with multi-view evidence and can compete with state-of-the-art performance",
    "checked": true,
    "id": "68fc5bc11f3fdb86dfabe17583989e96c948ac4b",
    "semantic_title": "a trusted multi-view evidential fusion framework for commonsense reasoning",
    "citation_count": 0,
    "authors": [
      "Shuo Yang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.153": {
    "title": "Attack Named Entity Recognition by Entity Boundary Interference",
    "volume": "main",
    "abstract": "Named Entity Recognition (NER) is a cornerstone natural language processing task while its robustness has been given little attention. This paper rethinks the principles of the conventional text attack, as they can easily violate the label consistency between the original and adversarial NER samples. This is due to the fine-grained nature of NER, as even minor word changes in the sentence can result in the emergence or mutation of any entity, producing invalid adversarial samples. To this end, we propose a novel one-word modification NER attack based on a key insight, NER models are always vulnerable to the boundary position of an entity to make their decision. We thus strategically insert a new boundary into the sentence and trigger the victim model to make a wrong recognition either on this boundary word or on other words in the sentence. We call this attack Virtual Boundary Attack (ViBA), which is shown to be remarkably effective when attacking both English and Chinese models with a 70%-90% attack success rate on state-of-the-art language models, and also significantly faster than previous methods",
    "checked": true,
    "id": "56ae003ce7e1943748fbe1b360bf3f59615d6ed2",
    "semantic_title": "attack named entity recognition by entity boundary interference",
    "citation_count": 2,
    "authors": [
      "Yifei Yang",
      "Hongqiu Wu",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.154": {
    "title": "At the Crossroad of Cuneiform and NLP: Challenges for Fine-grained Part-of-speech Tagging",
    "volume": "main",
    "abstract": "The study of ancient Middle Eastern cultures is dominated by the vast number of cuneiform texts. Multiple languages and language families were expressed in cuneiform. The most dominant language written in cuneiform is the Semitic Akkadian, which is the focus of this paper. We are specifically focusing on letters written in the dialect used in modern-day Baghdad and south towards the Persian Gulf during the Old Babylonian period (c. 2000-1600 B.C.E.). The Akkadian language was rediscovered in the 19th century and is now being scrutinised by Natural Language Processing (NLP) methods. However, existing Akkadian text publications are not always suitable for digital editions. We therefore risk applying NLP methods onto renderings of Akkadian unfit for the purpose. In this paper we want to investigate the input material and try to initiate a discussion about best-practices in the crossroad where NLP meets cuneiform studies. Specifically, we want to question the use of pre-trained embeddings, sentence segmentation and the type of cuneiform input used to fine-tune language models for the task of fine-grained part-of-speech tagging. We examine the issues by theoretical and practical approaches in a way that we hope spurs discussions that are relevant for automatic processing of other ancient languages",
    "checked": true,
    "id": "11cc16e55a5d0dd51528c1e7d4af8e3be1fb4868",
    "semantic_title": "at the crossroad of cuneiform and nlp: challenges for fine-grained part-of-speech tagging",
    "citation_count": 1,
    "authors": [
      "Gustav Ryberg Smidt",
      "Els Lefever",
      "Katrien de Graef"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.155": {
    "title": "A Tulu Resource for Machine Translation",
    "volume": "main",
    "abstract": "We present the first parallel dataset for English–Tulu translation. Tulu, classified within the South Dravidian linguistic family branch, is predominantly spoken by approximately 2.5 million individuals in southwestern India. Our dataset is constructed by integrating human translations into the multilingual machine translation resource FLORES-200. Furthermore, we use this dataset for evaluation purposes in developing our English–Tulu machine translation model. For the model's training, we leverage resources available for related South Dravidian languages. We adopt a transfer learning approach that exploits similarities between high-resource and low-resource languages. This method enables the training of a machine translation system even in the absence of parallel data between the source and target language, thereby overcoming a significant obstacle in machine translation development for low-resource languages. Our English–Tulu system, trained without using parallel English–Tulu data, outperforms Google Translate by 19 BLEU points (in September 2023). The dataset and code are available here: https://github.com/manunarayanan/Tulu-NMT",
    "checked": true,
    "id": "5d3beccca85f0b2f7cf31a403918acc1231e50be",
    "semantic_title": "a tulu resource for machine translation",
    "citation_count": 0,
    "authors": [
      "Manu Narayanan",
      "Noëmi Aepli"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.156": {
    "title": "A Two-Stage Framework with Self-Supervised Distillation for Cross-Domain Text Classification",
    "volume": "main",
    "abstract": "Cross-domain text classification is a crucial task as it enables models to adapt to a target domain that lacks labeled data. It leverages or reuses rich labeled data from the different but related source domain(s) and unlabeled data from the target domain. To this end, previous work focuses on either extracting domain-invariant features or task-agnostic features, ignoring domain-aware features that may be present in the target domain and could be useful for the downstream task. In this paper, we propose a two-stage framework for cross-domain text classification. In the first stage, we finetune the model with mask language modeling (MLM) and labeled data from the source domain. In the second stage, we further fine-tune the model with self-supervised distillation (SSD) and unlabeled data from the target domain. We evaluate its performance on a public cross-domain text classification benchmark and the experiment results show that our method achieves new state-of-the-art results for both single-source domain adaptations (94.17% +1.03%) and multi-source domain adaptations (95.09% +1.34%)",
    "checked": true,
    "id": "488523a765a53c733273f25a08cdffe0958e9f66",
    "semantic_title": "a two-stage framework with self-supervised distillation for cross-domain text classification",
    "citation_count": 1,
    "authors": [
      "Yunlong Feng",
      "Bohan Li",
      "Libo Qin",
      "Xiao Xu",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.157": {
    "title": "A Two-Stage Prediction-Aware Contrastive Learning Framework for Multi-Intent NLU",
    "volume": "main",
    "abstract": "Multi-intent natural language understanding (NLU) presents a formidable challenge due to the model confusion arising from multiple intents within a single utterance. While previous works train the model contrastively to increase the margin between different multi-intent labels, they are less suited to the nuances of multi-intent NLU. They ignore the rich information between the shared intents, which is beneficial to constructing a better embedding space, especially in low-data scenarios. We introduce a two-stage Prediction-Aware Contrastive Learning (PACL) framework for multi-intent NLU to harness this valuable knowledge. Our approach capitalizes on shared intent information by integrating word-level pre-training and prediction-aware contrastive fine-tuning. We construct a pre-training dataset using a word-level data augmentation strategy. Subsequently, our framework dynamically assigns roles to instances during contrastive fine-tuning while introducing a prediction-aware contrastive loss to maximize the impact of contrastive learning. We present experimental results and empirical analysis conducted on three widely used datasets, demonstrating that our method surpasses the performance of three prominent baselines on both low-data and full-data scenarios",
    "checked": true,
    "id": "64a931fc0015bc57242eca8c93d7b3262133aade",
    "semantic_title": "a two-stage prediction-aware contrastive learning framework for multi-intent nlu",
    "citation_count": 0,
    "authors": [
      "Guanhua Chen",
      "Yutong Yao",
      "Derek F. Wong",
      "Lidia S. Chao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.158": {
    "title": "A Typology of Errors for User Utterances in Chatbots",
    "volume": "main",
    "abstract": "This paper discusses the challenges non-prescriptive language uses in chatbot communication create for Semantic Parsing (SP). To help SP developers improve their systems, we propose a flexible error typology based on an analysis of a sample of non-prescriptive language uses mined from a domain-specific chatbot logs. This typology is not tied to any specific language model. We also present a framework for automatically mapping these errors to the typology. Finally, we show how our framework can help evaluate SP systems from a linguistic robustness perspective. Our framework can be expanded to include new classes of errors across different domains and user demographics",
    "checked": true,
    "id": "b870205e4bf4bfbe68262c6692440a7806116e62",
    "semantic_title": "a typology of errors for user utterances in chatbots",
    "citation_count": 0,
    "authors": [
      "Anu Singh",
      "Esme Manandise"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.159": {
    "title": "Audiocite.net : A Large Spoken Read Dataset in French",
    "volume": "main",
    "abstract": "The advent of self-supervised learning (SSL) in speech processing has allowed the use of large unlabeled datasets to learn pre-trained models, serving as powerful encoders for various downstream tasks. However, the application of these SSL methods to languages such as French has proved difficult due to the scarcity of large French speech datasets. To advance the emergence of pre-trained models for French speech, we present the Audiocite.net corpus composed of 6,682 hours of recordings from 130 readers. This corpus is composed of audiobooks from the audiocite.net website, shared by 130 readers. In addition to describing the creation process and final statistics, we also show how this dataset impacted the models of LeBenchmark project in its 14k version for speech processing downstream tasks",
    "checked": true,
    "id": "64824c0a06abdabf1e684b0b867598875adf70fd",
    "semantic_title": "audiocite.net : a large spoken read dataset in french",
    "citation_count": 0,
    "authors": [
      "Soline Felice",
      "Solene Virginie Evain",
      "Solange Rossato",
      "François Portet"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.160": {
    "title": "AuRoRA: A One-for-all Platform for Augmented Reasoning and Refining with Task-Adaptive Chain-of-Thought Prompting",
    "volume": "main",
    "abstract": "Large language models (LLMs) empowered by chain-of-thought (CoT) prompting have yielded remarkable prowess in reasoning tasks. Nevertheless, current methods predominantly lean on handcrafted or task-specific demonstrations, lack reliable knowledge basis and thus struggle for trustworthy responses in an automated pattern. While recent works endeavor to improve upon one certain aspect, they ignore the importance and necessity of establishing an integrated and interpretable reasoning system. To address these drawbacks and provide a universal solution, we propose AuRoRA: a one-for-all platform for augmented reasoning and refining based on CoT prompting that excels in adaptability, reliability, integrity, and interpretability. The system exhibits superior performances across six reasoning tasks and offers real-time visual analysis, which has pivotal academic and application value in the era of LLMs. The AuRoRA platform is available at https://huggingface.co/spaces/Anni123/AuRoRA",
    "checked": true,
    "id": "2ed0504ec0a97c3419a97e888abbf50f2b6976ab",
    "semantic_title": "aurora: a one-for-all platform for augmented reasoning and refining with task-adaptive chain-of-thought prompting",
    "citation_count": 0,
    "authors": [
      "Anni Zou",
      "Zhuosheng Zhang",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.161": {
    "title": "Automated Extraction of Prosodic Structure from Unannotated Sign Language Video",
    "volume": "main",
    "abstract": "As in oral phonology, prosody is an important carrier of linguistic information in sign languages. One of the most prominent ways this reveals itself is in the time structure of signs: their rhythm and intensity of articulation. To be able to empirically see these effects, the velocity of the hands can be computed throughout the execution of a sign. In this article, we propose a method for extracting this information from unlabeled videos of sign language, exploiting CoTracker, a recent advancement in computer vision which can track every point in a video without the need of any calibration or fine-tuning. The dominant hand is identified via clustering of the computed point velocities, and its dynamic profile plotted to make apparent the prosodic structure of signing. We apply our method to different datasets and sign languages, and perform a preliminary visual exploration of results. This exploration supports the usefulness of our methodology for linguistic analysis, though issues to be tackled remain, such as bi-manual signs and a formal and numerical evaluation of accuracy. Nonetheless, the absence of any preprocessing requirements may make it useful for other researchers and datasets",
    "checked": true,
    "id": "26ec8d35ed4f4bea6fdfd15cd7b6adbfd637e12a",
    "semantic_title": "automated extraction of prosodic structure from unannotated sign language video",
    "citation_count": 0,
    "authors": [
      "Antonio F. G. Sevilla",
      "José María Lahoz-Bengoechea",
      "Alberto Diaz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.162": {
    "title": "Automatically Estimating Textual and Phonemic Complexity for Cued Speech: How to See the Sounds from French Texts",
    "volume": "main",
    "abstract": "In this position paper we present a methodology to automatically annotate French text for Cued Speech (CS), a communication system developed for people with hearing loss to complement speech reading at the phonetic level. This visual communication mode uses handshapes in different placements near the face in combination with the mouth movements (called ‘cues' or ‘keys') to make the phonemes of spoken language look different from each other. CS is used to acquire skills in lip reading, in oral communication and for reading. Despite many studies demonstrating its benefits, there are few resources available for learning and practicing it, especially in French. We thus propose a methodology to phonemize written corpora so that each word is aligned with the corresponding CS key(s). This methodology is proposed as part of a wider project aimed at creating an augmented reality system displaying a virtual coding hand where the user will be able to choose a text upon its complexity for cueing",
    "checked": true,
    "id": "5a3432d16f8285a5dd96ab2b3e629788c4d3355d",
    "semantic_title": "automatically estimating textual and phonemic complexity for cued speech: how to see the sounds from french texts",
    "citation_count": 0,
    "authors": [
      "Núria Gala",
      "Brigitte Bigi",
      "Marie Bauer"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.163": {
    "title": "Automatic Animacy Classification for Romanian Nouns",
    "volume": "main",
    "abstract": "We introduce the first Romanian animacy classifier, specifically a type-based binary classifier of Romanian nouns into the classes human/non-human, using pre-trained word embeddings and animacy information derived from Romanian WordNet. By obtaining a seed set of labeled nouns and their embeddings, we are able to train classifiers that generalize to unseen nouns. We compare three different architectures and observe good performance on classifying word types. In addition, we manually annotate a small corpus for animacy to perform a token-based evaluation of Romanian animacy classification in a naturalistic setting, which reveals limitations of the type-based classification approach",
    "checked": true,
    "id": "aa5398f1e5ab9ec4768605b79644a24a843c90d3",
    "semantic_title": "automatic animacy classification for romanian nouns",
    "citation_count": 0,
    "authors": [
      "Maria Tepei",
      "Jelke Bloem"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.164": {
    "title": "Automatic Annotation of Grammaticality in Child-Caregiver Conversations",
    "volume": "main",
    "abstract": "The acquisition of grammar has been a central question to adjudicate between theories of language acquisition. In order to conduct faster, more reproducible, and larger-scale corpus studies on grammaticality in child-caregiver conversations, tools for automatic annotation can offer an effective alternative to tedious manual annotation. We propose a coding scheme for context-dependent grammaticality in child-caregiver conversations and annotate more than 4,000 utterances from a large corpus of transcribed conversations. Based on these annotations, we train and evaluate a range of NLP models. Our results show that fine-tuned Transformer-based models perform best, achieving human inter-annotation agreement levels. As a first application and sanity check of this tool, we use the trained models to annotate a corpus almost two orders of magnitude larger than the manually annotated data and verify that children's grammaticality shows a steady increase with age. This work contributes to the growing literature on applying state-of-the-art NLP methods to help study child language acquisition at scale",
    "checked": true,
    "id": "22599f3e3dd9374b689b9e969569f22b5cc348ab",
    "semantic_title": "automatic annotation of grammaticality in child-caregiver conversations",
    "citation_count": 0,
    "authors": [
      "Mitja Nikolaus",
      "Abhishek Agrawal",
      "Petros Kaklamanis",
      "Alex Warstadt",
      "Abdellah Fourtassi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.165": {
    "title": "Automatic Authorship Analysis in Human-AI Collaborative Writing",
    "volume": "main",
    "abstract": "As the quality of AI-generated text increases with the development of new Large Language Models, people use them to write in a variety of contexts. Human-AI collaborative writing poses a potential challenge for existing AI analysis techniques, which have been primarily tested either on human-written text only, or on samples independently generated by humans and AI. In this work, we investigate the extent to which existing AI detection and authorship analysis models can perform classification on data generated in human-AI collaborative writing sessions. Results show that, for AI text detection in the cowriting setting, classifiers based on authorship embeddings (Rivera-Soto et al., 2021) outperform classifiers used in prior work distinguishing AI vs. human text generated independently. However, these embeddings are not optimal for finer-grained authorship identification tasks: for authorship verification, n-gram based models are more robust to human-AI co-written text, and authorship attribution performance degrades compared to baselines that use human-written text only. Taken together, this suggests that the rise of human-AI co-written text will require adapting AI detection tools and authorship analysis techniques in the near future. We release our code at https://github.com/AARichburg/Human-AI_Authorship_Analysis",
    "checked": true,
    "id": "bcce12ed08778332b2b0954722c48ffb0a86d88c",
    "semantic_title": "automatic authorship analysis in human-ai collaborative writing",
    "citation_count": 0,
    "authors": [
      "Aquia Richburg",
      "Calvin Bao",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.166": {
    "title": "Automatic Coding of Contingency in Child-Caregiver Conversations",
    "volume": "main",
    "abstract": "One of the most important communicative skills children have to learn is to engage in meaningful conversations with people around them. At the heart of this learning lies the mastery of contingency, i.e., the ability to contribute to an ongoing exchange in a relevant fashion (e.g., by staying on topic). Current research on this question relies on the manual annotation of a small sample of children, which limits our ability to draw general conclusions about development. Here, we propose to mitigate the limitations of manual labor by relying on automatic tools for contingency judgment in children's early natural interactions with caregivers. Drawing inspiration from the field of dialogue systems evaluation, we built and compared several automatic classifiers. We found that a Transformer-based pre-trained language model – when fine-tuned on a relatively small set of data we annotated manually (around 3,500 turns) – provided the best predictions. We used this model to automatically annotate, new and large-scale data, almost two orders of magnitude larger than our fine-tuning set. It was able to replicate existing results and generate new data-driven hypotheses. The broad impact of the work is to provide resources that can help the language development community study communicative development at scale, leading to more robust theories",
    "checked": true,
    "id": "9953fb16718ff997bad315ac8706620534e8852f",
    "semantic_title": "automatic coding of contingency in child-caregiver conversations",
    "citation_count": 0,
    "authors": [
      "Abhishek Agrawal",
      "Mitja Nikolaus",
      "Benoit Favre",
      "Abdellah Fourtassi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.167": {
    "title": "Automatic Construction of a Chinese Review Dataset for Aspect Sentiment Triplet Extraction via Iterative Weak Supervision",
    "volume": "main",
    "abstract": "Aspect Sentiment Triplet Extraction (ASTE), introduced in 2020, is a task that involves the extraction of three key elements: target aspects, descriptive opinion spans, and their corresponding sentiment polarity. This process, however, faces a significant hurdle, particularly when applied to Chinese languages, due to the lack of sufficient datasets for model training, largely attributable to the arduous manual labeling process. To address this issue, we present an innovative framework that facilitates the automatic construction of ASTE via Iterative Weak Supervision, negating the need for manual labeling, aided by a discriminator to weed out subpar samples. The objective is to successively improve the quality of this raw data and generate supplementary data. The effectiveness of our approach is underscored by our results, which include the creation of a substantial Chinese review dataset. This dataset encompasses over 60,000 Google restaurant reviews in Chinese and features more than 200,000 extracted triplets. Moreover, we have also established a robust baseline model by leveraging a novel method of weak supervision. Both our dataset and model are openly accessible to the public",
    "checked": true,
    "id": "621e6344d5194fa9cdd7a9c0cf70737f2913f104",
    "semantic_title": "automatic construction of a chinese review dataset for aspect sentiment triplet extraction via iterative weak supervision",
    "citation_count": 0,
    "authors": [
      "Chia-Wen Lu",
      "Ching-Wen Yang",
      "Wei-Yun Ma"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.168": {
    "title": "Automatic Construction of a Large-Scale Corpus for Geoparsing Using Wikipedia Hyperlinks",
    "volume": "main",
    "abstract": "Geoparsing is the task of estimating the latitude and longitude (coordinates) of location expressions in texts. Geoparsing must deal with the ambiguity of the expressions that indicate multiple locations with the same notation. For evaluating geoparsing systems, several corpora have been proposed in previous work. However, these corpora are small-scale and suffer from the coverage of location expressions on general domains. In this paper, we propose Wikipedia Hyperlink-based Location Linking (WHLL), a novel method to construct a large-scale corpus for geoparsing from Wikipedia articles. WHLL leverages hyperlinks in Wikipedia to annotate multiple location expressions with coordinates. With this method, we constructed the WHLL corpus, a new large-scale corpus for geoparsing. The WHLL corpus consists of 1.3M articles, each containing about 7.8 unique location expressions. 45.6% of location expressions are ambiguous and refer to more than one location with the same notation. In each article, location expressions of the article title and those hyperlinks to other articles are assigned with coordinates. By utilizing hyperlinks, we can accurately assign location expressions with coordinates even with ambiguous location expressions in the texts. Experimental results show that there remains room for improvement by disambiguating location expressions",
    "checked": true,
    "id": "b9de9c5afe7753c64254b28cdee25790397511b5",
    "semantic_title": "automatic construction of a large-scale corpus for geoparsing using wikipedia hyperlinks",
    "citation_count": 0,
    "authors": [
      "Keyaki Ohno",
      "Hirotaka Kameko",
      "Keisuke Shirai",
      "Taichi Nishimura",
      "Shinsuke Mori"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.169": {
    "title": "Automatic Data Visualization Generation from Chinese Natural Language Questions",
    "volume": "main",
    "abstract": "Data visualization has emerged as an effective tool for getting insights from massive datasets. Due to the hardness of manipulating the programming languages of data visualization, automatic data visualization generation from natural languages (Text-to-Vis) is becoming increasingly popular. Despite the plethora of research effort on the English Text-to-Vis, studies have yet to be conducted on data visualization generation from questions in Chinese. Motivated by this, we propose a Chinese Text-to-Vis dataset in the paper and demonstrate our first attempt to tackle this problem. Our model integrates multilingual BERT as the encoder, boosts the cross-lingual ability, and infuses the n-gram information into our word representation learning. Our experimental results show that our dataset is challenging and deserves further research",
    "checked": true,
    "id": "13045d2522c679113b0007db59bbea1091ece35b",
    "semantic_title": "automatic data visualization generation from chinese natural language questions",
    "citation_count": 2,
    "authors": [
      "Yan Ge",
      "Victor Junqiu Wei",
      "Yuanfeng Song",
      "Jason Chen Zhang",
      "Raymond Chi-Wing Wong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.170": {
    "title": "Automatic Decomposition of Text Editing Examples into Primitive Edit Operations: Toward Analytic Evaluation of Editing Systems",
    "volume": "main",
    "abstract": "This paper presents our work on a task of automatic decomposition of text editing examples into primitive edit operations. Toward a detailed analysis of the behavior of text editing systems, identification of fine-grained edit operations performed by the systems is essential. Given a pair of source and edited sentences, the goal of our task is to generate a non-redundant sequence of primitive edit operations, i.e., the semantically minimal edit operations preserving grammaticality, that iteratively converts the source sentence to the edited sentence. First, we formalize this task, explaining its significant features and specifying the constraints that primitive edit operations should satisfy. Then, we propose a method to automate this task, which consists of two steps: generation of an edit operation lattice and selection of an optimal path. To obtain a wide range of edit operation candidates in the first step, we combine a phrase aligner and a large language model. Experimental results show that our method perfectly decomposes 44% and 64% of editing examples in the text simplification and machine translation post-editing datasets, respectively. Detailed analyses also provide insights into the difficulties of this task, suggesting directions for improvement",
    "checked": true,
    "id": "0f9bb7274052c8890565457ca31e441fd317ddb1",
    "semantic_title": "automatic decomposition of text editing examples into primitive edit operations: toward analytic evaluation of editing systems",
    "citation_count": 0,
    "authors": [
      "Daichi Yamaguchi",
      "Rei Miyata",
      "Atsushi Fujita",
      "Tomoyuki Kajiwara",
      "Satoshi Sato"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.171": {
    "title": "Automatic Extraction of Language-Specific Biomarkers of Healthy Aging in Icelandic",
    "volume": "main",
    "abstract": "This study examines the influence of task type and healthy aging on various automatically extracted part-of-speech features in Icelandic. We administered three language tasks to participants aged 60–80: picture description, trip planning, and description of one's childhood home. Our findings reveal significant task effects on 11 out of 14 linguistic variables studied, highlighting the substantial influence of sampling methods on language production. Among the variables showing statistically significant task effects, we find the rate of the genitive and subjunctive, variables which can only be studied in morphologically richer languages like Icelandic. On the other hand, rates of pronouns, adverbs, and prepositions remained stable across task types. Aging effects were more subtle, being evident in 3 of the 14 variables, including an interaction with task type for dative case marking. These findings underscore the significance of task selection in studies targeting linguistic features but also emphasize the need to examine languages other than English to fully understand the effects of aging on language production. Additionally, the results have clinical implications: understanding healthy aging's impact on language can help us better identify and study changes caused by Alzheimer's Disease in older adults' speech",
    "checked": true,
    "id": "c20c260ba53906183671a613913342ef6d09d6fb",
    "semantic_title": "automatic extraction of language-specific biomarkers of healthy aging in icelandic",
    "citation_count": 0,
    "authors": [
      "Elena Callegari",
      "Iris Edda Nowenstein",
      "Ingunn Jóhanna Kristjánsdóttir",
      "Anton Karl Ingason"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.172": {
    "title": "Automatic Extraction of Nominal Phrases from German Learner Texts of Different Proficiency Levels",
    "volume": "main",
    "abstract": "Correctly inflecting determiners and adjectives so that they agree with the noun in nominal phrases (NPs) is a big challenge for learners of German. Given the increasing number of available learner corpora, a large-scale corpus-based study on the acquisition of this aspect of German morphosyntax would be desirable. In this paper, we present a pilot study in which we investigate how well nouns, their grammatical heads and the dependents that have to agree with the noun can be extracted automatically via dependency parsing. For six samples of the German learner corpus MERLIN (one per proficiency level), we found that in spite of many ungrammatical sentences in texts of low proficiency levels, human annotators find only few true ambiguities that would make the extraction of NPs and their heads infeasible. The automatic parsers, however, perform rather poorly on extracting the relevant elements for texts on CEFR levels A1-B1 (< 70%) but quite well from level B2 onwards ( 90%). We discuss the sources of errors and how performance could potentially be increased in the future",
    "checked": true,
    "id": "dd785dc4885863dc4bb6d82cd476f0ac0d30be1b",
    "semantic_title": "automatic extraction of nominal phrases from german learner texts of different proficiency levels",
    "citation_count": 0,
    "authors": [
      "Ronja Laarmann-Quante",
      "Marco Müller",
      "Eva Belke"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.173": {
    "title": "Automatic Identification of COVID-19-Related Conspiracy Narratives in German Telegram Channels and Chats",
    "volume": "main",
    "abstract": "We are concerned with mapping the discursive landscape of conspiracy narratives surrounding the COVID-19 pandemic. In the present study, we analyse a corpus of more than 1,000 German Telegram posts tagged with 14 fine-grained conspiracy narrative labels by three independent annotators. Since emerging narratives on social media are short-lived and notoriously hard to track, we experiment with different state-of-the-art approaches to few-shot and zero-shot text classification. We report performance in terms of ROC-AUC and in terms of optimal F1, and compare fine-tuned methods with off-the-shelf approaches and human performance",
    "checked": true,
    "id": "c25341c5f7f15d81de3f529051c4410e41e3e4d4",
    "semantic_title": "automatic identification of covid-19-related conspiracy narratives in german telegram channels and chats",
    "citation_count": 0,
    "authors": [
      "Philipp Heinrich",
      "Andreas Blombach",
      "Bao Minh Doan Dang",
      "Leonardo Zilio",
      "Linda Havenstein",
      "Nathan Dykes",
      "Stephanie Evert",
      "Fabian Schäfer"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.174": {
    "title": "Automatic Partitioning of a Code-Switched Speech Corpus Using Mixed-Integer Programming",
    "volume": "main",
    "abstract": "Defining training, development and test set partitions for speech corpora is usually accomplished by hand. However, for the dataset under investigation, which contains a large number of speakers, eight different languages and code-switching between all the languages, this style of partitioning is not feasible. Therefore, we view the partitioning task as a resource allocation problem and propose to solve it automatically and optimally by the application of mixed-integer linear programming. Using this approach, we are able to partition a new 41.6-hour multilingual corpus of code-switched speech into training, development and testing partitions while maintaining a fixed number of speakers and a specific amount of code-switched speech in the development and test partitions. For this newly partitioned corpus, we present baseline speech recognition results using a state-of-the-art multilingual transformer model (Wav2Vec2-XLS-R) and show that the exclusion of very short utterances (<1s) results in substantially improved speech recognition performance",
    "checked": true,
    "id": "44c981d909a16031764770f819a7770d6198d7f4",
    "semantic_title": "automatic partitioning of a code-switched speech corpus using mixed-integer programming",
    "citation_count": 0,
    "authors": [
      "Joshua Miles Jansen van Vüren",
      "Febe de Wet",
      "Thomas Niesler"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.175": {
    "title": "Automatic Punctuation Model for Spanish Live Transcriptions",
    "volume": "main",
    "abstract": "With the widespread adoption of automatic transcription tools, acquiring speech transcriptions within seconds has become a reality. Nonetheless, many of these tools yield unpunctuated outputs, potentially incurring additional costs. This paper presents a novel approach to integrating punctuation into the transcriptions generated by such automatic tools, specifically focusing on Spanish-speaking contexts. Leveraging the RoBERTa-bne model pre-trained with data from the Spanish National Library, our training proposal is augmented with additional corpora to enhance performance on less common punctuation marks, such as question marks. Also, the proposed model has been trained through fine-tuning pre-trained models, involving adjustments for token classification and using SoftMax to identify the highest probability token. The proposed model obtains promising results when compared with other Spanish reference paper models. Ultimately, this model aims to facilitate punctuation on live transcriptions seamlessly and accurately. The proposed model will be applied to a real-case education project to improve the readability of the transcriptions",
    "checked": true,
    "id": "163d81d40dedbf557d51919d16378594f37d8c61",
    "semantic_title": "automatic punctuation model for spanish live transcriptions",
    "citation_count": 0,
    "authors": [
      "Mario Perez-Enriquez",
      "Jose Manuel Masiello-Ruiz",
      "Jose Luis Lopez-Cuadrado",
      "Israel Gonzalez-Carrasco",
      "Paloma Martinez-Fernandez",
      "Belen Ruiz-Mezcua"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.176": {
    "title": "Automatic Speech Interruption Detection: Analysis, Corpus, and System",
    "volume": "main",
    "abstract": "Interruption detection is a new yet challenging task in the field of speech processing. This article presents a comprehensive study on automatic speech interruption detection, from the definition of this task, the assembly of a specialized corpus, and the development of an initial baseline system. We provide three main contributions: Firstly, we define the task, taking into account the nuanced nature of interruptions within spontaneous conversations. Secondly, we introduce a new corpus of conversational data, annotated for interruptions, to facilitate research in this domain. This corpus serves as a valuable resource for evaluating and advancing interruption detection techniques. Lastly, we present a first baseline system, which use speech processing methods to automatically identify interruptions in speech with promising results. In this article, we derivate from theoretical notions of interruption to build a simplification of this notion based on overlapped speech detection. Our findings can not only serve as a foundation for further research in the field but also provide a benchmark for assessing future advancements in automatic speech interruption detection",
    "checked": true,
    "id": "04966aa09596010a670840a4f1f340ea88b69866",
    "semantic_title": "automatic speech interruption detection: analysis, corpus, and system",
    "citation_count": 0,
    "authors": [
      "Martin Lebourdais",
      "Marie Tahon",
      "Antoine Laurent",
      "Sylvain Meignier"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.177": {
    "title": "Automatic Speech Recognition for Gascon and Languedocian Variants of Occitan",
    "volume": "main",
    "abstract": "This paper describes different approaches for developing, for the first time, an automatic speech recognition system for two of the main dialects of Occitan, namely Gascon and Languedocian, and the results obtained in them. The difficulty of the task lies in the fact that Occitan is a less-resourced language. Although a great effort has been made to collect or create corpora of each variant (transcribed speech recordings for the acoustic models and two text corpora for the language models), the sizes of the corpora obtained are far from those of successful systems reported in the literature, and thus we have tested different techniques to compensate for the lack of resources. We have developed classical systems using Kaldi, creating an acoustic model for each variant and also creating language models from the collected corpora and from machine translated texts. We have also tried fine-tuning a Whisper model with our speech corpora. We report word error rates of 20.86 for Gascon and 13.52 for Languedocian with the Kaldi systems and 16.37 for Gascon and 11.74 for Languedocian with Whisper",
    "checked": true,
    "id": "3cf8bd7a03b94d22b1fed3c2b7aee01ca72015a4",
    "semantic_title": "automatic speech recognition for gascon and languedocian variants of occitan",
    "citation_count": 0,
    "authors": [
      "Iñigo Morcillo",
      "Igor Leturia",
      "Ander Corral",
      "Xabier Sarasola",
      "Michaël Barret",
      "Aure Séguier",
      "Benaset Dazéas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.178": {
    "title": "Automatic Speech Recognition System-Independent Word Error Rate Estimation",
    "volume": "main",
    "abstract": "Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems. In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript. Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent). These are also domain-dependent and inflexible in real-world applications. In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed. In contrast to prior work, the WER estimators are trained using data that simulates ASR system output. Hypotheses are generated using phonetically similar or linguistically more likely alternative words. In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data. On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME. The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset",
    "checked": true,
    "id": "c955d994d70f49612c9b04c34b3a75d1d8bdd9be",
    "semantic_title": "automatic speech recognition system-independent word error rate estimation",
    "citation_count": 0,
    "authors": [
      "Chanho Park",
      "Mingjie Chen",
      "Thomas Hain"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.179": {
    "title": "Automating Dataset Production Using Generative Text and Image Models",
    "volume": "main",
    "abstract": "Practical and ethical dataset collection remains a challenge blocking many empirical methods in natural language processing, resulting in a lack of benchmarks or data on which to test hypotheses. We propose a solution to some of these areas by presenting a pipeline to reduce the research burden of producing image and text datasets when datasets may not exist. Our approach, with accompanying software tools, involves (1) generating text with LLMs; (2) creating accompanying image vignettes with text–to–image transformers; and (3) low-cost human validation. Based on existing literature that has struggled with quantitative evaluation (due to difficulty of data collection), we present the creation of 3 relevant datasets, and conduct a user study that demonstrates this approach is able to aid researchers in obtaining previously-challenging datasets. We provide sample data generated with this technique, the source code used to produce it, and discuss applicability and limitations",
    "checked": true,
    "id": "bbeca04bc16a8073acf15754dad78e56f00f85a6",
    "semantic_title": "automating dataset production using generative text and image models",
    "citation_count": 0,
    "authors": [
      "Christopher Thierauf",
      "Mitchell Abrams",
      "Matthias Scheutz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.180": {
    "title": "Autonomous Aspect-Image Instruction a2II: Q-Former Guided Multimodal Sentiment Classification",
    "volume": "main",
    "abstract": "Multimodal aspect-oriented sentiment classification (MABSC) task has garnered significant attention, which aims to identify the sentiment polarities of aspects by combining both language and vision information. However, the limited multimodal data in this task has become a big gap for the vision-language multimodal fusion. While large-scale vision-language pretrained models have been adapted to multiple tasks, their use for MABSC task is still in a nascent stage. In this work, we present an attempt to use the instruction tuning paradigm to MABSC task and leverage the ability of large vision-language models to alleviate the limitation in the fusion of textual and image modalities. To tackle the problem of potential irrelevance between aspects and images, we propose a plug-and-play selector to autonomously choose the most appropriate instruction from the instruction pool, thereby reducing the impact of irrelevant image noise on the final sentiment classification results. We conduct extensive experiments in various scenarios and our model achieves state-of-the-art performance on benchmark datasets, as well as in few-shot settings",
    "checked": true,
    "id": "d9a12190eaf527e1c2885db59dd56119937221c7",
    "semantic_title": "autonomous aspect-image instruction a2ii: q-former guided multimodal sentiment classification",
    "citation_count": 0,
    "authors": [
      "Junjia Feng",
      "Mingqian Lin",
      "Lin Shang",
      "Xiaoying Gao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.181": {
    "title": "Auxiliary Knowledge-Induced Learning for Automatic Multi-Label Medical Document Classification",
    "volume": "main",
    "abstract": "The International Classification of Diseases (ICD) is an authoritative medical classification system of different diseases and conditions for clinical and management purposes. ICD indexing aims to assign a subset of ICD codes to a medical record. Since human coding is labour-intensive and error-prone, many studies employ machine learning techniques to automate the coding process. ICD coding is a challenging task, as it needs to assign multiple codes to each medical document from an extremely large hierarchically organized collection. In this paper, we propose a novel approach for ICD indexing that adopts three ideas: (1) we use a multi-level deep dilated residual convolution encoder to aggregate the information from the clinical notes and learn document representations across different lengths of the texts; (2) we formalize the task of ICD classification with auxiliary knowledge of the medical records, which incorporates not only the clinical texts but also different clinical code terminologies and drug prescriptions for better inferring the ICD codes; and (3) we introduce a graph convolutional network to leverage the co-occurrence patterns among ICD codes, aiming to enhance the quality of label representations. Experimental results show the proposed method achieves state-of-the-art performance on a number of measures",
    "checked": true,
    "id": "d9e54c144c19a62f3429210e675d03e2c101d098",
    "semantic_title": "auxiliary knowledge-induced learning for automatic multi-label medical document classification",
    "citation_count": 0,
    "authors": [
      "Xindi Wang",
      "Robert E. Mercer",
      "Frank Rudzicz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.182": {
    "title": "A Virtual Patient Dialogue System Based on Question-Answering on Clinical Records",
    "volume": "main",
    "abstract": "In this work we present two datasets for the development of virtual patients and the first evaluation results. We firstly introduce a Spanish corpus of medical dialogue questions annotated with intents, built upon prior research in French. We also propose a second dataset of dialogues using a novel annotation approach that involves doctor questions, patient answers, and corresponding clinical records, organized as triples of the form (clinical report, question, patient answer). This way, the doctor-patient conversation is modeled as a question-answering system that tries to find responses to questions taking a clinical record as input. This approach can help to eliminate the need for manually structured patient records, as commonly used in previous studies, thereby expanding the pool of diverse virtual patients available. Leveraging these annotated corpora, we develop and assess an automatic system designed to answer medical dialogue questions posed by medical students to simulated patients in medical exams. Our approach demonstrates robust generalization, relying solely on medical records to generate new patient cases. The two datasets and the code will be freely available for the research community",
    "checked": true,
    "id": "b3e60ccbbcdace63eab1d0fb181a49ff41901df3",
    "semantic_title": "a virtual patient dialogue system based on question-answering on clinical records",
    "citation_count": 0,
    "authors": [
      "Janire Arana",
      "Mikel Idoyaga",
      "Maitane Urruela",
      "Elisa Espina",
      "Aitziber Atutxa Salazar",
      "Koldo Gojenola"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.183": {
    "title": "A Web Portal about the State of the Art of NLP Tasks in Spanish",
    "volume": "main",
    "abstract": "This paper presents a new web portal with information about the state of the art of natural language processing tasks in Spanish. It provides information about forums, competitions, tasks and datasets in Spanish, that would otherwise be spread in multiple articles and web sites. The portal consists of overview pages where information can be searched for and filtered by several criteria and individual pages with detailed information and hyperlinks to facilitate navigation. Information has been manually curated from publications that describe competitions and NLP tasks from 2013 until 2023 and will be updated as new tasks appear. A total of 185 tasks and 128 datasets from 94 competitions have been introduced",
    "checked": true,
    "id": "76404c8c62a46b10b0b55f300a72e6095f346a9b",
    "semantic_title": "a web portal about the state of the art of nlp tasks in spanish",
    "citation_count": 0,
    "authors": [
      "Enrique Amigó",
      "Jorge Carrillo-de-Albornoz",
      "Andrés Fernández",
      "Julio Gonzalo",
      "Guillermo Marco",
      "Roser Morante",
      "Laura Plaza",
      "Jacobo Pedrosa"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.184": {
    "title": "A Workflow for HTR-Postprocessing, Labeling and Classifying Diachronic and Regional Variation in Pre-Modern Slavic Texts",
    "volume": "main",
    "abstract": "We describe ongoing work for developing a workflow for the applied use case of classifying diachronic and regional language variation in Pre-Modern Slavic texts. The data were obtained via handwritten text recognition (HTR) on medieval manuscripts and printings and partly by manual transcription. Our goal is to develop a workflow for such historical language data, covering HTR-postprocessing, annotating and classifying the digitized texts. We test and adapt existing language resources to fit the pipeline with low-barrier tooling, accessible for Humanists with limited experience in research data infrastructures, computational analysis or advanced methods of natural language processing (NLP). The workflow starts by addressing ground truth (GT) data creation for diagnosing and correcting HTR errors via string metrics and data-driven methods. On GT and on HTR data, we subsequently show classification results using transfer learning on sentence-level text snippets. Next, we report on our token-level data labeling efforts. Each step of the workflow is complemented with describing current limitations and our corresponding work in progress",
    "checked": true,
    "id": "26809c6c770fc49c40b131afd337b4372653d48a",
    "semantic_title": "a workflow for htr-postprocessing, labeling and classifying diachronic and regional variation in pre-modern slavic texts",
    "citation_count": 0,
    "authors": [
      "Piroska Lendvai",
      "Maarten van Gompel",
      "Anna Jouravel",
      "Elena Renje",
      "Uwe Reichel",
      "Achim Rabus",
      "Eckhart Arnold"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.185": {
    "title": "A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks",
    "volume": "main",
    "abstract": "The recent emergence of Large Language Models (LLMs) has enabled significant advances in the field of Natural Language Processing (NLP). While these new models have demonstrated superior performance on various tasks, their application and potential are still underexplored, both in terms of the diversity of tasks they can handle and their domain of application. In this context, we evaluate four state-of-the-art instruction-tuned LLMs (ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca) on a set of 13 real-world clinical and biomedical NLP tasks in English, including named-entity recognition (NER), question-answering (QA), relation extraction (RE), and more. Our overall results show that these evaluated LLMs approach the performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, particularly excelling in the QA task, even though they have never encountered examples from these tasks before. However, we also observe that the classification and RE tasks fall short of the performance achievable with specifically trained models designed for the medical field, such as PubMedBERT. Finally, we note that no single LLM outperforms all others across all studied tasks, with some models proving more suitable for certain tasks than others",
    "checked": true,
    "id": "75059feaaec7dd0a8810d8f4ec6985f5d00b73d5",
    "semantic_title": "a zero-shot and few-shot study of instruction-finetuned large language models applied to clinical and biomedical tasks",
    "citation_count": 7,
    "authors": [
      "Yanis Labrak",
      "Mickael Rouvier",
      "Richard Dufour"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.186": {
    "title": "Backdoor NLP Models via AI-Generated Text",
    "volume": "main",
    "abstract": "Backdoor attacks pose a critical security threat to natural language processing (NLP) models by establishing covert associations between trigger patterns and target labels without affecting normal accuracy. Existing attacks usually disregard fluency and semantic fidelity of poisoned text, rendering the malicious data easily detectable. However, text generation models can produce coherent and content-relevant text given prompts. Moreover, potential differences between human-written and AI-generated text may be captured by NLP models while being imperceptible to humans. More insidious threats could arise if attackers leverage latent features of AI-generated text as trigger patterns. We comprehensively investigate backdoor attacks on NLP models using AI-generated poisoned text obtained via continued writing or paraphrasing, exploring three attack scenarios: data, model and pre-training. For data poisoning, we fine-tune generators with attribute control to enhance the attack performance. For model poisoning, we leverage downstream tasks to derive specialized generators. For pre-training poisoning, we train multiple attribute-based generators and align their generated text with pre-defined vectors, enabling task-agnostic migration attacks. Experiments demonstrate that our method achieves effective attacks while maintaining fluency and semantic similarity across all scenarios. We hope this work can raise awareness of the security risks hidden in AI-generated text",
    "checked": true,
    "id": "529817d61227f90160375ba9f8c15bc8d091518f",
    "semantic_title": "backdoor nlp models via ai-generated text",
    "citation_count": 0,
    "authors": [
      "Wei Du",
      "Tianjie Ju",
      "Ge Ren",
      "GaoLei Li",
      "Gongshen Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.187": {
    "title": "BalsuTalka.lv - Boosting the Common Voice Corpus for Low-Resource Languages",
    "volume": "main",
    "abstract": "Open speech corpora of substantial size are seldom available for less-spoken languages, and this was recently the case also for Latvian with its 1.5M native speakers. While there exist several closed Latvian speech corpora of 100+ hours, used to train competitive models for automatic speech recognition (ASR), there were only a few tiny open datasets available at the beginning of 2023, the 18-hour Latvian Common Voice 13.0 dataset being the largest one. In the result of a successful national crowdsourcing initiative, organised jointly by several institutions, the size and speaker diversity of the Latvian Common Voice 17.0 release have increased more than tenfold in less than a year. A successful follow-up initiative was also launched for Latgalian, which has been recognized as an endangered historic variant of Latvian with 150k speakers. The goal of these initiatives is not only to enlarge the datasets but also to make them more diverse in terms of speakers and accents, text genres and styles, intonations, grammar and lexicon. They have already become considerable language resources for both improving ASR and conducting linguistic research. Since we use the Mozilla Common Voice platform to record and validate speech samples, this paper focuses on (i) the selection of text snippets to enrich the language data and to stimulate various intonations, (ii) an indicative evaluation of the acquired corpus and the first ASR models fine-tuned on this data, (iii) our social campaigns to boost and maintain this initiative",
    "checked": true,
    "id": "7654f105a5f4d232ccf3c9fce809f1260d384ce9",
    "semantic_title": "balsutalka.lv - boosting the common voice corpus for low-resource languages",
    "citation_count": 0,
    "authors": [
      "Roberts Dargis",
      "Arturs Znotins",
      "Ilze Auzina",
      "Baiba Saulite",
      "Sanita Reinsone",
      "Raivis Dejus",
      "Antra Klavinska",
      "Normunds Gruzitis"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.188": {
    "title": "BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have achieved dramatic proficiency over NLP tasks with normal length. Recently, multiple studies have committed to extending the context length and enhancing the long text modeling capabilities of LLMs. To comprehensively evaluate the long context ability of LLMs, we propose BAMBOO, a multi-task long context benchmark. BAMBOO has been designed with four principles: comprehensive capacity evaluation, avoidance of data contamination, accurate automatic evaluation, and different length levels. It consists of 10 datasets from 5 different long text understanding tasks, i.e., question answering, hallucination detection, text sorting, language modeling, and code completion, to cover various domains and core capacities of LLMs. We conduct experiments with five widely-used long-context models and further discuss five key questions for long text research. In the end, we discuss problems of current long-context models and point out future directions for enhancing long text modeling capacities. We release our data, prompts, and code at https://anonymous.4open.science/r/BAMBOO/",
    "checked": true,
    "id": "42d83576ee920c1b6df318e212047d9ba57fc4fd",
    "semantic_title": "bamboo: a comprehensive benchmark for evaluating long text modeling capacities of large language models",
    "citation_count": 8,
    "authors": [
      "Zican Dong",
      "Tianyi Tang",
      "Junyi Li",
      "Wayne Xin Zhao",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.189": {
    "title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic Neural Graph Filtering",
    "volume": "main",
    "abstract": "Knowledge Graphs (KGs) have proven essential in information processing and reasoning applications because they link related entities and give context-rich information, supporting efficient information retrieval and knowledge discovery; presenting information flow in a very effective manner. Despite being widely used globally, Bangla is relatively underrepresented in KGs due to a lack of comprehensive datasets, encoders, NER (named entity recognition) models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient information processing and reasoning applications in the language. Addressing the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework that is able to automatically construct Bengali KGs from any Bangla text. We utilize multilingual LLMs to understand various languages and correlate entities and relations universally. By employing a translation dictionary to identify English equivalents and extracting word features from pre-trained BERT models, we construct the foundational KG. To reduce noise and align word embeddings with our goal, we employ graph-based polynomial filters. Lastly, we implement a GNN-based semantic filter, which elevates contextual understanding and trims unnecessary edges, culminating in the formation of the definitive KG. Empirical findings and case studies demonstrate the universal effectiveness of our model, capable of autonomously constructing semantically enriched KGs from any text. Data and code are available here: https://github.com/azminewasi/BanglaAutoKG",
    "checked": true,
    "id": "5a940f2ad582c1ce14e30a6be9185a35cbecd8e6",
    "semantic_title": "banglaautokg: automatic bangla knowledge graph construction with semantic neural graph filtering",
    "citation_count": 0,
    "authors": [
      "Azmine Toushik Wasi",
      "Taki Hasan Rafi",
      "Raima Islam",
      "Dong-Kyu Chae"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.190": {
    "title": "BAN-PL: A Polish Dataset of Banned Harmful and Offensive Content from Wykop.pl Web Service",
    "volume": "main",
    "abstract": "Since the Internet is flooded with hate, it is one of the main tasks for NLP experts to master automated online content moderation. However, advancements in this field require improved access to publicly available accurate and non-synthetic datasets of social media content. For the Polish language, such resources are very limited. In this paper, we address this gap by presenting a new open dataset of offensive social media content for the Polish language. The dataset comprises content from Wykop.pl, a popular online service often referred to as the Polish Reddit, reported by users and banned in the internal moderation process. It contains a total of 691,662 posts and comments, evenly divided into two categories: harmful and neutral (non-harmful). The anonymized subset of the BAN-PL dataset consisting on 24,000 pieces (12,000 for each class), along with preprocessing scripts have been made publicly available. Furthermore the paper offers valuable insights into real-life content moderation processes and delves into an analysis of linguistic features and content characteristics of the dataset. Moreover, a comprehensive anonymization procedure has been meticulously described and applied. The prevalent biases encountered in similar datasets, including post-moderation and pre-selection biases, are also discussed",
    "checked": true,
    "id": "3f825e0b46bb5a2290425e50a723a4e7343af16e",
    "semantic_title": "ban-pl: a polish dataset of banned harmful and offensive content from wykop.pl web service",
    "citation_count": 0,
    "authors": [
      "Anna Kolos",
      "Inez Okulska",
      "Kinga Głąbińska",
      "Agnieszka Karlinska",
      "Emilia Wisnios",
      "Paweł Ellerik",
      "Andrzej Prałat"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.191": {
    "title": "Barking up the Right Tree\", a GAN-Based Pun Generation Model through Semantic Pruning",
    "volume": "main",
    "abstract": "In the realm of artificial intelligence and linguistics, the automatic generation of humor, particularly puns, remains a complex task. This paper introduces an innovative approach that employs a Generative Adversarial Network (GAN) and semantic pruning techniques to generate humorous puns. We initiate our process by identifying potential pun candidates via semantic pruning. This is followed by the use of contrastive learning to decode the unique characteristics of puns, emphasizing both correct and incorrect interpretations. The learned features from contrastive learning are utilized within our GAN model to better capture the semantic nuances of puns. Specifically, the generator exploits the pruned semantic tree to generate pun texts, while the discriminator evaluates the generated puns, ensuring both linguistic correctness and humor. Evaluation results highlight our model's capacity to produce semantically coherent and humorous puns, demonstrating an enhancement over prior methods and approach human-level performance. This work contributes significantly to the field of computational humor, advancing the capabilities of automatic pun generation",
    "checked": true,
    "id": "0590e37908634f621afd9a2cf0c79bfaefa00335",
    "semantic_title": "barking up the right tree\", a gan-based pun generation model through semantic pruning",
    "citation_count": 0,
    "authors": [
      "JingJie Zeng",
      "Liang Yang",
      "Jiahao Kang",
      "Yufeng Diao",
      "Zhihao Yang",
      "Hongfei Lin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.192": {
    "title": "Basque and Spanish Counter Narrative Generation: Data Creation and Evaluation",
    "volume": "main",
    "abstract": "Counter Narratives (CNs) are non-negative textual responses to Hate Speech (HS) aiming at defusing online hatred and mitigating its spreading across media. Despite the recent increase in HS content posted online, research on automatic CN generation has been relatively scarce and predominantly focused on English. In this paper, we present CONAN-EUS, a new Basque and Spanish dataset for CN generation developed by means of Machine Translation (MT) and professional post-edition. Being a parallel corpus, also with respect to the original English CONAN, it allows to perform novel research on multilingual and crosslingual automatic generation of CNs. Our experiments on CN generation with mT5, a multilingual encoder-decoder model, shows that generation greatly benefits from training on post-edited data, as opposed to relying on silver MT data only. These results are confirmed by their correlation with a qualitative manual evaluation, demonstrating that manually revised training data remains crucial for the quality of the generated CNs. Furthermore, multilingual data augmentation improves results over monolingual settings for structurally similar languages such as English and Spanish, while being detrimental for Basque, a language isolate. Similar findings occur in zero-shot crosslingual evaluations, where model transfer (fine-tuning in English and generating in a different target language) outperforms fine-tuning mT5 on machine translated data for Spanish but not for Basque. This provides an interesting insight into the asymmetry in the multilinguality of generative models, a challenging topic which is still open to research. Data and code will be made publicly available upon publication",
    "checked": true,
    "id": "69c2170ae3a3ffb8788a645a77acb39a56c944ac",
    "semantic_title": "basque and spanish counter narrative generation: data creation and evaluation",
    "citation_count": 1,
    "authors": [
      "Jaione Bengoetxea",
      "Yi-Ling Chung",
      "Marco Guerini",
      "Rodrigo Agerri"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.193": {
    "title": "Becoming a High-Resource Language in Speech: The Catalan Case in the Common Voice Corpus",
    "volume": "main",
    "abstract": "Collecting voice resources for speech recognition systems is a multifaceted challenge, involving legal, technical, and diversity considerations. However, it is crucial to ensure fair access to voice-driven technology across diverse linguistic backgrounds. We describe an ongoing effort to create an extensive, high-quality, publicly available voice dataset for future development of speech technologies in Catalan through the Mozilla Common Voice crowd-sourcing platform. We detail the specific approaches used to address the challenges faced in recruiting contributors and managing the collection, validation, and recording of sentences. This detailed overview can serve as a source of guidance for similar initiatives across other projects and linguistic contexts. The success of this project is evident in the latest corpus release, version 16.1, where Catalan ranks as the most prominent language in the corpus, both in terms of recorded hours and when considering validated hours. This establishes Catalan as a language with significant speech resources for language technology development and significantly raises its international visibility",
    "checked": true,
    "id": "d6d69d8a8e8b77fc64fbc0308cb15e11fc2b8f07",
    "semantic_title": "becoming a high-resource language in speech: the catalan case in the common voice corpus",
    "citation_count": 0,
    "authors": [
      "Carme Armentano-Oller",
      "Montserrat Marimon",
      "Marta Villegas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.194": {
    "title": "BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language",
    "volume": "main",
    "abstract": "The BEIR dataset is a large, heterogeneous benchmark for Information Retrieval (IR), garnering considerable attention within the research community. However, BEIR and analogous datasets are predominantly restricted to English language. Our objective is to establish extensive large-scale resources for IR in the Polish language, thereby advancing the research in this NLP area. In this work, inspired by mMARCO and Mr. TyDi datasets, we translated all accessible open IR datasets into Polish, and we introduced the BEIR-PL benchmark – a new benchmark which comprises 13 datasets, facilitating further development, training and evaluation of modern Polish language models for IR tasks. We executed an evaluation and comparison of numerous IR models on the newly introduced BEIR-PL benchmark. Furthermore, we publish pre-trained open IR models for Polish language, marking a pioneering development in this field. The BEIR-PL is included in MTEB Benchmark and also available with trained models at URL https://huggingface.co/clarin-knext",
    "checked": true,
    "id": "820a0ebf6d482c38c66ae66d2d46694554945259",
    "semantic_title": "beir-pl: zero shot information retrieval benchmark for the polish language",
    "citation_count": 6,
    "authors": [
      "Konrad Wojtasik",
      "Kacper Wołowiec",
      "Vadim Shishkin",
      "Arkadiusz Janz",
      "Maciej Piasecki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.195": {
    "title": "Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have revolutionized the field of Natural Language Processing thanks to their ability to reuse knowledge acquired on massive text corpora on a wide variety of downstream tasks, with minimal (if any) tuning steps. At the same time, it has been repeatedly shown that LLMs lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution. In this work, we offer a systematic benchmarking of GPT-4, one of the most advanced LLMs available, on three algorithmic tasks characterized by the possibility to control the problem difficulty with two parameters. We compare the performance of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the Transformer-Encoder architecture recently introduced to solve similar tasks, the Neural Data Router. We find that the deployment of advanced prompting techniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating that state-of-the-art LLMs constitute a very strong baseline also in challenging tasks that require systematic generalization",
    "checked": true,
    "id": "0550e63749f0270b5ff7655b50a902f614cfb281",
    "semantic_title": "benchmarking gpt-4 on algorithmic problems: a systematic evaluation of prompting strategies",
    "citation_count": 1,
    "authors": [
      "Flavio Petruzzellis",
      "Alberto Testolin",
      "Alessandro Sperduti"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.196": {
    "title": "Benchmarking Hallucination in Large Language Models Based on Unanswerable Math Word Problem",
    "volume": "main",
    "abstract": "Large language models (LLMs) are highly effective in various natural language processing (NLP) tasks. However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination. This paper presents a new method for evaluating LLM hallucination in Question Answering (QA) based on the unanswerable math word problem (MWP). To support this approach, we innovatively develop a dataset called Unanswerable Math Word Problem (UMWP) which comprises 5200 questions across five categories. We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether LLM considers the question unanswerable. The results of extensive experiments conducted on 31 LLMs, including GPT-3, InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and reinforcement learning with human feedback (RLHF) training significantly enhance the model's ability to avoid hallucination. We show that utilizing MWP is a reliable and effective approach to assess hallucination. Our code and data are available at https://github.com/Yuki-Asuuna/UMWP",
    "checked": true,
    "id": "31968a970f14beab3cbadf9f6ad45c1a51f4ea95",
    "semantic_title": "benchmarking hallucination in large language models based on unanswerable math word problem",
    "citation_count": 0,
    "authors": [
      "YuHong Sun",
      "Zhangyue Yin",
      "Qipeng Guo",
      "Jiawen Wu",
      "Xipeng Qiu",
      "Hui Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.197": {
    "title": "Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT",
    "volume": "main",
    "abstract": "This paper explores the efficacy of large language models (LLMs) for Persian. While ChatGPT and consequent LLMs have shown remarkable performance in English, their efficiency for more low-resource languages remains an open question. We present the first comprehensive benchmarking study of LLMs across diverse Persian language tasks. Our primary focus is on GPT-3.5-turbo, but we also include GPT-4 and OpenChat-3.5 to provide a more holistic evaluation. Our assessment encompasses a diverse set of tasks categorized into classic, reasoning, and knowledge-based domains. To enable a thorough comparison, we evaluate LLMs against existing task-specific fine-tuned models. Given the limited availability of Persian datasets for reasoning tasks, we introduce two new benchmarks: one based on elementary school math questions and another derived from the entrance exams for 7th and 10th grades. Our findings reveal that while LLMs, especially GPT-4, excel in tasks requiring reasoning abilities and a broad understanding of general knowledge, they often lag behind smaller pretrained models fine-tuned specifically for particular tasks. Additionally, we observe improved performance when test sets are translated to English before inputting them into GPT-3.5. These results highlight the significant potential for enhancing LLM performance in the Persian language. This is particularly noteworthy due to the unique attributes of Persian, including its distinct alphabet and writing styles. We have made our codes, prompts, and data available here: https://github.com/Ipouyall/Benchmarking_ChatGPT_for_Persian",
    "checked": true,
    "id": "086c6252d9b0e81a541dbad3dc5ee1fbd01330b3",
    "semantic_title": "benchmarking large language models for persian: a preliminary study focusing on chatgpt",
    "citation_count": 0,
    "authors": [
      "Amirhossein Abaskohi",
      "Sara Baruni",
      "Mostafa Masoudi",
      "Nesa Abbasi",
      "Mohammad Hadi Babalou",
      "Ali Edalat",
      "Sepehr Kamahi",
      "Samin Mahdizadeh Sani",
      "Nikoo Naghavian",
      "Danial Namazifard",
      "Pouya Sadeghi",
      "Yadollah Yaghoobzadeh"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.198": {
    "title": "Benchmarking the Performance of Machine Translation Evaluation Metrics with Chinese Multiword Expressions",
    "volume": "main",
    "abstract": "To investigate the impact of Multiword Expressions (MWEs) on the fine-grained performance of the state-of-the-art metrics for Machine Translation Evaluation (MTE), we conduct experiments on the WMT22 Metrics Shared Task dataset with a preliminary focus on the Chinese-to-English language pair. We further annotate 28 types of Chinese MWEs on the source texts and then examine the performance of 31 MTE metrics on groups of sentences containing different MWEs. We have 3 interesting findings: 1) Machine Translation (MT) systems tend to perform worse on most Chinese MWE categories, confirming the previous claim that MWEs are a bottleneck of MT; 2) automatic metrics tend to overrate the translation of sentences containing MWEs; 3) most neural-network-based metrics perform better than string-overlap-based metrics. It concludes that both MT systems and MTE metrics still suffer from MWEs, suggesting richer annotation of data to facilitate MWE-aware automatic MTE and MT",
    "checked": true,
    "id": "a7f831b7eb94a77790571e73c92380a63ed587a0",
    "semantic_title": "benchmarking the performance of machine translation evaluation metrics with chinese multiword expressions",
    "citation_count": 0,
    "authors": [
      "Huacheng Song",
      "Hongzhi Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.199": {
    "title": "Benchmarking the Simplification of Dutch Municipal Text",
    "volume": "main",
    "abstract": "Text simplification (TS) makes written information more accessible to all people, especially those with cognitive or language impairments. Despite much progress in TS due to advances in NLP technology, the bottleneck issue of lack of data for low-resource languages persists. Dutch is one of these languages that lack a monolingual simplification corpus. In this paper, we use English as a pivot language for the simplification of Dutch medical and municipal text. We experiment with augmenting training data and corpus choice for this pivot-based approach. We compare the results to a baseline and an end-to-end LLM approach using the GPT 3.5 Turbo model. Our evaluation shows that, while we can substantially improve the results of the pivot pipeline, the zero-shot end-to-end GPT-based simplification performs better on all metrics. Our work shows how an existing pivot-based pipeline can be improved for simplifying Dutch medical text. Moreover, we provide baselines for the comparison in the domain of Dutch municipal text and make our corresponding evaluation dataset publicly available",
    "checked": true,
    "id": "9b30507f659b32196fbe780a80b6ca74e36d2763",
    "semantic_title": "benchmarking the simplification of dutch municipal text",
    "citation_count": 0,
    "authors": [
      "Daniel Vlantis",
      "Iva Gornishka",
      "Shuai Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.200": {
    "title": "BengaliLCP: A Dataset for Lexical Complexity Prediction in the Bengali Texts",
    "volume": "main",
    "abstract": "Encountering intricate or ambiguous terms within a sentence produces distress for the reader during comprehension. Lexical Complexity Prediction (LCP) deals with predicting the complexity score of a word or a phrase considering its context. This task poses several challenges including ambiguity, context sensitivity, and subjectivity in perceiving complexity. Despite having 300 million native speakers and ranking as the seventh most spoken language in the world, Bengali falls behind in the research on lexical complexity when compared to other languages. To bridge this gap, we introduce the first annotated Bengali dataset, that assists in performing the task of LCP in this language. Besides, we propose a transformer-based deep neural approach with a pairwise multi-head attention mechanism and LSTM model to predict the lexical complexity of Bengali tokens. The outcomes demonstrate that the proposed neural approach outperformed the existing state-of-the-art models for the Bengali language",
    "checked": true,
    "id": "2697b77e0e13ce566c4c3f42096e7c8681fb25d6",
    "semantic_title": "bengalilcp: a dataset for lexical complexity prediction in the bengali texts",
    "citation_count": 0,
    "authors": [
      "Nabila Ayman",
      "Md. Akram Hossain",
      "Abdul Aziz",
      "Rokan Uddin Faruqui",
      "Abu Nowshed Chy"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.201": {
    "title": "BenLLM-Eval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have emerged as one of the most important breakthroughs in natural language processing (NLP) for their impressive skills in language generation and other language-specific tasks. Though LLMs have been evaluated in various tasks, mostly in English, they have not yet undergone thorough evaluation in under-resourced languages such as Bengali (Bangla). To this end, this paper introduces BenLLM-Eval, which consists of a comprehensive evaluation of LLMs to benchmark their performance in the low-resourced Bangla language. In this regard, we select various important and diverse Bangla NLP tasks, such as text summarization, question answering, paraphrasing, natural language inference, text classification, and sentiment analysis for zero-shot evaluation of popular LLMs, namely, ChatGPT, LLaMA-2, and Claude-2. Our experimental results demonstrate that while in some Bangla NLP tasks, zero-shot LLMs could achieve performance on par, or even better than current SOTA fine-tuned models; in most tasks, their performance is quite poor (with the performance of open-source LLMs like LLaMA-2 being significantly bad) in comparison to the current SOTA results. Therefore, it calls for further efforts to develop a better understanding of LLMs in low-resource languages like Bangla",
    "checked": true,
    "id": "dd41f3d40f89c93ba866ba482c3c8c617a0b0bad",
    "semantic_title": "benllm-eval: a comprehensive evaluation into the potentials and pitfalls of large language models on bengali nlp",
    "citation_count": 2,
    "authors": [
      "Mohsinul Kabir",
      "Mohammed Saidul Islam",
      "Md Tahmid Rahman Laskar",
      "Mir Tafseer Nayeem",
      "M Saiful Bari",
      "Enamul Hoque"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.202": {
    "title": "BERT-BC: A Unified Alignment and Interaction Model over Hierarchical BERT for Response Selection",
    "volume": "main",
    "abstract": "Recently, we have witnessed a significant performance boosting for dialogue response selection task achieved by Cross-Encoder based models. However, such models directly feed the concatenation of context and response into the pre-trained model for interactive inference, ignoring the comprehensively independent representation modeling of context and response. Moreover, randomly sampling negative responses from other dialogue contexts is simplistic, and the learned models have poor generalization capability in realistic scenarios. In this paper, we propose a response selection model called BERT-BC that combines the representation-based Bi-Encoder and interaction-based Cross-Encoder. Three contrastive learning methods are devised for the Bi-Encoder to align context and response to obtain the better semantic representation. Meanwhile, according to the alignment difficulty of context and response semantics, the harder samples are dynamically selected from the same batch with negligible cost and sent to Cross-Encoder to enhance the model's interactive reasoning ability. Experimental results show that BERT-BC can achieve state-of-the-art performance on three benchmark datasets for multi-turn response selection",
    "checked": true,
    "id": "6bfe046de5c05a58161bac0d433c121ed38b7239",
    "semantic_title": "bert-bc: a unified alignment and interaction model over hierarchical bert for response selection",
    "citation_count": 0,
    "authors": [
      "Zhenfei Yang",
      "Beiming Yu",
      "Yuan Cui",
      "Shi Feng",
      "Daling Wang",
      "Yifei Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.203": {
    "title": "Beyond Binary: Towards Embracing Complexities in Cyberbullying Detection and Intervention - a Position Paper",
    "volume": "main",
    "abstract": "In the digital age, cyberbullying (CB) poses a significant concern, impacting individuals as early as primary school and leading to severe or lasting consequences, including an increased risk of self-harm. CB incidents, are not limited to bullies and victims, but include bystanders with various roles, and usually have numerous sub-categories and variations of online harms. This position paper emphasises the complexity of CB incidents by drawing on insights from psychology, social sciences, and computational linguistics. While awareness of CB complexities is growing, existing computational techniques tend to oversimplify CB as a binary classification task, often relying on training datasets that capture peripheries of CB behaviours. Inconsistent definitions and categories of CB-related online harms across various platforms further complicates the issue. Ethical concerns arise when CB research involves children to role-play CB incidents to curate datasets. Through multi-disciplinary collaboration, we propose strategies for consideration when developing CB detection systems. We present our position on leveraging large language models (LLMs) such as Claude-2 and Llama2-Chat as an alternative approach to generate CB-related role-playing datasets. Our goal is to assist researchers, policymakers, and online platforms in making informed decisions regarding the automation of CB incident detection and intervention. By addressing these complexities, our research contributes to a more nuanced and effective approach to combating CB especially in young people",
    "checked": true,
    "id": "c700057b5768836e49da6720633b94a34c70b782",
    "semantic_title": "beyond binary: towards embracing complexities in cyberbullying detection and intervention - a position paper",
    "citation_count": 0,
    "authors": [
      "Kanishk Verma",
      "Kolawole John Adebayo",
      "Joachim Wagner",
      "Megan Reynolds",
      "Rebecca Umbach",
      "Tijana Milosevic",
      "Brian Davis"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.204": {
    "title": "Beyond Canonical Fine-tuning: Leveraging Hybrid Multi-Layer Pooled Representations of BERT for Automated Essay Scoring",
    "volume": "main",
    "abstract": "The challenging yet relevant task of automated essay scoring (AES) continuously gains attention from multiple disciplines over the years. With the advent of pre-trained large language models such as BERT, fine-tuning those models has become the dominant technique in various natural language processing (NLP) tasks. Several studies fine-tune BERT for the AES task but only utilize the final pooled output from its last layer. With BERT's multi-layer architecture that encodes hierarchical linguistic information, we believe we can improve overall essay scoring performance by leveraging information from its intermediate layers. In this study, we diverge from the canonical fine-tuning paradigm by exploring different combinations of model outputs and single- and multi-layer pooling strategies, as well as architecture modifications to the task-specific component of the model. Using a hybrid pooling strategy, experimental results show that our best essay representa- tion combined with a simple architectural modification outperforms the average QWK score of the basic fine-tuned BERT with default output on the ASAP AES dataset, suggesting its effectiveness for the AES task and potentially other long-text tasks",
    "checked": true,
    "id": "20d829a722c62958a55dd59b1a6650439433ae2b",
    "semantic_title": "beyond canonical fine-tuning: leveraging hybrid multi-layer pooled representations of bert for automated essay scoring",
    "citation_count": 0,
    "authors": [
      "Eujene Nikka V. Boquio",
      "Prospero C. Naval, Jr."
    ]
  },
  "https://aclanthology.org/2024.lrec-main.205": {
    "title": "Beyond Code: Evaluate Thought Steps for Complex Code Generation",
    "volume": "main",
    "abstract": "Code generation aims to generate code in a general-purpose programming language, such as C++, based on natural language intents. Existing efforts primarily focus on relatively simple programming problems and fail to evaluate the thought process involved in complex programming scenarios. In this paper, we introduce \"steps-guided code generation,\" a task that assesses the quality of both thought steps and code implementation to evaluate the overall management of handling a complex programming problem. To support this task, we construct CodeStepsEval, a real-world scenario dataset of complex programming problems in the C++ programming language with varying levels of difficulty. Comprehensive experiments on this dataset demonstrate the importance of high-quality steps in enhancing code generation performance and the challenges faced by the code LLMs in this task",
    "checked": true,
    "id": "5546211e872bb1c5b3561a01741ebfb2e21df778",
    "semantic_title": "beyond code: evaluate thought steps for complex code generation",
    "citation_count": 0,
    "authors": [
      "Liuwen Cao",
      "Yi Cai",
      "Jiexin Wang",
      "Hongkui He",
      "Hailin Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.206": {
    "title": "Beyond Full Fine-tuning: Harnessing the Power of LoRA for Multi-Task Instruction Tuning",
    "volume": "main",
    "abstract": "Low-Rank Adaptation (LoRA) is a widespread parameter-efficient fine-tuning algorithm for large-scale language models. It has been commonly accepted that LoRA mostly achieves promising results in single-task, low-resource settings, and struggles to handle multi-task instruction tuning scenarios. In this paper, we conduct a systematic study of LoRA on diverse tasks and rich resources with different learning capacities, examining its performance on seen tasks during training and its cross-task generalization on unseen tasks. Our findings challenge the prevalent assumption that the limited learning capacity will inevitably result in performance decline. In fact, our study reveals that when configured with an appropriate rank, LoRA can achieve remarkable performance in high-resource and multi-task scenarios, even comparable to that achieved through full fine-tuning. It turns out that the constrained learning capacity encourages LoRA to prioritize conforming to instruction requirements rather than memorizing specialized features of particular tasks or instances. This study reveals the underlying connection between learning capacity and generalization capabilities for robust parameter-efficient fine-tuning, highlighting a promising direction for the broader application of LoRA across various tasks and settings",
    "checked": true,
    "id": "a587befe1e66a05ac6a3e3fbd7093298e138a873",
    "semantic_title": "beyond full fine-tuning: harnessing the power of lora for multi-task instruction tuning",
    "citation_count": 0,
    "authors": [
      "Chunlei Xin",
      "Yaojie Lu",
      "Hongyu Lin",
      "Shuheng Zhou",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Zhongyi Liu",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.207": {
    "title": "Beyond Linguistic Cues: Fine-grained Conversational Emotion Recognition via Belief-Desire Modelling",
    "volume": "main",
    "abstract": "Emotion recognition in conversation (ERC) is essential for dialogue systems to identify the emotions expressed by speakers. Although previous studies have made significant progress, accurate recognition and interpretation of similar fine-grained emotion properly accounting for individual variability remains a challenge. One particular under-explored area is the role of individual beliefs and desires in modelling emotion. Inspired by the Belief-Desire Theory of Emotion, we propose a novel method for conversational emotion recognition that incorporates both belief and desire to accurately identify emotions. We extract emotion-eliciting events from utterances and construct graphs that represent beliefs and desires in conversations. By applying message passing between nodes, our graph effectively models the utterance context, speaker's global state, and the interaction between emotional beliefs, desires, and utterances. We evaluate our model's performance by conducting extensive experiments on four popular ERC datasets and comparing it with multiple state-of-the-art models. The experimental results demonstrate the superiority of our proposed model and validate the effectiveness of each module in the model",
    "checked": true,
    "id": "40c514a2fd615a7747bb646ddc0802285fc04617",
    "semantic_title": "beyond linguistic cues: fine-grained conversational emotion recognition via belief-desire modelling",
    "citation_count": 0,
    "authors": [
      "Bo Xu",
      "Longjiao Li",
      "Wei Luo",
      "Mehdi Naseriparsa",
      "Zhehuan Zhao",
      "Hongfei Lin",
      "Feng Xia"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.208": {
    "title": "Beyond Model Performance: Can Link Prediction Enrich French Lexical Graphs?",
    "volume": "main",
    "abstract": "This paper presents a resource-centric study of link prediction approaches over French lexical-semantic graphs. Our study incorporates two graphs, RezoJDM16k and RL-fr, and we evaluated seven link prediction models, with CompGCN-ConvE emerging as the best performer. We also conducted a qualitative analysis of the predictions using manual annotations. Based on this, we found that predictions with higher confidence scores were more valid for inclusion. Our findings highlight different benefits for the dense graph compared to the sparser graph RL-fr. While the addition of new triples to RezoJDM16k offers limited advantages, RL-fr can benefit substantially from our approach",
    "checked": true,
    "id": "9ee238b125d52b8695a4fc105089999716fd5e1c",
    "semantic_title": "beyond model performance: can link prediction enrich french lexical graphs?",
    "citation_count": 0,
    "authors": [
      "Hee-Soo Choi",
      "Priyansh Trivedi",
      "Mathieu Constant",
      "Karen Fort",
      "Bruno Guillaume"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.209": {
    "title": "Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities",
    "volume": "main",
    "abstract": "With the rise of Large Language Models (LLMs), AI assistants' ability to utilize tools, especially through API calls, has advanced notably. This progress has necessitated more accurate evaluation methods. Many existing studies adopt static evaluation, where they assess AI assistants' API call based on pre-defined dialogue histories. However, such evaluation method can be misleading, as an AI assistant might fail in generating API calls from preceding human interaction in real cases. Instead of the resource-intensive method of direct human-machine interactions, we propose Automated Dynamic Evaluation (AutoDE) to assess an assistant's API call capability without human involvement. In our framework, we endeavor to closely mirror genuine human conversation patterns in human-machine interactions, using a LLM-based user agent, equipped with a user script to ensure human alignment. Experimental results highlight that AutoDE uncovers errors overlooked by static evaluations, aligning more closely with human assessment. Testing four AI assistants using our crafted benchmark, our method further mirrored human evaluation compared to conventional static evaluations",
    "checked": true,
    "id": "9f7db53338c8e76d708faf1a126e7b43331a57b2",
    "semantic_title": "beyond static evaluation: a dynamic approach to assessing ai assistants' api invocation capabilities",
    "citation_count": 0,
    "authors": [
      "Honglin Mu",
      "Yang Xu",
      "Yunlong Feng",
      "Xiaofeng Han",
      "Yitong Li",
      "Yutai Hou",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.210": {
    "title": "Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection",
    "volume": "main",
    "abstract": "Out-of-domain (OOD) intent detection aims to examine whether the user's query falls outside the predefined domain of the system, which is crucial for the proper functioning of task-oriented dialogue (TOD) systems. Previous methods address it by fine-tuning discriminative models. Recently, some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, but it is still unclear for their ability on OOD detection task.This paper conducts a comprehensive evaluation of LLMs under various experimental settings, and then outline the strengths and weaknesses of LLMs. We find that LLMs exhibit strong zero-shot and few-shot capabilities, but is still at a disadvantage compared to models fine-tuned with full resource. More deeply, through a series of additional analysis experiments, we discuss and summarize the challenges faced by LLMs and provide guidance for future work including injecting domain knowledge, strengthening knowledge transfer from IND(In-domain) to OOD, and understanding long instructions",
    "checked": true,
    "id": "585414ccf796388803fad8583b30efa8383d3354",
    "semantic_title": "beyond the known: investigating llms performance on out-of-domain intent detection",
    "citation_count": 0,
    "authors": [
      "Pei Wang",
      "Keqing He",
      "Yejie Wang",
      "Xiaoshuai Song",
      "Yutao Mou",
      "Jingang Wang",
      "Yunsen Xian",
      "Xunliang Cai",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.211": {
    "title": "Beyond Words: Decoding Facial Expression Dynamics in Motivational Interviewing",
    "volume": "main",
    "abstract": "Authors : Nezih Younsi, Catherine Pelachaud, Laurence Chaby Title : Beyond Words: Decoding Facial Expression Dynamics in Motivational Interviewing Abstract : This paper focuses on studying the facial expressions of both client and therapist in the context of Motivational Interviewing (MI). The annotation system Motivational Interview Skill Code MISC defines three types of talk, namely sustain, change, and neutral for the client and information, question, or reflection for the therapist. Most studies on MI look at the verbal modality. Our research aims to understand the variation and dynamics of facial expressions of both interlocutors over a counseling session. We apply a sequence mining algorithm to identify categories of facial expressions for each type. Using co-occurrence analysis, we derive the correlation between the facial expressions and the different types of talk, as well as the interplay between interlocutors' expressions",
    "checked": true,
    "id": "a94140f2a924e97daf8f6778bf94c814b1e8754c",
    "semantic_title": "beyond words: decoding facial expression dynamics in motivational interviewing",
    "citation_count": 0,
    "authors": [
      "Nezih Younsi",
      "Catherine Pelachaud",
      "Laurence Chaby"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.212": {
    "title": "BigNLI: Native Language Identification with Big Bird Embeddings",
    "volume": "main",
    "abstract": "Native Language Identification (NLI) intends to classify an author's native language based on their writing in another language. Historically, the task has heavily relied on time-consuming linguistic feature engineering, and NLI transformer models have thus far failed to offer effective, practical alternatives. The current work shows input size is a limiting factor, and that classifiers trained using Big Bird embeddings outperform linguistic feature engineering models (for which we reproduce previous work) by a large margin on the Reddit-L2 dataset. Additionally, we provide further insight into input length dependencies, show consistent out-of-sample (Europe subreddit) and out-of-domain (TOEFL-11) performance, and qualitatively analyze the embedding space. Given the effectiveness and computational efficiency of this method, we believe it offers a promising avenue for future NLI work",
    "checked": true,
    "id": "c58c26e6703fdb33db62f2380566900308931692",
    "semantic_title": "bignli: native language identification with big bird embeddings",
    "citation_count": 0,
    "authors": [
      "Sergey Kramp",
      "Giovanni Cassani",
      "Chris Emmery"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.213": {
    "title": "Biomedical Concept Normalization over Nested Entities with Partial UMLS Terminology in Russian",
    "volume": "main",
    "abstract": "We present a new manually annotated dataset of PubMed abstracts for concept normalization in Russian. It contains over 23,641 entity mentions in 756 documents linked to 4,544 unique concepts from the UMLS ontology. Compared to existing corpora, we explore two novel annotation characteristics: the nestedness of named entities and the incompleteness of the Russian medical terminology in UMLS. 4,424 entity mentions are linked to 1,535 unique English concepts absent in the Russian part of the UMLS ontology. We present several baselines for normalization over nested named entities obtained with state-of-the-art models such as SapBERT. Our experimental results show that models pre-trained on graph structural data from UMLS achieve superior performance in a zero-shot setting on bilingual terminology",
    "checked": true,
    "id": "b56afae99dc36728c3ade381161d98c788eec356",
    "semantic_title": "biomedical concept normalization over nested entities with partial umls terminology in russian",
    "citation_count": 0,
    "authors": [
      "Natalia Loukachevitch",
      "Andrey Sakhovskiy",
      "Elena Tutubalina"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.214": {
    "title": "Biomedical Entity Linking as Multiple Choice Question Answering",
    "volume": "main",
    "abstract": "Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms state-of-the-art baselines on several datasets",
    "checked": true,
    "id": "38a8d0e5fa83dc823744beab08a62511ad9ad227",
    "semantic_title": "biomedical entity linking as multiple choice question answering",
    "citation_count": 0,
    "authors": [
      "Zhenxi Lin",
      "Ziheng Zhang",
      "Xian Wu",
      "Yefeng Zheng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.215": {
    "title": "Bits and Pieces: Investigating the Effects of Subwords in Multi-task Parsing across Languages and Domains",
    "volume": "main",
    "abstract": "Neural parsing is very dependent on the underlying language model. However, very little is known about how choices in the language model affect parsing performance, especially in multi-task learning. We investigate questions on how the choice of subwords affects parsing, how subword sharing is responsible for gains or negative transfer in a multi-task setting where each task is parsing of a specific domain of the same language. More specifically, we investigate these issues across four languages: English, German, Italian, and Turkish. We find a general preference for averaged or last subwords across languages and domains. However, specific POS tags may require different subwords, and the distributional overlap between subwords across domains is perhaps a more influential factor in determining positive or negative transfer than discrepancies in the data sizes",
    "checked": true,
    "id": "7484529d793bb6eee5f082c7db70372226eb9428",
    "semantic_title": "bits and pieces: investigating the effects of subwords in multi-task parsing across languages and domains",
    "citation_count": 0,
    "authors": [
      "Daniel Dakota",
      "Sandra Kübler"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.216": {
    "title": "BiVert: Bidirectional Vocabulary Evaluation Using Relations for Machine Translation",
    "volume": "main",
    "abstract": "Neural machine translation (NMT) has progressed rapidly in the past few years, promising improvements and quality translations for different languages. Evaluation of this task is crucial to determine the quality of the translation. Overall, insufficient emphasis is placed on the actual sense of the translation in traditional methods. We propose a bidirectional semantic-based evaluation method designed to assess the sense distance of the translation from the source text. This approach employs the comprehensive multilingual encyclopedic dictionary BabelNet. Through the calculation of the semantic distance between the source and its back translation of the output, our method introduces a quantifiable approach that empowers sentence comparison on the same linguistic level. Factual analysis shows a strong correlation between the average evaluation scores generated by our method and the human assessments across various machine translation systems for English-German language pair. Finally, our method proposes a new multilingual approach to rank MT systems without the need for parallel corpora",
    "checked": true,
    "id": "5e2c4264bea7638d01c2d65e7893383154989c67",
    "semantic_title": "bivert: bidirectional vocabulary evaluation using relations for machine translation",
    "citation_count": 0,
    "authors": [
      "Carinne Cherf",
      "Yuval Pinter"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.217": {
    "title": "BKEE: Pioneering Event Extraction in the Vietnamese Language",
    "volume": "main",
    "abstract": "Event Extraction (EE) is a fundamental task in information extraction, aimed at identifying events and their associated arguments within textual data. It holds significant importance in various applications and serves as a catalyst for the development of related tasks. Despite the availability of numerous datasets and methods for event extraction in various languages, there has been a notable absence of a dedicated dataset for the Vietnamese language. To address this limitation, we propose BKEE, a novel event extraction dataset for Vietnamese. BKEE encompasses over 33 distinct event types and 28 different event argument roles, providing a labeled dataset for entity mentions, event mentions, and event arguments on 1066 documents. Additionally, we establish robust baselines for potential downstream tasks on this dataset, facilitating the analysis of challenges and future development prospects in the field of Vietnamese event extraction",
    "checked": true,
    "id": "02dbbfa962dbe72addee1502ffa5ba59952bd478",
    "semantic_title": "bkee: pioneering event extraction in the vietnamese language",
    "citation_count": 0,
    "authors": [
      "Thi-Nhung Nguyen",
      "Bang Tien Tran",
      "Trong-Nghia Luu",
      "Thien Huu Nguyen",
      "Kiem-Hieu Nguyen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.218": {
    "title": "BlendX: Complex Multi-Intent Detection with Blended Patterns",
    "volume": "main",
    "abstract": "Task-oriented dialogue (TOD) systems are commonly designed with the presumption that each utterance represents a single intent. However, this assumption may not accurately reflect real-world situations, where users frequently express multiple intents within a single utterance. While there is an emerging interest in multi-intent detection (MID), existing in-domain datasets such as MixATIS and MixSNIPS have limitations in their formulation. To address these issues, we present BlendX, a suite of refined datasets featuring more diverse patterns than their predecessors, elevating both its complexity and diversity. For dataset construction, we utilize both rule-based heuristics as well as a generative tool—OpenAI's ChatGPT—which is augmented with a similarity-driven strategy for utterance selection. To ensure the quality of the proposed datasets, we also introduce three novel metrics that assess the statistical properties of an utterance related to word count, conjunction use, and pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art MID models struggle with the challenges posed by the new datasets, highlighting the need to reexamine the current state of the MID field. The dataset is available at https://github.com/HYU-NLP/BlendX",
    "checked": true,
    "id": "1a98d909b65ab7e0034e28ed304a65b5cd910687",
    "semantic_title": "blendx: complex multi-intent detection with blended patterns",
    "citation_count": 0,
    "authors": [
      "Yejin Yoon",
      "Jungyeon Lee",
      "Kangsan Kim",
      "Chanhee Park",
      "Taeuk Kim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.219": {
    "title": "BLN600: A Parallel Corpus of Machine/Human Transcribed Nineteenth Century Newspaper Texts",
    "volume": "main",
    "abstract": "We present a publicly available corpus of nineteenth-century newspaper text focused on crime in London, derived from the Gale British Library Newspapers corpus parts 1 and 2. The corpus comprises 600 newspaper excerpts and for each excerpt contains the original source image, the machine transcription of that image as found in the BLN and a gold standard manual transcription that we have created. We envisage the corpus will be helpful for the training and development of OCR and post-OCR correction methodologies for historical newspaper machine transcription—for which there is currently a dearth of publicly available resources. In this paper, we discuss the rationale behind gathering such a corpus, the methodology used to select, process, and align the data, and the corpus' potential utility for historians and digital humanities researchers—particularly within the realms of neural machine translation-based post-OCR correction approaches, and other natural language processing tasks that are critically affected by erroneous OCR",
    "checked": true,
    "id": "d7095d0f92f374b18c88aec01c3f486cd7c5fe32",
    "semantic_title": "bln600: a parallel corpus of machine/human transcribed nineteenth century newspaper texts",
    "citation_count": 0,
    "authors": [
      "Callum William Booth",
      "Alan Thomas",
      "Robert Gaizauskas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.220": {
    "title": "Bootstrapping UMR Annotations for Arapaho from Language Documentation Resources",
    "volume": "main",
    "abstract": "Uniform Meaning Representation (UMR) is a semantic labeling system in the AMR family designed to be uniformly applicable to typologically diverse languages. The UMR labeling system is quite thorough and can be time-consuming to execute, especially if annotators are starting from scratch. In this paper, we focus on methods for bootstrapping UMR annotations for a given language from existing resources, and specifically from typical products of language documentation work, such as lexical databases and interlinear glossed text (IGT). Using Arapaho as our test case, we present and evaluate a bootstrapping process that automatically generates UMR subgraphs from IGT. Additionally, we describe and evaluate a method for bootstrapping valency lexicon entries from lexical databases for both the target language and English. We are able to generate enough basic structure in UMR graphs from the existing Arapaho interlinearized texts to automate UMR labeling to a significant extent. Our method thus has the potential to streamline the process of building meaning representations for new languages without existing large-scale computational resources",
    "checked": true,
    "id": "73e7067194c617b98441a8d080a7d79b2b97f4b8",
    "semantic_title": "bootstrapping umr annotations for arapaho from language documentation resources",
    "citation_count": 1,
    "authors": [
      "Matthew J. Buchholz",
      "Julia Bonn",
      "Claire Benet Post",
      "Andrew Cowell",
      "Alexis Palmer"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.221": {
    "title": "BootTOD: Bootstrap Task-oriented Dialogue Representations by Aligning Diverse Responses",
    "volume": "main",
    "abstract": "Pre-trained language models have been successful in many scenarios. However, their usefulness in task-oriented dialogues is limited due to the intrinsic linguistic differences between general text and task-oriented dialogues. Current task-oriented dialogue pre-training methods rely on a contrastive framework, which faces challenges such as selecting true positives and hard negatives, as well as lacking diversity. In this paper, we propose a novel dialogue pre-training model called BootTOD. It learns task-oriented dialogue representations via a self-bootstrapping framework. Unlike contrastive counterparts, BootTOD aligns context and context+response representations and dismisses the requirements of contrastive pairs. BootTOD also uses multiple appropriate response targets to model the intrinsic one-to-many diversity of human conversations. Experimental results show that BootTOD outperforms strong TOD baselines on diverse downstream dialogue tasks",
    "checked": true,
    "id": "430d786b97b32c482c80a6f978b0498690005b81",
    "semantic_title": "boottod: bootstrap task-oriented dialogue representations by aligning diverse responses",
    "citation_count": 0,
    "authors": [
      "Weihao Zeng",
      "Keqing He",
      "Yejie Wang",
      "Dayuan Fu",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.222": {
    "title": "Born a BabyNet with Hierarchical Parental Supervision for End-to-End Text Image Machine Translation",
    "volume": "main",
    "abstract": "Text image machine translation (TIMT) aims at translating source language texts in images into another target language, which has been proven successful by bridging text image recognition encoder and text translation decoder. However, it is still an open question of how to incorporate fine-grained knowledge supervision to make it consistent between recognition and translation modules. In this paper, we propose a novel TIMT method named as BabyNet, which is optimized with hierarchical parental supervision to improve translation performance. Inspired by genetic recombination and variation in the field of genetics, the proposed BabyNet is inherited from the recognition and translation parent models with a variation module of which parameters can be updated when training on the TIMT task. Meanwhile, hierarchical and multi-granularity supervision from parent models is introduced to bridge the gap between inherited modules in BabyNet. Extensive experiments on both synthetic and real-world TIMT tests show that our proposed method significantly outperforms existing methods. Further analyses of various parent model combinations show the good generalization of our method",
    "checked": true,
    "id": "f58f12931b2295aa1a4facf13c2420035ad5c58c",
    "semantic_title": "born a babynet with hierarchical parental supervision for end-to-end text image machine translation",
    "citation_count": 0,
    "authors": [
      "Cong Ma",
      "Yaping Zhang",
      "Zhiyang Zhang",
      "Yupu Liang",
      "Yang Zhao",
      "Yu Zhou",
      "Chengqing Zong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.223": {
    "title": "BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue Generation",
    "volume": "main",
    "abstract": "Medical dialogue generation (MDG) has gained increasing attention due to its substantial practical value. Previous works typically employ a sequence-to-sequence framework to generate medical responses by modeling dialogue context as sequential text with annotated medical entities. While these methods have been successful in generating fluent responses, they fail to provide process explanations of reasoning and require extensive entity annotation. To address these limitations, we propose the method Bootstrap Prompting for Explicit Reasoning in MDG (BP4ER), which explicitly model MDG's multi-step reasoning process and iteratively enhance this reasoning process. We employ a least-to-most prompting strategy to guide a large language model (LLM) in explicit reasoning, breaking down MDG into simpler sub-questions. These sub-questions build on answers from previous ones. Additionally, we also introduce two distinct bootstrapping techniques for prompting, which autonomously correct errors and facilitate the LLM's explicit reasoning. This approach eliminates the need for entity annotation and increases the transparency of the MDG process by explicitly generating the intermediate reasoning chain. Experimental results on the two publicly datasets show that BP4ER outperforms state-of-the-art methods across both objective and subjective evaluation",
    "checked": true,
    "id": "19f49c9d10724ab7b90cd3c8a78b7e14446de297",
    "semantic_title": "bp4er: bootstrap prompting for explicit reasoning in medical dialogue generation",
    "citation_count": 0,
    "authors": [
      "Yuhong He",
      "Yongqi Zhang",
      "Shizhu He",
      "Jun Wan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.224": {
    "title": "Breakthrough from Nuance and Inconsistency: Enhancing Multimodal Sarcasm Detection with Context-Aware Self-Attention Fusion and Word Weight Calculation",
    "volume": "main",
    "abstract": "Multimodal sarcasm detection has received considerable attention due to its unique role in social networks. Existing methods often rely on feature concatenation to fuse different modalities or model the inconsistencies among modalities. However, sarcasm is often embodied in local and momentary nuances in a subtle way, which causes difficulty for sarcasm detection. To effectively incorporate these nuances, this paper presents Context-Aware Self-Attention Fusion (CAAF) to integrate local and momentary multimodal information into specific words. Furthermore, due to the instantaneous nature of sarcasm, the connotative meanings of words post-multimodal integration generally deviate from their denotative meanings. Therefore, Word Weight Calculation (WWC) is presented to compute the weight of specific words based on CAAF's fusion nuances, illustrating the inconsistency between connotation and denotation. We evaluate our method on the MUStARD dataset, achieving an accuracy of 76.9 and an F1 score of 76.1, which surpasses the current state-of-the-art IWAN model by 1.7 and 1.6 respectively",
    "checked": true,
    "id": "445c004abd0035dc5109c26c939ae12f02c37330",
    "semantic_title": "breakthrough from nuance and inconsistency: enhancing multimodal sarcasm detection with context-aware self-attention fusion and word weight calculation",
    "citation_count": 0,
    "authors": [
      "Hongfei Xue",
      "Linyan Xu",
      "Yu Tong",
      "Rui Li",
      "Jiali Lin",
      "Dazhi Jiang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.225": {
    "title": "Bridging Computational Lexicography and Corpus Linguistics: A Query Extension for OntoLex-FrAC",
    "volume": "main",
    "abstract": "OntoLex, the dominant community standard for machine-readable lexical resources in the context of RDF, Linked Data and Semantic Web technologies, is currently extended with a designated module for Frequency, Attestations and Corpus-based Information (OntoLex-FrAC). We propose a novel component for OntoLex-FrAC, addressing the incorporation of corpus queries for (a) linking dictionaries with corpus engines, (b) enabling RDF-based web services to exchange corpus queries and responses data dynamically, and (c) using conventional query languages to formalize the internal structure of collocations, word sketches, and colligations. The primary field of application of the query extension is in digital lexicography and corpus linguistics, and we present a proof-of-principle implementation in backend components of a novel platform designed to support digital lexicography for the Serbian language",
    "checked": true,
    "id": "c1b6ad27b248aa364b89b5bb98f4e9d03e1e8349",
    "semantic_title": "bridging computational lexicography and corpus linguistics: a query extension for ontolex-frac",
    "citation_count": 0,
    "authors": [
      "Christian Chiarcos",
      "Ranka Stanković",
      "Maxim Ionov",
      "Gilles Sérasset"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.226": {
    "title": "Bridging Textual and Tabular Worlds for Fact Verification: A Lightweight, Attention-Based Model",
    "volume": "main",
    "abstract": "FEVEROUS is a benchmark and research initiative focused on fact extraction and verification tasks involving unstructured text and structured tabular data. In FEVEROUS, existing works often rely on extensive preprocessing and utilize rule-based transformations of data, leading to potential context loss or misleading encodings. This paper introduces a simple yet powerful model that nullifies the need for modality conversion, thereby preserving the original evidence's context. By leveraging pre-trained models on diverse text and tabular datasets and by incorporating a lightweight attention-based mechanism, our approach efficiently exploits latent connections between different data types, thereby yielding comprehensive and reliable verdict predictions. The model's modular structure adeptly manages multi-modal information, ensuring the integrity and authenticity of the original evidence are uncompromised. Comparative analyses reveal that our approach exhibits competitive performance, aligning itself closely with top-tier models on the FEVEROUS benchmark",
    "checked": true,
    "id": "851383ef24b1c342dfc5ad58e1e55247b2dd5a79",
    "semantic_title": "bridging textual and tabular worlds for fact verification: a lightweight, attention-based model",
    "citation_count": 0,
    "authors": [
      "Shirin Dabbaghi Varnosfaderani",
      "Canasai Kruengkrai",
      "Ramin Yahyapour",
      "Junichi Yamagishi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.227": {
    "title": "Bridging the Code Gap: A Joint Learning Framework across Medical Coding Systems",
    "volume": "main",
    "abstract": "Automated Medical Coding (AMC) is the task of automatically converting free-text medical documents into predefined codes according to a specific medical coding system. Although deep learning has significantly advanced AMC, the class imbalance problem remains a significant challenge. To address this issue, most existing methods consider only a single coding system and disregard the potential benefits of reflecting the relevance between different coding systems. To bridge this gap, we introduce a Joint learning framework for Across Medical coding Systems (JAMS), which jointly learns different coding systems through multi-task learning. It learns various representations using a shared encoder and explicitly captures the relationships across these coding systems using the medical code attention network, a modification of the graph attention network. In the experiments on the MIMIC-IV ICD-9 and MIMIC-IV ICD-10 datasets, connected through General Equivalence Mappings, JAMS improved the performance consistently regardless of the backbone models. This result demonstrates its model-agnostic characteristic, which is not constrained by specific model structures. Notably, JAMS significantly improved the performance of low-frequency codes. Our analysis shows that these performance gains are due to the connections between the codes of the different coding systems",
    "checked": true,
    "id": "58556addb3d71fcfc79579e229f82aa73facf003",
    "semantic_title": "bridging the code gap: a joint learning framework across medical coding systems",
    "citation_count": 0,
    "authors": [
      "Geunyeong Jeong",
      "Seokwon Jeong",
      "Juoh Sun",
      "Harksoo Kim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.228": {
    "title": "Bring Invariant to Variant: A Contrastive Prompt-based Framework for Temporal Knowledge Graph Forecasting",
    "volume": "main",
    "abstract": "Temporal knowledge graph forecasting aims to reason over known facts to complete the missing links in the future. Existing methods are highly dependent on the structures of temporal knowledge graphs and commonly utilize recurrent or graph neural networks for forecasting. However, entities that are infrequently observed or have not been seen recently face challenges in learning effective knowledge representations due to insufficient structural contexts. To address the above disadvantages, in this paper, we propose a Contrastive Prompt-based framework with Entity background information for TKG forecasting, which we named CoPET. Specifically, to bring the time-invariant entity background information to time-variant structural information, we employ a dual encoder architecture consisting of a candidate encoder and a query encoder. A contrastive learning framework is used to encourage the query representation to be closer to the candidate representation. We further propose three kinds of trainable time-variant prompts aimed at capturing temporal structural information. Experiments on two datasets demonstrate that our method is effective and stays competitive in inference with limited structural information. Our code is available at https://github.com/qianxinying/CoPET",
    "checked": true,
    "id": "8fdafdd43a224b0d9cfd8fb5bf9cae60a582067e",
    "semantic_title": "bring invariant to variant: a contrastive prompt-based framework for temporal knowledge graph forecasting",
    "citation_count": 0,
    "authors": [
      "Ying Zhang",
      "Xinying Qian",
      "Yu Zhao",
      "Baohang Zhou",
      "Kehui Song",
      "Xiaojie Yuan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.229": {
    "title": "Building a Broad Infrastructure for Uniform Meaning Representations",
    "volume": "main",
    "abstract": "This paper reports the first release of the UMR (Uniform Meaning Representation) data set. UMR is a graph-based meaning representation formalism consisting of a sentence-level graph and a document-level graph. The sentence-level graph represents predicate-argument structures, named entities, word senses, aspectuality of events, as well as person and number information for entities. The document-level graph represents coreferential, temporal, and modal relations that go beyond sentence boundaries. UMR is designed to capture the commonalities and variations across languages and this is done through the use of a common set of abstract concepts, relations, and attributes as well as concrete concepts derived from words from invidual languages. This UMR release includes annotations for six languages (Arapaho, Chinese, English, Kukama, Navajo, Sanapana) that vary greatly in terms of their linguistic properties and resource availability. We also describe on-going efforts to enlarge this data set and extend it to other genres and modalities. We also briefly describe the available infrastructure (UMR annotation guidelines and tools) that others can use to create similar data sets",
    "checked": true,
    "id": "da33d118ed85d08babaa60e517d6dc353563437f",
    "semantic_title": "building a broad infrastructure for uniform meaning representations",
    "citation_count": 0,
    "authors": [
      "Julia Bonn",
      "Matthew J. Buchholz",
      "Jayeol Chun",
      "Andrew Cowell",
      "William Croft",
      "Lukas Denk",
      "Sijia Ge",
      "Jan Hajič",
      "Kenneth Lai",
      "James H. Martin",
      "Skatje Myers",
      "Alexis Palmer",
      "Martha Palmer",
      "Claire Benet Post",
      "James Pustejovsky",
      "Kristine Stenzel",
      "Haibo Sun",
      "Zdeňka Urešová",
      "Rosa Vallejos",
      "Jens E. L. Van Gysel",
      "Meagan Vigus",
      "Nianwen Xue",
      "Jin Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.230": {
    "title": "Building a Database of Conversational Routines",
    "volume": "main",
    "abstract": "This paper discusses the Routinicon, a new constructicographic resource for the description of conversational routines. Conversational routines are defined as conventional formulaic expressions that language speakers use in standard extralinguistic situations (cf. Bless you! as a reaction to sneezing or Who's there? as a typical answer to a knock on the door). The Routinicon's goal is to accumulate the routines that constitute the inventory of conventional expressions in Russian language and systematically describe them in a way that would enable future cross-linguistic comparison and typological research. Conceptually, the Routinicon is a natural extension of such projects as the Russian Constructicon and Pragmaticon. It inherits their approach to the systematization of phraseological units as well as to the data collection. At the same time, the new project focuses on a fundamentally different domain of units and hence offers a radically new structure of linguistic annotation. Its principles and challenges are addressed in the paper",
    "checked": true,
    "id": "c34d0b663a7055a0e5d705c5d42a12a6b882c7d3",
    "semantic_title": "building a database of conversational routines",
    "citation_count": 0,
    "authors": [
      "Polina Bychkova",
      "Alyaxey Yaskevich",
      "Serafima Gyulasaryan",
      "Ekaterina Rakhilina"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.231": {
    "title": "Building a Data Infrastructure for a Mid-Resource Language: The Case of Catalan",
    "volume": "main",
    "abstract": "Current LLM-based applications are becoming steadily available for everyone with a reliable access to technology and the internet. These applications offer benefits to their users that leave those without access to them at a serious disadvantage. Given the vastly large amount of data needed to train LLMs, the gap between languages with access to such quantity of data and those without it is currently larger than ever. Aimed at saving this gap, the Aina Project was created to provide Catalan with the necessary resources to keep being relevant in the context of AI/NLP applications based on LLMs. We thus present a set of strategies to consider when improving technology support for a mid- or low-resource language, specially addressing sustainability of high-quality data acquisition and the challenges involved in the process. We also introduce a large amount of new annotated data for Catalan. Our hope is that those interested in replicating this work for another language can learn from what worked for us, the challenges that we faced, and the sometimes disheartening truth of working with mid- and low-resource languages",
    "checked": true,
    "id": "ac8e1888733971a50da48f32840fa4e5180a9b1e",
    "semantic_title": "building a data infrastructure for a mid-resource language: the case of catalan",
    "citation_count": 2,
    "authors": [
      "Aitor Gonzalez-Agirre",
      "Montserrat Marimon",
      "Carlos Rodriguez-Penagos",
      "Javier Aula-Blasco",
      "Irene Baucells",
      "Carme Armentano-Oller",
      "Jorge Palomar-Giner",
      "Baybars Kulebi",
      "Marta Villegas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.232": {
    "title": "Building a Japanese Document-Level Relation Extraction Dataset Assisted by Cross-Lingual Transfer",
    "volume": "main",
    "abstract": "Document-level Relation Extraction (DocRE) is the task of extracting all semantic relationships from a document. While studies have been conducted on English DocRE, limited attention has been given to DocRE in non-English languages. This work delves into effectively utilizing existing English resources to promote DocRE studies in non-English languages, with Japanese as the representative case. As an initial attempt, we construct a dataset by transferring an English dataset to Japanese. However, models trained on such a dataset are observed to suffer from low recalls. We investigate the error cases and attribute the failure to different surface structures and semantics of documents translated from English and those written by native speakers. We thus switch to explore if the transferred dataset can assist human annotation on Japanese documents. In our proposal, annotators edit relation predictions from a model trained on the transferred dataset. Quantitative analysis shows that relation recommendations suggested by the model help reduce approximately 50% of the human edit steps compared with the previous approach. Experiments quantify the performance of existing DocRE models on our collected dataset, portraying the challenges of Japanese and cross-lingual DocRE",
    "checked": true,
    "id": "f1418d23a977a622f76d95de4970e7bcf89f4729",
    "semantic_title": "building a japanese document-level relation extraction dataset assisted by cross-lingual transfer",
    "citation_count": 0,
    "authors": [
      "Youmi Ma",
      "An Wang",
      "Naoaki Okazaki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.233": {
    "title": "Building MUSCLE, a Dataset for MUltilingual Semantic Classification of Links between Entities",
    "volume": "main",
    "abstract": "In this paper we introduce MUSCLE, a dataset for MUltilingual lexico-Semantic Classification of Links between Entities. The MUSCLE dataset was designed to train and evaluate Lexical Relation Classification (LRC) systems with 27K pairs of universal concepts selected from Wikidata, a large and highly multilingual factual Knowledge Graph (KG). Each pair of concepts includes its lexical forms in 25 languages and is labeled with up to five possible lexico-semantic relations between the concepts: hypernymy, hyponymy, meronymy, holonymy, and antonymy. Inspired by Semantic Map theory, the dataset bridges lexical and conceptual semantics, is more challenging and robust than previous datasets for LRC, avoids lexical memorization, is domain-balanced across entities, and enables enrichment and hierarchical information retrieval",
    "checked": true,
    "id": "f0e8f199d39c5b82210b591bcb203be30f642450",
    "semantic_title": "building muscle, a dataset for multilingual semantic classification of links between entities",
    "citation_count": 0,
    "authors": [
      "Lucia Pitarch",
      "Carlos Bobed Lisbona",
      "David Abián",
      "Jorge Gracia",
      "Jordi Bernad"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.234": {
    "title": "Building Question-Answer Data Using Web Register Identification",
    "volume": "main",
    "abstract": "This article introduces a resource-efficient method for developing question-answer (QA) datasets by extracting QA pairs from web-scale data using machine learning (ML). Our method benefits from recent advances in web register (genre) identification and consists of two ML steps with an additional post-processing step. First, using XLM-R and the multilingual CORE web register corpus series with categories such as QA Forum, we train a multilingual classifier to retrieve documents that are likely to contain QA pairs from web-scale data. Second, we develop a NER-style token classifier to identify the QA text spans within these documents. To this end, we experiment with training on a semi-synthetic dataset built on top of the English LFQA, a small set of manually cleaned web QA pairs in English and Finnish, and a Finnish web QA pair dataset cleaned using ChatGPT. The evaluation of our pipeline demonstrates its capability to efficiently retrieve a substantial volume of QA pairs. While the approach is adaptable to any language given the availability of language models and extensive web data, we showcase its efficiency in English and Finnish, developing the first open, non-synthetic and non-machine translated QA dataset for Finnish – Turku WebQA – comprising over 200,000 QA pairs",
    "checked": true,
    "id": "04240f2a29465ef76a623da978d4d76fabae3e13",
    "semantic_title": "building question-answer data using web register identification",
    "citation_count": 0,
    "authors": [
      "Anni Eskelinen",
      "Amanda Myntti",
      "Erik Henriksson",
      "Sampo Pyysalo",
      "Veronika Laippala"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.235": {
    "title": "CAGK: Collaborative Aspect Graph Enhanced Knowledge-based Recommendation",
    "volume": "main",
    "abstract": "Auxiliary information, such as knowledge graph (KG), has become increasingly crucial in recommender systems. However, the current KG-based recommendation still has some limitations: (1) low link rates between items and KG entities, (2) redundant knowledge in KG. In this paper, we introduce the aspect, which refers to keywords describing item attributes in reviews, to KG-based recommendation, and propose a new model, Collaborative Aspect Graph enhanced Knowledge-based Network (CAGK). Firstly, CAGK builds a Collaborative Aspect Graph (CAG) with user-item interactions, aspects and KG, where aspects can fill most of the sparsity. Secondly, we leverage interactive information and aspect features to generate aspect-aware guidance signals to customize knowledge extraction and eliminate redundant knowledge. Lastly, we utilize low ratings and negative aspect sentiment to capture features of that users dislike to prevent repetitive recommendations of disliked items. Experimental results on two widely used benchmark datasets, Amazon-book and Yelp2018, confirm the superiority of CAGK",
    "checked": true,
    "id": "bca749a28d1c598d7a48a28b922dc4248fd81680",
    "semantic_title": "cagk: collaborative aspect graph enhanced knowledge-based recommendation",
    "citation_count": 0,
    "authors": [
      "Xiaotong Song",
      "Huiping Lin",
      "Jiatao Zhu",
      "Xinyi Gong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.236": {
    "title": "CALAMR: Component ALignment for Abstract Meaning Representation",
    "volume": "main",
    "abstract": "We present Component ALignment for Abstract Meaning Representation (Calamr), a novel method for graph alignment that can support summarization and its evaluation. First, our method produces graphs that explain what is summarized through their alignments, which can be used to train graph based summarization learners. Second, although numerous scoring methods have been proposed for abstract meaning representation (AMR) that evaluate semantic similarity, no AMR based summarization metrics exist despite years of work using AMR for this task. Calamr provides alignments on which new scores can be based. The contributions of this work include a) a novel approach to aligning AMR graphs, b) a new summarization based scoring methods for similarity of AMR subgraphs composed of one or more sentences, and c) the entire reusable source code to reproduce our results",
    "checked": true,
    "id": "9cb8b6ec4848ef03b363b11e617210b6eb72818a",
    "semantic_title": "calamr: component alignment for abstract meaning representation",
    "citation_count": 0,
    "authors": [
      "Paul Landes",
      "Barbara Di Eugenio"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.237": {
    "title": "Calibrating LLM-Based Evaluator",
    "volume": "main",
    "abstract": "Recent advancements in large language models (LLMs) and their emergent capabilities make LLM a promising reference-free evaluator on the quality of natural language generation, and a competent alternative to human evaluation. However, hindered by the closed-source or high computational demand to host and tune, there is a lack of practice to further calibrate an off-the-shelf LLM-based evaluator towards better human alignment. In this work, we propose AutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate and align an LLM-based evaluator toward human preference. Instead of explicitly modeling human preferences, we first implicitly encompass them within a set of human labels. Then, an initial set of scoring criteria is drafted by the language model itself, leveraging in-context learning on different few-shot examples. To further calibrate this set of criteria, we select the best performers and re-draft them with self-refinement. Our experiments on multiple text quality evaluation datasets illustrate a significant improvement in correlation with expert evaluation through calibration. Our comprehensive qualitative analysis conveys insightful intuitions and observations on the essence of effective scoring criteria",
    "checked": true,
    "id": "2d12f95dd521101f3092cc3bb04e7e88aba8f562",
    "semantic_title": "calibrating llm-based evaluator",
    "citation_count": 6,
    "authors": [
      "Yuxuan Liu",
      "Tianchi Yang",
      "Shaohan Huang",
      "Zihan Zhang",
      "Haizhen Huang",
      "Furu Wei",
      "Weiwei Deng",
      "Feng Sun",
      "Qi Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.238": {
    "title": "CAM 2.0: End-to-End Open Domain Comparative Question Answering System",
    "volume": "main",
    "abstract": "Comparative Question Answering (CompQA) is a Natural Language Processing task that combines Question Answering and Argument Mining approaches to answer subjective comparative questions in an efficient argumentative manner. In this paper, we present an end-to-end (full pipeline) system for answering comparative questions called CAM 2.0 as well as a public leaderboard called CompUGE that unifies the existing datasets under a single easy-to-use evaluation suite. As compared to previous web-form-based CompQA systems, it features question identification, object and aspect labeling, stance classification, and summarization using up-to-date models. We also select the most time- and memory-effective pipeline by comparing separately fine-tuned Transformer Encoder models which show state-of-the-art performance on the subtasks with Generative LLMs in few-shot and LoRA setups. We also conduct a user study for a whole-system evaluation",
    "checked": true,
    "id": "2a44e0bb71cd4e9ce4beb676b1c01de9e9deaefd",
    "semantic_title": "cam 2.0: end-to-end open domain comparative question answering system",
    "citation_count": 0,
    "authors": [
      "Ahmad Shallouf",
      "Hanna Herasimchyk",
      "Mikhail Salnikov",
      "Rudy Alexandro Garrido Veliz",
      "Natia Mestvirishvili",
      "Alexander Panchenko",
      "Chris Biemann",
      "Irina Nikishina"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.239": {
    "title": "CAMAL: A Novel Dataset for Multi-label Conversational Argument Move Analysis",
    "volume": "main",
    "abstract": "Understanding the discussion moves that teachers and students use to engage in classroom discussions is important to support pre-service teacher learning and teacher educators. This work introduces a novel conversational multi-label corpus of teaching transcripts collected from a simulated classroom environment for Conversational Argument Move AnaLysis (CAMAL). The dataset offers various argumentation moves used by pre-service teachers and students in mathematics and science classroom discussions. The dataset includes 165 transcripts from these discussions that pre-service elementary teachers facilitated in a simulated classroom environment of five student avatars. The discussion transcripts were annotated by education assessment experts for nine argumentation moves (aka. intents) used by the pre-service teachers and students during the discussions. In this paper, we describe the dataset, our annotation framework, and the models we employed to detect argumentation moves. Our experiments with state-of-the-art models demonstrate the complexity of the CAMAL task presented in the dataset. The result reveals that models that combined CNN and LSTM structures with speaker ID graphs improved the F1-score of our baseline models to detect speakers' intents by a large margin. Given the complexity of the CAMAL task, it creates research opportunities for future studies. We share the dataset, the source code, and the annotation framework publicly at http://github.com/uonlp/camal-dataset",
    "checked": true,
    "id": "972e8adc22631888f63469b135dd9fe215dbf907",
    "semantic_title": "camal: a novel dataset for multi-label conversational argument move analysis",
    "citation_count": 0,
    "authors": [
      "Viet Dac Lai",
      "Duy Ngoc Pham",
      "Jonathan Steinberg",
      "Jamie Mikeska",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.240": {
    "title": "Camel Morph MSA: A Large-Scale Open-Source Morphological Analyzer for Modern Standard Arabic",
    "volume": "main",
    "abstract": "We present Camel Morph MSA, the largest open-source Modern Standard Arabic morphological analyzer and generator. Camel Morph MSA has over 100K lemmas, and includes rarely modeled morphological features of Modern Standard Arabic with Classical Arabic origins. Camel Morph MSA can produce ∼1.45B analyses and ∼535M unique diacritizations, almost an order of magnitude larger than SAMA (Maamouri et al., 2010c), in addition to having ∼36% less OOV rate than SAMA on a 10B word corpus. Furthermore, Camel Morph MSA fills the gaps of many lemma paradigms by modeling linguistic phenomena consistently. Camel Morph MSA seamlessly integrates with the Camel Tools Python toolkit (Obeid et al., 2020), ensuring ease of use and accessibility",
    "checked": true,
    "id": "b5db30b6efd0626a5137ddbfd6f8eaf5a6ad93aa",
    "semantic_title": "camel morph msa: a large-scale open-source morphological analyzer for modern standard arabic",
    "citation_count": 0,
    "authors": [
      "Christian Khairallah",
      "Salam Khalifa",
      "Reham Marzouk",
      "Mayar Nassar",
      "Nizar Habash"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.241": {
    "title": "CamemBERT-bio: Leveraging Continual Pre-training for Cost-Effective Models on French Biomedical Data",
    "volume": "main",
    "abstract": "Clinical data in hospitals are increasingly accessible for research through clinical data warehouses. However these documents are unstructured and it is therefore necessary to extract information from medical reports to conduct clinical studies. Transfer learning with BERT-like models such as CamemBERT has allowed major advances for French, especially for named entity recognition. However, these models are trained for plain language and are less efficient on biomedical data. Addressing this gap, we introduce CamemBERT-bio, a dedicated French biomedical model derived from a new public French biomedical dataset. Through continual pre-training of the original CamemBERT, CamemBERT-bio achieves an improvement of 2.54 points of F1-score on average across various biomedical named entity recognition tasks, reinforcing the potential of continual pre-training as an equally proficient yet less computationally intensive alternative to training from scratch. Additionally, we highlight the importance of using a standard evaluation protocol that provides a clear view of the current state-of-the-art for French biomedical models",
    "checked": true,
    "id": "5dca76612ac1ec4f5ced72714334bb6922d152a9",
    "semantic_title": "camembert-bio: leveraging continual pre-training for cost-effective models on french biomedical data",
    "citation_count": 0,
    "authors": [
      "Rian Touchent",
      "Éric de la Clergerie"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.242": {
    "title": "CAMERA³: An Evaluation Dataset for Controllable Ad Text Generation in Japanese",
    "volume": "main",
    "abstract": "Ad text generation is the task of creating compelling text from an advertising asset that describes products or services, such as a landing page. In advertising, diversity plays an important role in enhancing the effectiveness of an ad text, mitigating a phenomenon called \"ad fatigue,\" where users become disengaged due to repetitive exposure to the same advertisement. Despite numerous efforts in ad text generation, the aspect of diversifying ad texts has received limited attention, particularly in non-English languages like Japanese. To address this, we present CAMERA³, an evaluation dataset for controllable text generation in the advertising domain in Japanese. Our dataset includes 3,980 ad texts written by expert annotators, taking into account various aspects of ad appeals. We make CAMERA³ publicly available, allowing researchers to examine the capabilities of recent NLG models in controllable text generation in a real-world scenario",
    "checked": true,
    "id": "b1d2a7450c9be2aedfd41f8b6b67107af4b43a2d",
    "semantic_title": "camera³: an evaluation dataset for controllable ad text generation in japanese",
    "citation_count": 0,
    "authors": [
      "Go Inoue",
      "Akihiko Kato",
      "Masato Mita",
      "Ukyo Honda",
      "Peinan Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.243": {
    "title": "Can Factual Statements Be Deceptive? The DeFaBel Corpus of Belief-based Deception",
    "volume": "main",
    "abstract": "If a person firmly believes in a non-factual statement, such as \"The Earth is flat\", and argues in its favor, there is no inherent intention to deceive. As the argumentation stems from genuine belief, it may be unlikely to exhibit the linguistic properties associated with deception or lying. This interplay of factuality, personal belief, and intent to deceive remains an understudied area. Disentangling the influence of these variables in argumentation is crucial to gain a better understanding of the linguistic properties attributed to each of them. To study the relation between deception and factuality, based on belief, we present the DeFaBel corpus, a crowd-sourced resource of belief-based deception. To create this corpus, we devise a study in which participants are instructed to write arguments supporting statements like \"eating watermelon seeds can cause indigestion\", regardless of its factual accuracy or their personal beliefs about the statement. In addition to the generation task, we ask them to disclose their belief about the statement. The collected instances are labelled as deceptive if the arguments are in contradiction to the participants' personal beliefs. Each instance in the corpus is thus annotated (or implicitly labelled) with personal beliefs of the author, factuality of the statement, and the intended deceptiveness. The DeFaBel corpus contains 1031 texts in German, out of which 643 are deceptive and 388 are non-deceptive. It is the first publicly available corpus for studying deception in German. In our analysis, we find that people are more confident in the persuasiveness of their arguments when the statement is aligned with their belief, but surprisingly less confident when they are generating arguments in favor of facts. The DeFaBel corpus can be obtained from https://www.ims.uni-stuttgart.de/data/defabel",
    "checked": true,
    "id": "011293bace1cae808f378148dfa6b8529dace8ec",
    "semantic_title": "can factual statements be deceptive? the defabel corpus of belief-based deception",
    "citation_count": 0,
    "authors": [
      "Aswathy Velutharambath",
      "Roman Klinger",
      "Amelie Wührl"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.244": {
    "title": "Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles",
    "volume": "main",
    "abstract": "The use of propaganda has spiked on mainstream and social media, aiming to manipulate or mislead users. While efforts to automatically detect propaganda techniques in textual, visual, or multimodal content have increased, most of them primarily focus on English content. The majority of the recent initiatives targeting medium to low-resource languages produced relatively small annotated datasets, with a skewed distribution, posing challenges for the development of sophisticated propaganda detection models. To address this challenge, we carefully develop the largest propaganda dataset to date, ArPro, comprised of 8K paragraphs from newspaper articles, labeled at the text span level following a taxonomy of 23 propagandistic techniques. Furthermore, our work offers the first attempt to understand the performance of large language models (LLMs), using GPT-4, for fine-grained propaganda detection from text. Results showed that GPT-4's performance degrades as the task moves from simply classifying a paragraph as propagandistic or not, to the fine-grained task of detecting propaganda techniques and their manifestation in text. Compared to models fine-tuned on the dataset for propaganda detection at different classification granularities, GPT-4 is still far behind. Finally, we evaluate GPT-4 on a dataset consisting of six other languages for span detection, and results suggest that the model struggles with the task across languages. We made the dataset publicly available for the community",
    "checked": true,
    "id": "deeb5f7cad3ab64a53646e9b1f87fc952c23b805",
    "semantic_title": "can gpt-4 identify propaganda? annotation and detection of propaganda spans in news articles",
    "citation_count": 0,
    "authors": [
      "Maram Hasanain",
      "Fatema Ahmad",
      "Firoj Alam"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.245": {
    "title": "Can Humans Identify Domains?",
    "volume": "main",
    "abstract": "Textual domain is a crucial property within the Natural Language Processing (NLP) community due to its effects on downstream model performance. The concept itself is, however, loosely defined and, in practice, refers to any non-typological property, such as genre, topic, medium or style of a document. We investigate the core notion of domains via human proficiency in identifying related intrinsic textual properties, specifically the concepts of genre (communicative purpose) and topic (subject matter). We publish our annotations in TGeGUM: A collection of 9.1k sentences from the GUM dataset (Zeldes, 2017) with single sentence and larger context (i.e., prose) annotations for one of 11 genres (source type), and its topic/subtopic as per the Dewey Decimal library classification system (Dewey, 1979), consisting of 10/100 hierarchical topics of increased granularity. Each instance is annotated by three annotators, for a total of 32.7k annotations, allowing us to examine the level of human disagreement and the relative difficulty of each annotation task. With a Fleiss' kappa of at most 0.53 on the sentence level and 0.66 at the prose level, it is evident that despite the ubiquity of domains in NLP, there is little human consensus on how to define them. By training classifiers to perform the same task, we find that this uncertainty also extends to NLP models",
    "checked": true,
    "id": "5756072767ece24632f81899aa5528e94caa7147",
    "semantic_title": "can humans identify domains?",
    "citation_count": 0,
    "authors": [
      "Maria Barrett",
      "Max Müller-Eberstein",
      "Elisa Bassignana",
      "Amalie Brogaard Pauli",
      "Mike Zhang",
      "Rob van der Goot"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.246": {
    "title": "Can Language Models Learn Embeddings of Propositional Logic Assertions?",
    "volume": "main",
    "abstract": "Natural language offers an appealing alternative to formal logics as a vehicle for representing knowledge. However, using natural language means that standard methods for automated reasoning can no longer be used. A popular solution is to use transformer-based language models (LMs) to directly reason about knowledge expressed in natural language, but this has two important limitations. First, the set of premises is often too large to be directly processed by the LM. This means that we need a retrieval strategy which can select the most relevant premises when trying to infer some conclusion. Second, LMs have been found to learn shortcuts and thus lack robustness, putting in doubt to what extent they actually understand the knowledge that is expressed. Given these limitations, we explore the following alternative: rather than using LMs to perform reasoning directly, we use them to learn embeddings of individual assertions. Reasoning is then carried out by manipulating the learned embeddings. We show that this strategy is feasible to some extent, while at the same time also highlighting the limitations of directly fine-tuning LMs to learn the required embeddings",
    "checked": true,
    "id": "fdae9b618ae71b3cc67544bd06729001fc116e51",
    "semantic_title": "can language models learn embeddings of propositional logic assertions?",
    "citation_count": 0,
    "authors": [
      "Nurul Fajrin Ariyani",
      "Zied Bouraoui",
      "Richard Booth",
      "Steven Schockaert"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.247": {
    "title": "Can Large Language Models Automatically Score Proficiency of Written Essays?",
    "volume": "main",
    "abstract": "Although several methods were proposed to address the problem of automated essay scoring (AES) in the last 50 years, there is still much to desire in terms of effectiveness. Large Language Models (LLMs) are transformer-based models that demonstrate extraordinary capabilities on various tasks. In this paper, we test the ability of LLMs, given their powerful linguistic knowledge, to analyze and effectively score written essays. We experimented with two popular LLMs, namely ChatGPT and Llama. We aim to check if these models can do this task and, if so, how their performance is positioned among the state-of-the-art (SOTA) models across two levels, holistically and per individual writing trait. We utilized prompt-engineering tactics in designing four different prompts to bring their maximum potential on this task. Our experiments conducted on the ASAP dataset revealed several interesting observations. First, choosing the right prompt depends highly on the model and nature of the task. Second, the two LLMs exhibited comparable average performance in AES, with a slight advantage for ChatGPT. Finally, despite the performance gap between the two LLMs and SOTA models in terms of predictions, they provide feedback to enhance the quality of the essays, which can potentially help both teachers and students",
    "checked": true,
    "id": "b31a0dec20b0185fdf260aa64c16f0d053a8f3e8",
    "semantic_title": "can large language models automatically score proficiency of written essays?",
    "citation_count": 0,
    "authors": [
      "Watheq Ahmad Mansour",
      "Salam Albatarni",
      "Sohaila Eltanbouly",
      "Tamer Elsayed"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.248": {
    "title": "Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences",
    "volume": "main",
    "abstract": "Hypothesis formulation and testing are central to empirical research. A strong hypothesis is a best guess based on existing evidence and informed by a comprehensive view of relevant literature. However, with exponential increase in the number of scientific articles published annually, manual aggregation and synthesis of evidence related to a given hypothesis is a challenge. Our work explores the ability of current large language models (LLMs) to discern evidence in support or refute of specific hypotheses based on the text of scientific abstracts. We share a novel dataset for the task of scientific hypothesis evidencing using community-driven annotations of studies in the social sciences. We compare the performance of LLMs to several state of the art methods and highlight opportunities for future research in this area. Our dataset is shared with the research community: https://github.com/Sai90000/ScientificHypothesisEvidencing.git",
    "checked": true,
    "id": "d6c939c06c7f5f4fd111ca3517a673d414e6ebf2",
    "semantic_title": "can large language models discern evidence for scientific hypotheses? case studies in the social sciences",
    "citation_count": 3,
    "authors": [
      "Sai Koneru",
      "Jian Wu",
      "Sarah Rajtmajer"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.249": {
    "title": "Can Large Language Models Learn Translation Robustness from Noisy-Source In-context Demonstrations?",
    "volume": "main",
    "abstract": "Large language models (LLMs) have been used for machine translation. When provided with prompts and source sentences, LLMs can achieve impressive translation results. However, the robustness of these LLMs remains a significant challenge, as they often struggle to accurately translate sentences in the presence of noise, even when using similarity-based in-context learning methods. This work proposes a research scheme for studying machine translation robustness on LLMs, investigating whether LLMs can learn translation robustness from noisy-source demonstration examples. Through experiments on different models, languages, and noise types, we empirically demonstrate that LLMs can learn how to handle noise and translation methods from noisy-source demonstration examples, thereby improving their translation performance on noisy sentences. Furthermore, we find that increasing the noise ratio appropriately for the noisy-source demonstration examples can enhance the translation robustness of LLMs. Additionally, we also attempt to investigate scenarios where LLMs are more likely to learn translation robustness for mixed and specific types of noise. We find that the model's performance varies across different noise settings",
    "checked": true,
    "id": "149ebe8c6850adac0e0b6ff9aa24b4d7a331bd6c",
    "semantic_title": "can large language models learn translation robustness from noisy-source in-context demonstrations?",
    "citation_count": 0,
    "authors": [
      "Leiyu Pan",
      "Yongqi Leng",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.250": {
    "title": "Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning?",
    "volume": "main",
    "abstract": "Multilingual pretraining and fine-tuning have remarkably succeeded in various natural language processing tasks. Transferring representations from one language to another is especially crucial for cross-lingual learning. One can expect machine translation objectives to be well suited to fostering such capabilities, as they involve the explicit alignment of semantically equivalent sentences from different languages. This paper investigates the potential benefits of employing machine translation as a continued training objective to enhance language representation learning, bridging multilingual pretraining and cross-lingual applications. We study this question through two lenses: a quantitative evaluation of the performance of existing models and an analysis of their latent representations. Our results show that, contrary to expectations, machine translation as the continued training fails to enhance cross-lingual representation learning in multiple cross-lingual natural language understanding tasks. We conclude that explicit sentence-level alignment in the cross-lingual scenario is detrimental to cross-lingual transfer pretraining, which has important implications for future cross-lingual transfer studies. We furthermore provide evidence through similarity measures and investigation of parameters that this lack of positive influence is due to output separability—which we argue is of use for machine translation but detrimental elsewhere",
    "checked": true,
    "id": "dbbd46753d98f2792076e038f1da2fd3e74b4eec",
    "semantic_title": "can machine translation bridge multilingual pretraining and cross-lingual transfer learning?",
    "citation_count": 0,
    "authors": [
      "Shaoxiong Ji",
      "Timothee Mickus",
      "Vincent Segonne",
      "Jörg Tiedemann"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.251": {
    "title": "Can Multiple-choice Questions Really Be Useful in Detecting the Abilities of LLMs?",
    "volume": "main",
    "abstract": "Multiple-choice questions (MCQs) are widely used in the evaluation of large language models (LLMs) due to their simplicity and efficiency. However, there are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required. The misalignment between the task and the evaluation method demands a thoughtful analysis of MCQ's efficacy, which we undertake in this paper by evaluating nine LLMs on four question-answering (QA) datasets in two languages: Chinese and English. We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position. We further quantify the gap between MCQs and long-form generation questions (LFGQs) by comparing their direct outputs, token logits, and embeddings. Our results reveal a relatively low correlation between answers from MCQs and LFGQs for identical questions. Additionally, we propose two methods to quantify the consistency and confidence of LLMs' output, which can be generalized to other QA evaluation benchmarks. Notably, our analysis challenges the idea that the higher the consistency, the greater the accuracy. We also find MCQs to be less reliable than LFGQs in terms of expected calibration error. Finally, the misalignment between MCQs and LFGQs is not only reflected in the evaluation performance but also in the embedding space. Our code and models can be accessed at https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs",
    "checked": true,
    "id": "cdac0a760e7e28d985db60696e61311c422a649c",
    "semantic_title": "can multiple-choice questions really be useful in detecting the abilities of llms?",
    "citation_count": 0,
    "authors": [
      "Wangyue Li",
      "Liangzhi Li",
      "Tong Xiang",
      "Xiao Liu",
      "Wei Deng",
      "Noa Garcia"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.252": {
    "title": "Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought",
    "volume": "main",
    "abstract": "We introduce a novel framework, LM-Guided CoT, that leverages a lightweight (i.e., <1B) language model (LM) for guiding a black-box large (i.e., >10B) LM in reasoning tasks. Specifically, the lightweight LM first generates a rationale for each input instance. The Frozen large LM is then prompted to predict a task output based on the rationale generated by the lightweight LM. Our approach is resource-efficient in the sense that it only requires training the lightweight LM. We optimize the model through 1) knowledge distillation and 2) reinforcement learning from rationale-oriented and task-oriented reward signals. We assess our method with multi-hop extractive question answering (QA) benchmarks, HotpotQA, and 2WikiMultiHopQA. Experimental results show that our approach outperforms all baselines regarding answer prediction accuracy. We also find that reinforcement learning helps the model to produce higher-quality rationales with improved QA performance",
    "checked": true,
    "id": "f19ddba513dfeca571a6e2ba7542a63677cd5e3b",
    "semantic_title": "can small language models help large language models reason better?: lm-guided chain-of-thought",
    "citation_count": 0,
    "authors": [
      "Jooyoung Lee",
      "Fan Yang",
      "Thanh Tran",
      "Qian Hu",
      "Emre Barut",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.253": {
    "title": "Can We Identify Stance without Target Arguments? A Study for Rumour Stance Classification",
    "volume": "main",
    "abstract": "Considering a conversation thread, rumour stance classification aims to identify the opinion (e.g. agree or disagree) of replies towards a target (rumour story). Although the target is expected to be an essential component in traditional stance classification, we show that rumour stance classification datasets contain a considerable amount of real-world data whose stance could be naturally inferred directly from the replies, contributing to the strong performance of the supervised models without awareness of the target. We find that current target-aware models underperform in cases where the context of the target is crucial. Finally, we propose a simple yet effective framework to enhance reasoning with the targets, achieving state-of-the-art performance on two benchmark datasets",
    "checked": true,
    "id": "ed13ecda335fbbf9426e533cd7e749308c4dd005",
    "semantic_title": "can we identify stance without target arguments? a study for rumour stance classification",
    "citation_count": 0,
    "authors": [
      "Yue Li",
      "Carolina Scarton"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.254": {
    "title": "Can We Learn Question, Answer, and Distractors All from an Image? A New Task for Multiple-choice Visual Question Answering",
    "volume": "main",
    "abstract": "Multiple-choice visual question answering (MC VQA) requires an answer picked from a list of distractors, based on a question and an image. This research has attracted wide interest from the fields of visual question answering, visual question generation, and visual distractor generation. However, these fields still stay in their own territories, and how to jointly generate meaningful questions, correct answers, and challenging distractors remains unexplored. In this paper, we introduce a novel task, Visual Question-Answer-Distractors Generation (VQADG), which can bridge this research gap as well as take as a cornerstone to promote existing VQA models. Specific to the VQADG task, we present a novel framework consisting of a vision-and-language model to encode the given image and generate QADs jointly, and contrastive learning to ensure the consistency of the generated question, answer, and distractors. Empirical evaluations on the benchmark dataset validate the performance of our model in the VQADG task",
    "checked": true,
    "id": "f525bcdb3870ea15528dd450b59db64586509205",
    "semantic_title": "can we learn question, answer, and distractors all from an image? a new task for multiple-choice visual question answering",
    "citation_count": 0,
    "authors": [
      "Wenjian Ding",
      "Yao Zhang",
      "Jun Wang",
      "Adam Jatowt",
      "Zhenglu Yang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.255": {
    "title": "CARE: Co-Attention Network for Joint Entity and Relation Extraction",
    "volume": "main",
    "abstract": "Joint entity and relation extraction is the fundamental task of information extraction, consisting of two subtasks: named entity recognition and relation extraction. However, most existing joint extraction methods suffer from issues of feature confusion or inadequate interaction between the two subtasks. Addressing these challenges, in this work, we propose a Co-Attention network for joint entity and Relation Extraction (CARE). Our approach includes adopting a parallel encoding strategy to learn separate representations for each subtask, aiming to avoid feature overlap or confusion. At the core of our approach is the co-attention module that captures two-way interaction between the two subtasks, allowing the model to leverage entity information for relation prediction and vice versa, thus promoting mutual enhancement. Through extensive experiments on three benchmark datasets for joint entity and relation extraction (NYT, WebNLG, and SciERC), we demonstrate that our proposed model outperforms existing baseline models. Our code will be available at https://github.com/kwj0x7f/CARE",
    "checked": true,
    "id": "b6d4e3e79958695464a3b4736cdb425fe056b37c",
    "semantic_title": "care: co-attention network for joint entity and relation extraction",
    "citation_count": 1,
    "authors": [
      "Wenjun Kong",
      "Yamei Xia"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.256": {
    "title": "CareCorpus: A Corpus of Real-World Solution-Focused Caregiver Strategies for Personalized Pediatric Rehabilitation Service Design",
    "volume": "main",
    "abstract": "In pediatric rehabilitation services, one intervention approach involves using solution-focused caregiver strategies to support children in their daily life activities. The manual sharing of these strategies is not scalable, warranting need for an automated approach to recognize and select relevant strategies. We introduce CareCorpus, a dataset of 780 real-world strategies written by caregivers. Strategies underwent dual-annotation by three trained annotators according to four established rehabilitation classes (i.e., environment/context, n=325 strategies; a child's sense of self, n=151 strategies; a child's preferences, n=104 strategies; and a child's activity competences, n=62 strategies) and a no-strategy class (n=138 instances) for irrelevant or indeterminate instances. The average percent agreement was 80.18%, with a Cohen's Kappa of 0.75 across all classes. To validate this dataset, we propose multi-grained classification tasks for detecting and categorizing strategies, and establish new performance benchmarks ranging from F1=0.53-0.79. Our results provide a first step towards a smart option to sort caregiver strategies for use in designing pediatric rehabilitation care plans. This novel, interdisciplinary resource and application is also anticipated to generalize to other pediatric rehabilitation service contexts that target children with developmental need",
    "checked": true,
    "id": "01d4e0265ac515ad97fbd0a3256755060ca341b1",
    "semantic_title": "carecorpus: a corpus of real-world solution-focused caregiver strategies for personalized pediatric rehabilitation service design",
    "citation_count": 0,
    "authors": [
      "Mina Valizadeh",
      "Vera C. Kaelin",
      "Mary A. Khetani",
      "Natalie Parde"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.257": {
    "title": "CASIMIR: A Corpus of Scientific Articles Enhanced with Multiple Author-Integrated Revisions",
    "volume": "main",
    "abstract": "Writing a scientific article is a challenging task as it is a highly codified and specific genre, consequently proficiency in written communication is essential for effectively conveying research findings and ideas. In this article, we propose an original textual resource on the revision step of the writing process of scientific articles. This new dataset, called CASIMIR, contains the multiple revised versions of 15,646 scientific articles from OpenReview, along with their peer reviews. Pairs of consecutive versions of an article are aligned at sentence-level while keeping paragraph location information as metadata for supporting future revision studies at the discourse level. Each pair of revised sentences is enriched with automatically extracted edits and associated revision intention. To assess the initial quality on the dataset, we conducted a qualitative study of several state-of-the-art text revision approaches and compared various evaluation metrics. Our experiments led us to question the relevance of the current evaluation methods for the text revision task",
    "checked": true,
    "id": "5e9b89d3f1e9578b996f358bcd14437f351e6238",
    "semantic_title": "casimir: a corpus of scientific articles enhanced with multiple author-integrated revisions",
    "citation_count": 0,
    "authors": [
      "Léane Isabelle Jourdan",
      "Florian Boudin",
      "Nicolas Hernandez",
      "Richard Dufour"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.258": {
    "title": "Categorial Grammar Induction with Stochastic Category Selection",
    "volume": "main",
    "abstract": "Grammar induction, the task of learning a set of syntactic rules from minimally annotated training data, provides a means of exploring the longstanding question of whether humans rely on innate knowledge to acquire language. Of the various formalisms available for grammar induction, categorial grammars provide an appealing option due to their transparent interface between syntax and semantics. However, to obtain competitive results, previous categorial grammar inducers have relied on shortcuts such as part-of-speech annotations or an ad hoc bias term in the objective function to ensure desirable branching behavior. We present a categorial grammar inducer that eliminates both shortcuts: it learns from raw data, and does not rely on a biased objective function. This improvement is achieved through a novel stochastic process used to select the set of available syntactic categories. On a corpus of English child-directed speech, the model attains a recall-homogeneity of 0.48, a large improvement over previous categorial grammar inducers",
    "checked": true,
    "id": "bd83b412df3358799e31edbae1d56423ecec9024",
    "semantic_title": "categorial grammar induction with stochastic category selection",
    "citation_count": 0,
    "authors": [
      "Christian Clark",
      "William Schuler"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.259": {
    "title": "Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: A Case Study on Hateful Memes",
    "volume": "main",
    "abstract": "Amidst the rapid expansion of Machine Learning (ML) and Large Language Models (LLMs), understanding the semantics within their mechanisms is vital. Causal analyses define semantics, while gradient-based methods are essential to eXplainable AI (XAI), interpreting the model's ‘black box'. Integrating these, we investigate how a model's mechanisms reveal its causal effect on evidence-based decision-making. Research indicates intersectionality - the combined impact of an individual's demographics - can be framed as an Average Treatment Effect (ATE). This paper demonstrates that hateful meme detection can be viewed as an ATE estimation using intersectionality principles, and summarized gradient-based attention scores highlight distinct behaviors of three Transformer models. We further reveal that LLM Llama-2 can discern the intersectional aspects of the detection through in-context learning and that the learning process could be explained via meta-gradient, a secondary form of gradient. In conclusion, this work furthers the dialogue on Causality and XAI. Our code is available online (see External Resources section)",
    "checked": true,
    "id": "3ac3c10e1317fe8419f794cf30ce3227e95e1f54",
    "semantic_title": "causal intersectionality and dual form of gradient descent for multimodal analysis: a case study on hateful memes",
    "citation_count": 0,
    "authors": [
      "Yosuke Miyanishi",
      "Minh Le Nguyen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.260": {
    "title": "CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models",
    "volume": "main",
    "abstract": "Holistically measuring societal biases of large language models is crucial for detecting and reducing ethical risks in highly capable AI models. In this work, we present a Chinese Bias Benchmark dataset that consists of over 100K questions jointly constructed by human experts and generative language models, covering stereotypes and societal biases in 14 social dimensions related to Chinese culture and values. The curation process contains 4 essential steps: bias identification, ambiguous context generation, AI-assisted disambiguous context generation, and manual review and recomposition. The testing instances in the dataset are automatically derived from 3K+ high-quality templates manually authored with stringent quality control. The dataset exhibits wide coverage and high diversity. Extensive experiments demonstrate the effectiveness of the dataset in evaluating model bias, with all 12 publicly available Chinese large language models exhibiting strong bias in certain categories. Additionally, we observe from our experiments that fine-tuned models could, to a certain extent, heed instructions and avoid generating harmful outputs, in the way of \"moral self-correction\". Our dataset is available at https://anonymous.4open.science/r/CBBQ-B860/",
    "checked": true,
    "id": "e11111dfda2a1f7aa9ecb8720032739233fb72f4",
    "semantic_title": "cbbq: a chinese bias benchmark dataset curated with human-ai collaboration for large language models",
    "citation_count": 5,
    "authors": [
      "Yufei Huang",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.261": {
    "title": "CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering",
    "volume": "main",
    "abstract": "The recent advancements in artificial intelligence highlight the potential of language models in psychological health support. While models trained on data from mental health service platform have achieved preliminary success, challenges persist in areas such as data scarcity, quality, and ensuring a solid foundation in psychological techniques. To address these challenges, this study introduces a novel approach to enhance the precision and efficacy of psychological support through large language models. Specifically, we design a specific prompt derived from principles of Cognitive Behavioral Therapy (CBT) and have generated the CBT QA dataset, specifically for Chinese psychological health Q&A based on CBT structured intervention strategies. Unlike previous methods, our dataset emphasizes professional and structured response. Utilizing this dataset, we fine-tuned the large language model, giving birth to CBT-LLM, the large-scale language model specifically designed for Cognitive Behavioral Therapy techniques. Empirical evaluations demonstrate that CBT-LLM excels in generating structured, professional, and highly relevant responses in psychological health support tasks, showcasing its practicality and quality. The model is available on Hugging Face: https://huggingface.co/Hongbin37/CBT-LLM",
    "checked": true,
    "id": "07a52d25dd5d091957fbdfa6abd28a394719ef15",
    "semantic_title": "cbt-llm: a chinese large language model for cognitive behavioral therapy-based mental health question answering",
    "citation_count": 0,
    "authors": [
      "Hongbin Na"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.262": {
    "title": "CB-Whisper: Contextual Biasing Whisper Using Open-Vocabulary Keyword-Spotting",
    "volume": "main",
    "abstract": "End-to-end automatic speech recognition (ASR) systems often struggle to recognize rare name entities, such as personal names, organizations and terminologies that are not frequently encountered in the training data. This paper presents Contextual Biasing Whisper (CB-Whisper), a novel ASR system based on OpenAI's Whisper model that can recognize user-defined name entities by performing open-vocabulary keyword-spotting (KWS) before the decoder. The KWS module leverages text-to-speech (TTS) techniques and a convolutional neural network (CNN) classifier to match the features between the entities and the utterances. To integrate the recognized entities into the Whipser decoder and avoid hallucinations, we carefully crafted multiple prompts with spoken form hints. Experiments show that the KWS module based on Whisper encoder's features can recognize unseen user-defined keywords effectively. More importantly, the proposed CB-Whisper substantially improves the mixed-error-rate (MER) and entity recall compared to the original Whisper model on three internal datasets and two publicly available datasets including Aishell and ACL datasets that cover English-only, Chinese-only, and code-switching scenarios",
    "checked": true,
    "id": "06c8eb7c9d448ebe6ed57f96dbd45e87ec249731",
    "semantic_title": "cb-whisper: contextual biasing whisper using open-vocabulary keyword-spotting",
    "citation_count": 0,
    "authors": [
      "Yuang Li",
      "Yinglu Li",
      "Min Zhang",
      "Chang Su",
      "Jiawei Yu",
      "Mengyao Piao",
      "Xiaosong Qiao",
      "Miaomiao Ma",
      "Yanqing Zhao",
      "Hao Yang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.263": {
    "title": "CEPT: A Contrast-Enhanced Prompt-Tuning Framework for Emotion Recognition in Conversation",
    "volume": "main",
    "abstract": "Emotion Recognition in Conversation (ERC) has attracted increasing attention due to its wide applications in public opinion analysis, empathetic conversation generation, and so on. However, ERC research suffers from the problems of data imbalance and the presence of similar linguistic expressions for different emotions. These issues can result in limited learning for minority emotions, biased predictions for common emotions, and the misclassification of different emotions with similar linguistic expressions. To alleviate these problems, we propose a Contrast-Enhanced Prompt-Tuning (CEPT) framework for ERC. We transform the ERC task into a Masked Language Modeling (MLM) generation task and generate the emotion for each utterance in the conversation based on the prompt-tuning of the Pre-trained Language Model (PLM), where a novel mixed prompt template and a label mapping strategy are introduced for better context and emotion feature modeling. Moreover, Supervised Contrastive Learning (SCL) is employed to help the PLM mine more information from the labels and learn a more discriminative representation space for utterances with different emotions. We conduct extensive experiments and the results demonstrate that CEPT outperforms the state-of-the-art methods on all three benchmark datasets and excels in recognizing minority emotions",
    "checked": true,
    "id": "fd0b1867084668f1b992e0d0588a1b536c999675",
    "semantic_title": "cept: a contrast-enhanced prompt-tuning framework for emotion recognition in conversation",
    "citation_count": 0,
    "authors": [
      "Qingqing Gao",
      "Jiuxin Cao",
      "Biwei Cao",
      "Xin Guan",
      "Bo Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.264": {
    "title": "CE-VDG: Counterfactual Entropy-based Bias Reduction for Video-grounded Dialogue Generation",
    "volume": "main",
    "abstract": "The Video-Grounded Dialogue generation (VDG) is a challenging task requiring a comprehensive understanding of the multi-modal information to produce a pertinent response. However, VDG models may rely on dataset bias as a shortcut and fail to learn the multi-modal knowledge from both video and audio. Counterfactual reasoning is an effective method that can estimate and eliminate bias on some special aspects of classification tasks. However, conventional counterfactual reasoning cannot be applied to VDG tasks directly due to the BPE algorithm. In this paper, we reformulate the counterfactual reasoning from the information entropy perspective and extend it from the classification task to the generative task, which can effectively reduce the question-related bias in the auto-regressive generation task. We design CE-VDG to demonstrate the effectiveness in bias elimination of the reformulated counterfactual reasoning by using the proposed counterfactual entropy as an external loss. Extensive experiment results on two popular VDG datasets show the superiority of CE-VDG over the existing baseline method, demonstrating the effective debiasing capability in our model considering counterfactual entropy",
    "checked": true,
    "id": "dc398b5cd91316e4881315e443c3811addc8a946",
    "semantic_title": "ce-vdg: counterfactual entropy-based bias reduction for video-grounded dialogue generation",
    "citation_count": 0,
    "authors": [
      "Hongcheng Liu",
      "Pingjie Wang",
      "Zhiyuan Zhu",
      "Yanfeng Wang",
      "Yu Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.265": {
    "title": "ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting",
    "volume": "main",
    "abstract": "Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of large language models (LLMs), establishing itself as a primary approach to solving complex reasoning tasks. Existing CoT synthesis approaches usually focus on simpler reasoning tasks and thus result in low-quality and inconsistent CoT prompts. In response to this challenge, we present an empirical investigation of CoT prompting and introduce CoTGenius, a novel framework designed for the automatic generation of superior CoT prompts. CoTGenius is developed based on three major evolution strategies, i.e., complicate, diversify, and specify—alongside two filtering mechanisms: evolutionary success judgement and correctness verification. We further employ CoTGenius to create an extensive CoT dataset, and subsequently fine-tune the Llama 2-Chat 7B and 13B models on this dataset. We call the resulting model ChainLM. To deal with the cumulative error issue in reasoning steps, we propose a step-level debating method, wherein multiple debaters discuss each reasoning step to arrive at the correct answer. Extensive experiments demonstrate that our ChainLM models exhibit enhanced proficiency in addressing a spectrum of complex reasoning problems compared to existing models. In addition, we conduct an in-depth analysis of the impact of data categories within CoTGenius on the model performance. We release our dataset and code at https://github.com/RUCAIBox/ChainLM",
    "checked": true,
    "id": "2f97fe9ce214394151536118b6e1f0bdd3b190ee",
    "semantic_title": "chainlm: empowering large language models with improved chain-of-thought prompting",
    "citation_count": 2,
    "authors": [
      "Xiaoxue Cheng",
      "Junyi Li",
      "Wayne Xin Zhao",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.266": {
    "title": "ChainNet: Structured Metaphor and Metonymy in WordNet",
    "volume": "main",
    "abstract": "The senses of a word exhibit rich internal structure. In a typical lexicon, this structure is overlooked: A word's senses are encoded as a list, without inter-sense relations. We present ChainNet, a lexical resource which for the first time explicitly identifies these structures, by expressing how senses in the Open English Wordnet are derived from one another. In ChainNet, every nominal sense of a word is either connected to another sense by metaphor or metonymy, or is disconnected (in the case of homonymy). Because WordNet senses are linked to resources which capture information about their meaning, ChainNet represents the first dataset of grounded metaphor and metonymy",
    "checked": true,
    "id": "35dfb2197f6446d7a26730d45399e7d724c9fc87",
    "semantic_title": "chainnet: structured metaphor and metonymy in wordnet",
    "citation_count": 0,
    "authors": [
      "Rowan Hall Maudslay",
      "Simone Teufel",
      "Francis Bond",
      "James Pustejovsky"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.267": {
    "title": "Challenges in Pre-Training Graph Neural Networks for Context-Based Fake News Detection: An Evaluation of Current Strategies and Resource Limitations",
    "volume": "main",
    "abstract": "Pre-training of neural networks has recently revolutionized the field of Natural Language Processing (NLP) and has before demonstrated its effectiveness in computer vision. At the same time, advances around the detection of fake news were mainly driven by the context-based paradigm, where different types of signals (e.g. from social media) form graph-like structures that hold contextual information apart from the news article to classify. We propose to merge these two developments by applying pre-training of Graph Neural Networks (GNNs) in the domain of context-based fake news detection. Our experiments provide an evaluation of different pre-training strategies for graph-based misinformation detection and demonstrate that transfer learning does currently not lead to significant improvements over training a model from scratch in the domain. We argue that a major current issue is the lack of suitable large-scale resources that can be used for pre-training",
    "checked": true,
    "id": "9dccad190bb76215964384deb3b691d81c2806f1",
    "semantic_title": "challenges in pre-training graph neural networks for context-based fake news detection: an evaluation of current strategies and resource limitations",
    "citation_count": 0,
    "authors": [
      "Gregor Donabauer",
      "Udo Kruschwitz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.268": {
    "title": "Challenging Negative Gender Stereotypes: A Study on the Effectiveness of Automated Counter-Stereotypes",
    "volume": "main",
    "abstract": "Gender stereotypes are pervasive beliefs about individuals based on their gender that play a significant role in shaping societal attitudes, behaviours, and even opportunities. Recognizing the negative implications of gender stereotypes, particularly in online communications, this study investigates eleven strategies to automatically counteract and challenge these views. We present AI-generated gender-based counter-stereotypes to (self-identified) male and female study participants and ask them to assess their offensiveness, plausibility, and potential effectiveness. The strategies of counter-facts and broadening universals (i.e., stating that anyone can have a trait regardless of group membership) emerged as the most robust approaches, while humour, perspective-taking, counter-examples, and empathy for the speaker were perceived as less effective. Also, the differences in ratings were more pronounced for stereotypes about the different targets than between the genders of the raters. Alarmingly, many AI-generated counter-stereotypes were perceived as offensive and/or implausible. Our analysis and the collected dataset offer foundational insight into counter-stereotype generation, guiding future efforts to develop strategies that effectively challenge gender stereotypes in online interactions",
    "checked": true,
    "id": "240bea263d38ffbc35a8bb3eeb1470a2a0120de2",
    "semantic_title": "challenging negative gender stereotypes: a study on the effectiveness of automated counter-stereotypes",
    "citation_count": 0,
    "authors": [
      "Isar Nejadgholi",
      "Kathleen C. Fraser",
      "Anna Kerkhof",
      "Svetlana Kiritchenko"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.269": {
    "title": "Characteristic AI Agents via Large Language Models",
    "volume": "main",
    "abstract": "The advancement of Large Language Models (LLMs) has led to significant enhancements in the performance of chatbot systems. Many researchers have dedicated their efforts to the development of bringing characteristics to chatbots. While there have been commercial products for developing role-driven chatbots using LLMs, it is worth noting that academic research in this area remains relatively scarce. Our research focuses on investigating the performance of LLMs in constructing Characteristic AI Agents by simulating real-life individuals across different settings. Current investigations have primarily focused on act on roles with simple profiles. In response to this research gap, we create a benchmark for the characteristic AI agents task, including dataset, techniques, and evaluation metrics. A dataset called \"Character100\" is built for this benchmark, comprising the most-visited people on Wikipedia for language models to role-play. With the constructed dataset, we conduct comprehensive assessment of LLMs across various settings. In addition, we devise a set of automatic metrics for quantitative performance evaluation. The experimental results underscore the potential directions for further improvement in the capabilities of LLMs in constructing characteristic AI agents. The benchmark is available at https://github.com/nuaa-nlp/Character100",
    "checked": true,
    "id": "88ab0e4974e7eda498063fc675b9f00959dc561c",
    "semantic_title": "characteristic ai agents via large language models",
    "citation_count": 0,
    "authors": [
      "Xi Wang",
      "Hongliang Dai",
      "Shen Gao",
      "Piji Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.270": {
    "title": "Character-level Language Models for Abbreviation and Long-form Detection",
    "volume": "main",
    "abstract": "Abbreviations and their associated long forms are important textual elements that are present in almost every scientific communication, and having information about these forms can help improve several NLP tasks. In this paper, our aim is to fine-tune language models for automatically identifying abbreviations and long forms. We used existing datasets which are annotated with abbreviations and long forms to train and test several language models, including transformer models, character-level language models, stacking of different embeddings, and ensemble methods. Our experiments showed that it was possible to achieve state-of-the-art results by stacking RoBERTa embeddings with domain-specific embeddings. However, the analysis of our first run showed that one of the datasets had issues in the BIO annotation, which led us to propose a revised dataset. After re-training selected models on the revised dataset, results show that character-level models achieve comparable results, especially when detecting abbreviations, but both RoBERTa large and the stacking of embeddings presented better results on biomedical data. When tested on a different subdomain (segments extracted from computer science texts), an ensemble method proved to yield the best results for the detection of long forms, and a character-level model had the best performance in detecting abbreviations",
    "checked": true,
    "id": "2eba3f3a848fa0fe6fbd005baab3de3f1cdff5ed",
    "semantic_title": "character-level language models for abbreviation and long-form detection",
    "citation_count": 0,
    "authors": [
      "Leonardo Zilio",
      "Shenbin Qian",
      "Diptesh Kanojia",
      "Constantin Orasan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.271": {
    "title": "Charles Translator: A Machine Translation System between Ukrainian and Czech",
    "volume": "main",
    "abstract": "We present Charles Translator, a machine translation system between Ukrainian and Czech, developed as part of a society-wide effort to mitigate the impact of the Russian-Ukrainian war on individuals and society. The system was developed in the spring of 2022 with the help of many language data providers in order to quickly meet the demand for such a service, which was not available at the time in the required quality. The translator was later implemented as an online web interface and as an Android app with speech input, both featuring Cyrillic-Latin script transliteration. The system translates directly, in comparison to other available systems that use English as a pivot, and thus makes advantage of the typological similarity of the two languages. It uses the block back-translation method which allows for efficient use of monolingual training data. The paper describes the development process including data collection and implementation, evaluation, mentions several use cases and outlines possibilities for further development of the system for educational purposes",
    "checked": true,
    "id": "5c9f1571ba849619967cb1f90664e2a9f6ddc0fa",
    "semantic_title": "charles translator: a machine translation system between ukrainian and czech",
    "citation_count": 0,
    "authors": [
      "Martin Popel",
      "Lucie Polakova",
      "Michal Novák",
      "Jindřich Helcl",
      "Jindřich Libovický",
      "Pavel Straňák",
      "Tomas Krabac",
      "Jaroslava Hlavacova",
      "Mariia Anisimova",
      "Tereza Chlanova"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.272": {
    "title": "Charting the Linguistic Landscape of Developing Writers: An Annotation Scheme for Enhancing Native Language Proficiency",
    "volume": "main",
    "abstract": "This study describes a pilot annotation task designed to capture orthographic, grammatical, lexical, semantic, and discursive patterns exhibited by college native English speakers participating in developmental education (DevEd) courses. The paper introduces an annotation scheme developed by two linguists aiming at pinpointing linguistic challenges that hinder effective written communication. The scheme builds upon patterns supported by the literature, which are known as predictors of student placement in DevEd courses and English proficiency levels. Other novel, multilayered, linguistic aspects that the literature has not yet explored are also presented. The scheme and its primary categories are succinctly presented and justified. Two trained annotators used this scheme to annotate a sample of 103 text units (3 during the training phase and 100 during the annotation task proper). Texts were randomly selected from a population of 290 community college intending students. An in-depth quality assurance inspection was conducted to assess tagging consistency between annotators and to discern (and address) annotation inaccuracies. Krippendorff's Alpha (K-alpha) interrater reliability coefficients were calculated, revealing a K-alpha score of k=0.40, which corresponds to a moderate level of agreement, deemed adequate for the complexity and length of the annotation task",
    "checked": true,
    "id": "64d1d04f4eab6467a2877d7882c665e561988908",
    "semantic_title": "charting the linguistic landscape of developing writers: an annotation scheme for enhancing native language proficiency",
    "citation_count": 0,
    "authors": [
      "Miguel Da Corte",
      "Jorge Baptista"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.273": {
    "title": "ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart Summarization",
    "volume": "main",
    "abstract": "Data visualization serves as a critical means for presenting data and mining its valuable insights. The task of chart summarization, through natural language processing techniques, facilitates in-depth data analysis of charts. However, there still are notable deficiencies in terms of visual-language matching and reasoning ability for existing approaches. To address these limitations, this study constructs a large-scale dataset of comprehensive chart-caption pairs and fine-tuning instructions on each chart. Thanks to the broad coverage of various topics and visual styles within this dataset, better matching degree can be achieved from the view of training data. Moreover, we propose an innovative chart summarization method, ChartThinker, which synthesizes deep analysis based on chains of thought and strategies of context retrieval, aiming to improve the logical coherence and accuracy of the generated summaries. Built upon the curated datasets, our trained model consistently exhibits superior performance in chart summarization tasks, surpassing 8 state-of-the-art models over 7 evaluation metrics. Our dataset and codes are publicly accessible",
    "checked": true,
    "id": "e3778b277248123d769eb43fd3d1987f758b3b77",
    "semantic_title": "chartthinker: a contextual chain-of-thought approach to optimized chart summarization",
    "citation_count": 0,
    "authors": [
      "Mengsha Liu",
      "Daoyuan Chen",
      "Yaliang Li",
      "Guian Fang",
      "Ying Shen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.274": {
    "title": "ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues",
    "volume": "main",
    "abstract": "Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU. Specifically, this TSA treats the ACR task as an auxiliary task to boost the performance of the primary ASU task, and further integrates trusted learning into reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination problem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to evaluate TSA, and extensive experiments show that our proposed TSA can significantly outperform several state-of-the-art baselines, justifying the effectiveness of TSA to ChatASU and the importance of considering the coreference and hallucination issues in ChatASU",
    "checked": true,
    "id": "dd6f91153a046538d77637c4c61057a2cdc2cae6",
    "semantic_title": "chatasu: evoking llm's reflexion to truly understand aspect sentiment in dialogues",
    "citation_count": 0,
    "authors": [
      "Yiding Liu",
      "Jingjing Wang",
      "Jiamin Luo",
      "Tao Zeng",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.275": {
    "title": "ChatEL: Entity Linking with Chatbots",
    "volume": "main",
    "abstract": "Entity Linking (EL) is an essential and challenging task in natural language processing that seeks to link some text representing an entity within a document or sentence with its corresponding entry in a dictionary or knowledge base. Most existing approaches focus on creating elaborate contextual models that look for clues the words surrounding the entity-text to help solve the linking problem. Although these fine-tuned language models tend to work, they can be unwieldy, difficult to train, and do not transfer well to other domains. Fortunately, Large Language Models (LLMs) like GPT provide a highly-advanced solution to the problems inherent in EL models, but simply naive prompts to LLMs do not work well. In the present work, we define ChatEL, which is a three-step framework to prompt LLMs to return accurate results. Overall the ChatEL framework improves the average F1 performance across 10 datasets by more than 2%. Finally, a thorough error analysis shows many instances with the ground truth labels were actually incorrect, and the labels predicted by ChatEL were actually correct. This indicates that the quantitative results presented in this paper may be a conservative estimate of the actual performance. All data and code are available as an open-source package on GitHub at https://github.com/yifding/In_Context_EL",
    "checked": true,
    "id": "3f0d81261c042fddffffc19776199ee884834577",
    "semantic_title": "chatel: entity linking with chatbots",
    "citation_count": 1,
    "authors": [
      "Yifan Ding",
      "Qingkai Zeng",
      "Tim Weninger"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.276": {
    "title": "ChatGPT Is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have made significant progress in NLP. However, their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point. In this paper, we specifically focus on ChatGPT, a widely used and easily accessible LLM, and ask the following questions: (1) Can ChatGPT effectively answer commonsense questions? (2) Is ChatGPT aware of the underlying commonsense knowledge for answering a specific question? (3) Is ChatGPT knowledgeable in commonsense? (4) Can ChatGPT effectively leverage commonsense for answering questions? We conduct a series of experiments on 11 datasets to evaluate ChatGPT's commonsense abilities, including answering commonsense questions, identifying necessary knowledge, generating knowledge descriptions, and using knowledge descriptions to answer questions again. Experimental results show that: (1) ChatGPT can achieve good QA accuracies in commonsense tasks, while still struggling with certain domains of datasets. (2) ChatGPT is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense for answering a specific question. These findings raise the need to explore improved mechanisms for effectively incorporating commonsense into LLMs like ChatGPT, such as better instruction following and commonsense guidance",
    "checked": true,
    "id": "4d7571441f507f39133209e8afa7ad088da2199c",
    "semantic_title": "chatgpt is a knowledgeable but inexperienced solver: an investigation of commonsense problem in large language models",
    "citation_count": 55,
    "authors": [
      "Ning Bian",
      "Xianpei Han",
      "Le Sun",
      "Hongyu Lin",
      "Yaojie Lu",
      "Ben He",
      "Shanshan Jiang",
      "Bin Dong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.277": {
    "title": "ChatGPT Rates Natural Language Explanation Quality like Humans: But on Which Scales?",
    "volume": "main",
    "abstract": "As AI becomes more integral in our lives, the need for transparency and responsibility grows. While natural language explanations (NLEs) are vital for clarifying the reasoning behind AI decisions, evaluating them through human judgments is complex and resource-intensive due to subjectivity and the need for fine-grained ratings. This study explores the alignment between ChatGPT and human assessments across multiple scales (i.e., binary, ternary, and 7-Likert scale). We sample 300 data instances from three NLE datasets and collect 900 human annotations for both informativeness and clarity scores as the text quality measurement. We further conduct paired comparison experiments under different ranges of subjectivity scores, where the baseline comes from 8,346 human annotations. Our results show that ChatGPT aligns better with humans in more coarse-grained scales. Also, paired comparisons and dynamic prompting (i.e., providing semantically similar examples in the prompt) improve the alignment. This research advances our understanding of large language models' capabilities to assess the text explanation quality in different configurations for responsible AI development",
    "checked": true,
    "id": "40f3aaf33ffd2c6687e405cffdb08f2dcc1f8ce4",
    "semantic_title": "chatgpt rates natural language explanation quality like humans: but on which scales?",
    "citation_count": 0,
    "authors": [
      "Fan Huang",
      "Haewoon Kwak",
      "Kunwoo Park",
      "Jisun An"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.278": {
    "title": "ChatGPT Role-play Dataset: Analysis of User Motives and Model Naturalness",
    "volume": "main",
    "abstract": "Recent advances in interactive large language models like ChatGPT have revolutionized various domains; however, their behavior in natural and role-play conversation settings remains underexplored. In our study, we address this gap by deeply investigating how ChatGPT behaves during conversations in different settings by analyzing its interactions in both a normal way and a role-play setting. We introduce a novel dataset of broad range of human-AI conversations annotated with user motives and model naturalness to examine (i) how humans engage with the conversational AI model, and (ii) how natural are AI model responses. Our study highlights the diversity of user motives when interacting with ChatGPT and variable AI naturalness, showing not only the nuanced dynamics of natural conversations between humans and AI, but also providing new avenues for improving the effectiveness of human-AI communication",
    "checked": true,
    "id": "3a6af940845c376c012967b23ce2ab4b5f50b7d8",
    "semantic_title": "chatgpt role-play dataset: analysis of user motives and model naturalness",
    "citation_count": 0,
    "authors": [
      "Yufei Tao",
      "Ameeta Agrawal",
      "Judit Dombi",
      "Tetyana Sydorenko",
      "Jung In Lee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.279": {
    "title": "ChatUIE: Exploring Chat-based Unified Information Extraction Using Large Language Models",
    "volume": "main",
    "abstract": "Recent advancements in large language models have shown impressive performance in general chat. However, their domain-specific capabilities, particularly in information extraction, have certain limitations. Extracting structured information from natural language that deviates from known schemas or instructions has proven challenging for previous prompt-based methods. This motivated us to explore domain-specific modeling in chat-based language models as a solution for extracting structured information from natural language. In this paper, we present ChatUIE, an innovative unified information extraction framework built upon ChatGLM. Simultaneously, reinforcement learning is employed to improve and align various tasks that involve confusing and limited samples. Furthermore, we integrate generation constraints to address the issue of generating elements that are not present in the input. Our experimental results demonstrate that ChatUIE can significantly improve the performance of information extraction with a slight decrease in chatting ability",
    "checked": true,
    "id": "d9fec1c2e4676b74efb37dd00c0f90fe6a8b6340",
    "semantic_title": "chatuie: exploring chat-based unified information extraction using large language models",
    "citation_count": 0,
    "authors": [
      "Jun Xu",
      "Mengshu Sun",
      "Zhiqiang Zhang",
      "Jun Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.280": {
    "title": "CHICA: A Developmental Corpus of Child-Caregiver's Face-to-face vs. Video Call Conversations in Middle Childhood",
    "volume": "main",
    "abstract": "Existing studies of naturally occurring language-in-interaction have largely focused on the two ends of the developmental spectrum, i.e., early childhood and adulthood, leaving a gap in our knowledge about how development unfolds, especially across middle childhood. The current work contributes to filling this gap by introducing CHICA (for Child Interpersonal Communication Analysis), a developmental corpus of child-caregiver conversations at home, involving groups of French-speaking children aged 7, 9, and 11 years old. Each dyad was recorded twice: once in a face-to-face setting and once using computer-mediated video calls. For the face-to-face settings, we capitalized on recent advances in mobile, lightweight eye-tracking and head motion detection technology to optimize the naturalness of the recordings, allowing us to obtain both precise and ecologically valid data. Further, we mitigated the challenges of manual annotation by relying – to the extent possible – on automatic tools in speech processing and computer vision. Finally, to demonstrate the richness of this corpus for the study of child communicative development, we provide preliminary analyses comparing several measures of child-caregiver conversational dynamics across developmental age, modality, and communicative medium. We hope the current corpus will allow new discoveries into the properties and mechanisms of multimodal communicative development across middle childhood",
    "checked": true,
    "id": "e313e27afd054bbcecffec5d35059a2fe40bee74",
    "semantic_title": "chica: a developmental corpus of child-caregiver's face-to-face vs. video call conversations in middle childhood",
    "citation_count": 0,
    "authors": [
      "Dhia Elhak Goumri",
      "Abhishek Agrawal",
      "Mitja Nikolaus",
      "Hong Duc Thang Vu",
      "Kübra Bodur",
      "Elias Emmar",
      "Cassandre Armand",
      "Chiara Mazzocconi",
      "Shreejata Gupta",
      "Laurent Prévot",
      "Benoit Favre",
      "Leonor Becerra-Bonache",
      "Abdellah Fourtassi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.281": {
    "title": "Chinese Morpheme-informed Evaluation of Large Language Models",
    "volume": "main",
    "abstract": "Previous evaluations of large language models (LLMs) focused on the perspective of various tasks or abilities. In this paper, we propose to evaluate from a linguistic viewpoint and argue that morpheme, a potential linguistic feature that captures both word-formation and lexical semantics, is another suitable component for evaluation that remains largely unexplored. In light of this, we construct MorphEval, a morpheme-informed benchmark, including three datasets following the bottom-up levels of characters, words, and sentences in Chinese, and then evaluate representative LLMs with both zero- and few-shot settings under two metrics. From this perspective, we reveal three aspects of issues LLMs nowadays encounter: dysfunctions in morphology and syntax, challenges with the long-tailed distribution of semantics, and difficulties from cultural implications. In these scenarios, even a smaller Chinese-targeted model may outperform ChatGPT, highlighting the actual challenges LLMs face and the necessity of language-specific improvements when applied to non-English languages. This new approach could also help guide model enhancements as well as get extended to other languages",
    "checked": true,
    "id": "9e36be70088416fa2fc8e9dd971e7d127dc62b06",
    "semantic_title": "chinese morpheme-informed evaluation of large language models",
    "citation_count": 0,
    "authors": [
      "Yaqi Yin",
      "Yue Wang",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.282": {
    "title": "Chinese Sequence Labeling with Semi-Supervised Boundary-Aware Language Model Pre-training",
    "volume": "main",
    "abstract": "Chinese sequence labeling tasks are sensitive to word boundaries. Although pretrained language models (PLM) have achieved considerable success in these tasks, current PLMs rarely consider boundary information explicitly. An exception to this is BABERT, which incorporates unsupervised statistical boundary information into Chinese BERT's pre-training objectives. Building upon this approach, we input supervised high-quality boundary information to enhance BABERT's learning, developing a semi-supervised boundary-aware PLM. To assess PLMs' ability to encode boundaries, we introduce a novel \"Boundary Information Metric\" that is both simple and effective. This metric allows comparison of different PLMs without task-specific fine-tuning. Experimental results on Chinese sequence labeling datasets demonstrate that the improved BABERT version outperforms the vanilla version, not only in these tasks but also in broader Chinese natural language understanding tasks. Additionally, our proposed metric offers a convenient and accurate means of evaluating PLMs' boundary awareness",
    "checked": true,
    "id": "898c7d93afbbad043085c6e9edc7b6854de8a66c",
    "semantic_title": "chinese sequence labeling with semi-supervised boundary-aware language model pre-training",
    "citation_count": 0,
    "authors": [
      "Longhui Zhang",
      "Dingkun Long",
      "Meishan Zhang",
      "Yanzhao Zhang",
      "Pengjun Xie",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.283": {
    "title": "CHisIEC: An Information Extraction Corpus for Ancient Chinese History",
    "volume": "main",
    "abstract": "Natural Language Processing (NLP) plays a pivotal role in the realm of Digital Humanities (DH) and serves as the cornerstone for advancing the structural analysis of historical and cultural heritage texts. This is particularly true for the domains of named entity recognition (NER) and relation extraction (RE). In our commitment to expediting ancient history and culture, we present the \"Chinese Historical Information Extraction Corpus\"(CHisIEC). CHisIEC is a meticulously curated dataset designed to develop and evaluate NER and RE tasks, offering a resource to facilitate research in the field. Spanning a remarkable historical timeline encompassing data from 13 dynasties spanning over 1830 years, CHisIEC epitomizes the extensive temporal range and text heterogeneity inherent in Chinese historical documents. The dataset encompasses four distinct entity types and twelve relation types, resulting in a meticulously labeled dataset comprising 14,194 entities and 8,609 relations. To establish the robustness and versatility of our dataset, we have undertaken comprehensive experimentation involving models of various sizes and paradigms. Additionally, we have evaluated the capabilities of Large Language Models (LLMs) in the context of tasks related to ancient Chinese history. The dataset and code are available at https://github.com/tangxuemei1995/CHisIEC",
    "checked": true,
    "id": "2f031f337f7f97b82db98030c5aee26b457015c2",
    "semantic_title": "chisiec: an information extraction corpus for ancient chinese history",
    "citation_count": 0,
    "authors": [
      "Xuemei Tang",
      "Qi Su",
      "Jun Wang",
      "Zekun Deng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.284": {
    "title": "Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues",
    "volume": "main",
    "abstract": "During task-oriented dialogues (TODs), human users naturally introduce chitchat that is beyond the immediate scope of the task, interfering with the flow of the conversation. To address this issue without the need for expensive manual data creation, we use few-shot prompting with Llama-2-70B to enhance the MultiWOZ dataset with user backstories, a typical example of chitchat interference in TODs. We assess the impact of this addition by testing two models: one trained solely on TODs and another trained on TODs with a preliminary chitchat interaction. Our analysis demonstrates that our enhanced dataset poses a challenge for these systems. Moreover, we demonstrate that our dataset can be effectively used for training purposes, enabling a system to consistently acknowledge the user's backstory while also successfully moving the task forward in the same turn, as confirmed by human evaluation. These findings highlight the benefits of generating novel chitchat-TOD scenarios to test TOD systems more thoroughly and improve their resilience to natural user interferences",
    "checked": true,
    "id": "8b5b9e9c675f781f59999cb61816905dd5fe0754",
    "semantic_title": "chitchat as interference: adding user backstories to task-oriented dialogues",
    "citation_count": 0,
    "authors": [
      "Armand Stricker",
      "Patrick Paroubek"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.285": {
    "title": "Choice-75: A Dataset on Decision Branching in Script Learning",
    "volume": "main",
    "abstract": "Script learning studies how daily events unfold. It enables machines to reason about narratives with implicit information. Previous works mainly consider a script as a linear sequence of events while ignoring the potential branches that arise due to people's circumstantial choices. We hence propose Choice-75, the first benchmark that challenges intelligent systems to make decisions given descriptive scenarios, containing 75 scripts and more than 600 scenarios. We also present preliminary results with current large language models (LLM). Although they demonstrate overall decent performances, there is still notable headroom in hard scenarios",
    "checked": true,
    "id": "27f165d57fa2c7020c87a6d0ab694c400d7d4493",
    "semantic_title": "choice-75: a dataset on decision branching in script learning",
    "citation_count": 2,
    "authors": [
      "Zhaoyi Hou",
      "Li Zhang",
      "Chris Callison-Burch"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.286": {
    "title": "C-Journal: A Journaling Application for Detecting and Classifying Cognitive Distortions Using Deep-Learning Based on a Crowd-sourced Dataset",
    "volume": "main",
    "abstract": "Cognitive distortions are negatively biased thinking patterns and erroneous self-statements resulting from and leading to logical errors in one's own internal reasoning. Cognitive distortions have an adverse effect on mental health and can lead to mental health disorders in extreme cases. This paper belongs to a bigger project which aims to provide an application for detecting and classifying cognitive distortions in texts. As no public data sets were available for the task, the first contribution of the proposed work lies in providing an open-source labeled dataset of 14 cognitive distortions consisting of 34370 entries collected via crowd-sourcing, user questionnaires, and re-purposing emotions dataset from social media. The dataset is collected in cooperation with a licensed psychologist. We implemented a baseline model using Naïve Bayes and Count Vectorizer and different CNN, LSTM, and DNN classifiers to classify cognitive distortions based on the dataset. We investigated the usage of different word embeddings with the best-performing models. The best-performing model relied on a CNN with pre-trained Sentence-BERT embedding with an F1-score of 84 % for classifying cognitive distortions. The best-performing model was built into C-Journal, a free journaling and mood-tracking mobile application that pinpoints potential thinking distortions to the users",
    "checked": true,
    "id": "090ffe1863011e44fec2f61c731e98a7179ebdb6",
    "semantic_title": "c-journal: a journaling application for detecting and classifying cognitive distortions using deep-learning based on a crowd-sourced dataset",
    "citation_count": 0,
    "authors": [
      "Nada Elsharawi",
      "Alia El Bolock"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.287": {
    "title": "Claim-Centric and Sentiment Guided Graph Attention Network for Rumour Detection",
    "volume": "main",
    "abstract": "Automatic rumour detection has gained attention due to the influence of social media on individuals and its pervasiveness. In this work, we construct a representation that takes into account the claim in the source tweet, considering both the propagation graph and the accompanying text alongside tweet sentiment. This is achieved through the implementation of a hierarchical attention mechanism, which not only captures the embedding of documents from individual word vectors but also combines these document representations as nodes within the propagation graph. Furthermore, to address potential overfitting concerns, we employ generative models to augment the existing datasets. This involves rephrasing the claims initially made in the source tweet, thereby creating a more diverse and robust dataset. In addition, we augment the dataset with sentiment labels to improve the performance of the rumour detection task. This holistic and refined approach yields a significant enhancement in the performance of our model across three distinct datasets designed for rumour detection. Quantitative and qualitative analysis proves the effectiveness of our methodology, surpassing the achievements of prior methodologies",
    "checked": true,
    "id": "2b6da5071347e882025e1afb35c76feb93bce518",
    "semantic_title": "claim-centric and sentiment guided graph attention network for rumour detection",
    "citation_count": 0,
    "authors": [
      "Sajad Ramezani",
      "Mauzama Firdaus",
      "Lili Mou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.288": {
    "title": "Clarifying Completions: Evaluating How LLMs Respond to Incomplete Questions",
    "volume": "main",
    "abstract": "People understand and produce language incrementally on a word by word basis. This gives rise to many characteristic conversational phenomena including long mid-sentence pauses that are followed by incremental clarification requests (iCRs) intended to recover the rest of the truncated turn (see Fig. 1; (A), (B), (C)). The ability to generate iCRs is important in natural conversational AI systems, and crucial to their accessibility to users with memory impairment. In this paper, we collect, release and analyse SLUICE-CR: a large corpus of 3000 human produced iCRs. We then use this corpus to probe the incremental processing capability of a number of state of the art LLMs by evaluating the quality of the model's generated iCRs in response to incomplete questions. Our evaluations show that the ability to generate contextually appropriate iCRs only emerges at larger LLM sizes, and only when prompted with example iCRs from our corpus. They also indicate that autoregressive LMs are, in principle, able to both understand and generate language incrementally",
    "checked": true,
    "id": "1481bafdb37bc1b1906c6bce4c6083874b3b4396",
    "semantic_title": "clarifying completions: evaluating how llms respond to incomplete questions",
    "citation_count": 0,
    "authors": [
      "Angus Addlesee",
      "Oliver Lemon",
      "Arash Eshghi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.289": {
    "title": "Classifying Social Media Users before and after Depression Diagnosis via Their Language Usage: A Dataset and Study",
    "volume": "main",
    "abstract": "Mental illness can significantly impact individuals' quality of life. Analysing social media data to uncover potential mental health issues in individuals via their posts is a popular research direction. However, most studies focus on the classification of users suffering from depression versus healthy users, or on the detection of suicidal thoughts. In this paper, we instead aim to understand and model linguistic changes that occur when users transition from a healthy to an unhealthy state. Addressing this gap could lead to better approaches for earlier depression detection when signs are not as obvious as in cases of severe depression or suicidal ideation. In order to achieve this goal, we have collected the first dataset of textual posts by the same users before and after reportedly being diagnosed with depression. We then use this data to build multiple predictive models (based on SVM, Random Forests, BERT, RoBERTa, MentalBERT, GPT-3, GPT-3.5, Bard, and Alpaca) for the task of classifying user posts. Transformer-based models achieved the best performance, while large language models used off-the-shelf proved less effective as they produced random guesses (GPT and Bard) or hallucinations (Alpaca)",
    "checked": true,
    "id": "b0d3ed986b710c8d0aa85be5553cd373103e0d6f",
    "semantic_title": "classifying social media users before and after depression diagnosis via their language usage: a dataset and study",
    "citation_count": 0,
    "authors": [
      "Falwah Alhamed",
      "Julia Ive",
      "Lucia Specia"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.290": {
    "title": "Class-Incremental Few-Shot Event Detection",
    "volume": "main",
    "abstract": "Event detection is one of the fundamental tasks in information extraction and knowledge graph. However, a realistic event detection system often needs to deal with new event classes constantly. These new classes usually have only a few labeled instances as it is time-consuming and labor-intensive to annotate a large number of unlabeled instances. Therefore, this paper proposes a new task, called class-incremental few-shot event detection. Nevertheless, there are two problems (i.e., old knowledge forgetting and new class overfitting) in this task. To solve these problems, this paper further presents a novel knowledge distillation and prompt learning based method, called Prompt-KD. Specifically, to reduce the forgetting issue about old knowledge, Prompt-KD develops an attention based multi-teacher knowledge distillation framework, where the ancestor teacher model pre-trained on base classes is reused in all learning sessions, and the father teacher model derives the current student model via adaptation. On the other hand, in order to cope with the few-shot learning scenario and alleviate the corresponding new class overfitting problem, Prompt-KD is also equipped with a prompt learning mechanism. Extensive experiments on two benchmark datasets, i.e., FewEvent and MAVEN, demonstrate the state-of-the-art performance of Prompt-KD",
    "checked": true,
    "id": "84e257f81da4dc526619e4bd3db5cfe1918ca3fc",
    "semantic_title": "class-incremental few-shot event detection",
    "citation_count": 0,
    "authors": [
      "Kailin Zhao",
      "Xiaolong Jin",
      "Long Bai",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.291": {
    "title": "CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched with Linguistic and Genre Annotation",
    "volume": "main",
    "abstract": "This paper presents a collection of highly comparable web corpora of Slovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian, covering thereby the whole spectrum of official languages in the South Slavic language space. The collection of these corpora comprises a total of 13 billion tokens of texts from 26 million documents. The comparability of the corpora is ensured by a comparable crawling setup and the usage of identical crawling and post-processing technology. All the corpora were linguistically annotated with the state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and enriched with document-level genre information via the Transformer-based multilingual X-GENRE classifier, which further enhances comparability at the level of linguistic annotation and metadata enrichment. The genre-focused analysis of the resulting corpora shows a rather consistent distribution of genres throughout the seven corpora, with variations in the most prominent genre categories being well-explained by the economic strength of each language community. A comparison of the distribution of genre categories across the corpora indicates that web corpora from less developed countries primarily consist of news articles. Conversely, web corpora from economically more developed countries exhibit a smaller proportion of news content, with a greater presence of promotional and opinionated texts",
    "checked": true,
    "id": "8a5f150fd0d4462269ac9e3e320f3ac6a7891e44",
    "semantic_title": "classla-web: comparable web corpora of south slavic languages enriched with linguistic and genre annotation",
    "citation_count": 0,
    "authors": [
      "Nikola Ljubešić",
      "Taja Kuzman"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.292": {
    "title": "CLAUSE-ATLAS: A Corpus of Narrative Information to Scale up Computational Literary Analysis",
    "volume": "main",
    "abstract": "We introduce CLAUSE-ATLAS, a resource of XIX and XX century English novels annotated automatically. This corpus, which contains 41,715 labeled clauses, allows to study stories as sequences of eventive, subjective and contextual information. We use it to investigate if recent large language models, in particular gpt-3.5-turbo with 16k tokens of context, constitute promising tools to annotate large amounts of data for literary studies (we show that this is the case). Moreover, by analyzing the annotations so collected, we find that our clause-based approach to literature captures structural patterns within books, as well as qualitative differences between them",
    "checked": true,
    "id": "1fd4668115dcbec2fa09ff7379a90cc4db325495",
    "semantic_title": "clause-atlas: a corpus of narrative information to scale up computational literary analysis",
    "citation_count": 0,
    "authors": [
      "Enrica Troiano",
      "Piek T.J.M. Vossen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.293": {
    "title": "CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments",
    "volume": "main",
    "abstract": "The integration of learning and reasoning is high on the research agenda in AI. Nevertheless, there is only a little attention to using existing background knowledge for reasoning about partially observed scenes to answer questions about the scene. Yet, we as humans use such knowledge frequently to infer plausible answers to visual questions (by eliminating all inconsistent ones). Such knowledge often comes in the form of constraints about objects and it tends to be highly domain or environment specific. We contribute a novel benchmark called CLEVR-POC for reasoning-intensive visual question answering (VQA) in partially observable environments under constraints. In CLEVR-POC, knowledge in the form of logical constraints needs to be leveraged in order to generate plausible answers to questions about a hidden object in a given partial scene. For instance, if one has the knowledge that all cups are colored either red, green or blue and that there is only one green cup, it becomes possible to deduce the color of an occluded cup as either red or blue, provided that all other cups, including the green one, are observed. Through experiments we observe that the performance of pre-trained vision language models like CLIP (approx. 22%) and a large language model (LLM) like GPT-4 (approx. 46%) on CLEVR-POC are not satisfactory, ascertaining the necessity for frameworks that can handle reasoning-intensive tasks where environment-specific background knowledge is available and crucial. Furthermore, our demonstration illustrates that a neuro-symbolic model, which integrates an LLM like GPT-4 with a visual perception network and a formal logical reasoner, exhibits exceptional performance on CLEVR-POC",
    "checked": true,
    "id": "472273b571268c148fba4e882608429e30fcc34a",
    "semantic_title": "clevr-poc: reasoning-intensive visual question answering in partially observable environments",
    "citation_count": 0,
    "authors": [
      "Savitha Sam Abraham",
      "Marjan Alirezaie",
      "Luc de Raedt"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.294": {
    "title": "CLFFRD: Curriculum Learning and Fine-grained Fusion for Multimodal Rumor Detection",
    "volume": "main",
    "abstract": "In an era where rumors can propagate rapidly across social media platforms such as Twitter and Weibo, automatic rumor detection has garnered considerable attention from both academia and industry. Existing multimodal rumor detection models often overlook the intricacies of sample difficulty, e.g., text-level difficulty, image-level difficulty, and multimodal-level difficulty, as well as their order when training. Inspired by the concept of curriculum learning, we propose the Curriculum Learning and Fine-grained Fusion-driven multimodal Rumor Detection (CLFFRD) framework, which employs curriculum learning to automatically select and train samples according to their difficulty at different training stages. Furthermore, we introduce a fine-grained fusion strategy that unifies entities from text and objects from images, enhancing their semantic cohesion. We also propose a novel data augmentation method that utilizes linear interpolation between textual and visual modalities to generate diverse data. Additionally, our approach incorporates deep fusion for both intra-modality (e.g., text entities and image objects) and inter-modality (e.g., CLIP and social graph) features. Extensive experimental results demonstrate that CLFFRD outperforms state-of-the-art models on both English and Chinese benchmark datasets for rumor detection in social media",
    "checked": true,
    "id": "6eea3496dfd9a29c398fa440583ab2537ee21ca2",
    "semantic_title": "clffrd: curriculum learning and fine-grained fusion for multimodal rumor detection",
    "citation_count": 0,
    "authors": [
      "Fan Xu",
      "Lei Zeng",
      "Bowei Zou",
      "Ai Ti Aw",
      "Huan Rong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.295": {
    "title": "CLHA: A Simple Yet Effective Contrastive Learning Framework for Human Alignment",
    "volume": "main",
    "abstract": "Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users. However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training. To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly. CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process. Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences. Using advanced methods, CLHA surpasses other algorithms, showcasing superior performance in terms of reward model scores, automatic evaluations, and human assessments on the widely used \"Helpful and Harmless\" dataset",
    "checked": true,
    "id": "ce13bba037382e240411dcb16105012f42335387",
    "semantic_title": "clha: a simple yet effective contrastive learning framework for human alignment",
    "citation_count": 0,
    "authors": [
      "Feiteng Fang",
      "Liang Zhu",
      "Xi Feng",
      "Jinchang Hou",
      "Qixuan Zhao",
      "Chengming Li",
      "Xiping Hu",
      "Ruifeng Xu",
      "Min Yang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.296": {
    "title": "CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean",
    "volume": "main",
    "abstract": "Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge. Because many existing Korean benchmark datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts. For the few benchmark datasets that are sourced from Korean data capturing cultural knowledge, only narrow tasks such as hate speech detection are offered. To address this gap, we introduce a benchmark of Cultural and Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs. CLIcK sources its data from official Korean exams and textbooks, partitioning the questions into eleven categories under the two main categories of language and culture. For each instance in click, we provide fine-grained annotation of which cultural and linguistic knowledge is required to correctly answer the question. Using CLIcK, we test 13 language models to assess their performance. Our evaluation uncovers insights into their performances across the categories, as well as the diverse factors affecting their comprehension. CLIcK offers the first large-scale comprehensive Korean-centric analysis of LLMs' proficiency in Korean language and culture",
    "checked": true,
    "id": "6ee1964f089db48c9750c99ce658fe7f17a33477",
    "semantic_title": "click: a benchmark dataset of cultural and linguistic intelligence in korean",
    "citation_count": 2,
    "authors": [
      "Eunsu Kim",
      "Juyoung Suk",
      "Philhoon Oh",
      "Haneul Yoo",
      "James Thorne",
      "Alice Oh"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.297": {
    "title": "Clue-Instruct: Text-Based Clue Generation for Educational Crossword Puzzles",
    "volume": "main",
    "abstract": "Crossword puzzles are popular linguistic games often used as tools to engage students in learning. Educational crosswords are characterized by less cryptic and more factual clues that distinguish them from traditional crossword puzzles. Despite there exist several publicly available clue-answer pair databases for traditional crosswords, educational clue-answer pairs datasets are missing. In this article, we propose a methodology to build educational clue generation datasets that can be used to instruct Large Language Models (LLMs). By gathering from Wikipedia pages informative content associated with relevant keywords, we use Large Language Models to automatically generate pedagogical clues related to the given input keyword and its context. With such an approach, we created clue-instruct, a dataset containing 44,075 unique examples with text-keyword pairs associated with three distinct crossword clues. We used clue-instruct to instruct different LLMs to generate educational clues from a given input content and keyword. Both human and automatic evaluations confirmed the quality of the generated clues, thus validating the effectiveness of our approach",
    "checked": true,
    "id": "d69ff16e5244afe8c28e0fb5aeb758fcb83b9eb5",
    "semantic_title": "clue-instruct: text-based clue generation for educational crossword puzzles",
    "citation_count": 1,
    "authors": [
      "Andrea Zugarini",
      "Kamyar Zeinalipour",
      "Surya Sai Kadali",
      "Marco Maggini",
      "Marco Gori",
      "Leonardo Rigutini"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.298": {
    "title": "CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for Boosting Metaphor Generation",
    "volume": "main",
    "abstract": "Metaphor is a prominent linguistic device in human language and literature, as they add color, imagery, and emphasis to enhance effective communication. This paper introduces a large-scale high quality annotated Chinese Metaphor Corpus, which comprises around 28K sentences drawn from a diverse range of Chinese literary sources, such as poems, prose, song lyrics, etc. To ensure the accuracy and consistency of our annotations, we introduce a comprehensive set of guidelines. These guidelines address the facets of metaphor annotation, including identifying tenors, vehicles, and grounds to handling the complexities of similes, personifications, juxtapositions, and hyperboles. Breaking tradition, our approach to metaphor generation emphasizes tenors and their distinct features rather than the conventional combination of tenors and vehicles. By integrating \"ground\" as a CoT (Chain of Thoughts) input, we are able to generate metaphors that resonate more with real-world intuition. We test generative models such as Belle, Baichuan, and Chinese-alpaca-33B using our annotated corpus. These models are able to generate creative and fluent metaphor sentences more frequently induced by selected samples from our dataset, demonstrating the value of our corpus for Chinese metaphor research",
    "checked": true,
    "id": "c4d76cfb9bf2ffcdd38739b41840141507bee0d3",
    "semantic_title": "cmdag: a chinese metaphor dataset with annotated grounds as cot for boosting metaphor generation",
    "citation_count": 1,
    "authors": [
      "Yujie Shao",
      "Xinrong Yao",
      "Xingwei Qu",
      "Chenghua Lin",
      "Shi Wang",
      "Wenhao Huang",
      "Ge Zhang",
      "Jie Fu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.299": {
    "title": "CMNEE:A Large-Scale Document-Level Event Extraction Dataset Based on Open-Source Chinese Military News",
    "volume": "main",
    "abstract": "Extracting structured event knowledge, including event triggers and corresponding arguments, from military texts is fundamental to many applications, such as intelligence analysis and decision assistance. However, event extraction in the military field faces the data scarcity problem, which impedes the research of event extraction models in this domain. To alleviate this problem, we propose CMNEE, a large-scale, document-level open-source Chinese Military News Event Extraction dataset. It contains 17,000 documents and 29,223 events, which are all manually annotated based on a pre-defined schema for the military domain including 8 event types and 11 argument role types. We designed a two-stage, multi-turns annotation strategy to ensure the quality of CMNEE and reproduced several state-of-the-art event extraction models with a systematic evaluation. The experimental results on CMNEE fall shorter than those on other domain datasets obviously, which demonstrates that event extraction for military domain poses unique challenges and requires further research efforts. Our code and data can be obtained from https://github.com/Mzzzhu/CMNEE. Keywords: Corpus,Information Extraction, Information Retrieval, Knowledge Discovery/Representation",
    "checked": true,
    "id": "1f862b3fe4cd9c02bc9f1551c07cc8d626421a25",
    "semantic_title": "cmnee:a large-scale document-level event extraction dataset based on open-source chinese military news",
    "citation_count": 0,
    "authors": [
      "Mengna Zhu",
      "Zijie Xu",
      "Kaisheng Zeng",
      "Kaiming Xiao",
      "Mao Wang",
      "Wenjun Ke",
      "Hongbin Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.300": {
    "title": "CM-Off-Meme: Code-Mixed Hindi-English Offensive Meme Detection with Multi-Task Learning by Leveraging Contextual Knowledge",
    "volume": "main",
    "abstract": "Detecting offensive content in internet memes is challenging as it needs additional contextual knowledge. While previous works have only focused on detecting offensive memes, classifying them further into implicit and explicit categories depending on their severity is still a challenging and underexplored area. In this work, we present an end-to-end multitask model for addressing this challenge by empirically investigating two correlated tasks simultaneously: (i) offensive meme detection and (ii) explicit-implicit offensive meme detection by leveraging the two self-supervised pre-trained models. The first pre-trained model, referred to as the \"knowledge encoder,\" incorporates contextual knowledge of the meme. On the other hand, the second model, referred to as the \"fine-grained information encoder\", is trained to understand the obscure psycho-linguistic information of the meme. Our proposed model utilizes contrastive learning to integrate these two pre-trained models, resulting in a more comprehensive understanding of the meme and its potential for offensiveness. To support our approach, we create a large-scale dataset, CM-Off-Meme, as there is no publicly available such dataset for the code-mixed Hindi-English (Hinglish) domain. Empirical evaluation, including both qualitative and quantitative analysis, on the CM-Off-Meme dataset demonstrates the effectiveness of the proposed model in terms of cross-domain generalization",
    "checked": true,
    "id": "f4f8bb3d9d9f93d74da73e40397381dae243d17a",
    "semantic_title": "cm-off-meme: code-mixed hindi-english offensive meme detection with multi-task learning by leveraging contextual knowledge",
    "citation_count": 0,
    "authors": [
      "Gitanjali Kumari",
      "Dibyanayan Bandyopadhyay",
      "Asif Ekbal",
      "Vinutha B. NarayanaMurthy"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.301": {
    "title": "CO3: Low-resource Contrastive Co-training for Generative Conversational Query Rewrite",
    "volume": "main",
    "abstract": "Generative query rewrite generates reconstructed query rewrites using the conversation history while rely heavily on gold rewrite pairs that are expensive to obtain. Recently, few-shot learning is gaining increasing popularity for this task, whereas these methods are sensitive to the inherent noise due to limited data size. Besides, both attempts face performance degradation when there exists language style shift between training and testing cases. To this end, we study low-resource generative conversational query rewrite that is robust to both noise and language style shift. The core idea is to utilize massive unlabeled data to make further improvements via a contrastive co-training paradigm. Specifically, we co-train two dual models (namely Rewriter and Simplifier) such that each of them provides extra guidance through pseudo-labeling for enhancing the other in an iterative manner. We also leverage contrastive learning with data augmentation, which enables our model pay more attention on the truly valuable information than the noise. Extensive experiments demonstrate the superiority of our model under both few-shot and zero-shot scenarios. We also verify the better generalization ability of our model when encountering language style shift",
    "checked": true,
    "id": "6b09867cfcddcd3a502d1a6b84b00bc76f4d9cd7",
    "semantic_title": "co3: low-resource contrastive co-training for generative conversational query rewrite",
    "citation_count": 0,
    "authors": [
      "Yifei Yuan",
      "Chen Shi",
      "Wang Runze",
      "Liyi Chen",
      "Renjun Hu",
      "Zengming Zhang",
      "Feijun Jiang",
      "Wai Lam"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.302": {
    "title": "CoANZSE Audio: Creation of an Online Corpus for Linguistic and Phonetic Analysis of Australian and New Zealand Englishes",
    "volume": "main",
    "abstract": "CoANZSE Audio is a searchable online version of the Corpus of Australian and New Zealand Spoken English, a 195-million-word collection of geo-located YouTube transcripts of local government channels. In addition to the part-of-speech-tagged and lemmatized transcript data, CoANZSE Audio provides access to almost all of the underlying audio, as well as to forced alignments of the audio with transcript content, in Praat's TextGrid format. This paper describes the methods used to create the corpus from open-source tools and the architecture of the CoANZSE Audio website. Two possible linguistic analyses based on CoANZSE Audio data are described: use of double modals, a rare syntactic feature, and raising of the mid front vowel /ɛ/ in New Zealand English. CoANZSE Audio can be considered to be among the first large, free, fully searchable online corpora containing data suitable for acoustic phonetic analyses in addition to lexical, grammatical, and discourse properties of Australian and New Zealand Englishes",
    "checked": true,
    "id": "d4baf9e4d849ed4eebb569da87286a7c0800b05b",
    "semantic_title": "coanzse audio: creation of an online corpus for linguistic and phonetic analysis of australian and new zealand englishes",
    "citation_count": 0,
    "authors": [
      "Steven Coats"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.303": {
    "title": "Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models",
    "volume": "main",
    "abstract": "Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations",
    "checked": true,
    "id": "66c99a60826402ddf6d35e7472ef52d0c353fd8a",
    "semantic_title": "coarse-tuning for ad-hoc document retrieval using pre-trained language models",
    "citation_count": 0,
    "authors": [
      "Atsushi Keyaki",
      "Ribeka Keyaki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.304": {
    "title": "CoBaLD Annotation: The Enrichment of the Enhanced Universal Dependencies with the Semantical Pattern",
    "volume": "main",
    "abstract": "The paper is devoted to the annotation format aimed at morphological, syntactic and especially semantic markup. The format combines the Enhanced UD morphosyntax and the Compreno semantic pattern, enriching the UD annotation with word meanings and labels for semantic relations between words. To adapt the Compreno semantics for the current purpose, we reduced the number of the semantic fields denoting lexical meanings by using hyperonym fields. Moreover, we used a generalized variant of the semantic relations as the original roles possess rather narrow meanings which makes them too numerous. Creating such a format demands the Compreno-to-UD morphosyntax conversion as well, which, in turn, demands solving the asymmetry problem between the models. The asymmetry concerns tokenization, lemmatization, POS-tagging, sets of grammatical features and dependency heads. To overcome this problem, the Compreno-to-UD converter was created. As an application, the work presents a 150,000 token corpus of English news annotated according to the standard",
    "checked": true,
    "id": "acb9ea2ede1c153d34c26910c735ec228fefa59a",
    "semantic_title": "cobald annotation: the enrichment of the enhanced universal dependencies with the semantical pattern",
    "citation_count": 0,
    "authors": [
      "Maria Andreevna Petrova",
      "Alexandra M. Ivoylova",
      "Anastasia Tishchenkova"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.305": {
    "title": "CoCoMIC: Code Completion by Jointly Modeling In-file and Cross-file Context",
    "volume": "main",
    "abstract": "While pre-trained language models (LM) for code have achieved great success in code completion, they generate code conditioned only on the contents within the file, i.e., in-file context, but ignore the rich semantics in other files within the same project, i.e., project-level cross-file context, a critical source of information that is especially useful in modern modular software development. Such overlooking constrains code LMs' capacity in code completion, leading to unexpected behaviors such as generating hallucinated class member functions or function calls with unexpected arguments. In this work, we propose CoCoMIC, a novel framework that jointly learns the in-file and cross-file context on top of code LMs. To empower CoCoMIC, we develop CCFinder, a static-analysis-based tool that locates and retrieves the most relevant project-level cross-file context for code completion. CoCoMIC successfully improves the existing code LM with a 33.94% relative increase in exact match and 28.69% in identifier matching for code completion when the cross-file context is provided. Finally, we perform a series of ablation studies and share valuable insights for future research on integrating cross-file context into code LMs",
    "checked": true,
    "id": "7b00c4deb35471194bbbbd338191165d53167fe5",
    "semantic_title": "cocomic: code completion by jointly modeling in-file and cross-file context",
    "citation_count": 30,
    "authors": [
      "Yangruibo Ding",
      "Zijian Wang",
      "Wasi U. Ahmad",
      "Murali Krishna Ramanathan",
      "Ramesh Nallapati",
      "Parminder Bhatia",
      "Dan Roth",
      "Bing Xiang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.306": {
    "title": "Code Defect Detection Using Pre-trained Language Models with Encoder-Decoder via Line-Level Defect Localization",
    "volume": "main",
    "abstract": "Recently, code Pre-trained Language Models (PLMs) trained on large amounts of code and comment, have shown great success in code defect detection tasks. However, most PLMs simply treated the code as a single sequence and only used the encoder of PLMs to determine if there exist defects in the entire code. For a more analyzable and explainable approach, it is crucial to identify which lines contain defects. In this paper, we propose a novel method for code defect detection that integrates line-level defect localization into a unified training process. To identify code defects at the line-level, we convert the code into a sequence separated by lines using a special token. Then, to utilize the characteristic that both the encoder and decoder of PLMs process information differently, we leverage both the encoder and decoder for line-level defect localization. By learning code defect detection and line-level defect localization tasks in a unified manner, our proposed method promotes knowledge sharing between the two tasks. We demonstrate that our proposed method significantly improves performance on four benchmark datasets for code defect detection. Additionally, we show that our method can be easily integrated with ChatGPT",
    "checked": true,
    "id": "c7e215f20012022684d61b7ecb4e1cf8f728048b",
    "semantic_title": "code defect detection using pre-trained language models with encoder-decoder via line-level defect localization",
    "citation_count": 0,
    "authors": [
      "Jimin An",
      "YunSeok Choi",
      "Jee-Hyong Lee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.307": {
    "title": "Code-Mixed Probes Show How Pre-Trained Models Generalise on Code-Switched Text",
    "volume": "main",
    "abstract": "Code-switching is a prevalent linguistic phenomenon in which multilingual individuals seamlessly alternate between languages. Despite its widespread use online and recent research trends in this area, research in code-switching presents unique challenges, primarily stemming from the scarcity of labelled data and available resources. In this study we investigate how pre-trained Language Models handle code-switched text in three dimensions: a) the ability of PLMs to detect code-switched text, b) variations in the structural information that PLMs utilise to capture code-switched text, and c) the consistency of semantic information representation in code-switched text. To conduct a systematic and controlled evaluation of the language models in question, we create a novel dataset of well-formed naturalistic code-switched text along with parallel translations into the source languages. Our findings reveal that pre-trained language models are effective in generalising to code-switched text, shedding light on abilities of these models to generalise representations to CS corpora. We release all our code and data, including the novel corpus, at https://github.com/francesita/code-mixed-probes",
    "checked": true,
    "id": "499dea891b0ec28ebab773c6bb51c85358b32e06",
    "semantic_title": "code-mixed probes show how pre-trained models generalise on code-switched text",
    "citation_count": 0,
    "authors": [
      "Frances Adriana Laureano De Leon",
      "Harish Tayyar Madabushi",
      "Mark Lee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.308": {
    "title": "Code-Mixed Text Augmentation for Latvian ASR",
    "volume": "main",
    "abstract": "Code-mixing has become mainstream in the modern, globalised world and affects low-resource languages, such as Latvian, in particular. Solutions to developing an automatic speech recognition system (ASR) for code-mixed speech often rely on specially created audio-text corpora, which are expensive and time-consuming to create. In this work, we attempt to tackle code-mixed Latvian-English speech recognition by improving the language model (LM) of a hybrid ASR system. We make a distinction between inflected transliterations and phonetic transcriptions as two different foreign word types. We propose an inflected transliteration model and a phonetic transcription model for the automatic generation of said word types. We then leverage a large human-translated English-Latvian parallel text corpus to generate synthetic code-mixed Latvian sentences by substituting in generated foreign words. Using the newly created augmented corpora, we train a new LM and combine it with our existing Latvian acoustic model (AM). For evaluation, we create a specialised foreign word test set on which our methods yield up to 15% relative CER improvement. We then further validate these results in a human evaluation campaign",
    "checked": true,
    "id": "8082fb72e0fa5a071d69b3fbd98cce6218ad9e2d",
    "semantic_title": "code-mixed text augmentation for latvian asr",
    "citation_count": 0,
    "authors": [
      "Martins Kronis",
      "Askars Salimbajevs",
      "Mārcis Pinnis"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.309": {
    "title": "Cognitive Information Bottleneck: Extracting Minimal Sufficient Cognitive Language Processing Signals",
    "volume": "main",
    "abstract": "In Reinforcement Learning from Human Feedback (RLHF), explicit human feedback, such as rankings, is employed to align Natural Language Processing (NLP) models with human preferences. In contrast, the potential of implicit human feedback, encompassing cognitive processing signals like eye-tracking and brain activity, remains underexplored. These signals capture unconscious human responses but are often marred by noise and redundancy, complicating their application to specific tasks. To address this issue, we introduce the Cognitive Information Bottleneck (CIB), a method that extracts only the task-relevant information from cognitive processing signals. Grounded in the principles of the information bottleneck, CIB aims to learn representations that maximize the mutual information between the representations and targets while minimizing the mutual information between inputs and representations. By employing CIB to filter out redundant information from cognitive processing signals, our goal is to provide representations that are both minimal and sufficient. This approach enables more efficient fitting of models to inputs. Our results show that the proposed method outperforms existing methods in efficiently compressing various cognitive processing signals and significantly enhances performance on downstream tasks. Evaluated on public datasets, our model surpasses contemporary state-of-the-art models. Furthermore, by analyzing these compressed representations, we offer insights into how cognitive processing signals can be leveraged to improve performance",
    "checked": true,
    "id": "7d841a9ebd8c498fa57f0469b34c2263421e39ad",
    "semantic_title": "cognitive information bottleneck: extracting minimal sufficient cognitive language processing signals",
    "citation_count": 0,
    "authors": [
      "Yuto Harada",
      "Yohei Oseki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.310": {
    "title": "CollabKG: A Learnable Human-Machine-Cooperative Information Extraction Toolkit for (Event) Knowledge Graph Construction",
    "volume": "main",
    "abstract": "In order to construct or extend entity-centric and event-centric knowledge graphs (KG and EKG), the information extraction (IE) annotation toolkit is essential. However, existing IE toolkits have several non-trivial problems, such as not supporting multi-tasks, and not supporting automatic updates. In this work, we present CollabKG, a learnable human-machine-cooperative IE toolkit for KG and EKG construction. Specifically, for the multi-task issue, CollabKG unifies different IE subtasks, including named entity recognition (NER), entity-relation triple extraction (RE), and event extraction (EE), and supports both KG and EKG. Then, combining advanced prompting-based IE technology, the human-machine-cooperation mechanism with Large Language Models (LLMs) as the assistant machine is presented which can provide a lower cost as well as a higher performance. Lastly, owing to the two-way interaction between the human and machine, CollabKG with learning ability allows self-renewal. Besides, CollabKG has several appealing features (e.g., customization, training-free, and label propagation) that make the system powerful and high-productivity. We holistically compare our toolkit with other existing tools on these features. Human evaluation quantitatively illustrates that CollabKG significantly improves annotation quality, efficiency, and stability simultaneously",
    "checked": true,
    "id": "b31c461b7c5f8c11ffd53ba2c261935479e71a3f",
    "semantic_title": "collabkg: a learnable human-machine-cooperative information extraction toolkit for (event) knowledge graph construction",
    "citation_count": 0,
    "authors": [
      "Xiang Wei",
      "Yufeng Chen",
      "Ning Cheng",
      "Xingyu Cui",
      "Jinan Xu",
      "Wenjuan Han"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.311": {
    "title": "Collecting and Analyzing Dialogues in a Tagline Co-Writing Task",
    "volume": "main",
    "abstract": "The potential usage scenarios of dialogue systems will be greatly expanded if they are able to collaborate more creatively with humans. Many studies have examined ways of building such systems, but most of them focus on problem-solving dialogues, and relatively little research has been done on systems that can engage in creative collaboration with users. In this study, we designed a tagline co-writing task in which two people collaborate to create taglines via text chat, created an interface for data collection, and collected dialogue logs, editing logs, and questionnaire results. In total, we collected 782 Japanese dialogues. We describe the characteristic interactions comprising the tagline co-writing task and report the results of our analysis, in which we examined the kind of utterances that appear in the dialogues as well as the most frequent expressions found in highly rated dialogues in subjective evaluations. We also analyzed the relationship between subjective evaluations and workflow utilized in the dialogues and the interplay between taglines and utterances",
    "checked": true,
    "id": "ca7b5c0b5ae3fcb3579ae82be6e924993861346b",
    "semantic_title": "collecting and analyzing dialogues in a tagline co-writing task",
    "citation_count": 0,
    "authors": [
      "Xulin Zhou",
      "Takuma Ichikawa",
      "Ryuichiro Higashinaka"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.312": {
    "title": "Collecting Human-Agent Dialogue Dataset with Frontal Brain Signal toward Capturing Unexpressed Sentiment",
    "volume": "main",
    "abstract": "Multimodal information such as text and audiovisual data has been used for emotion/sentiment estimation during human-agent dialogue; however, user sentiments are not necessarily expressed explicitly during dialogues. Biosignals such as brain signals recorded using an electroencephalogram (EEG) sensor have been the subject of focus in affective computing regions to capture unexpressed emotional changes in a controlled experimental environment. In this study, we collect and analyze multimodal data with an EEG during a human-agent dialogue toward capturing unexpressed sentiment. Our contributions are as follows: (1) a new multimodal human-agent dialogue dataset is created, which includes not only text and audiovisual data but also frontal EEGs and physiological signals during the dialogue. In total, about 500-minute chat dialogues were collected from thirty participants aged 20 to 70. (2) We present a novel method for dealing with eye-blink noise for frontal EEGs denoising. This method applies facial landmark tracking to detect and delete eye-blink noise. (3) An experimental evaluation showed the effectiveness of the frontal EEGs. It improved sentiment estimation performance when used with other modalities by multimodal fusion, although it only has three channels",
    "checked": true,
    "id": "ab7850ef59f36efb5e277b8449a9903b5009dbc4",
    "semantic_title": "collecting human-agent dialogue dataset with frontal brain signal toward capturing unexpressed sentiment",
    "citation_count": 0,
    "authors": [
      "Shun Katada",
      "Ryu Takeda",
      "Kazunori Komatani"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.313": {
    "title": "Collecting Linguistic Resources for Assessing Children's Pronunciation of Nordic Languages",
    "volume": "main",
    "abstract": "This paper reports on the experience collecting a number of corpora of Nordic languages spoken by children. The aim of the data collection is providing annotated data to develop and evaluate computer assisted pronunciation assessment systems both for non-native children learning a Nordic language (L2) and for L1 children with speech sound disorder (SSD). The paper presents the challenges encountered recording and annotating data for Finnish, Swedish and Norwegian, as well as the ethical considerations related with making this data publicly available. We hope that sharing this experience will encourage others to collect similar data for other languages. Of the different data collections, we were able to make the Norwegian corpus publicly available in the hope that it will serve as a reference in pronunciation assessment research",
    "checked": true,
    "id": "877903939bba70c6bb554f7315477a60b7ec1bdf",
    "semantic_title": "collecting linguistic resources for assessing children's pronunciation of nordic languages",
    "citation_count": 0,
    "authors": [
      "Anne Marte Haug Olstad",
      "Anna Smolander",
      "Sofia Strömbergsson",
      "Sari Ylinen",
      "Minna Lehtonen",
      "Mikko Kurimo",
      "Yaroslav Getman",
      "Tamás Grósz",
      "Xinwei Cao",
      "Torbjørn Svendsen",
      "Giampiero Salvi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.314": {
    "title": "Combining Discourse Coherence with Large Language Models for More Inclusive, Equitable, and Robust Task-Oriented Dialogue",
    "volume": "main",
    "abstract": "Large language models (LLMs) are capable of generating well-formed responses, but using LLMs to generate responses on the fly is not yet feasible for many task-oriented systems. Modular architectures are often still required for safety and privacy guarantees on the output. We hypothesize that an offline generation approach using discourse theories, formal grammar rules, and LLMs can allow us to generate human-like, coherent text in a more efficient, robust, and inclusive manner within a task-oriented setting. To this end, we present the first discourse-aware multimodal task-oriented dialogue system that combines discourse theories with offline LLM generation. We deploy our bot as an app to the general public and keep track of the user ratings for six months. Our user ratings show an improvement from 2.8 to 3.5 out of 5 with the introduction of discourse coherence theories. We also show that our model reduces misunderstandings in the dialect of African-American Vernacular English from 93% to 57%. While terms of use prevent us from releasing our entire codebase, we release our code in a format that can be integrated into most existing dialogue systems",
    "checked": true,
    "id": "3fe363c021dc2844ebb66e676dde1945f97c9e18",
    "semantic_title": "combining discourse coherence with large language models for more inclusive, equitable, and robust task-oriented dialogue",
    "citation_count": 0,
    "authors": [
      "Katherine Atwell",
      "Mert Inan",
      "Anthony B. Sicilia",
      "Malihe Alikhani"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.315": {
    "title": "COMET for Low-Resource Machine Translation Evaluation: A Case Study of English-Maltese and Spanish-Basque",
    "volume": "main",
    "abstract": "Trainable metrics for machine translation evaluation have been scoring the highest correlations with human judgements in the latest meta-evaluations, outperforming traditional lexical overlap metrics such as BLEU, which is still widely used despite its well-known shortcomings. In this work we look at COMET, a prominent neural evaluation system proposed in 2020, to analyze the extent of its language support restrictions, and to investigate strategies to extend this support to new, under-resourced languages. Our case study focuses on English-Maltese and Spanish-Basque. We run a crowd-based evaluation campaign to collect direct assessments and use the annotated dataset to evaluate COMET-22, further fine-tune it, and to train COMET models from scratch for the two language pairs. Our analysis suggests that COMET's performance can be improved with fine-tuning, and that COMET can be highly susceptible to the distribution of scores in the training data, which especially impacts low-resource scenarios",
    "checked": true,
    "id": "977b47d24bbc991598f5e75a4351a305069854cd",
    "semantic_title": "comet for low-resource machine translation evaluation: a case study of english-maltese and spanish-basque",
    "citation_count": 0,
    "authors": [
      "Júlia Falcão",
      "Claudia Borg",
      "Nora Aranberri",
      "Kurt Abela"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.316": {
    "title": "COMICORDA: Dialogue Act Recognition in Comic Books",
    "volume": "main",
    "abstract": "Dialogue act (DA) recognition is usually realized from a speech signal that is transcribed and segmented into text. However, only a little work in DA recognition from images exists. Therefore, this paper concentrates on this modality and presents a novel DA recognition approach for image documents, namely comic books. To the best of our knowledge, this is the first study investigating dialogue acts from comic books and represents the first steps to building a model for comic book understanding. The proposed method is composed of the following steps: speech balloon segmentation, optical character recognition (OCR), and DA recognition itself. We use YOLOv8 for balloon segmentation, Google Vision for OCR, and Transformer-based models for DA classification. The experiments are performed on a newly created dataset comprising 1,438 annotated comic panels. It contains bounding boxes, transcriptions, and dialogue act annotation. We have achieved nearly 98% average precision for speech balloon segmentation and exceeded the accuracy of 70% for the DA recognition task. We also present an analysis of dialogue structure in the comics domain and compare it with the standard DA datasets, representing another contribution of this paper",
    "checked": true,
    "id": "5af1bbbf5dbd04a6fb267a93960917de62d47cf8",
    "semantic_title": "comicorda: dialogue act recognition in comic books",
    "citation_count": 0,
    "authors": [
      "Jiri Martinek",
      "Pavel Kral",
      "Ladislav Lenc",
      "Josef Baloun"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.317": {
    "title": "Common European Language Data Space",
    "volume": "main",
    "abstract": "The Common European Language Data Space (LDS) is an integral part of the EU data strategy, which aims at developing a single market for data. Its decentralised technical infrastructure and governance scheme are currently being developed by the LDS project, which also has dedicated tasks for proof-of-concept prototypes, handling legal aspects, raising awareness and promoting the LDS through events and social media channels. The LDS is part of a broader vision for establishing all necessary components to develop European large language models",
    "checked": true,
    "id": "d9659d094b926754cd7475867da4987449adfed5",
    "semantic_title": "common european language data space",
    "citation_count": 0,
    "authors": [
      "Georg Rehm",
      "Stelios Piperidis",
      "Khalid Choukri",
      "Andrejs Vasiļjevs",
      "Katrin Marheinecke",
      "Victoria Arranz",
      "Aivars Bērziņš",
      "Miltos Deligiannis",
      "Dimitris Galanis",
      "Maria Giagkou",
      "Katerina Gkirtzou",
      "Dimitris Gkoumas",
      "Annika Grützner-Zahn",
      "Athanasia Kolovou",
      "Penny Labropoulou",
      "Andis Lagzdiņš",
      "Elena Leitner",
      "Valérie Mapelli",
      "Hélène Mazo",
      "Simon Ostermann",
      "Stefania Racioppa",
      "Mickaël Rigault",
      "Leon Voukoutis"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.318": {
    "title": "Common Ground Tracking in Multimodal Dialogue",
    "volume": "main",
    "abstract": "Within Dialogue Modeling research in AI and NLP, considerable attention has been spent on \"dialogue state tracking\" (DST), which is the ability to update the representations of the speaker's needs at each turn in the dialogue by taking into account the past dialogue moves and history. Less studied but just as important to dialogue modeling, however, is \"common ground tracking\" (CGT), which identifies the shared belief space held by all of the participants in a task-oriented dialogue: the task-relevant propositions all participants accept as true. In this paper we present a method for automatically identifying the current set of shared beliefs and \"questions under discussion\" (QUDs) of a group with a shared goal. We annotate a dataset of multimodal interactions in a shared physical space with speech transcriptions, prosodic features, gestures, actions, and facets of collaboration, and operationalize these features for use in a deep neural model to predict moves toward construction of common ground. Model outputs cascade into a set of formal closure rules derived from situated evidence and belief axioms and update operations. We empirically assess the contribution of each feature type toward successful construction of common ground relative to ground truth, establishing a benchmark in this novel, challenging task",
    "checked": true,
    "id": "a21cd2f6db9668e28ea96a0c0be469d243934a6b",
    "semantic_title": "common ground tracking in multimodal dialogue",
    "citation_count": 1,
    "authors": [
      "Ibrahim Khalil Khebour",
      "Kenneth Lai",
      "Mariah Bradford",
      "Yifan Zhu",
      "Richard A. Brutti",
      "Christopher Tam",
      "Jingxuan Tu",
      "Benjamin A. Ibarra",
      "Nathaniel Blanchard",
      "Nikhil Krishnaswamy",
      "James Pustejovsky"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.319": {
    "title": "Comparative Analysis of Sign Language Interpreting Agents Perception: A Study of the Deaf",
    "volume": "main",
    "abstract": "Prior research on sign language recognition has already demonstrated encouraging outcomes in achieving highly accurate and dependable automatic sign language recognition. The use of virtual characters as virtual assistants has significantly increased in the past decade. However, the progress in sign language generation and output that closely resembles physiologically believable human motions is still in its early stages. This assertion explains the lack of progress in virtual intelligent signing generative systems. Aside from the development of signing systems, scholarly research have revealed a significant deficiency in evaluating sign language generation systems by those who are deaf and use sign language. This paper presents the findings of a user study conducted with deaf signers. The study is aimed at comparing a state-of-the-art sign language generation system with a skilled sign language interpreter. The study focused on testing established metrics to gain insights into usability of such metrics for deaf signers and how deaf signers perceive signing agents",
    "checked": true,
    "id": "0f1cfa0513534cfedd381e1e04fe5163636351bd",
    "semantic_title": "comparative analysis of sign language interpreting agents perception: a study of the deaf",
    "citation_count": 0,
    "authors": [
      "Alfarabi Imashev",
      "Nurziya Oralbayeva",
      "Gulmira Baizhanova",
      "Anara Sandygulova"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.320": {
    "title": "Comparing Static and Contextual Distributional Semantic Models on Intrinsic Tasks: An Evaluation on Mandarin Chinese Datasets",
    "volume": "main",
    "abstract": "The field of Distributional Semantics has recently undergone important changes, with the contextual representations produced by Transformers taking the place of static word embeddings models. Noticeably, previous studies comparing the two types of vectors have only focused on the English language and a limited number of models. In our study, we present a comparative evaluation of static and contextualized distributional models for Mandarin Chinese, focusing on a range of intrinsic tasks. Our results reveal that static models remain stronger for some of the classical tasks that consider word meaning independent of context, while contextualized models excel in identifying semantic relations between word pairs and in the categorization of words into abstract semantic classes",
    "checked": true,
    "id": "000561f5090fbd72ae704d992cb8e9605eeb4a43",
    "semantic_title": "comparing static and contextual distributional semantic models on intrinsic tasks: an evaluation on mandarin chinese datasets",
    "citation_count": 0,
    "authors": [
      "A Pranav",
      "Yan Cong",
      "Emmanuele Chersoni",
      "Yu-Yin Hsu",
      "Alessandro Lenci"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.321": {
    "title": "Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition",
    "volume": "main",
    "abstract": "Thanks to the rise of deep learning and the availability of large-scale audio-visual databases, recent advances have been achieved in Visual Speech Recognition (VSR). Similar to other speech processing tasks, these end-to-end VSR systems are usually based on encoder-decoder architectures. While encoders are somewhat general, multiple decoding approaches have been explored, such as the conventional hybrid model based on Deep Neural Networks combined with Hidden Markov Models (DNN-HMM) or the Connectionist Temporal Classification (CTC) paradigm. However, there are languages and tasks in which data is scarce, and in this situation, there is not a clear comparison between different types of decoders. Therefore, we focused our study on how the conventional DNN-HMM decoder and its state-of-the-art CTC/Attention counterpart behave depending on the amount of data used for their estimation. We also analyzed to what extent our visual speech features were able to adapt to scenarios for which they were not explicitly trained, either considering a similar dataset or another collected for a different language. Results showed that the conventional paradigm reached recognition rates that improve the CTC/Attention model in data-scarcity scenarios along with a reduced training time and fewer parameters",
    "checked": true,
    "id": "a23e471d33e626a177e4bfbb2b342453bd3767e6",
    "semantic_title": "comparison of conventional hybrid and ctc/attention decoders for continuous visual speech recognition",
    "citation_count": 0,
    "authors": [
      "David Gimeno-Gómez",
      "Carlos-D. Martínez-Hinarejos"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.322": {
    "title": "Comparison of the Intimacy Process between Real and Acting-based Long-term Text Chats",
    "volume": "main",
    "abstract": "Long-term chatbots are expected to develop relationships with users. The major trend in this field's recent long-term chatbot studies is to train systems with virtual long-term chat data called Multi-Session Chat (MSC), which collects text chat from multiple sessions of crowd workers playing the roles of speakers with defined personas. However, no investigation has attempted to determine whether such virtual long-term chat can successfully simulate relationship-building between speakers. To clarify the difference between an actual long-term intimacy process and an MSC intimacy process, this study collects real long-term chat and MSC in Japanese and compares them in terms of speech form and dialogue acts. The results of analyzing these factors suggest that MSC have an unnatural tendency to behave as if they have a close relationship with non-polite speech levels compared to actual long-term chats, but also as if they have a shallow relationship with more questions than real long-term chats",
    "checked": true,
    "id": "f5dd182f7215aa67269b30576d5dd9111b6205f0",
    "semantic_title": "comparison of the intimacy process between real and acting-based long-term text chats",
    "citation_count": 0,
    "authors": [
      "Tsunehiro Arimoto",
      "Hiroaki Sugiyama",
      "Hiromi Narimatsu",
      "Masahiro Mizukami"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.323": {
    "title": "Complex Word Identification: A Comparative Study between ChatGPT and a Dedicated Model for This Task",
    "volume": "main",
    "abstract": "There are several works in natural language processing for identifying lexical complexity. This can be for various reasons, either for simplification, the selection of more suitable content, or for other specific tasks. Words can have multiple definitions and degrees of complexity depending on the context in which they appear. One solution being investigated is lexical complexity prediction, where computational methods are used to evaluate the difficulty of vocabulary for language learners and offer personalized assistance. In this work, we explore deep learning methods to assess the complexity of a word based on its context. Specifically, we investigate how to use pre-trained language models to encode both the sentence and the target word, and then fine-tune them by combining them with additional frequency-based features. Our approach achieved superior results compared to the best systems in SemEval-2021 (Shardlow et al., 2021), as demonstrated by an R2 score of 0.65. Finally, we carry out a comparative study with ChatGPT to assess its potential for predicting lexical complexity, to see whether prompt engineering can be an alternative to this task, we will discuss the advantages and limitations of ChatGPT",
    "checked": true,
    "id": "17beb282e7ce1e03ed48244ee8f3f3e629838382",
    "semantic_title": "complex word identification: a comparative study between chatgpt and a dedicated model for this task",
    "citation_count": 0,
    "authors": [
      "Abdelhak Kelious",
      "Mathieu Constant",
      "Christophe Coeur"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.324": {
    "title": "Comprehensive Study on German Language Models for Clinical and Biomedical Text Understanding",
    "volume": "main",
    "abstract": "Recent advances in natural language processing (NLP) can be largely attributed to the advent of pre-trained language models such as BERT and RoBERTa. While these models demonstrate remarkable performance on general datasets, they can struggle in specialized domains such as medicine, where unique domain-specific terminologies, domain-specific abbreviations, and varying document structures are common. This paper explores strategies for adapting these models to domain-specific requirements, primarily through continuous pre-training on domain-specific data. We pre-trained several German medical language models on 2.4B tokens derived from translated public English medical data and 3B tokens of German clinical data. The resulting models were evaluated on various German downstream tasks, including named entity recognition (NER), multi-label classification, and extractive question answering. Our results suggest that models augmented by clinical and translation-based pre-training typically outperform general domain models in medical contexts. We conclude that continuous pre-training has demonstrated the ability to match or even exceed the performance of clinical models trained from scratch. Furthermore, pre-training on clinical data or leveraging translated texts have proven to be reliable methods for domain adaptation in medical NLP tasks",
    "checked": true,
    "id": "164eb11a0b03b4b0d5b841d63b759f9dda8af253",
    "semantic_title": "comprehensive study on german language models for clinical and biomedical text understanding",
    "citation_count": 0,
    "authors": [
      "Ahmad Idrissi-Yaghir",
      "Amin Dada",
      "Henning Schäfer",
      "Kamyar Arzideh",
      "Giulia Baldini",
      "Jan Trienes",
      "Max Hasin",
      "Jeanette Bewersdorff",
      "Cynthia S. Schmidt",
      "Marie Bauer",
      "Kaleb E. Smith",
      "Jiang Bian",
      "Yonghui Wu",
      "Jörg Schlötterer",
      "Torsten Zesch",
      "Peter A. Horn",
      "Christin Seifert",
      "Felix Nensa",
      "Jens Kleesiek",
      "Christoph M. Friedrich"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.325": {
    "title": "Computational Modelling of Plurality and Definiteness in Chinese Noun Phrases",
    "volume": "main",
    "abstract": "Theoretical linguists have suggested that some languages (e.g., Chinese and Japanese) are \"cooler\" than other languages based on the observation that the intended meaning of phrases in these languages depends more on their contexts. As a result, many expressions in these languages are shortened, and their meaning is inferred from the context. In this paper, we focus on the omission of the plurality and definiteness markers in Chinese noun phrases (NPs) to investigate the predictability of their intended meaning given the contexts. To this end, we built a corpus of Chinese NPs, each of which is accompanied by its corresponding context, and by labels indicating its singularity/plurality and definiteness/indefiniteness. We carried out corpus assessments and analyses. The results suggest that Chinese speakers indeed drop plurality and definiteness markers very frequently. Building on the corpus, we train a bank of computational models using both classic machine learning models and state-of-the-art pre-trained language models to predict the plurality and definiteness of each NP. We report on the performance of these models and analyse their behaviours",
    "checked": true,
    "id": "46992919ba40b5fcf1cbab0da78ec6bede4b27f3",
    "semantic_title": "computational modelling of plurality and definiteness in chinese noun phrases",
    "citation_count": 0,
    "authors": [
      "Yuqi Liu",
      "Guanyi Chen",
      "Kees van Deemter"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.326": {
    "title": "CONAN-MT-SP: A Spanish Corpus for Counternarrative Using GPT Models",
    "volume": "main",
    "abstract": "This paper describes the automated generation of CounterNarratives (CNs) for Hate Speech (HS) in Spanish using GPT-based models. Our primary objective is to evaluate the performance of these models in comparison to human capabilities. For this purpose, the English CONAN Multitarget corpus is taken as a starting point and we use the DeepL API to automatically translate into Spanish. Two GPT-based models, GPT-3 and GPT-4, are applied to the HS segment through a few-shot prompting strategy to generate a new CN. As a consequence of our research, we have created a high quality corpus in Spanish that includes the original HS-CN pairs translated into Spanish, in addition to the CNs generated automatically with the GPT models and that have been evaluated manually. The resulting CONAN-MT-SP corpus and its evaluation will be made available to the research community, representing the most extensive linguistic resource of CNs in Spanish to date. The results demonstrate that, although the effectiveness of GPT-4 outperforms GPT-3, both models can be used as systems to automatically generate CNs to combat the HS. Moreover, these models consistently outperform human performance in most instances",
    "checked": true,
    "id": "3087cd3c90c1a70eddc499e5760c98e788edc6c8",
    "semantic_title": "conan-mt-sp: a spanish corpus for counternarrative using gpt models",
    "citation_count": 0,
    "authors": [
      "María Estrella Vallecillo Rodríguez",
      "Maria Victoria Cantero Romero",
      "Isabel Cabrera De Castro",
      "Arturo Montejo Ráez",
      "María Teresa Martín Valdivia"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.327": {
    "title": "Conceptual Pacts for Reference Resolution Using Small, Dynamically Constructed Language Models: A Study in Puzzle Building Dialogues",
    "volume": "main",
    "abstract": "Using Brennan and Clark's theory of a Conceptual Pact, that when interlocutors agree on a name for an object, they are forming a temporary agreement on how to conceptualize that object, we present an extension to a simple reference resolver which simulates this process over time with different conversation pairs. In a puzzle construction domain, we model pacts with small language models for each referent which update during the interaction. When features from these pact models are incorporated into a simple bag-of-words reference resolver, the accuracy increases compared to using a standard pre-trained model. The model performs equally to a competitor using the same data but with exhaustive re-training after each prediction, while also being more transparent, faster and less resource-intensive. We also experiment with reducing the number of training interactions, and can still achieve reference resolution accuracies of over 80% in testing from observing a single previous interaction, over 20% higher than a pre-trained baseline. While this is a limited domain, we argue the model could be applicable to larger real-world applications in human and human-robot interaction and is an interpretable and transparent model",
    "checked": true,
    "id": "0133e300aa9a07589ef24d11585232568fd75b6d",
    "semantic_title": "conceptual pacts for reference resolution using small, dynamically constructed language models: a study in puzzle building dialogues",
    "citation_count": 0,
    "authors": [
      "Julian Hough",
      "Sina Zarrieß",
      "Casey Kennington",
      "David Schlangen",
      "Massimo Poesio"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.328": {
    "title": "ConEC: Earnings Call Dataset with Real-world Contexts for Benchmarking Contextual Speech Recognition",
    "volume": "main",
    "abstract": "Knowing the particular context associated with a conversation can help improving the performance of an automatic speech recognition (ASR) system. For example, if we are provided with a list of in-context words or phrases — such as the speaker's contacts or recent song playlists — during inference, we can bias the recognition process towards this list. There are many works addressing contextual ASR; however, there is few publicly available real benchmark for evaluation, making it difficult to compare different solutions. To this end, we provide a corpus (\"ConEC\") and baselines to evaluate contextual ASR approaches, grounded on real-world applications. The ConEC corpus is based on public-domain earnings calls (ECs) and associated supplementary materials, such as presentation slides, earnings news release as well as a list of meeting participants' names and affiliations. We demonstrate that such real contexts are noisier than artificially synthesized contexts that contain the ground truth, yet they still make great room for future improvement of contextual ASR technology",
    "checked": true,
    "id": "ba913e993593a1a79f1e6769c849e6dc72ebf688",
    "semantic_title": "conec: earnings call dataset with real-world contexts for benchmarking contextual speech recognition",
    "citation_count": 0,
    "authors": [
      "Ruizhe Huang",
      "Mahsa Yarmohammadi",
      "Jan Trmal",
      "Jing Liu",
      "Desh Raj",
      "Leibny Paola Garcia",
      "Alexei V. Ivanov",
      "Patrick Ehlen",
      "Mingzhi Yu",
      "Dan Povey",
      "Sanjeev Khudanpur"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.329": {
    "title": "Conjoin after Decompose: Improving Few-Shot Performance of Named Entity Recognition",
    "volume": "main",
    "abstract": "Prompt-based methods have been widely used in few-shot named entity recognition (NER). In this paper, we first conduct a preliminary experiment and observe that the key to affecting the performance of prompt-based NER models is the capability to detect entity boundaries. However, most existing models fail to boost such capability. To solve the issue, we propose a novel model, ParaBART, which consists of a BART encoder and a specially designed parabiotic decoder. Specifically, the parabiotic decoder includes two BART decoders and a conjoint module. The two decoders are responsible for entity boundary detection and entity type classification, respectively. They are connected by the conjoint module, which is used to replace unimportant tokens' embeddings in one decoder with the average embedding of all the tokens in the other. We further present a novel boundary expansion strategy to enhance the model's capability in entity type classification. Experimental results show that ParaBART can achieve significant performance gains over state-of-the-art competitors",
    "checked": true,
    "id": "1c3dd4e308c59f0de6c5dde1997246a280f91a9e",
    "semantic_title": "conjoin after decompose: improving few-shot performance of named entity recognition",
    "citation_count": 0,
    "authors": [
      "Chengcheng Han",
      "Renyu Zhu",
      "Jun Kuang",
      "Fengjiao Chen",
      "Xiang Li",
      "Ming Gao",
      "Xuezhi Cao",
      "Yunsen Xian"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.330": {
    "title": "CoNLL#: Fine-grained Error Analysis and a Corrected Test Set for CoNLL-03 English",
    "volume": "main",
    "abstract": "Modern named entity recognition systems have steadily improved performance in the age of larger and more powerful neural models. However, over the past several years, the state-of-the-art has seemingly hit another plateau on the benchmark CoNLL-03 English dataset. In this paper, we perform a deep dive into the test outputs of the highest-performing NER models, conducting a fine-grained evaluation of their performance by introducing new document-level annotations on the test set. We go beyond F1 scores by categorizing errors in order to interpret the true state of the art for NER and guide future work. We review previous attempts at correcting the various flaws of the test set and introduce CoNLL#, a new corrected version of the test set that addresses its systematic and most prevalent errors, allowing for low-noise, interpretable error analysis",
    "checked": true,
    "id": "bddfa5ea149414d53b06abcd6cbbd4454db34bdc",
    "semantic_title": "conll#: fine-grained error analysis and a corrected test set for conll-03 english",
    "citation_count": 0,
    "authors": [
      "Andrew Rueda",
      "Elena Alvarez-Mellado",
      "Constantine Lignos"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.331": {
    "title": "Connecting Language Technologies with Rich, Diverse Data Sources Covering Thousands of Languages",
    "volume": "main",
    "abstract": "Contrary to common belief, there are rich and diverse data sources available for many thousands of languages, which can be used to develop technologies for these languages. In this paper, we provide an overview of some of the major online data sources, the types of data that they provide access to, potential applications of this data, and the number of languages that they cover. Even this covers only a small fraction of the data that exists; for example, printed books are published in many languages but few online aggregators exist",
    "checked": true,
    "id": "bdb275b41ab87b7f9b86dc09078ccfc0b4c19c3a",
    "semantic_title": "connecting language technologies with rich, diverse data sources covering thousands of languages",
    "citation_count": 1,
    "authors": [
      "Daan van Esch",
      "Sandy Ritchie",
      "Sebastian Ruder",
      "Julia Kreutzer",
      "Clara Rivera",
      "Ishank Saxena",
      "Isaac Caswell"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.332": {
    "title": "Constructing a Dependency Treebank for Second Language Learners of Korean",
    "volume": "main",
    "abstract": "We introduce a manually annotated syntactic treebank based on Universal Dependencies, derived from the written data of second language (L2) Korean learners. In developing this new dataset, we critically evaluated previous works and revised the annotation guidelines to better reflect the linguistic properties of Korean and the characteristics of L2 learners. The L2 Korean treebank encompasses 7,530 sentences (66,982 words; 129,333 morphemes) and is publicly available at: https://github.com/NLPxL2Korean/L2KW-corpus",
    "checked": true,
    "id": "72c9c25764a1599b3a5b621b591c715fffbfe525",
    "semantic_title": "constructing a dependency treebank for second language learners of korean",
    "citation_count": 0,
    "authors": [
      "Hakyung Sung",
      "Gyu-Ho Shin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.333": {
    "title": "Constructing Indonesian-English Travelogue Dataset",
    "volume": "main",
    "abstract": "Research in low-resource language is often hampered due to the under-representation of how the language is being used in reality. This is particularly true for Indonesian language because there is a limited variety of textual datasets, and majority were acquired from official sources with formal writing style. All the more for the task of geoparsing, which could be implemented for navigation and travel planning applications, such datasets are rare, even in the high-resource languages, such as English. Being aware of the need for a new resource in both languages for this specific task, we constructed a new dataset comprising both Indonesian and English from personal travelogue articles. Our dataset consists of 88 articles, exactly half of them written in each language. We covered both named and nominal expressions of four entity types related to travel: location, facility, transportation, and line. We also conducted experiments by training classifiers to recognise named entities and their nominal expressions. The results of our experiments showed a promising future use of our dataset as we obtained F1-score above 0.9 for both languages",
    "checked": true,
    "id": "ebeba6e5ff877699ef3db9db5513ed88b9025164",
    "semantic_title": "constructing indonesian-english travelogue dataset",
    "citation_count": 0,
    "authors": [
      "Eunike Andriani Kardinata",
      "Hiroki Ouchi",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.334": {
    "title": "Constructing Korean Learners' L2 Speech Corpus of Seven Languages for Automatic Pronunciation Assessment",
    "volume": "main",
    "abstract": "Multilingual L2 speech corpora for developing automatic speech assessment are currently available, but they lack comprehensive annotations of L2 speech from non-native speakers of various languages. This study introduces the methodology of designing a Korean learners' L2 speech corpus of seven languages: English, Japanese, Chinese, French, German, Spanish, and Russian. We describe the development of reading scripts, reading tasks, scoring criteria, and expert evaluation methods in detail. Our corpus contains 1,200 hours of L2 speech data from Korean learners (400 hours for English, 200 hours each for Japanese and Chinese, 100 hours each for French, German, Spanish, and Russian). The corpus is annotated with spelling and pronunciation transcription, expert pronunciation assessment scores (accuracy of pronunciation and fluency of prosody), and metadata such as gender, age, self-reported language proficiency, and pronunciation error types. We also propose a practical verification method and a reliability threshold to ensure the reliability and objectivity of large-scale subjective evaluation data",
    "checked": true,
    "id": "3dba8f88c0492098f739c5ffc88e6a0ad47c0c82",
    "semantic_title": "constructing korean learners' l2 speech corpus of seven languages for automatic pronunciation assessment",
    "citation_count": 0,
    "authors": [
      "Seunghee Han",
      "Sunhee Kim",
      "Minhwa Chung"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.335": {
    "title": "Construction of Paired Knowledge Graph - Text Datasets Informed by Cyclic Evaluation",
    "volume": "main",
    "abstract": "Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used to train forward and reverse neural models that generate text from KG and vice versa. However models trained on datasets where KG and text pairs are not equivalent can suffer from more hallucination and poorer recall. In this paper, we verify this empirically by generating datasets with different levels of noise and find that noisier datasets do indeed lead to more hallucination. We argue that the ability of forward and reverse models trained on a dataset to cyclically regenerate source KG or text is a proxy for the equivalence between the KG and the text in the dataset. Using cyclic evaluation we find that manually created WebNLG is much better than automatically created TeKGen and T-REx. Informed by these observations, we construct a new, improved dataset called LAGRANGE using heuristics meant to improve equivalence between KG and text and show the impact of each of the heuristics on cyclic evaluation. We also construct two synthetic datasets using large language models (LLMs), and observe that these are conducive to models that perform significantly well on cyclic generation of text, but less so on cyclic generation of KGs, probably because of a lack of a consistent underlying ontology",
    "checked": true,
    "id": "bb86a2592e9efa196aefd6bbc39bf62a3202e9db",
    "semantic_title": "construction of paired knowledge graph - text datasets informed by cyclic evaluation",
    "citation_count": 0,
    "authors": [
      "Ali Mousavi",
      "Xin Zhan",
      "He Bai",
      "Peng Shi",
      "Theodoros Rekatsinas",
      "Benjamin Han",
      "Yunyao Li",
      "Jeffrey Pound",
      "Joshua M. Susskind",
      "Natalie Schluter",
      "Ihab F. Ilyas",
      "Navdeep Jaitly"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.336": {
    "title": "Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons",
    "volume": "main",
    "abstract": "In this paper, we make a contribution that can be understood from two perspectives: from an NLP perspective, we introduce a small challenge dataset for NLI with large lexical overlap, which minimises the possibility of models discerning entailment solely based on token distinctions, and show that GPT-4 and Llama 2 fail it with strong bias. We then create further challenging sub-tasks in an effort to explain this failure. From a Computational Linguistics perspective, we identify a group of constructions with three classes of adjectives which cannot be distinguished by surface features. This enables us to probe for LLM's understanding of these constructions in various ways, and we find that they fail in a variety of ways to distinguish between them, suggesting that they don't adequately represent their meaning or capture the lexical properties of phrasal heads",
    "checked": true,
    "id": "d6ac1efa464141a3f51b5a32433a554c17135fc1",
    "semantic_title": "constructions are so difficult that even large language models get them right for the wrong reasons",
    "citation_count": 0,
    "authors": [
      "Shijia Zhou",
      "Leonie Weissweiler",
      "Taiqi He",
      "Hinrich Schütze",
      "David R. Mortensen",
      "Lori Levin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.337": {
    "title": "Context-Aware Non-Autoregressive Document-Level Translation with Sentence-Aligned Connectionist Temporal Classification",
    "volume": "main",
    "abstract": "Previous studies employ the autoregressive translation (AT) paradigm in the document-to-document neural machine translation. These methods extend the translation unit from a single sentence to a pseudo-document and encodes the full pseudo-document, avoiding the redundant computation problem in context. However, the AT methods cannot parallelize decoding and struggle with error accumulation, especially when the length of sentences increases. In this work, we propose a context-aware non-autoregressive framework with the sentence-aligned connectionist temporal classification (SA-CTC) loss for document-level neural machine translation. In particular, the SA-CTC loss reduces the search space of the decoding path by fixing the positions of the beginning and end tokens for each sentence in the document. Meanwhile, the context-aware architecture introduces preset nodes to represent sentence-level information and utilizes a hierarchical attention structure to regulate the attention hypothesis space. Experimental results show that our proposed method can achieve competitive performance compared with several strong baselines. Our method implements non-autoregressive modeling in Doc-to-Doc translation manner, achieving an average 46X decoding speedup compared to the document-level AT baselines on three benchmarks",
    "checked": true,
    "id": "6210c6bbec1431b45b196f3bad46a534f29dd378",
    "semantic_title": "context-aware non-autoregressive document-level translation with sentence-aligned connectionist temporal classification",
    "citation_count": 0,
    "authors": [
      "Hao Yu",
      "Kaiyu Huang",
      "Anqi Zhao",
      "Junpeng Liu",
      "Degen Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.338": {
    "title": "Context Matters: Enhancing Metaphor Recognition in Proverbs",
    "volume": "main",
    "abstract": "Despite the remarkable achievements of Large Language Models (LLMs) in various Natural Language Processing tasks, their competence in abstract language understanding remains a relatively under-explored territory. Figurative language interpretation serves as ideal testbed for assessing this as it requires models to navigate beyond the literal meaning and delve into underlying semantics of the figurative expressions. In this paper, we seek to examine the performance of GPT-3.5 in zero-shot setting through word-level metaphor detection. Specifically, we frame the task as annotation of word-level metaphors in proverbs. To this end, we employ a dataset of English proverbs and evaluated its performance by applying different prompting strategies. Our results show that the model shows a satisfactory performance at identifying word-level metaphors, particularly when it is prompted with a hypothetical context preceding the proverb. This observation underscores the pivotal role of well-designed prompts for zero-shot settings through which these models can be leveraged as annotators for subjective NLP tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gamze Goren",
      "Carlo Strapparava"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.339": {
    "title": "Context Shapes Emergent Communication about Concepts at Different Levels of Abstraction",
    "volume": "main",
    "abstract": "We study the communication of concepts at different levels of abstraction and in different contexts in an agent-based, interactive reference game. While playing the concept-level reference game, the neural network agents develop a communication system from scratch. We use a novel symbolic dataset that disentangles concept type (ranging from specific to generic) and context (ranging from fine to coarse) to study the influence of these factors on the emerging language. We compare two game scenarios: one in which speaker agents have access to context information (context-aware) and one in which the speaker agents do not have access to context information (context-unaware). First, we find that the agents learn higher-level concepts from the object inputs alone. Second, an analysis of the emergent communication system shows that only context-aware agents learn to communicate efficiently by adapting their messages to the context conditions and relying on context for unambiguous reference. Crucially, this behavior is not explicitly incentivized by the game, but efficient communication emerges and is driven by the availability of context alone. The emerging language we observe is reminiscent of evolutionary pressures on human languages and highlights the pivotal role of context in a communication system",
    "checked": true,
    "id": "a292a7eb437733fbd9c007af90ea09ddb629e146",
    "semantic_title": "context shapes emergent communication about concepts at different levels of abstraction",
    "citation_count": 0,
    "authors": [
      "Kristina Kobrock",
      "Xenia Isabel Ohmer",
      "Elia Bruni",
      "Nicole Gotzner"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.340": {
    "title": "Contextualizing Generated Citation Texts",
    "volume": "main",
    "abstract": "Abstractive citation text generation is usually framed as an infilling task, where a sequence-to-sequence model is trained to generate a citation given a reference paper and the context window around the target; the generated citation should be a brief discussion of the reference paper as it relates to the citing context. However, examining a recent LED-based citation generation system, we find that many of the generated citations are generic summaries of the reference paper's main contribution, ignoring the citation context's focus on a different topic. To address this problem, we propose a simple modification to the citation text generation task: the generation target is not only the citation itself, but the entire context window, including the target citation. This approach can be easily applied to any abstractive citation generation system, and our experimental results show that training in this way is preferred by human readers and allows the generation model to make use of contextual clues about what topic to discuss and what stance to take",
    "checked": true,
    "id": "2df47b9ee532a15ce5fbbf928a6300e79dc33124",
    "semantic_title": "contextualizing generated citation texts",
    "citation_count": 1,
    "authors": [
      "Biswadip Mandal",
      "Xiangci Li",
      "Jessica Ouyang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.341": {
    "title": "Contextual Modeling for Document-level ASR Error Correction",
    "volume": "main",
    "abstract": "Contextual information, including the sentences in the same document and in other documents of the dataset, plays a crucial role in improving the accuracy of document-level ASR Error Correction (AEC), while most previous works ignore this. In this paper, we propose a context-aware method that utilizes a k-Nearest Neighbors (kNN) approach to enhance the AEC model by retrieving a datastore containing contextual information. We conduct experiments on two English and two Chinese datasets, and the results demonstrate that our proposed model can effectively utilize contextual information to improve document-level AEC. Furthermore, the context information from the whole dataset provides even better results",
    "checked": true,
    "id": "592d20e0273ad79e3c3fcef880df50838be18259",
    "semantic_title": "contextual modeling for document-level asr error correction",
    "citation_count": 0,
    "authors": [
      "Jin Jiang",
      "Xunjian Yin",
      "Xiaojun Wan",
      "Wei Peng",
      "Rongjun Li",
      "Jingyuan Yang",
      "Yanquan Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.342": {
    "title": "Continual Few-shot Event Detection via Hierarchical Augmentation Networks",
    "volume": "main",
    "abstract": "Traditional continual event detection relies on abundant labeled data for training, which is often impractical to obtain in real-world applications. In this paper, we introduce continual few-shot event detection (CFED), a more commonly encountered scenario when a substantial number of labeled samples are not accessible. The CFED task is challenging as it involves memorizing previous event types and learning new event types with few-shot samples. To mitigate these challenges, we propose a memory-based framework: Hierarchical Augmentation Network (HANet). To memorize previous event types with limited memory, we incorporate prototypical augmentation into the memory set. For the issue of learning new event types in few-shot scenarios, we propose a contrastive augmentation module for token representations. Despite comparing with previous state-of-the-art methods, we also conduct comparisons with ChatGPT. Experiment results demonstrate that our method significantly outperforms all of these methods in multiple continual few-shot event detection tasks",
    "checked": true,
    "id": "6f2812dd797dc669b9d4cafb4f12a9e0785900af",
    "semantic_title": "continual few-shot event detection via hierarchical augmentation networks",
    "citation_count": 0,
    "authors": [
      "Chenlong Zhang",
      "Pengfei Cao",
      "Yubo Chen",
      "Kang Liu",
      "Zhiqiang Zhang",
      "Mengshu Sun",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.343": {
    "title": "Continual Reinforcement Learning for Controlled Text Generation",
    "volume": "main",
    "abstract": "Controlled Text Generation (CTG) steers the generation of continuations of a given context (prompt) by a Large Language Model (LLM) towards texts possessing a given attribute (e.g., topic, sentiment). In this paper we view CTG as a Continual Learning problem: how to learn at every step to steer next-word generation, without having to wait for end-of-sentence. This continual view is useful for online applications such as CTG for speech, where end-of-sentence is often uncertain. We depart from an existing model, the Plug-and-Play language models (PPLM), which perturbs the context at each step to better predict next-words that posses the desired attribute. While PPLM is intricate and has many hyper-parameters, we provide a proof that the PPLM objective function can be reduced to a Continual Reinforcement Learning (CRL) reward function, thereby simplifying PPLM and endowing it with a better understood learning framework. Subsequently, we present, the first of its kind, CTG algorithm that is fully based on CRL and exhibit promising empirical results",
    "checked": true,
    "id": "3ed8f36ecd740e6a177189e4f41d3c8dd23f7cd2",
    "semantic_title": "continual reinforcement learning for controlled text generation",
    "citation_count": 0,
    "authors": [
      "Velizar Shulev",
      "Khalil Sima’an"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.344": {
    "title": "Continued Pre-training on Sentence Analogies for Translation with Small Data",
    "volume": "main",
    "abstract": "This paper introduces Continued Pre-training on Analogies (CPoA) to incorporate pre-trained language models with analogical abilities, aiming at improving performance in low-resource translations without data augmentation. We continue training the models on sentence analogies retrieved from a translation corpus. Considering the sparsity of analogy in corpora, especially in low-resource scenarios, we propose exploring approximate analogies between sentences. We attempt to find sentence analogies that might not conform to formal criteria for entire sentences but partial pieces. When training the models, we introduce a weighting scalar pertaining to the quality of analogies to adjust the influence: emphasizing closer analogies while diminishing the impact of far ones. We evaluate our approach on a low-resource translation task: German-Upper Sorbian. The results show that CPoA using 10 times fewer instances can effectively attain gains of +1.4 and +1.3 BLEU points over the original model in two translation directions. This improvement is more pronounced when there are fewer parallel examples",
    "checked": true,
    "id": "74e46d363908f56612c309cf29af2d5fc2bc4bcc",
    "semantic_title": "continued pre-training on sentence analogies for translation with small data",
    "citation_count": 0,
    "authors": [
      "Liyan Wang",
      "Haotong Wang",
      "Yves Lepage"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.345": {
    "title": "Continuous Relational Diffusion Driven Topic Model with Multi-grained Text for Microblog",
    "volume": "main",
    "abstract": "Topic model is a statistical model that leverages unsupervised learning to mine hidden topics in document collections. The data sparsity and colloquialism of social texts make it difficult to accurately mine the topics. Traditional methods assume that there are only 0/1-state relationships between the two parties in the social networks, but the relationship status in real life is more complicated, such as continuously changing relationships with different degrees of intimacy. This paper proposes a continuous relational diffusion driven topic model (CRTM) with multi-grained text for microblog to realize the continuous representation of the relationship state and make up for the context and structural information lost by previous representation methods. Multi-grained text representation learning distinguishes the impact of formal and informal expression on the topics further and alleviates colloquialism problems. Specifically, based on the original social network, the reconstructed social network with continuous relationship status is obtained by using information diffusion technology. The graph convolution model is utilized to learn node embeddings through the new social network. Finally, the neural variational inference is applied to generate topics according to continuous relationships. We validate CRTM on three real datasets, and the experimental results show the effectiveness of the scheme",
    "checked": true,
    "id": "352b46cf5599d045c8eaa1ef7791d41123b18217",
    "semantic_title": "continuous relational diffusion driven topic model with multi-grained text for microblog",
    "citation_count": 0,
    "authors": [
      "Chenhao Wu",
      "Ruifang He",
      "Chang Liu",
      "Bo Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.346": {
    "title": "ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation Following the Metaphor Identification Procedure",
    "volume": "main",
    "abstract": "This paper presents ContrastWSD, a RoBERTa-based metaphor detection model that integrates the Metaphor Identification Procedure (MIP) and Word Sense Disambiguation (WSD) to extract and contrast the contextual meaning with the basic meaning of a word to determine whether it is used metaphorically in a sentence. By utilizing the word senses derived from a WSD model, our model enhances the metaphor detection process and outperforms other methods that rely solely on contextual embeddings or integrate only the basic definitions and other external knowledge. We evaluate our approach on various benchmark datasets and compare it with strong baselines, indicating the effectiveness in advancing metaphor detection",
    "checked": true,
    "id": "fe3e47a7b9bdddeaccab6c89410b2cebaa949a7b",
    "semantic_title": "contrastwsd: enhancing metaphor detection with word sense disambiguation following the metaphor identification procedure",
    "citation_count": 0,
    "authors": [
      "Mohamad MZ Elzohbi",
      "Richard Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.347": {
    "title": "Contribution of Move Structure to Automatic Genre Identification: An Annotated Corpus of French Tourism Websites",
    "volume": "main",
    "abstract": "The present work studies the contribution of move structure to automatic genre identification. This concept - well known in other branches of genre analysis - seems to have little application in natural language processing. We describe how we collect a corpus of websites in French related to tourism and annotate it with move structure. We conduct experiments on automatic genre identification with our corpus. Our results show that our approach for informing a model with move structure can increase its performance for automatic genre identification, and reduce the need for annotated data and computational power",
    "checked": true,
    "id": "170126a4bb0739b052e0c1d04c59714b1baafc5e",
    "semantic_title": "contribution of move structure to automatic genre identification: an annotated corpus of french tourism websites",
    "citation_count": 0,
    "authors": [
      "Rémi Cardon",
      "Trang Tran Hanh Pham",
      "Julien Zakhia Doueihi",
      "Thomas François"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.348": {
    "title": "Controllable Paraphrase Generation for Semantic and Lexical Similarities",
    "volume": "main",
    "abstract": "We developed a controllable paraphrase generation model for semantic and lexical similarities using a simple and intuitive mechanism: attaching tags to specify these values at the head of the input sentence. Lexically diverse paraphrases have been long coveted for data augmentation. However, their generation is not straightforward because diversifying surfaces easily degrades semantic similarity. Furthermore, our experiments revealed two critical features in data augmentation by paraphrasing: appropriate similarities of paraphrases are highly downstream task-dependent, and mixing paraphrases of various similarities negatively affects the downstream tasks. These features indicated that the controllability in paraphrase generation is crucial for successful data augmentation. We tackled these challenges by fine-tuning a pre-trained sequence-to-sequence model employing tags that indicate the semantic and lexical similarities of synthetic paraphrases selected carefully based on the similarities. The resultant model could paraphrase an input sentence according to the tags specified. Extensive experiments on data augmentation for contrastive learning and pre-fine-tuning of pretrained masked language models confirmed the effectiveness of the proposed model. We release our paraphrase generation model and a corpus of 87 million diverse paraphrases. (https://github.com/Ogamon958/ConPGS)",
    "checked": true,
    "id": "fbd41aa041166bf837d8c7a3585ed87b972c2c53",
    "semantic_title": "controllable paraphrase generation for semantic and lexical similarities",
    "citation_count": 0,
    "authors": [
      "Yuya Ogasa",
      "Tomoyuki Kajiwara",
      "Yuki Arase"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.349": {
    "title": "Controllable Sentence Simplification in Swedish Using Control Prefixes and Mined Paraphrases",
    "volume": "main",
    "abstract": "Making information accessible to diverse target audiences, including individuals with dyslexia and cognitive disabilities, is crucial. Automatic Text Simplification (ATS) systems aim to facilitate readability and comprehension by reducing linguistic complexity. However, they often lack customizability to specific user needs, and training data for smaller languages can be scarce. This paper addresses ATS in a Swedish context, using methods that provide more control over the simplification. A dataset of Swedish paraphrases is mined from large amounts of text and used to train ATS models utilizing prefix-tuning with control prefixes. We also introduce a novel data-driven method for selecting complexity attributes for controlling the simplification and compare it with previous approaches. Evaluation of the trained models using SARI and BLEU demonstrates significant improvements over the baseline — a fine-tuned Swedish BART model — and compared to previous Swedish ATS results. These findings highlight the effectiveness of employing paraphrase data in conjunction with controllable generation mechanisms for simplification. Additionally, the set of explored attributes yields similar results compared to previously used attributes, indicating their ability to capture important simplification aspects",
    "checked": true,
    "id": "8bac5271b942b94983edb02f60b2e63421602d95",
    "semantic_title": "controllable sentence simplification in swedish using control prefixes and mined paraphrases",
    "citation_count": 0,
    "authors": [
      "Julius Monsen",
      "Arne Jonsson"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.350": {
    "title": "Controlled Generation with Prompt Insertion for Natural Language Explanations in Grammatical Error Correction",
    "volume": "main",
    "abstract": "In Grammatical Error Correction (GEC), it is crucial to ensure the user's comprehension of a reason for correction. Existing studies present tokens, examples, and hints for corrections, but do not directly explain the reasons in natural language. Although methods that use Large Language Models (LLMs) to provide direct explanations in natural language have been proposed for various tasks, no such method exists for GEC. Generating explanations for GEC corrections involves aligning input and output tokens, identifying correction points, and presenting corresponding explanations consistently. However, it is not straightforward to specify a complex format to generate explanations, because explicit control of generation is difficult with prompts. This study introduces a method called controlled generation with Prompt Insertion (PI) so that LLMs can explain the reasons for corrections in natural language. In PI, LLMs first correct the input text, and then we automatically extract the correction points based on the rules. The extracted correction points are sequentially inserted into the LLM's explanation output as prompts, guiding the LLMs to generate explanations for the correction points. We also create an Explainable GEC (XGEC) dataset of correction reasons by annotating NUCLE, CoNLL2013, and CoNLL2014. Although generations from GPT-3.5 and ChatGPT using original prompts miss some correction points, the generation control using PI can explicitly guide to describe explanations for all correction points, contributing to improved performance in generating correction reasons",
    "checked": true,
    "id": "ebac8b472511bb1c53fe7e7b3decdb9493a484ce",
    "semantic_title": "controlled generation with prompt insertion for natural language explanations in grammatical error correction",
    "citation_count": 0,
    "authors": [
      "Masahiro Kaneko",
      "Naoaki Okazaki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.351": {
    "title": "ControversialQA: Exploring Controversy in Question Answering",
    "volume": "main",
    "abstract": "Controversy is widespread online. Previous studies mainly define controversy based on vague assumptions of its relation to sentiment such as hate speech and offensive words. This paper introduces the first question-answering dataset that defines content controversy by user perception, i.e., votes from plenty of users. It contains nearly 10K questions, and each question has a best answer and a most controversial answer. Experimental results reveal that controversy detection in question answering is essential and challenging, and there is no strong correlation between controversy and sentiment tasks. We also show that controversial answers and most acceptable answers cannot be distinguished by retrieval-based QA models, which may cause controversy issues. With these insights, we believe ControversialQA can inspire future research on controversy in QA systems",
    "checked": true,
    "id": "20d09339dfd593603cfa217fedec840ea46d4eb2",
    "semantic_title": "controversialqa: exploring controversy in question answering",
    "citation_count": 1,
    "authors": [
      "Zhen Wang",
      "Peide Zhu",
      "Jie Yang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.352": {
    "title": "Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units",
    "volume": "main",
    "abstract": "Successful conversations often rest on common understanding, where all parties are on the same page about the information being shared. This process, known as conversational grounding, is crucial for building trustworthy dialog systems that can accurately keep track of and recall the shared information. The proficiencies of an agent in grounding the conveyed information significantly contribute to building a reliable dialog system. Despite recent advancements in dialog systems, there exists a noticeable deficit in their grounding capabilities. Traum (Traum, 1995) provided a framework for conversational grounding introducing Grounding Acts and Grounding Units, but substantial progress, especially in the realm of Large Language Models, remains lacking. To bridge this gap, we present the annotation of two dialog corpora employing Grounding Acts, Grounding Units, and a measure of their degree of grounding. We discuss our key findings during the annotation and also provide a baseline model to test the performance of current Language Models in categorizing the grounding acts of the dialogs. Our work aims to provide a useful resource for further research in making conversations with machines better understood and more reliable in natural day-to-day collaborative dialogs",
    "checked": true,
    "id": "7fbe818780914547c729cf0b2c43442679104e19",
    "semantic_title": "conversational grounding: annotation and analysis of grounding acts and grounding units",
    "citation_count": 0,
    "authors": [
      "Biswesh Mohapatra",
      "Seemab Hassan",
      "Laurent Romary",
      "Justine Cassell"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.353": {
    "title": "Converting Legacy Data to CLDF: A FAIR Exit Strategy for Linguistic Web Apps",
    "volume": "main",
    "abstract": "In the mid 2000s, there were several large-scale US National Science Foundation (NSF) grants awarded to projects aiming at developing digital infrastructure and standards for different forms of linguistics data. For example, MultiTree encoded language family trees as phylogenies in XML and LL-MAP converted detailed geographic maps of endangered languages into KML. As early stand-alone website applications, these projects allowed researchers interested in comparative linguistics to explore language genealogies and areality, respectively. However as time passed, the technologies that supported these web apps became deprecated, unsupported, and inaccessible. Here we take a future-oriented approach to digital obsolescence and illustrate how to convert legacy linguistic resources into FAIR data via the Cross-Linguistic Data Formats (CLDF). CLDF is built on the W3C recommendations Model for Tabular Data and Metadata on the Web and Metadata Vocabulary for Tabular Data developed by the CSVW (CSV on the Web) working group. Thus, each dataset is modeled as a set of tabular data files described by metadata in JSON. These standards and the tools built to validate and manipulate them provide an accessible and extensible format for converting legacy linguistic web apps into FAIR datasets",
    "checked": true,
    "id": "ddf219cef2103fc32ccfa0745c15764ce7632393",
    "semantic_title": "converting legacy data to cldf: a fair exit strategy for linguistic web apps",
    "citation_count": 0,
    "authors": [
      "Robert Forkel",
      "Daniel G. Swanson",
      "Steven Moran"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.354": {
    "title": "CookingSense: A Culinary Knowledgebase with Multidisciplinary Assertions",
    "volume": "main",
    "abstract": "This paper introduces CookingSense, a descriptive collection of knowledge assertions in the culinary domain extracted from various sources, including web data, scientific papers, and recipes, from which knowledge covering a broad range of aspects is acquired. CookingSense is constructed through a series of dictionary-based filtering and language model-based semantic filtering techniques, which results in a rich knowledgebase of multidisciplinary food-related assertions. Additionally, we present FoodBench, a novel benchmark to evaluate culinary decision support systems. From evaluations with FoodBench, we empirically prove that CookingSense improves the performance of retrieval augmented language models. We also validate the quality and variety of assertions in CookingSense through qualitative analysis",
    "checked": true,
    "id": "be29557ece1a97281a0937b424ff46d31a85f8bd",
    "semantic_title": "cookingsense: a culinary knowledgebase with multidisciplinary assertions",
    "citation_count": 0,
    "authors": [
      "Donghee Choi",
      "Mogan Gim",
      "Donghyeon Park",
      "Mujeen Sung",
      "Hyunjae Kim",
      "Jaewoo Kang",
      "Jihun Choi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.355": {
    "title": "CoRelation: Boosting Automatic ICD Coding through Contextualized Code Relation Learning",
    "volume": "main",
    "abstract": "Automatic International Classification of Diseases (ICD) coding plays a crucial role in the extraction of relevant information from clinical notes for proper recording and billing. One of the most important directions for boosting the performance of automatic ICD coding is modeling ICD code relations. However, current methods insufficiently model the intricate relationships among ICD codes and often overlook the importance of context in clinical notes. In this paper, we propose a novel approach, a contextualized and flexible framework, to enhance the learning of ICD code representations. Our approach, unlike existing methods, employs a dependent learning paradigm that considers the context of clinical notes in modeling all possible code relations. We evaluate our approach on six public ICD coding datasets and the experimental results demonstrate the effectiveness of our approach compared to state-of-the-art baselines",
    "checked": true,
    "id": "88d1af08e4189a56eb4a7590216020401a9eb4d7",
    "semantic_title": "corelation: boosting automatic icd coding through contextualized code relation learning",
    "citation_count": 2,
    "authors": [
      "Junyu Luo",
      "Xiaochen Wang",
      "Jiaqi Wang",
      "Aofei Chang",
      "Yaqing Wang",
      "Fenglong Ma"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.356": {
    "title": "CORI: CJKV Benchmark with Romanization Integration - a Step towards Cross-lingual Transfer beyond Textual Scripts",
    "volume": "main",
    "abstract": "Naively assuming English as a source language may hinder cross-lingual transfer for many languages by failing to consider the importance of language contact. Some languages are more well-connected than others, and target languages can benefit from transferring from closely related languages; for many languages, the set of closely related languages does not include English. In this work, we study the impact of source language for cross-lingual transfer, demonstrating the importance of selecting source languages that have high contact with the target language. We also construct a novel benchmark dataset for close contact Chinese-Japanese-Korean-Vietnamese (CJKV) languages to further encourage in-depth studies of language contact. To comprehensively capture contact between these languages, we propose to integrate Romanized transcription beyond textual scripts via Contrastive Learning objectives, leading to enhanced cross-lingual representations and effective zero-shot cross-lingual transfer",
    "checked": true,
    "id": "87baa96f57a70b69c6a6cde8a28f40194bb38b23",
    "semantic_title": "cori: cjkv benchmark with romanization integration - a step towards cross-lingual transfer beyond textual scripts",
    "citation_count": 0,
    "authors": [
      "Hoang Nguyen",
      "Chenwei Zhang",
      "Ye Liu",
      "Natalie Parde",
      "Eugene Rohrbaugh",
      "Philip S. Yu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.357": {
    "title": "Corpus Creation and Automatic Alignment of Historical Dutch Dialect Speech",
    "volume": "main",
    "abstract": "The Dutch Dialect Database (also known as the ‘Nederlandse Dialectenbank') contains dialectal variations of Dutch that were recorded all over the Netherlands in the second half of the twentieth century. A subset of these recordings of about 300 hours were enriched with manual orthographic transcriptions, using non-standard approximations of dialectal speech. In this paper we describe the creation of a corpus containing both the audio recordings and their corresponding transcriptions and focus on our method for aligning the recordings with the transcriptions and the metadata",
    "checked": true,
    "id": "cf06300b5c755d44c9cf87b3d45132e2aa4946c5",
    "semantic_title": "corpus creation and automatic alignment of historical dutch dialect speech",
    "citation_count": 0,
    "authors": [
      "Martijn Bentum",
      "Eric Sanders",
      "Antal P.J. van den Bosch",
      "Douwe Zeldenrust",
      "Henk van den Heuvel"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.358": {
    "title": "Corpus Services: A Framework to Curate XML Corpus Data",
    "volume": "main",
    "abstract": "This paper provides a comprehensive description of the Corpus Services framework—a collection of Java validation tools for language corpora compiled in XML-based data formats, in particular those using EXMARaLDA corpus software. Having successfully found application in several research projects, the core functionality of the framework is currently integrated in the automated curation and publication workflows for EXMARaLDA-driven corpora of Northern Eurasian languages, as developed by the long-term project INEL. Preliminary stages of development and examples of practical use cases are covered, a structured explanation of the framework's current functionality and operational mechanisms is provided. Furthermore, the utilization of Corpus Services is extensively illustrated within the context of INEL workflows",
    "checked": true,
    "id": "ba2fdecc6e087320b3635f27702aebdb0cb48704",
    "semantic_title": "corpus services: a framework to curate xml corpus data",
    "citation_count": 0,
    "authors": [
      "Aleksandr Riaposov",
      "Elena Lazarenko"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.359": {
    "title": "Correcting Language Model Bias for Text Classification in True Zero-Shot Learning",
    "volume": "main",
    "abstract": "Combining pre-trained language models (PLMs) and manual templates is a common practice for text classification in zero-shot scenarios. However, the effect of this approach is highly volatile, ranging from random guesses to near state-of-the-art results, depending on the quality of the manual templates. In this paper, we show that this instability stems from the fact that language models tend toward predicting certain label words of text classification, and manual templates can influence this tendency. To address this, we develop a novel pipeline for annotating and filtering a few examples from unlabeled examples. Moreover, we propose a new method to measure model bias on label words that utilizes unlabeled examples as a validation set when tuning language models. Our approach does not require any pre-labeled examples. Experimental results on six text classification tasks demonstrate that the proposed approach significantly outperforms standard prompt learning in zero-shot settings, achieving up to 19.7% absolute improvement and 13.8% average improvement. More surprisingly, on IMDB and SST-2, our approach even exceeds all few-shot baselines",
    "checked": true,
    "id": "d4870bf5636c1637b558a7ee9d054d070cbe47fe",
    "semantic_title": "correcting language model bias for text classification in true zero-shot learning",
    "citation_count": 0,
    "authors": [
      "Feng Zhao",
      "Wan Xianlin",
      "Cheng Yan",
      "Chu Kiong Loo"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.360": {
    "title": "Correcting Pronoun Homophones with Subtle Semantics in Chinese Speech Recognition",
    "volume": "main",
    "abstract": "Speech recognition is becoming prevalent in daily life. However, due to the similar semantic context of the entities and the overlap of Chinese pronunciation, the pronoun homophone, especially \"他/她/它 (he/she/it)\", (their pronunciation is \"Tā\") is usually recognized incorrectly. It poses a challenge to automatically correct them during the post-processing of Chinese speech recognition. In this paper, we propose three models to address the common confusion issues in this domain, tailored to various application scenarios. We implement the language model, the LSTM model with semantic features, and the rule-based assisted Ngram model, enabling our models to adapt to a wide range of requirements, from high-precision to low-resource offline devices. The extensive experiments show that our models achieve the highest recognition rate for \"Tā\" correction with improvements from 70% in the popular voice input methods up to 90%. Further ablation analysis underscores the effectiveness of our models in enhancing recognition accuracy. Therefore, our models improve the overall experience of Chinese speech recognition of \"Tā\" and reduce the burden of manual transcription corrections",
    "checked": true,
    "id": "a0b183fcb7324566ce44649abd81aba70a56b31e",
    "semantic_title": "correcting pronoun homophones with subtle semantics in chinese speech recognition",
    "citation_count": 0,
    "authors": [
      "Zhaobo Zhang",
      "Rui Gan",
      "Pingpeng Yuan",
      "Hai Jin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.361": {
    "title": "Correlations between Multilingual Language Model Geometry and Crosslingual Transfer Performance",
    "volume": "main",
    "abstract": "A common approach to interpreting multilingual language models is to evaluate their internal representations. For example, studies have found that languages occupy distinct subspaces in the models' representation spaces, and geometric distances between languages often reflect linguistic properties such as language families and typological features. In our work, we investigate whether geometric distances between language representations correlate with zero-shot crosslingual transfer performance for POS-tagging and NER in three multilingual language models. We consider four distance metrics, including new metrics that identify a basis for a multilingual representation space that sorts axes based on their language-separability. We find that each distance metric either only moderately correlates or does not correlate with crosslingual transfer performance, and metrics do not generalize well across models, layers, and tasks. Although pairwise language separability is a reasonable predictor of crosslingual transfer, representational geometry overall is an inconsistent predictor for the crosslingual performance of multilingual language models",
    "checked": true,
    "id": "150abb1aaf75989790c68e198c06afa2593a6766",
    "semantic_title": "correlations between multilingual language model geometry and crosslingual transfer performance",
    "citation_count": 0,
    "authors": [
      "Cheril Shah",
      "Yashashree Chandak",
      "Atharv Mahesh Mane",
      "Benjamin Bergen",
      "Tyler A. Chang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.362": {
    "title": "Cost-Effective Discourse Annotation in the Prague Czech–English Dependency Treebank",
    "volume": "main",
    "abstract": "We present a cost-effective method for obtaining a high-quality annotation of explicit discourse relations in the Czech part of the Prague Czech–English Dependency Treebank, a corpus of almost 50 thousand sentences coming from the Czech translation of the Wall Street Journal part of the Penn Treebank. We use three different sources of information and combine them to obtain the discourse annotation: (i) annotation projection from the Penn Discourse Treebank 3.0, (ii) manual tectogrammatical (deep syntax) representation of sentences of the corpus, and (iii) the Lexicon of Czech Discourse Connectives CzeDLex. After solving as many discrepancies as possible automatically, the final discourse annotation is achieved by manual inspection of the remaining problematic cases. The discourse annotation of the corpus will be available both in the Prague format (on top of tectogrammatical trees) with the Prague taxonomy of discourse types, and in the Penn format (on plain texts) with the Penn Discourse Treebank 3.0 sense taxonomy",
    "checked": true,
    "id": "428d8fb1b6535b0942f387e1c36e78483cd46885",
    "semantic_title": "cost-effective discourse annotation in the prague czech–english dependency treebank",
    "citation_count": 0,
    "authors": [
      "Jiří Mírovský",
      "Pavlína Synková",
      "Lucie Polakova",
      "Marie Paclíková"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.363": {
    "title": "Counterfactual Dialog Mixing as Data Augmentation for Task-Oriented Dialog Systems",
    "volume": "main",
    "abstract": "High-quality training data for Task-Oriented Dialog (TOD) systems is costly to come by if no corpora are available. One method to extend available data is data augmentation. Yet, the research into and adaptation of data augmentation techniques for TOD systems is limited in comparison with other data modalities. We propose a novel, causally-flavored data augmentation technique called Counterfactual Dialog Mixing (CDM) that generates realistic synthetic dialogs via counterfactuals to increase the amount of training data. We demonstrate the method on a benchmark dataset and show that a model trained to classify the counterfactuals from the original data fails to do so, which strengthens the claim of creating realistic synthetic dialogs. To evaluate the effectiveness of CDM, we train a current architecture on a benchmark dataset and compare the performance with and without CDM. By doing so, we achieve state-of-the-art on some metrics. We further investigate the external generalizability and a lower resource setting. To evaluate the models, we adopted an interactive evaluation scheme",
    "checked": true,
    "id": "9c314af9b471e5f4be846c76e32f07ad16286c0d",
    "semantic_title": "counterfactual dialog mixing as data augmentation for task-oriented dialog systems",
    "citation_count": 0,
    "authors": [
      "Sebastian Steindl",
      "Ulrich Schäfer",
      "Bernd Ludwig"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.364": {
    "title": "Creating Terminological Resources in the Digital Age for Less-resourced Languages",
    "volume": "main",
    "abstract": "Multilingual terminological resources contain the most representative knowledge of specialized domains and allow professionals to create and translate specialized content in order to spread knowledge. Today, representative and useful multilingual terminological resources are available for the most resourced languages. This reduces or limits the development of knowledge in less-resourced languages across different specialized domains, mainly those that are constantly evolving and creating or adapting new concepts as needed. In this paper we present our methodology for carrying out terminological projects in Catalan, based entirely on open access linguistic resources and using natural language processing tools. The main objective of this research is to maximize the Catalan terminology currently available in open access, using a combination of natural language processing tools. The results are supervised by linguists and terminologist experts before being publicly available to the public. The findings of our research provide a new approach to terminology work, making it possible to design high-volume multilingual terminological projects that are manually revised by linguists and terminologists in the context of less-resourced languages",
    "checked": true,
    "id": "7149262751746d59ac55f4569c7531dd050e7348",
    "semantic_title": "creating terminological resources in the digital age for less-resourced languages",
    "citation_count": 0,
    "authors": [
      "Mercè Vàzquez"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.365": {
    "title": "Creation and Analysis of an International Corpus of Privacy Laws",
    "volume": "main",
    "abstract": "The landscape of privacy laws and regulations around the world is complex and ever-changing. National and super-national laws, agreements, decrees, and other government-issued rules form a patchwork that companies must follow to operate internationally. To examine the status and evolution of this patchwork, we introduce the Privacy Law Corpus, of 1,043 privacy laws, regulations, and guidelines, covering 183 jurisdictions. This corpus enables a large-scale quantitative and qualitative examination of legal focus on privacy. We examine the temporal distribution of when privacy laws were created and illustrate the dramatic increase in privacy legislation over the past 50 years, although a finer-grained examination reveals that the rate of increase varies depending on the personal data types that privacy laws address. Our exploration also demonstrates that most privacy laws respectively address relatively few personal data types. Additionally, topic modeling results show the prevalence of common themes in privacy laws, such as finance, healthcare, and telecommunications. Finally, we release the corpus to the research community to promote further study",
    "checked": true,
    "id": "3dbf9360dabbbdaa4fdc44e52f6077d0b229f5e0",
    "semantic_title": "creation and analysis of an international corpus of privacy laws",
    "citation_count": 5,
    "authors": [
      "Sonu Gupta",
      "Geetika Gopi",
      "Harish Balaji",
      "Ellen Poplavska",
      "Nora O’Toole",
      "Siddhant Arora",
      "Thomas Norton",
      "Norman Sadeh",
      "Shomir Wilson"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.366": {
    "title": "Croatian Idioms Integration: Enhancing the LIdioms Multilingual Linked Idioms Dataset",
    "volume": "main",
    "abstract": "Idioms, also referred to as phraseological units in some language terminologies, are a subset within the broader category of multi-word expressions. However, there is a lack of representation of idioms in Croatian, a low-resourced language, in the Linguistic Linked Open Data cloud (LLOD). To address this gap, we propose an extension of an existing RDF-based multilingual representation of idioms, referred to as the LIdioms dataset, which currently includes idioms from English, German, Italian, Portuguese, and Russian. This paper expands the existing resource by incorporating 1,042 Croatian idioms in an Ontolex Lemon format. In addition, to foster translation initiatives and facilitate intercultural exchange, these added Croatian idioms have also been linked to other idioms of the LIdioms dataset, with which they share similar meanings despite their differences in the expression aspect. This addition enriches the knowledge base of the LLOD community with a new language resource that includes Croatian idioms",
    "checked": true,
    "id": "521226d684c390c76a3136c1e401e61de16d627c",
    "semantic_title": "croatian idioms integration: enhancing the lidioms multilingual linked idioms dataset",
    "citation_count": 0,
    "authors": [
      "Ivana Filipović Petrović",
      "Miguel López Otal",
      "Slobodan Beliga"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.367": {
    "title": "CroCoSum: A Benchmark Dataset for Cross-Lingual Code-Switched Summarization",
    "volume": "main",
    "abstract": "Cross-lingual summarization (CLS) has attracted increasing interest in recent years due to the availability of large-scale web-mined datasets and the advancements of multilingual language models. However, given the rareness of naturally occurring CLS resources, the majority of datasets are forced to rely on translation which can contain overly literal artifacts. This restricts our ability to observe naturally occurring CLS pairs that capture organic diction, including instances of code-switching. This alteration between languages in mid-message is a common phenomenon in multilingual settings yet has been largely overlooked in cross-lingual contexts due to data scarcity. To address this gap, we introduce CroCoSum, a dataset of cross-lingual code-switched summarization of technology news. It consists of over 24,000 English source articles and 18,000 human-written Chinese news summaries, with more than 92% of the summaries containing code-switched phrases. For reference, we evaluate the performance of existing approaches including pipeline, end-to-end, and zero-shot methods. We show that leveraging existing CLS resources as a pretraining step does not improve performance on CroCoSum, indicating the limited generalizability of current datasets. Finally, we discuss the challenges of evaluating cross-lingual summarizers on code-switched generation through qualitative error analyses",
    "checked": true,
    "id": "1169e95078f6f2c5cb2fd97fa95a64498792aae2",
    "semantic_title": "crocosum: a benchmark dataset for cross-lingual code-switched summarization",
    "citation_count": 3,
    "authors": [
      "Ruochen Zhang",
      "Carsten Eickhoff"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.368": {
    "title": "Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish",
    "volume": "main",
    "abstract": "The rapid spread of misinformation through social media platforms has raised concerns regarding its impact on public opinion. While misinformation is prevalent in other languages, the majority of research in this field has concentrated on the English language. Hence, there is a scarcity of datasets for other languages, including Turkish. To address this concern, we have introduced the FCTR dataset, consisting of 3238 real-world claims. This dataset spans multiple domains and incorporates evidence collected from three Turkish fact-checking organizations. Additionally, we aim to assess the effectiveness of cross-lingual transfer learning for low-resource languages, with a particular focus on Turkish. We demonstrate in-context learning (zero-shot and few-shot) performance of large language models in this context. The experimental results indicate that the dataset has the potential to advance research in the Turkish language",
    "checked": true,
    "id": "45a8a7f4814797cfd8f7f256c45f2f85f40320e9",
    "semantic_title": "cross-lingual learning vs. low-resource fine-tuning: a case study with fact-checking in turkish",
    "citation_count": 0,
    "authors": [
      "Recep Firat Cekinel",
      "Çağrı Çöltekin",
      "Pinar Karagoz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.369": {
    "title": "Cross-lingual Named Entity Corpus for Slavic Languages",
    "volume": "main",
    "abstract": "This paper presents a corpus manually annotated with named entities for six Slavic languages — Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017–2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5,017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits — single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models — XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking",
    "checked": true,
    "id": "1b26c91ea7beca7c526cc82241f6eeeb15db396f",
    "semantic_title": "cross-lingual named entity corpus for slavic languages",
    "citation_count": 0,
    "authors": [
      "Jakub Piskorski",
      "Michał Marcińczuk",
      "Roman Yangarber"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.370": {
    "title": "Cross-Lingual NLU: Mitigating Language-Specific Impact in Embeddings Leveraging Adversarial Learning",
    "volume": "main",
    "abstract": "Low-resource languages and computational expenses pose significant challenges in the domain of large language models (LLMs). Currently, researchers are actively involved in various efforts to tackle these challenges. Cross-lingual natural language processing (NLP) remains one of the most promising strategies to address these issues. In this paper, we introduce a novel approach that utilizes adversarial techniques to mitigate the impact of language-specific information in contextual embeddings generated by large multilingual language models, with potential applications in cross-lingual tasks. The study encompasses five different languages, including both Latin and non-Latin ones, in the context of two fundamental tasks in natural language understanding: intent detection and slot filling. The results primarily show that our current approach excels in zero-shot scenarios for Latin languages like Spanish. However, it encounters limitations when applied to languages distant from English, such as Thai and Persian. This highlights that while our approach effectively reduces the effect of language-specific information on the core meaning, it performs better for Latin languages that share language-specific nuances with English, as certain characteristics persist in the overall meaning within embeddings",
    "checked": true,
    "id": "a459c5a4ffdcb8f78e25d44b842f9df7e590bb8d",
    "semantic_title": "cross-lingual nlu: mitigating language-specific impact in embeddings leveraging adversarial learning",
    "citation_count": 0,
    "authors": [
      "Saedeh Tahery",
      "Sahar Kianian",
      "Saeed Farzi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.371": {
    "title": "Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity",
    "volume": "main",
    "abstract": "Learning better sentence embeddings leads to improved performance for natural language understanding tasks including semantic textual similarity (STS) and natural language inference (NLI). As prior studies leverage large-scale labeled NLI datasets for fine-tuning masked language models to yield sentence embeddings, task performance for languages other than English is often left behind. In this study, we directly compared two data augmentation techniques as potential solutions for monolingual STS: - (a): _cross-lingual transfer_ that exploits English resources alone as training data to yield non-English sentence embeddings as zero-shot inference, and - (b) _machine translation_ that coverts English data into pseudo non-English training data in advance. In our experiments on monolingual STS in Japanese and Korean, we find that the two data techniques yield performance on par. In addition, we find a superiority of Wikipedia domain over NLI domain as unlabeled training data for these languages. Combining our findings, we further demonstrate that the cross-lingual transfer of Wikipedia data exhibits improved performance",
    "checked": true,
    "id": "60b5328e66866cda34de8008b4343b15a4c40a9e",
    "semantic_title": "cross-lingual transfer or machine translation? on data augmentation for monolingual semantic textual similarity",
    "citation_count": 0,
    "authors": [
      "Sho Hoshino",
      "Akihiko Kato",
      "Soichiro Murakami",
      "Peinan Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.372": {
    "title": "Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets",
    "volume": "main",
    "abstract": "Multilingual Language Models (MLLMs) exhibit robust cross-lingual transfer capabilities, or the ability to leverage information acquired in a source language and apply it to a target language. These capabilities find practical applications in well-established Natural Language Processing (NLP) tasks such as Named Entity Recognition (NER). This study aims to investigate the effectiveness of a source language when applied to a target language, particularly in the context of perturbing the input test set. We evaluate on 13 pairs of languages, each including one high-resource language (HRL) and one low-resource language (LRL) with a geographic, genetic, or borrowing relationship. We evaluate two well-known MLLMs—MBERT and XLM-R—on these pairs, in native LRL and cross-lingual transfer settings, in two tasks, under a set of different perturbations. Our findings indicate that NER cross-lingual transfer depends largely on the overlap of entity chunks. If a source and target language have more entities in common, the transfer ability is stronger. Models using cross-lingual transfer also appear to be somewhat more robust to certain perturbations of the input, perhaps indicating an ability to leverage stronger representations derived from the HRL. Our research provides valuable insights into cross-lingual transfer and its implications for NLP applications, and underscores the need to consider linguistic nuances and potential limitations when employing MLLMs across distinct languages",
    "checked": true,
    "id": "20e6b4ad254ad577fd703a16730f4065ab582df0",
    "semantic_title": "cross-lingual transfer robustness to lower-resource languages on adversarial datasets",
    "citation_count": 0,
    "authors": [
      "Shadi Manafi",
      "Nikhil Krishnaswamy"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.373": {
    "title": "CrossTune: Black-Box Few-Shot Classification with Label Enhancement",
    "volume": "main",
    "abstract": "Training or finetuning large-scale language models (LLMs) requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One approach is to treat these models as black boxes and use forward passes (Inference APIs) to interact with them. Current research focuses on adapting these black-box models to downstream tasks using gradient-free prompt optimization, but this often involves an expensive process of searching task-specific prompts. Therefore, we are motivated to study black-box language model adaptation without prompt search. Specifically, we introduce a label-enhanced cross-attention network called CrossTune, which models the semantic relatedness between the input text sequence and task-specific label descriptions. Its effectiveness is examined in the context of few-shot text classification. To improve the generalization of CrossTune, we utilize ChatGPT to generate additional training data through in-context learning. A switch mechanism is implemented to exclude low-quality ChatGPT-generated data. Through extensive experiments on seven benchmark text classification datasets, we demonstrate that our proposed approach outperforms the previous state-of-the-art gradient-free black-box tuning method by 5.7% on average. Even without using ChatGPT-augmented data, CrossTune performs better or comparably than previous black-box tuning methods, suggesting the effectiveness of our approach",
    "checked": true,
    "id": "4e2f99a2ad6660825a8b3cac03eb47dec2c06a1b",
    "semantic_title": "crosstune: black-box few-shot classification with label enhancement",
    "citation_count": 0,
    "authors": [
      "Danqing Luo",
      "Chen Zhang",
      "Yan Zhang",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.374": {
    "title": "Cross-type French Multiword Expression Identification with Pre-trained Masked Language Models",
    "volume": "main",
    "abstract": "Multiword expressions (MWEs) pose difficulties for natural language processing (NLP) due to their linguistic features, such as syntactic and semantic properties, which distinguish them from regular word groupings. This paper describes a combination of two systems: one that learns verbal multiword expressions (VMWEs) and another that learns non-verbal MWEs (nVMWEs). Together, these systems leverage training data from both types of MWEs to enhance performance on a cross-type dataset containing both VMWEs and nVMWEs. Such scenarios emerge when datasets are developed using differing annotation schemes. We explore the fine-tuning of several state-of-the-art neural transformers for each MWE type. Our experiments demonstrate the advantages of the combined system over multi-task approaches or single-task models, addressing the challenges posed by diverse tagsets within the training data. Specifically, we evaluated the combined system on a French treebank named Sequoia, which features an annotation layer encompassing all syntactic types of French MWEs. With this combined approach, we improved the F1-score by approximately 3% on the Sequoia dataset",
    "checked": true,
    "id": "38a22238301aa2fb9ed0f22e80c30c573799ec28",
    "semantic_title": "cross-type french multiword expression identification with pre-trained masked language models",
    "citation_count": 0,
    "authors": [
      "Van-Tuan Bui",
      "Agata Savary"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.375": {
    "title": "CSSWiki: A Chinese Sentence Simplification Dataset with Linguistic and Content Operations",
    "volume": "main",
    "abstract": "Sentence Simplification aims to make sentences easier to read and understand. With most effort on corpus development focused on English, the amount of annotated data is limited in Chinese. To address this need, we introduce CSSWiki, an open-source dataset for Chinese sentence simplification based on Wikipedia. This dataset contains 1.6k source sentences paired with their simplified versions. Each sentence pair is annotated with operation tags that distinguish between linguistic and content modifications. We analyze differences in annotation scheme and data statistics between CSSWiki and existing datasets. We then report baseline sentence simplification performance on CSSWiki using zero-shot and few-shot approaches with Large Language Models",
    "checked": true,
    "id": "47bb1103a819f3f09ced4a3e124bc8d210f7781f",
    "semantic_title": "csswiki: a chinese sentence simplification dataset with linguistic and content operations",
    "citation_count": 0,
    "authors": [
      "Fengkai Liu",
      "John S. Y. Lee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.376": {
    "title": "CTSM: Combining Trait and State Emotions for Empathetic Response Model",
    "volume": "main",
    "abstract": "Empathetic response generation endeavors to empower dialogue systems to perceive speakers' emotions and generate empathetic responses accordingly. Psychological research demonstrates that emotion, as an essential factor in empathy, encompasses trait emotions, which are static and context-independent, and state emotions, which are dynamic and context-dependent. However, previous studies treat them in isolation, leading to insufficient emotional perception of the context, and subsequently, less effective empathetic expression. To address this problem, we propose Combining Trait and State emotions for Empathetic Response Model (CTSM). Specifically, to sufficiently perceive emotions in dialogue, we first construct and encode trait and state emotion embeddings, and then we further enhance emotional perception capability through an emotion guidance module that guides emotion representation. In addition, we propose a cross-contrastive learning decoder to enhance the model's empathetic expression capability by aligning trait and state emotions between generated responses and contexts. Both automatic and manual evaluation results demonstrate that CTSM outperforms state-of-the-art baselines and can generate more empathetic responses. Our code is available at https://github.com/wangyufeng-empty/CTSM",
    "checked": true,
    "id": "b5932860c416cafa867d368de60a322d9270c82e",
    "semantic_title": "ctsm: combining trait and state emotions for empathetic response model",
    "citation_count": 0,
    "authors": [
      "Yufeng Wang",
      "Chao Chen",
      "Zhou Yang",
      "Shuhui Wang",
      "Xiangwen Liao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.377": {
    "title": "CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages",
    "volume": "main",
    "abstract": "Extensive training datasets represent one of the important factors for the impressive learning capabilities of large language models (LLMs). However, these training datasets for current LLMs, especially the recent state-of-the-art models, are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is released in Hugging Face facilitate research and advancements in multilingual LLMs: https://huggingface.co/datasets/uonlp/CulturaX",
    "checked": true,
    "id": "1ebcf1884390c28f24b3adaf5a7aba5b9453b48b",
    "semantic_title": "culturax: a cleaned, enormous, and multilingual dataset for large language models in 167 languages",
    "citation_count": 20,
    "authors": [
      "Thuat Nguyen",
      "Chien Van Nguyen",
      "Viet Dac Lai",
      "Hieu Man",
      "Nghia Trung Ngo",
      "Franck Dernoncourt",
      "Ryan A. Rossi",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.378": {
    "title": "Curation of Benchmark Templates for Measuring Gender Bias in Named Entity Recognition Models",
    "volume": "main",
    "abstract": "Named Entity Recognition (NER) constitutes a popular machine learning technique that empowers several natural language processing applications. As with other machine learning applications, NER models have been shown to be susceptible to gender bias. The latter is often assessed using benchmark datasets, which in turn are curated specifically for a given Natural Language Processing (NLP) task. In this work, we investigate the robustness of benchmark templates to detect gender bias and propose a novel method to improve the curation of such datasets. The method, based on masked token prediction, aims to filter out benchmark templates with a higher probability of detecting gender bias in NER models. We tested the method for English and German, using the corresponding fine-tuned BERT base model (cased) as the NER model. The gender gaps detected with templates classified as appropriate by the method were statistically larger than those detected with inappropriate templates. The results were similar for both languages and support the use of the proposed method in the curation of templates designed to detect gender bias",
    "checked": true,
    "id": "f7a6a6b924fe266f372bda21b49093a23f23ff94",
    "semantic_title": "curation of benchmark templates for measuring gender bias in named entity recognition models",
    "citation_count": 0,
    "authors": [
      "Ana Cimitan",
      "Ana Alves Pinto",
      "Michaela Geierhos"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.379": {
    "title": "CuRIAM: Corpus Re Interpretation and Metalanguage in U.S. Supreme Court Opinions",
    "volume": "main",
    "abstract": "Most judicial decisions involve the interpretation of legal texts. As such, judicial opinions use language as the medium to comment on or draw attention to other language (for example, through definitions and hypotheticals about the meaning of a term from a statute). Language used this way is called metalanguage. Focusing on the U.S. Supreme Court, we view metalanguage as reflective of justices' interpretive processes, bearing on current debates and theories about textualism in law and political science. As a step towards large-scale metalinguistic analysis with NLP, we identify 9 categories prominent in metalinguistic discussions, including key terms, definitions, and different kinds of sources. We annotate these concepts in a corpus of U.S. Supreme Court opinions. Our analysis of the corpus reveals high interannotator agreement, frequent use of quotes and sources, and several notable frequency differences between majority, concurring, and dissenting opinions. We observe fewer instances than expected of several legal interpretive categories. We discuss some of the challenges in developing the annotation schema and applying it and provide recommendations for how this corpus can be used for broader analyses",
    "checked": true,
    "id": "1998283e040b58b5365a7add8e0cdf476e80890f",
    "semantic_title": "curiam: corpus re interpretation and metalanguage in u.s. supreme court opinions",
    "citation_count": 0,
    "authors": [
      "Michael Kranzlein",
      "Nathan Schneider",
      "Kevin Tobia"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.380": {
    "title": "Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "Emotion recognition in conversation (ERC) is a crucial task in natural language processing and affective computing. This paper proposes MultiDAG+CL, a novel approach for Multimodal Emotion Recognition in Conversation (ERC) that employs Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual features within a unified framework. The model is enhanced by Curriculum Learning (CL) to address challenges related to emotional shifts and data imbalance. Curriculum learning facilitates the learning process by gradually presenting training samples in a meaningful order, thereby improving the model's performance in handling emotional variations and data imbalance. Experimental results on the IEMOCAP and MELD datasets demonstrate that the MultiDAG+CL models outperform baseline models. We release the code for and experiments: https://github.com/vanntc711/MultiDAG-CL",
    "checked": true,
    "id": "c621504ed6141b56da42a339f10a157639e9ae3f",
    "semantic_title": "curriculum learning meets directed acyclic graph for multimodal emotion recognition",
    "citation_count": 0,
    "authors": [
      "Cam-Van Thi Nguyen",
      "Cao-Bach Nguyen",
      "Duc-Trong Le",
      "Quang-Thuy Ha"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.381": {
    "title": "CuSINeS: Curriculum-driven Structure Induced Negative Sampling for Statutory Article Retrieval",
    "volume": "main",
    "abstract": "In this paper, we introduce CuSINeS, a negative sampling approach to enhance the performance of Statutory Article Retrieval (SAR). CuSINeS offers three key contributions. Firstly, it employs a curriculum-based negative sampling strategy guiding the model to focus on easier negatives initially and progressively tackle more difficult ones. Secondly, it leverages the hierarchical and sequential information derived from the structural organization of statutes to evaluate the difficulty of samples. Lastly, it introduces a dynamic semantic difficulty assessment using the being-trained model itself, surpassing conventional static methods like BM25, adapting the negatives to the model's evolving competence. Experimental results on a real-world expert-annotated SAR dataset validate the effectiveness of CuSINeS across four different baselines, demonstrating its versatility",
    "checked": true,
    "id": "c34d8e2bf0341eff87a2a8f439e43ba58491d99d",
    "semantic_title": "cusines: curriculum-driven structure induced negative sampling for statutory article retrieval",
    "citation_count": 0,
    "authors": [
      "Santosh T.y.s.s.",
      "Kristina Kaiser",
      "Matthias Grabmair"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.382": {
    "title": "CWTM: Leveraging Contextualized Word Embeddings from BERT for Neural Topic Modeling",
    "volume": "main",
    "abstract": "Most existing topic models rely on bag-of-words (BOW) representation, which limits their ability to capture word order information and leads to challenges with out-of-vocabulary (OOV) words in new documents. Contextualized word embeddings, however, show superiority in word sense disambiguation and effectively address the OOV issue. In this work, we introduce a novel neural topic model called the Contextlized Word Topic Model (CWTM), which integrates contextualized word embeddings from BERT. The model is capable of learning the topic vector of a document without BOW information. In addition, it can also derive the topic vectors for individual words within a document based on their contextualized word embeddings. Experiments across various datasets show that CWTM generates more coherent and meaningful topics compared to existing topic models, while also accommodating unseen words in newly encountered documents",
    "checked": true,
    "id": "a7ed80c424c47b985aade37c00215a3ec994d145",
    "semantic_title": "cwtm: leveraging contextualized word embeddings from bert for neural topic modeling",
    "citation_count": 0,
    "authors": [
      "Zheng Fang",
      "Yulan He",
      "Rob Procter"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.383": {
    "title": "CyberAgressionAdo-v2: Leveraging Pragmatic-Level Information to Decipher Online Hate in French Multiparty Chats",
    "volume": "main",
    "abstract": "As a part of the release of the CyberAgressionAdo-V2 dataset, this paper introduces a new tagset that includes tags marking pragmatic-level information occurring in cyberbullying situations. The previous version of this dataset, CyberAgressionAdo-V1, consists of aggressive multiparty chats in French annotated using a hierarchical tagset developed to describe bullying narrative events including the participant roles, the presence of hate speech, the type of verbal abuse, among others. In contrast, CyberAgressionAdo-V2 uses a multi-label, fine-grained tagset marking the discursive role of exchanged messages as well as the context in which they occur — for instance, attack (ATK), defend (DFN), counterspeech (CNS), abet/instigate (AIN), gaslight (GSL), etc. This paper provides a comprehensive overview of the annotation tagset and presents statistical insights derived from its application. Additionally, we address the challenges encountered when annotating pragmatic-level information in this context, conducting a thorough analysis of annotator disagreements. The resulting dataset comprises 19 conversations that have been manually annotated and is now available to facilitate further research in the field",
    "checked": true,
    "id": "804dec29b35ac38fa4e1628313ce6914ab35dfb2",
    "semantic_title": "cyberagressionado-v2: leveraging pragmatic-level information to decipher online hate in french multiparty chats",
    "citation_count": 0,
    "authors": [
      "Anais Ollagnier"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.384": {
    "title": "Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks",
    "volume": "main",
    "abstract": "In this paper, we introduce a novel Czech dataset for aspect-based sentiment analysis (ABSA), which consists of 3.1K manually annotated reviews from the restaurant domain. The dataset is built upon the older Czech dataset, which contained only separate labels for the basic ABSA tasks such as aspect term extraction or aspect polarity detection. Unlike its predecessor, our new dataset is specifically designed to allow its usage for more complex tasks, e.g. target-aspect-category detection. These advanced tasks require a unified annotation format, seamlessly linking sentiment elements (labels) together. Our dataset follows the format of the well-known SemEval-2016 datasets. This design choice allows effortless application and evaluation in cross-lingual scenarios, ultimately fostering cross-language comparisons with equivalent counterpart datasets in other languages. The annotation process engaged two trained annotators, yielding an impressive inter-annotator agreement rate of approximately 90%. Additionally, we provide 24M reviews without annotations suitable for unsupervised learning. We present robust monolingual baseline results achieved with various Transformer-based models and insightful error analysis to supplement our contributions. Our code and dataset are freely available for non-commercial research purposes",
    "checked": true,
    "id": "0122f11e2f92c2eae8a0d8001b97c5410f4e6f2e",
    "semantic_title": "czech dataset for complex aspect-based sentiment analysis tasks",
    "citation_count": 0,
    "authors": [
      "Jakub Šmíd",
      "Pavel Přibáň",
      "Ondrej Prazak",
      "Pavel Kral"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.385": {
    "title": "DACL: Disfluency Augmented Curriculum Learning for Fluent Text Generation",
    "volume": "main",
    "abstract": "Voice-driven software systems are in abundance. However, language models that power these systems are traditionally trained on fluent, written text corpora. Hence there can be a misalignment between the inherent disfluency of transcribed spoken content and the fluency of the written training data. Furthermore, gold-standard disfluency annotations of various complexities for incremental training can be expensive to collect. So, we propose in this paper a Disfluency Augmented Curriculum Learning (DACL) approach to tackle the complex structure of disfluent sentences and generate fluent texts from them, by using Curriculum Learning (CL) coupled with our synthetically augmented disfluent texts of various levels. DACL harnesses the tiered structure of our generated synthetic disfluent data using CL, by training the model on basic samples (i.e. more fluent) first before training it on more complex samples (i.e. more disfluent). In contrast to the random data exposure paradigm, DACL focuses on a simple-to-complex learning process. We comprehensively evaluate DACL on Switchboard Penn Treebank-3 and compare it to the state-of-the-art disfluency removal models. Our model surpasses existing techniques in word-based precision (by up to 1%) and has shown favorable recall and F1 scores",
    "checked": true,
    "id": "6244d48e25caa40963d881fac83622b5476ccb71",
    "semantic_title": "dacl: disfluency augmented curriculum learning for fluent text generation",
    "citation_count": 1,
    "authors": [
      "Rohan Chaudhury",
      "Maria Teleki",
      "Xiangjue Dong",
      "James Caverlee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.386": {
    "title": "DADIT: A Dataset for Demographic Classification of Italian Twitter Users and a Comparison of Prediction Methods",
    "volume": "main",
    "abstract": "Social scientists increasingly use demographically stratified social media data to study the attitudes, beliefs, and behavior of the general public. To facilitate such analyses, we construct, validate, and release publicly the representative DADIT dataset of 30M tweets of 20k Italian Twitter users, along with their bios and profile pictures. We enrich the user data with high-quality labels for gender, age, and location. DADIT enables us to train and compare the performance of various state-of-the-art models for the prediction of the gender and age of social media users. In particular, we investigate if tweets contain valuable information for the task, since popular classifiers like M3 don't leverage them. Our best XLM-based classifier improves upon the commonly used competitor M3 by up to 53% F1. Especially for age prediction, classifiers profit from including tweets as features. We also confirm these findings on a German test set",
    "checked": true,
    "id": "76c733baa00f0c39e7e367877c2c7b5b367947b6",
    "semantic_title": "dadit: a dataset for demographic classification of italian twitter users and a comparison of prediction methods",
    "citation_count": 0,
    "authors": [
      "Lorenzo Lupo",
      "Paul Bose",
      "Mahyar Habibi",
      "Dirk Hovy",
      "Carlo Schwarz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.387": {
    "title": "DANCER: Entity Description Augmented Named Entity Corrector for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end automatic speech recognition (E2E ASR) systems often suffer from mistranscription of domain-specific phrases, such as named entities, sometimes leading to catastrophic failures in downstream tasks. A family of fast and lightweight named entity correction (NEC) models for ASR have recently been proposed, which normally build on pho-netic-level edit distance algorithms and have shown impressive NEC performance. However, as the named entity (NE) list grows, the problems of phonetic confusion in the NE list are exacerbated; for example, homophone ambiguities increase substantially. In view of this, we proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER), which leverages entity descriptions to provide additional information to facilitate mitigation of phonetic con-fusion for NEC on ASR transcription. To this end, an efficient entity description augmented masked language model (EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to adapt swiftly to domain-specific entities for the NEC task. A series of experiments conducted on the AISHELL-1 and Homophone datasets confirm the effectiveness of our modeling approach. DANCER outperforms a strong baseline, the phonetic edit-distance-based NEC model (PED-NEC), by a character error rate (CER) reduction of about 7% relatively on AISHELL-1 for named entities. More notably, when tested on Homophone that contain named entities of high phonetic confusion, DANCER offers a more pronounced CER reduction of 46% relatively over PED-NEC for named entities. The code is available at https://github.com/Amiannn/Dancer",
    "checked": true,
    "id": "3e3dfa0b552f1c03c4434014689194dcf76038d4",
    "semantic_title": "dancer: entity description augmented named entity corrector for automatic speech recognition",
    "citation_count": 0,
    "authors": [
      "Yi-Cheng Wang",
      "Hsin-Wei Wang",
      "Bi-Cheng Yan",
      "Chi-Han Lin",
      "Berlin Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.388": {
    "title": "DanteLLM: Let's Push Italian LLM Research Forward!",
    "volume": "main",
    "abstract": "In recent years, the dominance of Large Language Models (LLMs) in the English language has become evident. However, there remains a pronounced gap in resources and evaluation tools tailored for non-English languages, underscoring a significant disparity in the global AI landscape. This paper seeks to bridge this gap, specifically focusing on the Italian linguistic context. We introduce a novel benchmark, and an open LLM Leaderboard, designed to evaluate LLMs' performance in Italian, providing a rigorous framework for comparative analysis. In our assessment of currently available models, we highlight their respective strengths and limitations against this standard. Crucially, we propose \"DanteLLM\", a state-of-the-art LLM dedicated to Italian. Our empirical evaluations underscore Dante's superiority, as it emerges as the most performant model on our benchmark, with improvements by up to 6 points. This research not only marks a significant stride in Italian-centric natural language processing but also offers a blueprint for the development and evaluation of LLMs in other languages, championing a more inclusive AI paradigm. Our code at: https://github.com/RSTLess-research/DanteLLM",
    "checked": true,
    "id": "720b7424240bf6970076bf9d64a1d5159e2d3e3f",
    "semantic_title": "dantellm: let's push italian llm research forward!",
    "citation_count": 0,
    "authors": [
      "Andrea Bacciu",
      "Cesare Campagnano",
      "Giovanni Trappolini",
      "Fabrizio Silvestri"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.389": {
    "title": "DARIUS: A Comprehensive Learner Corpus for Argument Mining in German-Language Essays",
    "volume": "main",
    "abstract": "In this paper, we present the DARIUS (Digital Argumentation Instruction for Science) corpus for argumentation quality on 4589 essays written by 1839 German secondary school students. The corpus is annotated according to a fine-grained annotation scheme, ranging from a broader perspective like content zones, to more granular features like argumentation coverage/reach and argumentative discourse units like claims and warrants. The features have inter-annotator agreements up to 0.83 Krippendorff's α. The corpus and dataset are publicly available for further research in argument mining",
    "checked": true,
    "id": "11c89973973bc322f694914d9e01d4c42e14e411",
    "semantic_title": "darius: a comprehensive learner corpus for argument mining in german-language essays",
    "citation_count": 0,
    "authors": [
      "Nils-Jonathan Schaller",
      "Andrea Horbach",
      "Lars Ingver Höft",
      "Yuning Ding",
      "Jan Luca Bahr",
      "Jennifer Meyer",
      "Thorben Jansen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.390": {
    "title": "Data Collection Pipeline for Low-Resource Languages: A Case Study on Constructing a Tetun Text Corpus",
    "volume": "main",
    "abstract": "This paper proposes Labadain Crawler, a data collection pipeline tailored to automate and optimize the process of constructing textual corpora from the web, with a specific target to low-resource languages. The system is built on top of Nutch, an open-source web crawler and data extraction framework, and incorporates language processing components such as a tokenizer and a language identification model. The pipeline efficacy is demonstrated through successful testing with Tetun, one of Timor-Leste's official languages, resulting in the construction of a high-quality Tetun text corpus comprising 321.7k sentences extracted from over 22k web pages. The contributions of this paper include the development of a Tetun tokenizer, a Tetun language identification model, and a Tetun text corpus, marking an important milestone in Tetun text information retrieval",
    "checked": true,
    "id": "8a71e44b13d47c7c6d81f952559fa4f42beab7a6",
    "semantic_title": "data collection pipeline for low-resource languages: a case study on constructing a tetun text corpus",
    "citation_count": 0,
    "authors": [
      "Gabriel de Jesus",
      "Sérgio Sobral Nunes"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.391": {
    "title": "Data Drift in Clinical Outcome Prediction from Admission Notes",
    "volume": "main",
    "abstract": "Clinical NLP research faces a scarcity of publicly available datasets due to privacy concerns. MIMIC-III marked a significant milestone, enabling substantial progress, and now, with MIMIC-IV, the dataset has expanded significantly, offering a broader scope. In this paper, we focus on the task of predicting clinical outcomes from clinical text. This is crucial in modern healthcare, aiding in preventive care, differential diagnosis, and capacity planning. We introduce a novel clinical outcome prediction dataset derived from MIMIC-IV. Furthermore, we provide initial insights into the performance of models trained on MIMIC-III when applied to our new dataset, with specific attention to potential data drift. We investigate challenges tied to evolving documentation standards and changing codes in the International Classification of Diseases (ICD) taxonomy, such as the transition from ICD-9 to ICD-10. We also explore variations in clinical text across different hospital wards. Our study aims to probe the robustness and generalization of clinical outcome prediction models, contributing to the ongoing advancement of clinical NLP in healthcare",
    "checked": true,
    "id": "6286c9698a43de20b6e6d745aa32bd3bd1922b15",
    "semantic_title": "data drift in clinical outcome prediction from admission notes",
    "citation_count": 0,
    "authors": [
      "Paul Grundmann",
      "Jens-Michalis Papaioannou",
      "Tom Oberhauser",
      "Thomas Steffek",
      "Amy Siu",
      "Wolfgang Nejdl",
      "Alexander Loeser"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.392": {
    "title": "Data-Informed Global Sparseness in Attention Mechanisms for Deep Neural Networks",
    "volume": "main",
    "abstract": "Attention mechanisms play a crucial role in the neural revolution of Natural Language Processing (NLP). With the growth of attention-based models, several pruning techniques have been developed to identify and exploit sparseness, making these models more efficient. Most efforts focus on hard-coding attention patterns or pruning attention weights based on training data. We propose Attention Pruning (AP), a framework that observes attention patterns in a fixed dataset and generates a global sparseness mask. AP saves 90% of attention computation for language modeling and about 50% for machine translation and GLUE tasks, maintaining result quality. Our method reveals important distinctions between self- and cross-attention patterns, guiding future NLP research. Our framework can reduce both latency and memory requirements for any attention-based model, aiding in the development of improved models for existing or new NLP applications. We have demonstrated this with encoder and autoregressive transformer models using Triton GPU kernels and make our code publicly available at https://github.com/irugina/AP",
    "checked": true,
    "id": "a044b5fb52db768f34315d300d2a9aee0305bf70",
    "semantic_title": "data-informed global sparseness in attention mechanisms for deep neural networks",
    "citation_count": 0,
    "authors": [
      "Ileana Rugina",
      "Rumen Dangovski",
      "Li Jing",
      "Preslav Nakov",
      "Marin Soljacic"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.393": {
    "title": "Dataset for Identification of Homophobia and Transphobia for Telugu, Kannada, and Gujarati",
    "volume": "main",
    "abstract": "Users of social media platforms are negatively affected by the proliferation of hate or abusive content. There has been a rise in homophobic and transphobic content in recent years targeting LGBT+ individuals. The increasing levels of homophobia and transphobia online can make online platforms harmful and threatening for LGBT+ persons, potentially inhibiting equality, diversity, and inclusion. We are introducing a new dataset for three languages, namely Telugu, Kannada, and Gujarati. Additionally, we have created an expert-labeled dataset to automatically identify homophobic and transphobic content within comments collected from YouTube. We provided comprehensive annotation rules to educate annotators in this process. We collected approximately 10,000 comments from YouTube for all three languages. Marking the first dataset of these languages for this task, we also developed a baseline model with pre-trained transformers",
    "checked": true,
    "id": "93c2d79648666fd1ff871a9f67e9c178e2a44455",
    "semantic_title": "dataset for identification of homophobia and transphobia for telugu, kannada, and gujarati",
    "citation_count": 0,
    "authors": [
      "Prasanna Kumar Kumaresan",
      "Rahul Ponnusamy",
      "Dhruv Sharma",
      "Paul Buitelaar",
      "Bharathi Raja Chakravarthi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.394": {
    "title": "Dataset of Quotation Attribution in German News Articles",
    "volume": "main",
    "abstract": "Extracting who says what to whom is a crucial part in analyzing human communication in today's abundance of data such as online news articles. Yet, the lack of annotated data for this task in German news articles severely limits the quality and usability of possible systems. To remedy this, we present a new, freely available, creative-commons-licensed dataset for quotation attribution in German news articles based on WIKINEWS. The dataset provides curated, high-quality annotations across 1000 documents (250,000 tokens) in a fine-grained annotation schema enabling various downstream uses for the dataset. The annotations not only specify who said what but also how, in which context, to whom and define the type of quotation. We specify our annotation schema, describe the creation of the dataset and provide a quantitative analysis. Further, we describe suitable evaluation metrics, apply two existing systems for quotation attribution, discuss their results to evaluate the utility of our dataset and outline use cases of our dataset in downstream tasks",
    "checked": true,
    "id": "72f870fa490c960a29fd0342ad4efb067c22df36",
    "semantic_title": "dataset of quotation attribution in german news articles",
    "citation_count": 0,
    "authors": [
      "Fynn Petersen-Frey",
      "Chris Biemann"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.395": {
    "title": "DC-MBR: Distributional Cooling for Minimum Bayesian Risk Decoding",
    "volume": "main",
    "abstract": "Minimum Bayesian Risk Decoding (MBR) emerges as a promising decoding algorithm in Neural Machine Translation. However, MBR performs poorly with label smoothing, which is surprising as label smoothing provides decent improvement with beam search and improves generality in various tasks. In this work, we show that the issue arises from the inconsistency of label smoothing on the token-level and sequence-level distributions. We demonstrate that even though label smoothing only causes a slight change in the token level, the sequence-level distribution is highly skewed. We coin the issue autoregressive over-smoothness. To address this issue, we propose a simple and effective method, Distributional Cooling MBR (DC-MBR), which manipulates the entropy of output distributions by tuning down the Softmax temperature. We theoretically prove the equivalence between the pre-tuning label smoothing factor and distributional cooling. Extensive experiments on NMT benchmarks validate that distributional cooling improves MBR in various settings",
    "checked": true,
    "id": "bd489bc7ea6e0b7272527adfc6d27aabde3a177e",
    "semantic_title": "dc-mbr: distributional cooling for minimum bayesian risk decoding",
    "citation_count": 2,
    "authors": [
      "Jianhao Yan",
      "Jin Xu",
      "Fandong Meng",
      "Jie Zhou",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.396": {
    "title": "DDxGym: Online Transformer Policies in a Knowledge Graph Based Natural Language Environment",
    "volume": "main",
    "abstract": "Differential diagnosis (DDx) is vital for physicians and challenging due to the existence of numerous diseases and their complex symptoms. Model training for this task is generally hindered by limited data access due to privacy concerns. To address this, we present DDxGym, a specialized OpenAI Gym environment for clinical differential diagnosis. DDxGym formulates DDx as a natural-language-based reinforcement learning (RL) problem, where agents emulate medical professionals, selecting examinations and treatments for patients with randomly sampled diseases. This RL environment utilizes data labeled from online resources, evaluated by medical professionals for accuracy. Transformers, while effective for encoding text in DDxGym, are unstable in online RL. For that reason we propose a novel training method using an auxiliary masked language modeling objective for policy optimization, resulting in model stabilization and significant performance improvement over strong baselines. Following this approach, our agent effectively navigates large action spaces and identifies universally applicable actions. All data, environment details, and implementation, including experiment reproduction code, are made publicly available",
    "checked": true,
    "id": "7c9d223794e23cfb05b1919e0f07d3a317e79bbe",
    "semantic_title": "ddxgym: online transformer policies in a knowledge graph based natural language environment",
    "citation_count": 0,
    "authors": [
      "Benjamin Winter",
      "Alexei Gustavo Figueroa Rosero",
      "Alexander Loeser",
      "Felix Alexander Gers",
      "Nancy Katerina Figueroa Rosero",
      "Ralf Krestel"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.397": {
    "title": "Dealing with Data Scarcity in Spoken Question Answering",
    "volume": "main",
    "abstract": "This paper focuses on dealing with data scarcity in spoken question answering (QA) using automatic question-answer generation and a carefully selected fine-tuning strategy that leverages limited annotated data (paragraphs and question-answer pairs). Spoken QA is a challenging task due to using spoken documents, i.e., erroneous automatic speech recognition (ASR) transcriptions, and the scarcity of spoken QA data. We propose a framework for utilizing limited annotated data effectively to improve spoken QA performance. To deal with data scarcity, we train a question-answer generation model with annotated data and then produce large amounts of question-answer pairs from unannotated data (paragraphs). Our experiments demonstrate that incorporating limited annotated data and the automatically generated data through a carefully selected fine-tuning strategy leads to 5.5% relative F1 gain over the model trained only with annotated data. Moreover, the proposed framework is also effective in high ASR errors",
    "checked": true,
    "id": "d5e2a9d79a84776ba3ccfa6bf6a141811f24997b",
    "semantic_title": "dealing with data scarcity in spoken question answering",
    "citation_count": 0,
    "authors": [
      "Merve Ünlü Menevşe",
      "Yusufcan Manav",
      "Ebru Arisoy",
      "Arzucan Özgür"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.398": {
    "title": "Debiasing Multi-Entity Aspect-Based Sentiment Analysis with Norm-Based Data Augmentation",
    "volume": "main",
    "abstract": "Bias in NLP models may arise from using pre-trained transformer models trained on biased corpora, or by training or fine-tuning directly on corpora with systemic biases. Recent research has explored strategies for reduce measurable biases in NLP predictions while maintaining prediction accuracy on held-out test sets, e.g., by modifying word embedding geometry after training, using purpose-built neural modules for training, or automatically augmenting training data with examples designed to reduce bias. This paper focuses on a debiasing strategy for aspect-based sentiment analysis (ABSA) by augmenting the training data using norm-based language templates derived from previous language resources. We show that the baseline model predicts lower sentiment toward some topics and individuals than others and has relatively high prediction bias (measured by standard deviation), even when the context is held constant. Our results show that our norm-based data augmentation reduces topical bias to less than half while maintaining prediction quality (measured by RMSE), by augmenting the training data by only 1.8%",
    "checked": true,
    "id": "a682150905e1b8ed3a6e1954484e5626baf27b40",
    "semantic_title": "debiasing multi-entity aspect-based sentiment analysis with norm-based data augmentation",
    "citation_count": 0,
    "authors": [
      "Scott Friedman",
      "Joan Zheng",
      "Hillel Steinmetz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.399": {
    "title": "Deciphering Emotional Landscapes in the Iliad: A Novel French-Annotated Dataset for Emotion Recognition",
    "volume": "main",
    "abstract": "One of the most significant pieces of ancient Greek literature, the Iliad, is part of humanity's collective cultural heritage. This work aims to provide the scientific community with an emotion-labeled dataset for classical literature and Western mythology in particular. To model the emotions of the poem, we use a multi-variate time series. We also evaluated the dataset by means of two methods. We compare the manual classification against a dictionary-based benchmark as well as employ a state-of-the-art deep learning masked language model that has been tuned using our data. Both evaluations return encouraging results (MSE and MAE Macro Avg 0.101 and 0.188 respectively) and highlight some interesting phenomena",
    "checked": true,
    "id": "b45f7a69c0ee99b1bbb383eaf7fb39a9db7a87da",
    "semantic_title": "deciphering emotional landscapes in the iliad: a novel french-annotated dataset for emotion recognition",
    "citation_count": 0,
    "authors": [
      "Davide Picca",
      "John Pavlopoulos"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.400": {
    "title": "DECM: Evaluating Bilingual ASR Performance on a Code-switching/mixing Benchmark",
    "volume": "main",
    "abstract": "Automatic Speech Recognition has made significant progress, but challenges persist. Code-switched (CSW) Speech presents one such challenge, involving the mixing of multiple languages by a speaker. Even when multilingual ASR models are trained, each utterance on its own usually remains monolingual. We introduce an evaluation dataset for German-English CSW, with German as the matrix language and English as the embedded language. The dataset comprises spontaneous speech from diverse domains, enabling realistic CSW evaluation in German-English. It includes splits with varying degrees of CSW to facilitate specialized model analysis. As it is difficult to collect CSW data for all language pairs, the provision of such evaluation data, is crucial for developing and analyzing ASR models capable of generalizing across unseen pairs. Detailed data statistics are presented, and state-of-the-art (SOTA) multilingual models are evaluated showing challanges of CSW speech",
    "checked": true,
    "id": "8c8b711e667a2406a7b6244d5dfd568be73ac925",
    "semantic_title": "decm: evaluating bilingual asr performance on a code-switching/mixing benchmark",
    "citation_count": 0,
    "authors": [
      "Enes Yavuz Ugan",
      "Ngoc-Quan Pham",
      "Alexander Waibel"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.401": {
    "title": "Decoding at the Speed of Thought: Harnessing Parallel Decoding of Lexical Units for LLMs",
    "volume": "main",
    "abstract": "Large language models have demonstrated exceptional capability in natural language understanding and generation. However, their generation speed is limited by the inherently sequential nature of their decoding process, posing challenges for real-time applications. This paper introduces Lexical Unit Decoding (LUD), a novel decoding methodology implemented in a data-driven manner, accelerating the decoding process without sacrificing output quality. The core of our approach is the observation that a pre-trained language model can confidently predict multiple contiguous tokens, forming the basis for a lexical unit, in which these contiguous tokens could be decoded in parallel. Extensive experiments validate that our method substantially reduces decoding time while maintaining generation quality, i.e., 33% speed up on natural language generation with no quality loss, and 30% speed up on code generation with a negligible quality loss of 3%. Distinctively, LUD requires no auxiliary models and does not require changes to existing architectures. It can also be integrated with other decoding acceleration methods, thus achieving an even more pronounced inference efficiency boost. We posit that the foundational principles of LUD could define a new decoding paradigm for future language models, enhancing their applicability for a broader spectrum of applications. All codes are be publicly available at https://github.com/tjunlp-lab/Lexical-Unit-Decoding-LUD-",
    "checked": true,
    "id": "87c57098566ebacbc60ea87eaba06ddf5636bdf0",
    "semantic_title": "decoding at the speed of thought: harnessing parallel decoding of lexical units for llms",
    "citation_count": 0,
    "authors": [
      "Chenxi Sun",
      "Hongzhi Zhang",
      "Zijia Lin",
      "Jingyuan Zhang",
      "Fuzheng Zhang",
      "Zhongyuan Wang",
      "Bin Chen",
      "Chengru Song",
      "Di Zhang",
      "Kun Gai",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.402": {
    "title": "Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models Using Minimal Pairs",
    "volume": "main",
    "abstract": "Inspired by cognitive neuroscience studies, we introduce a novel \"decoding probing\" method that uses minimal pairs benchmark (BLiMP) to probe internal linguistic characteristics in neural language models layer by layer. By treating the language model as the brain and its representations as \"neural activations\", we decode grammaticality labels of minimal pairs from the intermediate layers' representations. This approach reveals: 1) Self-supervised language models capture abstract linguistic structures in intermediate layers that GloVe and RNN language models cannot learn. 2) Information about syntactic grammaticality is robustly captured through the first third layers of GPT-2 and also distributed in later layers. As sentence complexity increases, more layers are required for learning grammatical capabilities. 3) Morphological and semantics/syntax interface-related features are harder to capture than syntax. 4) For Transformer-based models, both embeddings and attentions capture grammatical features but show distinct patterns. Different attention heads exhibit similar tendencies toward various linguistic phenomena, but with varied contributions",
    "checked": true,
    "id": "cf91a55280b875a57b253dddfc9afc882c0f0530",
    "semantic_title": "decoding probing: revealing internal linguistic structures in neural language models using minimal pairs",
    "citation_count": 0,
    "authors": [
      "Linyang He",
      "Peili Chen",
      "Ercong Nie",
      "Yuanning Li",
      "Jonathan R. Brennan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.403": {
    "title": "Decompose, Prioritize, and Eliminate: Dynamically Integrating Diverse Representations for Multimodal Named Entity Recognition",
    "volume": "main",
    "abstract": "Multi-modal Named Entity Recognition, a fundamental task for multi-modal knowledge graph construction, requires integrating multi-modal information to extract named entities from text. Previous research has explored the integration of multi-modal representations at different granularities. However, they struggle to integrate all these multi-modal representations to provide rich contextual information to improve multi-modal named entity recognition. In this paper, we propose DPE-MNER, which is an iterative reasoning framework that dynamically incorporates all the diverse multi-modal representations following the strategy of \"decompose, prioritize, and eliminate\". Within the framework, the fusion of diverse multi-modal representations is decomposed into hierarchically connected fusion layers that are easier to handle. The incorporation of multi-modal information prioritizes transitioning from \"easy-to-hard\" and \"coarse-to-fine\". The explicit modeling of cross-modal relevance eliminate the irrelevances that will mislead the MNER prediction. Extensive experiments on two public datasets have demonstrated the effectiveness of our approach",
    "checked": true,
    "id": "2fe1f5ffa49ed6ed99c2cee345c911394ca8f7f3",
    "semantic_title": "decompose, prioritize, and eliminate: dynamically integrating diverse representations for multimodal named entity recognition",
    "citation_count": 0,
    "authors": [
      "Zihao Zheng",
      "Zihan Zhang",
      "Zexin Wang",
      "Ruiji Fu",
      "Ming Liu",
      "Zhongyuan Wang",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.404": {
    "title": "Deconstructing In-Context Learning: Understanding Prompts via Corruption",
    "volume": "main",
    "abstract": "The ability of large language models (LLMs) to \"learn in context\" based on the provided prompt has led to an explosive growth in their use, culminating in the proliferation of AI assistants such as ChatGPT, Claude, and Bard. These AI assistants are known to be robust to minor prompt modifications, mostly due to alignment techniques that use human feedback. In contrast, the underlying pre-trained LLMs they use as a backbone are known to be brittle in this respect. Building high-quality backbone models remains a core challenge, and a common approach to assessing their quality is to conduct few-shot evaluation. Such evaluation is notorious for being highly sensitive to minor prompt modifications, as well as the choice of specific in-context examples. Prior work has examined how modifying different elements of the prompt can affect model performance. However, these earlier studies tended to concentrate on a limited number of specific prompt attributes and often produced contradictory results. Additionally, previous research either focused on models with fewer than 15 billion parameters or exclusively examined black-box models like GPT-3 or PaLM, making replication challenging. In the present study, we decompose the entire prompt into four components: task description, demonstration inputs, labels, and inline instructions provided for each demonstration. We investigate the effects of structural and semantic corruptions of these elements on model performance. We study models ranging from 1.5B to 70B in size, using ten datasets covering classification and generation tasks. We find that repeating text within the prompt boosts model performance, and bigger models (≥30B) are more sensitive to the semantics of the prompt. Finally, we observe that adding task and inline instructions to the demonstrations enhances model performance even when the instructions are semantically corrupted. The code is available at this URL",
    "checked": true,
    "id": "3bc5e93fd6c888b5cc91b03b2b0f437faf5b6558",
    "semantic_title": "deconstructing in-context learning: understanding prompts via corruption",
    "citation_count": 0,
    "authors": [
      "Namrata Shivagunde",
      "Vladislav Lialin",
      "Sherin Muckatira",
      "Anna Rumshisky"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.405": {
    "title": "DEEM: Dynamic Experienced Expert Modeling for Stance Detection",
    "volume": "main",
    "abstract": "Recent work has made a preliminary attempt to use large language models (LLMs) to solve the stance detection task, showing promising results. However, considering that stance detection usually requires detailed background knowledge, the vanilla reasoning method may neglect the domain knowledge to make a professional and accurate analysis. Thus, there is still room for improvement of LLMs reasoning, especially in leveraging the generation capability of LLMs to simulate specific experts (i.e., multi-agents) to detect the stance. In this paper, different from existing multi-agent works that require detailed descriptions and use fixed experts, we propose a Dynamic Experienced Expert Modeling (DEEM) method which can leverage the generated experienced experts and let LLMs reason in a semi-parametric way, making the experts more generalizable and reliable. Experimental results demonstrate that DEEM consistently achieves the best results on three standard benchmarks, outperforms methods with self-consistency reasoning, and reduces the bias of LLMs",
    "checked": true,
    "id": "f200dc0b550f318412bd8484f5c51836d78d93dc",
    "semantic_title": "deem: dynamic experienced expert modeling for stance detection",
    "citation_count": 0,
    "authors": [
      "Xiaolong Wang",
      "Yile Wang",
      "Sijie Cheng",
      "Peng Li",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.406": {
    "title": "Deep Learning Based Named Entity Recognition Models for Recipes",
    "volume": "main",
    "abstract": "Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. Recipes are cultural capsules transmitted across generations via unstructured text. Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation. Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels. Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER. Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity to create the machine-annotated dataset. A thorough investigation of NER approaches on these three datasets involving statistical, fine-tuning of deep learning-based language models and few-shot prompting on large language models (LLMs) provides deep insights. We conclude that few-shot prompting on LLMs has abysmal performance, whereas the fine-tuned spaCy-transformer emerges as the best model with macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated, augmented, and machine-annotated datasets, respectively",
    "checked": true,
    "id": "7f9d6d10c3af5e2fc82a48027065c3a830804f0b",
    "semantic_title": "deep learning based named entity recognition models for recipes",
    "citation_count": 0,
    "authors": [
      "Ayush Agarwal",
      "Janak Kapuriya",
      "Shubham Agrawal",
      "Akhil Vamshi Konam",
      "Mansi Goel",
      "Rishabh Gupta",
      "Shrey Rastogi",
      "Niharika Niharika",
      "Ganesh Bagler"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.407": {
    "title": "Deep Reinforcement Learning-based Dialogue Policy with Graph Convolutional Q-network",
    "volume": "main",
    "abstract": "Deep Reinforcement learning (DRL) has been successfully applied to the dialogue policy of task-oriented dialogue systems. However, one challenge in the existing DRL-based dialogue policy methods is their unstructured state-action representations without the ability to learn the relationship between dialogue states and actions. To alleviate this problem, we propose a graph-structured dialogue policy framework for task-oriented dialogue systems. More specifically, we use an unsupervised approach to construct two different bipartite graphs. Then, we generate the user-related and knowledge-related subgraphs based on the matching dialogue sub-states with bipartite graph nodes. A variant of graph convolutional network is employed to encode dialogue subgraphs. After that, we use a bidirectional gated cycle unit (BGRU) and self-attention mechanism to obtain the high-level historical state representations and employ a neural network for the high-level current state representations. The two state representations are joined to learn the action value of dialogue policy. Experiments implemented with different DRL algorithms demonstrate that the proposed framework significantly improves the effectiveness and stability of dialogue policies",
    "checked": true,
    "id": "473934905b6891b7ec05d69ad92eaa88dbdd1580",
    "semantic_title": "deep reinforcement learning-based dialogue policy with graph convolutional q-network",
    "citation_count": 0,
    "authors": [
      "Kai Xu",
      "Zhengyu Wang",
      "Yuxuan Long",
      "Qiaona Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.408": {
    "title": "Deep Reinforcement Learning with Hierarchical Action Exploration for Dialogue Generation",
    "volume": "main",
    "abstract": "Traditionally, approximate dynamic programming is employed in dialogue generation with greedy policy improvement through action sampling, as the natural language action space is vast. However, this practice is inefficient for reinforcement learning (RL) due to the sparsity of eligible responses with high action values, which leads to weak improvement sustained by random sampling. This paper presents theoretical analysis and experiments that reveal the performance of the dialogue policy is positively correlated with the sampling size. To overcome this limitation, we introduce a novel dual-granularity Q-function that explores the most promising response category to intervene in the sampling process. Our approach extracts actions based on a grained hierarchy, thereby achieving the optimum with fewer policy iterations. Additionally, we use offline RL and learn from multiple reward functions designed to capture emotional nuances in human interactions. Empirical studies demonstrate that our algorithm outperforms baselines across automatic metrics and human evaluations. Further testing reveals that our algorithm exhibits both explainability and controllability, as well as generates responses with higher expected rewards",
    "checked": true,
    "id": "f5cff6887606bc0c826194609013e9f36b45d09a",
    "semantic_title": "deep reinforcement learning with hierarchical action exploration for dialogue generation",
    "citation_count": 0,
    "authors": [
      "Itsugun Cho",
      "Ryota Takahashi",
      "Yusaku Yanase",
      "Hiroaki Saito"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.409": {
    "title": "DeFaktS: A German Dataset for Fine-Grained Disinformation Detection through Social Media Framing",
    "volume": "main",
    "abstract": "In today's rapidly evolving digital age, disinformation poses a significant threat to public sentiment and socio-political dynamics. To address this, we introduce a new dataset \"DeFaktS\", designed to understand and counter disinformation within German media. Distinctively curated across various news topics, DeFaktS offers an unparalleled insight into the diverse facets of disinformation. Our dataset, containing 105,855 posts with 20,008 meticulously labeled tweets, serves as a rich platform for in-depth exploration of disinformation's diverse characteristics. A key attribute that sets DeFaktS apart is, its fine-grain annotations based on polarized categories. Our annotation framework, grounded in the textual characteristics of news content, eliminates the need for external knowledge sources. Unlike most existing corpora that typically assign a singular global veracity value to news, our methodology seeks to annotate every structural component and semantic element of a news piece, ensuring a comprehensive and detailed understanding. In our experiments, we employed a mix of classical machine learning and advanced transformer-based models. The results underscored the potential of DeFaktS, with transformer models, especially the German variant of BERT, exhibiting pronounced effectiveness in both binary and fine-grained classifications",
    "checked": true,
    "id": "0be41c94be4989f44970ffba209bccbce79518e6",
    "semantic_title": "defakts: a german dataset for fine-grained disinformation detection through social media framing",
    "citation_count": 0,
    "authors": [
      "Shaina Ashraf",
      "Isabel Bezzaoui",
      "Ionut Andone",
      "Alexander Markowetz",
      "Jonas Fegert",
      "Lucie Flek"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.410": {
    "title": "DEIE: Benchmarking Document-level Event Information Extraction with a Large-scale Chinese News Dataset",
    "volume": "main",
    "abstract": "A text corpus centered on events is foundational to research concerning the detection, representation, reasoning, and harnessing of online events. The majority of current event-based datasets mainly target sentence-level tasks, thus to advance event-related research spanning from sentence to document level, this paper introduces DEIE, a unified large-scale document-level event information extraction dataset with over 56,000+ events and 242,000+ arguments. Three key features stand out: large-scale manual annotation (20,000 documents), comprehensive unified annotation (encompassing event trigger/argument, summary, and relation at once), and emergency events annotation (covering 19 emergency types). Notably, our experiments reveal that current event-related models struggle with DEIE, signaling a pressing need for more advanced event-related research in the future",
    "checked": true,
    "id": "96f8d643c21c9e6bb39bc12fbf1377f970ed9f37",
    "semantic_title": "deie: benchmarking document-level event information extraction with a large-scale chinese news dataset",
    "citation_count": 0,
    "authors": [
      "Yubing Ren",
      "Yanan Cao",
      "Hao Li",
      "Yingjie Li",
      "Zixuan ZM Ma",
      "Fang Fang",
      "Ping Guo",
      "Wei Ma"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.411": {
    "title": "DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning",
    "volume": "main",
    "abstract": "Vision-and-Language navigation (VLN) requires an agent to navigate in unseen environment by following natural language instruction. For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation and navigation history. Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective. Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision. To address this problem, we propose a Dual-levEL AligNment (DELAN) framework by cross-modal contrastive learning. This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making. Specifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation level according to their semantic correlations. We also reconstruct a dual-level instruction for adaptation to the dual-level alignment. As the training signals for pre-fusion alignment are extremely limited, self-supervised contrastive learning strategies are employed to enforce the matching between different modalities. Our approach seamlessly integrates with the majority of existing models, resulting in improved navigation performance on various VLN benchmarks, including R2R, R4R, RxR and CVDN",
    "checked": true,
    "id": "14bc5bc58930db71f7de31e704f0446e3e6a33c9",
    "semantic_title": "delan: dual-level alignment for vision-and-language navigation by cross-modal contrastive learning",
    "citation_count": 0,
    "authors": [
      "Mengfei Du",
      "Binhao Wu",
      "Jiwen Zhang",
      "Zhihao Fan",
      "Zejun Li",
      "Ruipu Luo",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.412": {
    "title": "Demonstration Retrieval-Augmented Generative Event Argument Extraction",
    "volume": "main",
    "abstract": "We tackle Event Argument Extraction (EAE) in the manner of template-based generation. Based on our exploration of generative EAE, it suffers from several issues, such as multiple arguments of one role, generating words out of context and inconsistency with prescribed format. We attribute it to the weakness of following complex input prompts. To address these problems, we propose the demonstration retrieval-augmented generative EAE (DRAGEAE), containing two components: event knowledge-injected generator (EKG) and demonstration retriever (DR). EKG employs event knowledge prompts to capture role dependencies and semantics. DR aims to search informative demonstrations from training data, facilitating the conditional generation of EKG. To train DR, we use the probability-based rankings from large language models (LLMs) as supervised signals. Experimental results on ACE-2005, RAMS and WIKIEVENTS demonstrate that our method outperforms all strong baselines and it can be generalized to various datasets. Further analysis is conducted to discuss the impact of diverse LLMs and prove that our model alleviates the above issues",
    "checked": true,
    "id": "0bcc6fd7eca41c6b646b70462e3aa37eb90b456c",
    "semantic_title": "demonstration retrieval-augmented generative event argument extraction",
    "citation_count": 0,
    "authors": [
      "Shiming He",
      "Yu Hong",
      "Shuai Yang",
      "Jianmin Yao",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.413": {
    "title": "Denoising Labeled Data for Comment Moderation Using Active Learning",
    "volume": "main",
    "abstract": "Noisily labeled textual data is ample on internet platforms that allow user-created content. Training models, such as offensive language detection models for comment moderation, on such data may prove difficult as the noise in the labels prevents the model to converge. In this work, we propose to use active learning methods for the purposes of denoising training data for model training. The goal is to sample examples the most informative examples with noisy labels with active learning and send them to the oracle for reannotation thus reducing the overall cost of reannotation. In this setting we tested three existing active learning methods, namely DBAL, Variance of Gradients (VoG) and BADGE. The proposed approach to data denoising is tested on the problem of offensive language detection. We observe that active learning can be effectively used for the purposes of data denoising, however care should be taken when choosing the algorithm for this purpose",
    "checked": true,
    "id": "1bc59e3ee5537b8e30f781581970ec5026f94090",
    "semantic_title": "denoising labeled data for comment moderation using active learning",
    "citation_count": 0,
    "authors": [
      "Andraž Pelicon",
      "Mladen Karan",
      "Ravi Shekhar",
      "Matthew Purver",
      "Senja Pollak"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.414": {
    "title": "Denoising Table-Text Retrieval for Open-Domain Question Answering",
    "volume": "main",
    "abstract": "In table-text open-domain question answering, a retriever system retrieves relevant evidence from tables and text to answer questions. Previous studies in table-text open-domain question answering have two common challenges: firstly, their retrievers can be affected by false-positive labels in training datasets; secondly, they may struggle to provide appropriate evidence for questions that require reasoning across the table. To address these issues, we propose Denoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a denoised training dataset with fewer false positive labels by discarding instances with lower question-relevance scores measured through a false positive detection model. Subsequently, we integrate table-level ranking information into the retriever to assist in finding evidence for questions that demand reasoning across the table. To encode this ranking information, we fine-tune a rank-aware column encoder to identify minimum and maximum values within a column. Experimental results demonstrate that DoTTeR significantly outperforms strong baselines on both retrieval recall and downstream QA tasks. Our code is available at https://github.com/deokhk/DoTTeR",
    "checked": true,
    "id": "eff199302b6b3959f6e8c4ab42becec2e5c0ba7b",
    "semantic_title": "denoising table-text retrieval for open-domain question answering",
    "citation_count": 0,
    "authors": [
      "Deokhyung Kang",
      "Baikjin Jung",
      "Yunsu Kim",
      "Gary Geunbae Lee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.415": {
    "title": "Dependencies over Times and Tools (DoTT)",
    "volume": "main",
    "abstract": "Purpose: Based on the examples of English and German, we investigate to what extent parsers trained on modern variants of these languages can be transferred to older language levels without loss. Methods: We developed a treebank called DoTT (https://github.com/texttechnologylab/DoTT) which covers, roughly, the time period from 1800 until today, in conjunction with the further development of the annotation tool DependencyAnnotator. DoTT consists of a collection of diachronic corpora enriched with dependency annotations using 3 parsers, 6 pre-trained language models, 5 newly trained models for German, and two tag sets (TIGER and Universal Dependencies). To assess how the different parsers perform on texts from different time periods, we created a gold standard sample as a benchmark. Results: We found that the parsers/models perform quite well on modern texts (document-level LAS ranging from 82.89 to 88.54) and slightly worse on older texts, as expected (average document-level LAS 84.60 vs. 86.14), but not significantly. For German texts, the (German) TIGER scheme achieved slightly better results than UD. Conclusion: Overall, this result speaks for the transferability of parsers to past language levels, at least dating back until around 1800. This very transferability, it is however argued, means that studies of language change in the field of dependency syntax can draw on dependency distance but miss out on some grammatical phenomena",
    "checked": true,
    "id": "ca30984488bd6b90399831a4a05ecca88dbbd9f9",
    "semantic_title": "dependencies over times and tools (dott)",
    "citation_count": 0,
    "authors": [
      "Andy Luecking",
      "Giuseppe Abrami",
      "Leon Hammerla",
      "Marc Rahn",
      "Daniel Baumartz",
      "Steffen Eger",
      "Alexander Mehler"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.416": {
    "title": "Depth Aware Hierarchical Replay Continual Learning for Knowledge Based Question Answering",
    "volume": "main",
    "abstract": "Continual learning is an emerging area of machine learning that deals with the issue where models adapt well to the latest data but lose the ability to remember past data due to changes in the data source. A widely adopted solution is by keeping a small memory of previous learned data that use replay. Most of the previous studies on continual learning focused on classification tasks, such as image classification and text classification, where the model needs only to categorize the input data. Inspired by the human ability to incrementally learn knowledge and solve different problems using learned knowledge, we considered a more pratical scenario, knowledge based quesiton answering about continual learning. In this scenario, each single question is different from others(means different fact trippes to answer them) while classification tasks only need to find feature boundaries of different categories, which are the curves or surfaces that separate different categories in the feature space. To address this issue, we proposed a depth aware hierarchical replay framework which include a tree structure classfier to have a sense of knowledge distribution and fill the gap between text classfication tasks and question-answering tasks for continual learning, a local sampler to grasp these critical samples and a depth aware learning network to reconstructe the feature space of a single learning round. In our experiments, we have demonstrated that our proposed model outperforms previous continual learning methods in mitigating the issue of catastrophic forgetting",
    "checked": true,
    "id": "383e49925b65f9fe8cb9108ea46e9106b6304c69",
    "semantic_title": "depth aware hierarchical replay continual learning for knowledge based question answering",
    "citation_count": 0,
    "authors": [
      "Zhixiong Cao",
      "Hai-Tao Zheng",
      "Yangning Li",
      "Jin Xu",
      "Rongsheng Li",
      "Hong-Gee Kim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.417": {
    "title": "Depth-Wise Attention (DWAtt): A Layer Fusion Method for Data-Efficient Classification",
    "volume": "main",
    "abstract": "Language Models pretrained on large textual data have been shown to encode different types of knowledge simultaneously. Traditionally, only the features from the last layer are used when adapting to new tasks or data. We put forward that, when using or finetuning deep pretrained models, intermediate layer features that may be relevant to the downstream task are buried too deep to be used efficiently in terms of needed samples or steps. To test this, we propose a new layer fusion method: Depth-Wise Attention (DWAtt), to help re-surface signals from non-final layers. We compare DWAtt to a basic concatenation-based layer fusion method (Concat), and compare both to a deeper model baseline—all kept within a similar parameter budget. Our findings show that DWAtt and Concat are more step- and sample-efficient than the baseline, especially in the few-shot setting. DWAtt outperforms Concat on larger data sizes. On CoNLL-03 NER, layer fusion shows 3.68 − 9.73% F1 gain at different few-shot sizes. The layer fusion models presented significantly outperform the baseline in various training scenarios with different data sizes, architectures, and training constraints",
    "checked": true,
    "id": "5dce85ea9ab0a716159077960f370a9a11541e65",
    "semantic_title": "depth-wise attention (dwatt): a layer fusion method for data-efficient classification",
    "citation_count": 1,
    "authors": [
      "Muhammad ElNokrashy",
      "Badr AlKhamissi",
      "Mona Diab"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.418": {
    "title": "Deriving Entity-Specific Embeddings from Multi-Entity Sequences",
    "volume": "main",
    "abstract": "Underpinning much of the recent progress in deep learning is the transformer architecture, which takes as input a sequence of embeddings E and emits an updated sequence of embeddings E'. A special [CLS] embedding is often included in this sequence, serving as a description of the sequence once processed and used as the basis for subsequent sequence-level tasks. The processed [CLS] embedding loses utility, however, when the model is presented with a multi-entity sequence and asked to perform an entity-specific task. When processing a multi-speaker dialogue, for example, the [CLS] embedding describes the entire dialogue, not any individual utterance/speaker. Existing methods toward entity-specific prediction involve redundant computation or post-processing outside of the transformer. We present a novel methodology for deriving entity-specific embeddings from a multi-entity sequence completely within the transformer, with a loose definition of entity amenable to many problem spaces. To show the generic applicability of our method, we apply it to widely different tasks: emotion recognition in conversation and player performance projection in baseball and show that it can be used to achieve SOTA in both. Code can be found at https://github.com/c-heat16/EntitySpecificEmbeddings",
    "checked": true,
    "id": "c736b146b01b5bb3a950a4fc8d85e0c421b3d887",
    "semantic_title": "deriving entity-specific embeddings from multi-entity sequences",
    "citation_count": 0,
    "authors": [
      "Connor Heaton",
      "Prasenjit Mitra"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.419": {
    "title": "DET: A Dual-Encoding Transformer for Relational Graph Embedding",
    "volume": "main",
    "abstract": "Despite recent successes in natural language processing and computer vision, Transformer faces scalability issues when processing graphs, e.g., computing the full node-to-node attention on knowledge graphs (KGs) with million of entities is still infeasible. The existing methods mitigate this problem by considering only the local neighbors, sacrificing the Transformer's ability to attend to elements at any distance. This paper proposes a new Transformer architecture called Dual-Encoding Transformer (DET). DET comprises a structural encoder to aggregate information from nearby neighbors, and a semantic encoder to seek for semantically relevant nodes. We adopt a semantic neighbor search approach inspired by multiple sequence alignment (MSA) algorithms used in biological sciences. By stacking the two encoders alternately, similar to the MSA Transformer for protein representation, our method achieves superior performance compared to state-of-the-art attention-based methods on complex relational graphs like KGs and citation networks. Additionally, DET remains competitive for smaller graphs such as molecules",
    "checked": true,
    "id": "574618947f65398bbe026c5749950778b8f58a1e",
    "semantic_title": "det: a dual-encoding transformer for relational graph embedding",
    "citation_count": 0,
    "authors": [
      "Lingbing Guo",
      "Zhuo Chen",
      "Jiaoyan Chen",
      "Qiang Zhang",
      "Huajun Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.420": {
    "title": "Detecting Conceptual Abstraction in LLMs",
    "volume": "main",
    "abstract": "We show a novel approach to detecting noun abstraction within a large language model (LLM). Starting from a psychologically motivated set of noun pairs in taxonomic relationships, we instantiate surface patterns indicating hypernymy and analyze the attention matrices produced by BERT. We compare the results to two sets of counterfactuals and show that we can detect hypernymy in the abstraction mechanism, which cannot solely be related to the distributional similarity of noun pairs. Our findings are a first step towards the explainability of conceptual abstraction in LLMs",
    "checked": true,
    "id": "7c32ee2b5523514ecd82b1fe268f652e120505ab",
    "semantic_title": "detecting conceptual abstraction in llms",
    "citation_count": 0,
    "authors": [
      "Michaela Regneri",
      "Alhassan Abdelhalim",
      "Soeren Laue"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.421": {
    "title": "Detecting Critical Errors Considering Cross-Cultural Factors in English-Korean Translation",
    "volume": "main",
    "abstract": "Recent machine translation (MT) systems have overcome language barriers for a wide range of users, yet they still carry the risk of critical meaning deviation. Critical error detection (CED) is a task that identifies an inherent risk of catastrophic meaning distortions in the machine translation output. With the importance of reflecting cultural elements in detecting critical errors, we introduce the culture-aware \"Politeness\" type in detecting English-Korean critical translation errors. Besides, we facilitate two tasks by providing multiclass labels: critical error detection and critical error type classification (CETC). Empirical evaluations reveal that our introduced data augmentation approach using a newly presented perturber significantly outperforms existing baselines in both tasks. Further analysis highlights the significance of multiclass labeling by demonstrating its superior effectiveness compared to binary labels",
    "checked": true,
    "id": "30df4c8a4d06fcff5999f80c235a79f6196bdfed",
    "semantic_title": "detecting critical errors considering cross-cultural factors in english-korean translation",
    "citation_count": 0,
    "authors": [
      "Sugyeong Eo",
      "Jungwoo Lim",
      "Chanjun Park",
      "DaHyun Jung",
      "Seonmin Koo",
      "Hyeonseok Moon",
      "Jaehyung Seo",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.422": {
    "title": "Detecting Cybercrimes in Accordance with Pakistani Law: Dataset and Evaluation Using PLMs",
    "volume": "main",
    "abstract": "Cybercrime is a serious and growing threat affecting millions of people worldwide. Detecting cybercrimes from text messages is challenging, as it requires understanding the linguistic and cultural nuances of different languages and regions. Roman Urdu is a widely used language in Pakistan and other South Asian countries, however, it lacks sufficient resources and tools for natural language processing and cybercrime detection. To address this problem, we make three main contributions in this paper. (1) We create and release CRU, a benchmark dataset for text-based cybercrime detection in Roman Urdu, which covers a number of cybercrimes as defined by the Prevention of Electronic Crimes Act (PECA) of Pakistan. This dataset is annotated by experts following a standardized procedure based on Pakistan's legal framework. (2) We perform experiments on four pre-trained language models (PLMs) for cybercrime text classification in Roman Urdu. Our results show that xlm-roberta-base is the best model for this task, achieving the highest performance on all metrics. (3) We explore the utility of prompt engineering techniques, namely prefix and cloze prompts, for enhancing the performance of PLMs for low-resource languages such as Roman Urdu. We analyze the impact of different prompt shapes and k-shot settings on the performance of xlm-roberta-base and bert-base-multilingual-cased. We find that prefix prompts are more effective than cloze prompts for Roman Urdu classification tasks, as they provide more contextually relevant completions for the models. Our work provides useful insights and resources for future research on cybercrime detection and text classification in low-resource languages",
    "checked": true,
    "id": "7a90b9273f5642c704f4168e341cbe64044482eb",
    "semantic_title": "detecting cybercrimes in accordance with pakistani law: dataset and evaluation using plms",
    "citation_count": 0,
    "authors": [
      "Faizad Ullah",
      "Ali Faheem",
      "Ubaid Azam",
      "Muhammad Sohaib Ayub",
      "Faisal Kamiran",
      "Asim Karim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.423": {
    "title": "Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics",
    "volume": "main",
    "abstract": "We explore a strategy to handle controversial topics in LLM-based chatbots based on Wikipedia's Neutral Point of View (NPOV) principle: acknowledge the absence of a single true answer and surface multiple perspectives. We frame this as retrieval augmented generation, where perspectives are retrieved from a knowledge base and the LLM is tasked with generating a fluent and faithful response from the given perspectives. As a starting point, we use a deterministic retrieval system and then focus on common LLM failure modes that arise during this approach to text generation, namely hallucination and coverage errors. We propose and evaluate three methods to detect such errors based on (1) word-overlap, (2) salience, and (3) LLM-based classifiers. Our results demonstrate that LLM-based classifiers, even when trained only on synthetic errors, achieve high error detection performance, with ROC AUC scores of 95.3% for hallucination and 90.5% for coverage error detection on unambiguous error cases. We show that when no training data is available, our other methods still yield good results on hallucination (84.0%) and coverage error (85.2%) detection",
    "checked": true,
    "id": "c1bceabbbd5459c3e1d268ed84ab31c6ad28e3b2",
    "semantic_title": "detecting hallucination and coverage errors in retrieval augmented generation for controversial topics",
    "citation_count": 0,
    "authors": [
      "Tyler A. Chang",
      "Katrin Tomanek",
      "Jessica Hoffmann",
      "Nithum Thain",
      "Erin MacMurray van Liemt",
      "Kathleen Meier-Hellstern",
      "Lucas Dixon"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.424": {
    "title": "Detecting Impact Relevant Sections in Scientific Research",
    "volume": "main",
    "abstract": "Impact assessment is an evolving area of research that aims at measuring and predicting the potential effects of projects or programs. Measuring the impact of scientific research is a vibrant subdomain, closely intertwined with impact assessment. A recurring obstacle pertains to the absence of an efficient framework which can facilitate the analysis of lengthy reports and text labeling. To address this issue, we propose a framework for automatically assessing the impact of scientific research projects by identifying pertinent sections in project reports that indicate the potential impacts. We leverage a mixed-method approach, combining manual annotations with supervised machine learning, to extract these passages from project reports. We experiment with different machine learning algorithms, including traditional statistical models as well as pre-trained transformer language models. Our experiments show that our proposed method achieves accuracy scores up to 0.81, and that our method is generalizable to scientific research from different domains and different languages",
    "checked": true,
    "id": "c7716a264992ffeffdb333c60cdc7151ad8427b1",
    "semantic_title": "detecting impact relevant sections in scientific research",
    "citation_count": 0,
    "authors": [
      "Maria Becker",
      "Kanyao Han",
      "Antonina Werthmann",
      "Rezvaneh Rezapour",
      "Haejin Lee",
      "Jana Diesner"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.425": {
    "title": "Detecting Loanwords in Emakhuwa: An Extremely Low-Resource Bantu Language Exhibiting Significant Borrowing from Portuguese",
    "volume": "main",
    "abstract": "The accurate identification of loanwords within a given text holds significant potential as a valuable tool for addressing data augmentation and mitigating data sparsity issues. Such identification can improve the performance of various natural language processing tasks, particularly in the context of low-resource languages that lack standardized spelling conventions.This research proposes a supervised method to identify loanwords in Emakhuwa, borrowed from Portuguese. Our methodology encompasses a two-fold approach. Firstly, we employ traditional machine learning algorithms incorporating handcrafted features, including language-specific and similarity-based features. We build upon prior studies to extract similarity features and propose utilizing two external resources: a Sequence-to-Sequence model and a dictionary. This innovative approach allows us to identify loanwords solely by analyzing the target word without prior knowledge about its donor counterpart. Furthermore, we fine-tune the pre-trained CANINE model for the downstream task of loanword detection, which culminates in the impressive achievement of the F1-score of 93%. To the best of our knowledge, this study is the first of its kind focusing on Emakhuwa, and the preliminary results are promising as they pave the way to further advancements",
    "checked": true,
    "id": "234cf9ae13ac0f4731308098f7c644f02c2e3fe4",
    "semantic_title": "detecting loanwords in emakhuwa: an extremely low-resource bantu language exhibiting significant borrowing from portuguese",
    "citation_count": 0,
    "authors": [
      "Felermino Dario Mario Ali",
      "Henrique Lopes Cardoso",
      "Rui Sousa-Silva"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.426": {
    "title": "Detecting Offensive Language in an Open Chatbot Platform",
    "volume": "main",
    "abstract": "While detecting offensive language in online spaces remains an important societal issue, there is still a significant gap in existing research and practial datasets specific to chatbots. Furthermore, many of the current efforts by service providers to automatically filter offensive language are vulnerable to users' deliberate text manipulation tactics, such as misspelling words. In this study, we analyze offensive language patterns in real logs of 6,254,261 chat utterance pairs from the commercial chat service Simsimi, which cover a variety of conversation topics. Based on the observed patterns, we introduce a novel offensive language detection method—a contrastive learning model that embeds chat content with a random masking strategy. We show that this model outperforms existing models in detecting offensive language in open-domain chat conversations while also demonstrating robustness against users' deliberate text manipulation tactics when using offensive language. We release our curated chatbot dataset to foster research on offensive language detection in open-domain conversations and share lessons learned from mitigating offensive language on a live platform",
    "checked": true,
    "id": "48406a9f61c007a9d654007f50a95cbe16d43e68",
    "semantic_title": "detecting offensive language in an open chatbot platform",
    "citation_count": 2,
    "authors": [
      "Hyeonho Song",
      "Jisu Hong",
      "Chani Jung",
      "Hyojin Chin",
      "Mingi Shin",
      "Yubin Choi",
      "Junghoi Choi",
      "Meeyoung Cha"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.427": {
    "title": "Detecting Sexual Content at the Sentence Level in First Millennium Latin Texts",
    "volume": "main",
    "abstract": "In this study, we propose to evaluate the use of deep learning methods for semantic classification at the sentence level to accelerate the process of corpus building in the field of humanities and linguistics, a traditional and time-consuming task. We introduce a novel corpus comprising around 2500 sentences spanning from 300 BCE to 900 CE including sexual semantics (medical, erotica, etc.). We evaluate various sentence classification approaches and different input embedding layers, and show that all consistently outperform simple token-based searches. We explore the integration of idiolectal and sociolectal metadata embeddings (centuries, author, type of writing), but find that it leads to overfitting. Our results demonstrate the effectiveness of this approach, achieving high precision and true positive rates (TPR) of respectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset size on the model performances (420 instead of 2013 training samples), and show that, while our models perform worse, they still offer a high enough precision and TPR, even without MLM, respectively 69% and 51%. Given the result, we provide an analysis of the attention mechanism as a supporting added value for humanists in order to produce more data",
    "checked": true,
    "id": "70f9145c752990e5b6542c91d13262950ab84f47",
    "semantic_title": "detecting sexual content at the sentence level in first millennium latin texts",
    "citation_count": 0,
    "authors": [
      "Thibault Clerice"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.428": {
    "title": "Detection, Diagnosis, and Explanation: A Benchmark for Chinese Medial Hallucination Evaluation",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have made significant progress recently. However, their practical use in healthcare is hindered by their tendency to generate hallucinations. One specific type, called snowballing hallucination, occurs when LLMs encounter misleading information, and poses a security threat to LLMs. To understand how well LLMs can resist these hallucination, we create the Chinese Medical Hallucination Evaluation benchmark (CMHE). This benchmark can be used to evaluate LLMs' ability to detect medical hallucinations, make accurate diagnoses in noisy conditions, and provide plausible explanations. The creation of this benchmark involves a combination of manual and model-based approaches. In addition, we use ICD-10 as well as MeSH, two specialized glossaries, to aid in the evaluation. Our experiments show that the LLM struggles to identify fake medical terms and makes poor diagnoses in distracting environments. However, improving the model's understanding of medical concepts can help it resist interference to some extent",
    "checked": true,
    "id": "595128473114d6e94ba945d988fd647d68ca26d4",
    "semantic_title": "detection, diagnosis, and explanation: a benchmark for chinese medial hallucination evaluation",
    "citation_count": 0,
    "authors": [
      "Chengfeng Dou",
      "Ying Zhang",
      "Yanyuan Chen",
      "Zhi Jin",
      "Wenpin Jiao",
      "Haiyan Zhao",
      "Yu Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.429": {
    "title": "Developing a Benchmark for Pronunciation Feedback: Creation of a Phonemically Annotated Speech Corpus of isiZulu Language Learner Speech",
    "volume": "main",
    "abstract": "Pronunciation of the phonemic inventory of a new language often presents difficulties to second language (L2) learners. These challenges can be alleviated by the development of pronunciation feedback tools that take speech input from learners and return information about errors in the utterance. This paper presents the development of a corpus designed for use in pronunciation feedback research. The corpus is comprised of gold standard recordings from isiZulu teachers and recordings from isiZulu L2 learners that have been annotated for pronunciation errors. Exploring the potential benefits of word-level versus phoneme-level feedback necessitates a speech corpus that has been annotated for errors on the phoneme-level. To aid in this discussion, this corpus of isiZulu L2 speech has been annotated for phoneme-errors in utterances, as well as suprasegmental errors in tone",
    "checked": true,
    "id": "e7c3ad49eaf565db837e5a296e89fd6bdd25120a",
    "semantic_title": "developing a benchmark for pronunciation feedback: creation of a phonemically annotated speech corpus of isizulu language learner speech",
    "citation_count": 0,
    "authors": [
      "Alexandra O’Neil",
      "Nils Hjortnaes",
      "Francis Tyers",
      "Zinhle Nkosi",
      "Thulile Ndlovu",
      "Zanele Mlondo",
      "Ngami Phumzile Pewa"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.430": {
    "title": "Developing a Rhetorical Structure Theory Treebank for Czech",
    "volume": "main",
    "abstract": "We introduce the first version of the Czech RST Discourse Treebank, a collection of Czech journalistic texts manually annotated using the Rhetorical Structure Theory (RST), a global coherence model proposed by Mann and Thompson (1988). Each document in the corpus is represented as a single tree-like structure, where discourse units are interconnected through hierarchical rhetorical relations and their relative importance for the main purpose of a text is modeled by the nuclearity principle. The treebank is freely available in the LINDAT/CLARIAH-CZ repository under the Creative Commons license; for some documents, it includes two gold annotations representing divergent yet relevant interpretations. The paper outlines the annotation process, provides corpus statistics and evaluation, and discusses the issue of consistency associated with the global level of textual interpretation. In general, good agreement on the structure and labeling could be achieved on the lowest, local tree level and on the identification of the most central (nuclear) elementary discourse units. Disagreements mostly concerned segmentation and, in the structure, differences in the stepwise process of linking the largest text blocks. The project contributes to the advancement of RST research and its application to real-world text analysis challenges",
    "checked": true,
    "id": "969df25767d9e3db35975af36dbd74fd398d69eb",
    "semantic_title": "developing a rhetorical structure theory treebank for czech",
    "citation_count": 0,
    "authors": [
      "Lucie Polakova",
      "Jiří Mírovský",
      "Šárka Zikánová",
      "Eva Hajicova"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.431": {
    "title": "Development and Evaluation of Pre-trained Language Models for Historical Danish and Norwegian Literary Texts",
    "volume": "main",
    "abstract": "We develop and evaluate the first pre-trained language models specifically tailored for historical Danish and Norwegian texts. Three models are trained on a corpus of 19th-century Danish and Norwegian literature: two directly on the corpus with no prior pre-training, and one with continued pre-training. To evaluate the models, we utilize an existing sentiment classification dataset, and additionally introduce a new annotated word sense disambiguation dataset focusing on the concept of fate. Our assessment reveals that the model employing continued pre-training outperforms the others in two downstream NLP tasks on historical texts. Specifically, we observe substantial improvement in sentiment classification and word sense disambiguation compared to models trained on contemporary texts. These results highlight the effectiveness of continued pre-training for enhancing performance across various NLP tasks in historical text analysis",
    "checked": true,
    "id": "bd8b50b6faec00365bb53db970b4720abde1f35e",
    "semantic_title": "development and evaluation of pre-trained language models for historical danish and norwegian literary texts",
    "citation_count": 0,
    "authors": [
      "Ali Al-Laith",
      "Alexander Conroy",
      "Jens Bjerring-Hansen",
      "Daniel Hershcovich"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.432": {
    "title": "Development of Community-Oriented Text-to-Speech Models for Māori ‘Avaiki Nui (Cook Islands Māori)",
    "volume": "main",
    "abstract": "In this paper we describe the development of a text-to-speech system for Māori ‘Avaiki Nui (Cook Islands Māori). We provide details about the process of community-collaboration that was followed throughout the project, a continued engagement where we are trying to develop speech and language technology for the benefit of the community. During this process we gathered a group of recordings that we used to train a TTS system. When training we used two approaches, the HMM-system MaryTTS (Schröder et al., 2011) and the deep learning system FastSpeech2 (Ren et al., 2020). We performed two evaluation tasks on the models: First, we measured their quality by having the synthesized speech transcribed by ASR. The human produced ground truth had lower error rates (CER=4.3, WER=18), but the FastSpeech2 audio has lower error rates (CER=11.8 and WER=42.7) than the MaryTTS voice (CER=17.9 and WER=48.1). The second evaluation was a survey amongst speakers of the language so they could judge the voice's quality. The ground truth was rated with the highest quality (MOS=4.6), but the FastSpeech2 voice had an overall quality of MOS=3.2, which was significantly higher than that of the MaryTTS synthesized recordings (MOS=2.0). We intend to use the FastSpeech2 model to create language learning tools for community members both on the Cook Islands and in the diaspora",
    "checked": true,
    "id": "13fded59ce576e6cc449e1864babab5d6f13f275",
    "semantic_title": "development of community-oriented text-to-speech models for māori ‘avaiki nui (cook islands māori)",
    "citation_count": 0,
    "authors": [
      "Jesin James",
      "Rolando Coto-Solano",
      "Sally Akevai Nicholas",
      "Joshua Zhu",
      "Bovey Yu",
      "Fuki Babasaki",
      "Jenny Tyler Wang",
      "Nicholas Derby"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.433": {
    "title": "DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation",
    "volume": "main",
    "abstract": "The method of training language models based on domain datasets has obtained significant achievements in the task of generating scientific paper abstracts. However, such models face problems of generalization and expensive training costs. The use of large language models (LLMs) to solve the task of generating paper abstracts saves the cost of model training. However, due to the hallucination problem of LLM, it is often necessary to improve the reliability of the results through multi-round query prompt approach such as Graph of Thoughts (GoT), which also brings additional reasoning costs. In this paper, we propose a Dynamic Graph of Thought (DGoT). It not only inherits the advantages of the existing GoT prompt approach, but also dynamically adjust the graph structure according to data characteristics while reducing model reasoning cost. Experimental results show that our method's cost-effectiveness in abstract generation tasks is only 43.7% to 56.4% of other multi-round query prompt approaches. Our code is available at https://github.com/JayceNing/DGoT",
    "checked": true,
    "id": "3ec6428566b0e5340fe71abf23ffeae6bf03affb",
    "semantic_title": "dgot: dynamic graph of thoughts for scientific abstract generation",
    "citation_count": 0,
    "authors": [
      "Xinyu Ning",
      "Yutong Zhao",
      "Yitong Liu",
      "Hongwen Yang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.434": {
    "title": "DGS-Fabeln-1: A Multi-Angle Parallel Corpus of Fairy Tales between German Sign Language and German Text",
    "volume": "main",
    "abstract": "We present the acquisition process and the data of DGS-Fabeln-1, a parallel corpus of German text and videos containing German fairy tales interpreted into the German Sign Language (DGS) by a native DGS signer. The corpus contains 573 segments of videos with a total duration of 1 hour and 32 minutes, corresponding with 1428 written sentences. It is the first corpus of semi-naturally expressed DGS that has been filmed from 7 angles, and one of the few sign language (SL) corpora globally which have been filmed from more than 3 angles and where the listener has been simultaneously filmed. The corpus aims at aiding research at SL linguistics, SL machine translation and affective computing, and is freely available for research purposes at the following address: https://doi.org/10.5281/zenodo.10822097",
    "checked": true,
    "id": "649b86d31d8ae10cc973da90e1e672d09c174764",
    "semantic_title": "dgs-fabeln-1: a multi-angle parallel corpus of fairy tales between german sign language and german text",
    "citation_count": 0,
    "authors": [
      "Fabrizio Nunnari",
      "Eleftherios Avramidis",
      "Cristina España-Bonet",
      "Marco González",
      "Anna Hennes",
      "Patrick Gebhard"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.435": {
    "title": "Dialogue Systems Can Generate Appropriate Responses without the Use of Question Marks?– a Study of the Effects of \"?\" for Spoken Dialogue Systems –",
    "volume": "main",
    "abstract": "When individuals engage in spoken discourse, various phenomena can be observed that differ from those that are apparent in text-based conversation. While written communication commonly uses a question mark to denote a query, in spoken discourse, queries are frequently indicated by a rising intonation at the end of a sentence. However, numerous speech recognition engines do not append a question mark to recognized queries, presenting a challenge when creating a spoken dialogue system. Specifically, the absence of a question mark at the end of a sentence can impede the generation of appropriate responses to queries in spoken dialogue systems. Hence, we investigate the impact of question marks on dialogue systems, with the results showing that they have a significant impact. Moreover, we analyze specific examples in an effort to determine which types of utterances have the impact on dialogue systems",
    "checked": true,
    "id": "99487e8d23783ed91491753a78e069a9681199f0",
    "semantic_title": "dialogue systems can generate appropriate responses without the use of question marks?– a study of the effects of \"?\" for spoken dialogue systems –",
    "citation_count": 0,
    "authors": [
      "Tomoya Mizumoto",
      "Takato Yamazaki",
      "Katsumasa Yoshikawa",
      "Masaya Ohagi",
      "Toshiki Kawamoto",
      "Toshinori Sato"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.436": {
    "title": "DiaSet: An Annotated Dataset of Arabic Conversations",
    "volume": "main",
    "abstract": "We introduce DiaSet, a novel dataset of dialectical Arabic speech, manually transcribed and annotated for two specific downstream tasks: sentiment analysis and named entity recognition. The dataset encapsulates the Palestine dialect, predominantly spoken in Palestine, Israel, and Jordan. Our dataset incorporates authentic conversations between YouTube influencers and their respective guests. Furthermore, we have enriched the dataset with simulated conversations initiated by inviting participants from various locales within the said regions. The participants were encouraged to engage in dialogues with our interviewer. Overall, DiaSet consists of 644.8K tokens and 23.2K annotated instances. Uniform writing standards were upheld during the transcription process. Additionally, we established baseline models by leveraging some of the pre-existing Arabic BERT language models, showcasing the potential applications and efficiencies of our dataset. We make DiaSet publicly available for further research",
    "checked": true,
    "id": "98bb13244da8d0e96458061a398ff6439b8f20e3",
    "semantic_title": "diaset: an annotated dataset of arabic conversations",
    "citation_count": 0,
    "authors": [
      "Abraham Israeli",
      "Aviv Naaman",
      "Guy Maduel",
      "Rawaa Makhoul",
      "Dana Qaraeen",
      "Amir Ejmail",
      "Dina Lisnanskey",
      "Julian Jubran",
      "Shai Fine",
      "Kfir Bar"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.437": {
    "title": "Did You Get It? A Zero-Shot Approach to Locate Information Transfers in Conversations",
    "volume": "main",
    "abstract": "Interaction theories suggest that the emergence of mutual understanding between speakers in natural conversations depends on the construction of a shared knowledge base (common ground), but the details of which information and the circumstances under which it is memorized are not explained by any model. Previous works have looked at metrics derived from Information Theory to quantify the dynamics of information exchanged between participants, but do not provide an efficient way to locate information that will enter the common ground. We propose a new method based on the segmentation of a conversation into themes followed by their summarization. We then obtain the location of information transfers by computing the distance between the theme summary and the different utterances produced by a speaker. We evaluate two Large Language Models (LLMs) on this pipeline, on the French conversational corpus Paco-Cheese. More generally, we explore how the recent developments in the field of LLMs provide us with the means to implement these new methods and more generally support research into questions that usually heavily relies on human annotators",
    "checked": true,
    "id": "02dabb78dc5569d65eb41b34a6d5fbe7080eec03",
    "semantic_title": "did you get it? a zero-shot approach to locate information transfers in conversations",
    "citation_count": 0,
    "authors": [
      "Eliot Maës",
      "Hossam Boudraa",
      "Philippe Blache",
      "Leonor Becerra-Bonache"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.438": {
    "title": "Difficulty-Focused Contrastive Learning for Knowledge Tracing with a Large Language Model-Based Difficulty Prediction",
    "volume": "main",
    "abstract": "This paper presents novel techniques for enhancing the performance of knowledge tracing (KT) models by focusing on the crucial factor of question and concept difficulty level. Despite the acknowledged significance of difficulty, previous KT research has yet to exploit its potential for model optimization and has struggled to predict difficulty from unseen data. To address these problems, we propose a difficulty-centered contrastive learning method for KT models and a Large Language Model (LLM)-based framework for difficulty prediction. These innovative methods seek to improve the performance of KT models and provide accurate difficulty estimates for unseen data. Our ablation study demonstrates the efficacy of these techniques by demonstrating enhanced KT model performance. Nonetheless, the complex relationship between language and difficulty merits further investigation",
    "checked": true,
    "id": "3a7c0416072824a00c9c89358c91ad8ef779f27d",
    "semantic_title": "difficulty-focused contrastive learning for knowledge tracing with a large language model-based difficulty prediction",
    "citation_count": 0,
    "authors": [
      "Unggi Lee",
      "Sungjun Yoon",
      "Joon Seo Yun",
      "Kyoungsoo Park",
      "YoungHoon Jung",
      "Damji Stratton",
      "Hyeoncheol Kim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.439": {
    "title": "Diffusion Based Counterfactual Augmentation for Dual Sentiment Classification",
    "volume": "main",
    "abstract": "State-of-the-art NLP models have demonstrated exceptional performance across various tasks, including sentiment analysis. However, concerns have been raised about their robustness and susceptibility to systematic biases in both training and test data, which may lead to performance challenges when these models encounter out-of-distribution data in real-world applications. Although various data augmentation and adversarial perturbation techniques have shown promise in tackling these issues, prior methods such as word embedding perturbation or synonymous sentence expansion have failed to mitigate the spurious association problem inherent in the original data. Recent counterfactual augmentation methods have attempted to tackle this issue, but they have been limited by rigid rules, resulting in inconsistent context and disrupted semantics. In response to these challenges, we introduce a diffusion-based counterfactual data augmentation (DCA) framework. It utilizes an antonymous paradigm to guide the continuous diffusion model and employs reinforcement learning in combination with contrastive learning to optimize algorithms for generating counterfactual samples with high diversity and quality. Furthermore, we use a dual sentiment classifier to validate the generated antonymous samples and subsequently perform sentiment classification. Our experiments on four benchmark datasets demonstrate that DCA achieves state-of-the-art performance in sentiment classification tasks",
    "checked": true,
    "id": "c07f73b66282f19b9a7259eb0728c3f59044936a",
    "semantic_title": "diffusion based counterfactual augmentation for dual sentiment classification",
    "citation_count": 0,
    "authors": [
      "Dancheng Xin",
      "Jiawei Yuan",
      "Yang Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.440": {
    "title": "DiffusionDialog: A Diffusion Model for Diverse Dialog Generation with Latent Space",
    "volume": "main",
    "abstract": "In real-life conversations, the content is diverse, and there exist one-to-many problems that require diverse generation. Previous studies attempted to introduce discrete or Gaussian-based latent variables to address the one-to-many problem, but the diversity is limited. Recently, diffusion models have made breakthroughs in computer vision and some attempts have been made in natural language processing. In this paper, we propose DiffusionDialog, a novel approach to enhance the diversity of dialogue generation with the help of diffusion model. In our approach, we introduce the continuous latent variables in the diffusion model instead of the discrete ones or VAE, which are often used in the previous studies. The problem of using discrete variables in dialog task is how to build a effective prior of latent space and inferring process to infer the proper latent given the context. Combining the encoder and latent-based diffusion model, we encode the latent of response in a continuous space as the prior instead of fixed Gaussian distribution in VAE or simply discrete ones, and we infer the latent by denoising step by step with diffusion model. The experimental results show that our model greatly enhance the diversity of dialog response while keeping the coherence. In further analysis, we find that our diffusion model achieved high inference efficiency which is the main challenge of applying diffusion model in natural language processing",
    "checked": true,
    "id": "0cc73bd4dbb9b899ae93fef6149051aa717be9e8",
    "semantic_title": "diffusiondialog: a diffusion model for diverse dialog generation with latent space",
    "citation_count": 0,
    "authors": [
      "Jianxiang Xiang",
      "Zhenhua Liu",
      "Haodong Liu",
      "Yin Bai",
      "Jia Cheng",
      "Wenliang Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.441": {
    "title": "DimA: A Parameter-efficient Fine-tuning Method with Knowledge Transfer Based on Transformer",
    "volume": "main",
    "abstract": "Fine-tuning is a widely used technique for leveraging pre-trained language models (PLMs) in downstream tasks, but it can be computationally expensive and storage-intensive. To address this challenge, researchers have developed parameter-efficient methods that balance performance and resource cost. However, these methods often come with trade-offs like increased inference latency, token length usage, or limited adaptability for multitasking scenarios. This paper introduces a novel parameter-efficient method called DimA(Dimensionality Augmentation), which enhances the Transformer architecture by increasing the dimensionality. DimA achieves state-of-the-art results in GLUE and XSUM tasks while utilizing less than 1% of the original model's parameters. Moreover, DimA introduces a novel approach to knowledge transfer that enables the simultaneous utilization of knowledge learned from multiple tasks to handle new tasks. This method significantly enhances the performance of the model on new tasks. Its versatility in model structure also enables its application to various Transformer-based models",
    "checked": true,
    "id": "237ce99e0893e5cb9adf0e9d4609ab0d7a87678a",
    "semantic_title": "dima: a parameter-efficient fine-tuning method with knowledge transfer based on transformer",
    "citation_count": 0,
    "authors": [
      "Wenxuan Zhang",
      "Min Huang",
      "Zhuoyang Song",
      "Qinghai Miao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.442": {
    "title": "Disambiguating Homographs and Homophones Simultaneously: A Regrouping Method for Japanese",
    "volume": "main",
    "abstract": "We present a method that re-groups surface forms into clusters representing synonyms, and help disambiguate homographs as well as homophone. The method is applied post-hoc to trained contextual word embeddings. It is beneficial to languages where both homographs and homophones abound, which compromise the efficiency of language model and causes the underestimation problem in evaluation. Taking Japanese as an example, we evaluate how accurate such disambiguation can be, and how much the underestimation can be mitigated",
    "checked": true,
    "id": "1add0b341daf4aa01189a84d8b83bdc759c20fec",
    "semantic_title": "disambiguating homographs and homophones simultaneously: a regrouping method for japanese",
    "citation_count": 0,
    "authors": [
      "Yo Sato"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.443": {
    "title": "DiscoGeM 2.0: A Parallel Corpus of English, German, French and Czech Implicit Discourse Relations",
    "volume": "main",
    "abstract": "We present DiscoGeM 2.0, a crowdsourced, parallel corpus of 12,834 implicit discourse relations, with English, German, French and Czech data. We propose and validate a new single-step crowdsourcing annotation method and apply it to collect new annotations in German, French and Czech. The corpus was constructed by having crowdsourced annotators choose a suitable discourse connective for each relation from a set of unambiguous candidates. Every instance was annotated by 10 workers. Our corpus hence represents the first multi-lingual resource that contains distributions of discourse interpretations for implicit relations. The results show that the connective insertion method of discourse annotation can be reliably extended to other languages. The resulting multi-lingual annotations also reveal that implicit relations inferred in one language may differ from those inferred in the translation, meaning the annotations are not always directly transferable. DiscoGem 2.0 promotes the investigation of cross-linguistic differences in discourse marking and could improve automatic discourse parsing applications. It is openly downloadable here: https://github.com/merelscholman/DiscoGeM",
    "checked": true,
    "id": "31417959f27eaf572e6413fe0a3c4e5213638238",
    "semantic_title": "discogem 2.0: a parallel corpus of english, german, french and czech implicit discourse relations",
    "citation_count": 0,
    "authors": [
      "Frances Yung",
      "Merel Scholman",
      "Sarka Zikanova",
      "Vera Demberg"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.444": {
    "title": "Discourse Structure for the Minecraft Corpus",
    "volume": "main",
    "abstract": "We provide a new linguistic resource: The Minecraft Structured Dialogue Corpus (MSDC), a discourse annotated version of the Minecraft Dialogue Corpus (MDC; Narayan-Chen et al., 2019), with complete, situated discourse structures in the style of SDRT (Asher and Lascarides, 2003). Our structures feature both linguistic discourse moves and nonlinguistic actions. To show computational tractability, we train a discourse parser with a novel \"2 pass architecture\" on MSDC that gives excellent results on attachment prediction and relation labeling tasks especially long distance attachments",
    "checked": true,
    "id": "6c329c81b2655f8e3e40a194e348d76fd14d2b52",
    "semantic_title": "discourse structure for the minecraft corpus",
    "citation_count": 0,
    "authors": [
      "Kate Thompson",
      "Julie Hunter",
      "Nicholas Asher"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.445": {
    "title": "Discriminative Language Model as Semantic Consistency Scorer for Prompt-based Few-Shot Text Classification",
    "volume": "main",
    "abstract": "A successful prompt-based finetuning method should have three prerequisites: task compatibility, input compatibility, and evidence abundance. Bearing this belief in mind, this paper designs a novel prompt-based method (called DLM-SCS) for few-shot text classification, which utilizes the discriminative language model ELECTRA that is pretrained to distinguish whether a token is original or replaced. The method is built upon the intuitive idea that the prompt instantiated with the true label should have higher semantic consistency score than other prompts with false labels. Since a prompt usually consists of several components (or parts), its semantic consistency can be decomposed accordingly, which means each part can provide information for semantic consistency discrimination. The semantic consistency of each component is then computed by making use of the pretrained ELECTRA model, where no extra parameters get introduced. Extensive experiments have shown that our model outperforms several state-of-the-art prompt-based few-shot methods on 10 widely-used text classification tasks",
    "checked": true,
    "id": "6e592680f2aedac64f9f0fd54d16a2b08d6e1b5e",
    "semantic_title": "discriminative language model as semantic consistency scorer for prompt-based few-shot text classification",
    "citation_count": 0,
    "authors": [
      "Zhipeng Xie",
      "Yahe Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.446": {
    "title": "Disentangling Pretrained Representation to Leverage Low-Resource Languages in Multilingual Machine Translation",
    "volume": "main",
    "abstract": "Multilingual neural machine translation aims to encapsulate multiple languages into a single model. However, it requires an enormous dataset, leaving the low-resource language (LRL) underdeveloped. As LRLs may benefit from shared knowledge of multilingual representation, we aspire to find effective ways to integrate unseen languages in a pre-trained model. Nevertheless, the intricacy of shared representation among languages hinders its full utilisation. To resolve this problem, we employed target language prediction and a central language-aware layer to improve representation in integrating LRLs. Focusing on improving LRLs in the linguistically diverse country of Indonesia, we evaluated five languages using a parallel corpus of 1,000 instances each, with experimental results measured by BLEU showing zero-shot improvement of 7.4 from the baseline score of 7.1 to a score of 15.5 at best. Further analysis showed that the gains in performance are attributed more to the disentanglement of multilingual representation in the encoder with the shift of the target language-specific representation in the decoder",
    "checked": true,
    "id": "5f2de1bd6fa1b6b0077a4777263698c659992349",
    "semantic_title": "disentangling pretrained representation to leverage low-resource languages in multilingual machine translation",
    "citation_count": 0,
    "authors": [
      "Frederikus Hudi",
      "Zhi Qu",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.447": {
    "title": "DISRPT: A Multilingual, Multi-domain, Cross-framework Benchmark for Discourse Processing",
    "volume": "main",
    "abstract": "This paper presents DISRPT, a multilingual, multi-domain, and cross-framework benchmark dataset for discourse processing, covering the tasks of discourse unit segmentation, connective identification, and relation classification. DISRPT includes 13 languages, with data from 24 corpora covering about 4 millions tokens and around 250,000 discourse relation instances from 4 discourse frameworks: RST, SDRT, PDTB, and Discourse Dependencies. We present an overview of the data, its development across three NLP shared tasks on discourse processing carried out in the past five years, and the latest modifications and added extensions. We also carry out an evaluation of state-of-the-art multilingual systems trained on the data for each task, showing plateau performance on segmentation, but important room for improvement for connective identification and relation classification. The DISRPT benchmark employs a unified format that we make available on GitHub and HuggingFace in order to encourage future work on discourse processing across languages, domains, and frameworks",
    "checked": true,
    "id": "a4dfbf35eafb47606374ac71e104b66610aa1cd8",
    "semantic_title": "disrpt: a multilingual, multi-domain, cross-framework benchmark for discourse processing",
    "citation_count": 0,
    "authors": [
      "Chloé Braud",
      "Amir Zeldes",
      "Laura Rivière",
      "Yang Janet Liu",
      "Philippe Muller",
      "Damien Sileo",
      "Tatsuya Aoyama"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.448": {
    "title": "Distantly Supervised Contrastive Learning for Low-Resource Scripting Language Summarization",
    "volume": "main",
    "abstract": "Code summarization provides a natural language description for a given piece of code. In this work, we focus on scripting code—programming languages that interact with specific devices through commands. The low-resource nature of scripting languages makes traditional code summarization methods challenging to apply. To address this, we introduce a novel framework: distantly supervised contrastive learning for low-resource scripting language summarization. This framework leverages limited atomic commands and category constraints to enhance code representations. Extensive experiments demonstrate our method's superiority over competitive baselines",
    "checked": true,
    "id": "dca3c858f30340832a77a9c526b16c103cbd0c51",
    "semantic_title": "distantly supervised contrastive learning for low-resource scripting language summarization",
    "citation_count": 0,
    "authors": [
      "Junzhe Liang",
      "Haifeng Sun",
      "Zirui Zhuang",
      "Qi Qi",
      "Jingyu Wang",
      "Jianxin Liao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.449": {
    "title": "Distillation with Explanations from Large Language Models",
    "volume": "main",
    "abstract": "Free-text explanations are crucial for enhancing the interpretability of AI models. However, training models to generate high-quality free-text explanations is challenging, primarily due to the requirement of a substantial amount of human-written explanations, which can be expensive. Recently, Large language models (LLMs) like ChatGPT and GPT-4 have made remarkable progress in various NLP tasks while also providing explanations alongside their answers. Leveraging LLMs for data labeling offers a more cost-effective alternative. However, a key concern arises from the fact that the answers provided by LLMs are not entirely accurate, potentially introducing noise to both task outputs and explanation generation. To remedy this, we propose a new mechanism, Distillation with Explanations from LLMs. we observe that despite the incorrectness in LLMs-generated answers, their explanations are consistent with their answers. Leveraging this consistency, our method combines the ground truth labels and answers-explanations generated by LLMs, to simultaneously generate more accurate answers and the corresponding free-text explanations. Experimental results demonstrate that our approach achieves improved predictive performance and also generates explanations that exhibit greater alignment with the model's task outputs",
    "checked": true,
    "id": "44c8e4fc0aaf749f73f4467fa4c3283ed714faf0",
    "semantic_title": "distillation with explanations from large language models",
    "citation_count": 0,
    "authors": [
      "Hanyu Zhang",
      "Xiting Wang",
      "Xiang Ao",
      "Qing He"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.450": {
    "title": "Distill, Fuse, Pre-train: Towards Effective Event Causality Identification with Commonsense-Aware Pre-trained Model",
    "volume": "main",
    "abstract": "Event Causality Identification (ECI) aims to detect causal relations between events in unstructured texts. This task is challenged by the lack of data and explicit causal clues. Some methods incorporate explicit knowledge from external knowledge graphs (KGs) into Pre-trained Language Models (PLMs) to tackle these issues, achieving certain accomplishments. However, they ignore that existing KGs usually contain trivial knowledge which may prejudice the performance. Moreover, they simply integrate the concept triplets, underutilizing the deep interaction between the text and external graph. In this paper, we propose an effective pipeline DFP, i.e., Distill, Fuse and Pre-train, to build a commonsense-aware pre-trained model which integrates reliable task-specific knowledge from commonsense graphs. This pipeline works as follows: (1) To leverage the reliable knowledge, commonsense graph distillation is proposed to distill commonsense graphs and obtain the meta-graph which contain credible task-oriented knowledge. (2) To model the deep interaction between the text and external graph, heterogeneous information fusion is proposed to fuse them through a commonsense-aware memory network. (3) Continual pre-training designs three continual pre-training tasks to further align and fuse the text and the commonsense meta-graph. Through extensive experiments on two benchmarks, we demonstrate the validity of our pipeline",
    "checked": true,
    "id": "c2523cf073563f2fa9e7c8a546e9c2f6e01a8c43",
    "semantic_title": "distill, fuse, pre-train: towards effective event causality identification with commonsense-aware pre-trained model",
    "citation_count": 0,
    "authors": [
      "Peixin Huang",
      "Xiang Zhao",
      "Minghao Hu",
      "Zhen Tan",
      "Weidong Xiao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.451": {
    "title": "Distilling Causal Effect of Data in Continual Few-shot Relation Learning",
    "volume": "main",
    "abstract": "Continual Few-Shot Relation Learning (CFRL) aims to learn an increasing number of new relational patterns from a data stream. However, due to the limited number of samples and the continual training mode, this method frequently encounters the catastrophic forgetting issues. The research on causal inference suggests that this issue is caused by the loss of causal effects from old data during the new training process. Inspired by the causal graph, we propose a unified causal framework for CFRL to restore the causal effects. Specifically, we establish two additional causal paths from old data to predictions by having the new data and memory data collide with old data separately in the old feature space. This augmentation allows us to preserve causal effects effectively and enhance the utilization of valuable information within memory data, thereby alleviating the phenomenon of catastrophic forgetting. Furthermore, we introduce a self-adaptive weight to achieve a delicate balance of causal effects between the new and old relation types. Extensive experiments demonstrate the superiority of our method over existing state-of-the-art approaches in CFRL task settings. Our codes are publicly available at: https://github.com/ywh140/CECF",
    "checked": true,
    "id": "73aef3154a45bb90195a15fd7eadb9b4a1ad6907",
    "semantic_title": "distilling causal effect of data in continual few-shot relation learning",
    "citation_count": 0,
    "authors": [
      "Weihang Ye",
      "Peng Zhang",
      "Jing Zhang",
      "Hui Gao",
      "Moyao Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.452": {
    "title": "Distractor Generation Using Generative and Discriminative Capabilities of Transformer-based Models",
    "volume": "main",
    "abstract": "Multiple Choice Questions (MCQs) are very common in both high-stakes and low-stakes examinations, and their effectiveness in assessing students relies on the quality and diversity of distractors, which are the incorrect answer options provided alongside the correct answer. Motivated by the progress in generative language models, we propose a two-step automatic distractor generation approach which is based on text to text transfer transformer models. Unlike most of previous methods for distractor generation, our approach does not rely on the correct answer options. Instead, it first generates both correct and incorrect answer options, and then discriminates potential correct options from distractors. Identified distractors are finally categorised based on semantic similarity scores into separate clusters, and the cluster heads are selected as our final distinct distractors. Experiments on two publicly available datasets show that our approach outperforms previous models both in the case of single-word answer options and longer-sequence reading comprehension questions",
    "checked": true,
    "id": "d61c96f1ee50496cff100f74cec20dae658646b1",
    "semantic_title": "distractor generation using generative and discriminative capabilities of transformer-based models",
    "citation_count": 0,
    "authors": [
      "Shiva Taslimipoor",
      "Luca Benedetto",
      "Mariano Felice",
      "Paula Buttery"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.453": {
    "title": "Distribution Aware Metrics for Conditional Natural Language Generation",
    "volume": "main",
    "abstract": "Traditional automated metrics for evaluating conditional natural language generation rely on pairwise comparisons between a single generated text and the best-matching gold-standard reference. This method is effective when ground truth data diversity can be attributed to noise, however, it falls short when diversity in references holds valuable contextual information, as in visual description or summarization, as it does not evaluate the ability of a model to generate text matching the diversity of the ground truth samples. In this paper, we challenge the adequacy of existing metrics in such semantically diverse contexts and introduce a novel approach for evaluating conditional language generation models, leveraging a family of meta-metrics that build on existing pairwise distance functions. These meta-metrics assess not just single-samples, but distributions of reference and model-generated captions using small sample sets. We demonstrate our approach through a case study of visual description in the English language which reveals not only how current models prioritize single-description quality over diversity, but further sheds light on the impact of sampling methods and temperature settings on description quality and diversity",
    "checked": true,
    "id": "04a55a02059d949a7667c4028ad1cd1ee3e5444d",
    "semantic_title": "distribution aware metrics for conditional natural language generation",
    "citation_count": 3,
    "authors": [
      "David M. Chan",
      "Yiming Ni",
      "David Ross",
      "Sudheendra Vijayanarasimhan",
      "Austin Myers",
      "John Canny"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.454": {
    "title": "Diversifying Question Generation over Knowledge Base via External Natural Questions",
    "volume": "main",
    "abstract": "Previous methods on knowledge base question generation (KBQG) primarily focus on refining the quality of a single generated question. However, considering the remarkable paraphrasing ability of humans, we believe that diverse texts can express identical semantics through varied expressions. The above insights make diversifying question generation an intriguing task, where the first challenge is evaluation metrics for diversity. Current metrics inadequately assess the aforementioned diversity. They calculate the ratio of unique n-grams in the generated question, which tends to measure duplication rather than true diversity. Accordingly, we devise a new diversity evaluation metric, which measures the diversity among top-k generated questions for each instance while ensuring their relevance to the ground truth. Clearly, the second challenge is how to enhance diversifying question generation. To address this challenge, we introduce a dual model framework interwoven by two selection strategies to generate diverse questions leveraging external natural questions. The main idea of our dual framework is to extract more diverse expressions and integrate them into the generation model to enhance diversifying question generation. Extensive experiments on widely used benchmarks for KBQG show that our approach can outperform pre-trained language model baselines and text-davinci-003 in diversity while achieving comparable performance with ChatGPT",
    "checked": true,
    "id": "eccd2e7771ee275f5f8ae143a5c279a2d629adc5",
    "semantic_title": "diversifying question generation over knowledge base via external natural questions",
    "citation_count": 2,
    "authors": [
      "Shasha Guo",
      "Jing Zhang",
      "Xirui Ke",
      "Cuiping Li",
      "Hong Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.455": {
    "title": "DMON: A Simple Yet Effective Approach for Argument Structure Learning",
    "volume": "main",
    "abstract": "Argument structure learning (ASL) entails predicting relations between arguments. Because it can structure a document to facilitate its understanding, it has been widely applied in many fields (medical, commercial, and scientific domains). Despite its broad utilization, ASL remains a challenging task because it involves examining the complex relationships between the sentences in a potentially unstructured discourse. To resolve this problem, we have developed a simple yet effective approach called Dual-tower Multi-scale cOnvolution neural Network (DMON) for the ASL task. Specifically, we organize arguments into a relationship matrix that together with the argument embeddings forms a relationship tensor and design a mechanism to capture relations with contextual arguments. Experimental results on three different-domain argument mining datasets demonstrate that our framework outperforms state-of-the-art models. We will release the code after paper acceptance",
    "checked": true,
    "id": "a846f14341ace616a634374682e79055741448c9",
    "semantic_title": "dmon: a simple yet effective approach for argument structure learning",
    "citation_count": 0,
    "authors": [
      "Sun Wei",
      "Mingxiao Li",
      "Jingyuan Sun",
      "Jesse Davis",
      "Marie-Francine Moens"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.456": {
    "title": "Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents via Semantic-Oriented Hierarchical Graphs",
    "volume": "main",
    "abstract": "Table-text document (e.g., financial reports) understanding has attracted increasing attention in recent two years. TAT-DQA is a realistic setting for the understanding of visually-rich table-text documents, which involves answering associated questions requiring discrete reasoning. Most existing work relies on token-level semantics, falling short in the reasoning across document elements such as quantities and dates. To address this limitation, we propose a novel Doc2SoarGraph model that exploits element-level semantics and employs Semantic-oriented hierarchical Graph structures to capture the differences and correlations among different elements within the given document and question. Extensive experiments on the TAT-DQA dataset reveal that our model surpasses the state-of-the-art conventional method (i.e., MHST) and large language model (i.e., ChatGPT) by 17.73 and 6.49 points respectively in terms of Exact Match (EM) metric, demonstrating exceptional effectiveness",
    "checked": true,
    "id": "0ed565e9c2ddb80e3d6cc54c921e08f95e569eb0",
    "semantic_title": "doc2soargraph: discrete reasoning over visually-rich table-text documents via semantic-oriented hierarchical graphs",
    "citation_count": 1,
    "authors": [
      "Fengbin Zhu",
      "Chao Wang",
      "Fuli Feng",
      "Zifeng Ren",
      "Moxin Li",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.457": {
    "title": "DOC-RAG: ASR Language Model Personalization with Domain-Distributed Co-occurrence Retrieval Augmentation",
    "volume": "main",
    "abstract": "We propose DOC-RAG - Domain-distributed Co-occurrence Retrieval Augmentation for ASR language model personalization aiming to improve the automatic speech recognition of rare word patterns in unseen domains. Our approach involves contrastively training a document retrieval module to rank external knowledge domains based on their semantic similarity with respect to the input query. We further use n-gram co-occurrence distribution to recognize rare word patterns associated with specific domains. We aggregate the next word probability distribution based on the relative importance of different domains. Extensive experiments on three user-specific speech-to-text tasks for meetings, TED talks, and financial earnings calls show that DOC-RAG significantly outperforms strong baselines with an 8-15% improvement in terms of perplexity and a 4-7% reduction in terms of Word Error Rates in various settings",
    "checked": true,
    "id": "db493c98fc87a53f8e74d54f18fb834611af1218",
    "semantic_title": "doc-rag: asr language model personalization with domain-distributed co-occurrence retrieval augmentation",
    "citation_count": 0,
    "authors": [
      "Puneet Mathur",
      "Zhe Liu",
      "Ke Li",
      "Yingyi Ma",
      "Gil Karen",
      "Zeeshan Ahmed",
      "Dinesh Manocha",
      "Xuedong Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.458": {
    "title": "DocScript: Document-level Script Event Prediction",
    "volume": "main",
    "abstract": "We present a novel task of document-level script event prediction, which aims to predict the next event given a candidate list of narrative events in long-form documents. To enable this, we introduce DocSEP, a challenging dataset in two new domains - contractual documents and Wikipedia articles, where timeline events may be paragraphs apart and may require multi-hop temporal and causal reasoning. We benchmark existing baselines and present a novel architecture called DocScript to learn sequential ordering between events at the document scale. Our experimental results on the DocSEP dataset demonstrate that learning longer-range dependencies between events is a key challenge and show that contemporary LLMs such as ChatGPT and FlanT5 struggle to solve this task, indicating their lack of reasoning abilities for understanding causal relationships and temporal sequences within long texts",
    "checked": true,
    "id": "b3795d73cadfacfb6aa0f723978915012070a3d5",
    "semantic_title": "docscript: document-level script event prediction",
    "citation_count": 0,
    "authors": [
      "Puneet Mathur",
      "Vlad I. Morariu",
      "Aparna Garimella",
      "Franck Dernoncourt",
      "Jiuxiang Gu",
      "Ramit Sawhney",
      "Preslav Nakov",
      "Dinesh Manocha",
      "Rajiv Jain"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.459": {
    "title": "Document-Level Event Extraction via Information Interaction Based on Event Relation and Argument Correlation",
    "volume": "main",
    "abstract": "Document-level Event Extraction (DEE) is a vital task in NLP as it seeks to automatically recognize and extract event information from a document. However, current approaches often overlook intricate relationships among events and subtle correlations among arguments within a document, which can significantly impact the effectiveness of event type recognition and the extraction of cross-sentence arguments in DEE task. This paper proposes a novel Correlation Association Interactive Network (CAINet), comprising two key components: event relationship graph and argument correlation graph. In particular, the event relationship graph models the relationship among various events through structural associations among event nodes and sentence nodes, to improve the accuracy of event recognition. On the other hand, the arguments correlation graph models the correlations among arguments by quantifying the strength of association among arguments, to effectively aggregate cross-sentence arguments, contributing to the overall success of DEE. Furthermore, we use the large language model to execute DEE task experiments. Experimental results show the proposed CAINet outperforms existing state-of-the-art models and large language models in terms of F1-score across two benchmark datasets",
    "checked": true,
    "id": "c3461c18f9ab3c99b8d0c120a605a01da87fbc42",
    "semantic_title": "document-level event extraction via information interaction based on event relation and argument correlation",
    "citation_count": 0,
    "authors": [
      "Bangze Pan",
      "Yang Li",
      "Suge Wang",
      "Xiaoli Li",
      "Deyu Li",
      "Jian Liao",
      "Jianxing Zheng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.460": {
    "title": "Document Set Expansion with Positive-Unlabeled Learning Using Intractable Density Estimation",
    "volume": "main",
    "abstract": "The Document Set Expansion (DSE) task involves identifying relevant documents from large collections based on a limited set of example documents. Previous research has highlighted Positive and Unlabeled (PU) learning as a promising approach for this task. However, most PU methods rely on the unrealistic assumption of knowing the class prior for positive samples in the collection. To address this limitation, this paper introduces a novel PU learning framework that utilizes intractable density estimation models. Experiments conducted on PubMed and Covid datasets in a transductive setting showcase the effectiveness of the proposed method for DSE. Code is available from https://github.com/Beautifuldog01/Document-set-expansion-puDE",
    "checked": true,
    "id": "24037ed50294d5f2f62df56b640589233f4367ee",
    "semantic_title": "document set expansion with positive-unlabeled learning using intractable density estimation",
    "citation_count": 0,
    "authors": [
      "Haiyang Zhang",
      "Qiuyi Chen",
      "Yanjie Zou",
      "Jia Wang",
      "Yushan Pan",
      "Mark Stevenson"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.461": {
    "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
    "volume": "main",
    "abstract": "Despite the superior performance, Large Language Models (LLMs) require significant computational resources for deployment and use. To overcome this issue, quantization methods have been widely applied to reduce the memory footprint of LLMs as well as increase the inference rate. However, a major challenge is that low-bit quantization methods often lead to performance degradation. It is important to understand how quantization impacts the capacity of LLMs. Different from previous studies focused on overall performance, this work aims to investigate the impact of quantization on emergent abilities, which are important characteristics that distinguish LLMs from small language models. Specifically, we examine the abilities of in-context learning, chain-of-thought reasoning, and instruction-following in quantized LLMs. Our empirical experiments show that these emergent abilities still exist in 4-bit quantization models, while 2-bit models encounter severe performance degradation on the test of these abilities. To improve the performance of low-bit models, we conduct two special experiments: (1) fine-gained impact analysis that studies which components (or substructures) are more sensitive to quantization, and (2) performance compensation through model fine-tuning. Our work derives a series of important findings to understand the impact of quantization on emergent abilities and sheds light on the possibilities of extremely low-bit quantization for LLMs",
    "checked": true,
    "id": "c48fc69b62c749e78928d6a3bae98ffe278f761a",
    "semantic_title": "do emergent abilities exist in quantized large language models: an empirical study",
    "citation_count": 17,
    "authors": [
      "Peiyu Liu",
      "Zikang Liu",
      "Ze-Feng Gao",
      "Dawei Gao",
      "Wayne Xin Zhao",
      "Yaliang Li",
      "Bolin Ding",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.462": {
    "title": "Does ChatGPT Know That It Does Not Know? Evaluating the Black-Box Calibration of ChatGPT",
    "volume": "main",
    "abstract": "Recently, ChatGPT has demonstrated remarkable performance in various downstream tasks such as open-domain question answering, machine translation, and code generation. As a general-purpose task solver, an intriguing inquiry arises: Does ChatGPT itself know that it does not know, without any access to internal states? In response to this query, we present an initial evaluation of ChatGPT for black-box calibration. We designed three types of proxy confidence, from three perspectives to assess its performance. Experiments are conducted on five datasets, spanning four tasks, and the results show that ChatGPT has a degree of capability for black-box calibration. Specifically, proxy confidence displayed a significantly positive Pearson correlation (95.16%) with accuracy in the TruthfulQA dataset, while revealing a negative correlation in the ModAr dataset. We delved deeper into ChatGPT's black-box calibration ability by examining failure cases in the ModAr dataset. Our analysis revealed that ChatGPT's tendency to exhibit overconfidence may stem from its reliance on semantic priors. Furthermore, we investigated why ChatGPT performs relatively well in TruthfulQA. The findings suggest that ChatGPT might implicitly acquire calibration skills during the reinforcement learning process, rather than relying solely on simplistic heuristics",
    "checked": true,
    "id": "34dfd99128381fd3e182014b906063cc9532706a",
    "semantic_title": "does chatgpt know that it does not know? evaluating the black-box calibration of chatgpt",
    "citation_count": 0,
    "authors": [
      "Youliang Yuan",
      "Wenxuan Wang",
      "Qingshuo Guo",
      "Yiming Xiong",
      "Chihao Shen",
      "Pinjia He"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.463": {
    "title": "Does the Generator Mind Its Contexts? An Analysis of Generative Model Faithfulness under Context Transfer",
    "volume": "main",
    "abstract": "he present study introduces the knowledge-augmented generator, which is specifically designed to produce information that remains grounded in contextual knowledge, regardless of alterations in the context. Previous research has predominantly focused on examining hallucinations stemming from static input, such as in the domains of summarization or machine translation. However, our investigation delves into the faithfulness of generative question answering in the presence of dynamic knowledge. Our objective is to explore the existence of hallucinations arising from parametric memory when contextual knowledge undergoes changes, while also analyzing the underlying causes for their occurrence. In order to efficiently address this issue, we propose a straightforward yet effective measure for detecting such hallucinations. Intriguingly, our investigation uncovers that all models exhibit a tendency to generate previous answers as hallucinations. To gain deeper insights into the underlying causes of this phenomenon, we conduct a series of experiments that verify the critical role played by context in hallucination, both during training and testing, from various perspectives",
    "checked": true,
    "id": "56c89d66d5490fd4e04f06bc479b8c1a13b1f518",
    "semantic_title": "does the generator mind its contexts? an analysis of generative model faithfulness under context transfer",
    "citation_count": 0,
    "authors": [
      "Xinshuo Hu",
      "Dongfang Li",
      "Xiaoguang Li",
      "Yuxiang Wu",
      "Lifeng Shang",
      "Baotian Hu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.464": {
    "title": "Does the Language Matter? Curriculum Learning over Neo-Latin Languages",
    "volume": "main",
    "abstract": "Curriculum Learning (CL) is emerging as a relevant technique to reduce the cost of pre-training Large Language Models. The idea, tested for the English language, is to train LLMs by organizing training examples from the simplest to the most complex. Complexity measures may depend on the specific language. Hence, this paper aims to investigate whether CL and the complexity measure can be easily exported to other languages. For this reason, we present a set of linguistically motivated measures to determine the complexity of examples, which has been used in English: these measures are based on text length, rarity, and comprehensibility. We then test the approach to two Romance languages: Italian and French. Our results show that the technique can be easily exported to languages other than English without adaptation",
    "checked": true,
    "id": "b0ca6d4a7dc6d950efa4e60de717400f4e8ec62e",
    "semantic_title": "does the language matter? curriculum learning over neo-latin languages",
    "citation_count": 0,
    "authors": [
      "Giulia Pucci",
      "Leonardo Ranaldi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.465": {
    "title": "Do Language Models Care about Text Quality? Evaluating Web-Crawled Corpora across 11 Languages",
    "volume": "main",
    "abstract": "Large, curated, web-crawled corpora play a vital role in training language models (LMs). They form the lion's share of the training data in virtually all recent LMs, such as the well-known GPT, LLaMA and XLM-RoBERTa models. However, despite this importance, relatively little attention has been given to the quality of these corpora. In this paper, we compare four of the currently most relevant large, web-crawled corpora (CC100, MaCoCu, mC4 and OSCAR) across eleven lower-resourced European languages. Our approach is two-fold: first, we perform an intrinsic evaluation by performing a human evaluation of the quality of samples taken from different corpora; then, we assess the practical impact of the qualitative differences by training specific LMs on each of the corpora and evaluating their performance on downstream tasks. We find that there are clear differences in quality of the corpora, with MaCoCu and OSCAR obtaining the best results. However, during the extrinsic evaluation, we actually find that the CC100 corpus achieves the highest scores. We conclude that, in our experiments, the quality of the web-crawled corpora does not seem to play a significant role when training LMs",
    "checked": true,
    "id": "1b0654c4e8d0e64ea8d7bd9aadfd6b5b2b7b2112",
    "semantic_title": "do language models care about text quality? evaluating web-crawled corpora across 11 languages",
    "citation_count": 0,
    "authors": [
      "Rik van Noord",
      "Taja Kuzman",
      "Peter Rupnik",
      "Nikola Ljubešić",
      "Miquel Esplà-Gomis",
      "Gema Ramírez-Sánchez",
      "Antonio Toral"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.466": {
    "title": "Do Large Language Models Understand Mansplaining? Well, Actually",
    "volume": "main",
    "abstract": "Gender bias has been widely studied by the NLP community. However, other more subtle variations of it, such as mansplaining, have yet received little attention. Mansplaining is a discriminatory behaviour that consists of a condescending treatment or discourse towards women. In this paper, we introduce and analyze Well, actually..., a corpus of 886 mansplaining stories experienced by women. We analyze the corpus in terms of features such as offensiveness, sentiment or misogyny, among others. We also explore to what extent Large Language Models (LLMs) can understand and identify mansplaining and other gender-related microaggressions. Specifically, we experiment with ChatGPT-3.5-Turbo and LLaMA-2 (13b and 70b), with both targeted and open questions. Our findings suggest that, although they can identify mansplaining to some extent, LLMs still struggle to point out this attitude and will even reproduce some of the social patterns behind mansplaining situations, for instance by praising men for giving unsolicited advice to women",
    "checked": true,
    "id": "b8e298f3b46722db96e46af6b16634b7dd9d22d5",
    "semantic_title": "do large language models understand mansplaining? well, actually",
    "citation_count": 0,
    "authors": [
      "Carla Perez Almendros",
      "Jose Camacho-Collados"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.467": {
    "title": "Domain Adaptation for Dense Retrieval and Conversational Dense Retrieval through Self-Supervision by Meticulous Pseudo-Relevance Labeling",
    "volume": "main",
    "abstract": "Recent studies have demonstrated that the ability of dense retrieval models to generalize to target domains with different distributions is limited, which contrasts with the results obtained with interaction-based models. Prior attempts to mitigate this challenge involved leveraging adversarial learning and query generation approaches, but both approaches nevertheless resulted in limited improvements. In this paper, we propose to combine the query-generation approach with a self-supervision approach in which pseudo-relevance labels are automatically generated on the target domain. To accomplish this, a T5-3B model is utilized for pseudo-positive labeling, and meticulous hard negatives are chosen. We also apply this strategy on conversational dense retrieval model for conversational search. A similar pseudo-labeling approach is used, but with the addition of a query-rewriting module to rewrite conversational queries for subsequent labeling. This proposed approach enables a model's domain adaptation with real queries and documents from the target dataset. Experiments on standard dense retrieval and conversational dense retrieval models both demonstrate improvements on baseline models when they are fine-tuned on the pseudo-relevance labeled data",
    "checked": true,
    "id": "00a0e6e85a7bf7105a2ea18bef4b03030fffccd8",
    "semantic_title": "domain adaptation for dense retrieval and conversational dense retrieval through self-supervision by meticulous pseudo-relevance labeling",
    "citation_count": 0,
    "authors": [
      "Minghan Li",
      "Eric Gaussier"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.468": {
    "title": "Domain-Agnostic Adapter Architecture for Deception Detection: Extensive Evaluations with the DIFrauD Benchmark",
    "volume": "main",
    "abstract": "Despite significant strides in training expansive transformer models, their deployment for niche tasks remains intricate. This paper delves into deception detection, assessing domain adaptation methodologies from a cross-domain lens using transformer Large Language Models (LLMs). We roll out a new corpus with roughly 100,000 honest and misleading statements in seven domains, designed to serve as a benchmark for multidomain deception detection. As a primary contribution, we present a novel parameter-efficient finetuning adapter, PreXIA, which was proposed and implemented as part of this work. The design is model-, domain- and task-agnostic, with broad applications that are not limited by the confines of deception or classification tasks. We comprehensively analyze and rigorously evaluate LLM tuning methods and our original design using the new benchmark, highlighting their strengths, pointing out weaknesses, and suggesting potential areas for improvement. The proposed adapter consistently outperforms all competition on the DIFrauD benchmark used in this study. To the best of our knowledge, it improves on the state-of-the-art in its class for the deception task. In addition, the evaluation process leads to unexpected findings that, at the very least, cast doubt on the conclusions made in some of the recently published research regarding reasoning ability's unequivocal dominance over representations quality with respect to the relative contribution of each one to a model's performance and predictions",
    "checked": true,
    "id": "b769d7b5d523854e2930bdf2b3d385c561bbe26b",
    "semantic_title": "domain-agnostic adapter architecture for deception detection: extensive evaluations with the difraud benchmark",
    "citation_count": 1,
    "authors": [
      "Dainis A. Boumber",
      "Fatima Zahra Qachfar",
      "Rakesh Verma"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.469": {
    "title": "Domain-aware and Co-adaptive Feature Transformation for Domain Adaption Few-shot Relation Extraction",
    "volume": "main",
    "abstract": "Few-shot relation extraction (FSRE) can alleviate the data scarcity problem in relation extraction. However, FSRE models often suffer a significant decline in performance when adapting to new domains. To overcome this issue, many researchers have focused on domain adaption FSRE (DAFSRE). Nevertheless, existing approaches primarily concentrate on the source domain, which makes it difficult to accurately transfer useful knowledge to the target domain. Additionally, the lack of distinction between relations further restricts the model performance. In this paper, we propose the domain-aware and co-adaptive feature transformation approach to address these issues. Specifically, we introduce a domain-aware transformation module that leverages the target domain distribution features to guide the domain-aware feature transformations. This can enhance the model's adaptability across domains, leading to improved target domain performance. Furthermore, we design co-adaptive prototypical networks to perform co-adaptive feature transformation through a transformer mechanism. This results in more robust and distinguishable relation prototypes. Experiments on DAFSRE benchmark datasets demonstrate the effectiveness of our method, which outperforms existing models and achieves state-of-the-art performance",
    "checked": true,
    "id": "ed8a8b296b991ae17f0d7e2b04732cdf2848048c",
    "semantic_title": "domain-aware and co-adaptive feature transformation for domain adaption few-shot relation extraction",
    "citation_count": 0,
    "authors": [
      "Yijun Liu",
      "Feifei Dai",
      "Xiaoyan Gu",
      "Minghui Zhai",
      "Bo Li",
      "Meiou Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.470": {
    "title": "Domain Generalization via Causal Adjustment for Cross-Domain Sentiment Analysis",
    "volume": "main",
    "abstract": "Domain adaption has been widely adapted for cross-domain sentiment analysis to transfer knowledge from the source domain to the target domain. Whereas, most methods are proposed under the assumption that the target (test) domain is known, making them fail to generalize well on unknown test data that is not always available in practice. In this paper, we focus on the problem of domain generalization for cross-domain sentiment analysis. Specifically, we propose a backdoor adjustment-based causal model to disentangle the domain-specific and domain-invariant representations that play essential roles in tackling domain shift. First, we rethink the cross-domain sentiment analysis task in a causal view to model the causal-and-effect relationships among different variables. Then, to learn an invariant feature representation, we remove the effect of domain confounders (e.g., domain knowledge) using the backdoor adjustment. A series of experiments over many homologous and diverse datasets show the great performance and robustness of our model by comparing it with the state-of-the-art domain generalization baselines",
    "checked": true,
    "id": "a8901995ed40168442be3049d1025d577119f2a3",
    "semantic_title": "domain generalization via causal adjustment for cross-domain sentiment analysis",
    "citation_count": 0,
    "authors": [
      "Siyin Wang",
      "Jie Zhou",
      "Qin Chen",
      "Qi Zhang",
      "Tao Gui",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.471": {
    "title": "Domain Transferable Semantic Frames for Expert Interview Dialogues",
    "volume": "main",
    "abstract": "Interviews are an effective method to elicit critical skills to perform particular processes in various domains. In order to understand the knowledge structure of these domain-specific processes, we consider semantic role and predicate annotation based on Frame Semantics. We introduce a dataset of interview dialogues with experts in the culinary and gardening domains, each annotated with semantic frames. This dataset consists of (1) 308 interview dialogues related to the culinary domain, originally assembled by Okahisa et al. (2022), and (2) 100 interview dialogues associated with the gardening domain, which we newly acquired. The labeling specifications take into account the domain-transferability by adopting domain-agnostic labels for frame elements. In addition, we conducted domain transfer experiments from the culinary domain to the gardening domain to examine the domain transferability with our dataset. The experimental results showed the effectiveness of our domain-agnostic labeling scheme",
    "checked": true,
    "id": "d4e8a9d16000d02f0f856aad38f9ebe469ba6a18",
    "semantic_title": "domain transferable semantic frames for expert interview dialogues",
    "citation_count": 0,
    "authors": [
      "Taishi Chika",
      "Taro Okahisa",
      "Takashi Kodama",
      "Yin Jou Huang",
      "Yugo Murawaki",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.472": {
    "title": "Do Neural Language Models Inferentially Compose Concepts the Way Humans Can?",
    "volume": "main",
    "abstract": "While compositional interpretation is the core of language understanding, humans also derive meaning via inference. For example, while the phrase \"the blue hat\" introduces a blue hat into the discourse via the direct composition of \"blue\" and \"hat,\" the same discourse entity is introduced by the phrase \"the blue color of this hat\" despite the absence of any local composition between \"blue\" and \"hat.\" Instead, we infer that if the color is blue and it belongs to the hat, the hat must be blue. We tested the performance of neural language models and humans on such inferentially driven conceptual compositions, eliciting probability estimates for a noun in a minimally composed phrase, \"This blue hat\", following contexts that had introduced the conceptual combinations of those nouns and adjectives either syntactically or inferentially. Surprisingly, our findings reveal significant disparities between the performance of neural language models and human judgments. Among the eight models evaluated, RoBERTa, BERT-large, and GPT-2 exhibited the closest resemblance to human responses, while other models faced challenges in accurately identifying compositions in the provided contexts. Our study reveals that language models and humans may rely on different approaches to represent and compose lexical items across sentence structure. All data and code are accessible at https://github.com/wangshaonan/BlueHat",
    "checked": true,
    "id": "36a7b34b3887d10add71588b11f5c3d5928aef5b",
    "semantic_title": "do neural language models inferentially compose concepts the way humans can?",
    "citation_count": 0,
    "authors": [
      "Amilleah Rodriguez",
      "Shaonan Wang",
      "Liina Pylkkänen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.473": {
    "title": "DORE: A Dataset for Portuguese Definition Generation",
    "volume": "main",
    "abstract": "Definition modelling (DM) is the task of automatically generating a dictionary definition of a specific word. Computational systems that are capable of DM can have numerous applications benefiting a wide range of audiences. As DM is considered a supervised natural language generation problem, these systems require large annotated datasets to train the machine learning (ML) models. Several DM datasets have been released for English and other high-resource languages. While Portuguese is considered a mid/high-resource language in most natural language processing tasks and is spoken by more than 200 million native speakers, there is no DM dataset available for Portuguese. In this research, we fill this gap by introducing DORE; the first dataset for Definition MOdelling for PoRtuguEse containing more than 100,000 definitions. We also evaluate several deep learning based DM models on DORE and report the results. The dataset and the findings of this paper will facilitate research and study of Portuguese in wider contexts",
    "checked": true,
    "id": "cd22baf6df50b59a2889a91ee466deaebc01aeb2",
    "semantic_title": "dore: a dataset for portuguese definition generation",
    "citation_count": 0,
    "authors": [
      "Anna Beatriz Dimas Furtado",
      "Tharindu Ranasinghe",
      "Frederic Blain",
      "Ruslan Mitkov"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.474": {
    "title": "DOSA: A Dataset of Social Artifacts from Different Indian Geographical Subcultures",
    "volume": "main",
    "abstract": "Generative models are increasingly being used in various applications, such as text generation, commonsense reasoning, and question-answering. To be effective globally, these models must be aware of and account for local socio-cultural contexts, making it necessary to have benchmarks to evaluate the models for their cultural familiarity. Since the training data for LLMs is web-based and the Web is limited in its representation of information, it does not capture knowledge present within communities that are not on the Web. Thus, these models exacerbate the inequities, semantic misalignment, and stereotypes from the Web. There has been a growing call for community-centered participatory research methods in NLP. In this work, we respond to this call by using participatory research methods to introduce DOSA, the first community-generated Dataset of 615 Social Artifacts, by engaging with 260 participants from 19 different Indian geographic subcultures. We use a gamified framework that relies on collective sensemaking to collect the names and descriptions of these artifacts such that the descriptions semantically align with the shared sensibilities of the individuals from those cultures. Next, we benchmark four popular LLMs and find that they show significant variation across regional sub-cultures in their ability to infer the artifacts",
    "checked": true,
    "id": "ca60cb73d770f7093100cdea12b19881e7610df1",
    "semantic_title": "dosa: a dataset of social artifacts from different indian geographical subcultures",
    "citation_count": 0,
    "authors": [
      "Agrima Seth",
      "Sanchit Ahuja",
      "Kalika Bali",
      "Sunayana Sitaram"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.475": {
    "title": "DP-CRE: Continual Relation Extraction via Decoupled Contrastive Learning and Memory Structure Preservation",
    "volume": "main",
    "abstract": "Continuous Relation Extraction (CRE) aims to incrementally learn relation knowledge from a non-stationary stream of data. Since the introduction of new relational tasks can overshadow previously learned information, catastrophic forgetting becomes a significant challenge in this domain. Current replay-based training paradigms prioritize all data uniformly and train memory samples through multiple rounds, which would result in overfitting old tasks and pronounced bias towards new tasks because of the imbalances of the replay set. To handle the problem, we introduce the DecouPled CRE (DP-CRE) framework that decouples the process of prior information preservation and new knowledge acquisition. This framework examines alterations in the embedding space as new relation classes emerge, distinctly managing the preservation and acquisition of knowledge. Extensive experiments show that DP-CRE significantly outperforms other CRE baselines across two datasets",
    "checked": true,
    "id": "dccdd1efa9a2f523c4a442509579487d78dada96",
    "semantic_title": "dp-cre: continual relation extraction via decoupled contrastive learning and memory structure preservation",
    "citation_count": 0,
    "authors": [
      "Mengyi Huang",
      "Meng Xiao",
      "Ludi Wang",
      "Yi Du"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.476": {
    "title": "Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering",
    "volume": "main",
    "abstract": "Open Domain Multi-Hop Question Answering (ODMHQA) plays a crucial role in Natural Language Processing (NLP) by aiming to answer complex questions through multi-step reasoning over retrieved information from external knowledge sources. Recently, Large Language Models (LLMs) have demonstrated remarkable performance in solving ODMHQA owing to their capabilities including planning, reasoning, and utilizing tools. However, LLMs may generate off-topic answers when attempting to solve ODMHQA, namely the generated answers are irrelevant to the original questions. This issue of off-topic answers accounts for approximately one-third of incorrect answers, yet remains underexplored despite its significance. To alleviate this issue, we propose the Discriminate→Re-Compose→Re- Solve→Re-Decompose (Dr3) mechanism. Specifically, the Discriminator leverages the intrinsic capabilities of LLMs to judge whether the generated answers are off-topic. In cases where an off-topic answer is detected, the Corrector performs step-wise revisions along the reversed reasoning chain (Re-Compose→Re-Solve→Re-Decompose) until the final answer becomes on-topic. Experimental results on the HotpotQA and 2WikiMultiHopQA datasets demonstrate that our Dr3 mechanism considerably reduces the occurrence of off-topic answers in ODMHQA by nearly 13%, improving the performance in Exact Match (EM) by nearly 3% compared to the baseline method without the Dr3 mechanism",
    "checked": true,
    "id": "509a5b8d52cab8c7d6a4e59d5e2086b7d05368b5",
    "semantic_title": "dr3: ask large language models not to give off-topic answers in open domain multi-hop question answering",
    "citation_count": 0,
    "authors": [
      "Yuan Gao",
      "Yiheng Zhu",
      "Yuanbin Cao",
      "Yinzhi Zhou",
      "Zhen Wu",
      "Yujie Chen",
      "Shenglan Wu",
      "Haoyuan Hu",
      "Xinyu Dai"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.477": {
    "title": "DRAMA: Dynamic Multi-Granularity Graph Estimate Retrieval over Tabular and Textual Question Answering",
    "volume": "main",
    "abstract": "The TableTextQA task requires finding the answer to the question from a combination of tabular and textual data, which has been gaining increasing attention. The row-based approaches have demonstrated remarkable effectiveness. However, they suffer from the following limitations: (1) a lack of interaction between rows; (2) excessively long input lengths; and (3) question attention shifts in the multi-hop QA task. To this end, we propose a novel method: Dynamic Multi-Granularity Graph Estimate Retrieval - DRAMA. Our method incorporates an interaction mechanism among multiple rows. Specifically, we utilize a memory bank to store the features of each row, thereby facilitating the construction of a heterogeneous graph with multi-row information. Besides, a Dynamic Graph Attention Network (DGAT) module is engaged to gauge the attention shift in the multi-hop question and eliminate the noise information dynamically. Empirical results on the widely used HybridQA and TabFact datasets demonstrate that the proposed model is effective",
    "checked": true,
    "id": "bcf87678e852c8b5c99287f9f02f3b45fc54f155",
    "semantic_title": "drama: dynamic multi-granularity graph estimate retrieval over tabular and textual question answering",
    "citation_count": 0,
    "authors": [
      "Ruize Yuan",
      "Xiang Ao",
      "Li Zeng",
      "Qing He"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.478": {
    "title": "DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain",
    "volume": "main",
    "abstract": "The biomedical domain has sparked a significant interest in the field of Natural Language Processing (NLP), which has seen substantial advancements with pre-trained language models (PLMs). However, comparing these models has proven challenging due to variations in evaluation protocols across different models. A fair solution is to aggregate diverse downstream tasks into a benchmark, allowing for the assessment of intrinsic PLMs qualities from various perspectives. Although still limited to few languages, this initiative has been undertaken in the biomedical field, notably English and Chinese. This limitation hampers the evaluation of the latest French biomedical models, as they are either assessed on a minimal number of tasks with non-standardized protocols or evaluated using general downstream tasks. To bridge this research gap and account for the unique sensitivities of French, we present the first-ever publicly available French biomedical language understanding benchmark called DrBenchmark. It encompasses 20 diversified tasks, including named-entity recognition, part-of-speech tagging, question-answering, semantic textual similarity, or classification. We evaluate 8 state-of-the-art pre-trained masked language models (MLMs) on general and biomedical-specific data, as well as English specific MLMs to assess their cross-lingual capabilities. Our experiments reveal that no single model excels across all tasks, while generalist models are sometimes still competitive",
    "checked": true,
    "id": "ed3057a99ddd416bf5dcaee83688ca7e43da678f",
    "semantic_title": "drbenchmark: a large language understanding evaluation benchmark for french biomedical domain",
    "citation_count": 2,
    "authors": [
      "Yanis Labrak",
      "Adrien Bazoge",
      "Oumaima El Khettari",
      "Mickael Rouvier",
      "Pacome Constant Dit Beaufils",
      "Natalia Grabar",
      "Béatrice Daille",
      "Solen Quiniou",
      "Emmanuel Morin",
      "Pierre-Antoine Gourraud",
      "Richard Dufour"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.479": {
    "title": "Dual Complex Number Knowledge Graph Embeddings",
    "volume": "main",
    "abstract": "Knowledge graph embedding, which aims to learn representations of entities and relations in large scale knowledge graphs, plays a crucial part in various downstream applications. The performance of knowledge graph embedding models mainly depends on the ability of modeling relation patterns, such as symmetry/antisymmetry, inversion and composition (commutative composition and non-commutative composition). Most existing methods fail in modeling the non-commutative composition patterns. Several methods support this kind of pattern by modeling in quaternion space or dihedral group. However, extending to such sophisticated spaces leads to a substantial increase in the amount of parameters, which greatly reduces the parameter efficiency. In this paper, we propose a new knowledge graph embedding method called dual complex number knowledge graph embeddings (DCNE), which maps entities to the dual complex number space, and represents relations as rotations in 2D space via dual complex number multiplication. The non-commutativity of the dual complex number multiplication empowers DCNE to model the non-commutative composition patterns. In the meantime, modeling relations as rotations in 2D space can effectively improve the parameter efficiency. Extensive experiments on multiple benchmark knowledge graphs empirically show that DCNE achieves significant performance in link prediction and path query answering",
    "checked": true,
    "id": "369b60a2762c92e9ad2d5b5461fb2380d04daa2b",
    "semantic_title": "dual complex number knowledge graph embeddings",
    "citation_count": 0,
    "authors": [
      "Yao Dong",
      "Qingchao Kong",
      "Lei Wang",
      "Yin Luo"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.480": {
    "title": "Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction",
    "volume": "main",
    "abstract": "Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained sentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to model the syntax-semantic relationships inherent in triplet elements. However, they have yet to fully tap into the vast potential of syntactic and semantic information within the ASTE task. In this work, we propose a Dual Encoder: Exploiting the potential of Syntactic and Semantic model (D2E2S), which maximizes the syntactic and semantic relationships among words. Specifically, our model utilizes a dual-channel encoder with a BERT channel to capture semantic information, and an enhanced LSTM channel for comprehensive syntactic information capture. Subsequently, we introduce the heterogeneous feature interaction module to capture intricate interactions between dependency syntax and attention semantics, and to dynamically select vital nodes. We leverage the synergy of these modules to harness the significant potential of syntactic and semantic information in ASTE tasks. Testing on public benchmarks, our D2E2S model surpasses the current state-of-the-art(SOTA), demonstrating its effectiveness",
    "checked": true,
    "id": "10c9dcc0010eb268f68ecbe3286b711e43a18ecf",
    "semantic_title": "dual encoder: exploiting the potential of syntactic and semantic for aspect sentiment triplet extraction",
    "citation_count": 0,
    "authors": [
      "Xiaowei Zhao",
      "Yong Zhou",
      "Xiujuan Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.481": {
    "title": "DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues",
    "volume": "main",
    "abstract": "User Simulators play a pivotal role in training and evaluating task-oriented dialogue systems. Traditional user simulators typically rely on human-engineered agendas, resulting in generated responses that often lack diversity and spontaneity. Although large language models (LLMs) exhibit a remarkable capacity for generating coherent and contextually appropriate utterances, they may fall short when tasked with generating responses that effectively guide users towards their goals, particularly in dialogues with intricate constraints and requirements. This paper introduces DuetSim, a novel framework designed to address the intricate demands of task-oriented dialogues by leveraging LLMs. DuetSim stands apart from conventional approaches by employing two LLMs in tandem: one dedicated to response generation and the other focused on verification. This dual LLM approach empowers DuetSim to produce responses that not only exhibit diversity but also demonstrate accuracy and are preferred by human users. We validate the efficacy of our method through extensive experiments conducted on the MultiWOZ dataset, highlighting improvements in response quality and correctness, largely attributed to the incorporation of the second LLM",
    "checked": true,
    "id": "98aa2bcb5b5b8701add317a75689b7c04f372759",
    "semantic_title": "duetsim: building user simulator with dual large language models for task-oriented dialogues",
    "citation_count": 0,
    "authors": [
      "Xiang Luo",
      "Zhiwen Tang",
      "Jin Wang",
      "Xuejie Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.482": {
    "title": "Dynamic Knowledge Prompt for Chest X-ray Report Generation",
    "volume": "main",
    "abstract": "Automatic generation of radiology reports can relieve the burden of radiologist. In the radiology library, the biased dataset and the sparse features of chest X-ray image make it difficult to generate reports. Many approaches strive to integrate prior information to enhance generation, but they fail to dynamically utilize pulmonary lesion knowledge at the instance-level. To alleviate above problem, we propose a novel Dynamic Knowledge Prompt (DKP) framework for chest X-ray report generation. The DKP can dynamically incorporate the pulmonary lesion information at the instance-level to facilitate report generation. Initially, we design a knowledge prompt for each pulmonary lesion using numerous radiology reports. After that, the DKP using an anomaly detector generates the dynamic knowledge prompt by extracting discriminative lesion features in the corresponding X-ray image. Finally, the knowledge prompt is encoded and fused with hidden states extracted from decoder, to form multi-modal features that guide visual features to generate reports. Extensive experiments on the public datasets MIMIC-CXR and IU X-Ray show that our approach achieves state-of-the-art performance",
    "checked": true,
    "id": "1964cdd8e2f1c26c89e47e0932a581c9f2c1a313",
    "semantic_title": "dynamic knowledge prompt for chest x-ray report generation",
    "citation_count": 0,
    "authors": [
      "Shenshen Bu",
      "Yujie Song",
      "Taiji Li",
      "Zhiming Dai"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.483": {
    "title": "Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for Counselor Reflection Generation",
    "volume": "main",
    "abstract": "In this paper, we study the problem of multi-reward reinforcement learning to jointly optimize for multiple text qualities for natural language generation. We focus on the task of counselor reflection generation, where we optimize the generators to simultaneously improve the fluency, coherence, and reflection quality of generated counselor responses. We introduce two novel bandit methods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining rewards into a single value and optimizing them simultaneously. Specifically, we employ non-contextual and contextual multi-arm bandits to dynamically adjust multiple reward weights during training. Through automatic and manual evaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt, outperform existing naive and bandit baselines, showcasing their potential for enhancing language models",
    "checked": true,
    "id": "73d1bff2768eecde45bd0ba7ca71976fd825948c",
    "semantic_title": "dynamic reward adjustment in multi-reward reinforcement learning for counselor reflection generation",
    "citation_count": 1,
    "authors": [
      "Do June Min",
      "Veronica Perez-Rosas",
      "Ken Resnicow",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.484": {
    "title": "Dynamic Spatial-Temporal Aggregation for Skeleton-Aware Sign Language Recognition",
    "volume": "main",
    "abstract": "Skeleton-aware sign language recognition (SLR) has gained popularity due to its ability to remain unaffected by background information and its lower computational requirements. Current methods utilize spatial graph modules and temporal modules to capture spatial and temporal features, respectively. However, their spatial graph modules are typically built on fixed graph structures such as graph convolutional networks or a single learnable graph, which only partially explore joint relationships. Additionally, a simple temporal convolution kernel is used to capture temporal information, which may not fully capture the complex movement patterns of different signers. To overcome these limitations, we propose a new spatial architecture consisting of two concurrent branches, which build input-sensitive joint relationships and incorporates specific domain knowledge for recognition, respectively. These two branches are followed by an aggregation process to distinguishe important joint connections. We then propose a new temporal module to model multi-scale temporal information to capture complex human dynamics. Our method achieves state-of-the-art accuracy compared to previous skeleton-aware methods on four large-scale SLR benchmarks. Moreover, our method demonstrates superior accuracy compared to RGB-based methods in most cases while requiring much fewer computational resources, bringing better accuracy-computation trade-off. Code is available at https://github.com/hulianyuyy/DSTA-SLR",
    "checked": true,
    "id": "174802705b3fc61f855eef68b7f5a29103ae185d",
    "semantic_title": "dynamic spatial-temporal aggregation for skeleton-aware sign language recognition",
    "citation_count": 0,
    "authors": [
      "Lianyu Hu",
      "Liqing Gao",
      "Zekang Liu",
      "Wei Feng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.485": {
    "title": "EcoVerse: An Annotated Twitter Dataset for Eco-Relevance Classification, Environmental Impact Analysis, and Stance Detection",
    "volume": "main",
    "abstract": "Anthropogenic ecological crisis constitutes a significant challenge that all within the academy must urgently face, including the Natural Language Processing (NLP) community. While recent years have seen increasing work revolving around climate-centric discourse, crucial environmental and ecological topics outside of climate change remain largely unaddressed, despite their prominent importance. Mainstream NLP tasks, such as sentiment analysis, dominate the scene, but there remains an untouched space in the literature involving the analysis of environmental impacts of certain events and practices. To address this gap, this paper presents EcoVerse, an annotated English Twitter dataset of 3,023 tweets spanning a wide spectrum of environmental topics. We propose a three-level annotation scheme designed for Eco-Relevance Classification, Stance Detection, and introducing an original approach for Environmental Impact Analysis. We detail the data collection, filtering, and labeling process that led to the creation of the dataset. Remarkable Inter-Annotator Agreement indicates that the annotation scheme produces consistent annotations of high quality. Subsequent classification experiments using BERT-based models, including ClimateBERT, are presented. These yield encouraging results, while also indicating room for a model specifically tailored for environmental texts. The dataset is made freely available to stimulate further research",
    "checked": true,
    "id": "7c1fe43175326d5a1627122d5500b4b44302e9c4",
    "semantic_title": "ecoverse: an annotated twitter dataset for eco-relevance classification, environmental impact analysis, and stance detection",
    "citation_count": 0,
    "authors": [
      "Francesca Grasso",
      "Stefano Locci",
      "Giovanni Siragusa",
      "Luigi Di Caro"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.486": {
    "title": "ECtHR-PCR: A Dataset for Precedent Understanding and Prior Case Retrieval in the European Court of Human Rights",
    "volume": "main",
    "abstract": "In common law jurisdictions, legal practitioners rely on precedents to construct arguments, in line with the doctrine of stare decisis. As the number of cases grow over the years, prior case retrieval (PCR) has garnered significant attention. Besides lacking real-world scale, existing PCR datasets do not simulate a realistic setting, because their queries use complete case documents while only masking references to prior cases. The query is thereby exposed to legal reasoning not yet available when constructing an argument for an undecided case as well as spurious patterns left behind by citation masks, potentially short-circuiting a comprehensive understanding of case facts and legal principles. To address these limitations, we introduce a PCR dataset based on judgements from the European Court of Human Rights (ECtHR), which explicitly separate facts from arguments and exhibit precedential practices, aiding us to develop this PCR dataset to foster systems' comprehensive understanding. We benchmark different lexical and dense retrieval approaches with various negative sampling strategies, adapting them to deal with long text sequences using hierarchical variants. We found that difficulty-based negative sampling strategies were not effective for the PCR task, highlighting the need for investigation into domain-specific difficulty criteria. Furthermore, we observe performance of the dense models degrade with time and calls for further research into temporal adaptation of retrieval models. Additionally, we assess the influence of different views , Halsbury's and Goodhart's, in practice in ECtHR jurisdiction using PCR task",
    "checked": true,
    "id": "b5b019e1aee09dbaf1147be7bfe0736ed51736ee",
    "semantic_title": "ecthr-pcr: a dataset for precedent understanding and prior case retrieval in the european court of human rights",
    "citation_count": 0,
    "authors": [
      "Santosh T.y.s.s.",
      "Rashid Haddad",
      "Matthias Grabmair"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.487": {
    "title": "EDDA: An Encoder-Decoder Data Augmentation Framework for Zero-Shot Stance Detection",
    "volume": "main",
    "abstract": "Stance detection aims to determine the attitude expressed in text towards a given target. Zero-shot stance detection (ZSSD) has emerged to classify stances towards unseen targets during inference. Recent data augmentation techniques for ZSSD increase transferable knowledge between targets through text or target augmentation. However, these methods exhibit limitations. Target augmentation lacks logical connections between generated targets and source text, while text augmentation relies solely on training data, resulting in insufficient generalization. To address these issues, we propose an encoder-decoder data augmentation (EDDA) framework. The encoder leverages large language models and chain-of-thought prompting to summarize texts into target-specific if-then rationales, establishing logical relationships. The decoder generates new samples based on these expressions using a semantic correlation word replacement strategy to increase syntactic diversity. We also analyze the generated expressions to develop a rationale-enhanced network that fully utilizes the augmented data. Experiments on benchmark datasets demonstrate our approach substantially improves over state-of-the-art ZSSD techniques. The proposed EDDA framework increases semantic relevance and syntactic variety in augmented texts while enabling interpretable rationale-based learning",
    "checked": true,
    "id": "97706706151b18f827709c4f53c87f9ad7ebde8a",
    "semantic_title": "edda: an encoder-decoder data augmentation framework for zero-shot stance detection",
    "citation_count": 0,
    "authors": [
      "Daijun Ding",
      "Li Dong",
      "Zhichao Huang",
      "Guangning Xu",
      "Xu Huang",
      "Bo Liu",
      "Liwen Jing",
      "Bowen Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.488": {
    "title": "EDEN: A Dataset for Event Detection in Norwegian News",
    "volume": "main",
    "abstract": "We present EDEN, the first Norwegian dataset annotated with event information at the sentence level, adapting the widely used ACE event schema to Norwegian. The paper describes the manual annotation of Norwegian text as well as transcribed speech in the news domain, together with inter-annotator agreement and discussions of relevant dataset statistics. We also present preliminary modeling results using a graph-based event parser. The resulting dataset will be freely available for download and use",
    "checked": true,
    "id": "da4c9c0906f111191d32f18807ec1d4870b4014e",
    "semantic_title": "eden: a dataset for event detection in norwegian news",
    "citation_count": 0,
    "authors": [
      "Samia Touileb",
      "Jeanett Murstad",
      "Petter Mæhlum",
      "Lubos Steskal",
      "Lilja Charlotte Storset",
      "Huiling You",
      "Lilja Øvrelid"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.489": {
    "title": "Educational Dialogue Systems for Visually Impaired Students: Introducing a Task-Oriented User-Agent Corpus",
    "volume": "main",
    "abstract": "This paper describes a corpus consisting of real-world dialogues in English between users and a task-oriented conversational agent, with interactions revolving around the description of finite state automata. The creation of this corpus is part of a larger research project aimed at developing tools for an easier access to educational content, especially in STEM fields, for users with visual impairments. The development of this corpus was precisely motivated by the aim of providing a useful resource to support the design of such tools. The core feature of this corpus is that its creation involved both sighted and visually impaired participants, thus allowing for a greater diversity of perspectives and giving the opportunity to identify possible differences in the way the two groups of participants interacted with the agent. The paper introduces this corpus, giving an account of the process that led to its creation, i.e. the methodology followed to obtain the data, the annotation scheme adopted, and the analysis of the results. Finally, the paper reports the results of a classification experiment on the annotated corpus, and an additional experiment to assess the annotation capabilities of three large language models, in view of a further expansion of the corpus",
    "checked": true,
    "id": "3581d7350f554803f6d9b62bfda905bfffc8aadb",
    "semantic_title": "educational dialogue systems for visually impaired students: introducing a task-oriented user-agent corpus",
    "citation_count": 0,
    "authors": [
      "Elisa Di Nuovo",
      "Manuela Sanguinetti",
      "Pier Felice Balestrucci",
      "Luca Anselma",
      "Cristian Bernareggi",
      "Alessandro Mazzei"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.490": {
    "title": "EEE-QA: Exploring Effective and Efficient Question-Answer Representations",
    "volume": "main",
    "abstract": "Current approaches to question answering rely on pre-trained language models (PLMs) like RoBERTa. This work challenges the existing question-answer encoding convention and explores finer representations. We begin with testing various pooling methods compared to using the begin-of-sentence token as a question representation for better quality. Next, we explore opportunities to simultaneously embed all answer candidates with the question. This enables cross-reference between answer choices and improves inference throughput via reduced memory usage. Despite their simplicity and effectiveness, these methods have yet to be widely studied in current frameworks. We experiment with different PLMs, and with and without the integration of knowledge graphs. Results prove that the memory efficacy of the proposed techniques with little sacrifice in performance. Practically, our work enhances 38-100% throughput with 26-65% speedups on consumer-grade GPUs by allowing for considerably larger batch sizes. Our work sends a message to the community with promising directions in both representation quality and efficiency for the question-answering task in natural language processing",
    "checked": true,
    "id": "57c78e9a22d33683bd04aabf069ef756fe121924",
    "semantic_title": "eee-qa: exploring effective and efficient question-answer representations",
    "citation_count": 0,
    "authors": [
      "Zhanghao Hu",
      "Yijun Yang",
      "Junjie Xu",
      "Yifu Qiu",
      "Pinzhen Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.491": {
    "title": "Eesthetic: A Paralex Lexicon of Estonian Paradigms",
    "volume": "main",
    "abstract": "We introduce Eesthetic, a comprehensive Estonian noun and verb lexicon sourced from the Ekilex database. It documents 5475 nouns inflecting for 28 paradigm cells and 5076 verbs inflecting for 51 cells, and comprises a total of 452885 inflected forms. Our openly accessible machine-readable dataset adheres to the Paralex standard. It comprises CSV tables linked by formal relationships. Metadata in JSON format, following the Frictionless standard, provides detailed descriptions of the tables and dataset. The lexicon offers extensive linguistic annotations, including orthographic forms, automatically transcribed phonemic transcriptions, non-canonical morphological phenomena such as overabundance and defectiveness, rich mapping of the paradigm cells and feature-values to other notation schemes, a decomposition of phonemes in distinctive features, and annotation of inflection classes. It is suited for both monolingual and comparative research, enabling qualitative and quantitative analysis. This paper outlines the creation process, rationale, and resulting structure, along with our set of rules for automatic orthography-to-phonemic transcription conversion",
    "checked": true,
    "id": "83bce1bc2cee31439c6b0fa301ff25b4bd109df5",
    "semantic_title": "eesthetic: a paralex lexicon of estonian paradigms",
    "citation_count": 0,
    "authors": [
      "Sacha Beniamine",
      "Mari Aigro",
      "Matthew Baerman",
      "Jules Bouton",
      "Maria Copot"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.492": {
    "title": "Effective Distillation of Table-based Reasoning Ability from LLMs",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their enormous parameter size and extremely high requirements for compute power pose challenges for their practical deployment. Recent research has revealed that specific capabilities of LLMs, such as numerical reasoning, can be transferred to smaller models through distillation. Some studies explore the potential of leveraging LLMs to perform table-based reasoning. However, there has been no prior work focusing on table reasoning skills in smaller models specifically tailored for scientific table-to-text generation tasks. In this paper, we propose a novel table-based reasoning distillation approach, with the aim of distilling LLMs into tailored smaller models. Our experimental results have shown that a 220 million parameter model (Flan-T5-base) fine-tuned using distilled data, not only achieves a significant improvement compared to traditionally fine-tuned baselines, but also surpasses specific LLMs on a scientific table-to-text generation dataset. Our code is available at https://github.com/Bernard-Yang/DistillTableCoT",
    "checked": true,
    "id": "4f249487c670263700df7b2269cdb92a265bc21f",
    "semantic_title": "effective distillation of table-based reasoning ability from llms",
    "citation_count": 8,
    "authors": [
      "Bohao Yang",
      "Chen Tang",
      "Kun Zhao",
      "Chenghao Xiao",
      "Chenghua Lin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.493": {
    "title": "Effective Integration of Text Diffusion and Pre-Trained Language Models with Linguistic Easy-First Schedule",
    "volume": "main",
    "abstract": "Diffusion models have become a powerful generative modeling paradigm, achieving great success in continuous data patterns. However, the discrete nature of text data results in compatibility issues between continuous diffusion models (CDMs) and pre-trained language models (PLMs). That is, the performance of diffusion models even degrades when combined with PLMs. To alleviate this issue, we propose to utilize a pre-trained decoder to convert the denoised embedding vectors into natural language instead of using the widely used rounding operation. In this way, CDMs can be more effectively combined with PLMs. Additionally, considering that existing noise schedules in text diffusion models do not take into account the linguistic differences among tokens, which violates the easy-first policy for text generation, we propose a linguistic easy-first schedule that incorporates the measure of word importance, conforming to easy-first-generation linguistic features and bringing about improved generation quality. Experiment results on the E2E dataset and five controllable tasks show that our approach can combine the merits of CDMs and PLMs, significantly outperforming other diffusion-based models",
    "checked": true,
    "id": "dea1a827a959416af36579be00bc6b176bff1562",
    "semantic_title": "effective integration of text diffusion and pre-trained language models with linguistic easy-first schedule",
    "citation_count": 0,
    "authors": [
      "Yimin Ou",
      "Ping Jian"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.494": {
    "title": "Efficiency and Effectiveness in Task-Oriented Dialogue: On Construction Repetition, Information Rate, and Task Success",
    "volume": "main",
    "abstract": "We investigate the roles that efficiency and effectiveness play in speakers' repetition of shared word sequences, or constructions, in task-oriented dialogue. We find that repeating constructions has negative effects on information rate and positive effects on rate of delivery, that information rate managing strategies are predictive of task success, and that this varies by the communicative function of the constructions being repeated. More effective dialogue is characterised by greater levels of shared construction usage and more efficient task-related repetition; while task-agnostic repetition can seem redundant, it can serve important efficiency and effectiveness functions. Our results provide a nuanced picture of the importance of repetition and of developing a shared lexicon for both efficiency and effectiveness in task-oriented dialogue",
    "checked": true,
    "id": "b9db3b440ef8c70fbea39ecc29f5f429f895e071",
    "semantic_title": "efficiency and effectiveness in task-oriented dialogue: on construction repetition, information rate, and task success",
    "citation_count": 0,
    "authors": [
      "Jun Sen Yee",
      "Mario Giulianelli",
      "Arabella J. Sinclair"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.495": {
    "title": "Efficient AMR Parsing with CLAP: Compact Linearization with an Adaptable Parser",
    "volume": "main",
    "abstract": "Sequence-to-sequence models have become the de facto standard for Abstract Meaning Representation (AMR) parsing due to their high-quality performance. However, these systems face efficiency challenges because of their large model size and computational time, which limit their accessibility within the research community. This paper aims to break down these barriers by introducing a novel linearization and system that significantly enhances the efficiency and accessibility of previous AMR parsers. First, we propose our novel Compact linearization that simplifies encoding, thereby reducing the number of tokens by between 40% and 50%. Second, we present CLAP, an innovative modular system that maintains the model's high performance while achieving remarkable 80% reduction in training and inference times. Furthermore, CLAP is compatible with multiple autoregressive Language Models (LM) and tokenizers, such as BART, T5, and others. These advancements underscore the importance of optimizing sequence-to-sequence models in AMR parsing, thus democratizing access to high-quality semantic analysis. Our code is publicly available at https://github.com/SapienzaNLP/clap/",
    "checked": true,
    "id": "14d025629c8b41f0e82d7b1038704653e9d6f750",
    "semantic_title": "efficient amr parsing with clap: compact linearization with an adaptable parser",
    "citation_count": 0,
    "authors": [
      "Abelardo Carlos Martinez Lorenzo",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.496": {
    "title": "Efficient and Accurate Contextual Re-Ranking for Knowledge Graph Question Answering",
    "volume": "main",
    "abstract": "The efficacy of neural \"retrieve and generate\" systems is well established for question answering (QA) over unstructured text. Recent efforts seek to extend this approach to knowledge graph (KG) QA by converting structured triples to unstructured text. However, the relevance of KG triples retrieved by these systems limits their accuracy. In this paper, we improve the relevance of retrieved triples using a carefully designed re-ranker. Specifically, our pipeline (i) retrieves over documents of triples grouped by entity, (ii) re-ranks triples from these documents with context: triples in the 1-hop neighborhood of the documents' subject entity, and (iii) generates an answer from highly relevant re-ranked triples. To train our re-ranker, we propose a novel \"triple-level\" labeling strategy that infers fine-grained labels and shows that these significantly improve the relevance of retrieved information. We show that the resulting \"retrieve, re-rank, and generate\" pipeline significantly improves upon prior KGQA systems, achieving a new state-of-the-art on FreebaseQA by 5.56% Exact Match. We perform multiple ablations that reveal the distinct benefits of our contextual re-ranker and labeling strategy and conclude with a case study that highlights opportunities for future works",
    "checked": true,
    "id": "8bc08a3fa45a25822f5926fbdc306f192dcfbd20",
    "semantic_title": "efficient and accurate contextual re-ranking for knowledge graph question answering",
    "citation_count": 0,
    "authors": [
      "Kexuan Sun",
      "Nicolaas Paul Jedema",
      "Karishma Sharma",
      "Ruben Janssen",
      "Jay Pujara",
      "Pedro Szekely",
      "Alessandro Moschitti"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.497": {
    "title": "EFTNAS: Searching for Efficient Language Models in First-Order Weight-Reordered Super-Networks",
    "volume": "main",
    "abstract": "Transformer-based models have demonstrated outstanding performance in natural language processing (NLP) tasks and many other domains, e.g., computer vision. Depending on the size of these models, which have grown exponentially in the past few years, machine learning practitioners might be restricted from deploying them in resource-constrained environments. This paper discusses the compression of transformer-based models for multiple resource budgets. Integrating neural architecture search (NAS) and network pruning techniques, we effectively generate and train weight-sharing super-networks that contain efficient, high-performing, and compressed transformer-based models. A common challenge in NAS is the design of the search space, for which we propose a method to automatically obtain the boundaries of the search space and then derive the rest of the intermediate possible architectures using a first-order weight importance technique. The proposed end-to-end NAS solution, EFTNAS, discovers efficient subnetworks that have been compressed and fine-tuned for downstream NLP tasks. We demonstrate EFTNAS on the General Language Understanding Evaluation (GLUE) benchmark and the Stanford Question Answering Dataset (SQuAD), obtaining high-performing smaller models with a reduction of more than 5x in size without or with little degradation in performance",
    "checked": true,
    "id": "b64351ef2d935f2c95d7cb8d4ce49df506dd1a85",
    "semantic_title": "eftnas: searching for efficient language models in first-order weight-reordered super-networks",
    "citation_count": 2,
    "authors": [
      "Juan Pablo Munoz",
      "Yi Zheng",
      "Nilesh Jain"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.498": {
    "title": "Eliciting Motivational Interviewing Skill Codes in Psychotherapy with LLMs: A Bilingual Dataset and Analytical Study",
    "volume": "main",
    "abstract": "Behavioral coding (BC) in motivational interviewing (MI) holds great potential for enhancing the efficacy of MI counseling. However, manual coding is labor-intensive, and automation efforts are hindered by the lack of data due to the privacy of psychotherapy. To address these challenges, we introduce BiMISC, a bilingual dataset of MI conversations in English and Dutch, sourced from real counseling sessions. Expert annotations in BiMISC adhere strictly to the motivational interviewing skills code (MISC) scheme, offering a pivotal resource for MI research. Additionally, we present a novel approach to elicit the MISC expertise from Large language models (LLMs) for MI coding. Through the in-depth analysis of BiMISC and the evaluation of our proposed approach, we demonstrate that the LLM-based approach yields results closely aligned with expert annotations and maintains consistent performance across different languages. Our contributions not only furnish the MI community with a valuable bilingual dataset but also spotlight the potential of LLMs in MI coding, laying the foundation for future MI research",
    "checked": true,
    "id": "b3afff5675d36c065b2303baadaf9c18940a9e2d",
    "semantic_title": "eliciting motivational interviewing skill codes in psychotherapy with llms: a bilingual dataset and analytical study",
    "citation_count": 0,
    "authors": [
      "Xin Sun",
      "Jiahuan Pei",
      "Jan de Wit",
      "Mohammad Aliannejadi",
      "Emiel Krahmer",
      "Jos T.P. Dobber",
      "Jos A. Bosch"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.499": {
    "title": "ELLEN: Extremely Lightly Supervised Learning for Efficient Named Entity Recognition",
    "volume": "main",
    "abstract": "In this work, we revisit the problem of semi-supervised named entity recognition (NER) focusing on extremely light supervision, consisting of a lexicon containing only 10 examples per class. We introduce ELLEN, a simple, fully modular, neuro-symbolic method that blends fine-tuned language models with linguistic rules. These rules include insights such as \"One Sense Per Discourse\", using a Masked Language Model as an unsupervised NER, leveraging part-of-speech tags to identify and eliminate unlabeled entities as false negatives, and other intuitions about classifier confidence scores in local and global context. ELLEN achieves very strong performance on the CoNLL-2003 dataset when using the minimal supervision from the lexicon above. It also outperforms most existing (and considerably more complex) semi-supervised NER methods under the same supervision settings commonly used in the literature (i.e., 5% of the training data). Further, we evaluate our CoNLL-2003 model in a zero-shot scenario on WNUT-17 where we find that it outperforms GPT-3.5 and achieves comparable performance to GPT-4. In a zero-shot setting, ELLEN also achieves over 75% of the performance of a strong, fully supervised model trained on gold data. Our code is publicly available",
    "checked": true,
    "id": "e62a6fb154622a4edc348d7beac16dab3d009c18",
    "semantic_title": "ellen: extremely lightly supervised learning for efficient named entity recognition",
    "citation_count": 0,
    "authors": [
      "Haris Riaz",
      "Razvan Gabriel Dumitru",
      "Mihai Surdeanu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.500": {
    "title": "EMAD: A Bridge Tagset for Unifying Arabic POS Annotations",
    "volume": "main",
    "abstract": "There have been many attempts to model the morphological richness and complexity of Arabic, leading to numerous Part-of-Speech (POS) tagsets that differ in terms of (a) which morphological features they represent, (b) how they represent them, and (c) the degree of specification of said features. Tagset granularity plays an important role in determining how annotated data can be used and for what applications. Due to the diversity among existing tagsets, many annotated corpora for Arabic cannot be easily combined, which exacerbates the Arabic resource poverty situation. In this work, we propose an intermediate tagset designed to facilitate the conversion and unification of different tagsets used to annotate Arabic corpora. This new tagset acts as a bridge between different annotation schemes, simplifying the integration of annotated corpora and promoting collaboration across the projects using them",
    "checked": true,
    "id": "7cef7a7ba53bf9009d13f1a1a26d2130933230b6",
    "semantic_title": "emad: a bridge tagset for unifying arabic pos annotations",
    "citation_count": 0,
    "authors": [
      "Omar Kallas",
      "Go Inoue",
      "Nizar Habash"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.501": {
    "title": "Emancipating Event Extraction from the Constraints of Long-Tailed Distribution Data Utilizing Large Language Models",
    "volume": "main",
    "abstract": "Event Extraction (EE) is a challenging task that aims to extract structural event-related information from unstructured text. Traditional methods for EE depend on manual annotations, which are both expensive and scarce. Furthermore, the existing datasets mostly follow the long-tail distribution, severely hindering the previous methods of modeling tail types. Two techniques can address this issue: transfer learning and data generation. However, the existing methods based on transfer learning still rely on pre-training with a large amount of labeled data in the source domain. Additionally, the quality of data generated by previous data generation methods is difficult to control. In this paper, leveraging Large Language Models (LLMs), we propose novel methods for event extraction and generation based on dialogues, overcoming the problems of relying on source domain data and maintaining data quality. Specifically, this paper innovatively transforms the EE task into multi-turn dialogues, guiding LLMs to learn event schemas from historical dialogue information and output structural events. Furthermore, we introduce a novel LLM-based method for generating high-quality data, significantly improving traditional models' performance with various paradigms and structures, especially on tail types. Adequate experiments on real-world datasets demonstrate the effectiveness of the proposed event extraction and data generation methods",
    "checked": true,
    "id": "5d2f163cedf351ffae2dee568e7156d90947ad65",
    "semantic_title": "emancipating event extraction from the constraints of long-tailed distribution data utilizing large language models",
    "citation_count": 0,
    "authors": [
      "Zhigang Kan",
      "Liwen Peng",
      "Linbo Qiao",
      "Dongsheng Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.502": {
    "title": "EMOLIS App and Dataset to Find Emotionally Close Cartoons",
    "volume": "main",
    "abstract": "We propose EMOLIS Dataset that contains annotated emotional transcripts of scenes from Walt Disney cartoons at the same time as physiological signals from spectators (breathing, ECG, eye movements). The dataset is used in EMOLIS App, our second proposal. EMOLIS App allows to display the identified emotions while a video is playing and suggest emotionally comparable videos. We propose to estimate an emotional distance between videos using multimodal neural representations (text, audio, video) that also combine physiological signals. This enables personalized results that can be used for cognitive therapies focusing on awareness of felt emotions. The dataset is designed to be suitable for all audiences and autistic people who have difficulties to recognize and express emotions",
    "checked": true,
    "id": "4af0d348460445a2d5609e87d1deeb7f09b56142",
    "semantic_title": "emolis app and dataset to find emotionally close cartoons",
    "citation_count": 0,
    "authors": [
      "Soëlie Lerch",
      "Patrice Bellot",
      "Elisabeth Murisasco",
      "Emmanuel Bruno"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.503": {
    "title": "EmoProgress: Cumulated Emotion Progression Analysis in Dreams and Customer Service Dialogues",
    "volume": "main",
    "abstract": "Emotion analysis often involves the categorization of isolated textual units, but these are parts of longer discourses, like dialogues or stories. This leads to two different established emotion classification setups: (1) Classification of a longer text into one or multiple emotion categories. (2) Classification of the parts of a longer text (sentences or utterances), either (2a) with or (2b) without consideration of the context. None of these settings, does, however, enable to answer the question which emotion is presumably experienced at a specific moment in time. For instance, a customer's request of \"My computer broke.\" would be annotated with anger. This emotion persists in a potential follow-up reply \"It is out of warranty.\" which would also correspond to the global emotion label. An alternative reply \"We will send you a new one.\" might, in contrast, lead to relief. Modeling these label relations requires classification of textual parts under consideration of the past, but without access to the future. Consequently, we propose a novel annotation setup for emotion categorization corpora, in which the annotations reflect the emotion up to the annotated sentence. We ensure this by uncovering the textual parts step-by-step to the annotator, asking for a label in each step. This perspective is important to understand the final, global emotion, while having access to the individual sentence's emotion contributions to this final emotion. In modeling experiments, we use these data to check if the context is indeed required to automatically predict such cumulative emotion progressions",
    "checked": true,
    "id": "109a1c60af9c9fe6553870b43357be4690aa24be",
    "semantic_title": "emoprogress: cumulated emotion progression analysis in dreams and customer service dialogues",
    "citation_count": 0,
    "authors": [
      "Eileen Wemmer",
      "Sofie Labat",
      "Roman Klinger"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.504": {
    "title": "EmoPrompt-ECPE: Emotion Knowledge-aware Prompt-tuning for Emotion-Cause Pair Extraction",
    "volume": "main",
    "abstract": "Emotion-cause pair extraction (ECPE) main focus is on extracting all potential emotion clauses and corresponding cause clauses from unannotated documents. Existing methods achieve promising results with the help of fine-tuning and prompt paradigms, but they present three downsides. First, most approaches cannot distinguish between the emotion-cause pairs that belong to different types of emotions, limiting the existing approaches' applicability. Second, existing prompt methods utilize a one-to-one mapping relation to achieve label words to category mapping, which brings considerable bias to the results. Third, existing methods achieve the cause extraction task supported by explicit semantic understanding or basic prompt templates, ignoring the implicit information contained in the cause clauses themselves. To solve these issues, we propose an Emotion knowledge-aware Prompt-tuning for Emotion-Cause Pair Extraction (EmoPrompt-ECPE) method, which integrate the knowledge of emotion categories in the ECPE task and mine the implicit knowledge of cause clauses. Specifically, we inject the latent knowledge of the cause clauses and the emotion types into the prompt template. Besides, we extend the emotion labels for many-to-one mapping of label words to categories with an external emotion word base. Furthermore, we utilize the cosine similarity filtering of the label word base to reduce the noise caused by knowledge introduction. Experiments on both Chinese and English benchmark datasets show that our approach can achieve state-of-the-art results. Our code and data can be found at: https://github.com/xy-xiaotudou/EmoPrompt-ECPE",
    "checked": true,
    "id": "d3d10cf3a74d552618ea7671db20c95ffde8faf1",
    "semantic_title": "emoprompt-ecpe: emotion knowledge-aware prompt-tuning for emotion-cause pair extraction",
    "citation_count": 0,
    "authors": [
      "Xue Gu",
      "Zhihan Zhou",
      "Ziyao Meng",
      "Jian Li",
      "Tiago Gomes",
      "Adriano Tavares",
      "Hao Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.505": {
    "title": "Emotags: Computer-Assisted Verbal Labelling of Expressive Audiovisual Utterances for Expressive Multimodal TTS",
    "volume": "main",
    "abstract": "We developped a web app for ascribing verbal descriptions to expressive audiovisual utterances. These descriptions are limited to lists of adjectives that are either suggested via a navigation in emotional latent spaces built using discriminant analysis of BERT embeddings or entered freely by subjects. We show that such verbal descriptions collected on-line via Prolific on massive data (310 participants, 12620 labelled utterances up-to-now) provide Expressive Multimodal Text-to-Speech Synthesis with precise verbal control over desired emotional content",
    "checked": true,
    "id": "de6ad035a33018cb210ab55e9c1071c16b61302e",
    "semantic_title": "emotags: computer-assisted verbal labelling of expressive audiovisual utterances for expressive multimodal tts",
    "citation_count": 0,
    "authors": [
      "Gérard Bailly",
      "Romain Legrand",
      "Martin Lenglet",
      "Frédéric Elisei",
      "Maëva Hueber",
      "Olivier Perrotin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.506": {
    "title": "Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions",
    "volume": "main",
    "abstract": "Emotions are a central aspect of communication. Consequently, emotion analysis (EA) is a rapidly growing field in natural language processing (NLP). However, there is no consensus on scope, direction, or methods. In this paper, we conduct a thorough review of 154 relevant NLP publications from the last decade. Based on this review, we address four different questions: (1) How are EA tasks defined in NLP? (2) What are the most prominent emotion frameworks and which emotions are modeled? (3) Is the subjectivity of emotions considered in terms of demographics and cultural factors? and (4) What are the primary NLP applications for EA? We take stock of trends in EA and tasks, emotion frameworks used, existing datasets, methods, and applications. We then discuss four lacunae: (1) the absence of demographic and cultural aspects does not account for the variation in how emotions are perceived, but instead assumes they are universally experienced in the same manner; (2) the poor fit of emotion categories from the two main emotion theories to the task; (3) the lack of standardized EA terminology hinders gap identification, comparison, and future goals; and (4) the absence of interdisciplinary research isolates EA from insights in other fields. Our work will enable more focused research into EA and a more holistic approach to modeling emotions in NLP",
    "checked": true,
    "id": "9679cff6484d01d47b869687f5a5aefccfa0c1b2",
    "semantic_title": "emotion analysis in nlp: trends, gaps and roadmap for future directions",
    "citation_count": 0,
    "authors": [
      "Flor Miriam Plaza-del-Arco",
      "Alba A. Cercas Curry",
      "Amanda Cercas Curry",
      "Dirk Hovy"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.507": {
    "title": "Emotion Recognition in Conversation via Dynamic Personality",
    "volume": "main",
    "abstract": "Emotion recognition in conversation (ERC) is a field that aims to classify the emotion of each utterance within conversational contexts. This presents significant challenges, particularly in handling emotional ambiguity across various speakers and contextual factors. Existing ERC approaches have primarily focused on modeling conversational contexts while incorporating only superficial speaker attributes such as names, memories, and interactions. Recent works introduce personality as an essential deep speaker factor for emotion recognition, but relies on static personality, overlooking dynamic variability during conversations. Advances in personality psychology conceptualize personality as dynamic, proposing that personality states can change across situations. In this paper, we introduce ERC-DP, a novel model considering the dynamic personality of speakers during conversations. ERC-DP accounts for past utterances from the same speaker as situation impacting dynamic personality. It combines personality modeling with prompt design and fine-grained classification modules. Through a series of comprehensive experiments, ERC-DP demonstrates superior performance on three benchmark conversational datasets",
    "checked": true,
    "id": "df8ff4ebc469834eb3396d849da6adbd0ca751c9",
    "semantic_title": "emotion recognition in conversation via dynamic personality",
    "citation_count": 0,
    "authors": [
      "Yan Wang",
      "Bo Wang",
      "Yachao Zhao",
      "Dongming Zhao",
      "Xiaojia Jin",
      "Jijun Zhang",
      "Ruifang He",
      "Yuexian Hou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.508": {
    "title": "EmoTrans: Emotional Transition-based Model for Emotion Recognition in Conversation",
    "volume": "main",
    "abstract": "In an emotional conversation, emotions are causally transmitted among communication participants, constituting a fundamental conversational feature that can facilitate the comprehension of intricate changes in emotional states during the conversation and contribute to neutralizing emotional semantic bias in utterance caused by the absence of modality information. Therefore, emotional transition (ET) plays a crucial role in the task of Emotion Recognition in Conversation (ERC) that has not received sufficient attention in current research. In light of this, an Emotional Transition-based Emotion Recognizer (EmoTrans) is proposed in this paper. Specifically, we concatenate the most recent utterances with their corresponding speakers to construct the model input, known as samples, each with several placeholders to implicitly express the emotions of contextual utterances. Based on these placeholders, two components are developed to make the model sensitive to emotions and effectively capture the ET features in the sample. Furthermore, an ET-based Contrastive Learning (CL) is developed to compact the representation space, making the model achieve more robust sample representations. We conducted exhaustive experiments on four widely used datasets and obtained competitive experimental results, especially, new state-of-the-art results obtained on MELD and IEMOCAP, demonstrating the superiority of EmoTrans",
    "checked": true,
    "id": "96ab09fe04b942b3682cdb79160a862803559c96",
    "semantic_title": "emotrans: emotional transition-based model for emotion recognition in conversation",
    "citation_count": 0,
    "authors": [
      "Zhongquan Jian",
      "Ante Wang",
      "Jinsong Su",
      "Junfeng Yao",
      "Meihong Wang",
      "Qingqiang Wu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.509": {
    "title": "EmpCRL: Controllable Empathetic Response Generation via In-Context Commonsense Reasoning and Reinforcement Learning",
    "volume": "main",
    "abstract": "Empathetic response generation aims to understand the user's feelings emotionally and generate responses with appropriate emotion. According to psychological theories, empathy consists of two main aspects: affection and cognition. However, existing works lack the perception of fine-grained dialogue emotion propagation, as well as have limitations in reasoning about the intentions of users on cognition, which affect the quality of empathetic response. To this end, we propose to generate Empathetic response based on in-context Commonsense reasoning and Reinforcement Learning (EmpCRL). First, we use a current popular large language model combined with multi-view contextual reasoning to broaden the cognitive boundaries through in-context learning. Furthermore, we infer the response emotion by jointly modeling the dialogue history and emotion flow, and achieve the control of response emotion and diversity through reinforcement learning. Extensive experiments on EmpatheticDialogues dataset show that our model outperforms state-of-the-art models in both automatic and human evaluation",
    "checked": true,
    "id": "2db4515ddb85016351e2d5b4a1930637c4c689f9",
    "semantic_title": "empcrl: controllable empathetic response generation via in-context commonsense reasoning and reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Mingxiu Cai",
      "Daling Wang",
      "Shi Feng",
      "Yifei Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.510": {
    "title": "Empowering Low-Resource Regional Languages with Lexicons : A Comparative Study of NLP Tools for Morphosyntactic Analysis",
    "volume": "main",
    "abstract": "We investigate the effect of integrating lexicon information to an extremely low-resource language when annotated data is scarce for morpho-syntactic analysis. Obtaining such data and linguistic resources for these languages are usually constrained by a lack of human and financial resources making this task particularly challenging. In this paper, we describe the collection and leverage of a bilingual lexicon for Poitevin-Saintongeais, a regional language of France, to create augmented data through a neighbor-based distributional method. We assess this lexicon-driven approach in improving POS tagging while using different lexicon and augmented data sizes. To evaluate this strategy, we compare two distinct paradigms: neural networks, which typically require extensive data, and a conventional probabilistic approach, in which a lexicon is instrumental in its performance. Our findings reveal that the lexicon is a valuable asset for all models, but in particular for neural, demonstrating an enhanced generalization across diverse classes without requiring an extensive lexicon size",
    "checked": true,
    "id": "1229b4b82dcd59bc2c276589594288b207552cfa",
    "semantic_title": "empowering low-resource regional languages with lexicons : a comparative study of nlp tools for morphosyntactic analysis",
    "citation_count": 0,
    "authors": [
      "Cristina Garcia Holgado",
      "Marianne Vergez-Couret"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.511": {
    "title": "Empowering Oneida Language Revitalization: Development of an Oneida Verb Conjugator",
    "volume": "main",
    "abstract": "In this paper, we present the development of a digital Oneida verb conjugator through using the Gramble framework. This project is a collaborative effort with the Twatati Adult Oneida Language program. Oneida is a polysynthetic North American Indigenous language. Its verb roots can be conjugated with multiple affixes, and long verbal complexes can be used as utterances. Each Oneida affix encodes important grammatical information, and its form often varies based on various factors, such as its position in the utterance and its phonological environment. The distinct morphosyntactic structures complicate acquisition of the language by learners who are native speakers of English. With an alarmingly small number of native speakers of Oneida, supporting and accelerating adult second language leaners' acquisition process has become a pressing necessity. The Oneida verb conjugator can demonstrate its users the correct conjugations of verbs and can also let learners generate practice materials tailored to their unique learning trajectories. This paper presents the preliminary stages and outcomes of the project and outlines the areas for improvement to be addressed in our subsequent endeavors",
    "checked": true,
    "id": "eb47a7eb7bab131ceb9d85b4d86d3d7e5e3d2c8d",
    "semantic_title": "empowering oneida language revitalization: development of an oneida verb conjugator",
    "citation_count": 1,
    "authors": [
      "Yanfei Lu",
      "Patrick Littell",
      "Keren Rice"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.512": {
    "title": "Empowering Small-Scale Knowledge Graphs: A Strategy of Leveraging General-Purpose Knowledge Graphs for Enriched Embeddings",
    "volume": "main",
    "abstract": "Knowledge-intensive tasks pose a significant challenge for Machine Learning (ML) techniques. Commonly adopted methods, such as Large Language Models (LLMs), often exhibit limitations when applied to such tasks. Nevertheless, there have been notable endeavours to mitigate these challenges, with a significant emphasis on augmenting LLMs through Knowledge Graphs (KGs). While KGs provide many advantages for representing knowledge, their development costs can deter extensive research and applications. Addressing this limitation, we introduce a framework for enriching embeddings of small-scale domain-specific Knowledge Graphs with well-established general-purpose KGs. Adopting our method, a modest domain-specific KG can benefit from a performance boost in downstream tasks when linked to a substantial general-purpose KG. Experimental evaluations demonstrate a notable enhancement, with up to a 44% increase observed in the Hits@10 metric. This relatively unexplored research direction can catalyze more frequent incorporation of KGs in knowledge-intensive tasks, resulting in more robust, reliable ML implementations, which hallucinates less than prevalent LLM solutions",
    "checked": true,
    "id": "6fcb834ac66cde71d50a54f83286bf4cbd92eebd",
    "semantic_title": "empowering small-scale knowledge graphs: a strategy of leveraging general-purpose knowledge graphs for enriched embeddings",
    "citation_count": 0,
    "authors": [
      "Albert Sawczyn",
      "Jakub Binkowski",
      "Piotr Bielak",
      "Tomasz Kajdanowicz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.513": {
    "title": "Empowering Tree-structured Entailment Reasoning: Rhetorical Perception and LLM-driven Interpretability",
    "volume": "main",
    "abstract": "The study delves into the construction of entailment trees for science question answering (SQA), employing a novel framework termed Tree-structured Entailment Reasoning (TER). Current research on entailment tree construction presents significant challenges, primarily due to the ambiguities and similarities among candidate science facts, which considerably complicate the fact retrieval process. Moreover, the existing models exhibit limitations in effectively modeling the sequence of reasoning states, understanding the intricate relations between neighboring entailment tree nodes, and generating intermediate conclusions. To this end, we explore enhancing the TER performance from three aspects: First, improving retrieval capabilities by modeling and referring to the chained reasoning states; Second, enhancing TER by infusing knowledge that bridges the gap between reasoning types and rhetorical relations. Third, exploring a task-specific large language model tuning scheme to mitigate deficiencies in intermediate conclusion generation. Experiments on the English EntailmentBank demonstrate the effectiveness of the proposed methods in augmenting the quality of tree-structured entailment reasoning to a certain extent",
    "checked": true,
    "id": "23b7efdeb2e2ec5cddb0c0d68a363ec218447546",
    "semantic_title": "empowering tree-structured entailment reasoning: rhetorical perception and llm-driven interpretability",
    "citation_count": 0,
    "authors": [
      "Longyin Zhang",
      "Bowei Zou",
      "Ai Ti Aw"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.514": {
    "title": "Emstremo: Adapting Emotional Support Response with Enhanced Emotion-Strategy Integrated Selection",
    "volume": "main",
    "abstract": "To provide effective support, it is essential for a skilled supporter to emotionally resonate with the help-seeker's current emotional state. In conversational interactions, this emotional alignment is further influenced by the comforting strategies employed by the supporter. Different strategies guide the interlocutors to align their emotions in nuanced patterns. However, the incorporation of strategy into emotional alignment in the context of emotional support agents remains underexplored. To address this limitation, we propose an improved emotional support agent called Emstremo. Emstremo aims to achieve strategic control of emotional alignment by perceiving and responding to the user's emotions. Our system's state-of-the-art performance emphasizes the importance of integrating emotions and strategies in modeling conversations that provide emotional support",
    "checked": true,
    "id": "9c621566ee421d7bbc0f22b72262d49086fb71ac",
    "semantic_title": "emstremo: adapting emotional support response with enhanced emotion-strategy integrated selection",
    "citation_count": 0,
    "authors": [
      "Junlin Li",
      "Bo Peng",
      "Yu-Yin Hsu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.515": {
    "title": "Encoding Gesture in Multimodal Dialogue: Creating a Corpus of Multimodal AMR",
    "volume": "main",
    "abstract": "Abstract Meaning Representation (AMR) is a general-purpose meaning representation that has become popular for its clear structure, ease of annotation and available corpora, and overall expressiveness. While AMR was designed to represent sentence meaning in English text, recent research has explored its adaptation to broader domains, including documents, dialogues, spatial information, cross-lingual tasks, and gesture. In this paper, we present an annotated corpus of multimodal (speech and gesture) AMR in a task-based setting. Our corpus is multilayered, containing temporal alignments to both the speech signal and to descriptions of gesture morphology. We also capture coreference relationships across modalities, enabling fine-grained analysis of how the semantics of gesture and natural language interact. We discuss challenges that arise when identifying cross-modal coreference and anaphora, as well as in creating and evaluating multimodal corpora in general. Although we find AMR's abstraction away from surface form (in both language and gesture) occasionally too coarse-grained to capture certain cross-modal interactions, we believe its flexibility allows for future work to fill in these gaps. Our corpus and annotation guidelines are available at https://github.com/klai12/encoding-gesture-multimodal-dialogue",
    "checked": true,
    "id": "a6411dde2e73fa2f0418a351ac31312f28578e2d",
    "semantic_title": "encoding gesture in multimodal dialogue: creating a corpus of multimodal amr",
    "citation_count": 0,
    "authors": [
      "Kenneth Lai",
      "Richard Brutti",
      "Lucia Donatelli",
      "James Pustejovsky"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.516": {
    "title": "Endowing Neural Language Learners with Human-like Biases: A Case Study on Dependency Length Minimization",
    "volume": "main",
    "abstract": "Natural languages show a tendency to minimize the linear distance between heads and their dependents in a sentence, known as dependency length minimization (DLM). Such a preference, however, has not been consistently replicated with neural agent simulations. Comparing the behavior of models with that of human learners can reveal which aspects affect the emergence of this phenomenon. In this work, we investigate the minimal conditions that may lead neural learners to develop a DLM preference. We add three factors to the standard neural-agent language learning and communication framework to make the simulation more realistic, namely: (i) the presence of noise during listening, (ii) context-sensitivity of word use through non-uniform conditional word distributions, and (iii) incremental sentence processing, or the extent to which an utterance's meaning can be guessed before hearing it entirely. While no preference appears in production, we show that the proposed factors can contribute to a small but significant learning advantage of DLM for listeners of verb-initial languages",
    "checked": true,
    "id": "e9901506c63f72a74bcf25630d7ce435e08291dc",
    "semantic_title": "endowing neural language learners with human-like biases: a case study on dependency length minimization",
    "citation_count": 0,
    "authors": [
      "Yuqing Zhang",
      "Tessa Verhoef",
      "Gertjan van Noord",
      "Arianna Bisazza"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.517": {
    "title": "End-to-end Parsing of Procedural Text into Flow Graphs",
    "volume": "main",
    "abstract": "We focus on the problem of parsing procedural text into fine-grained flow graphs that encode actions and entities, as well as their interactions. Specifically, we focus on parsing cooking recipes, and address a few limitations of existing parsers. Unlike SOTA approaches to flow graph parsing that work in two separate stages identifying actions and entities (tagging) and encoding their interactions via connecting edges (graph generation). we propose an end-to-end multi-task framework that simultaneously performs tagging and graph generation. In addition, due to the end-to-end nature of our proposed model, we can unify the input representation, and moreover can use compact encoders, resulting in small models with significantly fewer parameters than SOTA models. Another key challenge in training flow graph parsers is the lack of sufficient annotated data, due to the costly nature of the fine-grained annotations. We address this problem by taking advantage of the abundant unlabelled recipes, and show that pre-training on automatically-generated noisy silver annotations (from unlabelled recipes) results in a large improvement in flow graph parsing",
    "checked": true,
    "id": "2ccba0b59af366e372b535f039d64911322d363b",
    "semantic_title": "end-to-end parsing of procedural text into flow graphs",
    "citation_count": 0,
    "authors": [
      "Dhaivat J. Bhatt",
      "Seyed Ahmad Abdollahpouri Hosseini",
      "Federico Fancellu",
      "Afsaneh Fazly"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.518": {
    "title": "Enhanced Coherence-Aware Network with Hierarchical Disentanglement for Aspect-Category Sentiment Analysis",
    "volume": "main",
    "abstract": "Aspect-category-based sentiment analysis (ACSA), which aims to identify aspect categories and predict their sentiments has been intensively studied due to its wide range of NLP applications. Most approaches mainly utilize intrasentential features. However, a review often includes multiple different aspect categories, and some of them do not explicitly appear in the review. Even in a sentence, there is more than one aspect category with its sentiments, and they are entangled intra-sentence, which makes the model fail to discriminately preserve all sentiment characteristics. In this paper, we propose an enhanced coherence-aware network with hierarchical disentanglement (ECAN) for ACSA tasks. Specifically, we explore coherence modeling to capture the contexts across the whole review and to help the implicit aspect and sentiment identification. To address the issue of multiple aspect categories and sentiment entanglement, we propose a hierarchical disentanglement module to extract distinct categories and sentiment features. Extensive experimental and visualization results show that our ECAN effectively decouples multiple categories and sentiments entangled in the coherence representations and achieves state-of-the-art (SOTA) performance. Our codes and data are available online: https://github.com/cuijin-23/ECAN",
    "checked": true,
    "id": "54f031908e808e59aaac8da9552db3fb51e96bd6",
    "semantic_title": "enhanced coherence-aware network with hierarchical disentanglement for aspect-category sentiment analysis",
    "citation_count": 0,
    "authors": [
      "Jin Cui",
      "Fumiyo Fukumoto",
      "Xinfeng Wang",
      "Yoshimi Suzuki",
      "Jiyi Li",
      "Noriko Tomuro",
      "Wanzeng Kong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.519": {
    "title": "Enhanced Facet Generation with LLM Editing",
    "volume": "main",
    "abstract": "In information retrieval, facet identification of a user query is an important task. If a search service can recognize the facets of a user's query, it has the potential to offer users a much broader range of search results. Previous studies can enhance facet prediction by leveraging retrieved documents and related queries obtained through a search engine. However, there are challenges in extending it to other applications when a search engine operates as part of the model. First, search engines are constantly updated. Therefore, additional information may change during training and test, which may reduce performance. The second challenge is that public search engines cannot search for internal documents. Therefore, a separate search system needs to be built to incorporate documents from private domains within the company. We propose two strategies that focus on a framework that can predict facets by taking only queries as input without a search engine. The first strategy is multi-task learning to predict SERP. By leveraging SERP as a target instead of a source, the proposed model deeply understands queries without relying on external modules. The second strategy is to enhance the facets by combining Large Language Model (LLM) and the small model. Overall performance improves when small model and LLM are combined rather than facet generation individually",
    "checked": true,
    "id": "7b84561c35826e710eb32f51aa8e5ff1e913e7d0",
    "semantic_title": "enhanced facet generation with llm editing",
    "citation_count": 0,
    "authors": [
      "Joosung Lee",
      "Jinhong Kim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.520": {
    "title": "Enhance Robustness of Language Models against Variation Attack through Graph Integration",
    "volume": "main",
    "abstract": "The widespread use of pre-trained language models (PLMs) in natural language processing (NLP) has greatly improved performance outcomes. However, these models' vulnerability to adversarial attacks (e.g., camouflaged hints from drug dealers), particularly in the Chinese language with its rich character diversity/variation and complex structures, hatches vital apprehension. In this study, we propose a novel method, CHinese vAriatioN Graph Enhancement (CHANGE), to increase the robustness of PLMs against character variation attacks in Chinese content. CHANGE presents a novel approach to incorporate a Chinese character variation graph into the PLMs. Through designing different supplementary tasks utilizing the graph structure, CHANGE essentially enhances PLMs' interpretation of adversarially manipulated text. Experiments conducted in a multitude of NLP tasks show that CHANGE outperforms current language models in combating against adversarial attacks and serves as a valuable contribution to robust language model research. Moreover, these findings highlight the substantial potential of graph-guided pre-training strategies for real-world applications",
    "checked": true,
    "id": "06423ebe5360f368275cae8b9f75c39d192cc2bc",
    "semantic_title": "enhance robustness of language models against variation attack through graph integration",
    "citation_count": 0,
    "authors": [
      "Zi Xiong",
      "Lizhi Qing",
      "Yangyang Kang",
      "Jiawei Liu",
      "Hongsong Li",
      "Changlong Sun",
      "Xiaozhong Liu",
      "Wei Lu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.521": {
    "title": "Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning Ability of LLMs",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have recently made significant advances in code generation through the ‘Chain-of-Thought' prompting technique. This technique empowers the model to autonomously devise \"solution plans\" to tackle intricate programming challenges, thereby improving its performance in code generation. Nevertheless, smaller models have been struggling to keep up with LLMs in deducing these plans, adversely affecting their code generation capabilities. Given the considerable size and associated deployment costs, along with concerns about data security, many teams opt for deploying smaller models for code generation. Consequently, there arises a compelling need for transferring LLMs' code generation reasoning abilities to the smaller models. In this paper, we propose the CodePLAN framework, which aims to transfer LLMs' reasoning capabilities to smaller models through distillation. We adopt a multi-task learning approach, jointly undertaking code generation and solution plan generation tasks, to enhance the code generation capabilities of smaller model. To ensure the superior quality of the solution plans, we advocate for the utilization of backward reasoning and plan sampling strategies. Our experiments show that in comparison to the conventional fine-tuning approach, our approach improves the smaller model's code generation performance (measured in pass@1 metric) by over 130% on the challenging APPS benchmark",
    "checked": true,
    "id": "9c56d82f6b7dcd79705a9e6e5bc784681acf4f78",
    "semantic_title": "enhancing code generation performance of smaller models by distilling the reasoning ability of llms",
    "citation_count": 0,
    "authors": [
      "Zhihong Sun",
      "Chen Lyu",
      "Bolun Li",
      "Yao Wan",
      "Hongyu Zhang",
      "Ge Li",
      "Zhi Jin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.522": {
    "title": "Enhancing Court View Generation with Knowledge Injection and Guidance",
    "volume": "main",
    "abstract": "Court View Generation (CVG) is a challenging task in the field of Legal Artificial Intelligence (LegalAI), which aims to generate court views based on the plaintiff claims and the fact descriptions. While Pretrained Language Models (PLMs) have showcased their prowess in natural language generation, their application to the complex, knowledge-intensive domain of CVG often reveals inherent limitations. In this paper, we present a novel approach, named Knowledge Injection and Guidance (KIG), designed to bolster CVG using PLMs. To efficiently incorporate domain knowledge during the training stage, we introduce a knowledge-injected prompt encoder for prompt tuning, thereby reducing computational overhead. Moreover, to further enhance the model's ability to utilize domain knowledge, we employ a generating navigator, which dynamically guides the text generation process in the inference stage without altering the model's architecture, making it readily transferable. Comprehensive experiments on real-world data demonstrate the effectiveness of our approach compared to several established baselines, especially in the responsivity of claims, where it outperforms the best baseline by 11.87%",
    "checked": true,
    "id": "f40853cd389df06c959b8e06ad9db60569b37707",
    "semantic_title": "enhancing court view generation with knowledge injection and guidance",
    "citation_count": 0,
    "authors": [
      "Ang Li",
      "Yiquan Wu",
      "Yifei Liu",
      "Kun Kuang",
      "Fei Wu",
      "Ming Cai"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.523": {
    "title": "Enhancing Cross-Document Event Coreference Resolution by Discourse Structure and Semantic Information",
    "volume": "main",
    "abstract": "Existing cross-document event coreference resolution models, which either compute mention similarity directly or enhance mention representation by extracting event arguments (such as location, time, agent, and patient), lackingmthe ability to utilize document-level information. As a result, they struggle to capture long-distance dependencies. This shortcoming leads to their underwhelming performance in determining coreference for the events where their argument information relies on long-distance dependencies. In light of these limitations, we propose the construction of document-level Rhetorical Structure Theory (RST) trees and cross-document Lexical Chains to model the structural and semantic information of documents. Subsequently, cross-document heterogeneous graphs are constructed and GAT is utilized to learn the representations of events. Finally, a pair scorer calculates the similarity between each pair of events and co-referred events can be recognized using standard clustering algorithm. Additionally, as the existing cross-document event coreference datasets are limited to English, we have developed a large-scale Chinese cross-document event coreference dataset to fill this gap, which comprises 53,066 event mentions and 4,476 clusters. After applying our model on the English and Chinese datasets respectively, it outperforms all baselines by large margins",
    "checked": true,
    "id": "22d2d1d2f5e3c31b23d9645a44b891d1aeeb1123",
    "semantic_title": "enhancing cross-document event coreference resolution by discourse structure and semantic information",
    "citation_count": 0,
    "authors": [
      "Qiang Gao",
      "Bobo Li",
      "Zixiang Meng",
      "Yunlong Li",
      "Jun Zhou",
      "Fei Li",
      "Chong Teng",
      "Donghong Ji"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.524": {
    "title": "Enhancing Distantly Supervised Named Entity Recognition with Strong Label Guided Lottery Training",
    "volume": "main",
    "abstract": "In low-resource Named Entity Recognition (NER) scenarios, only a limited quantity of strongly labeled data is available, while a vast amount of weakly labeled data can be easily acquired through distant supervision. However, weakly labeled data may fail to improve the model performance or even harm it due to the inevitable noise. While training on noisy data, only certain parameters are essential for model learning, termed safe parameters, whereas the other parameters tend to fit noise. In this paper, we propose a noise-robust learning framework where safe parameters can be identified with guidance from the small set of strongly labeled data, and non-safe parameters are suppressed during training on weakly labeled data for better generalization. Our method can effectively mitigate the impact of noise in weakly labeled data, and it can be easily integrated with data level noise-robust learning methods for NER. We conduct extensive experiments on multiple datasets and the results show that our approach outperforms the state-of-the-art methods",
    "checked": true,
    "id": "3a9d4102fd038c4827046243578dffa2401b4c7c",
    "semantic_title": "enhancing distantly supervised named entity recognition with strong label guided lottery training",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Ma",
      "Jintao Du",
      "Changhua Meng",
      "Weiqiang Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.525": {
    "title": "Enhancing Effectiveness and Robustness in a Low-Resource Regime via Decision-Boundary-aware Data Augmentation",
    "volume": "main",
    "abstract": "Efforts to leverage deep learning models in low-resource regimes have led to numerous augmentation studies. However, the direct application of methods, such as mixup and cutout, is limited due to the discrete characteristics of the textual data. While methods using pre trained language models have exhibited good efficiency, they require additional considerations for robustness. Inspired by recent studies on decision boundaries, this paper proposes a decision-boundary-aware data augmentation strategy to enhance robustness using pretrained language models. The proposed technique first focuses on shifting the latent features closer to the decision boundary, followed by reconstruction to generate an ambiguous version with a soft label. Additionally, mid-K sampling is suggested to enhance the diversity of the generated sentences. This paper demonstrates the performance of the proposed augmentation strategy compared to other methods through extensive experiments. Furthermore, the ablation study demonstrates the effect of soft labels and mid-K sampling and the extensibility of the method with curriculum data augmentation",
    "checked": true,
    "id": "65212978e20c296b1b131fab05f81071214054c8",
    "semantic_title": "enhancing effectiveness and robustness in a low-resource regime via decision-boundary-aware data augmentation",
    "citation_count": 0,
    "authors": [
      "Kyohoon Jin",
      "Junho Lee",
      "Juhwan Choi",
      "Sangmin Song",
      "Youngbin Kim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.526": {
    "title": "Enhancing Emotion Prediction in News Headlines: Insights from ChatGPT and Seq2Seq Models for Free-Text Generation",
    "volume": "main",
    "abstract": "Predicting emotions elicited by news headlines can be challenging as the task is largely influenced by the varying nature of people's interpretations and backgrounds. Previous works have explored classifying discrete emotions directly from news headlines. We provide a different approach to tackling this problem by utilizing people's explanations of their emotion, written in free-text, on how they feel after reading a news headline. Using the dataset BU-NEmo+ (Gao et al., 2022), we found that for emotion classification, the free-text explanations have a strong correlation with the dominant emotion elicited by the headlines. The free-text explanations also contain more sentimental context than the news headlines alone and can serve as a better input to emotion classification models. Therefore, in this work we explored generating emotion explanations from headlines by training a sequence-to-sequence transformer model and by using pretrained large language model, ChatGPT (GPT-4). We then used the generated emotion explanations for emotion classification. In addition, we also experimented with training the pretrained T5 model for the intermediate task of explanation generation before fine-tuning it for emotion classification. Using McNemar's significance test, methods that incorporate GPT-generated free-text emotion explanations demonstrated significant improvement (P-value < 0.05) in emotion classification from headlines, compared to methods that only use headlines. This underscores the value of using intermediate free-text explanations for emotion prediction tasks with headlines",
    "checked": true,
    "id": "f58207d75838a6f719ec07b5b54ca09ada7bd226",
    "semantic_title": "enhancing emotion prediction in news headlines: insights from chatgpt and seq2seq models for free-text generation",
    "citation_count": 0,
    "authors": [
      "Ge Gao",
      "Jongin Kim",
      "Sejin Paik",
      "Ekaterina Novozhilova",
      "Yi Liu",
      "Sarah T. Bonna",
      "Margrit Betke",
      "Derry Tanti Wijaya"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.527": {
    "title": "Enhancing Few-Shot Topic Classification with Verbalizers. a Study on Automatic Verbalizer and Ensemble Methods",
    "volume": "main",
    "abstract": "As pretrained language model emerge and consistently develop, prompt-based training has become a well-studied paradigm to improve the exploitation of models for many natural language processing tasks. Furthermore, prompting demonstrates great performance compared to conventional fine-tuning in scenarios with limited annotated data, such as zero-shot or few-shot situations. Verbalizers are crucial in this context, as they help interpret masked word distributions generated by language models into output predictions. This study introduces a benchmarking approach to assess three common baselines of verbalizers for topic classification in few-shot learning scenarios. Additionally, we find that increasing the number of label words for automatic label word searching enhances model performance. Moreover, we investigate the effectiveness of template assembling with various aggregation strategies to develop stronger classifiers that outperform models trained with individual templates. Our approach achieves comparable results to prior research while using significantly fewer resources. Our code is available at https://github.com/quang-anh-nguyen/verbalizer_benchmark.git",
    "checked": true,
    "id": "60cfcd8b702c48d8e16b345328bab8527dac567c",
    "semantic_title": "enhancing few-shot topic classification with verbalizers. a study on automatic verbalizer and ensemble methods",
    "citation_count": 0,
    "authors": [
      "Quang Anh Nguyen",
      "Nadi Tomeh",
      "Mustapha Lebbah",
      "Thierry Charnois",
      "Hanene Azzag",
      "Santiago Cordoba Muñoz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.528": {
    "title": "Enhancing Hindi Feature Representation through Fusion of Dual-Script Word Embeddings",
    "volume": "main",
    "abstract": "Pretrained language models excel in various natural language processing tasks but often neglect the integration of different scripts within a language, constraining their ability to capture richer semantic information, such as in Hindi. In this work, we present a dual-script enhanced feature representation method for Hindi. We combine single-script features from Devanagari and Romanized Hindi Roberta using concatenation, addition, cross-attention, and convolutional networks. The experiment results show that using a dual-script approach significantly improves model performance across various tasks. The addition fusion technique excels in sequence generation tasks, while for text classification, the CNN-based dual-script enhanced representation performs best with longer sentences, and the addition fusion technique is more effective for shorter sequences. Our approach shows significant advantages in multiple natural language processing tasks, providing a new perspective on feature representation for Hindi. Our code has been released on https://github.com/JohnnyChanV/Hindi-Fusion",
    "checked": true,
    "id": "ea65f28eed97bf75e96f241601ccd44ed1226b0f",
    "semantic_title": "enhancing hindi feature representation through fusion of dual-script word embeddings",
    "citation_count": 0,
    "authors": [
      "Lianxi Wang",
      "Yujia Tian",
      "Zhuowei Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.529": {
    "title": "Enhancing Image-to-Text Generation in Radiology Reports through Cross-modal Multi-Task Learning",
    "volume": "main",
    "abstract": "Image-to-text generation involves automatically generating descriptive text from images and has applications in medical report generation. However, traditional approaches often exhibit a semantic gap between visual and textual information. In this paper, we propose a multi-task learning framework to leverage both visual and non-imaging data for generating radiology reports. Along with chest X-ray images, 10 additional features comprising numeric, binary, categorical, and text data were incorporated to create a unified representation. The model was trained to generate text, predict the degree of patient severity, and identify medical findings. Multi-task learning, especially with text generation prioritisation, improved performance over single-task baselines across language generation metrics. The framework also mitigated overfitting in auxiliary tasks compared to single-task models. Qualitative analysis showed logically coherent narratives and accurate identification of findings, though some repetition and disjointed phrasing remained. This work demonstrates the benefits of multi-modal, multi-task learning for image-to-text generation applications",
    "checked": true,
    "id": "e862ef27753da3fa3bd602cbcc10fb4e4277e24d",
    "semantic_title": "enhancing image-to-text generation in radiology reports through cross-modal multi-task learning",
    "citation_count": 0,
    "authors": [
      "Nurbanu Aksoy",
      "Nishant Ravikumar",
      "Serge Sharoff"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.530": {
    "title": "Enhancing Knowledge Retrieval with Topic Modeling for Knowledge-Grounded Dialogue",
    "volume": "main",
    "abstract": "Knowledge retrieval is one of the major challenges in building a knowledge-grounded dialogue system. A common method is to use a neural retriever with a distributed approximate nearest-neighbor database to quickly find the relevant knowledge sentences. In this work, we propose an approach that utilizes topic modeling on the knowledge base to further improve retrieval accuracy and as a result, improve response generation. Additionally, we experiment with a large language model (LLM), ChatGPT, to take advantage of the improved retrieval performance to further improve the generation results. Experimental results on two datasets show that our approach can increase retrieval and generation performance. The results also indicate that ChatGPT is a better response generator for knowledge-grounded dialogue when relevant knowledge is provided",
    "checked": true,
    "id": "da0435f6acb0e7014f8d12003c7de449c382ea23",
    "semantic_title": "enhancing knowledge retrieval with topic modeling for knowledge-grounded dialogue",
    "citation_count": 0,
    "authors": [
      "Nhat Tran",
      "Diane Litman"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.531": {
    "title": "Enhancing Knowledge Selection via Multi-level Document Semantic Graph",
    "volume": "main",
    "abstract": "Knowledge selection is a crucial sub-task of Document Grounded Dialogue System. Existing methods view knowledge selection as a sentence matching or classification. However, those methods can't capture the semantic relationships within complex document. We propose a flexible method that can construct multi-level document semantic graph from the grounding document automatically and store semantic relationships within the documents effectively. Besides, we also devise an auxiliary task to leverage the graph more efficiently and can help the optimization of knowledge selection task. We conduct extensive experiments on public datasets: WoW(CITATION) and Holl-E(CITATION). And we achieves state-of-the-art result on WoW. Our code has been released at https://github.com/ddf62/multi-level-semantic-document-graph",
    "checked": true,
    "id": "d3c897a7f8a5d33ab18771c6d9220f00412282de",
    "semantic_title": "enhancing knowledge selection via multi-level document semantic graph",
    "citation_count": 0,
    "authors": [
      "Haoran Zhang",
      "Tan Yongmei"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.532": {
    "title": "Enhancing Large Language Models through Transforming Reasoning Problems into Classification Tasks",
    "volume": "main",
    "abstract": "In this paper, we introduce a novel approach for enhancing the reasoning capabilities of large language models (LLMs) for constraint satisfaction problems (CSPs), by converting reasoning problems into classification tasks. Our method leverages the LLM's ability to decide when to call a function from a set of logical-linguistic primitives, each of which can interact with a local \"scratchpad\" memory and logical inference engine. Invocation of these primitives in the correct order writes the constraints to the scratchpad memory and enables the logical engine to verifiably solve the problem. We additionally propose a formal framework for exploring the \"linguistic\" hardness of CSP reasoning-problems for LLMs. Our experimental results demonstrate that under our proposed method, tasks with significant computational hardness can be converted to a form that is easier for LLMs to solve and yields a 40% improvement over baselines. This opens up new avenues for future research into hybrid cognitive models that integrate symbolic and neural approaches",
    "checked": true,
    "id": "55a06fcf751864e698103ab6f5aa918060e48896",
    "semantic_title": "enhancing large language models through transforming reasoning problems into classification tasks",
    "citation_count": 0,
    "authors": [
      "Tarun Raheja",
      "Raunak Sinha",
      "Advit Deepak",
      "Will Healy",
      "Jayanth Srinivasa",
      "Myungjin Lee",
      "Ramana Kompella"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.533": {
    "title": "Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt. In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL. Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple text classification datasets",
    "checked": true,
    "id": "dcaf89e95478226a3caec5811a56c5a68ca35c8f",
    "semantic_title": "enhancing low-resource llms classification with peft and synthetic data",
    "citation_count": 0,
    "authors": [
      "Parth Patwa",
      "Simone Filice",
      "Zhiyu Chen",
      "Giuseppe Castellucci",
      "Oleg Rokhlenko",
      "Shervin Malmasi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.534": {
    "title": "Enhancing Parameter-efficient Fine-tuning with Simple Calibration Based on Stable Rank",
    "volume": "main",
    "abstract": "Lightweight fine-tuning is widely used as an important technique for efficiently adapting pre-trained language models (PLM) to downstream tasks. Despite the reduction in trainable parameters, existing lightweight fine-tuning methods are found to be effective in low-resource settings but often fail in high-resource settings, leading to unreliable outcomes. This limitation can be attributed to inflexible strategies: they identify the parameters of the model to be trained before fine-tuning and remain unchanged without taking into account the inherent variance of generalization ability in model components (i.e., feed-forward, attention layers) and potential changes during the fine-tuning process. In this paper, we introduce a simple but effective calibration for lightweight fine-tuning PLMs based on the matrix's stable rank according to both model components and the training process. We proposed both theoretical analyses and experimental verification for the proposed calibration strategy. Considering efficiency, we further propose time-aware and structure-aware strategies to determine the most crucial time to commence the fine-tuning procedure and selectively apply parameter matrices for lightweight fine-tuning, respectively. Extensive experiments demonstrate the superiority of our proposed fine-tuning approach (average improvement 3.1 for GLUE score compared to lightweight fine-tuning method)",
    "checked": true,
    "id": "8fbe853c25b519952a83f136b74f229c26128f52",
    "semantic_title": "enhancing parameter-efficient fine-tuning with simple calibration based on stable rank",
    "citation_count": 0,
    "authors": [
      "Peiyu Liu",
      "Ze-Feng Gao",
      "Xiao Zhang",
      "Wayne Xin Zhao",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.535": {
    "title": "Enhancing Phrase Representation by Information Bottleneck Guided Text Diffusion Process for Keyphrase Extraction",
    "volume": "main",
    "abstract": "Keyphrase extraction (KPE) is an important task in Natural Language Processing for many scenarios, which aims to extract keyphrases that are present in a given document. Many existing supervised methods treat KPE as sequential labeling, span-level classification, or generative tasks. However, these methods lack the ability to utilize keyphrase information, which may result in biased results. In this study, we propose Diff-KPE, which leverages the supervised Variational Information Bottleneck (VIB) to guide the text diffusion process for generating enhanced keyphrase representations. Diff-KPE first generates the desired keyphrase embeddings conditioned on the entire document and then injects the generated keyphrase embeddings into each phrase representation. A ranking network and VIB are then optimized together with rank loss and classification loss, respectively. This design of Diff-KPE allows us to rank each candidate phrase by utilizing both the information of keyphrases and the document. Experiments show that Diff-KPE outperforms existing KPE methods on a large open domain keyphrase extraction benchmark, OpenKP, and a scientific domain dataset, KP20K",
    "checked": true,
    "id": "2f46af76b5c193db29e6bcd03ccbc6f80736804f",
    "semantic_title": "enhancing phrase representation by information bottleneck guided text diffusion process for keyphrase extraction",
    "citation_count": 0,
    "authors": [
      "Yuanzhen Luo",
      "Qingyu Zhou",
      "Feng Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.536": {
    "title": "Enhancing Scientific Document Summarization with Research Community Perspective and Background Knowledge",
    "volume": "main",
    "abstract": "Scientific paper summarization has been the focus of much recent research. Unlike previous research which summarizes only the paper in question, or which summarizes the paper and the papers that it references, or which summarizes the paper and the citing sentences from the papers that cite it, this work puts all three of these summarization techniques together. To accomplish this, we have, by utilizing the citation network, introduced a corpus for scientific document summarization that provides information about the document being summarized, the papers referenced by it, as well as the papers that have cited it. The proposed summarizer model utilizes the referenced articles as background information and citing articles to capture the impact of the scientific document on the research community. Another aspect of the proposed model is its ability to generate both the extractive and abstractive summaries in parallel. The parallel training helps the counterparts to improve their individual performance. Results have shown that the summaries are of high quality when considering the standard metrics",
    "checked": true,
    "id": "dc18454daf955989620b5d57a8aea1c993a3f834",
    "semantic_title": "enhancing scientific document summarization with research community perspective and background knowledge",
    "citation_count": 0,
    "authors": [
      "Sudipta Singha Roy",
      "Robert E. Mercer"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.537": {
    "title": "Enhancing Semantics in Multimodal Chain of Thought via Soft Negative Sampling",
    "volume": "main",
    "abstract": "Chain of thought (CoT) has proven useful for problems requiring complex reasoning. Many of these problems are both textual and multimodal. Given the inputs in different modalities, a model generates a rationale and then uses it to answer a question. Because of the hallucination issue, the generated soft negative rationales with high textual quality but illogical semantics do not always help improve answer accuracy. This study proposes a rationale generation method using soft negative sampling (SNSE-CoT) to mitigate hallucinations in multimodal CoT. Five methods were applied to generate soft negative samples that shared highly similar text but had different semantics from the original. Bidirectional margin loss (BML) was applied to introduce them into the traditional contrastive learning framework that involves only positive and negative samples. Extensive experiments on the ScienceQA dataset demonstrated the effectiveness of the proposed method. Code and data are released at https://github.com/zgMin/SNSE-CoT",
    "checked": true,
    "id": "5e515ba374ceef2d65edcb80948cb88fc2b0b4a8",
    "semantic_title": "enhancing semantics in multimodal chain of thought via soft negative sampling",
    "citation_count": 0,
    "authors": [
      "Guangmin Zheng",
      "Jin Wang",
      "Xiaobing Zhou",
      "Xuejie Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.538": {
    "title": "Enhancing Taiwanese Hokkien Dual Translation by Exploring and Standardizing of Four Writing Systems",
    "volume": "main",
    "abstract": "Machine translation focuses mainly on high-resource languages (HRLs), while low-resource languages (LRLs) like Taiwanese Hokkien are relatively under-explored. The study aims to address this gap by developing a dual translation model between Taiwanese Hokkien and both Traditional Mandarin Chinese and English. We employ a pre-trained LLaMA 2-7B model specialized in Traditional Mandarin Chinese to leverage the orthographic similarities between Taiwanese Hokkien Han and Traditional Mandarin Chinese. Our comprehensive experiments involve translation tasks across various writing systems of Taiwanese Hokkien as well as between Taiwanese Hokkien and other HRLs. We find that the use of a limited monolingual corpus still further improves the model's Taiwanese Hokkien capabilities. We then utilize our translation model to standardize all Taiwanese Hokkien writing systems into Hokkien Han, resulting in further performance improvements. Additionally, we introduce an evaluation method incorporating back-translation and GPT-4 to ensure reliable translation quality assessment even for LRLs. The study contributes to narrowing the resource gap for Taiwanese Hokkien and empirically investigates the advantages and limitations of pre-training and fine-tuning based on LLaMA 2",
    "checked": true,
    "id": "a311b4184425eb9b7dec22bac3f03a6ff61b9f3f",
    "semantic_title": "enhancing taiwanese hokkien dual translation by exploring and standardizing of four writing systems",
    "citation_count": 0,
    "authors": [
      "Bo-Han Lu",
      "Yi-Hsuan Lin",
      "Annie Lee",
      "Richard Tzong-Han Tsai"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.539": {
    "title": "Enhancing Text-to-SQL Capabilities of Large Language Models through Tailored Promptings",
    "volume": "main",
    "abstract": "Large language models (LLMs) with prompting have achieved encouraging results on many natural language processing (NLP) tasks based on task-tailored promptings. Text-to-SQL is a critical task that generates SQL queries from natural language questions. However, prompting on LLMs haven't show superior performance on Text-to-SQL task due to the absence of tailored promptings. In this work, we propose three promptings specifically designed for Text-to-SQL: SL-prompt, CC-prompt, and SL+CC prompt. SL-prompt is designed to guide LLMs to identify relevant tables; CC-prompt directs LLMs to generate SQL clause by clause; and SL+CC prompt is proposed to combine the strengths of these above promptings. The three prompting strategies makes three solutions for Text-to-SQL. Then, another prompting strategy, the RS-prompt is proposed to direct LLMs to select the best answer from the results of the solutions. We conducted extensive experiments, and experimental results show that our method achieved an execution accuracy of 86.2% and a test-suite accuracy of 76.9%, which is 1.1%, and 2.7% higher than the current state-of-the-art Text-to-SQL methods, respectively. The results confirmed that the proposed promptings enhanced the capabilities of LLMs on Text-to-SQL. Experimental results also show that the granularity of schema linking and the order of clause generation have great impact on the performance, which are considered little in previous research",
    "checked": true,
    "id": "05029b32c290adf87e56fde8585021a6c78600d4",
    "semantic_title": "enhancing text-to-sql capabilities of large language models through tailored promptings",
    "citation_count": 0,
    "authors": [
      "Zhao Tan",
      "Xiping Liu",
      "Qing Shu",
      "Xi Li",
      "Changxuan Wan",
      "Dexi Liu",
      "Qizhi Wan",
      "Guoqiong Liao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.540": {
    "title": "Enhancing Translation Ability of Large Language Models by Leveraging Task-Related Layers",
    "volume": "main",
    "abstract": "Fine-tuning Large Language Models (LLMs) for machine translation is effective but costly. It also increases the risk of overfitting and catastrophic forgetting, especially when training data is limited. To tackle these challenges, we propose a novel method that involves adjusting task-related layers in large models to better harness their machine translation capabilities. This method aims to retain the model's knowledge on other tasks while optimizing performance on translation tasks. By revealing the structure and characteristics of attention weights through singular value decomposition (SVD), we can make fine adjustments to specific layers, leveraging the model's potential for more accurate and efficient translations. Our method not only addresses computational resource consumption and catastrophic forgetting but also offers a new perspective on utilizing the capabilities of large models effectively. Experimental validation shows that adjusting task-related layers significantly improves performance on translation tasks while maintaining stability and accuracy on other tasks. This finding provides valuable insights for fine-tuning and applying large models, advancing the field of machine translation",
    "checked": true,
    "id": "ef22a08230c5cd61b072b1ae5de98d979e7f6fe5",
    "semantic_title": "enhancing translation ability of large language models by leveraging task-related layers",
    "citation_count": 0,
    "authors": [
      "Pei Cheng",
      "Xiayang Shi",
      "Yinlin Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.541": {
    "title": "Enhancing Unrestricted Cross-Document Event Coreference with Graph Reconstruction Networks",
    "volume": "main",
    "abstract": "Event Coreference Resolution remains a challenging discourse-oriented task within the domain of Natural Language Processing. In this paper we propose a methodology where we combine traditional mention-pair coreference models with a lightweight and modular graph reconstruction algorithm. We show that building graph models on top of existing mention-pair models leads to improved performance for both a wide range of baseline mention-pair algorithms as well as a recently developed state-of-the-art model and this at virtually no added computational cost. Moreover, additional experiments seem to indicate that our method is highly robust in low-data settings and that its performance scales with increases in performance for the underlying mention-pair models",
    "checked": true,
    "id": "b4996fe9f9f8737c8ed884ad2eaa9652686cfc56",
    "semantic_title": "enhancing unrestricted cross-document event coreference with graph reconstruction networks",
    "citation_count": 0,
    "authors": [
      "Loic de Langhe",
      "Orphee de Clercq",
      "Veronique Hoste"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.542": {
    "title": "Enhancing Writing Proficiency Classification in Developmental Education: The Quest for Accuracy",
    "volume": "main",
    "abstract": "Developmental Education (DevEd) courses align students' college-readiness skills with higher education literacy demands. These courses often use automated assessment tools like Accuplacer for student placement. Existing literature raises concerns about these exams' accuracy and placement precision due to their narrow representation of the writing process. These concerns warrant further attention within the domain of automatic placement systems, particularly in the establishment of a reference corpus of annotated essays for these systems' machine/deep learning. This study aims at an enhanced annotation procedure to assess college students' writing patterns more accurately. It examines the efficacy of machine-learning-based DevEd placement, contrasting Accuplacer's classification of 100 college-intending students' essays into two levels (Level 1 and 2) against that of 6 human raters. The classification task encompassed the assessment of the 6 textual criteria currently used by Accuplacer: mechanical conventions, sentence variety & style, idea development & support, organization & structure, purpose & focus, and critical thinking. Results revealed low inter-rater agreement, both on the individual criteria and the overall classification, suggesting human assessment of writing proficiency can be inconsistent in this context. To achieve a more accurate determination of writing proficiency and improve DevEd placement, more robust classification methods are thus required",
    "checked": true,
    "id": "5c19c6850119523d107be8b674803209f4be1d1f",
    "semantic_title": "enhancing writing proficiency classification in developmental education: the quest for accuracy",
    "citation_count": 0,
    "authors": [
      "Miguel Da Corte",
      "Jorge Baptista"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.543": {
    "title": "Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic",
    "volume": "main",
    "abstract": "Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts), a self-improvement prompting framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in diverse domains, including arithmetic, commonsense, symbolic, causal inference, and social problems, demonstrate the efficacy of enhanced reasoning by logic. The implementation code for LoT can be accessed at: https://github.com/xf-zhao/LoT",
    "checked": true,
    "id": "eed2a631d672a4130407f8d69a0ad9118a1e6e7d",
    "semantic_title": "enhancing zero-shot chain-of-thought reasoning in large language models through logic",
    "citation_count": 6,
    "authors": [
      "Xufeng Zhao",
      "Mengdi Li",
      "Wenhao Lu",
      "Cornelius Weber",
      "Jae Hee Lee",
      "Kun Chu",
      "Stefan Wermter"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.544": {
    "title": "Enough Is Enough! a Case Study on the Effect of Data Size for Evaluation Using Universal Dependencies",
    "volume": "main",
    "abstract": "When creating a new dataset for evaluation, one of the first considerations is the size of the dataset. If our evaluation data is too small, we risk making unsupported claims based on the results on such data. If, on the other hand, the data is too large, we waste valuable annotation time and costs that could have been used to widen the scope of our evaluation (i.e. annotate for more domains/languages). Hence, we investigate the effect of the size and a variety of sampling strategies of evaluation data to optimize annotation efforts, using dependency parsing as a test case. We show that for in-language in-domain datasets, 5,000 tokens is enough to obtain a reliable ranking of different parsers; especially if the data is distant enough from the training split (otherwise, we recommend 10,000). In cross-domain setups, the same amounts are required, but in cross-lingual setups much less (2,000 tokens) is enough",
    "checked": true,
    "id": "6601c58aecc63c6da4ee6408cee83ebdf6ffb938",
    "semantic_title": "enough is enough! a case study on the effect of data size for evaluation using universal dependencies",
    "citation_count": 0,
    "authors": [
      "Rob van der Goot",
      "Zoey Liu",
      "Max Müller-Eberstein"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.545": {
    "title": "Enriching a Time-Domain Astrophysics Corpus with Named Entity, Coreference and Astrophysical Relationship Annotations",
    "volume": "main",
    "abstract": "Interest in Astrophysical Natural Language Processing (NLP) has increased recently, fueled by the development of specialized language models for information extraction. However, the scarcity of annotated resources for this domain is still a significant challenge. Most existing corpora are limited to Named Entity Recognition (NER) tasks, leaving a gap in resource diversity. To address this gap and facilitate a broader spectrum of NLP research in astrophysics, we introduce astroECR, an extension of our previously built Time-Domain Astrophysics Corpus (TDAC). Our contributions involve expanding it to cover named entities, coreferences, annotations related to astrophysical relationships, and normalizing celestial object names. We showcase practical utility through baseline models for four NLP tasks and provide the research community access to our corpus, code, and models",
    "checked": true,
    "id": "ee9e18c238881d87af7f338470833c9708e95749",
    "semantic_title": "enriching a time-domain astrophysics corpus with named entity, coreference and astrophysical relationship annotations",
    "citation_count": 0,
    "authors": [
      "Atilla Kaan Alkan",
      "Felix Grezes",
      "Cyril Grouin",
      "Fabian Schussler",
      "Pierre Zweigenbaum"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.546": {
    "title": "Enriching Word Usage Graphs with Cluster Definitions",
    "volume": "main",
    "abstract": "We present a dataset of word usage graphs (WUGs), where the existing WUGs for multiple languages are enriched with cluster labels functioning as sense definitions. They are generated from scratch by fine-tuned encoder-decoder language models. The conducted human evaluation has shown that these definitions match the existing clusters in WUGs better than the definitions chosen from WordNet by two baseline systems. At the same time, the method is straightforward to use and easy to extend to new languages. The resulting enriched datasets can be extremely helpful for moving on to explainable semantic change modeling",
    "checked": true,
    "id": "ea6b4034c981885da708839f1420f2e0891af9f5",
    "semantic_title": "enriching word usage graphs with cluster definitions",
    "citation_count": 0,
    "authors": [
      "Andrey Kutuzov",
      "Mariia Fedorova",
      "Dominik Schlechtweg",
      "Nikolay Arefyev"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.547": {
    "title": "Ensembles of Hybrid and End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "We propose a method to combine the hybrid Kaldi-based Automatic Speech Recognition (ASR) system with the end-to-end wav2vec 2.0 XLS-R ASR using confidence measures. Our research is focused on the low-resource Irish language. Given the limited available open-source resources, neither the standalone hybrid ASR nor the end-to-end ASR system can achieve optimal performance. By applying the Recognizer Output Voting Error Reduction (ROVER) technique, we illustrate how ensemble learning could facilitate mutual error correction between both ASR systems. This paper outlines the strategies for merging the hybrid Kaldi ASR model and the end-to-end XLS-R model with the help of confidence scores. Although contemporary state-of-the-art end-to-end ASR models face challenges related to prediction overconfidence, we utilize Renyi's entropy-based confidence approach, tuned with temperature scaling, to align it with the Kaldi ASR confidence. Although there was no significant difference in the Word Error Rate (WER) between the hybrid and end-to-end ASR, we could achieve a notable reduction in WER after ensembling through ROVER. This resulted in an almost 14% Word Error Rate Reduction (WERR) on our primary test set and an approximately 20% WERR on other noisy and imbalanced test data",
    "checked": true,
    "id": "9af6b6df465319a13d0359a210197edc4f2fe432",
    "semantic_title": "ensembles of hybrid and end-to-end speech recognition",
    "citation_count": 0,
    "authors": [
      "Aditya Kamlesh Parikh",
      "Louis ten Bosch",
      "Henk van den Heuvel"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.548": {
    "title": "EpiGEN: An Efficient Multi-Api Code GENeration Framework under Enterprise Scenario",
    "volume": "main",
    "abstract": "In recent years, Large Language Models (LLMs) have demonstrated exceptional performance in code-generation tasks. However, under enterprise scenarios where private APIs are pre-built, general LLMs often fail to meet expectations. Existing approaches are confronted with drawbacks of high resource consumption and inadequate handling of multi-API tasks. To address these challenges, we propose EpiGEN, an Efficient multi-Api code GENeration framework under enterprise scenario. It consists of three core modules: Task Decomposition Module (TDM), API Retrieval Module (ARM), and Code Generation Module (CGM), in which Langchain played an important role. Through a series of experiments, EpiGEN shows good acceptability and readability, compared to fully fine-tuned LLM with a larger number of parameters. Particularly, in medium and hard level tasks, the performance of EpiGEN on a single-GPU machine even surpasses that of a fully fine-tuned LLM that requires multi-GPU configuration. Generally, EpiGEN is model-size agnostic, facilitating a balance between the performance of code generation and computational requirements",
    "checked": true,
    "id": "bb41589039f7df93b57fa0a1a99da9ba6a87ac90",
    "semantic_title": "epigen: an efficient multi-api code generation framework under enterprise scenario",
    "citation_count": 0,
    "authors": [
      "Sijie Li",
      "Sha Li",
      "Hao Zhang",
      "Shuyang Li",
      "Kai Chen",
      "Jianyong Yuan",
      "Yi Cao",
      "Lvqing Yang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.549": {
    "title": "EpLSA: Synergy of Expert-prefix Mixtures and Task-Oriented Latent Space Adaptation for Diverse Generative Reasoning",
    "volume": "main",
    "abstract": "Existing models for diverse generative reasoning still struggle to generate multiple unique and plausible results. Through an in-depth examination, we argue that it is critical to leverage a mixture of experts as prefixes to enhance the diversity of generated results and make task-oriented adaptation in the latent space of the generation models to improve the quality of the responses. At this point, we propose EpLSA, an innovative model based on the synergy of expert-prefix mixtures and task-oriented latent space adaptation for diverse generative reasoning. Specifically, we use expert-prefixes mixtures to encourage the model to create multiple responses with different semantics and design a loss function to address the problem that the semantics is interfered by the expert-prefixes. Meanwhile, we design a task-oriented adaptation block to make the pre-trained encoder within the generation model more effectively adapted to the pre-trained decoder in the latent space, thus further improving the quality of the generated text. Extensive experiments on three different types of generative reasoning tasks demonstrate that EpLSA outperforms existing baseline models in terms of both the quality and diversity of the generated outputs. Our code is publicly available at https://github.com/IMU-MachineLearningSXD/EpLSA",
    "checked": true,
    "id": "def7aa58be8740d3ff66355285f52d4bbde27085",
    "semantic_title": "eplsa: synergy of expert-prefix mixtures and task-oriented latent space adaptation for diverse generative reasoning",
    "citation_count": 0,
    "authors": [
      "Fujun Zhang",
      "Xiangdong Su",
      "Jiang Li",
      "Rong Yan",
      "Guanglai Gao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.550": {
    "title": "EPOQUE: An English-Persian Quality Estimation Dataset",
    "volume": "main",
    "abstract": "Translation quality estimation (QE) is an important component in real-world machine translation applications. Unfortunately, human labeled QE datasets, which play an important role in developing and assessing QE models, are only available for limited language pairs. In this paper, we present the first English-Persian QE dataset, called EPOQUE, which has manually annotated direct assessment labels. EPOQUE contains 1000 sentences translated from English to Persian and annotated by three human annotators. It is publicly available, and thus can be used as a zero-shot test set, or for other scenarios in future work. We also evaluate and report the performance of two state-of-the-art QE models, i.e., Transquest and CometKiwi, as baselines on our dataset. Furthermore, our experiments show that using a small subset of the proposed dataset containing 300 sentences to fine-tune Transquest, can improve its performance by more that 8% in terms of the Pearson correlation with a held-out test set",
    "checked": true,
    "id": "cbc1597a31e3a7bad662b59da9f0a7d82f9abfbd",
    "semantic_title": "epoque: an english-persian quality estimation dataset",
    "citation_count": 0,
    "authors": [
      "Mohammed Hossein Jafari Harandi",
      "Fatemeh Azadi",
      "Mohammad Javad Dousti",
      "Heshaam Faili"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.551": {
    "title": "EROS:Entity-Driven Controlled Policy Document Summarization",
    "volume": "main",
    "abstract": "Privacy policy documents have a crucial role in educating individuals about the collection, usage, and protection of users' personal data by organizations. However, they are notorious for their lengthy, complex, and convoluted language especially involving privacy-related entities. Hence, they pose a significant challenge to users who attempt to comprehend organization's data usage policy. In this paper, we propose to enhance the interpretability and readability of policy documents by using controlled abstractive summarization – we enforce the generated summaries to include critical privacy-related entities (e.g., data and medium) and organization's rationale (e.g., target and reason) in collecting those entities. To achieve this, we develop PD-Sum, a policy-document summarization dataset with marked privacy-related entity labels. Our proposed model, EROS, identifies critical entities through a span-based entity extraction model and employs them to control the information content of the summaries using proximal policy optimization (PPO). Comparison shows encouraging improvement over various baselines. Furthermore, we furnish qualitative and human evaluations to establish the efficacy of EROS",
    "checked": true,
    "id": "0c4c0ec09e30555deda979b437300fa9c0c8cba8",
    "semantic_title": "eros:entity-driven controlled policy document summarization",
    "citation_count": 0,
    "authors": [
      "Joykirat Singh",
      "Sehban Fazili",
      "Rohan Jain",
      "Md. Shad Akhtar"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.552": {
    "title": "Error Analysis of NLP Models and Non-Native Speakers of English Identifying Sarcasm in Reddit Comments",
    "volume": "main",
    "abstract": "This paper summarises the differences and similarities found between humans and three natural language processing models when attempting to identify whether English online comments are sarcastic or not. Three models were used to analyse 300 comments from the FigLang 2020 Reddit Dataset, with and without context. The same 300 comments were also given to 39 non-native speakers of English and the results were compared. The aim was to find whether there were any results that could be applied to English as a Foreign Language (EFL) teaching. The results showed that there were similarities between the models and non-native speakers, in particular the logistic regression model. They also highlighted weaknesses with both non-native speakers and the models in detecting sarcasm when the comments included political topics or were phrased as questions. This has potential implications for how the EFL teaching industry could implement the results of error analysis of NLP models in teaching practices",
    "checked": true,
    "id": "e14c2f2971918adfb1766eab60a3a30029c94f62",
    "semantic_title": "error analysis of nlp models and non-native speakers of english identifying sarcasm in reddit comments",
    "citation_count": 0,
    "authors": [
      "Oliver Cakebread-Andrews",
      "Le An Ha",
      "Ingo Frommholz",
      "Burcu Can"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.553": {
    "title": "Error-Robust Retrieval for Chinese Spelling Check",
    "volume": "main",
    "abstract": "Chinese Spelling Check (CSC) aims to detect and correct error tokens in Chinese contexts, which has a wide range of applications. However, it is confronted with the challenges of insufficient annotated data and the issue that previous methods may actually not fully leverage the existing datasets. In this paper, we introduce our plug-and-play retrieval method with error-robust information for Chinese Spelling Check (RERIC), which can be directly applied to existing CSC models. The datastore for retrieval is built completely based on the training data, with elaborate designs according to the characteristics of CSC. Specifically, we employ multimodal representations that fuse phonetic, morphologic, and contextual information in the calculation of query and key during retrieval to enhance robustness against potential errors. Furthermore, in order to better judge the retrieved candidates, the n-gram surrounding the token to be checked is regarded as the value and utilized for specific reranking. The experiment results on the SIGHAN benchmarks demonstrate that our proposed method achieves substantial improvements over existing work",
    "checked": true,
    "id": "e3e48c84e9877c115114ad3b01ba923c7e854a87",
    "semantic_title": "error-robust retrieval for chinese spelling check",
    "citation_count": 0,
    "authors": [
      "Xunjian Yin",
      "Xinyu Hu",
      "Jin Jiang",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.554": {
    "title": "EsCoLA: Spanish Corpus of Linguistic Acceptability",
    "volume": "main",
    "abstract": "Acceptability is one of the General Language Understanding Evaluation Benchmark (GLUE) probing tasks proposed to assess the linguistic capabilities acquired by a deep-learning transformer-based language model (LM). In this paper, we introduce the Spanish Corpus of Linguistic Acceptability EsCoLA. EsCoLA has been developed following the example of other linguistic acceptability data sets for English, Italian, Norwegian or Russian, with the aim of having a complete GLUE benchmark for Spanish. EsCoLA consists of 11,174 sentences and their acceptability judgements as found in well-known Spanish reference grammars. Additionally, all sentences have been annotated with the class of linguistic phenomenon the sentence is an example of, also following previous practices. We also provide as task baselines the results of fine-tuning four different language models with this data set and the results of a human annotation experiment. Results are also analyzed and commented to guide future research. EsCoLA is released under a CC-BY 4.0 license and freely available at https://doi.org/10.34810/data1138",
    "checked": true,
    "id": "1509b5dd76b251d61cd03f0bc26521da50edcf37",
    "semantic_title": "escola: spanish corpus of linguistic acceptability",
    "citation_count": 0,
    "authors": [
      "Nuria Bel",
      "Marta Punsola",
      "Valle Ruíz-Fernández"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.555": {
    "title": "ESCP: Enhancing Emotion Recognition in Conversation with Speech and Contextual Prefixes",
    "volume": "main",
    "abstract": "Emotion Recognition in Conversation (ERC) aims to analyze the speaker's emotional state in a conversation. Fully mining the information in multimodal and historical utterances plays a crucial role in the performance of the model. However, recent works in ERC focus on historical utterances modeling and generally concatenate the multimodal features directly, which neglects mining deep multimodal information and brings redundancy at the same time. To address the shortcomings of existing models, we propose a novel model, termed Enhancing Emotion Recognition in Conversation with Speech and Contextual Prefixes (ESCP). ESCP employs a directed acyclic graph (DAG) to model historical utterances in a conversation and incorporates a contextual prefix containing the sentiment and semantics of historical utterances. By adding speech and contextual prefixes, the inter- and intra-modal emotion information is efficiently modeled using the prior knowledge of the large-scale pre-trained model. Experiments conducted on several public benchmarks demonstrate that the proposed approach achieves state-of-the-art (SOTA) performances. These results affirm the effectiveness of the novel ESCP model and underscore the significance of incorporating speech and contextual prefixes to guide the pre-trained model",
    "checked": true,
    "id": "9e19ec0027ef0e7af7ebd130db69fa57854b9aca",
    "semantic_title": "escp: enhancing emotion recognition in conversation with speech and contextual prefixes",
    "citation_count": 0,
    "authors": [
      "Xiujuan Xu",
      "Xiaoxiao Shi",
      "Zhehuan Zhao",
      "Yu Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.556": {
    "title": "ESDM: Early Sensing Depression Model in Social Media Streams",
    "volume": "main",
    "abstract": "Depression impacts millions worldwide, with increasing efforts to use social media data for early detection and intervention. Traditional Risk Detection (TRD) uses a user's complete posting history for predictions, while Early Risk Detection (ERD) seeks early detection in a user's posting history, emphasizing the importance of prediction earliness. However, ERD remains relatively underexplored due to challenges in balancing accuracy and earliness, especially with evolving partial data. To address this, we introduce the Early Sensing Depression Model (ESDM), which comprises two modules classification with partial information module (CPI) and decision for classification moment module (DMC), alongside an early detection loss function. Experiments show ESDM outperforms benchmarks in both earliness and accuracy",
    "checked": true,
    "id": "2107a6c0087cc79750e055ba4b82ab6041e2dc45",
    "semantic_title": "esdm: early sensing depression model in social media streams",
    "citation_count": 0,
    "authors": [
      "Bichen Wang",
      "Yuzhe Zi",
      "Yanyan Zhao",
      "Pengfei Deng",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.557": {
    "title": "Esposito: An English-Persian Scientific Parallel Corpus for Machine Translation",
    "volume": "main",
    "abstract": "Neural machine translation requires large number of parallel sentences along with in-domain parallel data to attain best results. Nevertheless, no scientific parallel corpus for English-Persian language pair is available. In this paper, a parallel corpus called Esposito is introduced, which contains 3.5 million parallel sentences in the scientific domain for English-Persian language pair. In addition, we present a manually validated scientific test set that might serve as a baseline for future studies. We show that a system trained using Esposito along with other publicly available data improves the baseline on average by 7.6 and 8.4 BLEU scores for En->Fa and Fa->En directions, respectively. Additionally, domain analysis using the 5-gram KenLM model revealed notable distinctions between our parallel corpus and the existing generic parallel corpus. This dataset will be available to the public upon the acceptance of the paper",
    "checked": true,
    "id": "316e826d28c70894420f0120d9296f963c4413c3",
    "semantic_title": "esposito: an english-persian scientific parallel corpus for machine translation",
    "citation_count": 0,
    "authors": [
      "Mersad Esalati",
      "Mohammad Javad Dousti",
      "Heshaam Faili"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.558": {
    "title": "Estimating Lexical Complexity from Document-Level Distributions",
    "volume": "main",
    "abstract": "Existing methods for complexity estimation are typically developed for entire documents. This limitation in scope makes them inapplicable for shorter pieces of text, such as health assessment tools. These typically consist of lists of independent sentences, all of which are too short for existing methods to apply. The choice of wording in these assessment tools is crucial, as both the cognitive capacity and the linguistic competency of the intended patient groups could vary substantially. As a first step towards creating better tools for supporting health practitioners, we develop a two-step approach for estimating lexical complexity that does not rely on any pre-annotated data. We implement our approach for the Norwegian language and verify its effectiveness using statistical testing and a qualitative evaluation of samples from real assessment tools. We also investigate the relationship between our complexity measure and certain features typically associated with complexity in the literature, such as word length, frequency, and the number of syllables",
    "checked": true,
    "id": "07e4e9f7491da075b0ba9fb73bc9739c9badf283",
    "semantic_title": "estimating lexical complexity from document-level distributions",
    "citation_count": 0,
    "authors": [
      "Sondre Wold",
      "Petter Mæhlum",
      "Oddbjørn Hove"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.559": {
    "title": "Estimating the Causal Effects of Natural Logic Features in Transformer-Based NLI Models",
    "volume": "main",
    "abstract": "Rigorous evaluation of the causal effects of semantic features on language model predictions can be hard to achieve for natural language reasoning problems. However, this is such a desirable form of analysis from both an interpretability and model evaluation perspective, that it is valuable to investigate specific patterns of reasoning with enough structure and regularity to identify and quantify systematic reasoning failures in widely-used models. In this vein, we pick a portion of the NLI task for which an explicit causal diagram can be systematically constructed: the case where across two sentences (the premise and hypothesis), two related words/terms occur in a shared context. In this work, we apply causal effect estimation strategies to measure the effect of context interventions (whose effect on the entailment label is mediated by the semantic monotonicity characteristic) and interventions on the inserted word-pair (whose effect on the entailment label is mediated by the relation between these words). Extending related work on causal analysis of NLP models in different settings, we perform an extensive interventional study on the NLI task to investigate robustness to irrelevant changes and sensitivity to impactful changes of Transformers. The results strongly bolster the fact that similar benchmark accuracy scores may be observed for models that exhibit very different behaviour. Moreover, our methodology reinforces previously suspected biases from a causal perspective, including biases in favour of upward-monotone contexts and ignoring the effects of negation markers",
    "checked": true,
    "id": "3f180269930a98dbe2b587c9a66c18202f9ea0e3",
    "semantic_title": "estimating the causal effects of natural logic features in transformer-based nli models",
    "citation_count": 0,
    "authors": [
      "Julia Rozanova",
      "Marco Valentino",
      "André Freitas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.560": {
    "title": "Ethical Reasoning and Moral Value Alignment of LLMs Depend on the Language We Prompt Them in",
    "volume": "main",
    "abstract": "Ethical reasoning is a crucial skill for Large Language Models (LLMs). However, moral values are not universal, but rather influenced by language and culture. This paper explores how three prominent LLMs – GPT-4, ChatGPT, and Llama2Chat-70B – perform ethical reasoning in different languages and if their moral judgement depend on the language in which they are prompted. We extend the study of ethical reasoning of LLMs by (CITATION) to a multilingual setup following their framework of probing LLMs with ethical dilemmas and policies from three branches of normative ethics: deontology, virtue, and consequentialism. We experiment with six languages: English, Spanish, Russian, Chinese, Hindi, and Swahili. We find that GPT-4 is the most consistent and unbiased ethical reasoner across languages, while ChatGPT and Llama2Chat-70B show significant moral value bias when we move to languages other than English. Interestingly, the nature of this bias significantly vary across languages for all LLMs, including GPT-4",
    "checked": true,
    "id": "af8b26c3f10acfe86aa0a07d11ce4653e1dbf1cc",
    "semantic_title": "ethical reasoning and moral value alignment of llms depend on the language we prompt them in",
    "citation_count": 0,
    "authors": [
      "Utkarsh Agarwal",
      "Kumar Tanmay",
      "Aditi Khandelwal",
      "Monojit Choudhury"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.561": {
    "title": "EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation",
    "volume": "main",
    "abstract": "Large language models (LLMs) have gained popularity recently due to their outstanding performance in various downstream Natural Language Processing (NLP) tasks. However, low-resource languages are still lagging behind current state-of-the-art (SOTA) developments in the field of NLP due to insufficient resources to train LLMs. Ethiopian languages exhibit remarkable linguistic diversity, encompassing a wide array of scripts, and are imbued with profound religious and cultural significance. This paper introduces EthioLLM – multilingual large language models for five Ethiopian languages (Amharic, Ge'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark – a new benchmark dataset for various downstream NLP tasks. We evaluate the performance of these models across five downstream NLP tasks. We open-source our multilingual language models, new benchmark datasets for various downstream tasks, and task-specific fine-tuned language models and discuss the performance of the models. Our dataset and models are available at the https://huggingface.co/EthioNLP repository",
    "checked": true,
    "id": "7bf024694ee3d55465538f6c238179b401fd81b8",
    "semantic_title": "ethiollm: multilingual large language models for ethiopian languages with task evaluation",
    "citation_count": 0,
    "authors": [
      "Atnafu Lambebo Tonja",
      "Israel Abebe Azime",
      "Tadesse Destaw Belay",
      "Mesay Gemeda Yigezu",
      "Moges Ahmed Ah Mehamed",
      "Abinew Ali Ayele",
      "Ebrahim Chekol Jibril",
      "Michael Melese Woldeyohannis",
      "Olga Kolesnikova",
      "Philipp Slusallek",
      "Dietrich Klakow",
      "Seid Muhie Yimam"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.562": {
    "title": "European Language Grid: One Year after",
    "volume": "main",
    "abstract": "The European Language Grid (ELG) is a cloud platform for the whole European Language Technology community. While the EU project that developed the platform successfully concluded in June 2022, the ELG initiative has continued. This article provides a description of the current state of ELG in terms of user adoption and number of language resources and technologies available in early 2024. It also provides an overview of the various activities with regard to ELG since the end of the project and since the publication of the ELG book, especially the co-authors' attempt to integrate the ELG platform into various data space initiatives. The article also provides an overview of the Digital Language Equality (DLE) dashboard and the current state of DLE in Europe",
    "checked": true,
    "id": "aceb4a45bd28ee5e843cfd87f963c8eee0d20b05",
    "semantic_title": "european language grid: one year after",
    "citation_count": 0,
    "authors": [
      "Georg Rehm",
      "Stelios Piperidis",
      "Dimitris Galanis",
      "Penny Labropoulou",
      "Maria Giagkou",
      "Miltos Deligiannis",
      "Leon Voukoutis",
      "Martin Courtois",
      "Julian Moreno-Schneider",
      "Katrin Marheinecke"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.563": {
    "title": "Evaluating Automatic Subtitling: Correlating Post-editing Effort and Automatic Metrics",
    "volume": "main",
    "abstract": "Systems that automatically generate subtitles from video are gradually entering subtitling workflows, both for supporting subtitlers and for accessibility purposes. Even though robust metrics are essential for evaluating the quality of automatically-generated subtitles and for estimating potential productivity gains, there is limited research on whether existing metrics, some of which directly borrowed from machine translation (MT) evaluation, can fulfil such purposes. This paper investigates how well such MT metrics correlate with measures of post-editing (PE) effort in automatic subtitling. To this aim, we collect and publicly release a new corpus containing product-, process- and participant-based data from post-editing automatic subtitles in two language pairs (en→de,it). We find that different types of metrics correlate with different aspects of PE effort. Specifically, edit distance metrics have high correlation with technical and temporal effort, while neural metrics correlate well with PE speed",
    "checked": true,
    "id": "d152e0ef67cccfc859b705117bf64a19f7a452b7",
    "semantic_title": "evaluating automatic subtitling: correlating post-editing effort and automatic metrics",
    "citation_count": 0,
    "authors": [
      "Alina Karakanta",
      "Mauro Cettolo",
      "Matteo Negri",
      "Luisa Bentivogli"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.564": {
    "title": "Evaluating ChatGPT against Functionality Tests for Hate Speech Detection",
    "volume": "main",
    "abstract": "Large language models like ChatGPT have recently shown a great promise in performing several tasks, including hate speech detection. However, it is crucial to comprehend the limitations of these models to build robust hate speech detection systems. To bridge this gap, our study aims to evaluate the strengths and weaknesses of the ChatGPT model in detecting hate speech at a granular level across 11 languages. Our evaluation employs a series of functionality tests that reveals various intricate failures of the model which the aggregate metrics like macro F1 or accuracy are not able to unfold. In addition, we investigate the influence of complex emotions, such as the use of emojis in hate speech, on the performance of the ChatGPT model. Our analysis highlights the shortcomings of the generative models in detecting certain types of hate speech and highlighting the need for further research and improvements in the workings of these models",
    "checked": true,
    "id": "6715f34673d8f5939c09c09e99040cc116add03c",
    "semantic_title": "evaluating chatgpt against functionality tests for hate speech detection",
    "citation_count": 0,
    "authors": [
      "Mithun Das",
      "Saurabh Kumar Pandey",
      "Animesh Mukherjee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.565": {
    "title": "Evaluating Code-Switching Translation with Large Language Models",
    "volume": "main",
    "abstract": "Recent advances in large language models (LLMs) have shown they can match or surpass finetuned models on many natural language processing tasks. Currently, more studies are being carried out to assess whether this performance carries over across different languages. In this paper, we present a thorough evaluation of LLMs for the less well-researched code-switching translation setting, where inputs include a mixture of different languages. We benchmark the performance of six state-of-the-art LLMs across seven datasets, with GPT-4 and GPT-3.5 displaying strong ability relative to supervised translation models and commercial engines. GPT-4 was also found to be particularly robust against different code-switching conditions. Several methods to further improve code-switching translation are proposed including leveraging in-context learning and pivot translation. Through our code-switching experiments, we argue that LLMs show promising ability for cross-lingual understanding",
    "checked": true,
    "id": "dee6fc87aa9be963a528c6a57340d88cc571e9d0",
    "semantic_title": "evaluating code-switching translation with large language models",
    "citation_count": 0,
    "authors": [
      "Muhammad Huzaifah",
      "Weihua Zheng",
      "Nattapol Chanpaisit",
      "Kui Wu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.566": {
    "title": "Evaluating Gender Bias of Pre-trained Language Models in Natural Language Inference by Considering All Labels",
    "volume": "main",
    "abstract": "Discriminatory gender biases have been found in Pre-trained Language Models (PLMs) for multiple languages. In Natural Language Inference (NLI), existing bias evaluation methods have focused on the prediction results of one specific label out of three labels, such as neutral. However, such evaluation methods can be inaccurate since unique biased inferences are associated with unique prediction labels. Addressing this limitation, we propose a bias evaluation method for PLMs, called NLI-CoAL, which considers all the three labels of NLI task. First, we create three evaluation data groups that represent different types of biases. Then, we define a bias measure based on the corresponding label output of each data group. In the experiments, we introduce a meta-evaluation technique for NLI bias measures and use it to confirm that our bias measure can distinguish biased, incorrect inferences from non-biased incorrect inferences better than the baseline, resulting in a more accurate bias evaluation. We create the datasets in English, Japanese, and Chinese, and successfully validate the compatibility of our bias measure across multiple languages. Lastly, we observe the bias tendencies in PLMs of different languages. To our knowledge, we are the first to construct evaluation datasets and measure PLMs' bias from NLI in Japanese and Chinese",
    "checked": true,
    "id": "ccf2573eb882ef3f7301d61d53e0ec5b8c282039",
    "semantic_title": "evaluating gender bias of pre-trained language models in natural language inference by considering all labels",
    "citation_count": 10,
    "authors": [
      "Panatchakorn Anantaprayoon",
      "Masahiro Kaneko",
      "Naoaki Okazaki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.567": {
    "title": "Evaluating Generative Language Models in Information Extraction as Subjective Question Correction",
    "volume": "main",
    "abstract": "Modern Large Language Models (LLMs) have showcased remarkable prowess in various tasks necessitating sophisticated cognitive behaviors. Nevertheless, a paradoxical performance discrepancy is observed, where these models underperform in seemingly elementary tasks like relation extraction and event extraction due to two issues in conventional evaluation. (1) The imprecision of existing evaluation metrics that struggle to effectively gauge semantic consistency between model outputs and ground truth, and (2) The inherent incompleteness of evaluation benchmarks, primarily due to restrictive human annotation schemas, resulting in underestimated LLM performances. Inspired by the principles in subjective question correction, we propose a new evaluation method, SQC-Score. This method innovatively utilizes LLMs, fine-tuned through subjective question correction data, to refine matching between model outputs and golden labels. Additionally, by incorporating a Natural Language Inference (NLI) model, SQC-Score enriches golden labels, addressing benchmark incompleteness by acknowledging correct yet previously omitted answers. Results on three information extraction tasks show that SQC-Score is more preferred by human annotators than the baseline metrics. Utilizing SQC-Score, we conduct a comprehensive evaluation of the state-of-the-art LLMs and provide insights for future research for information extraction. Dataset and associated codes can be accessed at our <a href=https://github.com/THU-KEG/SQC-Score> GitHub repository </a>",
    "checked": true,
    "id": "de542a756ed96c360813b0293063b13653474708",
    "semantic_title": "evaluating generative language models in information extraction as subjective question correction",
    "citation_count": 0,
    "authors": [
      "Yuchen Fan",
      "Yantao Liu",
      "Zijun Yao",
      "Jifan Yu",
      "Lei Hou",
      "Juanzi Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.568": {
    "title": "Evaluating Performance of Pre-trained Word Embeddings on Assamese, a Low-resource Language",
    "volume": "main",
    "abstract": "Word embeddings and Language models are the building blocks of modern Deep Neural Network-based Natural Language Processing. They are extensively explored in high-resource languages and provide state-of-the-art (SOTA) performance for a wide range of downstream tasks. Nevertheless, these word embeddings are not explored in languages such as Assamese, where resources are limited. Furthermore, there has been limited study into the performance evaluation of these word embeddings for low-resource languages in downstream tasks. In this research, we explore the current state of Assamese pre-trained word embeddings. We evaluate these embeddings' performance on sequence labeling tasks such as Parts-of-speech and Named Entity Recognition. In order to assess the efficiency of the embeddings, experiments are performed utilizing both ensemble and individual word embedding approaches. The ensembling approach that uses three word embeddings outperforms the others. In the paper, the outcomes of the investigations are described. The results of this comparative performance evaluation may assist researchers in choosing an Assamese pre-trained word embedding for subsequent tasks",
    "checked": true,
    "id": "4bb47f1768a3cb6aa0d3b9e4021127667d9fff2b",
    "semantic_title": "evaluating performance of pre-trained word embeddings on assamese, a low-resource language",
    "citation_count": 0,
    "authors": [
      "Dhrubajyoti Pathak",
      "Sukumar Nandi",
      "Priyankoo Sarmah"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.569": {
    "title": "Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency",
    "volume": "main",
    "abstract": "This paper proposes an analysis of prompting strategies for grammatical error correction (GEC) with selected large language models (LLM) based on language proficiency. GEC using generative LLMs has been known for overcorrection where results obtain higher recall measures than precision measures. The writing examples of English language learners may be different from those of native speakers. Given that there is a significant differences in second language (L2) learners' error types by their proficiency levels, this paper attempts to reduce overcorrection by examining the interaction between LLM's performance and L2 language proficiency. Our method focuses on zero-shot and few-shot prompting and fine-tuning models for GEC for learners of English as a foreign language based on the different proficiency. We investigate GEC results and find that overcorrection happens primarily in advanced language learners' writing (proficiency C) rather than proficiency A (a beginner level) and proficiency B (an intermediate level). Fine-tuned LLMs, and even few-shot prompting with writing examples of English learners, actually tend to exhibit decreased recall measures. To make our claim concrete, we conduct a comprehensive examination of GEC outcomes and their evaluation results based on language proficiency",
    "checked": true,
    "id": "24d5c6e76d839e5c3a0d6cb6cf4a00d7f9a60015",
    "semantic_title": "evaluating prompting strategies for grammatical error correction based on language proficiency",
    "citation_count": 0,
    "authors": [
      "Min Zeng",
      "Jiexin Kuang",
      "Mengyang Qiu",
      "Jayoung Song",
      "Jungyeul Park"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.570": {
    "title": "Evaluating Saliency Explanations in NLP by Crowdsourcing",
    "volume": "main",
    "abstract": "Deep learning models have performed well on many NLP tasks. However, their internal mechanisms are typically difficult for humans to understand. The development of methods to explain models has become a key issue in the reliability of deep learning models in many important applications. Various saliency explanation methods, which give each feature of input a score proportional to the contribution of output, have been proposed to determine the part of the input which a model values most. Despite a considerable body of work on the evaluation of saliency methods, whether the results of various evaluation metrics agree with human cognition remains an open question. In this study, we propose a new human-based method to evaluate saliency methods in NLP by crowdsourcing. We recruited 800 crowd workers and empirically evaluated seven saliency methods on two datasets with the proposed method. We analyzed the performance of saliency methods, compared our results with existing automated evaluation methods, and identified notable differences between NLP and computer vision (CV) fields when using saliency methods. The instance-level data of our crowdsourced experiments and the code to reproduce the explanations are available at https://github.com/xtlu/lreccoling_evaluation",
    "checked": true,
    "id": "6548f4f482479c1fa66145574826bada8c6a5efd",
    "semantic_title": "evaluating saliency explanations in nlp by crowdsourcing",
    "citation_count": 0,
    "authors": [
      "Xiaotian Lu",
      "Jiyi Li",
      "Zhen Wan",
      "Xiaofeng Lin",
      "Koh Takeuchi",
      "Hisashi Kashima"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.571": {
    "title": "Evaluating Self-Supervised Speech Representations for Indigenous American Languages",
    "volume": "main",
    "abstract": "The application of self-supervision to speech representation learning has garnered significant interest in recent years, due to its scalability to large amounts of unlabeled data. However, much progress, both in terms of pre-training and downstream evaluation, has remained concentrated in monolingual models that only consider English. Few models consider other languages, and even fewer consider indigenous ones. In this work, benchmark the efficacy of large SSL models on 6 indigenous America languages: Quechua, Guarani , Bribri, Kotiria, Wa'ikhana, and Totonac on low-resource ASR. Our results show surprisingly strong performance by state-of-the-art SSL models, showing the potential generalizability of large-scale models to real-world data",
    "checked": true,
    "id": "49efabf8c0f0a66d66fb73b16838fbec7ffcf675",
    "semantic_title": "evaluating self-supervised speech representations for indigenous american languages",
    "citation_count": 2,
    "authors": [
      "Chih-Chen Chen",
      "William Chen",
      "Rodolfo Joel Zevallos",
      "John E. Ortega"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.572": {
    "title": "Evaluating Shortest Edit Script Methods for Contextual Lemmatization",
    "volume": "main",
    "abstract": "Modern contextual lemmatizers often rely on automatically induced Shortest Edit Scripts (SES), namely, the number of edit operations to transform a word form into its lemma. In fact, different methods of computing SES have been proposed as an integral component in the architecture of several state-of-the-art contextual lemmatizers currently available. However, previous work has not investigated the direct impact of SES in the final lemmatization performance. In this paper we address this issue by focusing on lemmatization as a token classification task where the only input that the model receives is the word-label pairs in context, where the labels correspond to previously induced SES. Thus, by modifying in our lemmatization system only the SES labels that the model needs to learn, we may then objectively conclude which SES representation produces the best lemmatization results. We experiment with seven languages of different morphological complexity, namely, English, Spanish, Basque, Russian, Czech, Turkish and Polish, using multilingual and language-specific pre-trained masked language encoder-only models as a backbone to build our lemmatizers. Comprehensive experimental results, both in- and out-of-domain, indicate that computing the casing and edit operations separately is beneficial overall, but much more clearly for languages with high-inflected morphology. Notably, multilingual pre-trained language models consistently outperform their language-specific counterparts in every evaluation setting",
    "checked": true,
    "id": "03ae6039d70872bd69773a3439541806a141ef5e",
    "semantic_title": "evaluating shortest edit script methods for contextual lemmatization",
    "citation_count": 0,
    "authors": [
      "Olia Toporkov",
      "Rodrigo Agerri"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.573": {
    "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
    "volume": "main",
    "abstract": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis",
    "checked": true,
    "id": "18f41334b89014a85c39cef0f9b71be4dcb0f960",
    "semantic_title": "evaluating text-to-speech synthesis from a large discrete token-based speech language model",
    "citation_count": 0,
    "authors": [
      "Siyang Wang",
      "Eva Szekely"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.574": {
    "title": "Evaluating the Efficacy of Large Acoustic Model for Documenting Non-Orthographic Tribal Languages in India",
    "volume": "main",
    "abstract": "Pre-trained Large Acoustic Models, when fine-tuned, have largely shown to improve the performances in various tasks related to spoken language technologies. However, their evaluation has been mostly on datasets that contain English or other widely spoken languages, and their potential for novel under-resourced languages is not fully known. In this work, four novel under-resourced tribal languages that do not have a standard writing system were introduced and the application of such large pre-trained models was assessed to document such languages using Automatic Speech Recognition and Direct Speech-to-Text Translation systems. The transcriptions for these tribal languages were generated by adapting scripts from those languages that held a prominent presence in the geographical regions where these tribal languages are spoken. The results from this study suggest a viable direction to document these languages in the electronic domain by using Spoken Language Technologies that incorporate LAMs. Additionally, this study helped in understanding the varying performances exhibited by the Large Acoustic Model between these four languages. This study not only informs the adoption of appropriate scripts for transliterating spoken-only languages based on the language family but also aids in making informed decisions in analyzing the behavior of particular Large Acoustic Model in linguistic contexts",
    "checked": true,
    "id": "8bcdbafbe9c57ff5337c7c9d4b9fd081aa8db1eb",
    "semantic_title": "evaluating the efficacy of large acoustic model for documenting non-orthographic tribal languages in india",
    "citation_count": 0,
    "authors": [
      "Tonmoy Rajkhowa",
      "Amartya Roy Chowdhury",
      "Hrishikesh Ravindra Karande",
      "S. R. Mahadeva Prasanna"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.575": {
    "title": "Evaluating the IWSLT2023 Speech Translation Tasks: Human Annotations, Automatic Metrics, and Segmentation",
    "volume": "main",
    "abstract": "Human evaluation is a critical component in machine translation system development and has received much attention in text translation research. However, little prior work exists on the topic of human evaluation for speech translation, which adds additional challenges such as noisy data and segmentation mismatches. We take the first steps to fill this gap by conducting a comprehensive human evaluation of the results of several shared tasks from the last International Workshop on Spoken Language Translation (IWSLT 2023). We propose an effective evaluation strategy based on automatic resegmentation and direct assessment with segment context. Our analysis revealed that: 1) the proposed evaluation strategy is robust and scores well-correlated with other types of human judgements; 2) automatic metrics are usually, but not always, well-correlated with direct assessment scores; and 3) COMET as a slightly stronger automatic metric than chrF, despite the segmentation noise introduced by the resegmentation step systems. We release the collected human-annotated data in order to encourage further investigation",
    "checked": true,
    "id": "dcdc00970e8cb65f8554469736f098c6daa12306",
    "semantic_title": "evaluating the iwslt2023 speech translation tasks: human annotations, automatic metrics, and segmentation",
    "citation_count": 0,
    "authors": [
      "Matthias Sperber",
      "Ondřej Bojar",
      "Barry Haddow",
      "Dávid Javorský",
      "Xutai Ma",
      "Matteo Negri",
      "Jan Niehues",
      "Peter Polák",
      "Elizabeth Salesky",
      "Katsuhito Sudoh",
      "Marco Turchi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.576": {
    "title": "Evaluating the Potential of Language-family-specific Generative Models for Low-resource Data Augmentation: A Faroese Case Study",
    "volume": "main",
    "abstract": "We investigate GPT-SW3, a generative language model for the Nordic languages, to assess its understanding of the low-resourced Faroese language. Our aim is to demonstrate the advantages of using language-family-specific generative models to augment data for related languages with fewer resources. We evaluate GPT-SW3 by prompting it for Faroese to English translation in a zero, one, and few-shot setting. We assess such translations with an ensemble score consisting of an arithmetic average between the BLEU and a semantic similarity score (SBERT). Moreover, we challenge the model's Faroese language understanding capabilities on a small dataset of curated Faroese trick sentences. There, we make a qualitative comparison of the model's performance with respect to Open AI's GPT-3.5 and GPT-4, demonstrating the advantages of using a language-family-specific generative model for navigating non-trivial scenarios. We evaluate the pipeline thus created and use it, as a proof of concept, to create an automatically annotated Faroese semantic textual similarity (STS) dataset",
    "checked": true,
    "id": "813ddfa7377ddf746206143e83aab8fbe7d85332",
    "semantic_title": "evaluating the potential of language-family-specific generative models for low-resource data augmentation: a faroese case study",
    "citation_count": 0,
    "authors": [
      "Barbara Scalvini",
      "Iben Nyholm Debess"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.577": {
    "title": "Evaluating the Quality of a Corpus Annotation Scheme Using Pretrained Language Models",
    "volume": "main",
    "abstract": "Pretrained language models and large language models are increasingly used to assist in a great variety of natural language tasks. In this work, we explore their use in evaluating the quality of alternative corpus annotation schemes. For this purpose, we analyze two alternative annotations of the Turkish BOUN treebank, versions 2.8 and 2.11, in the Universal Dependencies framework using large language models. Using a suitable prompt generated using treebank annotations, large language models are used to recover the surface forms of sentences. Based on the idea that the large language models capture the characteristics of the languages, we expect that the better annotation scheme would yield the sentences with higher success. The experiments conducted on a subset of the treebank show that the new annotation scheme (2.11) results in a successful recovery percentage of about 2 points higher. All the code developed for this work is available at https://github.com/boun-tabi/eval-ud",
    "checked": true,
    "id": "00c1ad08836802872b218e439f3b8f2391062baf",
    "semantic_title": "evaluating the quality of a corpus annotation scheme using pretrained language models",
    "citation_count": 0,
    "authors": [
      "Furkan Akkurt",
      "Onur Gungor",
      "Büşra Marşan",
      "Tunga Gungor",
      "Balkiz Ozturk Basaran",
      "Arzucan Özgür",
      "Susan Uskudarli"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.578": {
    "title": "Evaluating Topic Model on Asymmetric and Multi-Domain Financial Corpus",
    "volume": "main",
    "abstract": "Multiple recent research works in Finance try to quantify the exposure of market assets to various risks from text and how assets react if the risk materialize itself. We consider risk sections from french Financial Corporate Annual Reports, which are regulated documents with a mandatory section containing important risks the company is facing, to extract an accurate risk profile and exposure of companies. We identify multiple pitfalls of topic models when applied to corporate filing financial domain data for unsupervised risk distribution extraction which has not yet been studied on this domain. We propose two new metrics to evaluate the behavior of different types of topic models with respect to pitfalls previously mentioned about document risk distribution extraction. Our evaluation will focus on three aspects: regularizations, down-sampling and data augmentation. In our experiments, we found that classic Topic Models require down-sampling to obtain unbiased risks, while Topic Models using metadata and in-domain pre-trained word-embeddings partially correct the coherence imbalance per subdomain and remove sector's specific language from the detected themes. We then demonstrate the relevance and usefulness of the extracted information with visualizations that help to understand the content of such corpus and its evolution along the years",
    "checked": true,
    "id": "b60180cce7ee48b62768ed410869017367122eab",
    "semantic_title": "evaluating topic model on asymmetric and multi-domain financial corpus",
    "citation_count": 0,
    "authors": [
      "Corentin Masson",
      "Patrick Paroubek"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.579": {
    "title": "Evaluating Unsupervised Dimensionality Reduction Methods for Pretrained Sentence Embeddings",
    "volume": "main",
    "abstract": "Sentence embeddings produced by Pretrained Language Models (PLMs) have received wide attention from the NLP community due to their superior performance when representing texts in numerous downstream applications. However, the high dimensionality of the sentence embeddings produced by PLMs is problematic when representing large numbers of sentences in memory- or compute-constrained devices. As a solution, we evaluate unsupervised dimensionality reduction methods to reduce the dimensionality of sentence embeddings produced by PLMs. Our experimental results show that simple methods such as Principal Component Analysis (PCA) can reduce the dimensionality of sentence embeddings by almost 50%, without incurring a significant loss in performance in multiple downstream tasks. Surprisingly, reducing the dimensionality further improves performance over the original high dimensional versions for the sentence embeddings produced by some PLMs in some tasks",
    "checked": true,
    "id": "b6abfec582f98781978cd19a53f126f16c420c3d",
    "semantic_title": "evaluating unsupervised dimensionality reduction methods for pretrained sentence embeddings",
    "citation_count": 1,
    "authors": [
      "Gaifan Zhang",
      "Yi Zhou",
      "Danushka Bollegala"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.580": {
    "title": "Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale Annotations",
    "volume": "main",
    "abstract": "Rationales in the form of manually annotated input spans usually serve as ground truth when evaluating explainability methods in NLP. They are, however, time-consuming and often biased by the annotation process. In this paper, we debate whether human gaze, in the form of webcam-based eye-tracking recordings, poses a valid alternative when evaluating importance scores. We evaluate the additional information provided by gaze data, such as total reading times, gaze entropy, and decoding accuracy with respect to human rationale annotations. We compare WebQAmGaze, a multilingual dataset for information-seeking QA, with attention and explainability-based importance scores for 4 different multilingual Transformer-based language models (mBERT, distil-mBERT, XLMR, and XLMR-L) and 3 languages (English, Spanish, and German). Our pipeline can easily be applied to other tasks and languages. Our findings suggest that gaze data offers valuable linguistic insights that could be leveraged to infer task difficulty and further show a comparable ranking of explainability methods to that of human rationales",
    "checked": true,
    "id": "08c6dab5426643a260496f495c550fe1634c9f4b",
    "semantic_title": "evaluating webcam-based gaze data as an alternative for human rationale annotations",
    "citation_count": 0,
    "authors": [
      "Stephanie Brandl",
      "Oliver Eberle",
      "Tiago Ribeiro",
      "Anders Søgaard",
      "Nora Hollenstein"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.581": {
    "title": "Evaluating Word Expansion for Multilingual Sentiment Analysis of Parliamentary Speech",
    "volume": "main",
    "abstract": "This paper replicates and evaluates the word expansion (WE) method for sentiment lexicon generation from Rheault et al. (2016), applying it to two novel corpora of parliamentary speech from Denmark and Bulgaria. GloVe embeddings and vector similarity are leveraged to expand synonym seed lists with domain-specific terms from the speech corpora. The resulting Danish and Bulgarian lexica are compared to other multilingual lexica by analyzing a gold standard of speech excerpts annotated for sentiment. WE correlates best with hand-coded annotations for Danish, while a machine-translated Lexicoder dictionary does best for Bulgarian. WE performance is also found to be very sensitive to processing and scoring techniques, though this is also an issue with the other lexica. Overall, automatic lexicon translation best balances computational complexity and accuracy across both languages, but robust language-agnosticism remains elusive. Theoretical and practical problems of WE are discussed",
    "checked": true,
    "id": "6c8e0a5e4f8077ddb34b38c510103b7833922f5a",
    "semantic_title": "evaluating word expansion for multilingual sentiment analysis of parliamentary speech",
    "citation_count": 0,
    "authors": [
      "Yana Nikolova",
      "Costanza Navarretta"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.582": {
    "title": "Evaluating Workflows for Creating Orthographic Transcripts for Oral Corpora by Transcribing from Scratch or Correcting ASR-Output",
    "volume": "main",
    "abstract": "Research projects incorporating spoken data require either a selection of existing speech corpora, or they plan to record new data. In both cases, recordings need to be transcribed to make them accessible to analysis. Underestimating the effort of transcribing can be risky. Automatic Speech Recognition (ASR) holds the promise to considerably reduce transcription effort. However, few studies have so far attempted to evaluate this potential. The present paper compares efforts for manual transcription vs. correction of ASR-output. We took recordings from corpora of varying settings (interview, colloquial talk, dialectal, historic) and (i) compared two methods for creating orthographic transcripts: transcribing from scratch vs. correcting automatically created transcripts. And (ii) we evaluated the influence of the corpus characteristics on the correcting efficiency. Results suggest that for the selected data and transcription conventions, transcribing and correcting still take equally long with 7 times real-time on average. The more complex the primary data, the more time has to be spent on corrections. Despite the impressive latest developments in speech technology, to be a real help for conversation analysts or dialectologists, ASR systems seem to require even more improvement, or we need sufficient and appropriate data for training such systems",
    "checked": true,
    "id": "e2b44da151a812fd4401808401e73cb8f0589c4a",
    "semantic_title": "evaluating workflows for creating orthographic transcripts for oral corpora by transcribing from scratch or correcting asr-output",
    "citation_count": 0,
    "authors": [
      "Jan Gorisch",
      "Thomas Schmidt"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.583": {
    "title": "Evaluation Dataset for Lexical Translation Consistency in Chinese-to-English Document-level Translation",
    "volume": "main",
    "abstract": "Lexical translation consistency is one of the most common discourse phenomena in Chinese-to-English document-level translation. To better evaluate the performance of lexical translation consistency, previous researches assumes that all repeated source words should be translated consistently. However, constraining translations of repeated source words to be consistent will hurt word diversity and human translators tend to use different words in translation. Therefore, in this paper we construct a test set of 310 bilingual news articles to properly evaluate lexical translation consistency. We manually differentiate those repeated source words whose translations are consistent into two types: true consistency and false consistency. Then based on the constructed test set, we evaluate the performance of lexical translation consistency for several typical NMT systems",
    "checked": true,
    "id": "bc7eedd8a71b8f412a49556206d1732a5c1e7df4",
    "semantic_title": "evaluation dataset for lexical translation consistency in chinese-to-english document-level translation",
    "citation_count": 0,
    "authors": [
      "Xiangyu Lei",
      "Junhui Li",
      "Shimin Tao",
      "Hao Yang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.584": {
    "title": "Evaluation of Really Good Grammatical Error Correction",
    "volume": "main",
    "abstract": "Traditional evaluation methods for Grammatical Error Correction (GEC) fail to fully capture the full range of system capabilities and objectives. The emergence of large language models (LLMs) has further highlighted the shortcomings of these evaluation strategies, emphasizing the need for a paradigm shift in evaluation methodology. In the current study, we perform a comprehensive evaluation of various GEC systems using a recently published dataset of Swedish learner texts. The evaluation is performed using established evaluation metrics as well as human judges. We find that GPT-3 in a few-shot setting by far outperforms previous grammatical error correction systems for Swedish, a language comprising only about 0.1% of its training data. We also found that current evaluation methods contain undesirable biases that a human evaluation is able to reveal. We suggest using human post-editing of GEC system outputs to analyze the amount of change required to reach native-level human performance on the task, and provide a dataset annotated with human post-edits and assessments of grammaticality, fluency and meaning preservation of GEC system outputs",
    "checked": true,
    "id": "d793a972832563bbdf4936d8d94d9ecb9bafe604",
    "semantic_title": "evaluation of really good grammatical error correction",
    "citation_count": 0,
    "authors": [
      "Robert Östling",
      "Katarina Gillholm",
      "Murathan Kurfalı",
      "Marie Mattson",
      "Mats Wirén"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.585": {
    "title": "Event-enhanced Retrieval in Real-time Search",
    "volume": "main",
    "abstract": "The embedding-based retrieval (EBR) approach is widely used in mainstream search engine retrieval systems and is crucial in recent retrieval-augmented methods for eliminating LLM illusions. However, existing EBR models often face the \"semantic drift\" problem and insufficient focus on key information, leading to a low adoption rate of retrieval results in subsequent steps. This issue is especially noticeable in real-time search scenarios, where the various expressions of popular events on the Internet make real-time retrieval heavily reliant on crucial event information. To tackle this problem, this paper proposes a novel approach called EER, which enhances real-time retrieval performance by improving the dual-encoder model of traditional EBR. We incorporate contrastive learning to accompany pairwise learning for encoder optimization. Furthermore, to strengthen the focus on critical event information in events, we include a decoder module after the document encoder, introduce a generative event triplet extraction scheme based on prompt-tuning, and correlate the events with query encoder optimization through comparative learning. This decoder module can be removed during inference. Extensive experiments demonstrate that EER can significantly improve the real-time search retrieval performance. We believe that this approach will provide new perspectives in the field of information retrieval. The codes and dataset are available at https://github.com/open-event-hub/Event-enhanced_Retrieval",
    "checked": true,
    "id": "e58ac606d01aae256ac39e93f065dba07ca3d573",
    "semantic_title": "event-enhanced retrieval in real-time search",
    "citation_count": 0,
    "authors": [
      "Yanan Zhang",
      "Xiaoling Bai",
      "Tianhua Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.586": {
    "title": "Event Extraction in Basque: Typologically Motivated Cross-Lingual Transfer-Learning Analysis",
    "volume": "main",
    "abstract": "Cross-lingual transfer-learning is widely used in Event Extraction for low-resource languages and involves a Multilingual Language Model that is trained in a source language and applied to the target language. This paper studies whether the typological similarity between source and target languages impacts the performance of cross-lingual transfer, an under-explored topic. We first focus on Basque as the target language, which is an ideal target language because it is typologically different from surrounding languages. Our experiments on three Event Extraction tasks show that the shared linguistic characteristic between source and target languages does have an impact on transfer quality. Further analysis of 72 language pairs reveals that for tasks that involve token classification such as entity and event trigger identification, common writing script and morphological features produce higher quality cross-lingual transfer. In contrast, for tasks involving structural prediction like argument extraction, common word order is the most relevant feature. In addition, we show that when increasing the training size, not all the languages scale in the same way in the cross-lingual setting. To perform the experiments we introduce EusIE, an event extraction dataset for Basque, which follows the Multilingual Event Extraction dataset (MEE). The dataset and code are publicly available",
    "checked": true,
    "id": "07f78148576ad878bbff90af69806cd573f6486c",
    "semantic_title": "event extraction in basque: typologically motivated cross-lingual transfer-learning analysis",
    "citation_count": 0,
    "authors": [
      "Mikel Zubillaga",
      "Oscar Sainz",
      "Ainara Estarrona",
      "Oier Lopez de Lacalle",
      "Eneko Agirre"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.587": {
    "title": "EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs",
    "volume": "main",
    "abstract": "Narrative reasoning relies on the understanding of eventualities in story contexts, which requires a wealth of background world knowledge. To help machines leverage such knowledge, existing solutions can be categorized into two groups. Some focus on implicitly modeling eventuality knowledge by pretraining language models (LMs) with eventuality-aware objectives. However, this approach breaks down knowledge structures and lacks interpretability. Others explicitly collect world knowledge of eventualities into structured eventuality-centric knowledge graphs (KGs). However, existing research on leveraging these knowledge sources for free-texts is limited. In this work, we propose an initial comprehensive framework called EventGround, which aims to tackle the problem of grounding free-texts to eventuality-centric KGs for contextualized narrative reasoning. We identify two critical problems in this direction: the event representation and sparsity problems. We provide simple yet effective parsing and partial information extraction methods to tackle these problems. Experimental results demonstrate that our approach consistently outperforms baseline models when combined with graph neural network (GNN) or large language model (LLM) based graph reasoning models. Our framework, incorporating grounded knowledge, achieves state-of-the-art performance while providing interpretable evidence",
    "checked": true,
    "id": "8d89d38fedfa4a5374c726eb7926510016671441",
    "semantic_title": "eventground: narrative reasoning by grounding to eventuality-centric knowledge graphs",
    "citation_count": 1,
    "authors": [
      "Cheng Jiayang",
      "Lin Qiu",
      "Chunkit Chan",
      "Xin Liu",
      "Yangqiu Song",
      "Zheng Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.588": {
    "title": "Event Representation Learning with Multi-Grained Contrastive Learning and Triple-Mixture of Experts",
    "volume": "main",
    "abstract": "Event representation learning plays a crucial role in numerous natural language processing (NLP) tasks, as it facilitates the extraction of semantic features associated with events. Current methods of learning event representation based on contrastive learning processes positive examples with single-grain random masked language model (MLM), but fall short in learn information inside events from multiple aspects. In this paper, we introduce multi-grained contrastive learning and triple-mixture of experts (MCTM) for event representation learning. Our proposed method extends the random MLM by incorporating a specialized MLM designed to capture different grammatical structures within events, which allows the model to learn token-level knowledge from multiple perspectives. Furthermore, we have observed that mask tokens with different granularities affect the model differently, therefore, we incorporate mixture of experts (MoE) to learn importance weights associated with different granularities. Our experiments demonstrate that MCTM outperforms other baselines in tasks such as hard similarity and transitive sentence similarity, highlighting the superiority of our method",
    "checked": true,
    "id": "c1f01546a4f81e2ef39f9c8215bca0052ad69187",
    "semantic_title": "event representation learning with multi-grained contrastive learning and triple-mixture of experts",
    "citation_count": 0,
    "authors": [
      "Tianqi Hu",
      "Lishuang Li",
      "Xueyang Qin",
      "Yubo Feng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.589": {
    "title": "Every Verb in Its Right Place? A Roadmap for Operationalizing Developmental Stages in the Acquisition of L2 German",
    "volume": "main",
    "abstract": "Developmental stages are a linguistic concept claiming that language learning, despite its large inter-individual variance, generally progresses in an ordered, step-like manner. At the core of research has been the acquisition of verb placement by learners, as conceptualized within Processability Theory (Pienemann, 1989). The computational implementation of a system detecting developmental stages is a prerequisite for an automated analysis of L2 language development. However, such an implementation faces two main challenges. The first is the lack of a fully fleshed out, coherent linguistic specification of the stages. The second concerns the translation of the linguistic specification into computational procedures that can extract clauses from learner-produced text and assign them to a developmental stage based on verb placement. Our contribution provides the necessary linguistic specification of the stages as well as detaiiled discussion and recommendations regarding computational implementation",
    "checked": true,
    "id": "9cf8f604ccd4cf1715c6ba1f71fe916dc843347c",
    "semantic_title": "every verb in its right place? a roadmap for operationalizing developmental stages in the acquisition of l2 german",
    "citation_count": 0,
    "authors": [
      "Josef Ruppenhofer",
      "Matthias Schwendemann",
      "Annette Portmann",
      "Katrin Wisniewski",
      "Torsten Zesch"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.590": {
    "title": "Evidence-guided Inference for Neutralized Zero-shot Transfer",
    "volume": "main",
    "abstract": "Human annotation is costly and impractical when it comes to scarcely labeled data. Besides, the presence of biased language in well-known benchmarks notably misleads predictive models to perform incredibly well, not because of the model capability but due to the hidden false correlations in the linguistic corpus. Motivated by this, we propose a neutralized Knowledge Transfer framework (NKT) to equip pre-trained language models with neutralized transferability. Specifically, we construct debiased multi-source corpora (CV and EL) for two exemplary knowledge transfer tasks: claim verification and evidence learning, respectively. To counteract biased language, we design a neutralization mechanism in the presence of label skewness. We also design a label adaptation mechanism in light of the mixed label systems in the multi-source corpora. In extensive experiments, the proposed NKT framework shows effective transferability contrarily to the disability of dominant baselines, particularly in the zero-shot cross-domain transfer setting",
    "checked": true,
    "id": "445232b6d9ca35d277a182de2c404778cfe5b06f",
    "semantic_title": "evidence-guided inference for neutralized zero-shot transfer",
    "citation_count": 0,
    "authors": [
      "Xiaotong Feng",
      "Meng-Fen Chiang",
      "Wang-Chien Lee",
      "Zixin Kuang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.591": {
    "title": "EVil-Probe - a Composite Benchmark for Extensive Visio-Linguistic Probing",
    "volume": "main",
    "abstract": "Research probing the language comprehension of visio-linguistic models has gained traction due to their remarkable performance on various tasks. We introduce EViL-Probe, a composite benchmark that processes existing probing datasets into a unified format and reorganizes them based on the linguistic categories they probe. On top of the commonly used negative probes, this benchmark introduces positive probes to more rigorously test the robustness of models. Since the language side alone may introduce a bias models could exploit in solving the probes, we estimate the difficulty of the individual subsets with a language-only baseline. Using the benchmark to probe a set of state-of-the-art visio-linguistic models sheds light on how sensitive they are to the different linguistic categories. Results show that the benchmark is challenging for all models we probe, as their performance is around the chance baseline for many of the categories. The only category all models are able to handle relatively well are nouns. Additionally, models that use a Vision Transformer to process the images are also somewhat robust against probes targeting color and image type. Among these models, our enrichment of EViL-Probe with positive probes helps further discriminate performance, showing BLIP to be the overall best-performing model",
    "checked": true,
    "id": "b151ff0c6f58010c09c94d56fb621beb88bbf25a",
    "semantic_title": "evil-probe - a composite benchmark for extensive visio-linguistic probing",
    "citation_count": 0,
    "authors": [
      "Marie Bexte",
      "Andrea Horbach",
      "Torsten Zesch"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.592": {
    "title": "EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries",
    "volume": "main",
    "abstract": "While Large Language Models (LLMs) excel at the Winograd Schema Challenge (WSC), a coreference resolution task testing common-sense reasoning through pronoun disambiguation, they struggle with instances that feature minor alterations or rewording. To address this, we introduce EvoGrad, an open-source platform that harnesses a human-in-the-loop approach to create a dynamic dataset tailored to such altered WSC instances. Leveraging ChatGPT's capabilities, we expand our task instances from 182 to 3691, setting a new benchmark for diverse common-sense reasoning datasets. Additionally, we introduce the error depth metric, assessing model stability in dynamic tasks. Our results emphasize the challenge posed by EvoGrad: Even the best performing LLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2, a stark contrast to human performance of 92.8% accuracy without perturbation errors. This highlights ongoing model limitations and the value of dynamic datasets in uncovering them",
    "checked": true,
    "id": "01469112f6b947d05c67beb9756828224c2c2865",
    "semantic_title": "evograd: a dynamic take on the winograd schema challenge with human adversaries",
    "citation_count": 0,
    "authors": [
      "Jing Han Sun",
      "Ali Emami"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.593": {
    "title": "Evolving Knowledge Distillation with Large Language Models and Active Learning",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across various NLP tasks. However, their computational costs are prohibitively high. To address this issue, previous research has attempted to distill the knowledge of LLMs into smaller models by generating annotated data. Nonetheless, these works have mainly focused on the direct use of LLMs for text generation and labeling, without fully exploring their potential to comprehend the target task and acquire valuable knowledge. In this paper, we propose EvoKD: Evolving Knowledge Distillation, which leverages the concept of active learning to interactively enhance the process of data generation using large language models, simultaneously improving the task capabilities of small domain model (student model). Different from previous work, we actively analyze the student model's weaknesses, and then synthesize labeled samples based on the analysis. In addition, we provide iterative feedback to the LLMs regarding the student model's performance to continuously construct diversified and challenging samples. Experiments and analysis on different NLP tasks, namely, text classification and named entity recognition show the effectiveness of EvoKD",
    "checked": true,
    "id": "524fb3ee0cf8fd04570036cc8a1b31c9a8ddc534",
    "semantic_title": "evolving knowledge distillation with large language models and active learning",
    "citation_count": 0,
    "authors": [
      "Chengyuan Liu",
      "Fubang Zhao",
      "Kun Kuang",
      "Yangyang Kang",
      "Zhuoren Jiang",
      "Changlong Sun",
      "Fei Wu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.594": {
    "title": "Examining Temporalities on Stance Detection towards COVID-19 Vaccination",
    "volume": "main",
    "abstract": "Previous studies have highlighted the importance of vaccination as an effective strategy to control the transmission of the COVID-19 virus. It is crucial for policymakers to have a comprehensive understanding of the public's stance towards vaccination on a large scale. However, attitudes towards COVID-19 vaccination, such as pro-vaccine or vaccine hesitancy, have evolved over time on social media. Thus, it is necessary to account for possible temporal shifts when analysing these stances. This study aims to examine the impact of temporal concept drift on stance detection towards COVID-19 vaccination on Twitter. To this end, we evaluate a range of transformer-based models using chronological (splitting the training, validation, and test sets in order of time) and random splits (randomly splitting these three sets) of social media data. Our findings reveal significant discrepancies in model performance between random and chronological splits in several existing COVID-19-related datasets; specifically, chronological splits significantly reduce the accuracy of stance classification. Therefore, real-world stance detection approaches need to be further refined to incorporate temporal factors as a key consideration",
    "checked": true,
    "id": "1446490fa3c16ca7f5d49856e9a66ac5993da696",
    "semantic_title": "examining temporalities on stance detection towards covid-19 vaccination",
    "citation_count": 2,
    "authors": [
      "Yida Mu",
      "Mali Jin",
      "Kalina Bontcheva",
      "Xingyi Song"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.595": {
    "title": "Examining the Limitations of Computational Rumor Detection Models Trained on Static Datasets",
    "volume": "main",
    "abstract": "A crucial aspect of a rumor detection model is its ability to generalize, particularly its ability to detect emerging, previously unknown rumors. Past research has indicated that content-based (i.e., using solely source post as input) rumor detection models tend to perform less effectively on unseen rumors. At the same time, the potential of context-based models remains largely untapped. The main contribution of this paper is in the in-depth evaluation of the performance gap between content and context-based models specifically on detecting new, unseen rumors. Our empirical findings demonstrate that context-based models are still overly dependent on the information derived from the rumors' source post and tend to overlook the significant role that contextual information can play. We also study the effect of data split strategies on classifier performance. Based on our experimental results, the paper also offers practical suggestions on how to minimize the effects of temporal concept drift in static datasets during the training of rumor detection methods",
    "checked": true,
    "id": "3f46f847ed0a4de5681ec1581890d18cdeaaf2e0",
    "semantic_title": "examining the limitations of computational rumor detection models trained on static datasets",
    "citation_count": 1,
    "authors": [
      "Yida Mu",
      "Xingyi Song",
      "Kalina Bontcheva",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.596": {
    "title": "Executing Natural Language-Described Algorithms with Large Language Models: An Investigation",
    "volume": "main",
    "abstract": "Executing computer programs described in natural language has long been a pursuit of computer science. With the advent of enhanced natural language understanding capabilities exhibited by large language models (LLMs), the path toward this goal has been illuminated. In this paper, we seek to examine the capacity of present-day LLMs to comprehend and execute algorithms outlined in natural language. We established an algorithm test set sourced from Introduction to Algorithm, a well-known textbook that contains many representative widely-used algorithms. To systematically assess LLMs' code execution abilities, we selected 30 algorithms, generated 300 random-sampled instances in total, and evaluated whether popular LLMs can understand and execute these algorithms. Our findings reveal that LLMs, notably GPT-4, can effectively execute programs described in natural language, as long as no heavy numeric computation is involved. We believe our findings contribute to evaluating LLMs' code execution abilities and would encourage further investigation and application for the computation power of LLMs",
    "checked": true,
    "id": "1ede4f893f01093761034616f4410d302431bdcd",
    "semantic_title": "executing natural language-described algorithms with large language models: an investigation",
    "citation_count": 0,
    "authors": [
      "Xin Zheng",
      "Qiming Zhu",
      "Hongyu Lin",
      "Yaojie Lu",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.597": {
    "title": "Experimental versus In-Corpus Variation in Referring Expression Choice",
    "volume": "main",
    "abstract": "In this paper, we compare the results of three studies. The first explored feature-conditioned distributions of referring expression (RE) forms in the original corpus from which the contexts were taken. The second is a crowdsourcing study in which we asked participants to express entities within a pre-existing context, given fully specified referents. The third study replicates the crowdsourcing experiment using Large Language Models (LLMs). We evaluate how well the corpus itself can model the variation found when multiple informants (either human participants or LLMs) choose REs in the same contexts. We measure the similarity of the conditional distributions of form categories using the Jensen-Shannon Divergence metric and Description Length metric. We find that the experimental methodology introduces substantial noise, but by taking this noise into account, we can model the variation captured from the corpus and RE form choices made during experiments. Furthermore, we compared the three conditional distributions over the corpus, the human experimental results, and the GPT models. Against our expectations, the divergence is greatest between the corpus and the GPT model",
    "checked": true,
    "id": "8c243eab54cece511149e5819b2965af60ea27fc",
    "semantic_title": "experimental versus in-corpus variation in referring expression choice",
    "citation_count": 0,
    "authors": [
      "T. Mark Ellison",
      "Fahime Same"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.598": {
    "title": "Experiments on Speech Synthesis for Teochew, Can Taiwanese Help ?",
    "volume": "main",
    "abstract": "This paper reports on our preliminary experiments in speech processing for Teochew, an under-resourced Sinitic language spoken both in China and around the world in diasporan communities. Following the recent uptick of interest in Teochew from heritage speakers of the diaspora and in order to respond to the needs of this community, we develop a Teochew Text-to-Speech system. We describe experiments to build this system and to assess the possible contribution of available resources in Taiwanese Hokkien, the closest language with a significant body of resources. The results of these experiments are not as conclusive as we expected: the Taiwanese dataset did not help our model significantly, but considering our objectives, we find it encouraging that they show that a large training dataset was not necessary for this precise task. A promising model could still be obtained with only a small dataset of Teochew. We hope that this work inspires other communities of speakers of languages in a revitalization phase",
    "checked": true,
    "id": "6dc621cfe74df32f111bec5a72a7a95d067b1f45",
    "semantic_title": "experiments on speech synthesis for teochew, can taiwanese help ?",
    "citation_count": 0,
    "authors": [
      "Pierre Magistry",
      "Ilaine Wang",
      "Ty Eng Lim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.599": {
    "title": "Explainable Multi-hop Question Generation: An End-to-End Approach without Intermediate Question Labeling",
    "volume": "main",
    "abstract": "In response to the increasing use of interactive artificial intelligence, the demand for the capacity to handle complex questions has increased. Multi-hop question generation aims to generate complex questions that requires multi-step reasoning over several documents. Previous studies have predominantly utilized end-to-end models, wherein questions are decoded based on the representation of context documents. However, these approaches lack the ability to explain the reasoning process behind the generated multi-hop questions. Additionally, the question rewriting approach, which incrementally increases the question complexity, also has limitations due to the requirement of labeling data for intermediate-stage questions. In this paper, we introduce an end-to-end question rewriting model that increases question complexity through sequential rewriting. The proposed model has the advantage of training with only the final multi-hop questions, without intermediate questions. Experimental results demonstrate the effectiveness of our model in generating complex questions, particularly 3- and 4-hop questions, which are appropriately paired with input answers. We also prove that our model logically and incrementally increases the complexity of questions, and the generated multi-hop questions are also beneficial for training question answering models",
    "checked": true,
    "id": "a728ce5659a8ae930ff47d81909aa3696d8dea57",
    "semantic_title": "explainable multi-hop question generation: an end-to-end approach without intermediate question labeling",
    "citation_count": 0,
    "authors": [
      "Seonjeong Hwang",
      "Yunsu Kim",
      "Gary Geunbae Lee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.600": {
    "title": "Explaining Pre-Trained Language Models with Attribution Scores: An Analysis in Low-Resource Settings",
    "volume": "main",
    "abstract": "Attribution scores indicate the importance of different input parts and can, thus, explain model behaviour. Currently, prompt-based models are gaining popularity, i.a., due to their easier adaptability in low-resource settings. However, the quality of attribution scores extracted from prompt-based models has not been investigated yet. In this work, we address this topic by analyzing attribution scores extracted from prompt-based models w.r.t. plausibility and faithfulness and comparing them with attribution scores extracted from fine-tuned models and large language models. In contrast to previous work, we introduce training size as another dimension into the analysis. We find that using the prompting paradigm (with either encoder-based or decoder-based models) yields more plausible explanations than fine-tuning the models in low-resource settings and Shapley Value Sampling consistently outperforms attention and Integrated Gradients in terms of leading to more plausible and faithful explanations",
    "checked": true,
    "id": "24d06753d04dd989e88a220e55e3aefe93825f76",
    "semantic_title": "explaining pre-trained language models with attribution scores: an analysis in low-resource settings",
    "citation_count": 0,
    "authors": [
      "Wei Zhou",
      "Heike Adel",
      "Hendrik Schuff",
      "Ngoc Thang Vu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.601": {
    "title": "Explicit over Implict: Explicit Diversity Conditions for Effective Question Answer Generation",
    "volume": "main",
    "abstract": "Question Answer Generation (QAG) is an effective data augmentation technique to improve the accuracy of question answering systems, especially in low-resource domains. While recent pretrained and large language model-based QAG methods have made substantial progress, they face the critical issue of redundant QA pair generation, affecting downstream QA systems. Implicit diversity techniques such as sampling and diverse beam search are proven effective solutions but often yield smaller diversity. We present explicit diversity conditions for QAG, focusing on spatial aspects, question types, and entities, substantially increasing diversity in QA generation. Our work emphasizes the need of explicit diversity conditions for generating diverse question-answer synthetic data by showing significant improvements in downstream QA task over existing implicit diversity techniques. In particular, generated QA pairs from explicit diversity conditions result in an average 4.1% exact match and 4.5% F1 improvement over implicit sampling techniques on SQuAD-DU. Our work emphasizes the need for explicit diversity conditions even more in low-resource datasets (SubjQA), where average QA performance improvements are ~12% EM",
    "checked": true,
    "id": "60814d191321a92727d16844b075233482bd5f4c",
    "semantic_title": "explicit over implict: explicit diversity conditions for effective question answer generation",
    "citation_count": 0,
    "authors": [
      "Vikas Yadav",
      "Hyuk joon Kwon",
      "Vijay Srinivasan",
      "Hongxia Jin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.602": {
    "title": "Exploring and Mitigating Shortcut Learning for Generative Large Language Models",
    "volume": "main",
    "abstract": "Recent generative large language models (LLMs) have exhibited incredible instruction-following capabilities while keeping strong task completion ability, even without task-specific fine-tuning. Some works attribute this to the bonus of the new scaling law, in which the continuous improvement of model capacity yields emergent capabilities, e.g., reasoning and universal generalization. However, we point out that recent LLMs still show shortcut learning behavior, where the models tend to exploit spurious correlations between non-robust features and labels for prediction, which might lead to overestimating model capabilities. LLMs memorize more complex spurious correlations (i.e., task ↔ feature ↔ label) compared with that learned from previous pre-training and task-specific fine-tuning paradigm (i.e., feature ↔ label). Based on our findings, we propose FSLI, a framework for encouraging LLMs to Forget Spurious correlations and Learn from In-context information. Experiments on three tasks show that FSFI can effectively mitigate shortcut learning. Besides, we argue not to overestimate the capabilities of LLMs and conduct evaluations in more challenging and complete test scenarios",
    "checked": true,
    "id": "73b782f722ca45dec57db3b766b90248ee9b16b9",
    "semantic_title": "exploring and mitigating shortcut learning for generative large language models",
    "citation_count": 0,
    "authors": [
      "Zechen Sun",
      "Yisheng Xiao",
      "Juntao Li",
      "Yixin Ji",
      "Wenliang Chen",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.603": {
    "title": "Exploring BERT-Based Classification Models for Detecting Phobia Subtypes: A Novel Tweet Dataset and Comparative Analysis",
    "volume": "main",
    "abstract": "Phobias, characterized by irrational fears of specific objects or situations, can profoundly affect an individual's quality of life. This research presents a comprehensive investigation into phobia classification, where we propose a novel dataset of 811,569 English tweets from user timelines spanning 102 phobia subtypes over six months, including 47,614 self-diagnosed phobia users. BERT models were leveraged to differentiate non-phobia from phobia users and classify them into 65 specific phobia subtypes. The study produced promising results, with the highest f1-score of 78.44% in binary classification (phobic user or not phobic user) and 24.01% in a multi-class classification (detecting the specific phobia subtype of a user). This research provides insights into people with phobias on social media and emphasizes the capacity of natural language processing and machine learning to automate the evaluation and support of mental health",
    "checked": true,
    "id": "637dc4c07e4ac5a6d439eb79fcaf8eab17c20a3d",
    "semantic_title": "exploring bert-based classification models for detecting phobia subtypes: a novel tweet dataset and comparative analysis",
    "citation_count": 0,
    "authors": [
      "Anik Das",
      "Milton King",
      "James Alexander Hughes"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.604": {
    "title": "Exploring Geometric Representational Disparities between Multilingual and Bilingual Translation Models",
    "volume": "main",
    "abstract": "Multilingual machine translation has proven immensely useful for both parameter efficiency and overall performance across many language pairs via complete multilingual parameter sharing. However, some language pairs in multilingual models can see worse performance than in bilingual models, especially in the one-to-many translation setting. Motivated by their empirical differences, we examine the geometric differences in representations from bilingual models versus those from one-to-many multilingual models. Specifically, we compute the isotropy of these representations using intrinsic dimensionality and IsoScore, in order to measure how the representations utilize the dimensions in their underlying vector space. Using the same evaluation data in both models, we find that for a given language pair, its multilingual model decoder representations are consistently less isotropic and occupy fewer dimensions than comparable bilingual model decoder representations. Additionally, we show that much of the anisotropy in multilingual decoder representations can be attributed to modeling language-specific information, therefore limiting remaining representational capacity",
    "checked": true,
    "id": "8684205933aa68371d7f387874be2e3f9d6a6c94",
    "semantic_title": "exploring geometric representational disparities between multilingual and bilingual translation models",
    "citation_count": 0,
    "authors": [
      "Neha Verma",
      "Kenton Murray",
      "Kevin Duh"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.605": {
    "title": "Exploring Interpretability of Independent Components of Word Embeddings with Automated Word Intruder Test",
    "volume": "main",
    "abstract": "Independent Component Analysis (ICA) is an algorithm originally developed for finding separate sources in a mixed signal, such as a recording of multiple people in the same room speaking at the same time. Unlike Principal Component Analysis (PCA), ICA permits the representation of a word as an unstructured set of features, without any particular feature being deemed more significant than the others. In this paper, we used ICA to analyze word embeddings. We have found that ICA can be used to find semantic features of the words and these features can easily be combined to search for words that satisfy the combination. We show that most of the independent components represent such features. To quantify the interpretability of the components, we use the word intruder test, performed both by humans and by large language models. We propose to use the automated version of the word intruder test as a fast and inexpensive way of quantifying vector interpretability without the need for human effort",
    "checked": true,
    "id": "c67bacbb3180bfac68b32a51ee9f8fdf02043ade",
    "semantic_title": "exploring interpretability of independent components of word embeddings with automated word intruder test",
    "citation_count": 0,
    "authors": [
      "Tomáš Musil",
      "David Mareček"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.606": {
    "title": "Exploring Neural Topic Modeling on a Classical Latin Corpus",
    "volume": "main",
    "abstract": "The large availability of processable textual resources for Classical Latin has made it possible to study Latin literature through methods and tools that support distant reading. This paper describes a number of experiments carried out to test the possibility of investigating the thematic distribution of the Classical Latin corpus Opera Latina by means of topic modeling. For this purpose, we train, optimize and compare two neural models, Product-of-Experts LDA (ProdLDA) and Embedded Topic Model (ETM), opportunely revised to deal with the textual data from a Classical Latin corpus, to evaluate which one performs better both on the basis of topic diversity and topic coherence metrics, and from a human judgment point of view. Our results show that the topics extracted by neural models are coherent and interpretable and that they are significant from the perspective of a Latin scholar. The source code of the proposed model is available at https://github.com/MIND-Lab/LatinProdLDA",
    "checked": true,
    "id": "9fea13f8617be5224d249e726acc7590e9be5853",
    "semantic_title": "exploring neural topic modeling on a classical latin corpus",
    "citation_count": 0,
    "authors": [
      "Ginevra Martinelli",
      "Paola Impicciché",
      "Elisabetta Fersini",
      "Francesco Mambrini",
      "Marco Passarotti"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.607": {
    "title": "Exploring Pathological Speech Quality Assessment with ASR-Powered Wav2Vec2 in Data-Scarce Context",
    "volume": "main",
    "abstract": "Automatic speech quality assessment has raised more attention as an alternative or support to traditional perceptual clinical evaluation. However, most research so far only gains good results on simple tasks such as binary classification, largely due to data scarcity. To deal with this challenge, current works tend to segment patients' audio files into many samples to augment the datasets. Nevertheless, this approach has limitations, as it indirectly relates overall audio scores to individual segments. This paper introduces a novel approach where the system learns at the audio level instead of segments despite data scarcity. This paper proposes to use the pre-trained Wav2Vec2 architecture for both SSL, and ASR as feature extractor in speech assessment. Carried out on the HNC dataset, our ASR-driven approach established a new baseline compared with other approaches, obtaining average MSE = 0.73 and MSE = 1.15 for the prediction of intelligibility and severity scores respectively, using only 95 training samples. It shows that the ASR based Wav2Vec2 model brings the best results and may indicate a strong correlation between ASR and speech quality assessment. We also measure its ability on variable segment durations and speech content, exploring factors influencing its decision",
    "checked": true,
    "id": "7eb9675407513aa598d34c6fc70bdf3070109bb6",
    "semantic_title": "exploring pathological speech quality assessment with asr-powered wav2vec2 in data-scarce context",
    "citation_count": 0,
    "authors": [
      "Tuan Nguyen",
      "Corinne Fredouille",
      "Alain Ghio",
      "Mathieu Balaguer",
      "Virginie Woisard"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.608": {
    "title": "Exploring the Emotional Dimension of French Online Toxic Content",
    "volume": "main",
    "abstract": "One of the biggest hurdles for the effective analysis of data collected on social platforms is the need for deeper insights on the content and meaning of this data. Emotion annotation can bring new perspectives on this issue and can enable the identification of content–specific features. This study aims at investigating the ways in which variation in online content can be explored through emotion annotation and corpus-based analysis. The paper describes the emotion annotation of three data sets in French composed of extremist, sexist and hateful messages respectively. To this end, first a fine-grained, corpus annotation scheme was used to annotate the data sets and then several empirical studies were carried out to characterize the content in the light of emotional categories. Results suggest that emotion annotations can provide new insights for online content analysis and stronger empirical background for automatic content detection",
    "checked": true,
    "id": "6496668a283a724b6a5eed2fe92da20063aaded7",
    "semantic_title": "exploring the emotional dimension of french online toxic content",
    "citation_count": 0,
    "authors": [
      "Valentina Dragos",
      "Delphine Battistelli",
      "Fatou Sow",
      "Aline Etienne"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.609": {
    "title": "Exploring the Generalization of Cancer Clinical Trial Eligibility Classifiers across Diseases",
    "volume": "main",
    "abstract": "Clinical trials are pivotal in medical research, and NLP can enhance their success, with application in recruitment. This study aims to evaluate the generalizability of eligibility classification across a broad spectrum of clinical trials. Starting with phase 3 cancer trials, annotated with seven eligibility exclusions, then to determine how well models can generalize to non-cancer and non-phase 3 trials. To assess this, we have compiled eligibility criteria data for five types of trials: (1) additional phase 3 cancer trials, (2) phase 1 and 2 cancer trials, (3) heart disease trials, (4) type 2 diabetes trials, and (5) observational trials for any disease, comprising 2,490 annotated eligibility criteria across seven exclusion types. Our results show that models trained on the extensive cancer dataset can effectively handle criteria commonly found in non-cancer trials, such as autoimmune diseases. However, they struggle with criteria disproportionately prevalent in cancer trials, like prior malignancy. We also experiment with few-shot learning, demonstrating that a limited number of disease-specific examples can partially overcome this performance gap. We are releasing this new dataset of annotated eligibility statements to promote the development of cross-disease generalization in clinical trial classification",
    "checked": true,
    "id": "99d618cf4209da0eb2459682bfeb128113b2b08d",
    "semantic_title": "exploring the generalization of cancer clinical trial eligibility classifiers across diseases",
    "citation_count": 0,
    "authors": [
      "Yumeng Yang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.610": {
    "title": "Exploring the Impact of Human Evaluator Group on Chat-Oriented Dialogue Evaluation",
    "volume": "main",
    "abstract": "Human evaluation has been widely accepted as the standard for evaluating chat-oriented dialogue systems. However, there is a significant variation in previous work regarding who gets recruited as evaluators. Evaluator groups such as domain experts, university students, and crowdworkers have been used to assess and compare dialogue systems, although it is unclear to what extent the choice of an evaluator group can affect results. This paper analyzes the evaluator group impact on dialogue system evaluation by testing 4 state-of-the-art dialogue systems using 4 distinct evaluator groups. Our analysis reveals a robustness towards evaluator groups for Likert evaluations that is not seen for Pairwise, with only minor differences observed when changing evaluator groups. Furthermore, two notable limitations to this robustness are observed, which reveal discrepancies between evaluators with different levels of chatbot expertise and indicate that evaluator objectivity is beneficial for certain dialogue metrics",
    "checked": true,
    "id": "90e8c971b087cfc47a3bf3837466869684059ee6",
    "semantic_title": "exploring the impact of human evaluator group on chat-oriented dialogue evaluation",
    "citation_count": 0,
    "authors": [
      "Sarah E. Finch",
      "James D. Finch",
      "Jinho D. Choi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.611": {
    "title": "Exploring the Potential of Large Language Models (LLMs) for Low-resource Languages: A Study on Named-Entity Recognition (NER) and Part-Of-Speech (POS) Tagging for Nepali Language",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have made significant advancements in Natural Language Processing (NLP) by excelling in various NLP tasks. This study specifically focuses on evaluating the performance of LLMs for Named Entity Recognition (NER) and Part-of-Speech (POS) tagging for a low-resource language, Nepali. The aim is to study the effectiveness of these models for languages with limited resources by conducting experiments involving various parameters and fine-tuning and evaluating two datasets namely, ILPRL and EBIQUITY. In this work, we have experimented with eight LLMs for Nepali NER and POS tagging. While some prior works utilized larger datasets than ours, our contribution lies in presenting a comprehensive analysis of multiple LLMs in a unified setting. The findings indicate that NepBERTa, trained solely in the Nepali language, demonstrated the highest performance with F1-scores of 0.76 and 0.90 in ILPRL dataset. Similarly, it achieved 0.79 and 0.97 in EBIQUITY dataset for NER and POS respectively. This study not only highlights the potential of LLMs in performing classification tasks for low-resource languages but also compares their performance with that of alternative approaches deployed for the tasks",
    "checked": true,
    "id": "933802cd80c2249f887a433825a5eae3e66cd7e3",
    "semantic_title": "exploring the potential of large language models (llms) for low-resource languages: a study on named-entity recognition (ner) and part-of-speech (pos) tagging for nepali language",
    "citation_count": 0,
    "authors": [
      "Bipesh Subedi",
      "Sunil Regmi",
      "Bal Krishna Bal",
      "Praveen Acharya"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.612": {
    "title": "Exploring the Synergy of Dual-path Encoder and Alignment Module for Better Graph-to-Text Generation",
    "volume": "main",
    "abstract": "The mainstream approaches view the knowledge graph-to-text (KG-to-text) generation as a sequence-to-sequence task and fine-tune the pre-trained model (PLM) to generate the target text from the linearized knowledge graph. However, the linearization of knowledge graphs and the structure of PLMs lead to the loss of a large amount of graph structure information. Moreover, PLMs lack an explicit graph-text alignment strategy because of the discrepancy between structural and textual information. To solve these two problems, we propose a synergetic KG-to-text model with a dual-path encoder, an alignment module, and a guidance module. The dual-path encoder consists of a graph structure encoder and a text encoder, which can better encode the structure and text information of the knowledge graph. The alignment module contains a two-layer Transformer block and an MLP block, which aligns and integrates the information from the dual encoder. The guidance module combines an improved pointer network and an MLP block to avoid error-generated entities and ensures the fluency and accuracy of the generated text. Our approach obtains very competitive performance on three benchmark datasets. Our code is available from https://github.com/IMu-MachineLearningsxD/G2T",
    "checked": true,
    "id": "84d038a051d8e2a352dad148a3bb57c150c5c1fa",
    "semantic_title": "exploring the synergy of dual-path encoder and alignment module for better graph-to-text generation",
    "citation_count": 0,
    "authors": [
      "Tianxin Zhao",
      "Yingxin Liu",
      "Xiangdong Su",
      "Jiang Li",
      "Guanglai Gao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.613": {
    "title": "Exploring the Usability of Persuasion Techniques for Downstream Misinformation-related Classification Tasks",
    "volume": "main",
    "abstract": "We systematically explore the predictive power of features derived from Persuasion Techniques detected in texts, for solving different tasks of interest for media analysis; notably: detecting mis/disinformation, fake news, propaganda, partisan news and conspiracy theories. Firstly, we propose a set of meaningful features, aiming to capture the persuasiveness of a text. Secondly, we assess the discriminatory power of these features in different text classification tasks on 8 selected datasets from the literature using two metrics. We also evaluate the per-task discriminatory power of each Persuasion Technique and report on different insights. We find out that most of these features have a noticeable potential to distinguish conspiracy theories, hyperpartisan news and propaganda, while we observed mixed results in the context of fake news detection",
    "checked": true,
    "id": "cde0c801d5d21bdaaed1f6be1b735614b26137c4",
    "semantic_title": "exploring the usability of persuasion techniques for downstream misinformation-related classification tasks",
    "citation_count": 0,
    "authors": [
      "Nikolaos Nikolaidis",
      "Jakub Piskorski",
      "Nicolas Stefanovitch"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.614": {
    "title": "Extending AZee with Non-manual Gesture Rules for French Sign Language",
    "volume": "main",
    "abstract": "This paper presents a study on non-manual gestures, using a formal model named AZee. This is an approach which allows to formally represent Sign Language (SL) discourses, but also to animate them with a virtual signer. As non-manual gestures are essential in SL and therefore necessary for a quality synthesis, we wanted to extend AZee with them, by adding some production rules to the AZee production set. For this purpose, we applied a methodology which allows to find new production rules on a corpus representing one hour of French Sign Language, the 40 brèves (Challant and Filhol, 2022). 23 production rules for non-manual gestures in LSF have thus been determined. We took advantage of this study to directly insert these new rules in the first corpus of AZee discourses expressions, which describe with AZee the productions in SL of the 40 brèves corpus. 533 non-manual rules were inserted in the corpus, and some updates were made. This article proposes a new version of this AZee expressions corpus",
    "checked": true,
    "id": "439fb0cbe80b42925b8158e8caf5f629c8a45489",
    "semantic_title": "extending azee with non-manual gesture rules for french sign language",
    "citation_count": 0,
    "authors": [
      "Camille Challant",
      "Michael Filhol"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.615": {
    "title": "Extending the Discourse Analysis Tool Suite with Whiteboards for Visual Qualitative Analysis",
    "volume": "main",
    "abstract": "In this system demonstration paper, we describe the Whiteboards extension for an existing web-based platform for digital qualitative discourse analysis. Whiteboards comprise interactive graph-based interfaces to organize and manipulate objects, which can be qualitative research data, such as documents, images, etc., and analyses of these research data, such as annotations, tags, and code structures. The proposed extension offers a customizable view of the material and a wide range of actions that enable new ways of interacting and working with such resources. We show that the visualizations facilitate various use cases of qualitative data analysis, including reflection of the research process through sampling maps, creation of actor networks, and refining code taxonomies",
    "checked": true,
    "id": "3121449375953e3a3a9d18ab954c2d696d28902b",
    "semantic_title": "extending the discourse analysis tool suite with whiteboards for visual qualitative analysis",
    "citation_count": 0,
    "authors": [
      "Tim Fischer",
      "Florian Schneider",
      "Fynn Petersen-Frey",
      "Anja Silvia Mollah Haque",
      "Isabel Eiser",
      "Gertraud Koch",
      "Chris Biemann"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.616": {
    "title": "Extracting Biomedical Entities from Noisy Audio Transcripts",
    "volume": "main",
    "abstract": "Automatic Speech Recognition (ASR) technology is fundamental in transcribing spoken language into text, with considerable applications in the clinical realm, including streamlining medical transcription and integrating with Electronic Health Record (EHR) systems. Nevertheless, challenges persist, especially when transcriptions contain noise, leading to significant drops in performance when Natural Language Processing (NLP) models are applied. Named Entity Recognition (NER), an essential clinical task, is particularly affected by such noise, often termed the ASR-NLP gap. Prior works have primarily studied ASR's efficiency in clean recordings, leaving a research gap concerning the performance in noisy environments. This paper introduces a novel dataset, BioASR-NER, designed to bridge the ASR-NLP gap in the biomedical domain, focusing on extracting adverse drug reactions and mentions of entities from the Brief Test of Adult Cognition by Telephone (BTACT) exam. Our dataset offers a comprehensive collection of almost 2,000 clean and noisy recordings. In addressing the noise challenge, we present an innovative transcript-cleaning method using GPT-4, investigating both zero-shot and few-shot methodologies. Our study further delves into an error analysis, shedding light on the types of errors in transcription software, corrections by GPT-4, and the challenges GPT-4 faces. This paper aims to foster improved understanding and potential solutions for the ASR-NLP gap, ultimately supporting enhanced healthcare documentation practices",
    "checked": true,
    "id": "8fe69d5b86ed316d7f42e0b5c186a88fdd6b25e3",
    "semantic_title": "extracting biomedical entities from noisy audio transcripts",
    "citation_count": 0,
    "authors": [
      "Nima Ebadi",
      "Kellen Morgan",
      "Adrian Tan",
      "Billy Linares",
      "Sheri Osborn",
      "Emma Majors",
      "Jeremy Davis",
      "Anthony Rios"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.617": {
    "title": "Extracting Financial Events from Raw Texts via Matrix Chunking",
    "volume": "main",
    "abstract": "Event Extraction (EE) is widely used in the Chinese financial field to provide valuable structured information. However, there are two key challenges for Chinese financial EE in application scenarios. First, events need to be extracted from raw texts, which sets it apart from previous works like the Automatic Content Extraction (ACE) EE task, where EE is treated as a classification problem given the entity spans. Second, recognizing financial entities can be laborious, as they may involve multiple elements. In this paper, we introduce CFTE, a novel task for Chinese Financial Text-to-Event extraction, which directly extracts financial events from raw texts. We further present FINEED, a Chinese FINancial Event Extraction Dataset, and an efficient MAtrix-ChunKing method called MACK, designed for the extraction of financial events from raw texts. Specifically, FINEED is manually annotated with rich linguistic features. We propose a novel two-dimensional annotation method for FINEED, which can visualize the interactions among text components. Our MACK method is fault-tolerant by preserving the tag frequency distribution when identifying financial entities. We conduct extensive experiments and the results verify the effectiveness of our MACK method",
    "checked": true,
    "id": "7adb8cd98752a7e88b8e51deef05e13d829c1681",
    "semantic_title": "extracting financial events from raw texts via matrix chunking",
    "citation_count": 0,
    "authors": [
      "Yusheng Huang",
      "Ning Hu",
      "Kunping Li",
      "Nan Wang",
      "Zhouhan Lin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.618": {
    "title": "Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods",
    "volume": "main",
    "abstract": "Social determinants of health (SDoH) play a critical role in shaping health outcomes, particularly in pediatric populations where interventions can have long-term implications. SDoH are frequently studied in the Electronic Health Record (EHR), which provides a rich repository for diverse patient data. In this work, we present a novel annotated corpus, the Pediatric Social History Annotation Corpus (PedSHAC), and evaluate the automatic extraction of detailed SDoH representations using fine-tuned and in-context learning methods with Large Language Models (LLMs). PedSHAC comprises annotated social history sections from 1,260 clinical notes obtained from pediatric patients within the University of Washington (UW) hospital system. Employing an event-based annotation scheme, PedSHAC captures ten distinct health determinants to encompass living and economic stability, prior trauma, education access, substance use history, and mental health with an overall annotator agreement of 81.9 F1. Our proposed fine-tuning LLM-based extractors achieve high performance at 78.4 F1 for event arguments. In-context learning approaches with GPT-4 demonstrate promise for reliable SDoH extraction with limited annotated examples, with extraction performance at 82.3 F1 for event triggers",
    "checked": true,
    "id": "6969f96cca4bc77936c4bf1540921168c04d995e",
    "semantic_title": "extracting social determinants of health from pediatric patient notes using large language models: novel corpus and methods",
    "citation_count": 1,
    "authors": [
      "Yujuan Fu",
      "Giridhar Kaushik Ramachandran",
      "Nicholas J. Dobbins",
      "Namu Park",
      "Michael Leu",
      "Abby R. Rosenberg",
      "Kevin Lybarger",
      "Fei Xia",
      "Özlem Uzuner",
      "Meliha Yetisgen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.619": {
    "title": "Eye-Tracking Features Masking Transformer Attention in Question-Answering Tasks",
    "volume": "main",
    "abstract": "Eye movement features are considered to be direct signals reflecting human attention distribution with a low cost to obtain, inspiring researchers to augment language models with eye-tracking (ET) data. In this study, we select first fixation duration (FFD) and total reading time (TRT) as the cognitive signals to guide Transformer attention in question-answering (QA) tasks. We design three different ET attention masks based on the two features, either collected from human reading events or generated by a gaze-predicting model. We augment BERT and ALBERT models with attention masks structured based on the ET data. We find that augmenting a model with ET data carries linguistic features complementing the information captured by the model. It improves the models' performance but compromises the stability. Different Transformer models benefit from different types of ET attention masks, while ALBERT performs better than BERT. Moreover, ET data collected from real-life reading events has better model augmenting ability than the model-predicted data",
    "checked": true,
    "id": "988913a4f334eed4441ca38d8493528566cf359b",
    "semantic_title": "eye-tracking features masking transformer attention in question-answering tasks",
    "citation_count": 0,
    "authors": [
      "Leran Zhang",
      "Nora Hollenstein"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.620": {
    "title": "Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation",
    "volume": "main",
    "abstract": "Previous Sign Language Translation (SLT) methods achieve superior performance by relying on gloss annotations. However, labeling high-quality glosses is a labor-intensive task, which limits the further development of SLT. Although some approaches work towards gloss-free SLT through jointly training the visual encoder and translation network, these efforts still suffer from poor performance and inefficient use of the powerful Large Language Model (LLM). Most seriously, we find that directly introducing LLM into SLT will lead to insufficient learning of visual representations as LLM dominates the learning curve. To address these problems, we propose Factorized Learning assisted with Large Language Model (FLa-LLM) for gloss-free SLT. Concretely, we factorize the training process into two stages. In the visual initialing stage, we employ a lightweight translation model after the visual encoder to pre-train the visual encoder. In the LLM fine-tuning stage, we freeze the acquired knowledge in the visual encoder and integrate it with a pre-trained LLM to inspire the LLM's translation potential. This factorized training strategy proves to be highly effective as evidenced by significant improvements achieved across three SLT datasets which are all conducted under the gloss-free setting",
    "checked": true,
    "id": "1153e5379790c30b9c2c9bb111aec55965a1500f",
    "semantic_title": "factorized learning assisted with large language model for gloss-free sign language translation",
    "citation_count": 0,
    "authors": [
      "Zhigang Chen",
      "Benjia Zhou",
      "Jun Li",
      "Jun Wan",
      "Zhen Lei",
      "Ning Jiang",
      "Quan Lu",
      "Guoqing Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.621": {
    "title": "FaGANet: An Evidence-Based Fact-Checking Model with Integrated Encoder Leveraging Contextual Information",
    "volume": "main",
    "abstract": "In the face of the rapidly growing spread of false and misleading information in the real world, manual evidence-based fact-checking efforts become increasingly challenging and time-consuming. In order to tackle this issue, we propose FaGANet, an automated and accurate fact-checking model that leverages the power of sentence-level attention and graph attention network to enhance performance. This model adeptly integrates encoder-only models with graph attention network, effectively fusing claims and evidence information for accurate identification of even well-disguised data. Experiment results showcase the significant improvement in accuracy achieved by our FaGANet model, as well as its state-of-the-art performance in the evidence-based fact-checking task. We release our code and data in https://github.com/WeiyaoLuo/FaGANet",
    "checked": true,
    "id": "118b53ca7ec63f1fddfff1a44e45cc69d64f3bc4",
    "semantic_title": "faganet: an evidence-based fact-checking model with integrated encoder leveraging contextual information",
    "citation_count": 0,
    "authors": [
      "Weiyao Luo",
      "Junfeng Ran",
      "Zailong Tian",
      "Sujian Li",
      "Zhifang Sui"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.622": {
    "title": "FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based Sentiment Analysis",
    "volume": "main",
    "abstract": "Multi-domain aspect-based sentiment analysis (ABSA) seeks to capture fine-grained sentiment across diverse domains. While existing research narrowly focuses on single-domain applications constrained by methodological limitations and data scarcity, the reality is that sentiment naturally traverses multiple domains. Although large language models (LLMs) offer a promising solution for ABSA, it is difficult to integrate effectively with established techniques, including graph-based models and linguistics, because modifying their internal architecture is not easy. To alleviate this problem, we propose a novel framework, Feature-aware In-context Learning for Multi-domain ABSA (FaiMA). The core insight of FaiMA is to utilize in-context learning (ICL) as a feature-aware mechanism that facilitates adaptive learning in multi-domain ABSA tasks. Specifically, we employ a multi-head graph attention network as a text encoder optimized by heuristic rules for linguistic, domain, and sentiment features. Through contrastive learning, we optimize sentence representations by focusing on these diverse features. Additionally, we construct an efficient indexing mechanism, allowing FaiMA to stably retrieve highly relevant examples across multiple dimensions for any given input. To evaluate the efficacy of FaiMA, we build the first multi-domain ABSA benchmark dataset. Extensive experimental results demonstrate that FaiMA achieves significant performance improvements in multiple domains compared to baselines, increasing F1 by 2.07% on average. Source code and data sets are available at https://github.com/SupritYoung/FaiMA",
    "checked": true,
    "id": "cd75f7cacd0d4b88e6679d0dda03f2695acca4ae",
    "semantic_title": "faima: feature-aware in-context learning for multi-domain aspect-based sentiment analysis",
    "citation_count": 0,
    "authors": [
      "Songhua Yang",
      "Xinke Jiang",
      "Hanjie Zhao",
      "Wenxuan Zeng",
      "Hongde Liu",
      "Yuxiang Jia"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.623": {
    "title": "FAIRification of LeiLanD",
    "volume": "main",
    "abstract": "LeiLanD (Leiden Language Data) is a searchable catalogue initiated by the Leiden University Centre for Linguistics (LUCL) with the support of CLARIAH. The catalogue contains metadata about language datasets collected at LUCL and other institutes of Leiden University. This paper describes a project to FAIRify the datasets increasing their findability and accessibility through a standardised metadata format CMDI so as to obtain a rich metadata description for all resources and to make them findable through CLARIN's Virtual Language Observatory. The paper describes the creation of the catalogue and the steps that led from unstructured metadata to CMDI standards. This FAIRifi- cation of LeiLanD has enhanced the findability and accessibility of incredibly diverse collection of language datasets",
    "checked": true,
    "id": "e816a004d7fc506302730ca60d36a60526663916",
    "semantic_title": "fairification of leiland",
    "citation_count": 0,
    "authors": [
      "Eric Sanders",
      "Sara Petrollino",
      "Gilles R. Scheifer",
      "Henk van den Heuvel",
      "Christopher Handy"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.624": {
    "title": "FalAI: A Dataset for End-to-end Spoken Language Understanding in a Low-Resource Scenario",
    "volume": "main",
    "abstract": "End-to-end (E2E) Spoken Language Understanding (SLU) systems infer structured information directly from the speech signal using a single model. Due to the success of virtual assistants and the increasing demand for speech interfaces, these architectures are being actively researched for their potential to improve system performance by exploiting acoustic information and avoiding the cascading errors of traditional architectures. However, these systems require large amounts of specific, well-labelled speech data for training, which is expensive to obtain even in English, where the number of public audio datasets for SLU is limited. In this paper, we release the FalAI dataset, the largest public SLU dataset in terms of hours (250 hours), recordings (260,000) and participants (over 10,000), which is also the first SLU dataset in Galician and the first to be obtained in a low-resource scenario. Furthermore, we present new measures of complexity for the text corpora, the strategies followed for the design, collection and validation of the dataset, and we define splits for noisy audio, hesitant audio and audio where the sentence has changed but the structured information is preserved. These novel splits provide a unique resource for testing SLU systems in challenging, real-world scenarios",
    "checked": true,
    "id": "d15514983e11d4799b96732d65c127f8ef46cb1c",
    "semantic_title": "falai: a dataset for end-to-end spoken language understanding in a low-resource scenario",
    "citation_count": 0,
    "authors": [
      "Andres Pineiro-Martin",
      "Carmen Garcia-Mateo",
      "Laura Docio-Fernandez",
      "Maria del Carmen Lopez-Perez",
      "Jose Gandarela-Rodriguez"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.625": {
    "title": "Fast Adaptation via Prompted Data: An Efficient Cross-Domain Fine-tuning Method for Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have achieved great success in a variety of natural language understanding tasks. However, domain discrepancies between the downstream task and the pre-training corpora may have hurdled LLMs to excel further in the vertical applications. Contrary to prior computational-heavy methods, we propose a lightweight solution to further bridge the gap in applying LLMs to diverse downstream tasks — a Fast Adaptation method for LLMs via Prompted Data, in short FAvPD. Notably, with FAvPD, we establish an additional adaptive tuning procedure, wherein we integrate downstream text corpora, gold labels as well as external knowledge sources and then envelop them into a form of highly controllable prompt. As a simple, easy-to-use, and versatile solution, FAvPD lies in the intersection of regimes like knowledge-augmented LLMs, fine-tuning, and adaptation techniques. With extensive experiments, we prove that FAvPD excels in both performance efficacy and training efficiency over related prior works. FAvPD is publicly available at https://github.com/Hyatio/FAvPD",
    "checked": true,
    "id": "47ad867a1adb71475a36861cb6a107cc5a341483",
    "semantic_title": "fast adaptation via prompted data: an efficient cross-domain fine-tuning method for large language models",
    "citation_count": 0,
    "authors": [
      "Yiming Zhang",
      "Hantao Yang",
      "Haobo Wang",
      "Jake Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.626": {
    "title": "FastSpell: The LangId Magic Spell",
    "volume": "main",
    "abstract": "Language identification is a crucial component in the automated production of language resources, particularly in multilingual and big data contexts. However, commonly used language identifiers struggle to differentiate between similar or closely-related languages. This paper introduces FastSpell, a language identifier that combines fastText (a pre-trained language identifier tool) and Hunspell (a spell checker) with the aim of having a refined second-opinion before deciding which language should be assigned to a text. We provide a description of the FastSpell algorithm along with an explanation on how to use and configure it. To that end, we motivate the need of such a tool and present a benchmark including some popular language identifiers evaluated during the development of FastSpell. We show how FastSpell is useful not only to improve identification of similar languages, but also to identify new ones ignored by other tools",
    "checked": true,
    "id": "f71f4dda501cea806cbe59c924e60308625e1e87",
    "semantic_title": "fastspell: the langid magic spell",
    "citation_count": 0,
    "authors": [
      "Marta Bañón",
      "Gema Ramírez-Sánchez",
      "Jaume Zaragoza-Bernabeu",
      "Sergio Ortiz Rojas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.627": {
    "title": "FCDS: Fusing Constituency and Dependency Syntax into Document-Level Relation Extraction",
    "volume": "main",
    "abstract": "Document-level Relation Extraction (DocRE) aims to identify relation labels between entities within a single document. It requires handling several sentences and reasoning over them. State-of-the-art DocRE methods use a graph structure to connect entities across the document to capture dependency syntax information. However, this is insufficient to fully exploit the rich syntax information in the document. In this work, we propose to fuse constituency and dependency syntax into DocRE. It uses constituency syntax to aggregate the whole sentence information and select the instructive sentences for the pairs of targets. It exploits dependency syntax in a graph structure with constituency syntax enhancement and chooses the path between entity pairs based on the dependency graph. The experimental results on datasets from various domains demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "ebbb34f492eb3fe47be897b8e71d097552fec2a5",
    "semantic_title": "fcds: fusing constituency and dependency syntax into document-level relation extraction",
    "citation_count": 0,
    "authors": [
      "Xudong Zhu",
      "Zhao Kang",
      "Bei Hui"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.628": {
    "title": "Feature Structure Matching for Multi-source Sentiment Analysis with Efficient Adaptive Tuning",
    "volume": "main",
    "abstract": "Recently, fine-tuning the large pre-trained language models on the labeled sentiment dataset achieves appealing performance. However, the obtained model may not generalize well to the other domains due to the domain shift, and it is expensive to update the entire parameters within the large models. Although some existing domain matching methods are proposed to alleviate the above issues, there are multiple relevant source domains in practice which makes the whole training more costly and complicated. To this end, we focus on the efficient unsupervised multi-source sentiment adaptation task which is more challenging and beneficial for real-world applications. Specifically, we propose to extract multi-layer features from the large pre-trained model, and design a dynamic parameters fusion module to exploit these features for both efficient and adaptive tuning. Furthermore, we propose a novel feature structure matching constraint, which enforces similar feature-wise correlations across different domains. Compared with the traditional domain matching methods which tend to pull all feature instances close, we show that the proposed feature structure matching is more robust and generalizable in the multi-source scenario. Extensive experiments on several multi-source sentiment analysis benchmarks demonstrate the effectiveness and superiority of our proposed framework",
    "checked": true,
    "id": "3595495e95c2f7dfef6308fbbe89b8f4e410c0ec",
    "semantic_title": "feature structure matching for multi-source sentiment analysis with efficient adaptive tuning",
    "citation_count": 0,
    "authors": [
      "Rui Li",
      "Cheng Liu",
      "Yu Tong",
      "Jiang Dazhi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.629": {
    "title": "Federated Document-Level Biomedical Relation Extraction with Localized Context Contrast",
    "volume": "main",
    "abstract": "Existing studies on relation extraction focus at the document level in a centralized training environment, requiring the collection of documents from various sources. However, this raises concerns about privacy protection, especially in sensitive domains such as finance and healthcare. For the first time, this work extends document-level relation extraction to a federated environment. The proposed federated framework, called FedLCC, is tailored for biomedical relation extraction that enables collaborative training without sharing raw medical texts. To fully exploit the models of all participating clients and improve the local training on individual clients, we propose a novel concept of localized context contrast on the basis of contrastive learning. By comparing and rectifying the similarity of localized context in documents between clients and the central server, the global model can better represent the documents on individual clients. Due to the lack of a widely accepted measure of non-IID text data, we introduce a novel non-IID scenario based on graph structural entropy. Experimental results on three document-level biomedical relation extraction datasets demonstrate the effectiveness of our method. Our code is available at https://github.com/xxxxyan/FedLCC",
    "checked": true,
    "id": "b8ed55e743f7be7616f6d764d479f2acf295a79b",
    "semantic_title": "federated document-level biomedical relation extraction with localized context contrast",
    "citation_count": 0,
    "authors": [
      "Yan Xiao",
      "Yaochu Jin",
      "Kuangrong Hao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.630": {
    "title": "Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models",
    "volume": "main",
    "abstract": "Foundation Models (FMs), such as LLaMA, BERT, GPT, ViT, and CLIP, have demonstrated remarkable success in a wide range of applications, driven by their ability to leverage vast amounts of data for pre-training. However, optimizing FMs often requires access to sensitive data, raising privacy concerns and limiting their applicability in many domains. In this paper, we propose the Federated Foundation Models (FFMs) paradigm, which combines the benefits of FMs and Federated Learning (FL) to enable privacy-preserving and collaborative learning across multiple end-users. We discuss the potential benefits and challenges of integrating FL into the lifespan of FMs, covering pre-training, fine-tuning, and application. We further outline potential future research avenues in FFM, including FFM pre-training, FFM fine-tuning, and federated prompt tuning, which allow the development of more personalized and context-aware models while ensuring data privacy. Moreover, we explore the possibility of continual/lifelong learning in FFMs, as increased computational power at the edge may unlock the potential for optimizing FMs using newly generated private data close to the data source. The proposed FFM concepts offer a flexible and scalable framework for training large language models in a privacy-preserving manner, setting the stage for subsequent advancements in both FM training and federated learning",
    "checked": true,
    "id": "aa6ba4ade170abfb6c6c99d3ab5f1957b6ccec83",
    "semantic_title": "federated foundation models: privacy-preserving and collaborative learning for large models",
    "citation_count": 22,
    "authors": [
      "Sixing Yu",
      "Juan Pablo Munoz",
      "Ali Jannesari"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.631": {
    "title": "Few-Shot Learning for Cold-Start Recommendation",
    "volume": "main",
    "abstract": "Cold-start is a significant problem in recommender systems. Recently, with the development of few-shot learning and meta-learning techniques, many researchers have devoted themselves to adopting meta-learning into recommendation as the natural scenario of few-shots. Nevertheless, we argue that recent work has a huge gap between few-shot learning and recommendations. In particular, users are locally dependent, not globally independent in recommendation. Therefore, it is necessary to formulate the local relationships between users. To accomplish this, we present a novel Few-shot learning method for Cold-Start (FCS) recommendation that consists of three hierarchical structures. More concretely, this first hierarchy is the global-meta parameters for learning the global information of all users; the second hierarchy is the local-meta parameters whose goal is to learn the adaptive cluster of local users; the third hierarchy is the specific parameters of the target user. Both the global and local information are formulated, addressing the new user's problem in accordance with the few-shot records rapidly. Experimental results on two public real-world datasets show that the FCS method could produce stable improvements compared with the state-of-the-art",
    "checked": true,
    "id": "aa044a62b743eb9d57807666cd6f83b9e8ad8603",
    "semantic_title": "few-shot learning for cold-start recommendation",
    "citation_count": 0,
    "authors": [
      "Mingming Li",
      "Songlin Hu",
      "Fuqing Zhu",
      "Qiannan Zhu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.632": {
    "title": "Few-shot Link Prediction on Hyper-relational Facts",
    "volume": "main",
    "abstract": "Hyper-relational facts, which consist of a primary triple (head entity, relation, tail entity) and auxiliary attribute-value pairs, are widely present in real-world Knowledge Graphs (KGs). Link Prediction on Hyper-relational Facts (LPHFs) is to predict a missing element in a hyper-relational fact, which helps populate and enrich KGs. However, existing LPHFs studies usually require an amount of high-quality data. They overlook few-shot relations, which have limited instances, yet are common in real-world scenarios. Thus, we introduce a new task, Few-Shot Link Prediction on Hyper-relational Facts (FSLPHFs). It aims to predict a missing entity in a hyper-relational fact with limited support instances. To tackle FSLPHFs, we propose MetaRH, a model that learns Meta Relational information in Hyper-relational facts. MetaRH comprises three modules: relation learning, support-specific adjustment, and query inference. By capturing meta relational information from limited support instances, MetaRH can accurately predict the missing entity in a query. As there is no existing dataset available for this new task, we construct three datasets to validate the effectiveness of MetaRH. Experimental results on these datasets demonstrate that MetaRH significantly outperforms existing representative models",
    "checked": true,
    "id": "7f2814248162f3dc11959c2b53af56292528aeb6",
    "semantic_title": "few-shot link prediction on hyper-relational facts",
    "citation_count": 0,
    "authors": [
      "Jiyao Wei",
      "Saiping Guan",
      "Xiaolong Jin",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.633": {
    "title": "Few-Shot Multimodal Named Entity Recognition Based on Mutlimodal Causal Intervention Graph",
    "volume": "main",
    "abstract": "Multimodal Named Entity Recognition (MNER) models typically require a significant volume of labeled data for effective training to extract relations between entities. In real-world scenarios, we frequently encounter unseen relation types. Nevertheless, existing methods are predominantly tailored for complete datasets and are not equipped to handle these new relation types. In this paper, we introduce the Few-shot Multimodal Named Entity Recognition (FMNER) task to address these novel relation types. FMNER trains in the source domain (seen types) and tests in the target domain (unseen types) with different distributions. Due to limited available resources for sampling, each sampling instance yields different content, resulting in data bias and alignment problems of multimodal units (image patches and words). To alleviate the above challenge, we propose a novel Multimodal causal Intervention graphs (MOUSING) model for FMNER. Specifically, we begin by constructing a multimodal graph that incorporates fine-grained information from multiple modalities. Subsequently, we introduce the Multimodal Causal Intervention Strategy to update the multimodal graph. It aims to decrease spurious correlations and emphasize accurate correlations between multimodal units, resulting in effectively aligned multimodal representations. Extensive experiments on two multimodal named entity recognition datasets demonstrate the superior performance of our model in the few-shot setting",
    "checked": true,
    "id": "71db21a770d475389f6936ff077806f1e7594e4b",
    "semantic_title": "few-shot multimodal named entity recognition based on mutlimodal causal intervention graph",
    "citation_count": 0,
    "authors": [
      "Feihong Lu",
      "Xiaocui Yang",
      "Qian Li",
      "Qingyun Sun",
      "Ke Jiang",
      "Cheng Ji",
      "Jianxin Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.634": {
    "title": "Few-shot Named Entity Recognition via Superposition Concept Discrimination",
    "volume": "main",
    "abstract": "Few-shot NER aims to identify entities of target types with only limited number of illustrative instances. Unfortunately, few-shot NER is severely challenged by the intrinsic precise generalization problem, i.e., it is hard to accurately determine the desired target type due to the ambiguity stemming from information deficiency. In this paper, we propose Superposition Concept Discriminator (SuperCD), which resolves the above challenge via an active learning paradigm. Specifically, a concept extractor is first introduced to identify superposition concepts from illustrative instances, with each concept corresponding to a possible generalization boundary. Then a superposition instance retriever is applied to retrieve corresponding instances of these superposition concepts from large-scale text corpus. Finally, annotators are asked to annotate the retrieved instances and these annotated instances together with original illustrative instances are used to learn FS-NER models. To this end, we learn a universal concept extractor and superposition instance retriever using a large-scale openly available knowledge bases. Experiments show that SuperCD can effectively identify superposition concepts from illustrative instances, retrieve superposition instances from large-scale corpus, and significantly improve the few-shot NER performance with minimal additional efforts",
    "checked": true,
    "id": "c4675b7c11c60aaf21b58037fa7f77cbfb070298",
    "semantic_title": "few-shot named entity recognition via superposition concept discrimination",
    "citation_count": 0,
    "authors": [
      "Jiawei Chen",
      "Hongyu Lin",
      "Xianpei Han",
      "Yaojie Lu",
      "Shanshan Jiang",
      "Bin Dong",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.635": {
    "title": "Few-Shot Relation Extraction with Hybrid Visual Evidence",
    "volume": "main",
    "abstract": "The goal of few-shot relation extraction is to predict relations between name entities in a sentence when only a few labeled instances are available for training. Existing few-shot relation extraction methods focus on uni-modal information such as text only. This reduces performance when there is no clear contexts between the name entities described in text. We propose a multi-modal few-shot relation extraction model (MFS-HVE) that leverages both textual and visual semantic information to learn a multi-modal representation jointly. The MFS-HVE includes semantic feature extractors and multi-modal fusion components. The MFS-HVE semantic feature extractors are developed to extract both textual and visual features. The visual features include global image features and local object features within the image. The MFS-HVE multi-modal fusion unit integrates information from various modalities using image-guided attention, object-guided attention, and hybrid feature attention to fully capture the semantic interaction between visual regions of images and relevant texts. Extensive experiments conducted on two public datasets demonstrate that semantic visual information significantly improves performance of few-shot relation prediction",
    "checked": true,
    "id": "4026849c54041d4dad62fa8cf3e130bea242f5b4",
    "semantic_title": "few-shot relation extraction with hybrid visual evidence",
    "citation_count": 0,
    "authors": [
      "Jiaying Gong",
      "Hoda Eldardiry"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.636": {
    "title": "Few-Shot Semantic Dependency Parsing via Graph Contrastive Learning",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have achieved promising performance on semantic dependency parsing (SDP), owing to their powerful graph representation learning ability. However, training a high-performing GNN-based model requires a large amount of labeled data and it is prone to over-fitting in the absence of sufficient labeled data. To address this drawback, we propose a syntax-guided graph contrastive learning framework to pre-train GNNs with plenty of unlabeled data and fine-tune pre-trained GNNs with few-shot labeled SDP data. Through extensive experiments conducted on the SemEval-2015 Task 18 English dataset in three formalisms (DM, PAS, and PSD), we demonstrate that our framework achieves promising results when few-shot training samples are available. Furthermore, benefiting from the pre-training process, our framework exhibits notable advantages in the out-of-domain test sets",
    "checked": true,
    "id": "83a9576fa834979bd366a4c178bc41b02c953942",
    "semantic_title": "few-shot semantic dependency parsing via graph contrastive learning",
    "citation_count": 0,
    "authors": [
      "Bin Li",
      "Yunlong Fan",
      "Yikemaiti Sataer",
      "Chuanqi Shi",
      "Miao Gao",
      "Zhiqiang Gao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.637": {
    "title": "Few-shot Temporal Pruning Accelerates Diffusion Models for Text Generation",
    "volume": "main",
    "abstract": "Diffusion models have achieved significant success in computer vision and shown immense potential in natural language processing applications, particularly for text generation tasks. However, generating high-quality text using these models often necessitates thousands of iterations, leading to slow sampling rates. Existing acceleration methods either neglect the importance of the distribution of sampling steps, resulting in compromised performance with smaller number of iterations, or require additional training, introducing considerable computational overheads. In this paper, we present Few-shot Temporal Pruning, a novel technique designed to accelerate diffusion models for text generation without supplementary training while effectively leveraging limited data. Employing a Bayesian optimization approach, our method effectively eliminates redundant sampling steps during the sampling process, thereby enhancing the generation speed. A comprehensive evaluation of discrete and continuous diffusion models across various tasks, including machine translation, question generation, and paraphrasing, reveals that our approach achieves competitive performance even with minimal sampling steps after down to less than 1 minute of optimization, yielding a significant acceleration of up to 400x in text generation tasks",
    "checked": true,
    "id": "006cf91d266077d22dceed39a61e32cfe1b4fcff",
    "semantic_title": "few-shot temporal pruning accelerates diffusion models for text generation",
    "citation_count": 0,
    "authors": [
      "Bocheng Li",
      "Zhujin Gao",
      "Yongxin Zhu",
      "Kun Yin",
      "Haoyu Cao",
      "Deqiang Jiang",
      "Linli Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.638": {
    "title": "FFSTC: Fongbe to French Speech Translation Corpus",
    "volume": "main",
    "abstract": "In this paper, we introduce the Fongbe to French Speech Translation Corpus (FFSTC). This corpus encompasses approximately 31 hours of collected Fongbe language content, featuring both French transcriptions and corresponding Fongbe voice recordings. FFSTC represents a comprehensive dataset compiled through various collection methods and the efforts of dedicated individuals. Furthermore, we conduct baseline experiments using Fairseq's transformer_s and conformer models to evaluate data quality and validity. Our results indicate a score BLEU of 8.96 for the transformer_s model and 8.14 for the conformer model, establishing a baseline for the FFSTC corpus",
    "checked": true,
    "id": "6e6e5b13ea33cc4dc4bf0dae8f0febe97bdedfc8",
    "semantic_title": "ffstc: fongbe to french speech translation corpus",
    "citation_count": 0,
    "authors": [
      "D. Fortuné Kponou",
      "Fréjus A. A. Laleye",
      "Eugène Cokou Ezin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.639": {
    "title": "FinCorpus-DE10k: A Corpus for the German Financial Domain",
    "volume": "main",
    "abstract": "We introduce a predominantly German corpus comprising 12.5k PDF documents sourced from the financial domain. The corresponding extracted textual data encompasses more than 165 million tokens derived predominantly from German, and to a lesser extent, bilingual documents. We provide detailed information about the document types included in the corpus, such as final terms, base prospectuses, annual reports, information materials, law documents, international financial reporting standards, and monthly reports from the Bundesbank, accompanied by comprehensive statistical analysis. To our knowledge, it is the first non-email German financial corpus available, and we hope it will fill this gap and foster further research in the financial domain both in the German language and in multilingual contexts",
    "checked": true,
    "id": "69391ed3db9e75a27f7e3037c9abfd24b13ce8e0",
    "semantic_title": "fincorpus-de10k: a corpus for the german financial domain",
    "citation_count": 0,
    "authors": [
      "Serhii Hamotskyi",
      "Nata Kozaeva",
      "Christian Hänig"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.640": {
    "title": "Finding Educationally Supportive Contexts for Vocabulary Learning with Attention-Based Models",
    "volume": "main",
    "abstract": "When learning new vocabulary, both humans and machines acquire critical information about the meaning of an unfamiliar word through contextual information in a sentence or passage. However, not all contexts are equally helpful for learning an unfamiliar ‘target' word. Some contexts provide a rich set of semantic clues to the target word's meaning, while others are less supportive. We explore the task of finding educationally supportive contexts with respect to a given target word for vocabulary learning scenarios, particularly for improving student literacy skills. Because of their inherent context-based nature, attention-based deep learning methods provide an ideal starting point. We evaluate attention-based approaches for predicting the amount of educational support from contexts, ranging from a simple custom model using pre-trained embeddings with an additional attention layer, to a commercial Large Language Model (LLM). Using an existing major benchmark dataset for educational context support prediction, we found that a sophisticated but generic LLM had poor performance, while a simpler model using a custom attention-based approach achieved the best-known performance to date on this dataset",
    "checked": true,
    "id": "0acffa66a337d13cd59b0b36ae292ea3a8db6d98",
    "semantic_title": "finding educationally supportive contexts for vocabulary learning with attention-based models",
    "citation_count": 0,
    "authors": [
      "Sungjin Nam",
      "Kevyn Collins-Thompson",
      "David Jurgens",
      "Xin Tong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.641": {
    "title": "Finding Spoken Identifications: Using GPT-4 Annotation for an Efficient and Fast Dataset Creation Pipeline",
    "volume": "main",
    "abstract": "The growing emphasis on fairness in speech-processing tasks requires datasets with speakers from diverse subgroups that allow training and evaluating fair speech technology systems. However, creating such datasets through manual annotation can be costly. To address this challenge, we present a semi-automated dataset creation pipeline that leverages large language models. We use this pipeline to generate a dataset of speakers identifying themself or another speaker as belonging to a particular race, ethnicity, or national origin group. We use OpenaAI's GPT-4 to perform two complex annotation tasks- separating files relevant to our intended dataset from the irrelevant ones (filtering) and finding and extracting information on identifications within a transcript (tagging). By evaluating GPT-4's performance using human annotations as ground truths, we show that it can reduce resources required by dataset annotation while barely losing any important information. For the filtering task, GPT-4 had a very low miss rate of 6.93%. GPT-4's tagging performance showed a trade-off between precision and recall, where the latter got as high as 97%, but precision never exceeded 45%. Our approach reduces the time required for the filtering and tagging tasks by 95% and 80%, respectively. We also present an in-depth error analysis of GPT-4's performance",
    "checked": true,
    "id": "7699ecdd843d6c7d87886ee876c89c61285462ac",
    "semantic_title": "finding spoken identifications: using gpt-4 annotation for an efficient and fast dataset creation pipeline",
    "citation_count": 0,
    "authors": [
      "Maliha Jahan",
      "Helin Wang",
      "Thomas Thebaud",
      "Yinglun Sun",
      "Giang Ha Le",
      "Zsuzsanna Fagyal",
      "Odette Scharenborg",
      "Mark Hasegawa-Johnson",
      "Laureano Moro Velazquez",
      "Najim Dehak"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.642": {
    "title": "Find-the-Common: A Benchmark for Explaining Visual Patterns from Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuting Shi",
      "Naoya Inoue",
      "Houjing Wei",
      "Yufeng Zhao",
      "Tao Jin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.643": {
    "title": "Fine-grained Classification of Circumstantial Meanings within the Prague Dependency Treebank Annotation Scheme",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marie Mikulova"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.644": {
    "title": "Fine-Grained Legal Argument-Pair Extraction via Coarse-Grained Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaojun Xiao",
      "Yutao Sun",
      "Yuan Yao",
      "Xu Han",
      "Wenbin Zhang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.645": {
    "title": "Fine-Tuning a Pre-Trained Wav2Vec2 Model for Automatic Speech Recognition- Experiments with De Zahrar Sproche",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Gulli",
      "Francesco Costantini",
      "Diego Sidraschi",
      "Emanuela Li Destri"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.646": {
    "title": "First Steps Towards the Integration of Resources on Historical Glossing Traditions in the History of Chinese: A Collection of Standardized Fǎnqiè Spellings from the Guǎngyùn",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michele Pulini",
      "Johann-Mattis List"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.647": {
    "title": "Fisher Mask Nodes for Language Model Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thennal D K",
      "Ganesh Nathan",
      "Suchithra M S"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.648": {
    "title": "FlattenQuant: Breaking through the Inference Compute-bound for Large Language Models with Per-tensor Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zhang",
      "Fei Yang",
      "Shuang Peng",
      "Fangyu Wang",
      "Aimin Pan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.649": {
    "title": "Flexible Lexicalization in Rule-based Text Realization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avril Gazeau",
      "Francois Lareau"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.650": {
    "title": "FLOR: On the Effectiveness of Language Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Severino Da Dalt",
      "Joan Llop",
      "Irene Baucells",
      "Marc Pamies",
      "Yishi Xu",
      "Aitor Gonzalez-Agirre",
      "Marta Villegas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.651": {
    "title": "FoRC4CL: A Fine-grained Field of Research Classification and Annotated Dataset of NLP Articles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raia Abu Ahmad",
      "Ekaterina Borisova",
      "Georg Rehm"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.652": {
    "title": "FORECAST2023: A Forecast and Reasoning Corpus of Argumentation Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kamila Górska",
      "John Lawrence",
      "Chris Reed"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.653": {
    "title": "FoTo: Targeted Visual Topic Modeling for Focused Analysis of Short Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanuj Kumar",
      "Tuan Le"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.654": {
    "title": "FRACAS: a FRench Annotated Corpus of Attribution relations in newS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ange Richard",
      "Laura Cristina Alonzo Canul",
      "François Portet"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.655": {
    "title": "Frame2: A FrameNet-based Multimodal Dataset for Tackling Text-image Interactions in Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederico Belcavello",
      "Tiago Timponi Torrent",
      "Ely E. Matos",
      "Adriana S. Pagano",
      "Maucha Gamonal",
      "Natalia Sigiliano",
      "Lívia Vicente Dutra",
      "Helen de Andrade Abreu",
      "Mairon Samagaio",
      "Mariane Carvalho",
      "Franciany Campos",
      "Gabrielly Azalim",
      "Bruna Mazzei",
      "Mateus Fonseca de Oliveira",
      "Ana Carolina Loçasso Luz",
      "Lívia Pádua Ruiz",
      "Júlia Bellei",
      "Amanda Pestana",
      "Josiane Costa",
      "Iasmin Rabelo",
      "Anna Beatriz Silva",
      "Raquel Roza",
      "Mariana Souza",
      "Igor Oliveira"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.656": {
    "title": "Framed Multi30K: A Frame-Based Multimodal-Multilingual Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcelo Viridiano",
      "Arthur Lorenzi",
      "Tiago Timponi Torrent",
      "Ely E. Matos",
      "Adriana S. Pagano",
      "Natália Sathler Sigiliano",
      "Maucha Gamonal",
      "Helen de Andrade Abreu",
      "Lívia Vicente Dutra",
      "Mairon Samagaio",
      "Mariane Carvalho",
      "Franciany Campos",
      "Gabrielly Azalim",
      "Bruna Mazzei",
      "Mateus Fonseca de Oliveira",
      "Ana Carolina Luz",
      "Livia Padua Ruiz",
      "Júlia Bellei",
      "Amanda Pestana",
      "Josiane Costa",
      "Iasmin Rabelo",
      "Anna Beatriz Silva",
      "Raquel Roza",
      "Mariana Souza Mota",
      "Igor Oliveira",
      "Márcio Henrique Pelegrino de Freitas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.657": {
    "title": "FRASIMED: A Clinical French Annotated Resource Produced through Crosslingual BERT-Based Annotation Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jamil Zaghir",
      "Mina Bjelogrlic",
      "Jean-Philippe Goldman",
      "Soukaïna Aananou",
      "Christophe Gaudet-Blavignac",
      "Christian Lovis"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.658": {
    "title": "FReND: A French Resource of Negation Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hafida Le Cloirec - Ait Yahya",
      "Olga Seminck",
      "Pascal Amsili"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.659": {
    "title": "From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ang Li",
      "Qiangchao Chen",
      "Yiquan Wu",
      "Xiang Zhou",
      "Kun Kuang",
      "Fei Wu",
      "Ming Cai"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.660": {
    "title": "From Laughter to Inequality: Annotated Dataset for Misogyny Detection in Tamil and Malayalam Memes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Ponnusamy",
      "Kathiravan Pannerselvam",
      "Saranya R",
      "Prasanna Kumar Kumaresan",
      "Sajeetha Thavareesan",
      "Bhuvaneswari S",
      "Anshid K.a",
      "Susminu S Kumar",
      "Paul Buitelaar",
      "Bharathi Raja Chakravarthi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.661": {
    "title": "From Linguistic Linked Data to Big Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitar Trajanov",
      "Elena Apostol",
      "Radovan Garabik",
      "Katerina Gkirtzou",
      "Dagmar Gromann",
      "Chaya Liebeskind",
      "Cosimo Palma",
      "Michael Rosner",
      "Alexia Sampri",
      "Gilles Sérasset",
      "Blerina Spahiu",
      "Ciprian-Octavian Truică",
      "Giedre Valunaite Oleskeviciene"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.662": {
    "title": "From News to Summaries: Building a Hungarian Corpus for Extractive and Abstractive Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Botond Barta",
      "Dorina Lakatos",
      "Attila Nagy",
      "Milán Konor Nyist",
      "Judit Ács"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.663": {
    "title": "From Technology to Market. Bilingual Corpus on the Evaluation of Technology Opportunity Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Hazem",
      "Kazuyuki Motohashi",
      "Chen Zhu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.664": {
    "title": "From Text to Historical Ecological Knowledge: The Construction and Application of the Shan Jing Knowledge Base",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Liang",
      "Chu-Ren Huang",
      "Xin-Lan Jiang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.665": {
    "title": "From Text to Source: Results in Detecting Large Language Model-Generated Content",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wissam Antoun",
      "Benoît Sagot",
      "Djamé Seddah"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.666": {
    "title": "FUSE - FrUstration and Surprise Expressions: A Subtle Emotional Multimodal Language Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajesh Titung",
      "Cecilia Ovesdotter Alm"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.667": {
    "title": "Fusion-in-T5: Unifying Variant Signals for Simple and Effective Document Ranking with Attention Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shi Yu",
      "Chenghao Fan",
      "Chenyan Xiong",
      "David Jin",
      "Zhiyuan Liu",
      "Zhenghao Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.668": {
    "title": "GAATME: A Genetic Algorithm for Adversarial Translation Metrics Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josef Jon",
      "Ondřej Bojar"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.669": {
    "title": "GCNet: Global-and-Context Collaborative Learning for Aspect-Based Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Zhou",
      "Ying Shen",
      "Yinghui Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.670": {
    "title": "GECSum: Generative Evaluation-Driven Sequence Level Contrastive Learning for Abstractive Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawen Xie",
      "Shaoting Zhang",
      "Xiaofan Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.671": {
    "title": "Gendered Grammar or Ingrained Bias? Exploring Gender Bias in Icelandic Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steinunn Rut Friðriksdóttir",
      "Hafsteinn Einarsson"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.672": {
    "title": "Generating Clarification Questions for Disambiguating Contracts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anmol Singhal",
      "Chirag Jain",
      "Preethu Rose Anish",
      "Arkajyoti Chakraborty",
      "Smita Ghaisas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.673": {
    "title": "Generating Contextual Images for Long-Form Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avijit Mitra",
      "Nalin Gupta",
      "Chetan Naik",
      "Abhinav Sethy",
      "Kinsey Bice",
      "Zeynab Raeesy"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.674": {
    "title": "Generating Hard-Negative Out-of-Scope Data with ChatGPT for Intent Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijian Li",
      "Stefan Larson",
      "Kevin Leach"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.675": {
    "title": "Generating Multiple-choice Questions for Medical Question Answering with Distractors and Cue-masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damien Sileo",
      "Kanimozhi Uma",
      "Marie-Francine Moens"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.676": {
    "title": "Generative Multimodal Entity Linking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Senbao Shi",
      "Zhenran Xu",
      "Baotian Hu",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.677": {
    "title": "GENTRAC: A Tool for Tracing Trauma in Genocide and Mass Atrocity Court Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miriam Schirmer",
      "Christian Brechenmacher",
      "Juergen Pfeffer"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.678": {
    "title": "Geographically-Informed Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Dunn",
      "Lane Edwards-Brown"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.679": {
    "title": "GerDISDETECT: A German Multilabel Dataset for Disinformation Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mina Schütz",
      "Daniela Pisoiu",
      "Daria Liakhovets",
      "Alexander Schindler",
      "Melanie Siegel"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.680": {
    "title": "German Also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laura Mascarell",
      "Ribin Chalumattu",
      "Annette Rios"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.681": {
    "title": "German Parliamentary Corpus (GerParCor) Reloaded",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuseppe Abrami",
      "Mevlüt Bagci",
      "Alexander Mehler"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.682": {
    "title": "German SRL: Corpus Construction and Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxim Konca",
      "Andy Luecking",
      "Alexander Mehler"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.683": {
    "title": "GERMS-AT: A Sexism/Misogyny Dataset of Forum Comments from an Austrian Online Newspaper",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brigitte Krenn",
      "Johann Petrak",
      "Marina Kubina",
      "Christian Burger"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.684": {
    "title": "GIL-GALaD: Gender Inclusive Language - German Auto-Assembled Large Database",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna-Katharina Dick",
      "Matthias Drews",
      "Valentin Pickard",
      "Victoria Pierz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.685": {
    "title": "GLAMR: Augmenting AMR with GL-VerbNet Event Structure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingxuan Tu",
      "Timothy Obiso",
      "Bingyang Ye",
      "Kyeongmin Rim",
      "Keer Xu",
      "Liulu Yue",
      "Susan Windisch Brown",
      "Martha Palmer",
      "James Pustejovsky"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.686": {
    "title": "Global and Local Hierarchical Prompt Tuning Framework for Multi-level Implicit Discourse Relation Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Zeng",
      "Ruifang He",
      "Haowen Sun",
      "Jing Xu",
      "Chang Liu",
      "Bo Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.687": {
    "title": "GlotScript: A Resource and Tool for Low Resource Writing System Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Hossein Kargaran",
      "François Yvon",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.688": {
    "title": "GMEG-EXP: A Dataset of Human- and LLM-Generated Explanations of Grammatical and Fluency Edits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "S. Magalí López Cortez",
      "Mark Josef Norris",
      "Steve Duman"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.689": {
    "title": "GOLEM: GOld Standard for Learning and Evaluation of Motifs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "W. Victor Yarlott",
      "Anurag Acharya",
      "Diego Castro Estrada",
      "Diana Gomez",
      "Mark Finlayson"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.690": {
    "title": "Good or Bad News? Exploring GPT-4 for Sentiment Analysis for Faroese on a Public News Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iben Nyholm Debess",
      "Annika Simonsen",
      "Hafsteinn Einarsson"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.691": {
    "title": "Gos 2: A New Reference Corpus of Spoken Slovenian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Darinka Verdonik",
      "Kaja Dobrovoljc",
      "Tomaž Erjavec",
      "Nikola Ljubešić"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.692": {
    "title": "GPT-3.5 for Grammatical Error Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anisia Katinskaia",
      "Roman Yangarber"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.693": {
    "title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Mao",
      "Guanyi Chen",
      "Xulang Zhang",
      "Frank Guerin",
      "Erik Cambria"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.694": {
    "title": "GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiping Jin",
      "Leo Wanner",
      "Alexander Shvets"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.695": {
    "title": "GPT-SW3: An Autoregressive Language Model for the Scandinavian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ariel Ekgren",
      "Amaru Cuba Gyllensten",
      "Felix Stollenwerk",
      "Joey Öhman",
      "Tim Isbister",
      "Evangelia Gogoulou",
      "Fredrik Carlsson",
      "Judit Casademont",
      "Magnus Sahlgren"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.696": {
    "title": "Gradient Consistency-based Parameter Allocation for Multilingual Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenshuai Huo",
      "Xiaocheng Feng",
      "Yichong Huang",
      "Chengpeng Fu",
      "Hui Wang",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.697": {
    "title": "Gramble: A Tabular Programming Language for Collaborative Linguistic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Littell",
      "Darlene Stewart",
      "Fineen Davis",
      "Aidan Pine",
      "Roland Kuhn"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.698": {
    "title": "Grammatical Error Correction for Code-Switched Sentences by Learners of English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kelvin Wey Han Chan",
      "Christopher Bryant",
      "Li Nguyen",
      "Andrew Caines",
      "Zheng Yuan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.699": {
    "title": "Granular Change Accuracy: A More Accurate Performance Metric for Dialogue State Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taha Aksu",
      "Nancy Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.700": {
    "title": "GreekBART: The First Pretrained Greek Sequence-to-Sequence Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iakovos Evdaimon",
      "Hadi Abdine",
      "Christos Xypolopoulos",
      "Stamatis Outsios",
      "Michalis Vazirgiannis",
      "Giorgos Stamou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.701": {
    "title": "GRIT: A Dataset of Group Reference Recognition in Italian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergio E. Zanotto",
      "Qi Yu",
      "Miriam Butt",
      "Diego Frassinelli"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.702": {
    "title": "Grounded Multimodal Procedural Entity Recognition for Procedural Documents: A New Dataset and Baseline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haopeng Ren",
      "Yushi Zeng",
      "Yi Cai",
      "Zhenqi Ye",
      "Li Yuan",
      "Pinli Zhu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.703": {
    "title": "Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alistair Plum",
      "Tharindu Ranasinghe",
      "Christoph Purschke"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.704": {
    "title": "HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guijin Son",
      "Hanwool Lee",
      "Suwan Kim",
      "Huiseo Kim",
      "Jae cheol Lee",
      "Je Won Yeom",
      "Jihyu Jung",
      "Jung woo Kim",
      "Songseong Kim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.705": {
    "title": "Halwasa: Quantify and Analyze Hallucinations in Large Language Models: Arabic as a Case Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamdy Mubarak",
      "Hend Al-Khalifa",
      "Khaloud Suliman Alkhalefah"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.706": {
    "title": "HarmPot: An Annotation Framework for Evaluating Offline Harm Potential of Social Media Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ritesh Kumar",
      "Ojaswee Bhalla",
      "Madhu Vanthi",
      "Shehlat Maknoon Wani",
      "Siddharth Singh"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.707": {
    "title": "Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Qian",
      "Yiming Qian",
      "Yuting Song",
      "Fei Gao",
      "Hai Jin",
      "Chen Yu",
      "Xia Xie"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.708": {
    "title": "Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oana Ignat",
      "Zhijing Jin",
      "Artem Abzaliev",
      "Laura Biester",
      "Santiago Castro",
      "Naihao Deng",
      "Xinyi Gao",
      "Aylin Ece Gunal",
      "Jacky He",
      "Ashkan Kazemi",
      "Muhammad Khalifa",
      "Namho Koh",
      "Andrew Lee",
      "Siyang Liu",
      "Do June Min",
      "Shinka Mori",
      "Joan C. Nwatu",
      "Veronica Perez-Rosas",
      "Siqi Shen",
      "Zekun Wang",
      "Winston Wu",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.709": {
    "title": "HealthFC: Verifying Health Claims with Evidence-Based Medical Fact-Checking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juraj Vladika",
      "Phillip Schneider",
      "Florian Matthes"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.710": {
    "title": "Hierarchical Graph Convolutional Network Approach for Detecting Low-Quality Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeyoung Lee",
      "Joonwon Jang",
      "Misuk Kim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.711": {
    "title": "Hierarchical Selection of Important Context for Generative Event Causality Identification with Optimal Transports",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hieu Man",
      "Chien Van Nguyen",
      "Nghia Trung Ngo",
      "Linh Ngo",
      "Franck Dernoncourt",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.712": {
    "title": "Hierarchical Topic Modeling via Contrastive Learning and Hyperbolic Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhicheng Lin",
      "HeGang Chen",
      "Yuyin Lu",
      "Yanghui Rao",
      "Hao Xu",
      "Hanjiang Lai"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.713": {
    "title": "High-order Joint Constituency and Dependency Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanggan Gu",
      "Yang Hou",
      "Zhefeng Wang",
      "Xinyu Duan",
      "Zhenghua Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.714": {
    "title": "High-Order Semantic Alignment for Unsupervised Fine-Grained Image-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Gao",
      "Miaomiao Cheng",
      "Xu Han",
      "Wei Song"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.715": {
    "title": "HoLM: Analyzing the Linguistic Unexpectedness in Homeric Poetry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Pavlopoulos",
      "Ryan Sandell",
      "Maria Konstantinidou",
      "Chiara Bozzone"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.716": {
    "title": "How Diplomats Dispute: The UN Security Council Conflict Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karolina Zaczynska",
      "Peter Bourgonje",
      "Manfred Stede"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.717": {
    "title": "How Do Hyenas Deal with Human Speech? Speech Recognition and Translation with ConfHyena",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Gaido",
      "Sara Papi",
      "Matteo Negri",
      "Luisa Bentivogli"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.718": {
    "title": "How Far Is Too Far? Studying the Effects of Domain Discrepancy on Masked Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhradeep Kayal",
      "Alexander Rakhlin",
      "Ali Dashti",
      "Serguei Stepaniants"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.719": {
    "title": "How Gender Interacts with Political Values: A Case Study on Czech BERT Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adnan Al Ali",
      "Jindřich Libovický"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.720": {
    "title": "How Good Are LLMs at Out-of-Distribution Detection?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Liu",
      "Li-Ming Zhan",
      "Zexin Lu",
      "Yujie Feng",
      "Lei Xue",
      "Xiao-Ming Wu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.721": {
    "title": "How Important Is Tokenization in French Medical Masked Language Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanis Labrak",
      "Adrien Bazoge",
      "Béatrice Daille",
      "Mickael Rouvier",
      "Richard Dufour"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.722": {
    "title": "How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianjie Ju",
      "Weiwei Sun",
      "Wei Du",
      "Xinwei Yuan",
      "Zhaochun Ren",
      "Gongshen Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.723": {
    "title": "How Much Do Robots Understand Rudeness? Challenges in Human-Robot Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Andrew Orme",
      "Yanchao Yu",
      "Zhiyuan Tan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.724": {
    "title": "How Robust Are the QA Models for Hybrid Scientific Tabular Data? A Study Using Customized Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akash Ghosh",
      "Venkata Sahith Bathini",
      "Niloy Ganguly",
      "Pawan Goyal",
      "Mayank Singh"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.725": {
    "title": "How Speculative Can Speculative Decoding Be?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuorui Liu",
      "Chen Zhang",
      "Dawei Song"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.726": {
    "title": "How Susceptible Are LLMs to Logical Fallacies?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirreza Payandeh",
      "Dan Pluth",
      "Jordan Hosier",
      "Xuesu Xiao",
      "Vijay K. Gurbani"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.727": {
    "title": "How to Do Politics with Words: Investigating Speech Acts in Parliamentary Debates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ines Reinig",
      "Ines Rehbein",
      "Simone Paolo Ponzetto"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.728": {
    "title": "How to Encode Domain Information in Relation Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elisa Bassignana",
      "Viggo Unmack Gascou",
      "Frida Nøhr Laustsen",
      "Gustav Kristensen",
      "Marie Haahr Petersen",
      "Rob van der Goot",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.729": {
    "title": "How to Solve Few-Shot Abusive Content Detection Using the Data We Actually Have",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viktor Hangya",
      "Alexander Fraser"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.730": {
    "title": "How to Understand \"Support\"? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiamin Luo",
      "Jianing Zhao",
      "Jingjing Wang",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.731": {
    "title": "How Well Can BERT Learn the Grammar of an Agglutinative and Flexible-Order Language? The Case of Basque",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gorka Urbizu",
      "Muitze Zulaika",
      "Xabier Saralegi",
      "Ander Corral"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.732": {
    "title": "HS-GC: Holistic Semantic Embedding and Global Contrast for Effective Text Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Yang",
      "Bin Cao",
      "Jing Fan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.733": {
    "title": "HuLU: Hungarian Language Understanding Benchmark Kit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noémi Ligeti-Nagy",
      "Gergő Ferenczi",
      "Enikő Héja",
      "László János Laki",
      "Noémi Vadász",
      "Zijian Győző Yang",
      "Tamás Váradi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.734": {
    "title": "Human and System Perspectives on the Expression of Irony: An Analysis of Likelihood Labels and Rationales",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaron Maladry",
      "Alessandra Teresa Cignarella",
      "Els Lefever",
      "Cynthia van Hee",
      "Veronique Hoste"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.735": {
    "title": "HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiwei Peng",
      "Yekun Chai",
      "Xuhong Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.736": {
    "title": "Human in the Loop: How to Effectively Create Coherent Topics by Manually Labeling Only a Few Documents per Class",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anton F. Thielmann",
      "Christoph Weisser",
      "Benjamin Säfken"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.737": {
    "title": "Humanistic Buddhism Corpus: A Challenging Domain-Specific Dataset of English Translations for Classical and Modern Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youheng W. Wong",
      "Natalie Parde",
      "Erdem Koyuncu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.738": {
    "title": "Humanitarian Corpora for English, French and Spanish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Loryn Isaacs",
      "Santiago Chambó",
      "Pilar León-Araúz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.739": {
    "title": "Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Zhou",
      "Ben He",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.740": {
    "title": "Humans Need Context, What about Machines? Investigating Conversational Context in Abusive Language Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Bourgeade",
      "Zongmin Li",
      "Farah Benamara",
      "Véronique Moriceau",
      "Jian Su",
      "Aixin Sun"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.741": {
    "title": "Human vs. Machine Perceptions on Immigration Stereotypes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wolfgang S. Schmeisser-Nieto",
      "Pol Pastells",
      "Simona Frenda",
      "Mariona Taule"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.742": {
    "title": "Hybrid of Spans and Table-Filling for Aspect-Level Sentiment Triplet Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghua Nuo",
      "Chaofan Guo"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.743": {
    "title": "Hyperbolic Graph Neural Network for Temporal Knowledge Graph Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yancong Li",
      "Xiaoming Zhang",
      "Ying Cui",
      "Shuai Ma"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.744": {
    "title": "Hyperbolic Representations for Prompt Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Chen",
      "Xiangdong Su",
      "Feilong Bao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.745": {
    "title": "Hypergraph-Based Session Modeling: A Multi-Collaborative Self-Supervised Approach for Enhanced Recommender Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangping Zheng",
      "Bo Wu",
      "Alex X. Zhang",
      "Wei Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.746": {
    "title": "HyperMR: Hyperbolic Hypergraph Multi-hop Reasoning for Knowledge-based Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Wang",
      "Fuyong Xu",
      "Peiyu Liu",
      "Zhenfang Zhu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.747": {
    "title": "HYPERTTS: Parameter Efficient Adaptation in Text to Speech Using Hypernetworks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingting Li",
      "Rishabh Bhardwaj",
      "Ambuj Mehrish",
      "Bo Cheng",
      "Soujanya Poria"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.748": {
    "title": "HYRR: Hybrid Infused Reranking for Passage Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Lu",
      "Keith Hall",
      "Ji Ma",
      "Jianmo Ni"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.749": {
    "title": "IAD: In-Context Learning Ability Decoupler of Large Language Models in Meta-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Liu",
      "Xiuying Chen",
      "Gao Xing",
      "Ji Zhang",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.750": {
    "title": "IDC: Boost Text-to-image Retrieval via Indirect and Direct Connections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guowei Ge",
      "Kuangrong Hao",
      "Lingguang Hao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.751": {
    "title": "IDEATE: Detecting AI-Generated Text Using Internal and External Factual Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan Wang",
      "Licheng Zhang",
      "Zikang Guo",
      "Zhendong Mao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.752": {
    "title": "IDEM: The IDioms with EMotions Dataset for Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Prochnow",
      "Johannes E. Bendler",
      "Caroline Lange",
      "Foivos Ioannis Tzavellos",
      "Bas Marco Göritzer",
      "Marijn ten Thij",
      "Riza Batista-Navarro"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.753": {
    "title": "Identifying and Aligning Medical Claims Made on Social Media with Medical Evidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anthony James Hughes",
      "Xingyi Song"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.754": {
    "title": "Identifying Fine-grained Depression Signs in Social Media Posts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Augusto R. Mendes",
      "Helena Caseli"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.755": {
    "title": "Identifying Source Language Expressions for Pre-editing in Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Norizo Sakaguchi",
      "Yugo Murawaki",
      "Chenhui Chu",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.756": {
    "title": "Ideological Knowledge Representation: Framing Climate Change in EcoLexicon",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arianne Reimerink",
      "Melania Cabezas-García",
      "Pilar León-Araúz",
      "Pamela Faber"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.757": {
    "title": "ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayar Ghosh Roy",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.758": {
    "title": "ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paramita Mirza",
      "Viju Sudhi",
      "Soumya Ranjan Sahoo",
      "Sinchana Ramakanth Bhat"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.759": {
    "title": "Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huixuan Zhang",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.760": {
    "title": "Impact of Task Adapting on Transformer Models for Targeted Sentiment Analysis in Croatian Headlines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sofia Lee",
      "Jelke Bloem"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.761": {
    "title": "Impoverished Language Technology: The Lack of (Social) Class in NLP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amanda Cercas Curry",
      "Zeerak Talat",
      "Dirk Hovy"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.762": {
    "title": "Improved Neural Protoform Reconstruction via Reflex Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Lu",
      "Jingzhi Wang",
      "David R. Mortensen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.763": {
    "title": "Improved Out-of-Scope Intent Classification with Dual Encoding and Threshold-based Re-Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hossam Zawbaa",
      "Wael Rashwan",
      "Sourav Dutta",
      "Haytham Assem"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.764": {
    "title": "Improving Bengali and Hindi Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arif Shahriar",
      "Denilson Barbosa"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.765": {
    "title": "Improving Chinese Named Entity Recognition with Multi-grained Words and Part-of-Speech Tags via Joint Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhui Dou",
      "Chen Gong",
      "Zhenghua Li",
      "Zhefeng Wang",
      "Baoxing Huai",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.766": {
    "title": "Improving Content Recommendation: Knowledge Graph-Based Semantic Contrastive Learning for Diversity and Cold-Start Users",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yejin Kim",
      "Scott Rome",
      "Kevin Foley",
      "Mayur Nankani",
      "Rimon Melamed",
      "Javier Morales",
      "Abhay K. Yadav",
      "Maria Peifer",
      "Sardar Hamidian",
      "H. Howie Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.767": {
    "title": "Improving Continual Few-shot Relation Extraction through Relational Knowledge Distillation and Prototype Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiheng Zhang",
      "Daojian Zeng",
      "Xue Bai"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.768": {
    "title": "Improving Copy-oriented Text Generation via EDU Copy Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianxiang Wu",
      "Han Chen",
      "Luozheng Qin",
      "Ziqiang Cao",
      "Chunhui Ai"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.769": {
    "title": "Improving Cross-lingual Transfer with Contrastive Negative Learning and Self-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanlin Li",
      "Xuechen Zhao",
      "Amir Jafari",
      "Wenhao Shao",
      "Reza Farahbakhsh",
      "Noel Crespi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.770": {
    "title": "Improving Factual Consistency in Abstractive Summarization with Sentence Structure Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingxin Hu",
      "Xuanyu Zhang",
      "Xingyue Zhang",
      "Yiyang Li",
      "Dongsheng Chen",
      "Marina Litvak",
      "Natalia Vanetik",
      "Qing Yang",
      "Dongliang Xu",
      "Yanquan Zhou",
      "Lei Li",
      "Yuze Li",
      "Yingqi Zhu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.771": {
    "title": "Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taiji Li",
      "Zhi Li",
      "Yin Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.772": {
    "title": "Improving Grammatical Error Correction by Correction Acceptability Discrimination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Cao",
      "Kai Jiang",
      "Fayu Pan",
      "Chenlei Bao",
      "Jing Fan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.773": {
    "title": "Improving Implicit Discourse Relation Recognition with Semantics Confrontation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyang Cai",
      "Zhen Yang",
      "Ping Jian"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.774": {
    "title": "Improving Language Model Reasoning with Self-motivated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunlong Feng",
      "Yang Xu",
      "Libo Qin",
      "Yasheng Wang",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.775": {
    "title": "Improving Low-Resource Keyphrase Generation through Unsupervised Title Phrase Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byungha Kang",
      "Youhyun Shin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.776": {
    "title": "Improving Multi-view Document Clustering: Leveraging Multi-structure Processor and Hybrid Ensemble Clustering Module",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruina Bai",
      "Qi Bai"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.777": {
    "title": "Improving Personalized Sentiment Representation with Knowledge-enhanced and Parameter-efficient Layer Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "You Zhang",
      "Jin Wang",
      "Liang-Chih Yu",
      "Dan Xu",
      "Xuejie Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.778": {
    "title": "Improving Recall of Large Language Models: A Model Collaboration Approach for Relational Triple Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zepeng Ding",
      "Wenhao Huang",
      "Jiaqing Liang",
      "Yanghua Xiao",
      "Deqing Yang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.779": {
    "title": "Improving Robustness of GNN-based Anomaly Detection by Graph Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangping Zheng",
      "Bo Wu",
      "Alex X. Zhang",
      "Wei Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.780": {
    "title": "Improving Role-Oriented Dialogue Summarization with Interaction-Aware Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihong Guan",
      "Shi Feng",
      "Daling Wang",
      "Faliang Huang",
      "Yifei Zhang",
      "Yuan Cui"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.781": {
    "title": "Improving Text Readability through Segmentation into Rheses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Jamelot",
      "Solen Quiniou",
      "Sophie Hamon"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.782": {
    "title": "Improving the Robustness of Large Language Models via Consistency Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukun Zhao",
      "Lingyong Yan",
      "Weiwei Sun",
      "Guoliang Xing",
      "Shuaiqiang Wang",
      "Chong Meng",
      "Zhicong Cheng",
      "Zhaochun Ren",
      "Dawei Yin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.783": {
    "title": "Improving Unsupervised Neural Machine Translation via Training Data Self-Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinliang Lu",
      "Jiajun Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.784": {
    "title": "Improving Vietnamese-English Medical Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nhu Vo",
      "Dat Quoc Nguyen",
      "Dung D. Le",
      "Massimo Piccardi",
      "Wray Buntine"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.785": {
    "title": "InaGVAD : A Challenging French TV and Radio Corpus Annotated for Speech Activity Detection and Speaker Gender Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Doukhan",
      "Christine Maertens",
      "William Le Personnic",
      "Ludovic Speroni",
      "Reda Dehak"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.786": {
    "title": "In-Context Example Retrieval from Multi-Perspectives for Few-Shot Aspect-Based Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianlong Wang",
      "Hongling Xu",
      "Keyang Ding",
      "Bin Liang",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.787": {
    "title": "Incorporating Lexical and Syntactic Knowledge for Unsupervised Cross-Lingual Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyu Zheng",
      "Fengfei Fan",
      "Jianquan Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.788": {
    "title": "Incorporating Word-level Phonemic Decoding into Readability Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christine Pinney",
      "Casey Kennington",
      "Maria Soledad Pera",
      "Katherine Landau Wright",
      "Jerry Alan Fails"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.789": {
    "title": "IndicFinNLP: Financial Natural Language Processing for Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sohom Ghosh",
      "Arnab Maji",
      "Aswartha Narayana",
      "Sudip Kumar Naskar"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.790": {
    "title": "Indic-TEDST: Datasets and Baselines for Low-Resource Speech to Text Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nivedita Sethiya",
      "Saanvi Nair",
      "Chandresh Maurya"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.791": {
    "title": "IndirectQA: Understanding Indirect Answers to Implicit Polar Questions in French and Spanish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christin Müller",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.792": {
    "title": "Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akash Anil",
      "Victor Gutierrez-Basulto",
      "Yazmin Ibanez-Garcia",
      "Steven Schockaert"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.793": {
    "title": "InferBR: A Natural Language Inference Dataset in Portuguese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luciana Bencke",
      "Francielle Vasconcellos Pereira",
      "Moniele Kunrath Santos",
      "Viviane Moreira"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.794": {
    "title": "InfFeed: Influence Functions as a Feedback to Improve the Performance of Subjective Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Somnath Banerjee",
      "Maulindu Sarkar",
      "Punyajoy Saha",
      "Binny Mathew",
      "Animesh Mukherjee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.795": {
    "title": "InfoEnh: Towards Multimodal Sentiment Analysis via Information Bottleneck Filter and Optimal Transport Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifeng Xie",
      "Zhihong Zhu",
      "Xuan Lu",
      "Zhiqi Huang",
      "Haoran Xiong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.796": {
    "title": "Information Extraction with Differentiable Beam Search on Graph RNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niama El Khbir",
      "Nadi Tomeh",
      "Thierry Charnois"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.797": {
    "title": "INMT-Lite: Accelerating Low-Resource Language Data Collection via Offline Interactive Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harshita Diddee",
      "Anurag Shukla",
      "Tanuja Ganu",
      "Vivek Seshadri",
      "Sandipan Dandapat",
      "Monojit Choudhury",
      "Kalika Bali"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.798": {
    "title": "Integrating Headedness Information into an Auto-generated Multilingual CCGbank for Improved Semantic Interpretation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tu-Anh Tran",
      "Yusuke Miyao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.799": {
    "title": "Integrating Representation Subspace Mapping with Unimodal Auxiliary Loss for Attention-based Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xulong Du",
      "Xingnan Zhang",
      "Dandan Wang",
      "Yingying Xu",
      "Zhiyuan Wu",
      "Shiqing Zhang",
      "Xiaoming Zhao",
      "Jun Yu",
      "Liangliang Lou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.800": {
    "title": "Intent-Aware and Hate-Mitigating Counterspeech Generation via Dual-Discriminator Guided LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Wang",
      "Zhiliang Tian",
      "Xin Song",
      "Yue Zhang",
      "Yuchen Pan",
      "Hongkui Tu",
      "Minlie Huang",
      "Bin Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.801": {
    "title": "Intention and Face in Dialog",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adil Soubki",
      "Owen Rambow"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.802": {
    "title": "InteRead: An Eye Tracking Dataset of Interrupted Reading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesca Zermiani",
      "Prajit Dhar",
      "Ekta Sood",
      "Fabian Kögel",
      "Andreas Bulling",
      "Maria Wirzberger"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.803": {
    "title": "Interpretable Assessment of Speech Intelligibility Using Deep Learning: A Case Study on Speech Disorders Due to Head and Neck Cancers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sondes Abderrazek",
      "Corinne Fredouille",
      "Alain Ghio",
      "Muriel Lalain",
      "Christine Meunier",
      "Mathieu Balaguer",
      "Virginie Woisard"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.804": {
    "title": "Interpretable Short Video Rumor Detection Based on Modality Tampering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaixuan Wu",
      "Yanghao Lin",
      "Donglin Cao",
      "Dazhen Lin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.805": {
    "title": "Interpreting Themes from Educational Stories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yigeng Zhang",
      "Fabio Gonzalez",
      "Thamar Solorio"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.806": {
    "title": "Intrinsic Subgraph Generation for Interpretable Graph Based Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pascal Tilli",
      "Ngoc Thang Vu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.807": {
    "title": "Introducing a Parsed Corpus of Historical High German",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher D. Sapp",
      "Elliott Evans",
      "Rex Sprouse",
      "Daniel Dakota"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.808": {
    "title": "Introducing CQuAE : A New French Contextualised Question-Answering Corpus for the Education Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Gerald",
      "Anne Vilnat",
      "Sofiane Ettayeb",
      "Louis Tamames",
      "Patrick Paroubek"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.809": {
    "title": "Investigating the Robustness of Modelling Decisions for Few-Shot Cross-Topic Stance Detection: A Preregistered Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myrthe Reuver",
      "Suzan Verberne",
      "Antske Fokkens"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.810": {
    "title": "IR2: Information Regularization for Information Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyou Wang",
      "Kaicheng Wang",
      "Xiaoyue Wang",
      "Weili Cao",
      "Ramamohan Paturi",
      "Leon Bergen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.811": {
    "title": "I Remember You!: SUI Corpus for Remembering and Utilizing Users' Information in Chat-oriented Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuiko Tsunomori",
      "Ryuichiro Higashinaka"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.812": {
    "title": "ÌròyìnSpeech: A Multi-purpose Yorùbá Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tolulope Ogunremi",
      "Kola Tubosun",
      "Anuoluwapo Aremu",
      "Iroro Orife",
      "David Ifeoluwa Adelani"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.813": {
    "title": "Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Yang",
      "Gjergji Kasneci"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.814": {
    "title": "Is Gender Reference Gender-specific? Studies in a Polar Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manfred Klenner",
      "Dylan Massey"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.815": {
    "title": "Is It Possible to Modify Text to a Target Readability Level? An Initial Investigation Using Zero-Shot Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asma Farajidizaji",
      "Vatsal Raina",
      "Mark Gales"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.816": {
    "title": "Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyang Zhou",
      "Lu Chen",
      "Kai Yu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.817": {
    "title": "Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mateusz Klimaszewski",
      "Piotr Andruszkiewicz",
      "Alexandra Birch"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.818": {
    "title": "ISO 24617-12: A New Standard for Semantic Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harry Bunt"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.819": {
    "title": "IsraParlTweet: The Israeli Parliamentary and Twitter Resource",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guy Mor-Lan",
      "Effi Levi",
      "Tamir Sheafer",
      "Shaul R. Shenhav"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.820": {
    "title": "Is Spoken Hungarian Low-resource?: A Quantitative Survey of Hungarian Speech Data Sets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Mihajlik",
      "Katalin Mády",
      "Anna Kohári",
      "Fruzsina Sára Fruzsina",
      "Gábor Kiss",
      "Tekla Etelka Gráczi",
      "A. Seza Doğruöz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.821": {
    "title": "Is Summary Useful or Not? An Extrinsic Human Evaluation of Text Summaries on Downstream Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Pu",
      "Mingqi Gao",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.822": {
    "title": "IT2ACL Learning Easy-to-Hard Instructions via 2-Phase Automated Curriculum Learning for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Huang",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.823": {
    "title": "IT5: Text-to-text Pretraining for Italian Language Understanding and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriele Sarti",
      "Malvina Nissim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.824": {
    "title": "Italian Word Embeddings for the Medical Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Franco Alberto Cardillo",
      "Franca Debole"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.825": {
    "title": "It's Not under the Lamppost: Expanding the Reach of Conversational AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christy Doran",
      "Deborah A. Dahl"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.826": {
    "title": "JaParaPat: A Large-Scale Japanese-English Parallel Patent Application Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masaaki Nagata",
      "Makoto Morishita",
      "Katsuki Chousa",
      "Norihito Yasuda"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.827": {
    "title": "Jargon: A Suite of Language Models and Evaluation Tasks for French Specialized Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Segonne",
      "Aidan Mannion",
      "Laura Cristina Alonzo Canul",
      "Alexandre Daniel Audibert",
      "Xingyu Liu",
      "Cécile Macaire",
      "Adrien Pupier",
      "Yongxin Zhou",
      "Mathilde Aguiar",
      "Felix E. Herron",
      "Magali Norré",
      "Massih R Amini",
      "Pierrette Bouillon",
      "Iris Eshkol-Taravella",
      "Emmanuelle Esperança-Rodier",
      "Thomas François",
      "Lorraine Goeuriot",
      "Jérôme Goulian",
      "Mathieu Lafourcade",
      "Benjamin Lecouteux",
      "François Portet",
      "Fabien Ringeval",
      "Vincent Vandeghinste",
      "Maximin Coavoux",
      "Marco Dinarelli",
      "Didier Schwab"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.828": {
    "title": "JCoLA: Japanese Corpus of Linguistic Acceptability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taiga Someya",
      "Yushi Sugimoto",
      "Yohei Oseki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.829": {
    "title": "J-CRe3: A Japanese Conversation Dataset for Real-world Reference Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nobuhiro Ueda",
      "Hideko Habe",
      "Akishige Yuguchi",
      "Seiya Kawano",
      "Yasutomo Kawanishi",
      "Sadao Kurohashi",
      "Koichiro Yoshino"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.830": {
    "title": "JDocQA: Japanese Document Question Answering Dataset for Generative Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eri Onami",
      "Shuhei Kurita",
      "Taiki Miyanishi",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.831": {
    "title": "JEMHopQA: Dataset for Japanese Explainable Multi-Hop Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ai Ishii",
      "Naoya Inoue",
      "Hisami Suzuki",
      "Satoshi Sekine"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.832": {
    "title": "JFLD: A Japanese Benchmark for Deductive Reasoning Based on Formal Logic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Terufumi Morishita",
      "Atsuki Yamaguchi",
      "Gaku Morio",
      "Hikaru Tomonari",
      "Osamu Imaichi",
      "Yasuhiro Sogawa"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.833": {
    "title": "JLBert: Japanese Light BERT for Cross-Domain Short Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chandrai Kayal",
      "Sayantan Chattopadhyay",
      "Aryan Gupta",
      "Satyen Abrol",
      "Archie Gugol"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.834": {
    "title": "JL-Hate: An Annotated Dataset for Joint Learning of Hate Speech and Target Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaan Büyükdemirci",
      "Izzet Emre Kucukkaya",
      "Eren Ölmez",
      "Cagri Toraman"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.835": {
    "title": "JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atsumoto Ohashi",
      "Ryu Hirai",
      "Shinya Iizuka",
      "Ryuichiro Higashinaka"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.836": {
    "title": "Joint Annotation of Morphology and Syntax in Dependency Treebanks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bruno Guillaume",
      "Kim Gerdes",
      "Kirian Guiller",
      "Sylvain Kahane",
      "Yixuan Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.837": {
    "title": "JoTR: A Joint Transformer and Reinforcement Learning Framework for Dialogue Policy Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wai-Chung Kwan",
      "Huimin Wang",
      "Hongru Wang",
      "Zezhong Wang",
      "Bin Liang",
      "Xian Wu",
      "Yefeng Zheng",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.838": {
    "title": "JRC-Names-Retrieval: A Standardized Benchmark for Name Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philip Blair",
      "Kfir Bar"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.839": {
    "title": "J-SNACS: Adposition and Case Supersenses for Japanese Joshi",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatsuya Aoyama",
      "Chihiro Taguchi",
      "Nathan Schneider"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.840": {
    "title": "Jump to Conclusions: Short-Cutting Transformers with Linear Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Yom Din",
      "Taelin Karidi",
      "Leshem Choshen",
      "Mor Geva"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.841": {
    "title": "KazEmoTTS: A Dataset for Kazakh Emotional Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adal Abilbekov",
      "Saida Mussakhojayeva",
      "Rustem Yeshpanov",
      "Huseyin Atakan Varol"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.842": {
    "title": "KazParC: Kazakh Parallel Corpus for Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rustem Yeshpanov",
      "Alina Polonskaya",
      "Huseyin Atakan Varol"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.843": {
    "title": "KazQAD: Kazakh Open-Domain Question Answering Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rustem Yeshpanov",
      "Pavel Efimov",
      "Leonid Boytsov",
      "Ardak Shalkarbayuli",
      "Pavel Braslavski"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.844": {
    "title": "KazSAnDRA: Kazakh Sentiment Analysis Dataset of Reviews and Attitudes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rustem Yeshpanov",
      "Huseyin Atakan Varol"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.845": {
    "title": "KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilin Wang",
      "Minghao Hu",
      "Zhen Huang",
      "Dongsheng Li",
      "Dong Yang",
      "Xicheng Lu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.846": {
    "title": "KCL: Few-shot Named Entity Recognition with Knowledge Graph and Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shan Zhang",
      "Bin Cao",
      "Jing Fan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.847": {
    "title": "KEHRL: Learning Knowledge-Enhanced Language Representations with Hierarchical Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyang Li",
      "Taolin Zhang",
      "Longtao Huang",
      "Chengyu Wang",
      "Xiaofeng He",
      "Hui Xue"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.848": {
    "title": "KET-QA: A Dataset for Knowledge Enhanced Table Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengkang Hu",
      "Haoyu Dong",
      "Ping Luo",
      "Shi Han",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.849": {
    "title": "Keyphrase Generation: Lessons from a Reproducibility Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edwin Thomas",
      "Sowmya Vajjala"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.850": {
    "title": "KGConv, a Conversational Corpus Grounded in Wikidata",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quentin Brabant",
      "Lina M. Rojas Barahona",
      "Gwénolé Lecorvé",
      "Claire Gardent"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.851": {
    "title": "Khan Academy Corpus: A Multilingual Corpus of Khan Academy Lectures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominika Ďurišková",
      "Daniela Jurášová",
      "Matúš Žilinec",
      "Eduard Šubert",
      "Ondřej Bojar"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.852": {
    "title": "Killkan: The Automatic Speech Recognition Dataset for Kichwa with Morphosyntactic Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chihiro Taguchi",
      "Jefferson Saransig",
      "Dayana Velásquez",
      "David Chiang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.853": {
    "title": "KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for Fine-Tuning Korean Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongjun Jang",
      "Sungjoo Byun",
      "Hyemi Jo",
      "Hyopil Shin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.854": {
    "title": "Know-Adapter: Towards Knowledge-Aware Parameter-Efficient Transfer Learning for Few-shot Named Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binling Nie",
      "Yiming Shao",
      "Yigang Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.855": {
    "title": "Knowledge-augmented Graph Neural Networks with Concept-aware Attention for Adverse Drug Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ya Gao",
      "Shaoxiong Ji",
      "Pekka Marttinen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.856": {
    "title": "Knowledge-aware Attention Network for Medication Effectiveness Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingying Zhang",
      "Xian Wu",
      "Yu Zhang",
      "Yefeng Zheng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.857": {
    "title": "Knowledge Enhanced Pre-training for Cross-lingual Dense Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Zhang",
      "Yeyun Gong",
      "Dayiheng Liu",
      "Shunyu Zhang",
      "Xingwei He",
      "Jiancheng Lv",
      "Jian Guo"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.858": {
    "title": "Knowledge-enhanced Prompt Tuning for Dialogue-based Relation Extraction with Trigger and Label Semantic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao An",
      "Zhihong Zhu",
      "Xuxin Cheng",
      "Zhiqi Huang",
      "Yuexian Zou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.859": {
    "title": "Knowledge GeoGebra: Leveraging Geometry of Relation Embeddings in Knowledge Graph Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kossi Amouzouvi",
      "Bowen Song",
      "Sahar Vahdati",
      "Jens Lehmann"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.860": {
    "title": "Knowledge Graphs for Real-World Rumour Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Dougrez-Lewis",
      "Elena Kochkina",
      "Maria Liakata",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.861": {
    "title": "Knowledge-Guided Cross-Topic Visual Question Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongfei Liu",
      "Guohua Wang",
      "Jiayuan Xie",
      "Jiali Chen",
      "Wenhao Fang",
      "Yi Cai"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.862": {
    "title": "Knowledge Triplets Derivation from Scientific Publications via Dual-Graph Resonance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Zhang",
      "Pengcheng Li",
      "Kaisong Song",
      "Xurui Li",
      "Yangyang Kang",
      "Xuhong Zhang",
      "Xiaozhong Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.863": {
    "title": "KnowVrDU: A Unified Knowledge-aware Prompt-Tuning Framework for Visually-rich Document Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunqi Zhang",
      "Yubo Chen",
      "Jingzhe Zhu",
      "Jinyu Xu",
      "Shuai Yang",
      "Zhaoliang Wu",
      "Liang Huang",
      "Yongfeng Huang",
      "Shuai Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.864": {
    "title": "KoCoSa: Korean Context-aware Sarcasm Detection Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yumin Kim",
      "Heejae Suh",
      "Mingi Kim",
      "Dongyeon Won",
      "Hwanhee Lee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.865": {
    "title": "KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongbo Jang",
      "Seonghyeon Lee",
      "Hwanjo Yu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.866": {
    "title": "KoFREN: Comprehensive Korean Word Frequency Norms Derived from Large Scale Free Speech Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin-seo Kim",
      "Anna Seo Gyeong Choi",
      "Sunghye Cho"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.867": {
    "title": "Konidioms Corpus: A Dataset of Idioms in Konkani Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naziya Mahamdul Shaikh",
      "Jyoti D. Pawar",
      "Mubarak Banu Sayed"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.868": {
    "title": "Korean Bio-Medical Corpus (KBMC) for Medical Named Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungjoo Byun",
      "Jiseung Hong",
      "Sumin Park",
      "Dongjun Jang",
      "Jean Seo",
      "Minseok Kim",
      "Chaeyoung Oh",
      "Hyopil Shin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.869": {
    "title": "Korean Disaster Safety Information Sign Language Translation Benchmark Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wooyoung Kim",
      "TaeYong Kim",
      "Byeongjin Kim",
      "Myeong Jin MJ Lee",
      "Gitaek Lee",
      "Kirok Kim",
      "Jisoo Cha",
      "Wooju Kim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.870": {
    "title": "Kosmic: Korean Text Similarity Metric Reflecting Honorific Distinctions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yerin Hwang",
      "Yongil Kim",
      "Hyunkyung Bae",
      "Jeesoo Bang",
      "Hwanhee Lee",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.871": {
    "title": "KPatch: Knowledge Patch to Pre-trained Language Model for Zero-Shot Stance Detection on Social Media",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuohao Lin",
      "Wei Chen",
      "Yunpeng Gao",
      "Zhishu Jiang",
      "Mengqi Liao",
      "Zhiyu Zhang",
      "Shuyuan Zhao",
      "Huaiyu Wan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.872": {
    "title": "K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haven Kim",
      "Jongmin Jung",
      "Dasaem Jeong",
      "Juhan Nam"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.873": {
    "title": "Lˆ2GC:Lorentzian Linear Graph Convolutional Networks for Node Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiuyu Liang",
      "Weihua Wang",
      "Feilong Bao",
      "Guanglai Gao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.874": {
    "title": "Labeling Comic Mischief Content in Online Videos with a Multimodal Hierarchical-Cross-Attention Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elaheh Baharlouei",
      "Mahsa Shafaei",
      "Yigeng Zhang",
      "Hugo Jair Escalante",
      "Thamar Solorio"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.875": {
    "title": "Labeling Results of Topic Models: Word Sense Disambiguation as Key Method for Automatic Topic Labeling with GermaNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jennifer Ecker"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.876": {
    "title": "Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linzhi Wu",
      "Xingyu Zhang",
      "Yakun Zhang",
      "Changyan Zheng",
      "Tiejun Liu",
      "Liang Xie",
      "Ye Yan",
      "Erwei Yin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.877": {
    "title": "Language and Speech Technology for Central Kurdish Varieties",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sina Ahmadi",
      "Daban Jaff",
      "Md Mahfuz Ibn Alam",
      "Antonios Anastasopoulos"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.878": {
    "title": "Language Models and Semantic Relations: A Dual Relationship",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olivier Ferret"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.879": {
    "title": "Language Models for Text Classification: Is In-Context Learning Enough?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandra Edwards",
      "Jose Camacho-Collados"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.880": {
    "title": "Language Pivoting from Parallel Corpora for Word Sense Disambiguation of Historical Languages: A Case Study on Latin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iacopo Ghinassi",
      "Simone Tedeschi",
      "Paola Marongiu",
      "Roberto Navigli",
      "Barbara McGillivray"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.881": {
    "title": "Language Technologies as If People Mattered: Centering Communities in Language Technology Development",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nina Markl",
      "Lauren Hall-Lew",
      "Catherine Lai"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.882": {
    "title": "Language Variety Identification with True Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcos Zampieri",
      "Kai North",
      "Tommi Jauhiainen",
      "Mariano Felice",
      "Neha Kumari",
      "Nishant Nair",
      "Yash Mahesh Bangera"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.883": {
    "title": "LANID: LLM-assisted New Intent Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Fan",
      "Jiashu Pu",
      "Rongsheng Zhang",
      "Xiao-Ming Wu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.884": {
    "title": "Large Language Models Are Echo Chambers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Nehring",
      "Aleksandra Gabryszak",
      "Pascal Jürgens",
      "Aljoscha Burchardt",
      "Stefan Schaffer",
      "Matthias Spielkamp",
      "Birgit Stark"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.885": {
    "title": "Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Toyin D. Aguda",
      "Suchetha Siddagangappa",
      "Elena Kochkina",
      "Simerjot Kaur",
      "Dongsheng Wang",
      "Charese Smiley"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.886": {
    "title": "Large Language Models for Generative Recommendation: A Survey and Visionary Discussions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Li",
      "Yongfeng Zhang",
      "Dugang Liu",
      "Li Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.887": {
    "title": "Large Language Models Offer an Alternative to the Traditional Approach of Topic Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yida Mu",
      "Chun Dong",
      "Kalina Bontcheva",
      "Xingyi Song"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.888": {
    "title": "Latent vs Explicit Knowledge Representation: How ChatGPT Answers Questions about Low-Frequency Entities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arianna Graciotti",
      "Valentina Presutti",
      "Rocco Tripodi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.889": {
    "title": "LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shulin Huang",
      "Shirong Ma",
      "Yinghui Li",
      "Mengzuo Huang",
      "Wuhe Zou",
      "Weidong Zhang",
      "Haitao Zheng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.890": {
    "title": "LA-UCL: LLM-Augmented Unsupervised Contrastive Learning Framework for Few-Shot Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Zhang",
      "Hui Gao",
      "Peng Zhang",
      "Boda Feng",
      "Wenmin Deng",
      "Yuexian Hou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.891": {
    "title": "Layer-wise Regularized Dropout for Neural Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiwen Ni",
      "Min Yang",
      "Ruifeng Xu",
      "Chengming Li",
      "Xiping Xiping Hu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.892": {
    "title": "LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masato Fujitake"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.893": {
    "title": "LCGbank: A Corpus of Syntactic Analyses Based on Proof Nets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Bhargava",
      "Timothy A. D. Fowler",
      "Gerald Penn"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.894": {
    "title": "LeadEmpathy: An Expert Annotated German Dataset of Empathy in Written Leadership Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Didem Sedefoglu",
      "Allison Claire Lahnala",
      "Jasmin Wagner",
      "Lucie Flek",
      "Sandra Ohly"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.895": {
    "title": "Learning Bidirectional Morphological Inflection like Humans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akiyo Fukatsu",
      "Yuto Harada",
      "Yohei Oseki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.896": {
    "title": "Learning from Wrong Predictions in Low-Resource Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia Cheng Hu",
      "Roberto Cavicchioli",
      "Giulia Berardinelli",
      "Alessandro Capotondi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.897": {
    "title": "Learning Intrinsic Dimension via Information Bottleneck for Explainable Aspect-based Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxiao Cheng",
      "Jie Zhou",
      "Wen Wu",
      "Qin Chen",
      "Liang He"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.898": {
    "title": "Learning Strategies for Robust Argument Mining: An Analysis of Variations in Language and Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramon Ruiz-Dolz",
      "Chr-Jr Chiu",
      "Chung-Chi Chen",
      "Noriko Kando",
      "Hsin-Hsi Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.899": {
    "title": "Lemmatisation of Medieval Greek: Against the Limits of Transformer's Capabilities?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Colin Swaelens",
      "Pranaydeep Singh",
      "Ilse de Vos",
      "Els Lefever"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.900": {
    "title": "Leros: Learning Explicit Reasoning on Synthesized Data for Commonsense Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhao Wang",
      "Pengfei Cao",
      "Jiachun Li",
      "Yubo Chen",
      "Kang Liu",
      "Xiaojian Jiang",
      "Jiexin Xu",
      "Li Qiuxia",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.901": {
    "title": "Lessons from Deploying the First Bilingual Peruvian Sign Language - Spanish Online Dictionary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joe Huamani-Malca",
      "Miguel Rodriguez Mondoñedo",
      "Francisco Cerna-Herrera",
      "Gissella Bejarano",
      "Carlos Vásquez Roque",
      "Cesar Augusto Ramos Cantu",
      "Sabina Oporto Pérez"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.902": {
    "title": "Let's Rectify Step by Step: Improving Aspect-based Sentiment Analysis with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunyu Liu",
      "Jie Zhou",
      "Qunxi Zhu",
      "Qin Chen",
      "Qingchun Bai",
      "Jun Xiao",
      "Liang He"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.903": {
    "title": "Leveraging AMR Graph Structure for Better Sequence-to-Sequence AMR Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linyu Fan",
      "Wu Wu Yiheng",
      "Jun Xie",
      "Junhui Li",
      "Fang Kong",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.904": {
    "title": "Leveraging Domain Corpora for Enhanced Terminology: The Case of Estonian-English Remote Sensing Termbase",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liisi Jakobson",
      "Jelena Kallas",
      "Erko Jakobson"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.905": {
    "title": "Leveraging Information Redundancy of Real-World Data through Distant Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ariel Cohen",
      "Alexandrine Lanson",
      "Emmanuelle Kempf",
      "Xavier Tannier"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.906": {
    "title": "Leveraging Linguistically Enhanced Embeddings for Open Information Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fauzan Nayeem Farooqui",
      "Thanmay Jayakumar",
      "Pulkit Mathur",
      "Mansi A. Radke"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.907": {
    "title": "Leveraging Pre-existing Resources for Data-Efficient Counter-Narrative Generation in Korean",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungyoon Lee",
      "Chanjun Park",
      "DaHyun Jung",
      "Hyeonseok Moon",
      "Jaehyung Seo",
      "Sugyeong Eo",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.908": {
    "title": "Leveraging Social Context for Humor Recognition and Sense of Humor Evaluation in Social Media with a New Chinese Humor Corpus - HumorWB",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyuan Zeng",
      "Zefeng Li",
      "Liang Yang",
      "Hongfei Lin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.909": {
    "title": "Leveraging Syntactic Dependencies in Disambiguation: The Case of African American English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wilermine Previlon",
      "Alice Rozet",
      "Jotsna Gowda",
      "Bill Dyer",
      "Kevin Tang",
      "Sarah Moeller"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.910": {
    "title": "Leveraging the Interplay between Syntactic and Acoustic Cues for Optimizing Korean TTS Pause Formation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yejin Jeon",
      "Yunsu Kim",
      "Gary Geunbae Lee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.911": {
    "title": "LexAbSumm: Aspect-based Summarization of Legal Decisions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Santosh T.y.s.s.",
      "Mahmoud Aly",
      "Matthias Grabmair"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.912": {
    "title": "LexComSpaL2: A Lexical Complexity Corpus for Spanish as a Foreign Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jasper Degraeuwe",
      "Patrick Goethals"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.913": {
    "title": "LexDrafter: Terminology Drafting for Legislative Documents Using Retrieval Augmented Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashish Chouhan",
      "Michael Gertz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.914": {
    "title": "LexiVault: A Repository for Psycholinguistic Lexicons of Lesser-studied Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hind Saddiki",
      "Samantha Wray",
      "Daisy Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.915": {
    "title": "LFED: A Literary Fiction Evaluation Dataset for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linhao Yu",
      "Qun Liu",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.916": {
    "title": "LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuang Liu",
      "Renren Jin",
      "Yuqi Ren",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.917": {
    "title": "LI4: Label-Infused Iterative Information Interacting Based Fact Verification in Question-answering Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaocheng Zhang",
      "Chang Wang",
      "Guoping Zhao",
      "Xiaohong Su"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.918": {
    "title": "LightVLP: A Lightweight Vision-Language Pre-training via Gated Interactive Masked AutoEncoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingwu Sun",
      "Zhen Yang",
      "Ruobing Xie",
      "Fengzong Lian",
      "Zhanhui Kang",
      "Chengzhong Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.919": {
    "title": "Limitations of Human Identification of Automatically Generated Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nadège Alavoine",
      "Maximin Coavoux",
      "Emmanuelle Esperança-Rodier",
      "Romane Gallienne",
      "Carlos Gonzalez Gallardo",
      "Jérôme Goulian",
      "Jose G. Moreno",
      "Aurélie Névéol",
      "Didier Schwab",
      "Vincent Segonne",
      "Johanna Simoens"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.920": {
    "title": "Linear Cross-document Event Coreference Resolution with X-AMR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shafiuddin Rehan Ahmed",
      "George Arthur Baker",
      "Evi Judge",
      "Michael Reagan",
      "Kristin Wright-Bettner",
      "Martha Palmer",
      "James H. Martin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.921": {
    "title": "LinguaMeta: Unified Metadata for Thousands of Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sandy Ritchie",
      "Daan van Esch",
      "Uche Okonkwo",
      "Shikhar Vashishth",
      "Emily Drummond"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.922": {
    "title": "Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessio Miaschi",
      "Felice Dell’Orletta",
      "Giulia Venturi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.923": {
    "title": "Linguistic Nudges and Verbal Interaction with Robots, Smart-Speakers, and Humans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Natalia Kalashnikova",
      "Ioana Vasilescu",
      "Laurence Devillers"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.924": {
    "title": "Linguistic Rule Induction Improves Adversarial and OOD Robustness in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuoran Jiang",
      "Qingcai Chen",
      "Yang Xiang",
      "Youcheng Pan",
      "Yukang Lin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.925": {
    "title": "Linguistic Survey of India and Polyglotta Africana: Two Retrostandardized Digital Editions of Large Historical Collections of Multilingual Wordlists",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert Forkel",
      "Johann-Mattis List",
      "Christoph Rzymski",
      "Guillaume Segerer"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.926": {
    "title": "Linking Adaptive Structure Induction and Neuron Filtering: A Spectral Perspective for Aspect-based Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Niu",
      "Maoyi Wang",
      "Yun Xiong",
      "Biao Yang",
      "Xing Jia",
      "Zhonglei Guo"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.927": {
    "title": "Linking Judgement Text to Court Hearing Videos: UK Supreme Court as a Case Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadeel Saadany",
      "Constantin Orasan",
      "Sophie Walker",
      "Catherine Breslin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.928": {
    "title": "Linking Named Entities in Diderot's Encyclopédie to Wikidata",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Nugues"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.929": {
    "title": "Little Red Riding Hood Goes around the Globe: Crosslingual Story Planning and Generation with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evgeniia Razumovskaia",
      "Joshua Maynez",
      "Annie Louis",
      "Mirella Lapata",
      "Shashi Narayan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.930": {
    "title": "LlamaCare: An Instruction Fine-Tuned Large Language Model for Clinical NLP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rumeng Li",
      "Xun Wang",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.931": {
    "title": "Llama-VITS: Enhancing TTS Synthesis with Semantic Awareness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xincan Feng",
      "Akifumi Yoshimoto"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.932": {
    "title": "LLMR: Knowledge Distillation with a Large Language Model-Induced Reward",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongheng Li",
      "Yongchang Hao",
      "Lili Mou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.933": {
    "title": "LLMSegm: Surface-level Morphological Segmentation Using Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marko Pranjić",
      "Marko Robnik-Šikonja",
      "Senja Pollak"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.934": {
    "title": "LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Wang",
      "Baoxin Wang",
      "Yijun Liu",
      "Dayong Wu",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.935": {
    "title": "Locally Differentially Private In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunyan Zheng",
      "Keke Sun",
      "Wenhao Zhao",
      "Haibo Zhou",
      "Lixing Jiang",
      "Shaoyang Song",
      "Chunlai Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.936": {
    "title": "LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vijeta Deshpande",
      "Minhwa Lee",
      "Zonghai Yao",
      "Zihao Zhang",
      "Jason Brian Gibbons",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.937": {
    "title": "Loflòc: A Morphological Lexicon for Occitan using Universal Dependencies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marianne Vergez-Couret",
      "Myriam Bras",
      "Aleksandra Miletić",
      "Clamença Poujade"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.938": {
    "title": "Logging Keystrokes in Writing by English Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Velentzas",
      "Andrew Caines",
      "Rita Borgo",
      "Erin Pacquetet",
      "Clive Hamilton",
      "Taylor Arnold",
      "Diane Nicholls",
      "Paula Buttery",
      "Thomas Gaillat",
      "Nicolas Ballier",
      "Helen Yannakoudakis"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.939": {
    "title": "Logic Rules as Explanations for Legal Case Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZhongXiang Sun",
      "Kepu Zhang",
      "Weijie Yu",
      "Haoyu Wang",
      "Jun Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.940": {
    "title": "LoNAS: Elastic Low-Rank Adapters for Efficient Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan Pablo Munoz",
      "Jinjie Yuan",
      "Yi Zheng",
      "Nilesh Jain"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.941": {
    "title": "LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jennifer A. Bishop",
      "Sophia Ananiadou",
      "Qianqian Xie"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.942": {
    "title": "Longform Multimodal Lay Summarization of Scientific Papers: Towards Automatically Generating Science Blogs from Research Articles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sandeep Kumar",
      "Guneet Singh Kohli",
      "Tirthankar Ghosal",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.943": {
    "title": "Look before You Leap: Dual Logical Verification for Knowledge-based Visual Question Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xumeng Liu",
      "Wenya Guo",
      "Ying Zhang",
      "Xubo Liu",
      "Yu Zhao",
      "Shenglong Yu",
      "Xiaojie Yuan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.944": {
    "title": "LoSST-AD: A Longitudinal Corpus for Tracking Alzheimer's Disease Related Changes in Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ulla Petti",
      "Anna Korhonen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.945": {
    "title": "Low-Rank Prune-And-Factorize for Language Model Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyu Ren",
      "Kenny Q. Zhu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.946": {
    "title": "M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurish Thakkar",
      "Sherzod Hakimov",
      "Marko Tadić"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.947": {
    "title": "M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Bai",
      "Anthony Colas",
      "Christan Grant",
      "Zhe Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.948": {
    "title": "m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Yang",
      "Hongcheng Guo",
      "Yuwei Yin",
      "Jiaqi Bai",
      "Bing Wang",
      "Jiaheng Liu",
      "Xinnian Liang",
      "LinZheng Chai",
      "Liqun Yang",
      "Zhoujun Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.949": {
    "title": "M3TCM: Multi-modal Multi-task Context Model for Utterance Classification in Motivational Interviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayed Muddashir Hossain",
      "Jan Alexandersson",
      "Philipp Müller"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.950": {
    "title": "MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Priya Rani",
      "Theodorus Fransen",
      "John P. McCrae",
      "Gaurav Negi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.951": {
    "title": "MAGIC: Multi-Argument Generation with Self-Refinement for Domain Generalization in Automatic Fact-Checking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Yu Kao",
      "An-Zi Yen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.952": {
    "title": "MAGPIE: Multi-Task Analysis of Media-Bias Generalization with Pre-Trained Identification of Expressions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomáš Horych",
      "Martin Paul Wessel",
      "Jan Philip Wahle",
      "Terry Ruas",
      "Jerome Waßmuth",
      "André Greiner-Petter",
      "Akiko Aizawa",
      "Bela Gipp",
      "Timo Spinde"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.953": {
    "title": "MaiBaam: A Multi-Dialectal Bavarian Universal Dependency Treebank",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Verena Blaschke",
      "Barbara Kovačić",
      "Siyao Peng",
      "Hinrich Schütze",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.954": {
    "title": "MaintIE: A Fine-Grained Annotation Schema and Benchmark for Information Extraction from Maintenance Short Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tyler K. Bikaun",
      "Tim French",
      "Michael Stewart",
      "Wei Liu",
      "Melinda Hodkiewicz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.955": {
    "title": "Majority Rules Guided Aspect-Category Based Sentiment Analysis via Label Prior Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Li",
      "Shaopeng Tang",
      "Renwei Wu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.956": {
    "title": "Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiushi Sun",
      "Chengcheng Han",
      "Nuo Chen",
      "Renyu Zhu",
      "Jingyang Gong",
      "Xiang Li",
      "Ming Gao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.957": {
    "title": "Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengkun Ma",
      "Jiale Han",
      "Yi Liang",
      "Bo Cheng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.958": {
    "title": "Making Sentence Embeddings Robust to User-Generated Content",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lydia Nishimwe",
      "Benoît Sagot",
      "Rachel Bawden"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.959": {
    "title": "Malaysian English News Decoded: A Linguistic Resource for Named Entity and Relation Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MohanRaj Chanthran",
      "Lay-Ki Soon",
      "Huey Fang Ong",
      "Bhawani Selvaretnam"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.960": {
    "title": "mALBERT: Is a Compact Multilingual BERT Model Still Worth It?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christophe Servan",
      "Sahar Ghannay",
      "Sophie Rosset"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.961": {
    "title": "ManNER & ManPOS: Pioneering NLP for Endangered Manchu Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangah Lee",
      "Sungjoo Byun",
      "Jean Seo",
      "Minha Kang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.962": {
    "title": "Mapping the Past: Geographically Linking an Early 20th Century Swedish Encyclopedia with Wikidata",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Axel Ahlin",
      "Alfred Myrne Blåder",
      "Pierre Nugues"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.963": {
    "title": "Mapping Work Task Descriptions from German Job Ads on the O*NET Work Activities Ontology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ann-Sophie Gnehm",
      "Simon Clematide"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.964": {
    "title": "MARASTA: A Multi-dialectal Arabic Cross-domain Stance Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anis Charfi",
      "Mabrouka Ben-Sghaier",
      "Andria Samy Raouf Atalla",
      "Raghda Akasheh",
      "Sara Al-Emadi",
      "Wajdi Zaghouani"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.965": {
    "title": "Massively Multilingual Token-Based Typology Using the Parallel Bible Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amanda Kann"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.966": {
    "title": "Mathematical Entities: Corpora and Benchmarks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Collard",
      "Valeria de Paiva",
      "Eswaran Subrahmanian"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.967": {
    "title": "MccSTN: Multi-Scale Contrast and Fine-Grained Feature Fusion Networks for Subject-driven Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honggang Zhao",
      "Chunling Xiao",
      "Jiayi Yang",
      "Guozhu Jin",
      "Mingyong Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.968": {
    "title": "MCIL: Multimodal Counterfactual Instance Learning for Low-resource Entity-based Multimodal Information Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baohang Zhou",
      "Ying Zhang",
      "Kehui Song",
      "Hongru Wang",
      "Yu Zhao",
      "Xuhui Sui",
      "Xiaojie Yuan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.969": {
    "title": "MCTS: A Multi-Reference Chinese Text Simplification Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruining Chong",
      "Luming Lu",
      "Liner Yang",
      "Jinran Nie",
      "Zhenghao Liu",
      "Shuo Wang",
      "Shuhan Zhou",
      "Yaoxin Li",
      "Erhong Yang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.970": {
    "title": "MDS: A Fine-Grained Dataset for Multi-Modal Dialogue Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Liu",
      "Xiaoming Zhang",
      "Litian Zhang",
      "Zelong Yu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.971": {
    "title": "Measuring Cross-Text Cohesion for Segmentation Similarity Scoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gerardo Ocampo Diaz",
      "Jessica Ouyang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.972": {
    "title": "Medical Entity Disambiguation with Medical Mention Relation and Fine-grained Entity Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenpeng Lu",
      "Guobiao Zhang",
      "Xueping Peng",
      "Hongjiao Guan",
      "Shoujin Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.973": {
    "title": "Medical Vision-Language Pre-Training for Brain Abnormalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masoud Monajatipoor",
      "Zi-Yi Dou",
      "Aichi Chien",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.974": {
    "title": "MedMT5: An Open-Source Multilingual Text-to-Text LLM for the Medical Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iker García-Ferrero",
      "Rodrigo Agerri",
      "Aitziber Atutxa Salazar",
      "Elena Cabrio",
      "Iker de la Iglesia",
      "Alberto Lavelli",
      "Bernardo Magnini",
      "Benjamin Molinet",
      "Johana Ramirez-Romero",
      "German Rigau",
      "Jose Maria Villa-Gonzalez",
      "Serena Villata",
      "Andrea Zaninello"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.975": {
    "title": "MedQA-SWE - a Clinical Question & Answer Dataset for Swedish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niclas Hertzberg",
      "Anna Lokrantz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.976": {
    "title": "MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathanael Carraz Rakotonirina",
      "Marco Baroni"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.977": {
    "title": "MentalHelp: A Multi-Task Dataset for Mental Health in Social Media",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nishat Raihan",
      "Sadiya Sayara Chowdhury Puspo",
      "Shafkat Farabi",
      "Ana-Maria Bucur",
      "Tharindu Ranasinghe",
      "Marcos Zampieri"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.978": {
    "title": "MentalRiskES: A New Corpus for Early Detection of Mental Disorders in Spanish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alba M. Mármol Romero",
      "Adrián Moreno Muñoz",
      "Flor Miriam Plaza-del-Arco",
      "M. Dolores Molina González",
      "María Teresa Martín Valdivia",
      "L. Alfonso Ureña-López",
      "Arturo Montejo Ráez"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.979": {
    "title": "Meta-Adapter for Self-Supervised Speech Models: A Solution to Low-Resource Speech Recognition Challenges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaqi Chen",
      "Hao Zhang",
      "Xukui Yang",
      "Wenlin Zhang",
      "Dan Qu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.980": {
    "title": "Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoqun Li",
      "Hongyu Lin",
      "Yaojie Lu",
      "Hao Xiang",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.981": {
    "title": "Meta-Evaluation of Sentence Simplification Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noof Abdullah Alfear",
      "Dimitar Kazakov",
      "Hend Al-Khalifa"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.982": {
    "title": "Metaphors in Online Religious Communication: A Detailed Dataset and Cross-Genre Metaphor Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Reimann",
      "Tatjana Scheffler"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.983": {
    "title": "MEVTR: A Multilingual Model Enhanced with Visual Text Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohua Wang",
      "Wenlong Fei",
      "Min Hu",
      "Qingyu Zhang",
      "Aoqiang Zhu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.984": {
    "title": "mForms : Multimodal Form Filling with Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Larry Heck",
      "Simon Heck",
      "Anirudh S. Sundar"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.985": {
    "title": "MHGRL: An Effective Representation Learning Model for Electronic Health Records",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiyan Liu",
      "Liangzhi Li",
      "Xiaoli Wang",
      "Feng Luo",
      "Chang Liu",
      "Jinsong Su",
      "Yiming Qian"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.986": {
    "title": "MiDe22: An Annotated Multi-Event Tweet Dataset for Misinformation Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cagri Toraman",
      "Oguzhan Ozcelik",
      "Furkan Sahinuc",
      "Fazli Can"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.987": {
    "title": "Mind Your Neighbours: Leveraging Analogous Instances for Rhetorical Role Labeling for Legal Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Santosh T.y.s.s.",
      "Hassan Sarwat",
      "Ahmed Mohamed Abdelaal Abdou",
      "Matthias Grabmair"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.988": {
    "title": "MinT: Boosting Generalization in Mathematical Reasoning via Multi-view Fine-tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenwen Liang",
      "Dian Yu",
      "Xiaoman Pan",
      "Wenlin Yao",
      "Qingkai Zeng",
      "Xiangliang Zhang",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.989": {
    "title": "Mitigating Linguistic Artifacts in Emotion Recognition for Conversations from TV Scripts to Daily Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donovan Ong",
      "Shuo Sun",
      "Jian Su",
      "Bin Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.990": {
    "title": "Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yexin Wu",
      "Zhuosheng Zhang",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.991": {
    "title": "Mitigating Shortcuts in Language Models with Soft Label Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui He",
      "Huiqi Deng",
      "Haiyan Zhao",
      "Ninghao Liu",
      "Mengnan Du"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.992": {
    "title": "Mitigating Translationese in Low-resource Languages: The Storyboard Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Garry Kuwanto",
      "Eno-Abasi E. Urua",
      "Priscilla Amondi Amuok",
      "Shamsuddeen Hassan Muhammad",
      "Anuoluwapo Aremu",
      "Verrah Otiende",
      "Loice Emma Nanyanga",
      "Teresiah W. Nyoike",
      "Aniefon D. Akpan",
      "Nsima Ab Udouboh",
      "Idongesit Udeme Archibong",
      "Idara Effiong Moses",
      "Ifeoluwatayo A. Ige",
      "Benjamin Ajibade",
      "Olumide Benjamin Awokoya",
      "Idris Abdulmumin",
      "Saminu Mohammad Aliyu",
      "Ruqayya Nasir Iro",
      "Ibrahim Said Ahmad",
      "Deontae Smith",
      "Praise-EL Michaels",
      "David Ifeoluwa Adelani",
      "Derry Tanti Wijaya",
      "Anietie Andy"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.993": {
    "title": "MixRED: A Mix-lingual Relation Extraction Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingxing Kong",
      "Yougang Chu",
      "Zheng Ma",
      "Jianbing Zhang",
      "Liang He",
      "Jiajun Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.994": {
    "title": "Mixture-of-LoRAs: An Efficient Multitask Tuning Method for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenfeng Feng",
      "Chuzhan Hao",
      "Yuewei Zhang",
      "Yu Han",
      "Hao Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.995": {
    "title": "Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Wu",
      "Hsiu-Yuan Huang",
      "Fanyi Qu",
      "Yunfang Wu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.996": {
    "title": "MKeCL: Medical Knowledge-Enhanced Contrastive Learning for Few-shot Disease Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutian Zhao",
      "Huimin Wang",
      "Xian Wu",
      "Yefeng Zheng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.997": {
    "title": "MLDSP-MA: Multidimensional Attention for Multi-Round Long Dialogue Sentiment Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfei Yin",
      "Congrui Zou",
      "Zheng Yuan",
      "Xianjian Bao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.998": {
    "title": "MMAD:Multi-modal Movie Audio Description",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojun Ye",
      "Junhao Chen",
      "Xiang Li",
      "Haidong Xin",
      "Chao Li",
      "Sheng Zhou",
      "Jiajun Bu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.999": {
    "title": "MMAPS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Chen",
      "Ze Lin",
      "Hui Li",
      "Jiayi Ji",
      "Yiyi Zhou",
      "Guanbin Li",
      "Rongrong Ji"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1000": {
    "title": "MM-IGLU: Multi-Modal Interactive Grounded Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Claudiu Daniel Hromei",
      "Daniele Margiotta",
      "Danilo Croce",
      "Roberto Basili"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1001": {
    "title": "MNER-MI: A Multi-image Dataset for Multimodal Named Entity Recognition in Social Media",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shizhou Huang",
      "Bo Xu",
      "Changqun Li",
      "Jiabo Ye",
      "Xin Lin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1002": {
    "title": "Modalities Should Be Appropriately Leveraged: Uncertainty Guidance for Multimodal Chinese Spelling Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongliang Lin",
      "Zhen Zhang",
      "Mengting Hu",
      "Yufei Sun",
      "Yuzhi Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1003": {
    "title": "MoDE-CoTD: Chain-of-Thought Distillation for Complex Reasoning Tasks with Mixture of Decoupled LoRA-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Li",
      "Shizhu He",
      "Jiayu Wu",
      "Zhao Yang",
      "Yao Xu",
      "Yang jun Jun",
      "Haifeng Liu",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1004": {
    "title": "Model-Agnostic Cross-Lingual Training for Discourse Representation Structure Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangming Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1005": {
    "title": "Modeling Low-Resource Health Coaching Dialogues via Neuro-Symbolic Goal Summarization and Text-Units-Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhou",
      "Barbara Di Eugenio",
      "Brian Ziebart",
      "Lisa Sharp",
      "Bing Liu",
      "Nikolaos Agadakos"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1006": {
    "title": "Modeling Orthographic Variation Improves NLP Performance for Nigerian Pidgin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pin-Jie Lin",
      "Merel Scholman",
      "Muhammed Saeed",
      "Vera Demberg"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1007": {
    "title": "Modeling the Quality of Dialogical Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milad Alshomary",
      "Felix Lange",
      "Meisam Booshehri",
      "Meghdut Sengupta",
      "Philipp Cimiano",
      "Henning Wachsmuth"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1008": {
    "title": "Modelling and Linking an Old Latin-Portuguese Dictionary to the LiLa Knowledge Base",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Consolin Dezotti",
      "Marco Passarotti",
      "Francesco Mambrini"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1009": {
    "title": "Modelling Argumentation for an User Opinion Aggregation Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pablo Weingart",
      "Thiemo Wambsganss",
      "Matthias Soellner"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1010": {
    "title": "MoNMT: Modularly Leveraging Monolingual and Bilingual Knowledge for Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhui Pang",
      "Baosong Yang",
      "Derek F. Wong",
      "Dayiheng Liu",
      "Xiangpeng Wei",
      "Jun Xie",
      "Lidia S. Chao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1011": {
    "title": "Monolingual Paraphrase Detection Corpus for Low Resource Pashto Language at Sentence Level",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iqra Ali",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1012": {
    "title": "MoPE: Mixture of Prefix Experts for Zero-Shot Dialogue State Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianwen Tang",
      "Tong Zhu",
      "Haodong Liu",
      "Yin Bai",
      "Jia Cheng",
      "Wenliang Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1013": {
    "title": "MORE-3S:Multimodal-based Offline Reinforcement Learning with Shared Semantic Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Zheng",
      "Ge Zhang",
      "Xingwei Qu",
      "Ming Kuang",
      "Wenhao Huang",
      "Zhaofeng He"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1014": {
    "title": "Morpheme Sense Disambiguation: A New Task Aiming for Understanding the Language at Character Level",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Wang",
      "Hua Zheng",
      "Yaqi Yin",
      "Hansi Wang",
      "Qiliang Liang",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1015": {
    "title": "Motion Capture Analysis of Verb and Adjective Types in Austrian Sign Language (ÖGS)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julia Krebs",
      "Evguenia A. Malaia",
      "Isabella Fessl",
      "Hans-Peter Wiesinger",
      "Dietmar Roehm",
      "Ronnie Wilbur",
      "Hermann Schwameder"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1016": {
    "title": "Motion Generation from Fine-grained Textual Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunhang Li",
      "Yansong Feng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1017": {
    "title": "Motivational Interviewing Transcripts Annotated with Global Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Cohen",
      "Moreah Zisquit",
      "Stav Yosef",
      "Doron Friedman",
      "Kfir Bar"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1018": {
    "title": "MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiwen Ni",
      "Minghuan Tan",
      "Yuelin Bai",
      "Fuqiang Niu",
      "Min Yang",
      "Bowen Zhang",
      "Ruifeng Xu",
      "Xiaojun Chen",
      "Chengming Li",
      "Xiping Hu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1019": {
    "title": "MRC-based Nested Medical NER with Co-prediction and Adaptive Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojing Du",
      "Hanjie Zhao",
      "Danyan Xing",
      "Yuxiang Jia",
      "Hongying Zan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1020": {
    "title": "MRT: Multi-modal Short- and Long-range Temporal Convolutional Network for Time-sync Comment Video Behavior Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihao Zhao",
      "Weidong He",
      "Hao Wang",
      "Haoyang Bi",
      "Han Wu",
      "Chen Zhu",
      "Tong Xu",
      "Enhong Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1021": {
    "title": "MUCH: A Multimodal Corpus Construction for Conversational Humor Recognition Based on Chinese Sitcom",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Guo",
      "Wenbo Shang",
      "Xueyao Zhang",
      "Binyang Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1022": {
    "title": "Multi-Channel Spatio-Temporal Transformer for Sign Language Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Ma",
      "Rize Jin",
      "Tae-Sun Chung"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1023": {
    "title": "MULTICOLLAB: A Multimodal Corpus of Dialogues for Analyzing Collaboration and Frustration in Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Peechatt",
      "Cecilia Ovesdotter Alm",
      "Reynold Bailey"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1024": {
    "title": "Multi-Dimensional Machine Translation Evaluation: Model Evaluation and Resource for Korean",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dojun Park",
      "Sebastian Padó"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1025": {
    "title": "Multi-domain Hate Speech Detection Using Dual Contrastive Learning and Paralinguistic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Somaiyeh Dehghan",
      "Berrin Yanıkoğlu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1026": {
    "title": "Multi-Grained Conversational Graph Network for Retrieval-based Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan Tu",
      "Chongyang Tao",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1027": {
    "title": "Multi-Granularity Fusion Text Semantic Matching Based on WoBERT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongchun Yu",
      "Wei Pan",
      "Xing Fan",
      "Hanqi Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1028": {
    "title": "MultiLeg: Dataset for Text Sanitisation in Less-resourced Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rinalds Vīksna",
      "Inguna Skadiņa"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1029": {
    "title": "MultiLexBATS: Multilingual Dataset of Lexical Semantic Relations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dagmar Gromann",
      "Hugo Goncalo Oliveira",
      "Lucia Pitarch",
      "Elena-Simona Apostol",
      "Jordi Bernad",
      "Eliot Bytyçi",
      "Chiara Cantone",
      "Sara Carvalho",
      "Francesca Frontini",
      "Radovan Garabik",
      "Jorge Gracia",
      "Letizia Granata",
      "Fahad Khan",
      "Timotej Knez",
      "Penny Labropoulou",
      "Chaya Liebeskind",
      "Maria Pia Di Buono",
      "Ana Ostroški Anić",
      "Sigita Rackevičienė",
      "Ricardo Rodrigues",
      "Gilles Sérasset",
      "Linas Selmistraitis",
      "Mahammadou Sidibé",
      "Purificação Silvano",
      "Blerina Spahiu",
      "Enriketa Sogutlu",
      "Ranka Stanković",
      "Ciprian-Octavian Truică",
      "Giedre Valunaite Oleskeviciene",
      "Slavko Zitnik",
      "Katerina Zdravkova"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1030": {
    "title": "Multilingual Brain Surgeon: Large Language Models Can Be Compressed Leaving No Language behind",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongchuan Zeng",
      "Hongshen Xu",
      "Lu Chen",
      "Kai Yu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1031": {
    "title": "Multilingual Coreference Resolution in Low-resource South Asian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ritwik Mishra",
      "Pooja Desur",
      "Rajiv Ratn Shah",
      "Ponnurangam Kumaraguru"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1032": {
    "title": "Multilingual Generation in Abstractive Summarization: A Comparative Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinpeng Li",
      "Jiaze Chen",
      "Huadong Chen",
      "Dongyan Zhao",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1033": {
    "title": "Multilinguality or Back-translation? A Case Study with Estonian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elizaveta Korotkova",
      "Taido Purason",
      "Agnes Luhtaru",
      "Mark Fishel"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1034": {
    "title": "Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chihiro Yano",
      "Akihiko Fukuchi",
      "Shoko Fukasawa",
      "Hideyuki Tachibana",
      "Yotaro Watanabe"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1035": {
    "title": "Multilingual Substitution-based Word Sense Induction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Kokosinskii",
      "Nikolay Arefyev"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1036": {
    "title": "Multilingual Turn-taking Prediction Using Voice Activity Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Koji Inoue",
      "Bing’er Jiang",
      "Erik Ekstedt",
      "Tatsuya Kawahara",
      "Gabriel Skantze"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1037": {
    "title": "Multimodal and Multilingual Laughter Detection in Stand-Up Comedy Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Kuznetsova",
      "Carlo Strapparava"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1038": {
    "title": "Multimodal Behaviour in an Online Environment: The GEHM Zoom Corpus Collection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrizia Paggio",
      "Manex Agirrezabal",
      "Costanza Navarretta",
      "Leo Vitasovic"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1039": {
    "title": "Multimodal Cross-Document Event Coreference Resolution Using Linear Semantic Transfer and Mixed-Modality Ensembles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhijnan Nath",
      "Huma Jamil",
      "Shafiuddin Rehan Ahmed",
      "George Arthur Baker",
      "Rahul Ghosh",
      "James H. Martin",
      "Nathaniel Blanchard",
      "Nikhil Krishnaswamy"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1040": {
    "title": "Multimodal Cross-lingual Phrase Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanqi Dong",
      "Wenjie Zhou",
      "Xiangyu Duan",
      "Yuqi Zhang",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1041": {
    "title": "Multimodal Language Models Show Evidence of Embodied Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cameron R. Jones",
      "Sean Trott"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1042": {
    "title": "Multi-modal Semantic Understanding with Contrastive Cross-modal Feature Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Zhang",
      "Ke Chang",
      "Yunfang Wu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1043": {
    "title": "Multi-Objective Forward Reasoning and Multi-Reward Backward Refinement for Product Review Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Libo Sun",
      "Siyuan Wang",
      "Meng Han",
      "Ruofei Lai",
      "Xinyu Zhang",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1044": {
    "title": "Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Derong Xu",
      "Ziheng Zhang",
      "Zhenxi Lin",
      "Xian Wu",
      "Zhihong Zhu",
      "Tong Xu",
      "Xiangyu Zhao",
      "Yefeng Zheng",
      "Enhong Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1045": {
    "title": "Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Jain",
      "David M. Chan",
      "Pranav Dheram",
      "Aparna Khare",
      "Olabanji Shonibare",
      "Venkatesh Ravichandran",
      "Shalini Ghosh"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1046": {
    "title": "Multi-stream Information Fusion Framework for Emotional Support Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinan Bao",
      "Dou Hu",
      "Lingwei Wei",
      "Shuchong Wei",
      "Wei Zhou",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1047": {
    "title": "Multi-Tiered Cantonese Word Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles Lam",
      "Chaak-ming Lau",
      "Jackson L. Lee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1048": {
    "title": "Murre24: Dialect Identification of Finnish Internet Forum Messages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olli Kuparinen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1049": {
    "title": "MVP: Minimal Viable Phrase for Long Text Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Louis Clouatre",
      "Amal Zouaq",
      "Sarath Chandar"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1050": {
    "title": "MWE-Finder: A Demonstration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Odijk",
      "Martin Kroon",
      "Tijmen Baarda",
      "Ben Bonfil",
      "Sheean Spoel"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1051": {
    "title": "myMediCon: End-to-End Burmese Automatic Speech Recognition for Medical Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hay Man Htun",
      "Ye Kyaw Thu",
      "Hutchatai Chanlekha",
      "Kotaro Funakoshi",
      "Thepchai Supnithi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1052": {
    "title": "My Science Tutor (MyST)–a Large Corpus of Children's Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sameer Pradhan",
      "Ronald A. Cole",
      "Wayne H. Ward"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1053": {
    "title": "NAIST-SIC-Aligned: An Aligned English-Japanese Simultaneous Interpretation Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinming Zhao",
      "Katsuhito Sudoh",
      "Satoshi Nakamura",
      "Yuka Ko",
      "Kosuke Doi",
      "Ryo Fukuda"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1054": {
    "title": "NarrativeTime: Dense Temporal Annotation on a Timeline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Rogers",
      "Marzena Karpinska",
      "Ankita Gupta",
      "Vladislav Lialin",
      "Gregory Smelkov",
      "Anna Rumshisky"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1055": {
    "title": "Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yida Mu",
      "Ben P. Wu",
      "William Thorne",
      "Ambrose Robinson",
      "Nikolaos Aletras",
      "Carolina Scarton",
      "Kalina Bontcheva",
      "Xingyi Song"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1056": {
    "title": "NB Uttale: A Norwegian Pronunciation Lexicon with Dialect Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marie Iversdatter Røsok",
      "Ingerid Løyning Dale"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1057": {
    "title": "Negation Scope Conversion: Towards a Unified Negation-Annotated Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asahi Yoshida",
      "Yoshihide Kato",
      "Shigeki Matsubara"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1058": {
    "title": "Negation Triplet Extraction with Syntactic Dependency and Semantic Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Shi",
      "Deqing Yang",
      "Jingping Liu",
      "Yanghua Xiao",
      "Zongyu Wang",
      "Huimin Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1059": {
    "title": "nEMO: Dataset of Emotional Speech in Polish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iwona Christop"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1060": {
    "title": "NER-guided Comprehensive Hierarchy-aware Prompt Tuning for Hierarchical Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuhan Cai",
      "Duo Liu",
      "Zhongqiang Zhang",
      "Ge Liu",
      "Xiaozhe Yang",
      "Xiangzhong Fang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1061": {
    "title": "Nested Event Extraction upon Pivot Element Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weicheng Ren",
      "Zixuan Li",
      "Xiaolong Jin",
      "Long Bai",
      "Miao Su",
      "Yantao Liu",
      "Saiping Guan",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1062": {
    "title": "Nested Noun Phrase Identification Using BERT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shweta Misra",
      "Johan Boye"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1063": {
    "title": "Neural Machine Translation between Low-Resource Languages with Synthetic Pivoting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khalid Ahmed",
      "Jan Buys"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1064": {
    "title": "Neural Multimodal Topic Modeling: A Comprehensive Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felipe Gonzalez-Pizarro",
      "Giuseppe Carenini"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1065": {
    "title": "New Datasets for Automatic Detection of Textual Entailment and of Contradictions between Sentences in French",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximos Skandalis",
      "Richard Moot",
      "Christian Retoré",
      "Simon Robillard"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1066": {
    "title": "New Evaluation Methodology for Qualitatively Comparing Classification Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Aljanaideh"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1067": {
    "title": "New Intent Discovery with Attracting and Dispersing Prototype",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shun Zhang",
      "Jian Yang",
      "Jiaqi Bai",
      "Chaoran Yan",
      "Tongliang Li",
      "Zhao Yan",
      "Zhoujun Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1068": {
    "title": "New Methods for Exploring Intonosyntax: Introducing an Intonosyntactic Treebank for Nigerian Pidgin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emmett Strickland",
      "Anne Lacheret-Dujour",
      "Sylvain Kahane",
      "Marc Evrard",
      "Perrine Quennehen",
      "Bernard Caron",
      "Francis Egbokhare",
      "Bruno Guillaume"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1069": {
    "title": "New Proposal of Greenberg's Universal 14 from Typometrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoni Brosa-Rodríguez",
      "Sylvain Kahane"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1070": {
    "title": "New Semantic Task for the French Spoken Language Understanding MEDIA Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nadège Alavoine",
      "Gaëlle Laperrière",
      "Christophe Servan",
      "Sahar Ghannay",
      "Sophie Rosset"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1071": {
    "title": "NGLUEni: Benchmarking and Adapting Pretrained Language Models for Nguni Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francois Meyer",
      "Haiyue Song",
      "Abhisek Chakrabarty",
      "Jan Buys",
      "Raj Dabre",
      "Hideki Tanaka"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1072": {
    "title": "NLoPT: N-gram Enhanced Low-Rank Task Adaptive Pre-training for Efficient Language Model Adaption",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Gu",
      "Jiangyan Yi",
      "Zheng Lian",
      "Jianhua Tao",
      "Xinrui Yan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1073": {
    "title": "NLPre: A Revised Approach towards Language-centric Benchmarking of Natural Language Preprocessing Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martyna Wiącek",
      "Piotr Rybak",
      "Łukasz Pszenny",
      "Alina Wróblewska"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1074": {
    "title": "No Need for Large-Scale Search: Exploring Large Language Models in Complex Knowledge Base Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shouhui Wang",
      "Biao Qin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1075": {
    "title": "Non-Essential Is NEcessary: Order-agnostic Multi-hop Question Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyungho Kim",
      "Seongmin Park",
      "Junseo Lee",
      "Jihwa Lee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1076": {
    "title": "NSina: A News Corpus for Sinhala",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hansi Hettiarachchi",
      "Damith Premasiri",
      "Lasitha Randunu Chandrakantha Uyangodage",
      "Tharindu Ranasinghe"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1077": {
    "title": "Null Subjects in Spanish as a Machine Translation Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jose Diego Suarez",
      "Luis Chiruzzo"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1078": {
    "title": "NumHG: A Dataset for Number-Focused Headline Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian-Tao Huang",
      "Chung-Chi Chen",
      "Hen-Hsen Huang",
      "Hsin-Hsi Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1079": {
    "title": "NutFrame: Frame-based Conceptual Structure Induction with LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoru Guo",
      "Yubo Chen",
      "Kang Liu",
      "Ru Li",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1080": {
    "title": "OATS: A Challenge Dataset for Opinion Aspect Target Sentiment Joint Detection for Aspect-Based Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siva Uday Sampreeth Chebolu",
      "Franck Dernoncourt",
      "Nedim Lipka",
      "Thamar Solorio"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1081": {
    "title": "OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adnen Abdessaied",
      "Manuel Hochmeister",
      "Andreas Bulling"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1082": {
    "title": "On an Intermediate Task for Classifying URL Citations on Scholarly Papers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazuhiro Wada",
      "Masaya Tsunokake",
      "Shigeki Matsubara"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1083": {
    "title": "On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Wu",
      "Wasi U. Ahmad",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1084": {
    "title": "On Modelling Corpus Citations in Computational Lexical Resources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fahad Khan",
      "Maxim Ionov",
      "Christian Chiarcos",
      "Laurent Romary",
      "Gilles Sérasset",
      "Besim Kabashi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1085": {
    "title": "On the Adaptation of Unlimiformer for Decoder-Only Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kian Ahrabian",
      "Alon Benhaim",
      "Barun Patra",
      "Jay Pujara",
      "Saksham Singhal",
      "Xia Song"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1086": {
    "title": "On the Relationship between Skill Neurons and Robustness in Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leon Ackermann",
      "Xenia Isabel Ohmer"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1087": {
    "title": "On the Scaling Laws of Geographical Representation in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Godey",
      "Éric de la Clergerie",
      "Benoît Sagot"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1088": {
    "title": "On the Use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Wang",
      "Tianyin Wang",
      "Ziqian Zeng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1089": {
    "title": "On the Way to Lossless Compression of Language Transformers: Exploring Cross-Domain Properties of Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Martynov",
      "Aleksei Goncharov",
      "Gleb Kumichev",
      "Evgeniy Egorov",
      "Stanislav Vladimirovich Pavlov",
      "Mikhail Sergeevich Durinov",
      "Aleksandr Sergeevich Zuev",
      "Egor Anatolievich Filimonov"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1090": {
    "title": "On Zero-Shot Counterspeech Generation by LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Punyajoy Saha",
      "Aalok Agrawal",
      "Abhik Jana",
      "Chris Biemann",
      "Animesh Mukherjee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1091": {
    "title": "OOVs in the Spotlight: How to Inflect Them?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomáš Sourada",
      "Jana Straková",
      "Rudolf Rosa"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1092": {
    "title": "OpenMSD: Towards Multilingual Scientific Documents Similarity Measurement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Gao",
      "Ji Ma",
      "Ivan Korotkov",
      "Keith Hall",
      "Dana Alon",
      "Donald Metzler"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1093": {
    "title": "Opinion Mining Using Pre-Trained Large Language Models: Identifying the Type, Polarity, Intensity, Expression, and Source of Private States",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeed Ahmadnia",
      "Arash Yousefi Jordehi",
      "Mahsa Hosseini Khasheh Heyran",
      "SeyedAbolghasem Mirroshandel",
      "Owen Rambow"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1094": {
    "title": "Opinions Are Not Always Positive: Debiasing Opinion Summarization with Model-Specific and Model-Agnostic Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanyue Zhang",
      "Yilong Lai",
      "Zhenglin Wang",
      "Pengfei Li",
      "Deyu Zhou",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1095": {
    "title": "Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ChangSu Choi",
      "Yongbin Jeong",
      "Seoyoon Park",
      "Inho Won",
      "HyeonSeok Lim",
      "SangMin Kim",
      "Yejee Kang",
      "Chanhyuk Yoon",
      "Jaewan Park",
      "Yiseul Lee",
      "HyeJin Lee",
      "Younggyun Hahm",
      "Hansaem Kim",
      "KyungTae Lim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1096": {
    "title": "ORTicket: Let One Robust BERT Ticket Transfer across Different Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Zhou",
      "Wenxiang Chen",
      "Rui Zheng",
      "Zhiheng Xi",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1097": {
    "title": "Out-of-Domain Intent Detection Considering Multi-Turn Dialogue Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Lang",
      "Yinhe Zheng",
      "Binyuan Hui",
      "Fei Huang",
      "Yongbin Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1098": {
    "title": "Out of the Mouths of MPs: Speaker Attribution in Parliamentary Debates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ines Rehbein",
      "Josef Ruppenhofer",
      "Annelen Brunner",
      "Simone Paolo Ponzetto"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1099": {
    "title": "PACAR: Automated Fact-Checking with Planning and Customized Action Reasoning Using Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyan Zhao",
      "Lingzhi Wang",
      "Zhanghao Wang",
      "Hong Cheng",
      "Rui Zhang",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1100": {
    "title": "PAD: A Robustness Enhancement Ensemble Method via Promoting Attention Diversity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuting Yang",
      "Pei Huang",
      "Feifei Ma",
      "Juan Cao",
      "Jintao Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1101": {
    "title": "Palmyra 3.0: A User-Friendly Cloud-Based Platform for Morphology and Dependency Syntax Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammed AbuOdeh",
      "Long Phan",
      "Ahmed Farouk Zakaria Elshabrawy",
      "Nizar Habash"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1102": {
    "title": "Parameter-Efficient Transfer Learning for End-to-end Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunlong Zhao",
      "Kexin Wang",
      "Qianqian Dong",
      "Tom Ko"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1103": {
    "title": "ParaNames 1.0: Creating an Entity Name Corpus for 400+ Languages Using Wikidata",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonne Sälevä",
      "Constantine Lignos"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1104": {
    "title": "PaReNT (Parent Retrieval Neural Tool): A Deep Dive into Word Formation across Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emil Svoboda",
      "Magda Sevcikova"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1105": {
    "title": "Parsing for Mauritian Creole Using Universal Dependencies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neha Ramsurrun",
      "Rolando Coto-Solano",
      "Michael Gonzalez"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1106": {
    "title": "Parsing Headed Constituencies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katarzyna Krasnowska-Kieraś",
      "Marcin Woliński"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1107": {
    "title": "PASUM: A Pre-training Architecture for Social Media User Modeling Based on Text Graph",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Wu",
      "Xinyi Mou",
      "Lanqing Xue",
      "Zhenzhe Ying",
      "Weiqiang Wang",
      "Qi Zhang",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1108": {
    "title": "Pater Incertus? There Is a Solution: Automatic Discrimination between Cognates and Borrowings for Romance Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liviu P. Dinu",
      "Ana Sabina Uban",
      "Ioan-Bogdan Iordache",
      "Alina Maria Cristea",
      "Simona Georgescu",
      "Laurentiu Zoicas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1109": {
    "title": "PDAMeta: Meta-Learning Framework with Progressive Data Augmentation for Few-Shot Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xurui Li",
      "Kaisong Song",
      "Tianqianjing Lin",
      "Yangyang Kang",
      "Fubang Zhao",
      "Changlong Sun",
      "Xiaozhong Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1110": {
    "title": "PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on Scientific Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Zhang",
      "Connor Heaton",
      "Sean Timothy Okonsky",
      "Prasenjit Mitra",
      "Hilal Ezgi Toraman"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1111": {
    "title": "PECC: Problem Extraction and Coding Challenges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Haller",
      "Jonas Golde",
      "Alan Akbik"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1112": {
    "title": "PejorativITy: Disambiguating Pejorative Epithets to Improve Misogyny Detection in Italian Tweets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arianna Muti",
      "Federico Ruggeri",
      "Cagri Toraman",
      "Alberto Barrón-Cedeño",
      "Samuel Algherini",
      "Lorenzo Musetti",
      "Silvia Ronchi",
      "Gianmarco Saretto",
      "Caterina Zapparoli"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1113": {
    "title": "Persona-aware Multi-party Conversation Response Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khyati Mahajan",
      "Samira Shaikh"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1114": {
    "title": "Phonetic Segmentation of the UCLA Phonetics Lab Archive",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eleanor Chodroff",
      "Blaž Pažon",
      "Annie Baker",
      "Steven Moran"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1115": {
    "title": "Phonotactic Complexity across Dialects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Soh-Eun Shim",
      "Kalvin Chang",
      "David R. Mortensen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1116": {
    "title": "PILA: A Historical-Linguistic Dataset of Proto-Italic and Latin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen Bothwell",
      "Brian DuSell",
      "David Chiang",
      "Brian Krostenko"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1117": {
    "title": "PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Slawomir Dadas",
      "Michał Perełkiewicz",
      "Rafał Poświata"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1118": {
    "title": "PLAES: Prompt-generalized and Level-aware Learning Framework for Cross-prompt Automated Essay Scoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Chen",
      "Xia Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1119": {
    "title": "Plots Made Quickly: An Efficient Approach for Generating Visualizations from Natural Language Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henrik Voigt",
      "Kai Lawonn",
      "Sina Zarrieß"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1120": {
    "title": "Pluggable Neural Machine Translation Models via Memory-augmented Adapters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhuang Xu",
      "Shuo Wang",
      "Peng Li",
      "Xuebo Liu",
      "Xiaolong Wang",
      "Weidong Liu",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1121": {
    "title": "Pointing Out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gennaro Nolano",
      "Moritz Blum",
      "Basil Ell",
      "Philipp Cimiano"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1122": {
    "title": "Polish-ASTE: Aspect-Sentiment Triplet Extraction Datasets for Polish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marta Lango",
      "Borys Naglik",
      "Mateusz Lango",
      "Iwo Naglik"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1123": {
    "title": "Polish Discourse Corpus (PDC): Corpus Design, ISO-Compliant Annotation, Data Highlights, and Parser Development",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maciej Ogrodniczuk",
      "Aleksandra Tomaszewska",
      "Daniel Ziembicki",
      "Sebastian Żurowski",
      "Ryszard Tuora",
      "Aleksandra Zwierzchowska"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1124": {
    "title": "PolitiCause: An Annotation Scheme and Corpus for Causality in Political Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paulina Garcia Corral",
      "Hanna Bechara",
      "Ran Zhang",
      "Slava Jankin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1125": {
    "title": "PolQA: Polish Question Answering Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piotr Rybak",
      "Piotr Przybyła",
      "Maciej Ogrodniczuk"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1126": {
    "title": "PolyNERE: A Novel Ontology and Corpus for Named Entity Recognition and Relation Extraction in Polymer Science Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Van-Thuy Phi",
      "Hiroki Teranishi",
      "Yuji Matsumoto",
      "Hiroyuki Oka",
      "Masashi Ishii"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1127": {
    "title": "PopALM: Popularity-Aligned Language Models for Social Media Trendy Response Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erxin Yu",
      "Jing Li",
      "Chunpu Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1128": {
    "title": "PopAut: An Annotated Corpus for Populism Detection in Austrian News Comments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmadou Wagne",
      "Julia Neidhardt",
      "Thomas Elmar Kolb"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1129": {
    "title": "Positive and Risky Message Assessment for Music Products",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yigeng Zhang",
      "Mahsa Shafaei",
      "Fabio Gonzalez",
      "Thamar Solorio"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1130": {
    "title": "POS Tagging for the Endangered Dagur Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joanna Dolińska",
      "Delphine Bernhard"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1131": {
    "title": "Post-decoder Biasing for End-to-End Speech Recognition of Multi-turn Medical Interview",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heyang Liu",
      "Yanfeng Wang",
      "Yu Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1132": {
    "title": "PPORTAL_ner: An Annotated Corpus of Portuguese Literary Entities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mariana O. Silva",
      "Mirella M. Moro"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1133": {
    "title": "Predictive and Distinctive Linguistic Features in Schizophrenia-Bipolar Spectrum Disorders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martina Katalin Szabó",
      "Veronika Vincze",
      "Bernadett Dam",
      "Csenge Guba",
      "Anita Bagi",
      "István Szendi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1134": {
    "title": "Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guisheng Liu",
      "Yi Li",
      "Zhengcong Fei",
      "Haiyan Fu",
      "Xiangyang Luo",
      "Yanqing Guo"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1135": {
    "title": "Pre-Trained Language Models Represent Some Geographic Populations Better than Others",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Dunn",
      "Benjamin Adams",
      "Harish Tayyar Madabushi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1136": {
    "title": "Pre-training Cross-Modal Retrieval by Expansive Lexicon-Patch Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yiyuan",
      "Guodong Long",
      "Michael Blumenstein",
      "Xiubo Geng",
      "Chongyang Tao",
      "Tao Shen",
      "Daxin Jiang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1137": {
    "title": "PRIMO: Progressive Induction for Multi-hop Open Rule Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyu Liu",
      "Sheng Bi",
      "Guilin Qi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1138": {
    "title": "Principal Component Analysis as a Sanity Check for Bayesian Phylolinguistic Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yugo Murawaki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1139": {
    "title": "Prior Relational Schema Assists Effective Contrastive Learning for Inductive Knowledge Graph Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruilin Luo",
      "Jiayi Li",
      "Jianghangfan Zhang",
      "Jing Xiao",
      "Yujiu Yang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1140": {
    "title": "Probe Then Retrieve and Reason: Distilling Probing and Reasoning Capabilities into Smaller Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichun Zhao",
      "Shuheng Zhou",
      "Huijia Zhu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1141": {
    "title": "Probing Large Language Models for Scalar Adjective Lexical Semantics and Scalar Diversity Pragmatics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangru Lin",
      "Daniel Altshuler",
      "Janet B. Pierrehumbert"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1142": {
    "title": "Probing Multimodal Large Language Models for Global and Local Semantic Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingxu Tao",
      "Quzhe Huang",
      "Kun Xu",
      "Liwei Chen",
      "Yansong Feng",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1143": {
    "title": "ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehan Li",
      "Jianfei Zhang",
      "Chuantao Yin",
      "Yuanxin Ouyang",
      "Wenge Rong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1144": {
    "title": "PRODIS - a Speech Database and a Phoneme-based Language Model for the Study of Predictability Effects in Polish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zofia Malisz",
      "Jan Foremski",
      "Małgorzata Kul"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1145": {
    "title": "Producing a Parallel Universal Dependencies Treebank of Ancient Hebrew and Ancient Greek via Cross-Lingual Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel G. Swanson",
      "Bryce D. Bussert",
      "Francis Tyers"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1146": {
    "title": "Projective Methods for Mitigating Gender Bias in Pre-trained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hillary Dawkins",
      "Isar Nejadgholi",
      "Daniel Gillis",
      "Judi McCuaig"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1147": {
    "title": "Project MOSLA: Recording Every Moment of Second Language Acquisition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masato Hagiwara",
      "Joshua B. Tanner"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1148": {
    "title": "PROM: A Phrase-level Copying Mechanism with Pre-training for Abstractive Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinbei Ma",
      "Yeyun Gong",
      "Pengcheng He",
      "Hai Zhao",
      "Nan Duan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1149": {
    "title": "PromISe: Releasing the Capabilities of LLMs with Prompt Introspective Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minzheng Wang",
      "Nan Xu",
      "Jiahao Zhao",
      "Yin Luo",
      "Wenji Mao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1150": {
    "title": "Prompt-based Generation of Natural Language Explanations of Synthetic Lethality for Cancer Drug Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Zhang",
      "Yimiao Feng",
      "Jie Zheng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1151": {
    "title": "Prompt-based Zero-shot Relation Extraction with Semantic Knowledge Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaying Gong",
      "Hoda Eldardiry"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1152": {
    "title": "Prompt-fused Framework for Inductive Logical Query Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zezhong Xu",
      "Wen Zhang",
      "Peng Ye",
      "Lei Liang",
      "Huajun Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1153": {
    "title": "Prompting-based Synthetic Data Generation for Few-Shot Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Schmidt",
      "Andrea Bartezzaghi",
      "Ngoc Thang Vu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1154": {
    "title": "Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangming Huang",
      "Yunfei Long",
      "Cunjin Luo",
      "Jiaxing Shen",
      "Xia Sun"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1155": {
    "title": "Prompting for Numerical Sequences: A Case Study on Market Comment Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masayuki Kawarada",
      "Tatsuya Ishigaki",
      "Hiroya Takamura"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1156": {
    "title": "Prompting Large Language Models for Counterfactual Generation: An Empirical Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongqi Li",
      "Mayi Xu",
      "Xin Miao",
      "Shen Zhou",
      "Tieyun Qian"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1157": {
    "title": "PromptStream: Self-Supervised News Story Discovery Using Topic-Aware Article Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arezoo Hatefi",
      "Anton Eklund",
      "Mona Forsman"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1158": {
    "title": "Prompt Tuning for Few-shot Relation Extraction via Modeling Global and Local Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Zhang",
      "Yiyu Yang",
      "Benhui Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1159": {
    "title": "PrOnto: Language Model Evaluations for 859 Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luke Gessler"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1160": {
    "title": "Prophecy Distillation for Boosting Abstractive Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Duan",
      "Fengyu Lu",
      "Junfei Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1161": {
    "title": "Prototype-based Prompt-Instance Interaction with Causal Intervention for Few-shot Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyao Tang",
      "Lishuang Li",
      "Hongbin Lu",
      "Xueyang Qin",
      "Beibei Zhang",
      "Haiming Wu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1162": {
    "title": "Pruning before Fine-tuning: A Retraining-free Compression Framework for Pre-trained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingjie Wang",
      "Hongcheng Liu",
      "Yanfeng Wang",
      "Yu Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1163": {
    "title": "PSentScore: Evaluating Sentiment Polarity in Dialogue Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongxin Zhou",
      "Fabien Ringeval",
      "François Portet"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1164": {
    "title": "Pseudonymization Categories across Domain Boundaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria Irena Szawerna",
      "Simon Dobnik",
      "Therese Lindström Tiedemann",
      "Ricardo Muñoz Sánchez",
      "Xuan-Son Vu",
      "Elena Volodina"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1165": {
    "title": "PSE v1.0: The First Open Access Corpus of Public Service Encounters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ingrid Espinoza",
      "Steffen Frenzel",
      "Laurin Friedrich",
      "Wassiliki Siskou",
      "Steffen Eckhard",
      "Annette Hautli-Janisz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1166": {
    "title": "PSYDIAL: Personality-based Synthetic Dialogue Generation Using Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji-Eun Han",
      "Jun-Seok Koh",
      "Hyeon-Tae Seo",
      "Du-Seong Chang",
      "Kyung-Ah Sohn"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1167": {
    "title": "Puntuguese: A Corpus of Puns in Portuguese with Micro-edits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcio Lima Inacio",
      "Gabriela Wick-Pedro",
      "Renata Ramisch",
      "Luís Espírito Santo",
      "Xiomara S. Q. Chacon",
      "Roney Santos",
      "Rogério Sousa",
      "Rafael Anchiêta",
      "Hugo Goncalo Oliveira"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1168": {
    "title": "PWESuite: Phonetic Word Embeddings and Tasks They Facilitate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vilém Zouhar",
      "Kalvin Chang",
      "Chenxuan Cui",
      "Nate B. Carlson",
      "Nathaniel Romney Robinson",
      "Mrinmaya Sachan",
      "David R. Mortensen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1169": {
    "title": "PyRater: A Python Toolkit for Annotation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angelo Basile",
      "Marc Franco-Salvador",
      "Paolo Rosso"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1170": {
    "title": "Qabas: An Open-Source Arabic Lexicographic Database",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mustafa Jarrar",
      "Tymaa Hasanain Hammouda"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1171": {
    "title": "QA-based Event Start-Points Ordering for Clinical Temporal Relation Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seiji Shimizu",
      "Lis Pereira",
      "Shuntaro Yada",
      "Eiji Aramaki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1172": {
    "title": "QCAW 1.0: Building a Qatari Corpus of Student Argumentative Writing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wajdi Zaghouani",
      "Abdelhamid Ahmed",
      "Xiao Zhang",
      "Lameya Rezk"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1173": {
    "title": "QDMR-based Planning-and-Solving Prompting for Complex Reasoning Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinfeng Huang",
      "Qiaoqiao She",
      "Wenbin Jiang",
      "Hua Wu",
      "Yang Hao",
      "Tong Xu",
      "Feng Wu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1174": {
    "title": "Qsnail: A Questionnaire Dataset for Sequential Question Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Lei",
      "Liang Pang",
      "Yuanzhuo Wang",
      "Huawei Shen",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1175": {
    "title": "Quantifying the Impact of Disfluency on Spoken Content Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria Teleki",
      "Xiangjue Dong",
      "James Caverlee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1176": {
    "title": "QUEEREOTYPES: A Multi-Source Italian Corpus of Stereotypes towards LGBTQIA+ Community Members",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandra Teresa Cignarella",
      "Manuela Sanguinetti",
      "Simona Frenda",
      "Andrea Marra",
      "Cristina Bosco",
      "Valerio Basile"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1177": {
    "title": "Query-driven Relevant Paragraph Extraction from Legal Judgments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Santosh T.y.s.s.",
      "Elvin A. Quero Hernandez",
      "Matthias Grabmair"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1178": {
    "title": "QueryNER: Segmentation of E-commerce Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chester Palen-Michel",
      "Lizzie Liang",
      "Zhe Wu",
      "Constantine Lignos"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1179": {
    "title": "Question Answering over Tabular Data with DataBench: A Large-Scale Empirical Evaluation of LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jorge Osés Grijalba",
      "L. Alfonso Ureña-López",
      "Eugenio Martínez Cámara",
      "Jose Camacho-Collados"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1180": {
    "title": "Quite Good, but Not Enough: Nationality Bias in Large Language Models - a Case Study of ChatGPT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shucheng Zhu",
      "Weikang Wang",
      "Ying Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1181": {
    "title": "RAAMove: A Corpus for Analyzing Moves in Research Article Abstracts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongzheng Li",
      "Ruojin Wang",
      "Ge Shi",
      "Xing Lv",
      "Lei Lei",
      "Chong Feng",
      "Fang Liu",
      "Jinkun Lin",
      "Yangguang Mei",
      "Linnan Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1182": {
    "title": "RADCoT: Retrieval-Augmented Distillation to Specialization Models for Generating Chain-of-Thoughts in Query Expansion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sung-Min Lee",
      "Eunhwan Park",
      "DongHyeon Jeon",
      "Inho Kang",
      "Seung-Hoon Na"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1183": {
    "title": "RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Hu",
      "Yuan Ge",
      "Xiangnan Ma",
      "Hang Cao",
      "Qiang Li",
      "Yonghua Yang",
      "Tong Xiao",
      "Jingbo Zhu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1184": {
    "title": "Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikun Sun",
      "Zhen Wan",
      "Nobuhiro Ueda",
      "Sakiko Yahata",
      "Fei Cheng",
      "Chenhui Chu",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1185": {
    "title": "Rapidly Piloting Real-time Linguistic Assistance for Simultaneous Interpreters with Untrained Bilingual Surrogates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alvin C. Grissom II",
      "Jo Shoemaker",
      "Benjamin Goldman",
      "Ruikang Shi",
      "Craig Stewart",
      "C. Anton Rytting",
      "Leah Findlater",
      "Jordan Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1186": {
    "title": "Rationale-based Learning Using Self-Supervised Narrative Events for Text Summarisation of Interactive Digital Narratives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashwathy T Revi",
      "Stuart E. Middleton",
      "David E. Millard"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1187": {
    "title": "Reading Does Not Equal Reading: Comparing, Simulating and Exploiting Reading Behavior across Populations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David R. Reich",
      "Shuwen Deng",
      "Marina Björnsdóttir",
      "Lena Jäger",
      "Nora Hollenstein"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1188": {
    "title": "ReadLet: A Dataset for Oral, Visual and Tactile Text Reading Data of Early and Mature Readers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcello Ferro",
      "Claudia Marzi",
      "Andrea Nadalini",
      "Loukia Taxitari",
      "Alessandro Lento",
      "Vito Pirrelli"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1189": {
    "title": "Reassessing Semantic Knowledge Encoded in Large Language Models through the Word-in-Context Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoshihiko Hayashi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1190": {
    "title": "Rebalancing Label Distribution While Eliminating Inherent Waiting Time in Multi Label Active Learning Applied to Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Arens",
      "Lucile Callebert",
      "Mohand Boughanem",
      "Jose G. Moreno"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1191": {
    "title": "ReCAP: Semantic Role Enhanced Caption Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhidip Bhattacharyya",
      "Martha Palmer",
      "Christoffer Heckman"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1192": {
    "title": "Recent Trends in Personalized Dialogue Generation: A Review of Datasets, Methodologies, and Evaluations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Pei Chen",
      "Noriki Nishida",
      "Hideki Nakayama",
      "Yuji Matsumoto"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1193": {
    "title": "RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieun Han",
      "Haneul Yoo",
      "Junho Myung",
      "Minsun Kim",
      "Tak Yeon Lee",
      "So-Yeon Ahn",
      "Alice Oh"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1194": {
    "title": "Recognizing Social Cues in Crisis Situations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Wang",
      "Yuan Zhuang",
      "Ellen Riloff",
      "Marina Kogan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1195": {
    "title": "Recognizing Value Resonance with Resonance-Tuned RoBERTa Task Definition, Experimental Validation, and Robust Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noam K. Benkler",
      "Scott Friedman",
      "Sonja Schmer-Galunder",
      "Drisana Marissa Mosaphir",
      "Robert P. Goldman",
      "Ruta Wheelock",
      "Vasanth Sarathy",
      "Pavan Kantharaju",
      "Matthew D. McLure"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1196": {
    "title": "Recommending Missed Citations Identified by Reviewers: A New Task, Dataset and Baselines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kehan Long",
      "Shasha Li",
      "Pancheng Wang",
      "Chenlong Bao",
      "Jintao Tang",
      "Ting Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1197": {
    "title": "Reconstruction of Cuneiform Literary Texts as Text Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabian Simonjetz",
      "Jussi Laasonen",
      "Yunus Cobanoglu",
      "Alexander Fraser",
      "Enrique Jiménez"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1198": {
    "title": "Reduce Redundancy Then Rerank: Enhancing Code Summarization with a Novel Pipeline Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Hu",
      "Xu Zhang",
      "Zexu Lin",
      "Deyu Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1199": {
    "title": "Re-evaluating the Tomes for the Times",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Brate",
      "Marieke van Erp",
      "Antal van den Bosch"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1200": {
    "title": "REFeREE: A REference-FREE Model-Based Metric for Text Simplification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Huang",
      "Ekaterina Kochmar"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1201": {
    "title": "Reference-guided Style-Consistent Content Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Fan Chen",
      "Milad Alshomary",
      "Maja Stahl",
      "Khalid Al Khatib",
      "Benno Stein",
      "Henning Wachsmuth"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1202": {
    "title": "Reference-less Analysis of Context Specificity in Translation with Personalised Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Vincent",
      "Rowanne Sumner",
      "Alice Dowek",
      "Charlotte Prescott",
      "Emily Preston",
      "Chris Bayliss",
      "Chris Oakley",
      "Carolina Scarton"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1203": {
    "title": "Refining Idioms Semantics Comprehension via Contrastive Learning and Cross-Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingmin Wu",
      "Guixin Su",
      "Yongcheng Zhang",
      "Zhongqiang Huang",
      "Ying Sha"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1204": {
    "title": "Refining rtMRI Landmark-Based Vocal Tract Contour Labels with FCN-Based Smoothing and Point-to-Curve Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mushaffa Rasyid Ridha",
      "Sakriani Sakti"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1205": {
    "title": "Reflecting the Male Gaze: Quantifying Female Objectification in 19th and 20th Century Novels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kexin Luo",
      "Yue Mao",
      "Bei Zhang",
      "Sophie Hao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1206": {
    "title": "Reflections & Resonance: Two-Agent Partnership for Advancing LLM-based Story Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuetian Chen",
      "Mei Si"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1207": {
    "title": "ReflectSumm: A Benchmark for Course Reflection Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Elaraby",
      "Yang Zhong",
      "Diane Litman",
      "Ahmed Ashraf Butt",
      "Muhsin Menekse"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1208": {
    "title": "Reimagining Intent Prediction: Insights from Graph-Based Dialogue Modeling and Sentence Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daria Romanovna Ledneva",
      "Denis Pavlovich Kuznetsov"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1209": {
    "title": "Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box LLM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Zhang",
      "Wei Gao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1210": {
    "title": "Related Work Is All You Need",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rodolfo Joel Zevallos",
      "John E. Ortega",
      "Benjamin Irving"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1211": {
    "title": "Relation between Cross-Genre and Cross-Topic Transfer in Dependency Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vera Danilova",
      "Sara Stymne"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1212": {
    "title": "Relation Classification via Bidirectional Prompt Learning with Data Augmentation by Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhi Jiang",
      "Jinlong Li",
      "Huanhuan Chen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1213": {
    "title": "Release of Pre-Trained Models for the Japanese Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kei Sawada",
      "Tianyu Zhao",
      "Makoto Shing",
      "Kentaro Mitsui",
      "Akio Kaga",
      "Yukiya Hono",
      "Toshiaki Wakatsuki",
      "Koh Mitsuda"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1214": {
    "title": "Releasing the Capacity of GANs in Non-Autoregressive Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Da Ren",
      "Qing Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1215": {
    "title": "RENN: A Rule Embedding Enhanced Neural Network Framework for Temporal Knowledge Graph Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linlin Zong",
      "Zhenrong Xie",
      "Chi Ma",
      "Xinyue Liu",
      "Xianchao Zhang",
      "Bo Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1216": {
    "title": "Replace, Paraphrase or Fine-tune? Evaluating Automatic Simplification for Medical Texts in Spanish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonardo Campillos-Llanos",
      "Ana Rosa Terroba",
      "Rocío Bartolomé",
      "Ana Valverde-Mateos",
      "Cristina González",
      "Adrián Capllonch-Carrión",
      "Jonathan Heras"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1217": {
    "title": "Representation Degeneration Problem in Prompt-based Models for Natural Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyan Zhao",
      "Ruifang He",
      "Jinpeng Zhang",
      "Chang Liu",
      "Bo Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1218": {
    "title": "Representing Compounding with OntoLex. An Evaluation of Vocabularies for Word Formation Resources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elena Benzoni",
      "Matteo Pellegrini",
      "Francesco Dedè",
      "Marco Passarotti"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1219": {
    "title": "Reranking Overgenerated Responses for End-to-End Task-Oriented Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songbo Hu",
      "Ivan Vulić",
      "Fangyu Liu",
      "Anna Korhonen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1220": {
    "title": "Resolving Legalese: A Multilingual Exploration of Negation Scope Resolution in Legal Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramona Christen",
      "Anastassia Shaitarova",
      "Matthias Stürmer",
      "Joel Niklaus"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1221": {
    "title": "Restoring Ancient Ideograph: A Multimodal Multitask Neural Network Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyu Duan",
      "Jun Wang",
      "Qi Su"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1222": {
    "title": "Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boxi Cao",
      "Qiaoyu Tang",
      "Hongyu Lin",
      "Shanshan Jiang",
      "Bin Dong",
      "Xianpei Han",
      "Jiawei Chen",
      "Tianshu Wang",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1223": {
    "title": "Rethinking Word-level Adversarial Attack: The Trade-off between Efficiency, Effectiveness, and Imperceptibility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengwei Zhan",
      "Jing Yang",
      "He Wang",
      "Chao Zheng",
      "Liming Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1224": {
    "title": "Retrieval-Augmented Modular Prompt Tuning for Low-Resource Data-to-Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruitao Feng",
      "Xudong Hong",
      "Mayank Jobanputra",
      "Mattes Warning",
      "Vera Demberg"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1225": {
    "title": "Retrieval-based Question Answering with Passage Expansion Using a Knowledge Graph",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benno Kruit",
      "Yiming Xu",
      "Jan-Christoph Kalo"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1226": {
    "title": "Revisiting Context Choices for Context-aware Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matiss Rikters",
      "Toshiaki Nakazawa"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1227": {
    "title": "Revisiting Data Reconstruction Attacks on Real-world Dataset for Federated Natural Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuo Zhang",
      "Jintao Huang",
      "Xiangjing Hu",
      "Jingyuan Zhang",
      "Yating Zhang",
      "Hui Wang",
      "Yue Yu",
      "Qifan Wang",
      "Lizhen Qu",
      "Zenglin Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1228": {
    "title": "Revisiting the Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Narayan Sankaran",
      "Vigneshwaran Shankaran",
      "Sampath Lonka",
      "Rajesh Sharma"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1229": {
    "title": "Revisiting the Self-Consistency Challenges in Multi-Choice Question Formats for Large Language Model Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Zhou",
      "Qiang Wang",
      "Mingzhou Xu",
      "Ming Chen",
      "Xiangyu Duan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1230": {
    "title": "Revisiting Three Text-to-Speech Synthesis Experiments with a Web-Based Audience Response System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christina Tånnander",
      "Jens Edlund",
      "Joakim Gustafson"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1231": {
    "title": "Rewiring the Transformer with Depth-Wise LSTMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongfei Xu",
      "Yang Song",
      "Qiuhui Liu",
      "Josef van Genabith",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1232": {
    "title": "RISE: Robust Early-exiting Internal Classifiers for Suicide Risk Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ritesh Singh Soun",
      "Atula Tejaswi Neerkaje",
      "Ramit Sawhney",
      "Nikolaos Aletras",
      "Preslav Nakov"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1233": {
    "title": "RoBERTa Low Resource Fine Tuning for Sentiment Analysis in Albanian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krenare Pireva Nuci",
      "Paul Landes",
      "Barbara Di Eugenio"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1234": {
    "title": "RoboVox: A Single/Multi-channel Far-field Speaker Recognition Benchmark for a Mobile Robot",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Mohammadamini",
      "Driss Matrouf",
      "Michael Rouvier",
      "Jean-Francois Bonastre",
      "Romain Serizel",
      "Theophile Gonos"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1235": {
    "title": "Robust and Scalable Model Editing for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingfa Chen",
      "Zhengyan Zhang",
      "Xu Han",
      "Chaojun Xiao",
      "Zhiyuan Liu",
      "Chen Chen",
      "Kuai Li",
      "Tao Yang",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1236": {
    "title": "RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Cosma",
      "Ioan-Bogdan Iordache",
      "Paolo Rosso"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1237": {
    "title": "RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuansen Zhang",
      "Xiao Wang",
      "Zhiheng Xi",
      "Han Xia",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1238": {
    "title": "RT-VQ2A2: Real Time Vector Quantized Question Answering with ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyungho Kim",
      "Seongmin Park",
      "Jihwa Lee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1239": {
    "title": "RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking on Russia-Ukraine Conflict",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yirong Zeng",
      "Xiao Ding",
      "Yi Zhao",
      "Xiangyu Li",
      "Jie Zhang",
      "Chao Yao",
      "Ting Liu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1240": {
    "title": "RuBia: A Russian Language Bias Detection Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Veronika Grigoreva",
      "Anastasiia Ivanova",
      "Ilseyar Alimova",
      "Ekaterina Artemova"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1241": {
    "title": "Russian Learner Corpus: Towards Error-Cause Annotation for L2 Russian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniil Kosakin",
      "Sergei Obiedkov",
      "Ivan Smirnov",
      "Ekaterina Rakhilina",
      "Anastasia Vyrenkova",
      "Ekaterina Zalivina"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1242": {
    "title": "S3Prompt: Instructing the Model with Self-calibration, Self-recall and Self-aggregation to Improve In-context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junda Chen",
      "Jianting Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1243": {
    "title": "SaGE: Evaluating Moral Consistency in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vamshi Krishna Bonagiri",
      "Sreeram Vennam",
      "Priyanshul Govil",
      "Ponnurangam Kumaraguru",
      "Manas Gaur"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1244": {
    "title": "Saliency-Aware Interpolative Augmentation for Multimodal Financial Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samyak Jain",
      "Parth Chhabra",
      "Atula Tejaswi Neerkaje",
      "Puneet Mathur",
      "Ramit Sawhney",
      "Shivam Agarwal",
      "Preslav Nakov",
      "Sudheer Chava",
      "Dinesh Manocha"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1245": {
    "title": "Samayik: A Benchmark and Dataset for English-Sanskrit Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayush Maheshwari",
      "Ashim Gupta",
      "Amrith Krishna",
      "Atul Kumar Singh",
      "Ganesh Ramakrishnan",
      "Anil Kumar Gourishetty",
      "Jitin Singla"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1246": {
    "title": "SamróMur MilljóN: An ASR Corpus of One Million Verified Read Prompts in Icelandic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos Daniel Hernandez Mena",
      "Þorsteinn Daði Gunnarsson",
      "Jon Gudnason"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1247": {
    "title": "Sarcasm Detection in a Disaster Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiberiu Sosea",
      "Junyi Jessy Li",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1248": {
    "title": "SarcNet: A Multilingual Multimodal Sarcasm Detection Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tan Yue",
      "Xuzhao Shi",
      "Rui Mao",
      "Zonghai Hu",
      "Erik Cambria"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1249": {
    "title": "Scalable Patent Classification with Aggregated Multi-View Ranking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Li",
      "Vikrant Yadav",
      "Zi Long Zhu",
      "Maziar Moradi Fard",
      "Zubair Afzal",
      "George Tsatsaronis"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1250": {
    "title": "Scale-VAE: Preventing Posterior Collapse in Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianbao Song",
      "Jingbo Sun",
      "Xin Liu",
      "Weiming Peng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1251": {
    "title": "Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feifan Song",
      "Bowen Yu",
      "Hao Lang",
      "Haiyang Yu",
      "Fei Huang",
      "Houfeng Wang",
      "Yongbin Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1252": {
    "title": "Scansion-based Lyrics Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwen Chen",
      "Simone Teufel"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1253": {
    "title": "Schema-based Data Augmentation for Event Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaomeng Jin",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1254": {
    "title": "Schema Learning Corpus: Data and Annotation Focused on Complex Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Chen",
      "Jennifer Tracey",
      "Ann Bies",
      "Stephanie Strassel"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1255": {
    "title": "Schroedinger's Threshold: When the AUC Doesn't Predict Accuracy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juri Opitz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1256": {
    "title": "SciDMT: A Large-Scale Corpus for Detecting Scientific Mentions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huitong Pan",
      "Qi Zhang",
      "Cornelia Caragea",
      "Eduard Dragut",
      "Longin Jan Latecki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1257": {
    "title": "SciMRC: Multi-perspective Scientific Machine Reading Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Zhang",
      "Heqi Zheng",
      "Yuxiang Nie",
      "Heyan Huang",
      "Xian-Ling Mao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1258": {
    "title": "SciNews: From Scholarly Complexities to Public Narratives – a Dataset for Scientific News Report Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongqi Pu",
      "Yifan Wang",
      "Jia E. Loy",
      "Vera Demberg"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1259": {
    "title": "SCOUT: A Situated and Multi-Modal Human-Robot Dialogue Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephanie M. Lukin",
      "Claire Bonial",
      "Matthew Marge",
      "Taylor A. Hudson",
      "Cory J. Hayes",
      "Kimberly Pollard",
      "Anthony Baker",
      "Ashley N. Foots",
      "Ron Artstein",
      "Felix Gervits",
      "Mitchell Abrams",
      "Cassidy Henry",
      "Lucia Donatelli",
      "Anton Leuski",
      "Susan G. Hill",
      "David Traum",
      "Clare Voss"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1260": {
    "title": "SDA: Simple Discrete Augmentation for Contrastive Sentence Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongsheng Zhu",
      "Zhenyu Mao",
      "Jinghui Lu",
      "Rui Zhao",
      "Fei Tan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1261": {
    "title": "Searching by Code: A New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivan Sedykh",
      "Nikita Sorokin",
      "Dmitry Abulkhanov",
      "Sergey I. Nikolenko",
      "Valentin Malykh"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1262": {
    "title": "Sebastian, Basti, Wastl?! Recognizing Named Entities in Bavarian Dialectal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyao Peng",
      "Zihang Sun",
      "Huangyan Shan",
      "Marie Kolm",
      "Verena Blaschke",
      "Ekaterina Artemova",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1263": {
    "title": "Seeing Eye-to-Eye: Cross-Modal Coherence Relations Inform Eye-gaze Patterns During Comprehension & Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mert Inan",
      "Malihe Alikhani"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1264": {
    "title": "Seeing Is Believing! towards Knowledge-Infused Multi-modal Medical Dialogue Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhisek Tiwari",
      "Shreyangshu Bera",
      "Preeti Verma",
      "Jaithra Varma Manthena",
      "Sriparna Saha",
      "Pushpak Bhattacharyya",
      "Minakshi Dhar",
      "Sarbajeet Tiwari"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1265": {
    "title": "Segmentation of Complex Question Turns for Argument Mining: A Corpus-based Study in the Financial Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giulia D’Agostino",
      "Chris A. Reed",
      "Daniele Puccinelli"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1266": {
    "title": "Select and Reorder: A Novel Approach for Neural Sign Language Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harry Walsh",
      "Ben Saunders",
      "Richard Bowden"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1267": {
    "title": "Select High-quality Synthetic QA Pairs to Augment Training Data in MRC under the Reward Guidance of Generative Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Jin",
      "Houfeng Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1268": {
    "title": "Selective Temporal Knowledge Graph Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongni Hou",
      "Xiaolong Jin",
      "Zixuan Li",
      "Long Bai",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1269": {
    "title": "Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Gao",
      "Ting-En Lin",
      "Hangyu Li",
      "Min Yang",
      "Yuchuan Wu",
      "Wentao Ma",
      "Fei Huang",
      "Yongbin Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1270": {
    "title": "Self-Improvement Programming for Temporal Knowledge Graph Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuo Chen",
      "Zhao Zhang",
      "Zixuan Li",
      "Fei Wang",
      "Yutao Zeng",
      "Xiaolong Jin",
      "Yongjun Xu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1271": {
    "title": "Self-Knowledge Distillation for Knowledge Graph Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Xu",
      "Yuhua Wang",
      "Jiahui Fan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1272": {
    "title": "Self-reported Demographics and Discourse Dynamics in a Persuasive Online Forum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Agnieszka Falenska",
      "Eva Maria Vecchi",
      "Gabriella Lapesa"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1273": {
    "title": "Semantic Frame Extraction in Multilingual Olfactory Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Menini"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1274": {
    "title": "Semantic Map-based Generation of Navigation Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengzu Li",
      "Chao Zhang",
      "Simone Teufel",
      "Rama Sanand Doddipatla",
      "Svetlana Stoyanchev"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1275": {
    "title": "Semantic Role Labeling Guided Out-of-distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinan Zou",
      "Maihao Guo",
      "Yu Tian",
      "Yuhao Lin",
      "Haiyao Cao",
      "Lingqiao Liu",
      "Ehsan Abbasnejad",
      "Javen Qinfeng Shi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1276": {
    "title": "Semantics-Aware Dual Graph Convolutional Networks for Argument Pair Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minzhao Guan",
      "Zhixun Qiu",
      "Fenghuan Li",
      "Yun Xue"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1277": {
    "title": "Semantics-enhanced Cross-modal Masked Image Modeling for Vision-Language Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haowei Liu",
      "Yaya Shi",
      "Haiyang Xu",
      "Chunfeng Yuan",
      "Qinghao Ye",
      "Chenliang Li",
      "Ming Yan",
      "Ji Zhang",
      "Fei Huang",
      "Bing Li",
      "Weiming Hu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1278": {
    "title": "Sense of the Day: Short Timeframe Temporal-Aware Word Sense Disambiguation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Wei",
      "Milton King"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1279": {
    "title": "SENTA: Sentence Simplification System for Slovene",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleš Žagar",
      "Matej Klemen",
      "Marko Robnik-Šikonja",
      "Iztok Kosem"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1280": {
    "title": "SentiCSE: A Sentiment-aware Contrastive Sentence Embedding Framework with Sentiment-guided Textual Similarity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaemin Kim",
      "Yohan Na",
      "Kangmin Kim",
      "Sang-Rak Lee",
      "Dong-Kyu Chae"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1281": {
    "title": "Sequence Reducible Holdout Loss for Language Model Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raghuveer Thirukovalluru",
      "Nicholas Monath",
      "Bhuwan Dhingra",
      "Sam Wiseman"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1282": {
    "title": "Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gustave Cortal"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1283": {
    "title": "Sequence-to-Sequence Spanish Pre-trained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladimir Araujo",
      "Maria Mihaela Trusca",
      "Rodrigo Tufiño",
      "Marie-Francine Moens"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1284": {
    "title": "Sequential and Repetitive Pattern Learning for Temporal Knowledge Graph Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuefei Li",
      "Huiwei Zhou",
      "Weihong Yao",
      "Wenchu Li",
      "Yingyu Lin",
      "Lei Du"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1285": {
    "title": "SGCM: Salience-Guided Context Modeling for Question Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuyao Ding",
      "Yu Hong",
      "Jianmin Yao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1286": {
    "title": "ShadowSense: A Multi-annotated Dataset for Evaluating Word Sense Induction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ondřej Herman",
      "Miloš Jakubíček"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1287": {
    "title": "Sharing the Cost of Success: A Game for Evaluating and Learning Collaborative Multi-Agent Instruction Giving and Following Policies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Sadler",
      "Sherzod Hakimov",
      "David Schlangen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1288": {
    "title": "SIGA: A Naturalistic NLI Dataset of English Scalar Implicatures with Gradable Adjectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rashid Nizamani",
      "Sebastian Schuster",
      "Vera Demberg"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1289": {
    "title": "SignBLEU: Automatic Evaluation of Multi-channel Sign Language Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jung-Ho Kim",
      "Mathew John Huerta-Enochian",
      "Changyong Ko",
      "Du Hui Lee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1290": {
    "title": "SilverAlign: MT-Based Silver Data Algorithm for Evaluating Word Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdullatif Koksal",
      "Silvia Severini",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1291": {
    "title": "Silver Retriever: Advancing Neural Passage Retrieval for Polish Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piotr Rybak",
      "Maciej Ogrodniczuk"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1292": {
    "title": "SimLex-999 for Dutch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lizzy Brans",
      "Jelke Bloem"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1293": {
    "title": "Sinkhorn Distance Minimization for Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Cui",
      "Yulei Qin",
      "Yuting Gao",
      "Enwei Zhang",
      "Zihan Xu",
      "Tong Wu",
      "Ke Li",
      "Xing Sun",
      "Wengang Zhou",
      "Houqiang Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1294": {
    "title": "SI-NLI: A Slovene Natural Language Inference Dataset and Its Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matej Klemen",
      "Aleš Žagar",
      "Jaka Čibej",
      "Marko Robnik-Šikonja"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1295": {
    "title": "SkOTaPA: A Dataset for Skepticism Detection in Online Text after Persuasion Attempt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Smitha Muthya Sudheendra",
      "Maral Abdollahi",
      "Dongyeop Kang",
      "Jisu Huh",
      "Jaideep Srivastava"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1296": {
    "title": "SLaCAD: A Spoken Language Corpus for Early Alzheimer's Disease Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shahla Farzana",
      "Edoardo Stoppa",
      "Alex Leow",
      "Tamar Gollan",
      "Raeanne Moore",
      "David Salmon",
      "Douglas Galasko",
      "Erin Sundermann",
      "Natalie Parde"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1297": {
    "title": "Slot and Intent Detection Resources for Bavarian and Lithuanian: Assessing Translations vs Natural Queries to Digital Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miriam Winkler",
      "Virginija Juozapaityte",
      "Rob van der Goot",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1298": {
    "title": "SlovakSum: A Large Scale Slovak Summarization Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viktoria Ondrejova",
      "Marek Suppa"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1299": {
    "title": "Small Language Models Are Good Too: An Empirical Study of Zero-Shot Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Lepagnol",
      "Thomas Gerald",
      "Sahar Ghannay",
      "Christophe Servan",
      "Sophie Rosset"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1300": {
    "title": "SmartTrim: Adaptive Tokens and Attention Pruning for Efficient Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekun Wang",
      "Jingchang Chen",
      "Wangchunshu Zhou",
      "Haichao Zhu",
      "Jiafeng Liang",
      "Liping Shan",
      "Ming Liu",
      "Dongliang Xu",
      "Qing Yang",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1301": {
    "title": "SM-FEEL-BG - the First Bulgarian Datasets and Classifiers for Detecting Feelings, Emotions, and Sentiments of Bulgarian Social Media Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Irina Temnikova",
      "Iva Marinova",
      "Silvia Gargova",
      "Ruslana Margova",
      "Alexander Komarov",
      "Tsvetelina Stefanova",
      "Veneta Kireva",
      "Dimana Vyatrova",
      "Nevena Grigorova",
      "Yordan Mandevski",
      "Stefan Minkov"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1302": {
    "title": "SOBR: A Corpus for Stylometry, Obfuscation, and Bias on Reddit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Emmery",
      "Marilù Miotto",
      "Sergey Kramp",
      "Bennett Kleinberg"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1303": {
    "title": "Social Convos: Capturing Agendas and Emotions on Social Media",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankita Bhaumik",
      "Ning Sa",
      "Gregorios Katsios",
      "Tomek Strzalkowski"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1304": {
    "title": "Social Orientation: A New Feature for Dialogue Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Todd Morrill",
      "Zhaoyuan Deng",
      "Yanda Chen",
      "Amith Ananthram",
      "Colin Wayne Leach",
      "Kathleen McKeown"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1305": {
    "title": "SoftMCL: Soft Momentum Contrastive Learning for Fine-grained Sentiment-aware Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Wang",
      "Liang-Chih Yu",
      "Xuejie Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1306": {
    "title": "Soft-Prompting with Graph-of-Thought for Multi-modal Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Cheng Yang",
      "Zuchao Li",
      "Shuai Xie",
      "Wei Yu",
      "Shijun Li",
      "Bo Du"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1307": {
    "title": "Soft Well-Formed Semantic Parsing with Score-Based Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangming Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1308": {
    "title": "So Hateful! Building a Multi-Label Hate Speech Annotated Arabic Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wajdi Zaghouani",
      "Hamdy Mubarak",
      "Md. Rafiul Biswas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1309": {
    "title": "Sonos Voice Control Bias Assessment Dataset: A Methodology for Demographic Bias Assessment in Voice Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chloe Sekkat",
      "Fanny Leroy",
      "Salima Mdhaffar",
      "Blake Perry Smith",
      "Yannick Estève",
      "Joseph Dureau",
      "Alice Coucke"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1310": {
    "title": "Source-free Domain Adaptation for Aspect-based Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zishuo Zhao",
      "Ziyang Ma",
      "Zhenzhou Lin",
      "Jingyou Xie",
      "Yinghui Li",
      "Ying Shen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1311": {
    "title": "SPACE-IDEAS: A Dataset for Salient Information Detection in Space Innovation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andres Garcia-Silva",
      "Cristian Berrio",
      "Jose Manuel Gomez-Perez"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1312": {
    "title": "Spanish Resource Grammar Version 2023",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olga Zamaraeva",
      "Lorena S. Allegue",
      "Carlos Gómez-Rodríguez"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1313": {
    "title": "Spanless Event Annotation for Corpus-Wide Complex Event Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ann Bies",
      "Jennifer Tracey",
      "Ann O’Brien",
      "Song Chen",
      "Stephanie Strassel"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1314": {
    "title": "Sparse Logistic Regression with High-order Features for Automatic Grammar Rule Extraction from Treebanks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Santiago Herrera",
      "Caio Corro",
      "Sylvain Kahane"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1315": {
    "title": "Specifying Genericity through Inclusiveness and Abstractness Continuous Scales",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Claudia Collacciani",
      "Andrea Amelio Ravelli",
      "Marianna Bolognesi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1316": {
    "title": "SpeechAlign: A Framework for Speech Translation Alignment Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Belen Alastruey",
      "Aleix Sant",
      "Gerard I. Gállego",
      "David Dale",
      "Marta R. Costa-jussà"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1317": {
    "title": "Speech Analysis of Language Varieties in Italy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moreno La Quatra",
      "Alkis Koudounas",
      "Elena Baralis",
      "Sabato Marco Siniscalchi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1318": {
    "title": "Speech Corpus for Korean Children with Autism Spectrum Disorder: Towards Automatic Assessment Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonwoo Lee",
      "Jihyun Mun",
      "Sunhee Kim",
      "Minhwa Chung"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1319": {
    "title": "Speech Recognition Corpus of the Khinalug Language for Documenting Endangered Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaolin Li",
      "Monika Rind-Pawlowski",
      "Jan Niehues"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1320": {
    "title": "SPICED: News Similarity Detection Dataset with Multiple Topics and Complexity Levels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elena Shushkevich",
      "Long Thanh Mai",
      "Manuel V. Loureiro",
      "Steven Derby",
      "Tri Kurniawan Wijaya"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1321": {
    "title": "SPLICE: A Singleton-Enhanced PipeLIne for Coreference REsolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Zhu",
      "Siyao Peng",
      "Sameer Pradhan",
      "Amir Zeldes"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1322": {
    "title": "SPOTTER: A Framework for Investigating Convention Formation in a Visually Grounded Human-Robot Reference Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaap Kruijt",
      "Peggy van Minkelen",
      "Lucia Donatelli",
      "Piek T.J.M. Vossen",
      "Elly Konijn",
      "Thomas Baier"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1323": {
    "title": "SpreadNaLa: A Naturalistic Code Generation Evaluation Dataset of Spreadsheet Formulas",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Schuster",
      "Ayesha Ansar",
      "Om Agarwal",
      "Vera Demberg"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1324": {
    "title": "STAF: Pushing the Boundaries of Test-Time Adaptation towards Practical Noise Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Xiong",
      "Xinchun Zhang",
      "Leixin Yang",
      "Yu Xiang",
      "Gang Fang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1325": {
    "title": "STAGE: Simple Text Data Augmentation by Graph Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ho-Seung Kim",
      "YongHoon Kang",
      "Jee-Hyong Lee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1326": {
    "title": "Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maksym Taranukhin",
      "Vered Shwartz",
      "Evangelos Milios"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1327": {
    "title": "STEntConv: Predicting Disagreement between Reddit Users with Stance Detection and a Signed Graph Convolutional Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isabelle Lorge",
      "Li Zhang",
      "Xiaowen Dong",
      "Janet Pierrehumbert"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1328": {
    "title": "Step-by-Step: Controlling Arbitrary Style in Text with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pusheng Liu",
      "Lianwei Wu",
      "Linyong Wang",
      "Sensen Guo",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1329": {
    "title": "Step Feasibility-Aware and Error-Correctable Entailment Tree Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyue Song",
      "Xin Wu",
      "Yi Cai"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1330": {
    "title": "Still All Greeklish to Me: Greeklish to Greek Transliteration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anastasios Toumazatos",
      "John Pavlopoulos",
      "Ion Androutsopoulos",
      "Stavros Vassos"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1331": {
    "title": "Stories and Personal Experiences in the COVID-19 Discourse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neele Falk",
      "Gabriella Lapesa"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1332": {
    "title": "Strengthening the WiC: New Polysemy Dataset in Hindi and Lack of Cross Lingual Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haim Dubossarsky",
      "Farheen Dairkee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1333": {
    "title": "StructAM: Enhancing Address Matching through Semantic Understanding of Structure-aware Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoqi Zhang",
      "Pasquale Balsebre",
      "Siqiang Luo",
      "Zhen Hai",
      "Jiangping Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1334": {
    "title": "Structure-aware Fine-tuning for Code Pre-trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Wu",
      "Renyu Zhu",
      "Nuo Chen",
      "Qiushi Sun",
      "Xiang Li",
      "Ming Gao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1335": {
    "title": "Structure-aware Generation Model for Cross-Domain Aspect-based Sentiment Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shichen Li",
      "Zhongqing Wang",
      "Yanzhi Xu",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1336": {
    "title": "StyleFlow: Disentangle Latent Representations via Normalizing Flow for Unsupervised Text Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangchen Zhu",
      "Zhiliang Tian",
      "Jingyu Wei",
      "Ruifeng Luo",
      "Yiping Song",
      "Xiaoguang Mao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1337": {
    "title": "Submodular-based In-context Example Selection for LLMs-based Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baijun Ji",
      "Xiangyu Duan",
      "Zhenyu Qiu",
      "Tong Zhang",
      "Junhui Li",
      "Hao Yang",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1338": {
    "title": "Subspace Defense: Discarding Adversarial Perturbations by Learning a Subspace for Clean Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Zheng",
      "Yuhao Zhou",
      "Zhiheng Xi",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1339": {
    "title": "Sub-Table Rescorer for Table Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atsushi Kojima"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1340": {
    "title": "SUK 1.0: A New Training Corpus for Linguistic Annotation of Modern Standard Slovene",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Špela Arhar Holdt",
      "Jaka Čibej",
      "Kaja Dobrovoljc",
      "Tomaž Erjavec",
      "Polona Gantar",
      "Simon Krek",
      "Tina Munda",
      "Nejc Robida",
      "Luka Terčon",
      "Slavko Zitnik"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1341": {
    "title": "SuperST: Superficial Self-Training for Few-Shot Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ju-Hyoung Lee",
      "Joonghyuk Hahn",
      "Hyeon-Tae Seo",
      "Jiho Park",
      "Yo-Sub Han"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1342": {
    "title": "SwissSLi: The Multi-parallel Sign Language Corpus for Switzerland",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zifan Jiang",
      "Anne Göhring",
      "Amit Moryossef",
      "Rico Sennrich",
      "Sarah Ebling"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1343": {
    "title": "Synergetic Interaction Network with Cross-task Attention for Joint Relational Triple Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Da Luo",
      "Run Lin",
      "Qiao Liu",
      "Yuxiang Cai",
      "Xueyi Liu",
      "Yanglei Gan",
      "Rui Hou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1344": {
    "title": "SynPrompt: Syntax-aware Enhanced Prompt Engineering for Aspect-based Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Yin",
      "Cencen Liu",
      "Yi Xu",
      "Ahmad Raza Wahla",
      "Huang Yiting",
      "Dezhang Zheng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1345": {
    "title": "Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartik Kartik",
      "Sanjana Soni",
      "Anoop Kunchukuttan",
      "Tanmoy Chakraborty",
      "Md. Shad Akhtar"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1346": {
    "title": "SynTOD: Augmented Response Synthesis for Robust End-to-End Task-Oriented Dialogue System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nguyen Quang Chieu",
      "Quang-Minh Tran",
      "Khac-Hoai Nam Bui"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1347": {
    "title": "Tackling Long Code Search with Splitting, Encoding, and Aggregating",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Hu",
      "Yanlin Wang",
      "Lun Du",
      "Hongyu Zhang",
      "Dongmei Zhang",
      "Xirong Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1348": {
    "title": "TacoERE: Cluster-aware Compression for Event Relation Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yong Guan",
      "Xiaozhi Wang",
      "Lei Hou",
      "Juanzi Li",
      "Jeff Z. Pan",
      "Jiaoyan Chen",
      "Freddy Lecue"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1349": {
    "title": "TACO – Twitter Arguments from COnversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Feger",
      "Stefan Dietze"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1350": {
    "title": "TAeKD: Teacher Assistant Enhanced Knowledge Distillation for Closed-Source Multilingual Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Lv",
      "Xin Liu",
      "Kaiwen Wei",
      "Ping Luo",
      "Yue Yu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1351": {
    "title": "TaiChi: Improving the Robustness of NLP Models by Seeking Common Ground While Reserving Differences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huimin Chen",
      "Chengyu Wang",
      "Yanhao Wang",
      "Cen Chen",
      "Yinggui Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1352": {
    "title": "Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Xu",
      "Keqin Peng",
      "Liang Ding",
      "Dacheng Tao",
      "Xiliang Lu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1353": {
    "title": "Take Its Essence, Discard Its Dross! Debiasing for Toxic Language Detection via Counterfactual Causal Effect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Lu",
      "Bo Xu",
      "Xiaokun Zhang",
      "Kaiyuan Liu",
      "Dongyu Zhang",
      "Liang Yang",
      "Hongfei Lin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1354": {
    "title": "TAPASGO: Transfer Learning towards a German-Language Tabular Question Answering Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Andreas Kowieski",
      "Michael Hellwig",
      "Thomas Feilhauer"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1355": {
    "title": "Target-Adaptive Consistency Enhanced Prompt-Tuning for Multi-Domain Stance Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangkang Wang",
      "Li Pan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1356": {
    "title": "Targeted Syntactic Evaluation on the Chomsky Hierarchy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taiga Someya",
      "Ryo Yoshida",
      "Yohei Oseki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1357": {
    "title": "TARIC-SLU: A Tunisian Benchmark Dataset for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Salima Mdhaffar",
      "Fethi Bougares",
      "Renato de Mori",
      "Salah Zaiem",
      "Mirco Ravanelli",
      "Yannick Estève"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1358": {
    "title": "TARN-VIST: Topic Aware Reinforcement Network for Visual Storytelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiran Chen",
      "Xin Li",
      "Jiaqi Su",
      "Guiqian Zhu",
      "Ying Li",
      "Yi Ji",
      "Chunping Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1359": {
    "title": "Task-agnostic Distillation of Encoder-Decoder Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Zhang",
      "Yang Yang",
      "Qiuchi Li",
      "Jingang Wang",
      "Dawei Song"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1360": {
    "title": "Task-Oriented Paraphrase Analytics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcel Gohsen",
      "Matthias Hagen",
      "Martin Potthast",
      "Benno Stein"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1361": {
    "title": "tasksource: A Large Collection of NLP tasks with a Structured Dataset Preprocessing Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damien Sileo"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1362": {
    "title": "Teaching Large Language Models to Translate on Low-resource Languages with Textbook Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ping Guo",
      "Yubing Ren",
      "Yue Hu",
      "Yunpeng Li",
      "Jiarui Zhang",
      "Xingsheng Zhang",
      "Heyan Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1363": {
    "title": "TECA: A Two-stage Approach with Controllable Attention Soft Prompt for Few-shot Nested Named Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyuan Xu",
      "Linhai Zhang",
      "Deyu Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1364": {
    "title": "TeClass: A Human-Annotated Relevance-based Headline Classification and Generation Dataset for Telugu",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gopichand Kanumolu",
      "Lokesh Madasu",
      "Nirmal Surange",
      "Manish Shrivastava"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1365": {
    "title": "TED-EL: A Corpus for Speech Entity Linking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Silin Li",
      "Ruoyu Song",
      "Tianwei Lan",
      "Zeming Liu",
      "Yuhang Guo"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1366": {
    "title": "Tell Me Again! a Large-Scale Dataset of Multiple Summaries for the Same Story",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hans Ole Hatzel",
      "Chris Biemann"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1367": {
    "title": "Temporal Knowledge Graph Reasoning with Dynamic Hypergraph Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyue Liu",
      "Jianan Zhang",
      "Chi Ma",
      "Wenxin Liang",
      "Bo Xu",
      "Linlin Zong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1368": {
    "title": "Term-Driven Forward-Looking Claim Synthesis in Earnings Calls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chung-Chi Chen",
      "Hiroya Takamura"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1369": {
    "title": "text2story: A Python Toolkit to Extract and Visualize Story Components of Narrative Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evelin Amorim",
      "Ricardo Campos",
      "Alipio Jorge",
      "Pedro Mota",
      "Rúben Almeida"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1370": {
    "title": "Text2Story Lusa: A Dataset for Narrative Analysis in European Portuguese News Articles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sérgio Nunes",
      "Alípio Mario Jorge",
      "Evelin Amorim",
      "Hugo Sousa",
      "António Leal",
      "Purificação Moura Silvano",
      "Inês Cantante",
      "Ricardo Campos"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1371": {
    "title": "Text360Nav: 360-Degree Image Captioning Dataset for Urban Pedestrians Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chieko Nishimura",
      "Shuhei Kurita",
      "Yohei Seki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1372": {
    "title": "Text Filtering Classifiers for Medium-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jón Daðason",
      "Hrafn Loftsson"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1373": {
    "title": "Text Style Transfer Evaluation Using Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phil Sidney Ostheimer",
      "Mayank Kumar Nagda",
      "Marius Kloft",
      "Sophie Fellenz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1374": {
    "title": "Text-to-Multimodal Retrieval with Bimodal Input Fusion in Shared Cross-Modal Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranav Arora",
      "Selen Pehlivan",
      "Jorma Laaksonen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1375": {
    "title": "Textual Coverage of Eventive Entries in Lexical Semantic Resources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eva Fučíková",
      "Cristina Fernández Alcaina",
      "Jan Hajič",
      "Zdeňka Urešová"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1376": {
    "title": "The Challenges of Creating a Parallel Multilingual Hate Speech Corpus: An Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katerina Korre",
      "Arianna Muti",
      "Alberto Barrón-Cedeño"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1377": {
    "title": "The Contextual Variability of English Nouns: The Impact of Categorical Specificity beyond Conceptual Concreteness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giulia Rambelli",
      "Marianna Bolognesi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1378": {
    "title": "The Corpus AIKIA: Using Ranking Annotation for Offensive Language Detection in Modern Greek",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stella Markantonatou",
      "Vivian Stamou",
      "Christina Christodoulou",
      "Georgia Apostolopoulou",
      "Antonis Balas",
      "George Ioannakis"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1379": {
    "title": "The Distracted Ear: How Listeners Shape Conversational Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Auriane Boudin",
      "Stéphane Rauzy",
      "Roxane Bertrand",
      "Magalie Ochs",
      "Philippe Blache"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1380": {
    "title": "The Effects of Pretraining in Video-Guided Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ammon Shurtz",
      "Lawry Sorenson",
      "Stephen D. Richardson"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1381": {
    "title": "The ELCo Dataset: Bridging Emoji and Lexical Composition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi Yun Yang",
      "Ziqing Zhang",
      "Yisong Miao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1382": {
    "title": "The Emergence of Semantic Units in Massively Multilingual Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Gregor de Varda",
      "Marco Marelli"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1383": {
    "title": "The Ethical Question – Use of Indigenous Corpora for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linda Wiechetek",
      "Flammie A. Pirinen",
      "Børre Gaup",
      "Trond Trosterud",
      "Maja Lisa Kappfjell",
      "Sjur Moshagen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1384": {
    "title": "The IgboAPI Dataset: Empowering Igbo Language Technologies through Multi-dialectal Enrichment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Chinenye Emezue",
      "Ifeoma Okoh",
      "Chinedu Emmanuel Mbonu",
      "Chiamaka Chukwuneke",
      "Daisy Monika Lal",
      "Ignatius Ezeani",
      "Paul Rayson",
      "Ijemma Onwuzulike",
      "Chukwuma Onyebuchi Okeke",
      "Gerald Okey Nweya",
      "Bright Ikechukwu Ogbonna",
      "Chukwuebuka Uchenna Oraegbunam",
      "Esther Chidinma Awo-Ndubuisi",
      "Akudo Amarachukwu Osuagwu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1385": {
    "title": "The Impact of Stance Object Type on the Quality of Stance Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxwell A. Weinzierl",
      "Sanda M. Harabagiu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1386": {
    "title": "The Influence of Automatic Speech Recognition on Linguistic Features and Automatic Alzheimer's Disease Detection from Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Heitz",
      "Gerold Schneider",
      "Nicolas Langer"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1387": {
    "title": "The Key Points: Using Feature Importance to Identify Shortcomings in Sign Language Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruth M. Holmes",
      "Ellen Rushe",
      "Anthony Ventresque"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1388": {
    "title": "The Low Saxon LSDC Dataset at Universal Dependencies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Janine Siewert",
      "Jack Rueter"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1389": {
    "title": "The Onomastic Repertoire of the Roman d'Alexandre (ORNARE). Designing an Integrated Digital Onomastic Tool for Medieval French Romance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marta Milazzo",
      "Giorgio Maria Di Nunzio"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1390": {
    "title": "The Open-World Lottery Ticket Hypothesis for OOD Intent Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhua Zhou",
      "Pengyu Wang",
      "Peiju Liu",
      "Yuxin Wang",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1391": {
    "title": "Theoretical and Empirical Advantages of Dense-Vector to One-Hot Encoding of Intent Classes in Open-World Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paulo Cavalin",
      "Claudio Santos Pinhanez"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1392": {
    "title": "The ParCoLab Parallel Corpus and Its Extension to Four Regional Languages of France",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dejan Stosic",
      "Saša Marjanović",
      "Delphine Bernhard",
      "Myriam Bras",
      "Laurent Kevers",
      "Stella Retali-Medori",
      "Marianne Vergez-Couret",
      "Carole Werner"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1393": {
    "title": "The ParlaSent Multilingual Training Dataset for Sentiment Identification in Parliamentary Proceedings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Mochtak",
      "Peter Rupnik",
      "Nikola Ljubešić"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1394": {
    "title": "There's Something New about the Italian Parliament: The IPSA Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valentino Frasnelli",
      "Alessio Palmero Aprosio"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1395": {
    "title": "The RIP Corpus of Collaborative Hypothesis-Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ella Schad",
      "Jacky Visser",
      "Chris Reed"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1396": {
    "title": "The Role of Creaky Voice in Turn Taking and the Perception of Speaker Stance: Experiments Using Controllable TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harm Lameris",
      "Eva Szekely",
      "Joakim Gustafson"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1397": {
    "title": "The Role of Syntactic Span Preferences in Post-Hoc Explanation Disagreement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Kamp",
      "Lisa Beinborn",
      "Antske Fokkens"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1398": {
    "title": "The SAMER Arabic Text Simplification Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bashar Alhafni",
      "Reem Hazim",
      "Juan David Pineros Liberato",
      "Muhamed Al Khalil",
      "Nizar Habash"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1399": {
    "title": "The Slovak Autistic and Non-Autistic Child Speech Corpus:Task-Oriented Child-Adult Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joanna Kruyt",
      "Róbert Sabo",
      "Katarína Polónyiová",
      "Daniela Ostatníková",
      "Štefan Beňuš"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1400": {
    "title": "The Swedish Parliament Corpus 1867 – 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Väinö Aleksi Yrjänäinen",
      "Fredrik Mohammadi Norén",
      "Robert Borges",
      "Johan Jarlbrink",
      "Lotta Åberg Brorsson",
      "Anders P. Olsson",
      "Pelle Snickars",
      "Måns Magnusson"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1401": {
    "title": "The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom S Juzek"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1402": {
    "title": "The Touché23-ValueEval Dataset for Identifying Human Values behind Arguments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nailia Mirzakhmedova",
      "Johannes Kiesel",
      "Milad Alshomary",
      "Maximilian Heinrich",
      "Nicolas Handke",
      "Xiaoni Cai",
      "Valentin Barriere",
      "Doratossadat Dastgheib",
      "Omid Ghahroodi",
      "MohammadAli SadraeiJavaheri",
      "Ehsaneddin Asgari",
      "Lea Kawaletz",
      "Henning Wachsmuth",
      "Benno Stein"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1403": {
    "title": "TIGER: A Unified Generative Model Framework for Multimodal Dialogue Response Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanheng Kong",
      "Peidong Wang",
      "Shi Feng",
      "Daling Wang",
      "Yifei Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1404": {
    "title": "TIGQA: An Expert-Annotated Question-Answering Dataset in Tigrinya",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hailay Kidu Teklehaymanot",
      "Dren Fazlija",
      "Niloy Ganguly",
      "Gourab Kumar Patro",
      "Wolfgang Nejdl"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1405": {
    "title": "Time-aware COMET: A Commonsense Knowledge Model with Temporal Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eiki Murata",
      "Daisuke Kawahara"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1406": {
    "title": "Title-based Extractive Summarization via MRC Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjin Kim",
      "Jai-Eun Kim",
      "Harksoo Kim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1407": {
    "title": "TMFN: A Target-oriented Multi-grained Fusion Network for End-to-end Aspect-based Multimodal Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Wang",
      "Yuzheng He",
      "Xiao Liang",
      "Yumin Tian",
      "Shaofeng Li",
      "Lin Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1408": {
    "title": "To Drop or Not to Drop? Predicting Argument Ellipsis Judgments: A Case Study in Japanese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukiko Ishizuki",
      "Tatsuki Kuribayashi",
      "Yuichiroh Matsubayashi",
      "Ryohei Sasano",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1409": {
    "title": "To Err Is Human, How about Medical Large Language Models? Comparing Pre-trained Language Models for Medical Assessment Errors and Reliability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen-wai Yim",
      "Yujuan Fu",
      "Asma Ben Abacha",
      "Meliha Yetisgen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1410": {
    "title": "Token-length Bias in Minimal-pair Paradigm Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naoya Ueda",
      "Masato Mita",
      "Teruaki Oka",
      "Mamoru Komachi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1411": {
    "title": "To Learn or Not to Learn: Replaced Token Detection for Learning the Meaning of Negation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gunjan Bhattarai",
      "Katrin Erk"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1412": {
    "title": "ToNER: Type-oriented Named Entity Recognition with Generative Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guochao Jiang",
      "Ziqin Luo",
      "Yuchen Shi",
      "Dixuan Wang",
      "Jiaqing Liang",
      "Deqing Yang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1413": {
    "title": "ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhang Zheng",
      "Peng Li",
      "Wei Liu",
      "Yang Liu",
      "Jian Luan",
      "Bin Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1414": {
    "title": "Topic Classification and Headline Generation for Maltese Using a Public News Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amit Kumar Chaudhary",
      "Kurt Micallef",
      "Claudia Borg"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1415": {
    "title": "Topic-Controllable Summarization: Topic-Aware Evaluation and Transformer Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatiana Passali",
      "Grigorios Tsoumakas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1416": {
    "title": "Topic Detection and Tracking with Time-Aware Document Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Jiang",
      "Doug Beeferman",
      "Weiquan Mao",
      "Deb Roy"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1417": {
    "title": "TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiamin Luo",
      "Jingjing Wang",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1418": {
    "title": "Topics as Entity Clusters: Entity-based Topics from Large Language Models and Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel V. Loureiro",
      "Steven Derby",
      "Tri Kurniawan Wijaya"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1419": {
    "title": "To Share or Not to Share: What Risks Would Laypeople Accept to Give Sensitive Data to Differentially-Private NLP Systems?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Weiss",
      "Frauke Kreuter",
      "Ivan Habernal"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1420": {
    "title": "Towards a Corpus of Spoken Maltese: Korpus tal-Malti Mitkellem, KMM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandra (Sandra) Vella",
      "Sarah Agius",
      "Aiden Williams",
      "Claudia Borg"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1421": {
    "title": "Towards a Danish Semantic Reasoning Benchmark - Compiled from Lexical-Semantic Resources for Assessing Selected Language Understanding Capabilities of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bolette Pedersen",
      "Nathalie Sørensen",
      "Sussi Olsen",
      "Sanni Nimb",
      "Simon Gray"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1422": {
    "title": "Towards a Framework for Evaluating Explanations in Automated Fact Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neema Kotonya",
      "Francesca Toni"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1423": {
    "title": "Towards Algorithmic Fidelity: Mental Health Representation across Demographics in Synthetic vs. Human-generated Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shinka Mori",
      "Oana Ignat",
      "Andrew Lee",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1424": {
    "title": "Towards an Ideal Tool for Learner Error Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Špela Arhar Holdt",
      "Tomaž Erjavec",
      "Iztok Kosem",
      "Elena Volodina"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1425": {
    "title": "Towards Answering Health-related Questions from Medical Videos: Datasets and Approaches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deepak Gupta",
      "Kush Attal",
      "Dina Demner-Fushman"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1426": {
    "title": "Towards a Unified Taxonomy of Deep Syntactic Relations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kira Droganova",
      "Daniel Zeman"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1427": {
    "title": "Towards Autonomous Tool Utilization in Language Models: A Unified, Efficient and Scalable Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Li",
      "Yicheng Li",
      "Hequan Ye",
      "Yin Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1428": {
    "title": "Towards a Zero-Data, Controllable, Adaptive Dialog System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dirk Väth",
      "Lindsey Vanderlyn",
      "Ngoc Thang Vu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1429": {
    "title": "Towards Building the LEMI Readability Platform for Children's Literature in the Romanian Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Madalina Chitez",
      "Mihai Dascalu",
      "Aura Cristina Udrea",
      "Cosmin Strilețchi",
      "Karla Csürös",
      "Roxana Rogobete",
      "Alexandru Oravițan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1430": {
    "title": "Towards Comprehensive Language Analysis for Clinically Enriched Spontaneous Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baris Karacan",
      "Ankit Aich",
      "Avery Quynh",
      "Amy Pinkham",
      "Philip Harvey",
      "Colin Depp",
      "Natalie Parde"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1431": {
    "title": "Towards Cost-effective Multi-style Conversations: A Pilot Study in Task-oriented Dialogue Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiziano Labruna",
      "Bernardo Magnini"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1432": {
    "title": "Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artem Abzaliev",
      "Humberto Perez-Espinosa",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1433": {
    "title": "Towards Equitable Natural Language Understanding Systems for Dialectal Cohorts: Debiasing Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khadige Abboud",
      "Gokmen Oz"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1434": {
    "title": "Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Santosh T.y.s.s.",
      "Nina Baumgartner",
      "Matthias Stürmer",
      "Matthias Grabmair",
      "Joel Niklaus"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1435": {
    "title": "Towards Few-shot Entity Recognition in Document Images: A Graph Neural Network Approach Robust to Image Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prashant Krishnan",
      "Zilong Wang",
      "Yangkun Wang",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1436": {
    "title": "Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhouhao Sun",
      "Xiao Ding",
      "Li Du",
      "Bibo Cai",
      "Jinglong Gao",
      "Ting Liu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1437": {
    "title": "Towards Graph-hop Retrieval and Reasoning in Complex Question Answering over Textual Database",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minjun Zhu",
      "Yixuan Weng",
      "Shizhu He",
      "Kang Liu",
      "Haifeng Liu",
      "Yang jun Jun",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1438": {
    "title": "Towards Human-aligned Evaluation for Linear Programming Word Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linzi Xing",
      "Xinglu Wang",
      "Yuxi Feng",
      "Zhenan Fan",
      "Jing Xiong",
      "Zhijiang Guo",
      "Xiaojin Fu",
      "Rindra Ramamonjison",
      "Mahdi Mostajabdaveh",
      "Xiongwei Han",
      "Zirui Zhou",
      "Yong Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1439": {
    "title": "Towards Human-Like Machine Comprehension: Few-Shot Relational Learning in Visually-Rich Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Wang",
      "Tang Li",
      "Chenhui Chu",
      "Rui Wang",
      "Pinpin Zhu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1440": {
    "title": "Towards More Realistic Chinese Spell Checking with New Benchmark and Specialized Expert Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Wang",
      "Zilong Zheng",
      "Juntao Li",
      "Zhihui Liu",
      "Jinxiong Chang",
      "Qishen Zhang",
      "Zhongyi Liu",
      "Guannan Zhang",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1441": {
    "title": "Towards Multi-modal Sarcasm Detection via Disentangled Multi-grained Multi-modal Distilling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihong Zhu",
      "Xuxin Cheng",
      "Guimin Hu",
      "Yaowei Li",
      "Zhiqi Huang",
      "Yuexian Zou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1442": {
    "title": "Towards Realistic Few-Shot Relation Extraction: A New Meta Dataset and Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fahmida Alam",
      "Md Asiful Islam",
      "Robert Vacareanu",
      "Mihai Surdeanu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1443": {
    "title": "Towards Robust Evidence-Aware Fake News Detection via Improving Semantic Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yike Wu",
      "Yang Xiao",
      "Mengting Hu",
      "Mengying Liu",
      "Pengcheng Wang",
      "Mingming Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1444": {
    "title": "Towards Robust In-Context Learning for Machine Translation with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaolin Zhu",
      "Menglong Cui",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1445": {
    "title": "Towards Robust Temporal Activity Localization Learning with Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daizong Liu",
      "Xiaoye Qu",
      "Xiang Fang",
      "Jianfeng Dong",
      "Pan Zhou",
      "Guoshun Nan",
      "Keke Tang",
      "Wanlong Fang",
      "Yu Cheng"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1446": {
    "title": "Towards Semantic Tagging for Irish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Czerniak",
      "Elaine Uí Dhonnchadha"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1447": {
    "title": "Towards Standardized Annotation and Parsing for Korean FrameNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yige Chen",
      "Jae Ihn",
      "KyungTae Lim",
      "Jungyeul Park"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1448": {
    "title": "Towards the WhAP Corpus: A Resource for the Study of Italian on WhatsApp",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilaria Fiorentini",
      "Marco Forlano",
      "Nicholas Nese"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1449": {
    "title": "Towards Understanding the Relationship between In-context Learning and Compositional Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungjun Han",
      "Sebastian Padó"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1450": {
    "title": "Towards Universal Dependencies for Ancash Quechua",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johanna Cordova"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1451": {
    "title": "TP-Link: Fine-grained Pre-Training for Text-to-SQL Parsing with Linking Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqiang Liu",
      "Shujie Li",
      "Zefeng Cai",
      "Xiangyu Li",
      "Yunshui Li",
      "Chengming Li",
      "Xiping Hu",
      "Ruifeng Xu",
      "Min Yang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1452": {
    "title": "Training BERT Models to Carry over a Coding System Developed on One Corpus to Another",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dalma Galambos",
      "Pal Zsamboki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1453": {
    "title": "TransCoder: Towards Unified Transferable Code Representation Learning Inspired by Human Skills",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiushi Sun",
      "Nuo Chen",
      "Jianing Wang",
      "Ming Gao",
      "Xiang Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1454": {
    "title": "TransERR: Translation-based Knowledge Graph Embedding via Efficient Relation Rotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiang Li",
      "Xiangdong Su",
      "Fujun Zhang",
      "Guanglai Gao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1455": {
    "title": "Transfer Fine-tuning for Quality Estimation of Text Simplification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Hironaka",
      "Tomoyuki Kajiwara",
      "Takashi Ninomiya"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1456": {
    "title": "Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piotr Rybak"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1457": {
    "title": "Transformer-based Joint Modelling for Automatic Essay Scoring and Off-Topic Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sourya Dipta Das",
      "Yash A. Vadi",
      "Kuldeep Yadav"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1458": {
    "title": "Transformer-based Swedish Semantic Role Labeling through Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dana Dannélls",
      "Richard Johansson",
      "Lucy Yang Buhr"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1459": {
    "title": "Transformers for Bridging Persian Dialects: Transliteration Model for Tajiki and Iranian Scripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MohammadAli SadraeiJavaheri",
      "Ehsaneddin Asgari",
      "Hamid Reza Rabiee"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1460": {
    "title": "Tree-Instruct: A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingxiu Zhao",
      "Bowen Yu",
      "Binyuan Hui",
      "Haiyang Yu",
      "Minghao Li",
      "Fei Huang",
      "Nevin L. Zhang",
      "Yongbin Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1461": {
    "title": "TRELM: Towards Robust and Efficient Pre-training for Knowledge-Enhanced Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junbing Yan",
      "Chengyu Wang",
      "Taolin Zhang",
      "Xiaofeng He",
      "Jun Huang",
      "Wei Zhang",
      "Longtao Huang",
      "Hui Xue"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1462": {
    "title": "Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhinav Sukumar Rao",
      "Atharva Roshan Naik",
      "Sachin Vashistha",
      "Somak Aditya",
      "Monojit Choudhury"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1463": {
    "title": "Triple-R: Automatic Reasoning for Fact Verification Using Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammadamin Kanaani"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1464": {
    "title": "Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource Agglutinative Data-to-Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francois Meyer",
      "Jan Buys"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1465": {
    "title": "Trustworthiness and Self-awareness in Large Language Models: An Exploration through the Think-Solve-Verify Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhendong Liu",
      "Changhong Xia",
      "Wei He",
      "Chongjun Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1466": {
    "title": "Tug-of-War between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoran Jin",
      "Pengfei Cao",
      "Yubo Chen",
      "Kang Liu",
      "Xiaojian Jiang",
      "Jiexin Xu",
      "Li Qiuxia",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1467": {
    "title": "TunArTTS: Tunisian Arabic Text-To-Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Imen Laouirine",
      "Rami Kammoun",
      "Fethi Bougares"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1468": {
    "title": "TweetTER: A Benchmark for Target Entity Retrieval on Twitter without Knowledge Bases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiamehr Rezaee",
      "Jose Camacho-Collados",
      "Mohammad Taher Pilehvar"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1469": {
    "title": "Two Counterexamples to Tokenization and the Noiseless Channel",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Cognetta",
      "Vilém Zouhar",
      "Sangwhan Moon",
      "Naoaki Okazaki"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1470": {
    "title": "Typos Correction Training against Misspellings from Text-to-Text Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guicai Xie",
      "Ke Zhang",
      "Lei Duan",
      "Wei Zhang",
      "Zeqian Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1471": {
    "title": "UCxn: Typologically-Informed Annotation of Constructions Atop Universal Dependencies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonie Weissweiler",
      "Nina Böbel",
      "Kirian Guiller",
      "Santiago Herrera",
      "Wesley Samuel Scivetti",
      "Arthur Lorenzi",
      "Nurit Melnik",
      "Archna Bhatia",
      "Hinrich Schütze",
      "Lori Levin",
      "Amir Zeldes",
      "Joakim Nivre",
      "William Croft",
      "Nathan Schneider"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1472": {
    "title": "UDMorph: Morphosyntactically Tagged UD Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maarten Janssen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1473": {
    "title": "UkraiNER: A New Corpus and Annotation Scheme towards Comprehensive Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lauriane Aufrant",
      "Lucie Chasseur"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1474": {
    "title": "UMTIT: Unifying Recognition, Translation, and Generation for Multimodal Text Image Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liqiang Niu",
      "Fandong Meng",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1475": {
    "title": "Uncertainty-Aware Cross-Modal Alignment for Hate Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanpeng Yang",
      "Fuqing Zhu",
      "Yaxin Liu",
      "Jizhong Han",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1476": {
    "title": "Uncovering Agendas: A Novel French & English Dataset for Agenda Detection on Social Media",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gregorios Katsios",
      "Ning Sa",
      "Ankita Bhaumik",
      "Tomek Strzalkowski"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1477": {
    "title": "Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaxin Fan",
      "Feng Jiang",
      "Peifeng Li",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1478": {
    "title": "Understanding How Positional Encodings Work in Transformer Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taro Miyazaki",
      "Hideya Mino",
      "Hiroyuki Kaneko"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1479": {
    "title": "Unicode Normalization and Grapheme Parsing of Indic Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nazmuddoha Ansary",
      "Quazi Adibur Rahman Adib",
      "Tahsin Reasat",
      "Asif Shahriyar Sushmit",
      "Ahmed Imtiaz Humayun",
      "Sazia Mehnaz",
      "Kanij Fatema",
      "Mohammad Mamun Or Rashid",
      "Farig Sadeque"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1480": {
    "title": "Unifying Latent and Lexicon Representations for Effective Video-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haowei Liu",
      "Yaya Shi",
      "Haiyang Xu",
      "Chunfeng Yuan",
      "Qinghao Ye",
      "Chenliang Li",
      "Ming Yan",
      "Ji Zhang",
      "Fei Huang",
      "Bing Li",
      "Weiming Hu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1481": {
    "title": "UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucheng Cai",
      "Wentao Ma",
      "Yuchuan Wu",
      "Shuzheng Si",
      "Yuan Shao",
      "Zhijian Ou",
      "Yongbin Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1482": {
    "title": "UniPSDA: Unsupervised Pseudo Semantic Data Augmentation for Zero-Shot Cross-Lingual Natural Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyang Li",
      "Taolin Zhang",
      "Jiali Deng",
      "Longtao Huang",
      "Chengyu Wang",
      "Xiaofeng He",
      "Hui Xue"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1483": {
    "title": "UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongru Wang",
      "Boyang Xue",
      "Baohang Zhou",
      "Rui Wang",
      "Fei Mi",
      "Weichao Wang",
      "Yasheng Wang",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1484": {
    "title": "Universal Anaphora: The First Three Years",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Massimo Poesio",
      "Maciej Ogrodniczuk",
      "Vincent Ng",
      "Sameer Pradhan",
      "Juntao Yu",
      "Nafise Sadat Moosavi",
      "Silviu Paun",
      "Amir Zeldes",
      "Anna Nedoluzhko",
      "Michal Novák",
      "Martin Popel",
      "Zdeněk Žabokrtský",
      "Daniel Zeman"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1485": {
    "title": "Universal Dependencies: Extensions for Modern and Historical German",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefanie Dipper",
      "Cora Haiber",
      "Anna Maria Schröter",
      "Alexandra Wiemann",
      "Maike Brinkschulte"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1486": {
    "title": "Universal Dependencies for Learner Russian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alla Rozovskaya"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1487": {
    "title": "Unleashing the Power of Imbalanced Modality Information for Multi-modal Knowledge Graph Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichi Zhang",
      "Zhuo Chen",
      "Lei Liang",
      "Huajun Chen",
      "Wen Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1488": {
    "title": "Unlocking Instructive In-Context Learning with Tabular Prompting for Relational Triple Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guozheng Li",
      "Wenjun Ke",
      "Peng Wang",
      "Zijie Xu",
      "Ke Ji",
      "Jiajun Liu",
      "Ziyu Shang",
      "Qiqing Luo"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1489": {
    "title": "Unmasking Biases: Exploring Gender Bias in English-Catalan Machine Translation through Tokenization Analysis and Novel Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Audrey Mash",
      "Carlos Escolano",
      "Aleix Sant",
      "Maite Melero",
      "Francesca de Luca Fornaciari"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1490": {
    "title": "Unpacking Bias: An Empirical Study of Bias Measurement Metrics, Mitigation Algorithms, and Their Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felipe Bravo-Marquez",
      "Maria Jose Zambrano"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1491": {
    "title": "Unraveling Spontaneous Speech Dimensions for Cross-Corpus ASR System Evaluation for French",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Solene Virginie Evain",
      "Solange Rossato",
      "François Portet"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1492": {
    "title": "Unsupervised Grouping of Public Procurement Similar Items: Which Text Representation Should I Use?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro P. V. Brum",
      "Mariana O. Silva",
      "Gabriel P. Oliveira",
      "Lucas G. L. Costa",
      "Anisio Lacerda",
      "Gisele Pappa"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1493": {
    "title": "Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yantao Liu",
      "Zijun Yao",
      "Xin Lv",
      "Yuchen Fan",
      "Shulin Cao",
      "Jifan Yu",
      "Lei Hou",
      "Juanzi Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1494": {
    "title": "Unveiling Project-Specific Bias in Neural Code Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiming Li",
      "Yanzhou Li",
      "Tianlin Li",
      "Mengnan Du",
      "Bozhi Wu",
      "Yushi Cao",
      "Junzhe Jiang",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1495": {
    "title": "Unveiling Strengths and Weaknesses of NLP Systems Based on a Rich Evaluation Corpus: The Case of NER in French",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alice Millour",
      "Yoann Dupont",
      "Karen Fort",
      "Liam Duignan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1496": {
    "title": "Unveiling Vulnerability of Self-Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khai Jiet Liong",
      "Hongqiu Wu",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1497": {
    "title": "UQA: Corpus for Urdu Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samee Arif",
      "Sualeha Farid",
      "Awais Athar",
      "Agha Ali Raza"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1498": {
    "title": "UrduMASD: A Multimodal Abstractive Summarization Dataset for Urdu",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Faheem",
      "Faizad Ullah",
      "Muhammad Sohaib Ayub",
      "Asim Karim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1499": {
    "title": "User Guide for KOTE: Korean Online That-gul Emotions Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duyoung Jeon",
      "Junho Lee",
      "Cheongtag Kim"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1500": {
    "title": "Using Bibliodata LODification to Create Metadata-Enriched Literary Corpora in Line with FAIR Principles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Agnieszka Karlinska",
      "Cezary Rosiński",
      "Marek Kubis",
      "Patryk Hubar",
      "Jan Wieczorek"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1501": {
    "title": "Using Persuasive Writing Strategies to Explain and Detect Health Misinformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danial Kamali",
      "Joseph D. Romain",
      "Huiyi Liu",
      "Wei Peng",
      "Jingbo Meng",
      "Parisa Kordjamshidi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1502": {
    "title": "Using Pre-Trained Language Models in an End-to-End Pipeline for Antithesis Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramona Kühn",
      "Khouloud Saadi",
      "Jelena Mitrović",
      "Michael Granitzer"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1503": {
    "title": "Using Speech Technology to Test Theories of Phonetic and Phonological Typology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anisia Popescu",
      "Lori Lamel",
      "Ioana Vasilescu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1504": {
    "title": "Utilizing Local Hierarchy with Adversarial Training for Hierarchical Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Wang",
      "Peiyi Wang",
      "Houfeng Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1505": {
    "title": "Utilizing Longer Context than Speech Bubbles in Automated Manga Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroto Kaino",
      "Soichiro Sugihara",
      "Tomoyuki Kajiwara",
      "Takashi Ninomiya",
      "Joshua B. Tanner",
      "Shonosuke Ishiwatari"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1506": {
    "title": "UzbekVerbDetection: Rule-based Detection of Verbs in Uzbek Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maksud Sharipov",
      "Elmurod Kuriyozov",
      "Ollabergan Yuldashev",
      "Ogabek Sobirov"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1507": {
    "title": "Validating and Exploring Large Geographic Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Dunn"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1508": {
    "title": "Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David R. Mortensen",
      "Valentina Izrailevitch",
      "Yunze Xiao",
      "Hinrich Schütze",
      "Leonie Weissweiler"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1509": {
    "title": "VietMed: A Dataset and Benchmark for Automatic Speech Recognition of Vietnamese in the Medical Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khai Le-Duc"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1510": {
    "title": "VI-OOD: A Unified Framework of Representation Learning for Textual Out-of-distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li-Ming Zhan",
      "Bo Liu",
      "Xiao-Ming Wu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1511": {
    "title": "Visual-Linguistic Dependency Encoding for Image-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxin Guo",
      "Lei Zhang",
      "Kun Zhang",
      "Yi Liu",
      "Zhendong Mao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1512": {
    "title": "Visual-Textual Entailment with Quantities Using Model Checking and Knowledge Injection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nobuyuki Iokawa",
      "Hitomi Yanaka"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1513": {
    "title": "Vygotsky Distance: Measure for Benchmark Task Similarity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxim K. Surkov",
      "Ivan P. Yamshchikov"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1514": {
    "title": "WaCadie: Towards an Acadian French Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy Robichaud",
      "Paul Cook"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1515": {
    "title": "Well Begun Is Half Done: An Implicitly Augmented Generative Framework with Distribution Modification for Hierarchical Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huawen Feng",
      "Jingsong Yan",
      "Junlong Liu",
      "Junhao Zheng",
      "Qianli Ma"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1516": {
    "title": "What Are the Implications of Your Question? Non-Information Seeking Question-Type Identification in CNN Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Sun",
      "Anastasiia Tatlubaeva",
      "Zhihan Li",
      "Chester Palen-Michel"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1517": {
    "title": "What Can Diachronic Contexts and Topics Tell Us about the Present-Day Compositionality of English Noun Compounds?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samin Mahdizadeh Sani",
      "Malak Rassem",
      "Chris W. Jenkins",
      "Filip Miletić",
      "Sabine Schulte im Walde"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1518": {
    "title": "What Do Transformers Know about Government?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jue Hou",
      "Anisia Katinskaia",
      "Lari Kotilainen",
      "Sathianpong Trangcasanchai",
      "Anh-Duc Vu",
      "Roman Yangarber"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1519": {
    "title": "What Factors Influence LLMs' Judgments? A Case Study on Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Chen",
      "Bobo Li",
      "Li Zheng",
      "Haining Wang",
      "Zixiang Meng",
      "Runfeng Shi",
      "Hao Fei",
      "Jun Zhou",
      "Fei Li",
      "Chong Teng",
      "Donghong Ji"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1520": {
    "title": "What Happens to a Dataset Transformed by a Projection-based Concept Removal Method?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richard Johansson"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1521": {
    "title": "What Has LeBenchmark Learnt about French Syntax?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zdravko Dugonjić",
      "Adrien Pupier",
      "Benjamin Lecouteux",
      "Maximin Coavoux"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1522": {
    "title": "What Is Needed for Intra-document Disambiguation of Math Identifiers?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takuto Asakura",
      "Yusuke Miyao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1523": {
    "title": "When Argumentation Meets Cohesion: Enhancing Automatic Feedback in Student Writing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuning Ding",
      "Omid Kashefi",
      "Swapna Somasundaran",
      "Andrea Horbach"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1524": {
    "title": "When Cohesion Lies in the Embedding Space: Embedding-Based Reference-Free Metrics for Topic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iacopo Ghinassi",
      "Lin Wang",
      "Chris Newell",
      "Matthew Purver"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1525": {
    "title": "When Do \"More Contexts\" Help with Sarcasm Recognition?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ojas Nimase",
      "Sanghyun Hong"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1526": {
    "title": "When Your Cousin Has the Right Connections: Unsupervised Bilingual Lexicon Induction for Related Data-Imbalanced Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niyati Bafna",
      "Cristina España-Bonet",
      "Josef van Genabith",
      "Benoît Sagot",
      "Rachel Bawden"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1527": {
    "title": "Which Sense Dominates Multisensory Semantic Understanding? A Brain Decoding Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dandan Huang",
      "Lu Cao",
      "Zhenting Li",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1528": {
    "title": "Who Did You Blame When Your Project Failed? Designing a Corpus for Presupposition Generation in Cross-Examination Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria Francis",
      "Julius Steuer",
      "Dietrich Klakow",
      "Volha Petukhova"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1529": {
    "title": "Who Is Bragging More Online? A Large Scale Analysis of Bragging in Social Media",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mali Jin",
      "Daniel Preotiuc-Pietro",
      "A. Seza Doğruöz",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1530": {
    "title": "Who Said What: Formalization and Benchmarks for the Task of Quote Attribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Zhong",
      "Jason Naradowsky",
      "Hiroya Takamura",
      "Ichiro Kobayashi",
      "Yusuke Miyao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1531": {
    "title": "Why Voice Biomarkers of Psychiatric Disorders Are Not Used in Clinical Practice? Deconstructing the Myth of the Need for Objective Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent P. Martin",
      "Jean-Luc Rouas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1532": {
    "title": "WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hichem Ammar Khodja",
      "Frederic Bechet",
      "Quentin Brabant",
      "Alexis Nasr",
      "Gwénolé Lecorvé"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1533": {
    "title": "WikiSplit++: Easy Data Refinement for Split and Rephrase",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hayato Tsukagoshi",
      "Tsutomu Hirao",
      "Makoto Morishita",
      "Katsuki Chousa",
      "Ryohei Sasano",
      "Koichi Takeda"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1534": {
    "title": "Willkommens-Merkel, Chaos-Johnson, and Tore-Klose: Modeling the Evaluative Meaning of German Personal Name Compounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Annerose Eichel",
      "Tana Deeg",
      "Andre Blessing",
      "Milena Belosevic",
      "Sabine Arndt-Lappe",
      "Sabine Schulte im Walde"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1535": {
    "title": "WkNER: Enhancing Named Entity Recognition with Word Segmentation Constraints and kNN Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanchun Li",
      "Senlin Deng",
      "Dongsu Shen",
      "Shujuan Tian",
      "Saiqin Long"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1536": {
    "title": "Word-Aware Modality Stimulation for Multimodal Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhei Tateishi",
      "Makoto Nakatsuji",
      "Yasuhito Osugi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1537": {
    "title": "Word-level Commonsense Knowledge Selection for Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Yang",
      "Yu Hong",
      "Shiming He",
      "Qingting Xu",
      "Jianmin Yao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1538": {
    "title": "WordNet under Scrutiny: Dictionary Examples in the Era of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fatemah Yousef Almeman",
      "Steven Schockaert",
      "Luis Espinosa Anke"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1539": {
    "title": "WorldValuesBench: A Large-Scale Benchmark Dataset for Multi-Cultural Value Awareness of Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenlong Zhao",
      "Debanjan Mondal",
      "Niket Tandon",
      "Danica Dillion",
      "Kurt Gray",
      "Yuling Gu"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1540": {
    "title": "Would You Like to Make a Donation? A Dialogue System to Persuade You to Donate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Song",
      "Houfeng Wang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1541": {
    "title": "WW-CSL: A New Dataset for Word-Based Wearable Chinese Sign Language Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Xu",
      "Kai Liu",
      "Yifeng Yang",
      "Keyu Yan"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1542": {
    "title": "XAI-Attack: Utilizing Explainable AI to Find Incorrectly Learned Patterns for Black-Box Adversarial Example Creation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Markus Bayer",
      "Markus Neiczer",
      "Maximilian Samsinger",
      "Björn Buchhold",
      "Christian Reuter"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1543": {
    "title": "XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haopeng Zhang",
      "Hayate Iso",
      "Sairam Gurajada",
      "Nikita Bhutani"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1544": {
    "title": "XVD: Cross-Vocabulary Differentiable Training for Generative Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Roth",
      "Inigo Jauregi Unanue",
      "Alsharif Abuadbba",
      "Massimo Piccardi"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1545": {
    "title": "Your Stereotypical Mileage May Vary: Practical Challenges of Evaluating Biases in Multiple Languages and Cultural Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karen Fort",
      "Laura Alonso Alemany",
      "Luciana Benotti",
      "Julien Bezançon",
      "Claudia Borg",
      "Marthese Borg",
      "Yongjian Chen",
      "Fanny Ducel",
      "Yoann Dupont",
      "Guido Ivetta",
      "Zhijian Li",
      "Margot Mieskes",
      "Marco Naguib",
      "Yuyan Qian",
      "Matteo Radaelli",
      "Wolfgang S. Schmeisser-Nieto",
      "Emma Raimundo Schulz",
      "Thiziri Saci",
      "Sarah Saidi",
      "Javier Torroba Marchante",
      "Shilin Xie",
      "Sergio E. Zanotto",
      "Aurélie Névéol"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1546": {
    "title": "ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Injy Hamed",
      "Fadhl Eryani",
      "David Palfreyman",
      "Nizar Habash"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1547": {
    "title": "ZeLa: Advancing Zero-Shot Multilingual Semantic Parsing with Large Language Models and Chain-of-Thought Strategies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Truong Dinh Do",
      "Phuong Minh Nguyen",
      "Minh Nguyen"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1548": {
    "title": "ZenPropaganda: A Comprehensive Study on Identifying Propaganda Techniques in Russian Coronavirus-Related Media",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anton Chernyavskiy",
      "Svetlana Shomova",
      "Irina Dushakova",
      "Ilya Kiriya",
      "Dmitry Ilvovsky"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1549": {
    "title": "Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md. Arid Hasan",
      "Shudipta Das",
      "Afiyat Anjum",
      "Firoj Alam",
      "Anika Anjum",
      "Avijit Sarker",
      "Sheak Rashed Haider Noori"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1550": {
    "title": "Zero-shot Cross-lingual Automated Essay Scoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi He",
      "Xia Li"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1551": {
    "title": "Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhitao He",
      "Pengfei Cao",
      "Zhuoran Jin",
      "Yubo Chen",
      "Kang Liu",
      "Zhiqiang Zhang",
      "Mengshu Sun",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1552": {
    "title": "Zero-shot Event Detection Using a Textual Entailment Model as an Enhanced Annotator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqian Zeng",
      "Runyu Wu",
      "Yuxiang Xiao",
      "Xiaoda Zhong",
      "Hanlin Wang",
      "Zhengdong Lu",
      "Huiping Zhuang"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1553": {
    "title": "Zero-shot Learning for Multilingual Discourse Relation Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eleni Metheniti",
      "Philippe Muller",
      "Chloé Braud",
      "Margarita Hernández Casas"
    ]
  },
  "https://aclanthology.org/2024.lrec-main.1554": {
    "title": "Zero-Shot Spoken Language Understanding via Large Language Models: A Preliminary Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihong Zhu",
      "Xuxin Cheng",
      "Hao An",
      "Zhichang Wang",
      "Dongsheng Chen",
      "Zhiqi Huang"
    ]
  },
  "https://aclanthology.org/2024.lrec-tutorials.1": {
    "title": "From Multimodal LLM to Human-level AI: Modality, Instruction, Reasoning, Efficiency and beyond",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Fei",
      "Yuan Yao",
      "Zhuosheng Zhang",
      "Fuxiao Liu",
      "Ao Zhang",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2024.lrec-tutorials.2": {
    "title": "Geo-Cultural Representation and Inclusion in Language Technologies",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunipa Dev",
      "Rida Qadri"
    ]
  },
  "https://aclanthology.org/2024.lrec-tutorials.3": {
    "title": "Meaning Representations for Natural Languages: Design, Models and Applications",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julia Bonn",
      "Jeffrey Flanigan",
      "Jan Hajič",
      "Ishan Jindal",
      "Yunyao Li",
      "Nianwen Xue"
    ]
  },
  "https://aclanthology.org/2024.lrec-tutorials.4": {
    "title": "Navigating the Modern Evaluation Landscape: Considerations in Benchmarks and Frameworks for Large Language Models (LLMs)",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leshem Choshen",
      "Ariel Gera",
      "Yotam Perlitz",
      "Michal Shmueli-Scheuer",
      "Gabriel Stanovsky"
    ]
  },
  "https://aclanthology.org/2024.lrec-tutorials.5": {
    "title": "Mining, Assessing, and Improving Arguments in NLP and the Social Sciences",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriella Lapesa",
      "Eva Maria Vecchi",
      "Serena Villata",
      "Henning Wachsmuth"
    ]
  },
  "https://aclanthology.org/2024.lrec-tutorials.6": {
    "title": "Knowledge Editing for Large Language Models",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ningyu Zhang",
      "Yunzhi Yao",
      "Shumin Deng"
    ]
  },
  "https://aclanthology.org/2024.lrec-tutorials.7": {
    "title": "The DBpedia Databus Tutorial: Increase the Visibility and Usability of Your Data",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milan Dojchinovski"
    ]
  },
  "https://aclanthology.org/2024.lrec-tutorials.8": {
    "title": "NLP for Chemistry – Introduction and Recent Advances",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Camilo Thorne",
      "Saber Akhondi"
    ]
  },
  "https://aclanthology.org/2024.lrec-tutorials.9": {
    "title": "Formal Semantic Controls over Language Models",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danilo Silva de Carvalho",
      "Yingji Zhang",
      "André Freitas"
    ]
  },
  "https://aclanthology.org/2024.lrec-tutorials.10": {
    "title": "Towards a Human-Computer Collaborative Scientific Paper Lifecycle: A Pilot Study and Hands-On Tutorial",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyun Wang",
      "Carl Edwards",
      "Heng Ji",
      "Tom Hope"
    ]
  },
  "https://aclanthology.org/2024.lrec-tutorials.11": {
    "title": "Tutorial Proposal: Hallucination in Large Language Models",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vipula Rawte",
      "Aman Chadha",
      "Amit Sheth",
      "Amitava Das"
    ]
  },
  "https://aclanthology.org/2024.lrec-tutorials.12": {
    "title": "Addressing Bias and Hallucination in Large Language Models",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nihar Ranjan Sahoo",
      "Ashita Saxena",
      "Kishan Maharaj",
      "Arif A. Ahmad",
      "Abhijit Mishra",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2024.lrec-tutorials.13": {
    "title": "Knowledge-enhanced Response Generation in Dialogue Systems: Current Advancements and Emerging Horizons",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Priyanshu Priya",
      "Deeksha Varshney",
      "Mauajama Firdaus",
      "Asif Ekbal"
    ]
  }
}