{
  "https://openreview.net/forum?id=yIEyHP7AvH": {
    "title": "Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language?",
    "volume": "main",
    "abstract": "Evaluating Large Language Models (LLMs) is challenging due to their generative nature, necessitating precise evaluation methodologies. Additionally, non-English LLM evaluation lags behind English, resulting in the absence or weakness of LLMs for many languages. In response to this necessity, we introduce Khayyam Challenge (also known as PersianMMLU), a meticulously curated collection comprising 20,805 four-choice questions sourced from 38 diverse tasks extracted from Persian examinations, spanning a wide spectrum of subjects, complexities, and ages. The primary objective of the Khayyam Challenge is to facilitate the rigorous evaluation of LLMs that support the Persian language. Distinctive features of the Khayyam Challenge are (i) its comprehensive coverage of various topics, including literary comprehension, mathematics, sciences, logic, intelligence testing, etc aimed at assessing different facets of LLMs such as language comprehension, reasoning, and information retrieval across various educational stages, from lower primary school to upper secondary school (ii) its inclusion of rich metadata such as human response rates, difficulty levels, and descriptive answers (iii) its utilization of new data to avoid data contamination issues prevalent in existing frameworks (iv) its use of original, non-translated data tailored for Persian speakers, ensuring the framework is free from translation challenges and errors while encompassing cultural nuances (v) its inherent scalability for future data updates and evaluations without requiring special human effort. Previous works lacked an evaluation framework that combined all of these features into a single comprehensive benchmark. Furthermore, we evaluate a wide range of existing LLMs that support the Persian language, with statistical analyses and interpretations of their outputs. We believe that the Khayyam Challenge will improve advancements in LLMs for the Persian language by highlighting the existing limitations of current models, while also enhancing the precision and depth of evaluations on LLMs, even within the English language context",
    "checked": true,
    "id": "f6cde173286679ccaef67a1d602ad6e9d8cde9af",
    "semantic_title": "khayyam challenge (persianmmlu): is your llm truly wise to the persian language?",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=sJvhwDtFhQ": {
    "title": "TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have recently showcased remarkable reasoning abilities. However, larger models often surpass their smaller counterparts in reasoning tasks, posing the challenge of effectively transferring these capabilities from larger models. Existing approaches heavily rely on extensive fine-tuning data or continuous interactions with a superior teacher LLM during inference. We introduce a principle-based teacher-student framework called Teaching via Principle Discovery (TPD) to address these limitations. Inspired by human learning mechanisms, TPD mimics the interaction between a teacher and a student using a principle-based approach. The teacher LLM generates problem-solving instructions and corrective principles based on the student LLM's errors. These principles guide the refinement of instructions and the selection of instructive examples from a validation set. This enables the student model to learn from both the teacher's guidance and its own mistakes. Once the student model begins making inferences, TPD requires no further intervention from the teacher LLM. Through extensive experiments across eight reasoning tasks, we demonstrate the effectiveness of TPD. Compared to standard chain-of-thought prompting, TPD significantly improves the student model's performance, achieving an average improvement of 6.2\\%",
    "checked": true,
    "id": "2c6896c025b29a2cc3d90bbf9b77646b255b2090",
    "semantic_title": "tpd: enhancing student language model reasoning via principle discovery and guidance",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=FgHpT6u7pk": {
    "title": "CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration",
    "volume": "main",
    "abstract": "The deployment of multimodal large language models (MLLMs) has demonstrated remarkable success in engaging in conversations involving visual inputs, thanks to the superior power of large language models (LLMs). Those MLLMs are typically built based on the LLMs, with an image encoder to process images into the token embedding space of the LLMs. However, the integration of visual modality has introduced a unique vulnerability: the MLLM becomes susceptible to malicious visual inputs and prone to generating sensitive or harmful responses, even though the LLM has been trained on textual dataset to align with human value. In this paper, we first raise the following question: ``Do the MLLMs possess safety-awareness against malicious image inputs?\". We find that after adding a principle that specifies the safety requirement into the input of the MLLM, the model's safety awareness becomes boosted. This phenomenon verifies the existence of MLLM's safety-awareness against image inputs, it is only weakened by the modality gap. We then introduce a simple yet effective technique termed CoCA, which amplifies the safety-awareness of the MLLM by calibrating its output distribution. Our proposed strategy helps the model reclaim its original safety awareness without losing its original capabilities. We verify the effectiveness of our approach on both multimodal safety and understanding benchmarks",
    "checked": true,
    "id": "edce2a27c4ad6cd243423718243e6e4646880972",
    "semantic_title": "coca: regaining safety-awareness of multimodal large language models with constitutional calibration",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=lkrH6ovzsj": {
    "title": "Multi-FAct: Assessing Factuality of Multilingual LLMs using FActScore",
    "volume": "main",
    "abstract": "Evaluating the factuality of long-form large language model (LLM)-generated text is an important challenge. Recently there has been a surge of interest in factuality evaluation for English, but little is known about the factuality evaluation of multilingual LLMs, specially when it comes to long-form generation. This paper systematically evaluates multilingual LLMs' factual accuracy across languages and geographic regions. We introduce a simple pipeline for multilingual factuality evaluation, by applying FActScore \\citep{min2023factscore} for diverse languages. In addition to evaluating multilingual factual generation, we evaluate the factual accuracy of long-form text generation in topics that reflect regional diversity. We also examine the feasibility of running the FActScore pipeline using non-English Wikipedia and provide comprehensive guidelines on multilingual factual evaluation for regionally diverse topics",
    "checked": true,
    "id": "6dc4cfc068e02b7d9386677e5ea4a44d8f6fd6c9",
    "semantic_title": "multi-fact: assessing factuality of multilingual llms using factscore",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=XII0Wp1XA9": {
    "title": "A Dynamic LLM-Powered Agent Network for Task-Oriented Agent Collaboration",
    "volume": "main",
    "abstract": "Recent studies show that collaborating multiple large language model (LLM) powered agents is a promising way for task solving. However, current approaches are constrained by using a fixed number of agents and static communication structures. In this work, we propose automatically selecting a team of agents from candidates to collaborate in a dynamic communication structure toward different tasks and domains. Specifically, we build a framework named Dynamic LLM-Powered Agent Network ($\\textbf{DyLAN}$) for LLM-powered agent collaboration, operating a two-stage paradigm: (1) Team Optimization and (2) Task Solving. During the first stage, we utilize an agent selection algorithm, based on an unsupervised metric called Agent Importance Score, enabling the selection of best agents according to their contributions in a preliminary trial, oriented to the given task. Then, in the second stage, the selected agents collaborate dynamically according to the query. Empirically, we demonstrate that DyLAN outperforms strong baselines in code generation, decision-making, general reasoning, and arithmetic reasoning tasks with moderate computational cost. On specific subjects in MMLU, selecting a team of agents in the team optimization stage improves accuracy by up to 25.0% in DyLAN",
    "checked": true,
    "id": "5c002fd415b0284fe188dbd5d57a0acbed06de55",
    "semantic_title": "a dynamic llm-powered agent network for task-oriented agent collaboration",
    "citation_count": 141,
    "authors": []
  },
  "https://openreview.net/forum?id=2oHnsM9M9D": {
    "title": "ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation",
    "volume": "main",
    "abstract": "Evaluating the quality of free-text explanations is a multifaceted, subjective, and labor-intensive task. Large language models (LLMs) present an appealing alternative due to their potential for consistency, scalability, and cost-efficiency. In this work, we present ACORN, a new dataset of 3,500 free-text explanations and aspect-wise quality ratings, and use it to evaluate how LLMs rate explanations. We observed that larger models outputted labels that maintained or increased the inter-annotator agreement, suggesting that they are within the expected variance between human raters. However, their correlation with majority-voted human ratings varied across different quality aspects, indicating that they are not a complete replacement. In turn, using LLMs as a supplement to a smaller group of human raters in some cases improved the correlation with the original majority labels. However, the effect was limited to cases where human raters were scarce, and an additional human rater had a more pronounced effect in all cases. Overall, we recommend against using LLMs as a complete replacement for human raters but encourage using them in configurations that end with targeted human involvement",
    "checked": true,
    "id": "f7e48857a2e24fee32c0a15598b6e978691e6d86",
    "semantic_title": "acorn: aspect-wise commonsense reasoning explanation evaluation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Hvq9RtSoHG": {
    "title": "Chain-of-Symbol Prompting For Spatial Reasoning in Large Language Models",
    "volume": "main",
    "abstract": "While conventional Chain-of-Thought prompting shows promising performance on various language tasks for LLMs, the spatial scenarios are nearly unexplored. In this paper, we first investigate the performance of LLMs on complex spatial planning and understanding tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act or reason correspondingly in text. By evaluating on classic spatial planning scenarios through natural language descriptions, we found that current popular LLMs still lack abilities to handle spatial relationships in texts. This arises a question -- do the natural language is the best way to represent complex spatial environments for LLMs, or maybe other alternatives such as symbolic representations are both more efficient and effective for LLMs? To this end, we propose a novel method called CoS (Chain-of-Symbol Prompting) that represents the spatial relationships with condensed symbols during the chained intermediate thinking steps. CoS is easy to use and does not need additional training on LLMs. Extensive experiments indicate that CoS clearly surpasses the performance of the Chain-of-Thought (CoT) Prompting described in natural langauge in all three spatial planning tasks and existing spatial QA benchmark, with even fewer tokens used in the inputs compared with CoT. The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%) on Brick World scenarios for GPT-3.5-Turbo. CoS also reduces the number of tokens in the prompt obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on the Brick World task. Interestingly, we also observed emergent ability of abstract symbols understanding when the size of models scales up",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ybaK4asBT2": {
    "title": "LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown exceptional proficiency in natural language processing but often fall short of generating creative and original responses to open-ended questions. To enhance LLM creativity, our key insight is to emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives. To this end, we propose LLM Discussion, a three-phase discussion framework that facilitates vigorous and diverging idea exchanges and ensures convergence to creative answers. Moreover, we adopt a role-playing technique by assigning distinct roles to LLMs to combat the homogeneity of LLMs. We evaluate the efficacy of the proposed framework with the Alternative Uses Test, Similarities Test, Instances Test, and Scientific Creativity Test through both LLM evaluation and human study. The results show that our proposed framework outperforms single-LLM approaches and existing multi-LLM frameworks across various creativity metrics. The code is available at https://github.com/lawraa/LLM-Discussion",
    "checked": true,
    "id": "83d721abd07298ef0f5d05575410ae386b77bb4a",
    "semantic_title": "llm discussion: enhancing the creativity of large language models via discussion framework and role-play",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=YX7QnhxESU": {
    "title": "Mapping the Increasing Use of LLMs in Scientific Papers",
    "volume": "main",
    "abstract": "Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the $\\textit{arXiv}$, $\\textit{bioRxiv}$, and $\\textit{Nature}$ portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. The statistical framework operates on the population level without the need to perform inference on any individual instance. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5\\%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3\\%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded areas, and papers with shorter lengths. Our findings suggests that LLMs are being broadly used in scientific papers",
    "checked": true,
    "id": "882f3d1b718e833156ccd6f5bda3190cd06791ea",
    "semantic_title": "mapping the increasing use of llms in scientific papers",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=aajyHYjjsk": {
    "title": "The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have impressive capabilities, but are prone to outputting falsehoods. Recent work has developed techniques for inferring whether a LLM is telling the truth by training probes on the LLM's internal activations. However, this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues. In this work, we use high-quality datasets of simple true/false statements to study in detail the structure of LLM representations of truth, drawing on three lines of evidence: 1. Visualizations of LLM true/false statement representations, which reveal clear linear structure. 2. Transfer experiments in which probes trained on one dataset generalize to different datasets. 3. Causal evidence obtained by surgically intervening in a LLM's forward pass, causing it to treat false statements as true and vice versa. Overall, we present evidence that at sufficient scale, LLMs *linearly represent* the truth or falsehood of factual statements. We also show that simple difference-in-mean probes generalize as well as other probing techniques while identifying directions which are more causally implicated in model outputs",
    "checked": true,
    "id": "d59523889679aee15992c4bf6e52b134186d07d3",
    "semantic_title": "the geometry of truth: emergent linear structure in large language model representations of true/false datasets",
    "citation_count": 264,
    "authors": []
  },
  "https://openreview.net/forum?id=y6aGT625Lk": {
    "title": "PairEval: Open-domain Dialogue Evaluation Metric with Pairwise Comparisons",
    "volume": "main",
    "abstract": "Building a reliable and automated evaluation metric is a necessary but challenging problem for open-domain dialogue systems. Recent studies proposed evaluation metrics that assess generated responses by considering their relevance to previous dialogue histories. Although effective, these metrics evaluate individual responses directly rather than considering their relative quality compared to other responses. To handle this, we propose PairEval, a novel dialogue evaluation metric for assessing responses by comparing their quality against responses in different conversations. Our metric is built on top of open-sourced and moderate-size language models, and we make them specialized in pairwise comparison between dialogue responses. Extensive experiments on multiple benchmarks demonstrate that our metric exhibits a higher correlation with human judgments than baseline metrics. We also find that the proposed comparative metric is more robust in detecting common failures from open-domain dialogue systems, including repetition and speaker insensitivity. The codes and models will be publicly available after the paper is accepted",
    "checked": false,
    "id": "fd33493779317d30f5eba639968bff767d02c02a",
    "semantic_title": "paireval: open-domain dialogue evaluation with pairwise comparison",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=MNLAbfZwh2": {
    "title": "ScenicNL: Generating Probabilistic Scenario Programs from Natural Language",
    "volume": "main",
    "abstract": "For cyber-physical systems, including robotics and autonomous vehicles, mass deployment has been hindered by fatal errors that occur when operating in rare events. To better understand failure modes, companies meticulously recreate rare crash events in simulation, but current methods do not easily allow for exploring \"what if\" scenarios which could reveal how accidents might have been avoided. We present ScenicNL, an AI system that generates probabilistic scenario programs from natural language. Given the abundance of documented failures of autonomous vehicles due to regulatory requirements, we apply ScenicNL to police crash reports, providing a data-driven approach to capturing and understanding these failures. By using a probabilistic language such as Scenic, we can clearly and concisely represent such scenarios of interest and easily ask \"what if\" questions. We demonstrate how commonplace prompting techniques with Large Language Models are incapable of generating code for low-resource languages such as Scenic. We propose an AI system via the composition of several prompting techniques to extract the reasoning abilities needed to model probability distributions around the uncertainty in the crash events. Our system then uses Constrained Decoding and tools such as a compiler and simulator to produce scenario programs in this low-resource setting. We evaluate our system on publicly available autonomous vehicle crash reports in California from the last five years and share insights into how we generate code that is both semantically meaningful and syntactically correct. Finally, we release our code and a collection of over 500 crash reports from the California Department of Motor Vehicles",
    "checked": false,
    "id": "e7b924757176fd5eca61d50b182775b1e5114d7b",
    "semantic_title": "generating probabilistic scenario programs from natural language",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=mkYCfO822n": {
    "title": "AmbigDocs: Reasoning across Documents on Different Entities under the Same Name",
    "volume": "main",
    "abstract": "Different entities with the same name can be difficult to distinguish. Handling confusing entity mentions is a crucial skill for language models (LMs). For example, given the question \"Where was Michael Jordan educated?\" and a set of documents discussing different people named Michael Jordan, can LMs distinguish entity mentions to generate a cohesive answer to the question? To test this ability, we introduce a new benchmark, AmbigDocs. By leveraging Wikipedia's disambiguation pages, we identify a set of documents, belonging to different entities who share an ambiguous name. From these documents, we generate questions containing an ambiguous name and their corresponding sets of answers. Our analysis reveals that current state-of-the-art models often yield ambiguous answers or incorrectly merge information belonging to different entities. We establish an ontology categorizing four types of incomplete answers and automatic evaluation metrics to identify such categories. We lay the foundation for future work on reasoning across multiple documents with ambiguous entities",
    "checked": true,
    "id": "8dc4876573fe0b3fa11dc8fbd08ad44bd5e4d97e",
    "semantic_title": "ambigdocs: reasoning across documents on different entities under the same name",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=lJMioZBoR8": {
    "title": "Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope. While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms. In this paper, we conduct a comprehensive examination of the role of rejection, introducing the alignment goal of model reliability along with corresponding metrics. This goal requires the model to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations. To improve the inherent reliability of LLMs, we present a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically determine the model's knowledge boundary and trains a reliable reward model to encourage the rejection of out-of-knowledge questions. Experimental results on mathematical and question answering datasets affirm the substantial efficacy of RLKF in significantly enhancing LLM reliability",
    "checked": true,
    "id": "8e6124739e03c9d7ea4903de00c3370d2f1a8387",
    "semantic_title": "rejection improves reliability: training llms to refuse unknown questions using rl from knowledge feedback",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=rXEwxmnGQs": {
    "title": "PhonATe: Impact of Type-Written Phonological Features of African American Language on Generative Language Modeling Tasks",
    "volume": "main",
    "abstract": "Current Large Language Models perform poorly on African American Language (AAL) texts in tasks like toxicity detection and sentiment analysis. AAL is underrepresented in both pre-training data and existing benchmarks for these tasks, hindering thorough evaluation and understanding of these biases. We introduce a novel approach to synthetically introduce type-written phonological features of AAL into text, a class of AAL features that has been overlooked in prior work. Our goal is to better understand how these features affect generative language models' performance on three tasks: toxicity detection, sentiment analysis, and masked span prediction. We find that fine-tuning with synthetic type-written phonological features lowers perceived biases on downstream tasks and our ablations reveal which features have particularly large negative impacts on model performance. Our results suggest that phonological features are vital to consider when designing bias mitigation techniques",
    "checked": false,
    "id": "e899213f4dcc62b4bce2a98f6ad9a5dc6ea8fc80",
    "semantic_title": "p hon at e : impact of type-written phonological features of african american language on generative language modeling tasks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=gQAEGSGVnN": {
    "title": "UniMem: Towards a Unified View of Long-Context Large Language Models",
    "volume": "main",
    "abstract": "Long-context processing is a critical ability that constrains the applicability of large language models (LLMs). Although there exist various methods devoted to enhancing the long-context processing ability of LLMs, they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments. In this paper, we introduce UniMem, a Unified framework that reformulates existing long-context methods from the view of Memory augmentation of LLMs. Distinguished by its four core dimensions—Memory Management, Memory Writing, Memory Reading, and Memory Injection, UniMem empowers researchers to conduct systematic exploration of long-context methods. We re-formulate 16 existing methods based on UniMem and analyze four representative methods: Transformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent UniMem forms to reveal their design principles and strengths. Based on these analyses, we propose UniMix, an innovative approach that integrates the strengths of these algorithms. Experimental results show that UniMix achieves superior performance in handling long contexts with significantly lower perplexity than baselines. The code is publicly available at https://github.com/thunlp/UniMem",
    "checked": true,
    "id": "c86b98d0b5fe07e46fd862f47f717271c45b96fe",
    "semantic_title": "unimem: towards a unified view of long-context large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=98ekcwQqb7": {
    "title": "Latent Causal Probing: A Formal Perspective on Probing with Causal Models of Data",
    "volume": "main",
    "abstract": "As language models (LMs) deliver increasing performance on a range of NLP tasks, *probing classifiers* have become an indispensable technique in the effort to better understand their inner workings. A typical setup involves (1) defining an auxiliary task consisting of a dataset of text annotated with labels, then (2) supervising small classifiers to predict the labels from the representations of a pretrained LM as it processes the dataset. A high probing accuracy is interpreted as evidence that the LM has learned to perform the auxiliary task as an unsupervised byproduct of its original pretraining objective. Despite the widespread usage of probes, however, the robust design and analysis of probing experiments remains a challenge. We develop a formal perspective on probing using *structural causal models* (SCM). Specifically, given an SCM which explains the distribution of tokens observed during training, we frame the central hypothesis as whether the LM has learned to represent the latent variables of the SCM. Empirically, we extend a recent study of LMs in the context of a synthetic grid-world navigation task, where having an exact model of the underlying causal structure allows us to draw strong inferences from the result of probing experiments. Our techniques provide robust empirical evidence for the ability of LMs to induce the latent concepts underlying text",
    "checked": true,
    "id": "eca3d92e298f1f042cae4ee55257f2d9ceb2182b",
    "semantic_title": "latent causal probing: a formal perspective on probing with causal models of data",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=cG1EbmWiSs": {
    "title": "Unified View of Grokking, Double Descent and Emergent Abilities: A Comprehensive Study on Algorithm Task",
    "volume": "main",
    "abstract": "Recent studies have uncovered intriguing phenomena in deep learning, such as *grokking*, *double descent*, and *emergent abilities* in large language models, which challenge human intuition and are crucial for a deeper understanding of neural models. In this paper, we present a comprehensive study on algorithm task to provide a unified view of these three phenomena, with a focus on the interplay between memorization and generalization. Through extensive experiments spanning a wide range of model sizes and training data quantities, we uncover four distinct training dynamics, each arising from unique combinations of model size and training data quantity, formulating a theoretical framework for further analysis. Utilizing this framework, we establish connections between *double descent* and *grokking* and propose two verifiable predictions regarding the occurrence of *double descent*, both substantiated by our experimental results. Moreover, we expand our experiments to the multi-task learning paradigm, demonstrating how algorithm tasks can be turned into emergent abilities by mixing some pure memorization data. This offers a novel perspective to understand *emergent abilities* in Large Language Models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ljFgX6A8NL": {
    "title": "Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models",
    "volume": "main",
    "abstract": "Safety-aligned large language models (LLMs) sometimes falsely refuse pseudo-harmful prompts, like \"how to kill a mosquito,\" which are actually harmless. Frequent false refusals not only frustrate users but also provoke public backlash against the very values alignment seeks to protect. In this paper, we propose the first method to auto-generate diverse, content-controlled, and model-dependent pseudo-harmful prompts. Using this method, we construct an evaluation dataset called PHTest, which is ten times larger than existing datasets, covers more false refusal patterns, and separately labels controversial prompts. We evaluate 20 LLMs on PHTest, uncovering new insights due to its scale and labeling. Our findings reveal a trade-off between minimizing false refusals and improving safety against jailbreak attacks. Moreover, we show that many jailbreak defenses significantly increase the false refusal rates, thereby undermining usability. Our method and dataset can help developers evaluate and fine-tune safer and more usable LLMs. Our code and dataset are available at \\href{https://github.com/umd-huang-lab/FalseRefusal}{https://github.com/umd-huang-lab/FalseRefusal}",
    "checked": true,
    "id": "ea6d88b4ba550dfd1d2ce1fa2bc628e57ca51572",
    "semantic_title": "automatic pseudo-harmful prompt generation for evaluating false refusals in large language models",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=xS6zx1aBI9": {
    "title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
    "volume": "main",
    "abstract": "Language agents have shown some ability to interact with an external environment, e.g., a virtual world such as ScienceWorld, to perform complex tasks, e.g., growing a plant, without the startup costs of reinforcement learning. While recent work, e.g., Reflexion, has demonstrated how such agents can also self-improve by adding a textual memory of ''hints'' learned from prior experience, such improvements have been limited both in size and scope. In contrast, our goal is a language agent that can robustly improve performance over time, including when both the task and environment are varied. Our approach is to have the agent learn a textual representation of how the world works (rather than just isolated hints), expressed as a memory of causal abstractions, to guide future decision-making. In experiments, we find CLIN is able to continually improve on repeated trials on the same task and environment, outperforming state-of-the-art reflective language agents like Reflexion by 23 points in ScienceWorld and 1.4 points in ALFWorld benchmarks. CLIN can also transfer its learning to new environments and tasks, enhancing performance by 21 points in ScienceWorld and 11 points in ALFWorld. This suggests that language agents with a textual causal memory can play a significant role in interactive environments, including being able to rapidly improve over time",
    "checked": true,
    "id": "767a7e949ba4520888e7442ee01e6a37c254fc53",
    "semantic_title": "clin: a continually learning language agent for rapid task adaptation and generalization",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=gpgMRWgv9Q": {
    "title": "TarGEN: Targeted Data Generation with Large Language Models",
    "volume": "main",
    "abstract": "We present TarGEN, a multi-step prompting strategy for generating high-quality synthetic datasets using LLMs. An advantage of TarGEN is its seedless nature; it does not require specific task instances, broadening its applicability beyond task replication. This differentiates it from other data generation techniques, as it can be leveraged for novel or highly domain-specific tasks with no existing data instances. We augment TarGEN with a self-correction module that enables LLMs to rectify inaccurately labeled instances during dataset creation, ensuring reliable labels. To assess our technique's effectiveness against existing baselines, we emulate eight tasks from the SuperGLUE benchmark to create a \"synthetic\" version and finetune various language models on both synthetic and original training sets. Evaluation on the original test set reveals that models trained on the synthetic datasets perform ∼ 1 − 3% points higher than those trained on original datasets. Finally, when pre-finetuned on our \"synthetic\" SuperGLUE dataset, Llama2 (7B) yields impressive results on the OpenLLM leaderboard, surpassing the model trained on the Self-Instruct dataset by 2.62% points. Our analysis reveals that the synthetic data generated by TarGEN not only improves model learning, but also has comparable or higher levels of complexity, diversity, and similar levels of bias in comparison with the original data",
    "checked": true,
    "id": "d9e771732100c0f696cefb2a5c82cdd953140bb0",
    "semantic_title": "targen: targeted data generation with large language models",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=BAakY1hNKS": {
    "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversations",
    "volume": "main",
    "abstract": "We present AutoGen, an open-source framework that allows developers to build LLM applications by composing multiple agents to converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. It also enables developers to create flexible agent behaviors and conversation patterns for different applications using both natural language and code. AutoGen serves as a generic infrastructure and is widely used by AI practitioners and researchers to build diverse applications of various complexities and LLM capacities. We demonstrate the framework's effectiveness with several pilot applications, with domains ranging from mathematics and coding to question-answering, supply-chain optimization, online decision-making, and entertainment",
    "checked": false,
    "id": "9ea0757c750ab1222a7442d3485a74d1c526b04c",
    "semantic_title": "autogen: enabling next-gen llm applications via multi-agent conversation",
    "citation_count": 503,
    "authors": []
  },
  "https://openreview.net/forum?id=DomBynQsqt": {
    "title": "3M-Diffusion: Latent Multi-Modal Diffusion for Language-Guided Molecular Structure Generation",
    "volume": "main",
    "abstract": "Generating molecular structures with desired properties is a critical task with broad applications in drug discovery and materials design. We propose 3M-Diffusion, a novel multi-modal molecular graph generation method, to generate diverse, ideally novel molecular structures with desired properties. 3M-Diffusion encodes molecular graphs into a graph latent space which it then aligns with the text space learned by encoder-based LLMs from textual descriptions. It then reconstructs the molecular structure and atomic attributes based on the given text descriptions using the molecule decoder. It then learns a probabilistic mapping from the text space to the latent molecular graph space using a diffusion model. The results of our extensive experiments on several datasets demonstrate that 3M-Diffusion can generate high-quality, novel and diverse molecular graphs that semantically match the textual description provided",
    "checked": true,
    "id": "7bcdf3417c77bd305096d0e6dd151f722fb9dfe6",
    "semantic_title": "3m-diffusion: latent multi-modal diffusion for language-guided molecular structure generation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=IPZ28ZqD4I": {
    "title": "Faithful and Unfaithful Error Recovery in Chain of Thought",
    "volume": "main",
    "abstract": "Large language models (LLMs) often improve their performance in downstream tasks when they generate Chain of Thought reasoning text before producing an answer. We investigate how LLMs recover from errors in Chain of Thought. Through analysis of error recovery behaviors, we find evidence for unfaithfulness in Chain of Thought, which occurs when models arrive at the correct answer despite invalid reasoning text. We identify factors that shift LLM recovery behavior: LLMs recover more frequently from obvious errors and in contexts that provide more evidence for the correct answer. Critically, these factors have divergent effects on faithful and unfaithful recoveries. Our results indicate that there are distinct mechanisms driving faithful and unfaithful error recoveries. Selective targeting of these mechanisms may be able to drive down the rate of unfaithful reasoning and improve model interpretability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q5Ft9ZJtHm": {
    "title": "ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs",
    "volume": "main",
    "abstract": "Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debiasing performance while also preserving internal knowledge of a pre-trained LLM; and (3) synthetic data exhibits generalizability across categories, effectively mitigating various biases, including intersectional ones. These findings underscore the potential of synthetic data in advancing the fairness of LLMs with minimal retraining cost",
    "checked": true,
    "id": "2871341240e3f814df0209225da8bbfd85cc1def",
    "semantic_title": "chatgpt based data augmentation for improved parameter-efficient debiasing of llms",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=8w0RApM5yG": {
    "title": "BumbleBee: Dynamic KV-Cache Streaming Submodular Summarization for Infinite-Context Transformers",
    "volume": "main",
    "abstract": "Transformer-based Large Language Models (LLMs) have shown tremendous advancements across various domains. However, their need to maintain key-value representations (a KV cache) of previously seen tokens in the GPU memory leads to a significant memory overhead that scales linearly with the sequence length and batch size. With the advent of extremely long context LLMs, efficiently modeling long-range dependencies becomes challenging. In this work, we focus on the problem of long context summarization by formulating it as a subset selection problem. Specifically, we propose a novel submodular optimization framework called BumbleBee that uses a mixture of submodular functions to balance the diversity amongst the context tokens in the key embedding space and their importance computed using accumulated attention attributed to them across different input tokens. Our framework can work for both the LLM prefill and decoding phases, utilizing offline or online versions of our submodular algorithm respectively. While the context sizes grow to be as large only as the summary size, the temporal extent of the contexts may grow unboundedly, justifying the moniker ‘‘Infinite-Context Transformers.'' Empirically, we validate the effectiveness of our framework across 13 different datasets using the LLaMA 7B and 13B models. Our results show that BumbleBee improves accuracy compared to state-of-the-art techniques at comparable context reduction ratios",
    "checked": false,
    "id": "bd49df3cab623e34f6378b18c6c61c486e8c2cd2",
    "semantic_title": "bumblebee : dynamic kv-cache streaming submodular summarization for infinite-context transformers",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=zSf8PJyQb2": {
    "title": "Transformer Circuit Evaluation Metrics Are Not Robust",
    "volume": "main",
    "abstract": "Mechanistic interpretability work attempts to reverse engineer the learned algorithms present inside neural networks. One focus of this work has been to discover 'circuits' - subgraphs of the full model that explain behaviour on specific tasks. But how do we measure the performance of such circuits? Prior work has attempted to measure circuit 'faithfulness' - the degree to which the circuit replicates the performance of the full model. In this work, we survey many considerations for designing experiments that measure circuit faithfulness by ablating portions of the model's computation. Concerningly, we find existing methods are highly sensitive to seemingly insignificant changes in the ablation methodology. We conclude that existing circuit faithfulness scores reflect _both_ the methodological choices of researchers as well as the actual components of the circuit - the task a circuit is required to perform depends on the ablation used to test it. The ultimate goal of mechanistic interpretability work is to understand neural networks, so we emphasize the need for more clarity in the precise claims being made about circuits. We open source a library at [this https URL](https://github.com/UFO-101/auto-circuit) that includes highly efficient implementations of a wide range of ablation methodologies and circuit discovery algorithms",
    "checked": false,
    "id": "013a6a5c28c8a08f1d81da77c2bdd3a4870e91ca",
    "semantic_title": "learning the expressibility of quantum circuit ansatz using transformer",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=av0D19pSkU": {
    "title": "Do Membership Inference Attacks Work on Large Language Models?",
    "volume": "main",
    "abstract": "Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model's training data. Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of large language models (LLMs). We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members. We also find that, when LLMs have been shown to be vulnerable to MIAs, this apparent success can be attributed to a distribution shift, e.g., members and non-members are seemingly drawn from identical domain but with different temporal ranges. Finally, we observe that existing MIAs are highly sensitive to even small changes in a sample. Such changes may cause samples that are lexically or semantically similar to members to be classified as non-members, which may be at odds with leakage that privacy auditors care about. We release our code and data as a unified benchmark package that includes all existing MIAs, supporting future work",
    "checked": true,
    "id": "f16c669c126c800dac38d445ddd178ffb9af5b7a",
    "semantic_title": "do membership inference attacks work on large language models?",
    "citation_count": 122,
    "authors": []
  },
  "https://openreview.net/forum?id=amhPBLFYWv": {
    "title": "Revenge of the Fallen? Recurrent Models Match Transformers at Predicting Human Language Comprehension Metrics",
    "volume": "main",
    "abstract": "Transformers have generally supplanted recurrent neural networks as the dominant architecture for both natural language processing tasks and for modelling the effect of predictability on online human language comprehension. However, two recently developed recurrent model architectures, RWKV and Mamba, appear to perform natural language tasks comparably to or better than transformers of equivalent scale. In this paper, we show that contemporary recurrent models are now also able to match—and in some cases, exceed—performance of comparably sized transformers at modeling online human language comprehension. This suggests that transformer language models are not uniquely suited to this task, and opens up new directions for debates about the extent to which architectural features of language models make them better or worse models of human language comprehension",
    "checked": true,
    "id": "ea726b734ad22927338234089c0c3742aab0478c",
    "semantic_title": "revenge of the fallen? recurrent models match transformers at predicting human language comprehension metrics",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=GqDntYTTbk": {
    "title": "Starling-7B: Improving Helpfulness and Harmlessness with RLAIF",
    "volume": "main",
    "abstract": "This paper presents Starling-7B, the current best-performing 7B chat model on Chatbot Arena, along with its training dataset Nectar, a high-quality preference dataset collected by prompting GPT-4 to rank responses. We propose an internal pairwise rating technique, where the model considers all pairings before providing a ranking decision, leveraging the proven pairwise rating capability of LLMs without the cost of individual pairwise calls. The resulting Nectar dataset comprises 182,954 chat prompts, each with seven responses from various models, ranked by GPT-4, equating to 3.8 million high-quality pairwise comparisons. We introduce Starling-RM-7B and Starling-RM-34B, the reward model suites trained with a K-wise preference loss on Nectar, outperforming pairwise counterparts. We benchmark reward model training pipelines across metrics such as human preference, truthfulness, and safety. Using Nectar and our new training pipeline, we fine-tuned Openchat-3.5 to create Starling-LM-7B, achieving significant performance enhancements on MT-Bench, AlpacaEval, and human evaluation metrics. To facilitate research and understanding of RLHF mechanisms, we open-source the Nectar dataset, the reward models, and the language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2wtj0up8rv": {
    "title": "Enhancing Language Models with Idiomatic Reasoning",
    "volume": "main",
    "abstract": "Advancements in Large Language Models (LLMs) have significantly propelled the field of Natural Language Processing (NLP); however, nuanced reasoning in the presence of non-canonical language forms, such as figurative language, remains an intricate challenge. These language forms, integral to human communication, elude standard LLM comprehension due to their inherent non-compositionality, contextual ambiguity, and sparse representation in text corpora. Addressing these challenges, this paper introduces an innovative approach to seamlessly incorporate idiomatic knowledge into pre-trained language models (PTLMs). Our methodology first employs a multi-view data augmentation strategy that uses idiomatic instances representing one property to generate training data for various idiom-related tasks. When combined with a novel parameter-efficient tuning mechanism that accounts for the unique attributes of idiomatic language, we embed task-specific and idiomaticity-aware inductive biases within a PTLM. Integrating a meta-pretraining protocol based on meta-learning principles, further equips the model with enhanced adaptability to diverse downstream idiom-aware tasks. Empirical validation on diverse benchmarks centered around idiom comprehension and reasoning, demonstrates the efficacy of our approach. Notably, our model surpasses various parameter-efficient fine-tuning baselines outperforming the conventional full fine-tuning paradigms, thereby creating more contextually aware and linguistically robust language models",
    "checked": false,
    "id": "5ea68952a55a0d47a58a409b66c9daee35a237b8",
    "semantic_title": "learn beyond the answer: training language models with reflection for mathematical reasoning",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=INivcBeIDK": {
    "title": "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models",
    "volume": "main",
    "abstract": "Red-teaming Large Language Models (LLMs) requires jailbreak attacks that can comprehensively characterize the vulnerabilities of LLMs. Current blackbox attacks are limited by predefined jailbreak strategies, while whitebox attacks can only generate gibberish attack prompts detectable by perplexity filters. In this paper, we propose a new whitebox attack, named AutoDAN, that merges gradient-based token-wise optimization with controllable text generation. AutoDAN can generate coherent attack prompts on various LLMs that bypass any perplexity filter while having high attack success rates. Notably, these attack prompts spontaneously exhibit jailbreak strategies commonly seen in manual jailbreaks, such as hypothetical scenarios and non-English languages, without any prior knowledge of them. These interpretable attack prompts also generalize better to unseen harmful behaviors and transfer better to blackbox LLMs than gibberish ones. Moreover, we apply AutoDAN to two other red-teaming tasks: prompt leaking and generating falsely censored harmless user requests, demonstrating its flexibility over blackbox attacks. Our work offers an additional tool for red-teaming and understanding jailbreak mechanisms via interpretability",
    "checked": true,
    "id": "1227c2fcb8437441b7d72a29a4bc9eef1f5275d2",
    "semantic_title": "autodan: interpretable gradient-based adversarial attacks on large language models",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=1Tny4KgGO2": {
    "title": "From Strategic Narratives to Code-Like Cognitive Models: An LLM-Based Approach in A Sorting Task",
    "volume": "main",
    "abstract": "One of the goals of Cognitive Science is to understand the cognitive processes underlying human behavior. Traditionally, this goal has been approached by analyzing simple behaviors, such as choices and response times, to try to indirectly infer mental processes. However, a more direct approach is to simply ask people to report their thoughts - for example, by having them Introspect after the fact about the thought processes they used to complete a task. However, the data generated by such verbal reports have been hard to analyze, and whether the reported thoughts are an accurate reflection of the underlying cognitive processes has been difficult to test. Here we take a first stab at addressing these questions by using large language models to analyze verbally reported strategies in a sorting task. In the task, participants sort lists of pictures with unknown orders by pairwise comparison. After completing the task, participants wrote a description of their strategy for completing the task. To test whether these strategy descriptions contained information about people's actual strategies, we compared their choice behavior with their descriptions of the task. First, we compared the descriptions and choices at the level of strategy, finding that people who used similar sorting algorithms (based on their choices) provided similar verbal descriptions (based on the embeddings of these descriptions in the LLM). Next, we generated code based on their strategy descriptions using GPT-4-Turbo and compared the simulated behaviors from the code to their actual choice behavior, showing that the LLM-generated code predicts choice more accurately than chance other, more stringent, controls. Finally, we also compare the simulated behaviors of generated codes with those from standard algorithms and induct the strategies that this code internally represents. In sum, our study offers a novel approach to modeling human cognitive processes by building code-like cognitive models from introspections, shedding light on the intersection of Artificial Intelligence and Cognitive Sciences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SwUsFTtM9h": {
    "title": "Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated Images",
    "volume": "main",
    "abstract": "With the digital imagery landscape rapidly evolving, image stocks and AI-generated image marketplaces have become central to visual media. Traditional stock images now exist alongside innovative platforms that trade in prompts for AI-generated visuals, driven by sophisticated APIs like DALL-E 3 and Midjourney. This paper studies the possibility of employing multi-modal models with enhanced visual understanding to mimic the outputs of these platforms, introducing an original attack strategy. Our method leverages fine-tuned CLIP models, a multi-label classifier, and the descriptive capabilities of GPT-4V to create prompts that generate images similar to those available in marketplaces and from premium stock image providers, yet at a markedly lower expense. In presenting this strategy, we aim to spotlight a new class of economic and security considerations within the realm of digital imagery. Our findings, supported by both automated metrics and human assessment, reveal that comparable visual content can be produced for a fraction of the prevailing market prices (\\$0.23 - \\$0.27 per image), emphasizing the need for awareness and strategic discussions about the integrity of digital media in an increasingly AI-integrated landscape. Additionally, this approach holds promise as a tool for data augmentation, potentially enhancing machine learning models by providing varied and cost-effective training data. Our work also contributes to the field by assembling a dataset consisting of approximately 19 million prompt-image pairs generated by the popular Midjourney platform, which we plan to release publicly",
    "checked": true,
    "id": "49a04d58d2d51a993593804c99e78396d4ad79ab",
    "semantic_title": "iteratively prompting multimodal llms to reproduce natural and ai-generated images",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=FmhPg4UJ9K": {
    "title": "Counting Like Transformers: Compiling Temporal Counting Logic Into Softmax Transformers",
    "volume": "main",
    "abstract": "Deriving formal bounds on the expressivity of transformers, as well as studying transformers that are constructed to implement known algorithms, are both effective methods for better understanding the computational power of transformers. Towards both ends, we introduce the temporal counting logic $\\textbf{K}_t$[#] alongside the RASP variant $\\textbf{C-RASP}$. We show they are equivalent to each other, and that together they are the best-known lower bound on the formal expressivity of future-masked soft attention transformers with unbounded input size. We prove this by showing all $\\textbf{K}_t$[#] formulas can be compiled into these transformers without any additional positional embeddings",
    "checked": true,
    "id": "52ba35523e01b1146f2519ca31bab52c4a54fe74",
    "semantic_title": "counting like transformers: compiling temporal counting logic into softmax transformers",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=otKo4zFKmH": {
    "title": "Task Success is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors",
    "volume": "main",
    "abstract": "Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is available, can we use large vision and language models (VLMs), which are approximately omniscient, as scalable Behavior Critics to help catch undesirable robot behaviors in videos? To answer this, we first construct a benchmark that contains diverse cases of goal-reaching yet undesirable robot policies. Then, we comprehensively evaluate VLM critics to gain a deeper understanding of their strengths and failure modes. Based on the evaluation, we provide guidelines on how to effectively utilize VLM critiques and showcase a practical way to integrate the feedback into an iterative process of policy refinement. The dataset and codebase are released at: https://guansuns.github.io/pages/vlm-critic",
    "checked": false,
    "id": "4c98e18cf16395b95ffaaeeac3eceffa608dcf8d",
    "semantic_title": "task success\" is not enough: investigating the use of video-language models as behavior critics for catching undesirable agent behaviors",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=4aqq9xTtih": {
    "title": "Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation",
    "volume": "main",
    "abstract": "With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method's simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model's performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.29$\\times$ and 1.25$\\times$ speed-ups in Gemma 7B and Llama 2 13B, respectively, on an NVIDIA L40). Code is available at https://github.com/hdong920/GRIFFIN",
    "checked": true,
    "id": "ef399ed62fcc00e73b02f286012080351652693c",
    "semantic_title": "prompt-prompted adaptive structured pruning for efficient llm generation",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=5Nsl0nlStc": {
    "title": "Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization",
    "volume": "main",
    "abstract": "Fusing knowledge from multiple Large Language Models (LLMs) can combine their diverse strengths to achieve improved performance on a given task. However, current fusion approaches either rely on learning-based fusers that do not generalize to new LLMs, or do not take into account how well each LLM understands the input. In this work, we study LLM fusion at test-time, which enables leveraging knowledge from arbitrary user-specified LLMs during inference. We introduce Pack of LLMs (PackLLM), an effective method for test-time fusion that leverages each LLM's expertise, given an input prompt. PackLLM performs model fusion by solving an optimization problem for determining each LLM's importance, so that perplexity over the input prompt is minimized. First, our simple PackLLM-sim variant validates that perplexity is a good indicator for measuring each LLM's expertise. Second, our PackLLM-opt variant approximately solves the perplexity minimization problem via a greedy algorithm. The derived importance weights are used to combine the LLMs during inference. We conduct experiments with over 100 total LLMs on a diverse set of tasks. Experimental results show that (i) perplexity is a reliable measure for LLM fusion, (ii) PackLLM outperforms test-time fusion baselines by 1.89% accuracy points, (iii) PackLLM can leverage new LLMs to improve performance over learning-based fusion approaches by 3.92–11.94% accuracy points, and (iv) PackLLM benefits over selecting the best or largest model and model merging in certain cases. Our code is provided at [https://github.com/cmavro/PackLLM](https://github.com/cmavro/PackLLM)",
    "checked": true,
    "id": "ecfa3f4802bf0feb6d4ecc5ff1ac19e8da784850",
    "semantic_title": "pack of llms: model fusion at test-time via perplexity optimization",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=46Zgqo4QIU": {
    "title": "Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation",
    "volume": "main",
    "abstract": "Several recent advances in AI systems solve problems by providing a \"scaffolding\" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed \"improver\" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. A variety of self-improvement strategies are proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not full recursive self-improvement. Nonetheless, it demonstrates that a modern language model, GPT-4 in our experiments, is capable of writing code that can call itself to improve itself. We consider concerns around the development of self-improving technologies and evaluate the frequency with which the generated code bypasses a sandbox",
    "checked": true,
    "id": "3fe940a1f121f083cb90c568fc6fa2951bb27dda",
    "semantic_title": "self-taught optimizer (stop): recursively self-improving code generation",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=Zt1dwG8xrK": {
    "title": "Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability",
    "volume": "main",
    "abstract": "While many capabilities of language models (LMs) improve with increased training budget, the influence of scale on hallucinations is not yet fully understood. Hallucinations come in many forms, and there is no universally accepted definition. We thus focus on studying only those hallucinations where a correct answer appears verbatim in the training set. To fully control the training data content, we construct a knowledge graph (KG)-based dataset, and use it to train a set of increasingly large LMs. We find that fora fixed dataset, larger and longer-trained LMs hallucinate less. However, hallucinating on≤5% of the training data requires an order of magnitude larger model, and thus an order of magnitude more compute, than Hoffmann et al. (2022) reported was optimal. Given this costliness, we study how hallucination detectors depend on scale. While we see detector size improves performance on fixed LM's outputs, we find an inverse relationship between the scale of the LM and the detectability of its hallucinations",
    "checked": true,
    "id": "9ff621c179e233b22dde94a01d2a9924a7bc79af",
    "semantic_title": "training language models on the knowledge graph: insights on hallucinations and their detectability",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=uILyEJGKWw": {
    "title": "Does Collaborative Human–LM Dialogue Generation Help Information Extraction from Human–Human Dialogues?",
    "volume": "main",
    "abstract": "The capabilities of pretrained language models (LMs) have opened opportunities to explore new application areas, but applications involving human-human interaction are limited by the fact that most data is protected from public release for privacy reasons. Problem-solving human-human dialogues in real applications can be much more complex than existing Wizard-of-Oz collections, preventing successful domain transfer. To support information extraction (IE) for a private call center dataset (AIC), we introduce a human-in-the-loop dialogue generation framework capable of synthesizing realistic dialogues. In IE experiments with AIC dialogues, we observe 25% relative improvement in F1 after augmenting a small set of real human-human conversations with synthetic data. In controlled experiments, we compare training with our human-in-the-loop-synthesized data vs. fully automatically LM-generated data and find that collaborating humans adds value both in the generation and annotation stages. We release code and our synthetic dataset to illustrate the complexity of call center conversations and encourage development of complex dialogue datasets that are more representative of natural data",
    "checked": false,
    "id": "9699910e7fa8a5cc14ad3f2bb84273f017a99cb1",
    "semantic_title": "does collaborative human-lm dialogue generation help information extraction from human dialogues?",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=wps3p2cqrA": {
    "title": "How Well Do LLMs Identify Cultural Unity in Diversity?",
    "volume": "main",
    "abstract": "Much work on the cultural awareness of large language models (LLMs) focuses on the models' sensitivity to geo-cultural diversity. However, in addition to cross-cultural differences, there also exists common ground across cultures. For instance, a bridal veil in the United States plays a similar cultural-relevant role as a honggaitou in China. In this study, we introduce a benchmark dataset CUNIT for evaluating decoder-only LLMs in understanding the cultural unity of concepts. Specifically, CUNIT consists of 1,425 evaluation examples building upon 285 traditional cultural-specific concepts across 10 countries. Based on a systematic manual annotation of cultural-relevant features per concept, we calculate the cultural association between any pair of cross-cultural concepts. Built upon this dataset, we design a contrastive matching task to evaluate the LLMs' capability to identify highly associated cross-cultural concept pairs. We evaluate 3 strong LLMs, using 3 popular prompting strategies, under the settings of either giving all extracted concept features or no features at all on CUNIT Interestingly, we find that cultural associations across countries regarding clothing concepts largely differ from food. Our analysis shows that LLMs are still limited to capturing cross-cultural associations between concepts compared to humans. Moreover, geo-cultural proximity shows a weak influence on model performance in capturing cross-cultural associations",
    "checked": true,
    "id": "dbc00bd1669f9d378dc78f04a1eef09e70331087",
    "semantic_title": "how well do llms identify cultural unity in diversity?",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=LWfDcI6txJ": {
    "title": "Forklift: An Extensible Neural Lifter",
    "volume": "main",
    "abstract": "The escalating demand to migrate legacy software across different Instruction Set Architectures (ISAs) has driven the development of assembly-to-assembly translators to map between their respective assembly languages. However, the development of these tools requires substantial engineering effort. State-of-the-art approaches use lifting, a technique where source assembly code is translated to an architecture-independent intermediate representation (IR) — for example, the LLVM IR — and use a pre-existing compiler to recompile the IR to the target ISA. However, the hand-written rules these lifters employ are sensitive to the particular compiler and optimization level used to generate the code and require significant engineering effort to support each new ISA. We propose Forklift, the first neural lifter that learns how to translate assembly to LLVM IR using a token-level encoder-decoder Transformer. We show how to incrementally add support to new ISAs by fine tuning the assembly encoder and freezing the IR decoder, improving the overall accuracy and efficiency. We collect millions of parallel LLVM IR, x86, ARM, and RISC-V programs across compilers and optimization levels to train Forklift and set up an input/output-based accuracy harness. We evaluate Forklift on two challenging benchmark suites and translate 2.5x more x86 programs than a state-of-the-art hand-written lifter and 4.4x more x86 programs than GPT-4 as well as enabling translation from new ISAs",
    "checked": true,
    "id": "3d65c9efae6f2c275ace52e254ade14a1df8292d",
    "semantic_title": "forklift: an extensible neural lifter",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=bttKwCZDkm": {
    "title": "Benchmarks as Microscopes: A Call for Model Metrology",
    "volume": "main",
    "abstract": "Modern language models (LMs) pose a new challenge in capability assessment. Static benchmarks inevitably saturate without providing confidence in the deployment tolerances of LM-based systems, but developers nonetheless claim that their models have generalized traits such as reasoning or open-domain language understanding based on these flawed metrics. The science and practice of LMs requires a new approach to benchmarking which measures specific capabilities with dynamic assessments. To be confident in our metrics, we need a new discipline of *model metrology*---one which focuses on how to generate benchmarks that predict performance under deployment. Motivated by our evaluation criteria, we outline how building a community of model metrology practitioners---one focused on building tools and studying how to measure system capabilities---is the best way to meet these needs to and add clarity to the AI discussion",
    "checked": true,
    "id": "818e2e4a5d4efd460a4b1148bc2cdd571e4e7470",
    "semantic_title": "benchmarks as microscopes: a call for model metrology",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=qyilOnIRHI": {
    "title": "Implicit Geometry of Next-token Prediction: From Language Sparsity Patterns to Model Representations",
    "volume": "main",
    "abstract": "Next-token prediction (NTP) over large text corpora has become the go-to paradigm to train large language models. Yet, it remains unclear how NTP influences the mapping of linguistic patterns to geometric properties of the resulting model representations. We frame training of large language models as soft-label classification over sparse probabilistic label vectors, coupled with an analytical approximation that allows unrestricted generation of context embeddings. This approach links NTP training to rank-constrained, nuclear-norm regularized optimization in the logit domain, offering a framework for analyzing the geometry of word and context embeddings. In large embedding spaces, we find that NTP implicitly favors learning logits with a sparse plus low-rank structure. While the sparse component captures the co-occurrence frequency of context-word pairs, the orthogonal low-rank component, which becomes dominant as training progresses, depends solely on the sparsity pattern of the co-occurrence matrix. Consequently, when projected onto an appropriate subspace, representations of contexts that are followed by the same set of next-tokens collapse—a phenomenon we term subspace-collapse. We validate our theory on synthetic and small-scale real language datasets. Finally, we outline potential research directions aimed at deepening the understanding of NTP's influence on the learning of linguistic patterns and regularities",
    "checked": true,
    "id": "759abef16bfe18168796051d5a73d7f6873b8a25",
    "semantic_title": "implicit geometry of next-token prediction: from language sparsity patterns to model representations",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=NPAQ6FKSmK": {
    "title": "Autonomous Evaluation and Refinement of Digital Agents",
    "volume": "main",
    "abstract": "We show that domain-general automatic evaluators can significantly improve the performance of agents for web navigation and device control. We experiment with multiple evaluation models that trade off between inference cost, modularity of design, and accuracy. We validate the performance of these models in several popular benchmarks for digital agents, finding between 74.4 and 92.9% agreement with oracle evaluation metrics. Finally, we use these evaluators to improve the performance of existing agents via fine-tuning and inference-time guidance. Without any additional supervision, we improve state-of-the-art performance by 29% on the popular benchmark WebArena, and achieve around 75% relative improvement in device control settings. We release our code and data at [https://github.com/Berkeley-NLP/Agent-Eval-Refine](https://github.com/Berkeley-NLP/Agent-Eval-Refine)",
    "checked": true,
    "id": "aed8f01122d1a89c43900e995c80bfda7936568e",
    "semantic_title": "autonomous evaluation and refinement of digital agents",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=ZDdLamBX4P": {
    "title": "Cookbook: A framework for improving LLM generative abilities via programmatic data generating templates",
    "volume": "main",
    "abstract": "Fine-tuning large language models (LLMs) on instruction datasets is a common way to improve their generative capabilities. However, instruction datasets can be expensive and time-consuming to manually curate, and while LLM-generated data is less labor-intensive, it may violate user privacy agreements or terms of service of LLM providers. Therefore, we seek a way of constructing instruction datasets with samples that are not generated by humans or LLMs but still improve LLM generative capabilities. In this work, we introduce Cookbook, a framework that programmatically generates training data consisting of simple patterns over random tokens, resulting in a scalable, cost-effective approach that avoids legal and privacy issues. First, Cookbook uses a template---a data generating Python function---to produce training data that encourages the model to learn an explicit pattern-based rule that corresponds to a desired task. We find that fine-tuning on Cookbook-generated data is able to improve performance on its corresponding task by up to 52.7 accuracy points. Second, since instruction datasets improve performance on multiple downstream tasks simultaneously, Cookbook algorithmically learns how to mix data from various templates to optimize performance on multiple tasks. On the standard multi-task GPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated dataset attains the best accuracy on average compared to other 7B parameter instruction-tuned models and is the best performing model on 3 out of 8 tasks. Finally, we analyze when and why Cookbook improves performance and present a metric that allows us to verify that the improvement is largely explained by the model's generations adhering better to template rules",
    "checked": true,
    "id": "25e370ee9ddd4d07354f132e52dbb657f8d3186f",
    "semantic_title": "cookbook: a framework for improving llm generative abilities via programmatic data generating templates",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Ecgev5ZZpq": {
    "title": "Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models",
    "volume": "main",
    "abstract": "With the emergence of large language models, such as LLaMA and OpenAI GPT-3, In-Context Learning (ICL) gained significant attention due to its effectiveness and efficiency. However, ICL is very sensitive to the choice, order, and verbaliser used to encode the demonstrations in the prompt. \\emph{Retrieval-Augmented ICL} methods try to address this problem by leveraging retrievers to extract semantically related examples as demonstrations. While this approach yields more accurate results, its robustness against various types of adversarial attacks, including perturbations on test samples, demonstrations, and retrieved data, remains under-explored. Our study reveals that retrieval-augmented models can enhance robustness against test sample attacks, outperforming vanilla ICL with a 4.87\\% reduction in Attack Success Rate (ASR); however, they exhibit overconfidence in the demonstrations, leading to a 2\\% increase in ASR for demonstration attacks. Adversarial training can help improve the robustness of ICL methods to adversarial attacks; however, such a training scheme can be too costly in the context of LLMs. As an alternative, we introduce an effective training-free adversarial defence method, \\emph{DARD}, which enriches the example pool with those attacked samples. We show that DARD yields improvements in performance and robustness, achieving a 15\\% reduction in ASR over the baselines. Code and data are available jointly with this submission as supplementary material",
    "checked": true,
    "id": "4dc32be4b712a4b0c65285b977968ff88a8a5f43",
    "semantic_title": "evaluating the adversarial robustness of retrieval-based in-context learning for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bnscREWUuc": {
    "title": "How Multilingual are Large Language Models Fine-tuned for Translation?",
    "volume": "main",
    "abstract": "A new paradigm for machine translation has recently emerged: fine-tuning large language models on parallel text has been shown to outperform dedicated translation systems trained in a supervised fashion on much larger amounts of parallel data (Xu et al. 2024, Alves et al. 2024). However, it remains unclear whether this paradigm can enable massively multilingual machine translation or whether it requires fine-tuning dedicated models for a small number of language pairs. How does translation fine-tuning impact the MT capabilities of LLMs for zero-shot languages, zero-shot language pairs, and translation tasks that do not involve English? To address these questions, we conduct an extensive empirical evaluation of the translation quality of the TOWER family of language models (Alves et al. 2024) on 132 translation tasks from the multi-parallel FLORES data. We find that translation fine-tuning improves translation quality even for zero-shot languages on average, but that the impact is uneven depending on the language pairs involved. These results call for further research to effectively enable massively multilingual translation with LLMs",
    "checked": true,
    "id": "f94155b751d2b01ce86bee0f6fcf57823508c425",
    "semantic_title": "how multilingual are large language models fine-tuned for translation?",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=ootI3ZO6TJ": {
    "title": "PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models",
    "volume": "main",
    "abstract": "Recent advances in large language models (LLMs) have led to their extensive global deployment, and ensuring their safety calls for comprehensive and multilingual toxicity evaluations. However, existing toxicity benchmarks are overwhelmingly focused on English, posing serious risks to deploying LLMs in other languages. We address this by introducing PolygloToxicityPrompts (PTP), the first large-scale multilingual toxicity evaluation benchmark of 425K naturally-occurring prompts spanning 17 languages. We overcome the scarcity of naturally occurring toxicity in web-text and ensure coverage across languages with varying resources by automatically scraping over 100M web-text documents. Using PTP, we investigate research questions to study the impact of model size, prompt language, and instruction and preference-tuning methods on toxicity by benchmarking over 60 LLMs. Notably, we find that toxicity increases as language resources decrease or model size increases. Although instruction- and preference-tuning reduce toxicity, the choice of preference-tuning method does not have any significant impact. Our findings shed light on crucial shortcomings of LLM safeguarding and highlight areas for future research",
    "checked": true,
    "id": "87c0b759783d919539860a0de7bb13fd0879687e",
    "semantic_title": "polyglotoxicityprompts: multilingual evaluation of neural toxic degeneration in large language models",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=tIpWtMYkzU": {
    "title": "Trust No Bot: Discovering Personal Disclosures in Human-LLM Conversations in the Wild",
    "volume": "main",
    "abstract": "Measuring personal disclosures made in human-chatbot interactions can provide a better understanding of users' AI literacy and facilitate privacy research for large language models (LLMs). We run an extensive, fine-grained analysis on the personal disclosures made by real users to commercial GPT models, investigating the leakage of personally identifiable and sensitive information. To understand the contexts in which users disclose to chatbots, we develop a taxonomy of tasks and sensitive topics, based on qualitative and quantitative analysis of naturally occurring conversations. We discuss these potential privacy harms and observe that: (1) personally identifiable information (PII) appears in unexpected contexts such as in translation or code editing (48% and 16% of the time, respectively) and (2) PII detection alone is insufficient to capture the sensitive topics that are common in human-chatbot interactions, such as detailed sexual preferences or specific drug use habits. We believe that these high disclosure rates are of significant importance for researchers and data curators, and we call for the design of appropriate nudging mechanisms to help users moderate their interactions",
    "checked": true,
    "id": "10ebf46ee0e51f7859541035bc97613441c804e7",
    "semantic_title": "trust no bot: discovering personal disclosures in human-llm conversations in the wild",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=GMalvQu0XL": {
    "title": "RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models",
    "volume": "main",
    "abstract": "In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of existing models and identify their limitations in in-context learning, primarily due to a mismatch between pretraining and inference, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training. Through extensive experiments, we demonstrate that our simple yet effective design significantly improves performance, achieving results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder language models for in-context learning and encourages further research in this direction",
    "checked": false,
    "id": "76c8e90dfd0f1e78e6a94d702a5b14b3e7206003",
    "semantic_title": "raven: in-context learning with retrieval augmented encoder-decoder language models",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=kLH4ccaL21": {
    "title": "GeniL: A Multilingual Dataset on Generalizing Language",
    "volume": "main",
    "abstract": "Generative language models are increasingly transforming our digital ecosystem, but they often inherit societal biases learned from their training data, for instance stereotypes associating certain attributes with specific identity groups. While whether and how these biases are mitigated may depend on the specific use cases, being able to effectively detect instances of stereotype perpetuation is a crucial first step. Current methods to assess presence of stereotypes in generated language rely on simple template or co-occurrence based measures, without accounting for the variety of sentential contexts they manifest in. We argue that the sentential context is crucial to determine if the co-occurrence of an identity term and an attribute is an instance of generalization. We distinguish two types of generalizations ---(1) where the language merely mentions the presence of a generalization (e.g., \"people think the French are very rude\"), and (2) where the language reinforces such a generalization (e.g., \"as French they must be rude\"---, from a non-generalizing context (e.g., \"My French friends think I am rude\"). For meaningful stereotype evaluations, we need scalable ways to reliably detect and distinguish such instances of generalizations. To address this gap, we introduce the new task of detecting generalization in language, and build GeniL, a multilingual dataset of over 50K sentences from 9 languages ---English, Arabic, Bengali, Spanish, French, Hindi, Indonesian, Malay, and Portuguese--- annotated for instances of generalizations and their types. We demonstrate that the likelihood of a co-occurrence being an instance of generalization is usually low, and varies across different languages, identity groups, and attributes, underscoring the inadequacy of simplistic co-occurrence based approaches. We also build classifiers that can detect generalization in language with an overall PR-AUC of 58.7, with varying degrees of performance across languages. Our research provides data and tools to enable a nuanced understanding of stereotype perpetuation, a crucial step towards more inclusive and responsible language technologies",
    "checked": true,
    "id": "97db70178d57ddafce90b1bff69a2684b42fd40f",
    "semantic_title": "genil: a multilingual dataset on generalizing language",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=zl16jLb91v": {
    "title": "Towards Measuring the Representation of Subjective Global Opinions in Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural stereotypes. When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily become the most similar to the opinions of speakers of those languages. We will release our dataset for others to use and build on upon acceptance",
    "checked": true,
    "id": "534c58762e69d7afbcb0f6a7e53c07484f6d4891",
    "semantic_title": "towards measuring the representation of subjective global opinions in language models",
    "citation_count": 271,
    "authors": []
  },
  "https://openreview.net/forum?id=XGJBEeziEb": {
    "title": "Data Checklist: On Unit-Testing Datasets with Usable Information",
    "volume": "main",
    "abstract": "Model checklists (Ribeiro et al., 2020) have emerged as a useful tool for understanding the behavior of LLMs, analogous to unit-testing in software engineering. However, despite datasets being a key determinant of model behavior, evaluating datasets -- e.g., for the existence of annotation artifacts -- is largely done ad hoc, once a problem in model behavior has already been found downstream. In this work, we take a more principled approach to unit-testing datasets by proposing a taxonomy based on the $\\mathcal{V}$-information literature. We call a collection of such unit tests a data checklist. Using the checklist, not only are we able to recover known artifacts in well-known datasets such as SNLI, but we also discover previously unknown artifacts in preference datasets for LLM alignment. Data checklists further enable a new kind of data filtering, which we use to improve the efficacy and data efficiency of preference alignment",
    "checked": true,
    "id": "62f25b86bd88a7cfca6625113f287b1d409845da",
    "semantic_title": "data checklist: on unit-testing datasets with usable information",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OJaWBhh61C": {
    "title": "Best Practices and Lessons Learned on Synthetic Data",
    "volume": "main",
    "abstract": "The success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs. Synthetic data has emerged as a promising solution by generating artificial data that mimics real-world patterns. This paper provides an overview of synthetic data research, discussing its applications, challenges, and future directions. We present empirical evidence from prior art to demonstrate its effectiveness and highlight the importance of ensuring its factuality, fidelity, and unbiasedness. We emphasize the need for responsible use of synthetic data to build more powerful, inclusive, and trustworthy language models",
    "checked": false,
    "id": "f9fc7b5ebd5fe30257abb03ad87d8138eeeb28d9",
    "semantic_title": "best practices and lessons learned on synthetic data for language models",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=TrloAXEJ2B": {
    "title": "LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition",
    "volume": "main",
    "abstract": "Low-rank adaptation (LoRA) is often employed to fine-tune large language models (LLMs) for new tasks. This paper investigates LoRA composability for cross-task generalization and introduces LoraHub, a simple framework devised for the purposive assembly of LoRA modules trained on diverse given tasks, with the objective of achieving adaptable performance on unseen tasks. With just a few examples from a new task, LoraHub can fluidly combine multiple LoRA modules, eliminating the need for human expertise and assumptions. Notably, the composition requires neither additional model parameters nor gradients. Empirical results on the Big-Bench Hard benchmark suggest that LoraHub, while not surpassing the performance of in-context learning, offers a notable performance-efficiency trade-off in few-shot scenarios by employing a significantly reduced number of tokens per example during inference. Notably, LoraHub establishes a better upper bound compared to in-context learning when paired with different demonstration examples, demonstrating its potential for future development. Our vision is to establish a platform for LoRA modules, empowering users to share their trained LoRA modules. This collaborative approach facilitates the seamless application of LoRA modules to novel tasks, contributing to an adaptive ecosystem",
    "checked": true,
    "id": "3f459219d75de63b5b7a26a8c6447ec1e79a985c",
    "semantic_title": "lorahub: efficient cross-task generalization via dynamic lora composition",
    "citation_count": 241,
    "authors": []
  },
  "https://openreview.net/forum?id=RLFca3arx7": {
    "title": "CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias",
    "volume": "main",
    "abstract": "As language models (LMs) become increasingly powerful and widely used, it is important to quantify them for sociodemographic bias with potential for harm. Prior measures of bias are sensitive to perturbations in the templates designed to compare performance across social groups, due to factors such as low diversity or limited number of templates. Also, most previous work considers only one NLP task. We introduce Comprehensive Assessment of Language Models (CALM) for robust measurement of social biases. We use sixteen datasets for question-answering, sentiment analysis and natural language inference and filter them to produce 224 templates with high diversity (e.g., length, vocabulary). This helps us create a novel dataset of 78,400 prompts covering the three NLP tasks. Our empirical evaluation shows that CALM bias scores are more robust and far less sensitive than previous bias measurements to perturbations in the templates, such as synonym substitution, or to random subset selection of templates. We apply CALM to 20 large language models, and find that for 2 LM series, larger parameter models tend to be more biased than smaller ones. The T0 series is the least biased model families, of the 20 LLMs investigated here",
    "checked": true,
    "id": "d2e57d6ba6989ed3e76884f3d0e0e77ba3118528",
    "semantic_title": "calm : a multi-task benchmark for comprehensive assessment of language model bias",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=aKkAwZB6JV": {
    "title": "Zephyr: Direct Distillation of LM Alignment",
    "volume": "main",
    "abstract": "We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, set a new state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpassed Llama2-Chat-70B, at the time the best open-access RLHF-based model",
    "checked": true,
    "id": "cdcf3f36866ef1e16eba26d57c2324362247ba84",
    "semantic_title": "zephyr: direct distillation of lm alignment",
    "citation_count": 431,
    "authors": []
  },
  "https://openreview.net/forum?id=Zb0ajZ7vAt": {
    "title": "Large Language Model Routing with Benchmark Datasets",
    "volume": "main",
    "abstract": "The number of open-source Large Language Models (LLMs) grows daily, as does the number of available benchmark datasets used to evaluate LLMs. While some models dominate these benchmarks, no single model achieves the best accuracy in all tasks and use cases. In light of this observation, we address the challenge of selecting the best LLM from a collection of pre-trained models, given a new task. While related work relies on evaluating each candidate model on a set of labeled examples, our new formulation does not assume any labeled data from the new task is available. Instead, we repurpose a collection of benchmark datasets---which may focus on different tasks than the one at hand---to learn a ''router'' model for LLM selection from inputs only; this problem reduces to a collection of binary classification tasks. Empirically, our strategy consistently improves performance over using any single model for all tasks",
    "checked": true,
    "id": "50176544c46b3226a05e7946f6e36ac31c68faf7",
    "semantic_title": "large language model routing with benchmark datasets",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=vL8BIGuFTF": {
    "title": "Predicting Emergent Capabilities by Finetuning",
    "volume": "main",
    "abstract": "A fundamental open challenge in modern LLM scaling is the lack of understanding around emergent capabilities. In particular, language model pretraining loss is known to be highly predictable as a function of compute. However, downstream capabilities are far less predictable---sometimes even exhibiting emergent jumps---which makes it challenging to anticipate the capabilities of future models. In this work, we first pose the task of emergence prediction: given access to current LLMs that have random few-shot accuracy on a task, can we predict whether future models (GPT-N+1) will have non-trivial accuracy on that task? We then discover a simple insight for this problem: directly finetuning LLMs on a given task can shift the point in scaling at which emergence occurs towards less capable models. To operationalize this insight, we can finetune LLMs with varying amounts of data and fit a parametric function that predicts when emergence will occur (i.e., ``emergence laws''). To validate this approach, we use four standard NLP benchmarks where large-scale open-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and CoLA). Using only small-scale LLMs, we find that, in some cases, we are able to accurately predict whether models trained with up to 4x more compute have emerged",
    "checked": true,
    "id": "3d6f4c281bd3c95a40f67393d668910ea190ce5a",
    "semantic_title": "predicting emergent capabilities by finetuning",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=V7HRrxXUhN": {
    "title": "An In-Context Learning Agent for Formal Theorem-Proving",
    "volume": "main",
    "abstract": "We present an in-context learning agent for formal theorem-proving in environments like Lean and Coq. Current state-of-the-art models for the problem are finetuned on environment-specific proof data. By contrast, our approach, called COPRA, repeatedly asks a high-capacity, general-purpose large language model (GPT-4) to propose tactic applications from within a stateful backtracking search. Proposed tactics are executed in the underlying proof environment. Feedback from the execution is used to build the prompt for the next model query, along with selected information from the search history and lemmas retrieved from an external database. We evaluate our implementation of COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the CompCert project. On these benchmarks, COPRA significantly outperforms few-shot invocations of GPT-4. It also compares favorably against finetuning-based approaches, outperforming REPROVER, a state-of-the-art finetuned approach for Lean, in terms of the pass@1 metric. Our code and data are available at https://github.com/trishullab/copra",
    "checked": true,
    "id": "ee71447d68f3d8666c974b8199e330e19aebf263",
    "semantic_title": "an in-context learning agent for formal theorem-proving",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=S4ZOkV1AHl": {
    "title": "Evaluating Cultural Adaptability of a Large Language Model via Simulation of Synthetic Personas",
    "volume": "main",
    "abstract": "The success of Large Language Models (LLMs) in multicultural environments hinges on their ability to understand users' diverse cultural backgrounds. We measure this capability by having an LLM simulate human profiles representing various nationalities within the scope of a questionnaire-style psychological experiment. Specifically, we employ GPT-3.5 to reproduce reactions to persuasive news articles of 7,286 participants from 15 countries; comparing the results with a dataset of real participants sharing the same demographic traits. Our analysis shows that specifying a person's country of residence improves GPT-3.5's alignment with their responses. In contrast, using native language prompting introduces shifts that significantly reduce overall alignment, with some languages particularly impairing performance. These findings suggest that while direct nationality information enhances the model's cultural adaptability, native language cues do not reliably improve simulation fidelity and can detract from the model's effectiveness",
    "checked": true,
    "id": "ba73f425dc7e815991f9b547227c7a674e9e000a",
    "semantic_title": "evaluating cultural adaptability of a large language model via simulation of synthetic personas",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=pUEDkZyPDl": {
    "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training",
    "volume": "main",
    "abstract": "FlashAttention effectively reduces the quadratic peak memory usage to linear in training transformer-based large language models (LLMs) on a single GPU. In this paper, we introduce DistFlashAttention, a distributed memory-efficient attention mechanism optimized for long-context LLMs training. We propose three key techniques: token-level workload balancing, overlapping key-value communication, and a rematerialization-aware gradient checkpointing algorithm. We evaluate DistFlashAttention on Llama-7B and variants with sequence lengths from 32K to 512K. DistFlashAttention achieves 8x longer sequences, 4.45 - 5.64x speedup compared to Ring Self-Attention, 2-8x longer sequences, 1.24- 2.01x speedup compared to Megatron-LM with FlashAttention. It achieves 1.67x and 1.26-1.88x speedup compared to recent Ring Attention and DeepSpeed-Ulysses. Codes are available at https://github.com/RulinShao/LightSeq",
    "checked": true,
    "id": "8511ea96d61593de57cbc2e996910e5cb3dbfe84",
    "semantic_title": "distflashattn: distributed memory-efficient attention for long-context llms training",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=LzpaUxcNFK": {
    "title": "From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples",
    "volume": "main",
    "abstract": "We analyze how well pre-trained large language models (e.g., Llama2, GPT-4, Claude 3, etc) can do linear and non-linear regression when given in-context examples, without any additional training or gradient updates. Our findings reveal that several large language models (e.g., GPT-4, Claude 3) are able to perform regression tasks with a performance rivaling (or even outperforming) that of traditional supervised methods such as Random Forest, Bagging, or Gradient Boosting. For example, on the challenging Friedman \\#2 regression dataset, Claude 3 outperforms many supervised methods such as AdaBoost, SVM, Random Forest, KNN, or Gradient Boosting. We then investigate how well the performance of large language models scales with the number of in-context exemplars. We borrow from the notion of regret from online learning and empirically show that LLMs are capable of obtaining a sub-linear regret",
    "checked": true,
    "id": "8c576a9352904c79d79edd386e63eab13c7bc8f8",
    "semantic_title": "from words to numbers: your large language model is secretly a capable regressor when given in-context examples",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=tEYskw1VY2": {
    "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
    "volume": "main",
    "abstract": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5x higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation",
    "checked": true,
    "id": "7bbc7595196a0606a07506c4fb1473e5e87f6082",
    "semantic_title": "mamba: linear-time sequence modeling with selective state spaces",
    "citation_count": 3439,
    "authors": []
  },
  "https://openreview.net/forum?id=1pgfvZj0Rx": {
    "title": "Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation",
    "volume": "main",
    "abstract": "Amidst the growing interest in developing task-autonomous AI for automated mental health care, this paper addresses the ethical and practical challenges associated with the issue and proposes a structured framework that delineates levels of autonomy, outlines ethical requirements, and defines beneficial default behaviors for AI agents in the context of mental health support. We also evaluate fourteen state-of-the-art language models (ten off-the-shelf, four fine-tuned) using 16 mental health-related questions designed to reflect various mental health conditions, such as psychosis, mania, depression, suicidal thoughts, and homicidal tendencies. The question design and response evaluations were conducted by mental health clinicians (M.D.s). We find that existing language models are insufficient to match the standard provided by human professionals who can navigate nuances and appreciate context. This is due to a range of issues, including overly cautious or sycophantic responses and the absence of necessary safeguards. Alarmingly, we find that most of the tested models could cause harm if accessed in mental health emergencies, failing to protect users and potentially exacerbating existing symptoms. We explore solutions to enhance the safety of current models. Before the release of increasingly task-autonomous AI systems in mental health, it is crucial to ensure that these models can reliably detect and manage symptoms of common psychiatric disorders to prevent harm to users. This involves aligning with the ethical framework and default behaviors outlined in our study. We contend that model developers are responsible for refining their systems per these guidelines to safeguard against the risks posed by current AI technologies to user mental health and safety. Trigger warning: Contains and discusses examples of sensitive mental health topics, including suicide and self-harm",
    "checked": true,
    "id": "9fd65c623d319dd21b99d67fcca2b4b4f2717ec5",
    "semantic_title": "risks from language models for automated mental healthcare: ethics and structure for implementation",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=uUIFTjBREk": {
    "title": "Efficient Hybrid Long Sequence Modeling with State Space Augmented Transformers",
    "volume": "main",
    "abstract": "Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for State Space Augmented Transformer. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks",
    "checked": false,
    "id": "661e8d555c4424b5953f17434f2ba910bfcf3afe",
    "semantic_title": "efficient long sequence modeling via state space augmented transformer",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=kEVcNxtqXk": {
    "title": "From r to Q ∗ : Your Language Model is Secretly a Q-Function",
    "volume": "main",
    "abstract": "Reinforcement Learning From Human Feedback (RLHF) has been a critical component of the success of the latest generation of generative AI models, including the GPT series. However, this is an involved and complex process and direct alignment algorithms, such as DPO have recently emerged as an alternative approach to the classical RLHF pipeline. Although DPO solves the same objective as the standard RLHF setup, there is a mismatch between the two approaches. Standard RLHF deploys reinforcement learning in a specific token-level MDP, while DPO is derived as a bandit problem in which the whole response of the model is treated as a single arm. In this work we rectify this difference, first we theoretically show that we can derive DPO in the token-level MDP as a general inverse Q-learning algorithm, which satisfies the Bellman equation. Using our theoretical results, we provide three concrete empirical insights. First, we show that because of its token level interpretation, DPO is able to perform some type of credit assignment. Next, we prove that under the token level formulation, classical search-based algorithms, such as MCTS, which have recently been applied to the language generation space, are equivalent to likelihood-based search on a DPO policy and empirically we show that a simple beam search yields meaningful improvement over the base DPO policy. Finally, we show how the choice of SFT policy causes implicit rewards to decline during training. We conclude by discussing applications of our work, including information elicitation in multi-tun dialogue, reasoning, agentic applications and end-to-end training of multi-model systems",
    "checked": false,
    "id": "4b46f71392ac7a323dbe28a9b839818819e8af66",
    "semantic_title": "from r to q*: your language model is secretly a q-function",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b0y6fbSUG0": {
    "title": "LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
    "volume": "main",
    "abstract": "Reasoning is a pivotal skill in the evolution of Large Language Models (LLMs), and constructing step-by-step reasoning chains is essential for enhancing their reasoning abilities. Despite a rich array of recent research aimed at deriving improved reasoning chains from LLMs, two major challenges hinder the progress in this field: the lack of effective methods to evaluate reasoning chains, and the absence of systematic analysis of reasoning algorithms. In this work, we introduce RICE, a novel LLM-based approach for automated evaluation of reasoning chains, which autonomously constructs a detailed evaluation criteria list to help itself recognize intermediate reason- ing mistakes. This fully automatic method proves to be more precise than existing metrics and offers a complementary angle to conventional answer-based evaluations. For the second challenge, we present a formulation that connects extensive existing reasoning algorithms. LLM Reasoners, a modular library for step-by-step reasoning algorithms, is developed based on the formulation. It enables users to specify problem domains and reasoning strategies with minimal effort. With the help of the new metric and library, we make a comprehensive study of the factors contributing to a reasoning algorithm, including the reward, the exploration strategy, the world model, and the prompt format, with interesting findings unveiled through RICE",
    "checked": true,
    "id": "b4f5d23f139e65a90c5806391e5d22a1cc3df248",
    "semantic_title": "llm reasoners: new evaluation, library, and analysis of step-by-step reasoning with large language models",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=IW1PR7vEBf": {
    "title": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders",
    "volume": "main",
    "abstract": "Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 4 popular LLMs ranging from 1.3B to 8B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data (as of May 24, 2024). Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data",
    "checked": true,
    "id": "18594d3f15b39385bace39657d14f0c8c7479db1",
    "semantic_title": "llm2vec: large language models are secretly powerful text encoders",
    "citation_count": 290,
    "authors": []
  },
  "https://openreview.net/forum?id=k2xZYPZo34": {
    "title": "Bring Your Own Data! Self-Sensitivity Evaluation for Large Language Models",
    "volume": "main",
    "abstract": "With the rise of Large Language Models (LLMs) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. For example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set. To alleviate these issues in traditional evaluation, we propose a complementary framework for additional self-sensitivity evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text. Self-sensitivity evaluation can directly monitor LLM behavior on datasets collected in-the-wild or streamed during live model deployment. We demonstrate self-sensitivity evaluation strategies for measuring closed-book knowledge, toxicity, long-range context dependence, in addition to sensitivity to grammatical structure and tokenization errors. When comparisons to similar human-labeled benchmarks are available, we find strong correlations between self-sensitivity and human-supervised evaluations. The self-sensitivity paradigm complements current evaluation strategies that rely on labeled data",
    "checked": false,
    "id": "dc2b36fb20490c0d540e01e74efd868d2e17faa3",
    "semantic_title": "bring your own data! self-supervised evaluation for large language models",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=pKMxO0wBYZ": {
    "title": "Web Retrieval Agents for Evidence-Based Misinformation Detection",
    "volume": "main",
    "abstract": "This paper develops an agent-based automated fact-checking approach for detecting misinformation. We demonstrate that combining a powerful LLM agent, which does not have access to the internet for searches, with an online web search agent yields better results than when each tool is used independently. Our approach is robust across multiple models, outperforming alternatives and increasing the macro F1 of misinformation detection by as much as 20 percent compared to LLMs without search. We also conduct extensive analyses on the sources our system leverages and their biases, decisions in the construction of the system like the search tool and the knowledge base, the type of evidence needed and its impact on the results, and other parts of the overall process. By combining strong performance with in-depth understanding, we hope to provide building blocks for future search-enabled misinformation mitigation systems",
    "checked": true,
    "id": "114c5164ea8e70b0a931a571e24ebee6357c1f81",
    "semantic_title": "web retrieval agents for evidence-based misinformation detection",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=kWnlCVcp6o": {
    "title": "Crystal: Illuminating LLM Abilities on Language and Code",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) specializing in code generation (which are also often referred to as code LLMs), e.g., StarCoder and Code Llama, play increasingly critical roles in various software development scenarios. It is also crucial for code LLMs to possess both code generation and natural language abilities for many specific applications, such as code snippet retrieval using natural language or code explanations. The intricate interaction between acquiring language and coding skills complicates the development of strong code LLMs. Furthermore, there is a lack of thorough prior studies on the LLM pretraining strategy that mixes code and natural language. In this work, we propose a pretraining strategy to enhance the integration of natural language and coding capabilities within a single LLM. Specifically, it includes two phases of training with appropriately adjusted code/language ratio. The resulting model, CRYSTAL, demonstrates remarkable capabilities in both domains. Specifically, it has natural language and coding performance comparable to that of Llama 2 and Code Llama, respectively. CRYSTAL exhibits better data efficiency, using 1.4 trillion tokens compared to the more than 2 trillion tokens used by Llama 2 and Code Llama. We verify our pretraining strategy by analyzing the training process and observe consistent improvements in most benchmarks. We also adopted a typical application adaption phase with a code-centric data mixture, only to find out that it did not lead to enhanced performance or training efficiency, underlining the importance of a carefully designed data recipe. To foster research within the community, we commit to open-sourcing every detail of the pretraining, including our training datasets, code, loggings and 136 checkpoints throughout the training",
    "checked": true,
    "id": "105271639df3594fa39ab68e5af4197d695a7d8d",
    "semantic_title": "crystal: illuminating llm abilities on language and code",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=kIoBbc76Sy": {
    "title": "RULER: What's the Real Context Size of Your Long-Context Language Models?",
    "volume": "main",
    "abstract": "The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve a piece of information (the \"needle\") from long distractor texts (the \"haystack\"), has been widely adopted to evaluate long-context language models (LMs). However, this simple retrieval-based test is indicative of only a superficial form of long-context understanding. To provide a more comprehensive evaluation of long-context LMs, we create a new synthetic benchmark RULER with flexible configurations for customized sequence length and task complexity. RULER expands upon the vanilla NIAH test to encompass variations with diverse types and quantities of needles. Moreover, RULER introduces new task categories multi-hop tracing and aggregation to test behaviors beyond searching from context. We evaluate 17 long-context LMs with 13 representative tasks in RULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, almost all models exhibit large performance drops as the context length increases. While these models all claim context sizes of 32K tokens or greater, only half of them can maintain satisfactory performance at the length of 32K. Our analysis of Yi-34B, which supports context length of 200K, reveals large room for improvement as we increase input length and task complexity. We open source RULER to spur comprehensive evaluation of long-context LMs",
    "checked": true,
    "id": "ac5824e9ff924a937d9eef379d0b581de2417678",
    "semantic_title": "ruler: what's the real context size of your long-context language models?",
    "citation_count": 370,
    "authors": []
  },
  "https://openreview.net/forum?id=rzQGHXNReU": {
    "title": "RAFT: Adapting Language Model to Domain Specific RAG",
    "volume": "main",
    "abstract": "Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally incorporate new information into the pretrained model either through RAG-based-prompting, or finetuning. However, the best methodology to incorporate information remains an open question. In this paper, we present Retrieval Augmented Fine Tuning (RAFT), a training recipe which improves the model's ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document to help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG",
    "checked": true,
    "id": "fdefb6a9b51c742d71740d25a76973116a2e0893",
    "semantic_title": "raft: adapting language model to domain specific rag",
    "citation_count": 235,
    "authors": []
  },
  "https://openreview.net/forum?id=YfHxQSoaWU": {
    "title": "FABLES: Evaluating faithfulness and content selection in book-length summarization",
    "volume": "main",
    "abstract": "While long-context large language models (LLMs) can technically summarize book-length documents (> 100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: CLAUDE-3-OPUS significantly outperforms all closedsource LLMs, while the open-source MIXTRAL is on par with GPT-3.5-TURBO. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book. We release FABLES to spur further research on the evaluation of book-length summarization",
    "checked": true,
    "id": "ba4548c01a8b29a406f269db80f7105b88ca9751",
    "semantic_title": "fables: evaluating faithfulness and content selection in book-length summarization",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=vwIIAot0ff": {
    "title": "Does your data spark joy? Performance gains from domain upsampling at the end of training",
    "volume": "main",
    "abstract": "Pretraining datasets for large language models (LLMs) have grown to trillions of tokens composed of large amounts of CommonCrawl (CC) web scrape along with smaller, domain-specific datasets. It is expensive to understand the impact of these domain-specific datasets on model capabilities as training at large FLOP scales is required to reveal significant changes to difficult and emergent benchmarks. Given the increasing cost of experimenting with pretraining data, how does one determine the optimal balance between the diversity in general web scrapes and the information density of domain specific data? In this work, we show how to leverage the smaller domain specific datasets by upsampling them relative to CC at the end of training to drive performance improvements on difficult benchmarks. This simple technique allows us to improve up to 6.90 pp on MMLU, 8.26 pp on GSM8K, and 6.17 pp on HumanEval relative to the base data mix for a 7B model trained for 1 trillion (T) tokens, thus rivaling Llama-2 (7B)—a model trained for twice as long. We experiment with ablating the duration of domain upsampling from 5% to 30% of training and find that 10% to 20% percent is optimal for navigating the tradeoff between general language modeling capabilities and targeted benchmarks. We also use domain upsampling to characterize at scale the utility of individual datasets for improving various benchmarks by removing them during this final phase of training. This tool opens up the ability to experiment with the impact of different pretraining datasets at scale, but at an order of magnitude lower cost compared to full pretraining runs",
    "checked": true,
    "id": "69858518c2e78c857b997fe1f2880465597e89c2",
    "semantic_title": "does your data spark joy? performance gains from domain upsampling at the end of training",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=bkY8zEDdH9": {
    "title": "O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models",
    "volume": "main",
    "abstract": "Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations with long interaction horizons. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to improve LLM-powered policies without fine-tuning. The proposed method O3D (Offline Data-driven Discovery and Distillation) automatically discovers reusable skills and distills generalizable knowledge across multiple tasks based on offline interaction data, advancing the capability of solving downstream tasks. Empirical results under two interactive decision-making benchmarks (ALFWorld and WebShop) verify that O3D can notably enhance the decision-making capabilities of LLMs through the offline discovery and distillation process, and consistently outperform baselines across various LLMs",
    "checked": true,
    "id": "30ad1ae844ad40b3412e3de361a57bf7ddad100e",
    "semantic_title": "o3d: offline data-driven discovery and distillation for sequential decision-making with large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=xWYRL1eR74": {
    "title": "FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt Optimization Across Tokenizers",
    "volume": "main",
    "abstract": "The widespread use of large language models has resulted in a multitude of tokenizers and embedding spaces, making knowledge transfer in prompt discovery tasks difficult. In this work, we propose FUSE (Flexible Unification of Semantic Embeddings), an inexpensive approach to approximating an adapter layer that maps from one model's textual embedding space to another, even across different tokenizers. We introduce a third-order tensor-based representation of a model's embedding space that aligns semantic embeddings that have been split apart by different tokenizers, and use this representation to derive an approximation of the gradient of one model's outputs with respect to another model's embedding space. We show the efficacy of our approach via multi-objective optimization over vision-language and causal language models for image captioning and sentiment-based image captioning",
    "checked": true,
    "id": "a4c21517155760551e5d195b1ee026536c3e66f5",
    "semantic_title": "fuse-ing language models: zero-shot adapter discovery for prompt optimization across tokenizers",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=stmqBSW2dV": {
    "title": "V-STaR: Training Verifiers for Self-Taught Reasoners",
    "volume": "main",
    "abstract": "Common self-improvement approaches for large language models (LLMs), such as STaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated solutions to improve their problem-solving ability. However, these approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions. To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference time to select one solution among many candidate solutions. Running V-STaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models",
    "checked": true,
    "id": "4fe4f0f9d39d708a6c3d7b8dfbfa2616cd376e1e",
    "semantic_title": "v-star: training verifiers for self-taught reasoners",
    "citation_count": 154,
    "authors": []
  },
  "https://openreview.net/forum?id=9JY1QLVFPZ": {
    "title": "Forcing Diffuse Distributions out of Language Models",
    "volume": "main",
    "abstract": "Despite being trained specifically to follow user instructions, today's instruction-tuned language models perform poorly when instructed to produce random outputs. For example, when prompted to pick a number uniformly between one and ten Llama-2-13B-chat disproportionately favors the number five, and when tasked with picking a first name at random, Mistral-7B-Instruct chooses Avery 40 times more often than we would expect based on the U.S. population. When these language models are used for real-world tasks where diversity of outputs is crucial, such as language model assisted dataset construction, their inability to produce diffuse distributions over valid choices is a major hurdle. In this work, we propose a fine-tuning method that encourages language models to output distributions that are diffuse over valid outcomes. The methods we introduce generalize across a variety of tasks and distributions and make large language models practical for synthetic dataset generation with little human intervention",
    "checked": true,
    "id": "28bb68c387909f94588ac546738a4298888267ab",
    "semantic_title": "forcing diffuse distributions out of language models",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=kGa4fMtP9l": {
    "title": "Can Language Models Solve Olympiad Programming?",
    "volume": "main",
    "abstract": "Olympiad programming is one of the hardest reasoning challenges for humans, yet it has been understudied as a domain to benchmark language models (LMs). In this paper, we introduce the USACO benchmark with 307 problems from USA Computing Olympiad contests, along with high-quality unit tests, reference code, and official analysis for each problem. These resources enable us to construct and test a range of LM inference methods beyond zero-shot prompting for competitive programming. We find state-of-the-art models in code generation, such as GPT-4, achieve only a 8.7\\% pass@1 accuracy with zero-shot chain-of-thought prompting, with our best inference method almost \\textit{doubling} zero-shot accuracy using a novel combination of retrieval augmentation and self-reflection. However, this is still far from solving the benchmark. To better understand the remaining challenges, we perform a novel human-in-the-loop study, and surprisingly find that a small number of targeted hints enable GPT-4 to solve 13 out of 15 problems previously unsolvable by any model and method. Our benchmark, baseline methods, quantitative results, and qualitative analysis thus serve as an initial step towards LMs with grounded, creative, and algorithmic reasoning",
    "checked": true,
    "id": "e6e15de9e3aa88c2690e2d3ec71b8a75a8002d55",
    "semantic_title": "can language models solve olympiad programming?",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=Fkr1yVUb9G": {
    "title": "CoLLEGe: Concept Embedding Generation for Large Language Models",
    "volume": "main",
    "abstract": "Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for few-shot word learning in NLP, relying on global word vectors, are less applicable to large language models. In this paper, we introduce a novel approach named **CoLLEGe** (**Co**ncept **L**earning with **L**anguage **E**mbedding **Ge**neration) to modernize few-shot concept learning. CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary meta-learning objective is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept learning in challenging real-world scenarios, including new word acquisition, definition inference, and verbal reasoning, and demonstrate that our method succeeds in each setting **without task-specific training**. Code and data for our project can be found at [https://college-concept-learning.github.io/](https://college-concept-learning.github.io/)",
    "checked": true,
    "id": "d36bdd3f253c7300daddc57555f7eedae31422f2",
    "semantic_title": "college: concept embedding generation for large language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=0o95CVdNuz": {
    "title": "Effective Prompt Extraction from Language Models",
    "volume": "main",
    "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold on market- places. However, anecdotal reports have shown adversarial users employ- ing prompt extraction attacks to recover these prompts. In this paper, we present a framework for systematically measuring the effectiveness of these attacks. In experiments with 3 different sources of prompts and 11 underly- ing large language models, we find that simple text-based attacks can in fact reveal prompts with high probability. Our framework determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination. Prompt extraction from real systems such as Claude 3 and ChatGPT further suggest that system prompts can be revealed by an adversary despite existing defenses in place",
    "checked": true,
    "id": "b9df0d4631f9fab1432c152765e243ae4cd667f4",
    "semantic_title": "effective prompt extraction from language models",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=LFfktMPAci": {
    "title": "What makes a good metric? Evaluating automatic metrics for text-to-image consistency",
    "volume": "main",
    "abstract": "Language models are increasingly being incorporated as components in larger AI systems for various purposes, from prompt optimization to automatic evaluation. In this work, we analyze the construct validity of four recent, commonly used methods for measuring text-to-image consistency---CLIPScore, TIFA, VPEval, and DSG---which rely on language models and/or VQA models as components. We define construct validity for text-image consistency metrics as a set of desiderata that text-image consistency metrics should have, and find that no tested metric satisfies all of them. We find that metrics lack sufficient sensitivity to language and visual properties. Next, we find that TIFA, VPEval and DSG contribute novel information above and beyond CLIPScore, but also that they correlate highly with each other. We also ablate different aspects of the text-image consistency metrics and find that not all model components are strictly necessary, also a symptom of insufficient sensitivity to visual information. Finally, we show that all three VQA-based metrics likely rely on familiar text shortcuts (such as yes-bias in QA) that call their aptitude as quantitative evaluations of model performance into question",
    "checked": true,
    "id": "f81d59aa6c616905eb31513359c3cbb2f9365d12",
    "semantic_title": "what makes a good metric? evaluating automatic metrics for text-to-image consistency",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=nUNbjMDBWC": {
    "title": "An Incomplete Loop: Instruction Inference, Instruction Following, and In-Context Learning in Language Models",
    "volume": "main",
    "abstract": "Modern language models (LMs) can learn to perform new tasks in different ways: in instruction following, the target task is described explicitly in natural language; in few-shot prompting, the task is specified implicitly with a small number of examples; in instruction inference, LMs are presented with in-context examples and are then prompted to generate a natural language task description before making predictions. Each of these procedures may be thought of as invoking a different form of reasoning: instruction following involves deductive reasoning, few-shot prompting involves inductive reasoning, and instruction inference is abductive reasoning. How do these different capabilities relate? Across four LMs (from the gpt and llama families) and two learning problems (involving arithmetic functions and machine translation) we find a strong dissociation between the different types of reasoning: LMs can sometimes learn effectively from few-shot prompts even when they are unable to explain their own prediction rules; conversely, they sometimes infer useful task descriptions while completely failing to learn from human-generated descriptions of the same task. Our results highlight the non-systematic nature of reasoning even in some of today's largest LMs, and underscore the fact that very different learning mechanisms may be invoked by seemingly similar prompting procedures",
    "checked": true,
    "id": "a966858e1c3f3168e26db39bd9f7e71b19d3c9c9",
    "semantic_title": "an incomplete loop: instruction inference, instruction following, and in-context learning in language models",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=cKBmZ2PZ6c": {
    "title": "ORAG: Ontology-Guided Retrieval-Augmented Generation for Theme-Specific Entity Typing",
    "volume": "main",
    "abstract": "Large language models (LLMs) incorporated with retrieval-augmented generation (RAG) have shown great power in many NLP tasks, including fine-grained entity typing (FET). However, we observe that recent LLMs can easily suffer from hallucinations on highly specialized and fast-evolving themes (e.g., redox-active organic electrode materials), especially in the following cases: (1) unseen entities: an entity never appears in the pre-training corpora of LLMs; and (2) misleading semantics: the context of an entity can potentially mislead an entity typing algorithm if the relevant knowledge is not correctly retrieved and utilized. To address these challenges, this paper proposes an Ontology-Guided Retrieval-Augmented Generation (ORAG) approach that incorporates ontology structures with RAG for the theme-specific entity typing task. ORAG first enriches the label ontology with external knowledge and constructs a structured knowledge unit for each node. Then, it retrieves the relevant nodes by dense passage retrieval and expands the retrieved results based on the ontological structure. In this way, more supporting knowledge will be retrieved within the limited input of LLMs for entity typing. In the evaluation, we construct a dataset with two themes for theme-specific entity typing with a focus on unseen entities and misleading semantics. We observe notable cases of hallucination when vanilla RAG is applied to Llama-3, GPT-3.5, and GPT-4, while ORAG can effectively mitigate such hallucinations and improve the results",
    "checked": true,
    "id": "9d1f6ef843260e8091c856a701d97c7af9b5bcb0",
    "semantic_title": "orag: ontology-guided retrieval-augmented generation for theme-specific entity typing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=18iNTRPx8c": {
    "title": "See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering LLM Weaknesses",
    "volume": "main",
    "abstract": "The impressive performance of Large Language Models (LLMs) has consistently surpassed numerous human-designed benchmarks, presenting new challenges in assessing the shortcomings of LLMs. Designing tasks and finding LLMs' limitations are becoming increasingly important. In this paper, we investigate the question of whether an LLM can discover its own limitations from the errors it makes. To this end, we propose a Self-Challenge evaluation framework with human-in-the-loop. Starting from seed instances that GPT-4 fails to answer, we prompt GPT-4 to summarize error patterns that can be used to generate new instances and incorporate human feedback on them to refine these patterns for generating more challenging data, iteratively. We end up with 8 diverse patterns, such as text manipulation and questions with assumptions. We then build a benchmark, SC-G4, consisting of 1,835 instances generated by GPT-4 using these patterns, with human-annotated gold responses. The SC-G4 serves as a challenging benchmark that allows for a detailed assessment of LLMs' abilities. Our results show that only 44.96\\% of instances in SC-G4 can be answered correctly by GPT-4. Interestingly, our pilot study indicates that these error patterns also challenge other LLMs, such as Claude-3 and Llama-3, and cannot be fully resolved through fine-tuning. Our work takes the first step to demonstrate that LLMs can autonomously identify their inherent flaws and provide insights for future dynamic and automatic evaluation",
    "checked": true,
    "id": "4ad45c61be8d635f56c1f2a2d6527cb33d715612",
    "semantic_title": "see what llms cannot answer: a self-challenge framework for uncovering llm weaknesses",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=y7JnjDcIQa": {
    "title": "How Susceptible are LLMs to Influence in Prompts?",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are highly sensitive to prompts, including additional context provided therein. As LLMs grow in capability, understanding their prompt-sensitivity becomes increasingly crucial for ensuring reliable and robust performance, particularly since evaluating these models becomes more challenging. In this work, we investigate how current models (Llama, Mixtral, Falcon) respond when presented with additional input from another model, mimicking a scenario where a more capable model -- or a system with access to more external information -- provides supplementary information to the target model. Across a diverse spectrum of question-answering tasks, we study how an LLM's response to multiple-choice questions changes when the prompt includes a prediction and explanation from another model. Specifically, we explore the influence of the presence of an explanation, the stated authoritativeness of the source, and the stated confidence of the supplementary input. Our findings reveal that models are strongly influenced, and when explanations are provided they are swayed irrespective of the quality of the explanation. The models are more likely to be swayed if the input is presented as being authoritative or confident, but the effect is small in size. This study underscores the significant prompt-sensitivity of LLMs and highlights the potential risks of incorporating outputs from external sources without thorough scrutiny and further validation. As LLMs continue to advance, understanding and mitigating such sensitivities will be crucial for their reliable and trustworthy deployment",
    "checked": true,
    "id": "7bc56b12f55e144738d9a04b4b7968f5bb053726",
    "semantic_title": "how susceptible are llms to influence in prompts?",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=oRcYFm8vyB": {
    "title": "Logits of API-Protected LLMs Leak Proprietary Information",
    "volume": "main",
    "abstract": "Large language model (LLM) providers often hide the architectural details and parameters of their proprietary models by restricting public access to a limited API. In this work we show that, with only a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1000 USD for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We exploit this fact to unlock several capabilities, including (but not limited to) obtaining cheap full-vocabulary outputs, auditing for specific types of model updates, identifying the source LLM given a single full LLM output, and even efficiently discovering the LLM's hidden size. Our empirical investigations show the effectiveness of our methods, which allow us to estimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4096. Lastly, we discuss ways that LLM providers can guard against these attacks, as well as how these capabilities can be viewed as a feature (rather than a bug) by allowing for greater transparency and accountability",
    "checked": true,
    "id": "5f2b88d1c0d98f3f2973221657ca5237a185cc37",
    "semantic_title": "logits of api-protected llms leak proprietary information",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=Jd0bCD12DS": {
    "title": "Mind the Privacy Unit! User-Level Differential Privacy for Language Model Fine-Tuning",
    "volume": "main",
    "abstract": "Large language models (LLMs) have emerged as powerful tools for tackling complex tasks across diverse domains, but they also raise privacy concerns when fine-tuned on sensitive data due to potential memorization. While differential privacy (DP) offers a promising solution by ensuring models are \"almost indistinguishable\" with or without any particular privacy unit, current evaluations on LLMs mostly treat each example (text record) as the privacy unit. This leads to uneven user privacy guarantees when contributions per user vary. We therefore study user-level DP motivated by applications where it is necessary to ensure uniform privacy protection across users. We present a systematic evaluation of user-level DP for LLM fine-tuning on natural language generation tasks. Focusing on two mechanisms for achieving user-level DP guarantees, Group Privacy and User-wise DP-SGD, we investigate design choices like data selection strategies and parameter tuning for the best privacy-utility tradeoff",
    "checked": true,
    "id": "2a7b922f6a943bfebb199e86d05b5204d39ebd8e",
    "semantic_title": "mind the privacy unit! user-level differential privacy for language model fine-tuning",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=yK2eGE8QVW": {
    "title": "NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment",
    "volume": "main",
    "abstract": "Aligning Large Language Models (LLMs) with human values and preferences is essential for making them helpful and safe. However, building efficient tools to perform alignment can be challenging, especially for the largest and most competent LLMs which often contain tens or hundreds of billions of parameters. We create NeMo-Aligner, a toolkit for model alignment that can efficiently scale to a thousand GPUs for training the largest open-source LLMs such as Nemotron 4 340B and Llama 3.1 405B. NeMo-Aligner comes with highly optimized and scalable implementations for major paradigms of model alignment such as: Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally, our toolkit supports running most of the alignment techniques in a Parameter Efficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed for extensibility, allowing support for other alignment techniques with minimal effort. It is open-sourced with Apache 2.0 License and we invite community contributions at https://github.com/NVIDIA/NeMo-Aligner",
    "checked": true,
    "id": "afb06e773d9f073a885a095a8bbb5a5b761a3ab5",
    "semantic_title": "nemo-aligner: scalable toolkit for efficient model alignment",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=ADtL6fgNRv": {
    "title": "Inspecting and Editing Knowledge Representations in Language Models",
    "volume": "main",
    "abstract": "Neural language models (LMs) represent facts about the world described by text. Sometimes these facts derive from training data (in most LMs, a representation of the word *banana* encodes the fact that bananas are fruits). Sometimes facts derive from input text itself (a representation of the sentence *I poured out the bottle* encodes the fact that the bottle became empty). We describe REMEDI, a method for learning to map statements in natural language to fact encodings in an LM's internal representation system. REMEDI encodings can be used as *knowledge editors*: when added to LM hidden representations, they modify downstream generation to be consistent with new facts. REMEDI encodings may also be used as *probes*: when compared to LM representations, they reveal which properties LMs already attribute to mentioned entities, in some cases making it possible to predict when LMs will generate outputs that conflict with background knowledge or input text. REMEDI thus links work on probing, prompting, and LM editing, and offers steps toward general tools for fine-grained inspection and control of knowledge in LMs",
    "checked": true,
    "id": "a206a0c96d6076c6ab081288b0c2c95d3c7efd64",
    "semantic_title": "inspecting and editing knowledge representations in language models",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=dWYRjT501w": {
    "title": "Unveiling LLMs: The Evolution of Latent Representations in a Dynamic Knowledge Graph",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) demonstrate an impressive capacity to recall a vast range of factual knowledge. However, understanding their underlying reasoning and internal mechanisms in exploiting this knowledge remains a key research area. This work unveils the factual information an LLM represents internally for sentence-level claim verification. We propose an end-to-end framework to decode factual knowledge embedded in token representations from a vector space to a set of ground predicates, showing its layer-wise evolution using a dynamic knowledge graph. Our framework employs activation patching, a vector-level technique that alters a token representation during inference, to extract encoded knowledge. Accordingly, we neither rely on training nor external models. Using factual and common-sense claims from two claim verification datasets, we showcase interpretability analyses at local and global levels. The local analysis highlights entity centrality in LLM reasoning, from claim-related information and multi-hop reasoning to representation errors causing erroneous evaluation. On the other hand, the global reveals trends in the underlying evolution, such as word-based knowledge evolving into claim-related facts. By interpreting semantics from LLM latent representations and enabling graph-related analyses, this work enhances the understanding of the factual knowledge resolution process",
    "checked": true,
    "id": "ae7a3e3b5b75b0bdc2ba10a976303e6a816988fc",
    "semantic_title": "unveiling llms: the evolution of latent representations in a dynamic knowledge graph",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=T9cOYH0wGF": {
    "title": "Optimising Calls to Large Language Models with Uncertainty-Based Two-Tier Selection",
    "volume": "main",
    "abstract": "Researchers and practitioners operating on a limited budget face the well-known cost-performance trade-off dilemma. The challenging decision often centers on whether to use a large LLM with better performance or a smaller one with reduced costs. This has motivated recent research in the optimisation of LLM calls. Either a cascading strategy is used, where a smaller LLM or both are called causally, or a routing strategy is used, where only one model is ever called. This is dependent on a decision criterion in both scenarios which is typically an auxiliary neural model. In this work, we propose a cost-effective solution; we use only the uncertainty of the generations of the small LLM as the decision criterion. We compare our approach with both cascading and routing strategies using three different pairs of pre-trained small and large LLMs, on nine different tasks and against approaches that require an additional neural model. Our experiments reveal this simple solution optimally balances cost and performance, outperforming existing methods on 25 out of 27 experimental setups",
    "checked": true,
    "id": "8d59f23de5a8635b00458c46f2d4cc507cb57c61",
    "semantic_title": "optimising calls to large language models with uncertainty-based two-tier selection",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=UPyWLwciYz": {
    "title": "Source-Aware Training Enables Knowledge Attribution in Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability, we explore source-aware training---a recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning stage to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training borrows from existing pretraining/fine-tuning frameworks and requires minimal changes to the model architecture or implementation. Through experiments on synthetic data, we demonstrate that our training recipe can enable faithful attribution to the pretraining data without a substantial impact on the model's perplexity compared to standard pretraining. Our findings also highlight the importance of pretraining data augmentation in achieving attribution",
    "checked": true,
    "id": "8719833751cf1bfc779c944fc7954a337b2c0833",
    "semantic_title": "source-aware training enables knowledge attribution in language models",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=yoVRyrEgix": {
    "title": "Locating and Editing Factual Associations in Mamba",
    "volume": "main",
    "abstract": "We investigate the mechanisms of factual recall in the Mamba state space model. Our work is inspired by previous findings in autoregressive transformer language models suggesting that their knowledge recall is localized to particular modules at specific token locations; we therefore ask whether factual recall in Mamba can be similarly localized. To investigate this, we conduct four lines of experiments on Mamba. First, we apply causal tracing or interchange interventions to localize key components inside Mamba that are responsible for recalling facts, revealing that specific components within middle layers show strong causal effects at the last token of the subject, while the causal effect of intervening on later layers is most pronounced at the last token of the prompt, matching previous findings on autoregressive transformers. Second, we show that rank-one model editing methods can successfully insert facts at specific locations, again resembling findings on transformer LMs. Finally we adapt attention-knockout techniques to Mamba in order to dissect information flow during factual recall. We compare Mamba directly to a similar-sized autoregressive transformer LM and conclude that despite significant differences in architectures, when it comes to factual recall, the two architectures share many similarities",
    "checked": true,
    "id": "5664f7f52264ff656faa3671d83e2009f4f390fd",
    "semantic_title": "locating and editing factual associations in mamba",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=U5BUzSn4tD": {
    "title": "Auxiliary task demands mask the capabilities of smaller language models",
    "volume": "main",
    "abstract": "Developmental psychologists have argued about when cognitive capacities such as language understanding or theory of mind emerge. These debates often hinge on the concept of \"task demands\" -- the auxiliary challenges associated with performing a particular evaluation -- that may mask the child's underlying ability. The same issues arise when measuring the capacities of language models (LMs): performance on a task is a function of the model's underlying knowledge, combined with the model's ability to interpret and perform the task given its available resources. Here, we show that for analogical reasoning, reflective reasoning, word prediction, and grammaticality judgments, evaluation methods with greater task demands yield lower performance than evaluations with reduced demands. This \"demand gap\" is most pronounced for models with fewer parameters and less training data. Our results illustrate that LM performance should not be interpreted as a direct indication of intelligence (or lack thereof), but as a reflection of capacities seen through the lens of researchers' design choices",
    "checked": true,
    "id": "59bcc59de2cc6f280bc777eeba34d14569f8984b",
    "semantic_title": "auxiliary task demands mask the capabilities of smaller language models",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=7xUtka9ck9": {
    "title": "Yes, no, maybe? Revisiting language models' response stability under paraphrasing for the assessment of political leaning",
    "volume": "main",
    "abstract": "An increasing number of studies are aimed at uncovering characteristics such as personality traits or political leanings of language models (LMs), using questionnaires developed for human respondents. From this previous body of work, it is evident that models are highly sensitive to prompt design, including the phrasing of questions and statements, as well as the format of the expected response (e.g., forced choice, vs open-ended). These sensitivities then often lead to inconsistent responses. However, most studies assess response stability on a small scale with low statistical power e.g., using less than ten paraphrases of the same question. In this work, we investigate the stability of responses to binary forced-choice questions using a large number of paraphrases. Specifically, we probe both masked language models (MLMs) and left-to-right generative language models (GLMs) on the political compass test, assessing response validity (i.e., the proportion of valid responses to a prompt) and response stability (i.e., the variability under paraphrasing) across 500 paraphrases of each statement. This large-scale assessment allows us to approximate the underlying distribution of model responses more precisely, both in terms of the overall stability of a model under paraphrasing as well as the stability of specific items (i.e., the intended meaning of a question). In addition, to investigate whether there are structural biases that drive model responses into a certain direction, we test the association between different word- and sentence-level features, and the models' responses. We find that while all MLMs exhibit a high degree of response validity, GLMs do not consistently produce valid responses when assessed via forced choice. In terms of response stability, we show that even models that exhibit high overall stability scores flip their responses given certain paraphrases. Crucially, even within-model, response stability can vary considerably between items. We also find that models tend to agree more with statements that show high positive sentiment scores. Based on our results, we argue that human-centered questionnaires might not be appropriate in the context of probing LMs as both their response validity and stability differ considerably between items. Moreover, although stability metrics represent useful descriptions of model properties, it should be emphasized that even for models exhibiting fairly high stability, specific paraphrases can lead to substantially different model responses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GC4mXVfquq": {
    "title": "JailBreakV: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks",
    "volume": "main",
    "abstract": "With the rapid advancements in Multimodal Large Language Models (MLLMs), securing these models against malicious inputs while align- ing them with human values has emerged as a critical challenge. In this paper, we investigate an important and unexplored question of whether techniques that successfully jailbreak Large Language Models (LLMs) can be equally effective in jailbreaking MLLMs. To explore this issue, we in- troduce JailBreakV-28K, a pioneering benchmark designed to assess the transferability of LLM jailbreak techniques to MLLMs, thereby evaluat- ing the robustness of MLLMs against diverse jailbreak attacks. Utilizing a dataset of 2, 000 malicious queries that is also proposed in this paper, we generate 20, 000 text-based jailbreak prompts using advanced jailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs from recent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test cases across a spectrum of adversarial scenarios. Our evaluation of 10 open- source MLLMs reveals a notably high Attack Success Rate (ASR) for attacks transferred from LLMs, highlighting a critical vulnerability in MLLMs that stems from their text-processing capabilities. Our findings underscore the urgent need for future research to address alignment vulnerabilities in MLLMs from both textual and visual inputs",
    "checked": true,
    "id": "f019c9661b253ddb611e930348e20ddcd350a952",
    "semantic_title": "jailbreakv: a benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=v74mJURD1L": {
    "title": "Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data",
    "volume": "main",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a popular method for aligning Language Models (LM) with human values and preferences. RLHF requires a large number of preference pairs as training data, which are often used in both the Supervised Fine-Tuning and Reward Model training and therefore publicly available datasets are commonly used. In this work, we study to what extent a malicious actor can manipulate the LMs generations by poisoning the preferences, i.e., injecting poisonous preference pairs into these datasets and the RLHF training process. We propose strategies to build poisonous preference pairs and test their performance by poisoning two widely used preference datasets. Our results show that preference poisoning is highly effective: injecting a small amount of poisonous data (1-5\\% of the original dataset), we can effectively manipulate the LM to generate a target entity in a target sentiment (positive or negative). The findings from our experiments also shed light on strategies to defend against the preference poisoning attack",
    "checked": true,
    "id": "521c2905e667ad6d2162ac369cf3f85d70e0f477",
    "semantic_title": "best-of-venom: attacking rlhf by injecting poisoned preference data",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=EIjJ6ykPnh": {
    "title": "D2PO: Discriminator-Guided DPO with Response Evaluation Models",
    "volume": "main",
    "abstract": "Varied approaches for aligning language models have been proposed, including supervised fine-tuning, RLHF, and direct optimization methods such as DPO. Although DPO has rapidly gained popularity due to its straightforward training process and competitive results, there is an open question of whether there remain practical advantages of using a discriminator, such as a reward model, to evaluate responses. We propose D2PO, discriminator-guided DPO, an approach for the online setting where preferences are being collected throughout learning. As we collect gold preferences, we use these not only to train our policy, but to train a discriminative response evaluation model to silver-label even more synthetic data for policy training. We explore this approach across a set of diverse tasks, including a realistic chat setting, and we find that our approach can lead to higher-quality outputs compared to DPO with the same data budget, and greater efficiency in terms of preference data requirements. Furthermore, we show that our silver labeling is most helpful when training the policy with DPO, outperforming traditional PPO, and benefits from maintaining a separate discriminator from the policy model",
    "checked": true,
    "id": "7a646b1f4e66c4e4e9a673834567e03341e89162",
    "semantic_title": "d2po: discriminator-guided dpo with response evaluation models",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=G8LaO1P0xv": {
    "title": "A Long Way to Go: Investigating Length Correlations in RLHF",
    "volume": "main",
    "abstract": "Great success has been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models, with open preference datasets enabling wider experimentation, particularly for \"helpfulness\" in tasks like dialogue and web question answering. Alongside these improvements, however, RLHF also often drives models to produce longer outputs. This paper demonstrates, on three diverse settings, that optimizing for response length is, much more than previously thought, a significant factor behind RLHF. Studying the strategies RL optimization uses to maximize reward, we find improvements in reward to largely be driven by increasing response length, instead of other features. Indeed, we find that even a *purely* length-based reward reproduces most downstream RLHF improvements over supervised fine-tuned models. Testing a comprehensive set of length-countering interventions, we identify the dominant source of these biases to be reward models, which, by studying training dynamics, we find are non-robust and easily influenced by length biases in preference data",
    "checked": true,
    "id": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
    "semantic_title": "a long way to go: investigating length correlations in rlhf",
    "citation_count": 173,
    "authors": []
  },
  "https://openreview.net/forum?id=sBxvoDhvao": {
    "title": "Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language Adaptation of LLMs for Low-Resource NLP",
    "volume": "main",
    "abstract": "The development of monolingual language models for low and mid-resource languages continues to be hindered by the difficulty in sourcing high-quality training data. In this study, we present a novel cross-lingual vocabulary transfer strategy, trans-tokenization, designed to tackle this challenge and enable more efficient language adaptation. Our approach focuses on adapting a high-resource monolingual LLM to an unseen target language by initializing the token embeddings of the target language using a weighted average of semantically similar token embeddings from the source language. For this, we leverage a translation resource covering both the source and target languages. We validate our method with the Tweeties, a series of trans-tokenized LLMs, and demonstrate their competitive performance on various downstream tasks across a small but diverse set of languages. Additionally, we introduce Hydra LLMs, models with multiple swappable language modeling heads and embedding tables, which further extend the capabilities of our trans-tokenization strategy. By designing a Hydra LLM based on the multilingual model TowerInstruct, we developed a state-of-the-art machine translation model for Tatar, in a zero-shot manner, completely bypassing the need for high-quality parallel data. This breakthrough is particularly significant for low-resource languages like Tatar, where high-quality parallel data is hard to come by. By lowering the data and time requirements for training high-quality models, our trans-tokenization strategy allows for the development of LLMs for a wider range of languages, especially those with limited resources. We hope that our work will inspire further research and collaboration in the field of cross-lingual vocabulary transfer and contribute to the empowerment of languages on a global scale",
    "checked": true,
    "id": "513aa6d06df62bfeea12a0bf5918f946d0f147cb",
    "semantic_title": "trans-tokenization and cross-lingual vocabulary transfers: language adaptation of llms for low-resource nlp",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=nI6JyFSnyV": {
    "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated the capability to process extended token sequences, enabling complex tasks such as book comprehension and long-form text generation. However, as context length increases, the key-value (KV) cache required for LLMs consumes substantial memory, becoming a bottleneck for deployment. This paper introduces SKVQ (Sliding-window KV cache Quantization), a strategy designed to address the challenge of extremely low bitwidth KV cache quantization. SKVQ rearranges the channels of the KV cache to enhance channel similarity within quantization groups and applies clipped dynamic quantization at the group level. Furthermore, SKVQ maintains high precision for the most recent window tokens in the KV cache, preserving accuracy for a small yet critical portion of the cache. Our evaluation of LLMs demonstrates that SKVQ achieves high compression ratios while maintaining accuracy, outperforming previous quantization methods. SKVQ enables the quantization of the KV cache to 2-bit keys and 1.5-bit values with minimal accuracy loss. This advancement allows processing context lengths of up to 1M tokens on an 80GB GPU for a 7B parameter model, resulting in up to 7 times faster decoding",
    "checked": true,
    "id": "d98c87a7d41ba41dc804f8cb1aea74787c02038f",
    "semantic_title": "skvq: sliding-window key and value cache quantization for large language models",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=bwo3GVsgOv": {
    "title": "Personalized Collaborative Fine-Tuning for On-Device Large Language Models",
    "volume": "main",
    "abstract": "We explore on-device collaborative fine-tuning of large language models under limited local data availability. We introduce three distinct dynamic collaborator selection schemes, allowing trust-weighted personalized update aggregation: model-similarity-based, prediction-similarity-based and validation-performance-based. To minimize communication overhead, we integrate Low-Rank Adaptation (LoRA) and only exchange LoRA model updates. Our protocols, driven by prediction and performance metrics, surpass both FedAvg and local fine-tuning methods, which is particularly evident in realistic distributed scenarios with more diverse local data distributions. The results underscore the effectiveness of our approach in addressing heterogeneity and scarcity of the local datasets",
    "checked": true,
    "id": "3141373292054e183fe0d7a1654ae968776b59d8",
    "semantic_title": "personalized collaborative fine-tuning for on-device large language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=Vd0KvChLXr": {
    "title": "Generating Synthetic Datasets for Few-shot Prompt Tuning",
    "volume": "main",
    "abstract": "A major limitation of prompt tuning is its dependence on large labeled training datasets. Under few-shot learning settings, prompt tuning lags far behind full-model fine-tuning, limiting its scope of application. In this paper, we leverage the powerful LLMs to synthesize task-specific labeled data for training the soft prompts. We first introduce a distribution-aligned weighted generator tuning (DawGen) method to encourage generating in-distribution data that aligns with the few-shot real data. Then, we train soft prompts on both synthetic and real datasets using a gradient surgery approach, which eliminates the conflicting gradients from different data sources. Experiments on seven sentence-pair classification datasets demonstrate the effectiveness of our proposed method for boosting prompt tuning in few-shot learning settings. Results on QQP, MRPC, and SICK datasets are even comparable to the performance of transfer learning from large real-world datasets, showing the promise of synthetic data as an alternative for enhancing soft prompt tuning",
    "checked": true,
    "id": "e3c4e93e931e03930328dc2593ed9cfdb6ce2261",
    "semantic_title": "generating synthetic datasets for few-shot prompt tuning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=X9yV4lFHt4": {
    "title": "MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs",
    "volume": "main",
    "abstract": "Generative large language models (LLMs) have been shown to exhibit harmful biases and stereotypes. While safety fine-tuning typically takes place in English, if at all, these models are being used by speakers of many different languages. There is existing evidence that the performance of these models is inconsistent across languages and that they discriminate based on demographic factors of the user. Motivated by this, we investigate whether the social stereotypes exhibited by LLMs differ as a function of the language used to prompt them, while controlling for cultural differences and task accuracy. To this end, we present MBBQ (Multilingual Bias Benchmark for Question-answering), a carefully curated version of the English BBQ dataset extended to Dutch, Spanish, and Turkish, which measures stereotypes commonly held across these languages. We further complement MBBQ with a parallel control dataset to measure task performance on the question-answering task independently of bias. Our results based on several open-source and proprietary LLMs confirm that some non-English languages suffer from bias more than English, even when controlling for cultural shifts. Moreover, we observe significant cross-lingual differences in bias behaviour for all except the most accurate models. With the release of MBBQ, we hope to encourage further research on bias in multilingual settings. The dataset and code are available at https://github.com/Veranep/MBBQ",
    "checked": true,
    "id": "0edc2883c836aad28e559d4a4ceff805dbe808bb",
    "semantic_title": "mbbq: a dataset for cross-lingual comparison of stereotypes in generative llms",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=ig6NI9oPhD": {
    "title": "LAMPO: Large Language Models as Preference Machines for Few-shot Ordinal Classification",
    "volume": "main",
    "abstract": "We introduce LAMPO, a novel paradigm that leverages Large Language Models (LLMs) for solving few-shot multi-class ordinal classification tasks. Unlike conventional methods, which concatenate all demonstration examples with the test instance and prompt LLMs to produce the pointwise prediction, our framework uses the LLM as a preference machine that makes a relative comparative decision between the test instance and each demonstration. A self-supervised method is then introduced to aggregate these binary comparisons into the final ordinal decision. LAMPO addresses several limitations inherent in previous methods, including context length constraints, ordering biases, and challenges associated with absolute point-wise estimation. Extensive experiments on seven public datasets demonstrate LAMPO's remarkably competitive performance across a diverse spectrum of applications (e.g., movie review analysis and hate speech detection). Notably, in certain applications, the improvement can be substantial, exceeding 20% in an absolute term. Moreover, we believe LAMPO represents an interesting addition to the non-parametric application layered on top of LLMs, as it supports black-box LLMs without necessitating the outputting of LLM's internal states (e.g., embeddings), as seen in previous approaches",
    "checked": true,
    "id": "8db08a36e70ceae7191ac1a53a4301b94ab95a21",
    "semantic_title": "lampo: large language models as preference machines for few-shot ordinal classification",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=yK8MT91dQY": {
    "title": "Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided",
    "volume": "main",
    "abstract": "Large language models (LLMs) have offered new opportunities for emotional support, and recent work has shown that they can produce empathic responses to people in distress. However, long-term mental well-being requires emotional self-regulation, where a one-time empathic response falls short. This work takes a first step by engaging with cognitive reappraisals, a strategy from psychology practitioners that uses language to targetedly change negative appraisals that an individual makes of the situation; such appraisals is known to sit at the root of human emotional experience. We hypothesize that psychologically grounded principles could enable such advanced psychology capabilities in LLMs, and design RESORT which consists of a series of reappraisal constitutions across multiple dimensions that can be used as LLM instructions. We conduct a first-of-its-kind expert evaluation (by clinical psychologists with M.S. or Ph.D. degrees) of an LLM's zero-shot ability to generate cognitive reappraisal responses to medium-length social media messages asking for support. This fine-grained evaluation showed that even LLMs at the 7B scale guided by RESORT are capable of generating empathic responses that can help users reappraise their situations",
    "checked": true,
    "id": "eeef23397297b4760a16e3b0a0ac8d72bc341300",
    "semantic_title": "large language models are capable of offering cognitive reappraisal, if guided",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=SHMj84U5SH": {
    "title": "Compression Represents Intelligence Linearly",
    "volume": "main",
    "abstract": "There is a belief that learning to compress well will lead to intelligence. Recently, language modeling has been shown to be equivalent to compression, which offers a compelling rationale for the success of large language models (LLMs): development of more advanced language models is essentially enhancing compression which facilitates intelligence. Despite such appealing discussions, little empirical evidence is present for the interplay between compression and intelligence. In this work, we examine the relationship between compression and intelligence in the context of LLMs, treating LLMs as data compressors. Given the abstract concept of \"intelligence\", we adopt the average downstream benchmark scores as a surrogate, specifically targeting intelligence related to knowledge and commonsense, coding, and mathematical reasoning. Across 12 benchmarks, our study brings together 31 public LLMs that vary in size and originate from diverse organizations. Remarkably, we find that LLMs' intelligence -- reflected by benchmark scores -- almost **linearly** correlates with their ability to compress external text corpora. These results provide concrete evidence supporting the belief that superior compression indicates greater intelligence. Furthermore, our findings suggest that compression efficiency, as an unsupervised metric derived from raw text corpora, serves as a reliable evaluation measure that is linearly associated with the model capabilities. This work advocates for the adoption of compression performance as a stable, flexible, and reliable metric for evaluating LLMs. We open-source our compression datasets as well as our data collection pipelines to facilitate future researchers to assess compression properly",
    "checked": true,
    "id": "4cf69c2074c86d0a114e8e77ddea69b53ebe435c",
    "semantic_title": "compression represents intelligence linearly",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=jq2kNXigPP": {
    "title": "DeStein: Navigating Detoxification of Language Models via Universal Steering Pairs and Head-wise Activation Fusion",
    "volume": "main",
    "abstract": "Despite the remarkable achievements of language models (LMs) across a broad spectrum of tasks, their propensity for generating toxic outputs remains a prevalent concern. Current solutions involving finetuning or auxiliary models usually require extensive computational resources, hindering their practicality in large language models (LLMs). In this paper, we propose DeStein, a novel method that detoxifies LMs by applying representation engineering in activation spaces with lower resource and time costs. Specifically, we derive detoxification vectors from self-induced, universal steering pairs through arithmetic operations in activation spaces. During inference, detoxification is achieved by fusing the detoxification vectors with the original representations in a head-wise manner. Empirical results demonstrate that our method significantly outperforms previous state-of-the-art approaches on various metrics, while also maintaining satisfactory generation quality and diversity. We further validate the practicality and scalability of DeStein with a series of white-box LLMs. Warning: Some example model outputs may contain highly offensive or disturbing text",
    "checked": true,
    "id": "25da56bc957c0a73088fa6980d1c5024f61a9f3a",
    "semantic_title": "destein: navigating detoxification of language models via universal steering pairs and head-wise activation fusion",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=ptvV5HGTNN": {
    "title": "Resolving Knowledge Conflicts in Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) often encounter knowledge conflicts, scenarios where discrepancy arises between the internal parametric knowledge of LLMs and non-parametric information provided in the prompt context. In this work we ask what are the desiderata for LLMs when a knowledge conflict arises and whether existing LLMs fulfill them. We posit that LLMs should 1) identify knowledge conflicts, 2) pinpoint conflicting information segments, and 3) provide distinct answers or viewpoints in conflicting scenarios. To this end, we introduce an evaluation framework for simulating contextual knowledge conflicts and quantitatively evaluating to what extent LLMs achieve these goals. It includes diverse and complex situations of knowledge conflict, knowledge from diverse entities and domains, two synthetic conflict creation methods, and settings with progressively increasing difficulty to reflect realistic knowledge conflicts. Extensive experiments with the framework reveal that while LLMs perform well in identifying the existence of knowledge conflicts, they struggle to determine the specific conflicting knowledge and produce a response with distinct answers amidst conflicting information. To address these challenges, we propose new instruction-based approaches that augment LLMs to better achieve the three goals. Further analysis shows that abilities to tackle knowledge conflicts are greatly impacted by factors such as knowledge domain, while generating robust responses to knowledge conflict scenarios remains an open research question",
    "checked": true,
    "id": "3f4ccf64ffe23b5dc095ae0401eecf9445deb024",
    "semantic_title": "resolving knowledge conflicts in large language models",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=0oiG1KigYN": {
    "title": "SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded Entity Retrieval",
    "volume": "main",
    "abstract": "Clinician must write a lengthy summary each time a patient is discharged from the hospital. This task is time-consuming due to the sheer number of unique clinical concepts covered in the admission. Identifying and covering salient entities is vital for the summary to be clinically useful. We fine-tune open-source LLMs (Mistral-7B-Instruct and Zephyr-7B-$\\beta$) on the task and find that they generate incomplete and unfaithful summaries. To increase entity coverage, we train a smaller, encoder-only model to predict salient entities, which are treated as content-plans to guide the LLM. To encourage the LLM to focus on specific mentions in the source notes, we propose SPEER: Sentence-level Planning via Embedded Entity Retrieval. Specifically, we mark each salient entity span with special \"{{ }}\" boundary tags and instruct the LLM to retrieve marked spans before generating each sentence. Sentence-level planning acts as a form of state tracking in that the model is explicitly recording the entities it uses. We fine-tune Mistral and Zephyr variants on a large-scale, diverse dataset of ~167k in-patient hospital admissions and evaluate on 3 datasets. SPEER shows gains in both coverage and faithfulness metrics over non-guided and guided baselines",
    "checked": true,
    "id": "e2ea572e7b4cc0f32fe521a76f4cffd2692e3478",
    "semantic_title": "speer: sentence-level planning of long clinical summaries via embedded entity retrieval",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=kpf7UbnSAm": {
    "title": "CA-LoRA: Adapting Existing LoRA for Compressed LLMs to Enable Efficient Multi-Tasking on Personal Devices",
    "volume": "main",
    "abstract": "Recently, there has been a demand to deploy Large Language Models (LLMs) on personal devices such as laptops and smartphones. These LLMs have different model variants when handling different tasks. However, personal devices have limited resources and require reduced storage overhead. To address this, there are two key methods available: the first is model compression, which compresses LLMs into smaller sizes; the second is LoRA, which can transfer an LLM to other tasks with very few parameters, avoiding the storage of multiple model variants in multi-task scenarios by only preserving LoRAs. However, our experiments show that directly combining these two methods yields sub-optimal performance. Considering that the open-source community has already contributed many LoRAs to LLMs, we propose to adapt these existing LoRAs from the LLMs to their compressed version and introduce a Compression-Aware LoRA (CA-LoRA) framework. We incorporate knowledge inheritance and recovery strategies to recover the lost knowledge caused by model compression. Experiment results demonstrate that CA-LoRA outperforms the vanilla LoRA methods applied to a compressed LLM and achieves comparable performance to the non-compressed LLM with existing LoRA modules. The source code of CA-LoRA is available at https://github.com/thunlp/CA-LoRA",
    "checked": true,
    "id": "e3c677111470321bf4190860a62b7619d2a50af3",
    "semantic_title": "ca-lora: adapting existing lora for compressed llms to enable efficient multi-tasking on personal devices",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=Rx3wC8sCTJ": {
    "title": "LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory",
    "volume": "main",
    "abstract": "Humans are not homo economicus (i.e., rational economic beings). As humans, we exhibit systematic behavioral biases such as loss aversion, anchoring, framing, etc., which lead us to make suboptimal economic decisions. Insofar as such biases may be embedded in text data on which large language models (LLMs) are trained, to what extent are LLMs prone to the same behavioral biases? Understanding these biases in LLMs is crucial for deploying LLMs to support human decision-making. We propose utility theory-a paradigm at the core of modern economic theory-as an approach to evaluate the economic biases of LLMs. Utility theory enables the quantification and comparison of economic behavior against benchmarks such as perfect rationality or human behavior. To demonstrate our approach, we quantify and compare the economic behavior of a variety of open- and closed-source LLMs. We find that the economic behavior of current LLMs is neither entirely human-like nor entirely economicus-like. We also find that most current LLMs struggle to maintain consistent economic behavior across settings. Finally, we illustrate how our approach can measure the effect of interventions such as prompting on economic biases",
    "checked": true,
    "id": "350000f6af55a7a11cef6e8ad660f1f34f9ae6a8",
    "semantic_title": "llm economicus? mapping the behavioral biases of llms via utility theory",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=TQdd1VhWbe": {
    "title": "Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities",
    "volume": "main",
    "abstract": "Cross-lingual continual pre-training of large language models (LLMs) initially trained on English corpus allows us to leverage the vast amount of English language resources and reduce the pre-training cost. In this study, we constructed Swallow, an LLM with enhanced Japanese capability, by extending the vocabulary of Llama 2 to include Japanese characters and conducting continual pre-training on a large Japanese web corpus. Experimental results confirmed that the performance on Japanese tasks drastically improved through continual pre-training, and the performance monotonically increased with the amount of training data up to 100B tokens. Consequently, Swallow achieved superior performance compared to other LLMs that were trained from scratch in English and Japanese. An analysis of the effects of continual pre-training revealed that it was particularly effective for Japanese question answering tasks. Furthermore, to elucidate effective methodologies for cross-lingual continual pre-training from English to Japanese, we investigated the impact of vocabulary expansion and the effectiveness of incorporating parallel corpora. The results showed that the efficiency gained through vocabulary expansion had no negative impact on performance, except for the summarization task, and that the combined use of parallel corpora enhanced translation ability",
    "checked": true,
    "id": "89e13c80ff90c6e2f2dda700b5dd6c3be1aabf7d",
    "semantic_title": "continual pre-training for cross-lingual llm adaptation: enhancing japanese language capabilities",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=4HNAwZFDcH": {
    "title": "WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting",
    "volume": "main",
    "abstract": "We introduce WorkBench: a benchmark dataset for evaluating agents' ability to execute tasks in a workplace setting. WorkBench contains a sandbox environment with five databases, 26 tools, and 690 tasks. These tasks represent common business activities, such as sending emails and scheduling meetings. The tasks in WorkBench are challenging as they require planning, tool selection, and often multiple actions. If a task has been successfully executed, one (or more) of the database values may change. The correct outcome for each task is unique and unambiguous, which allows for robust, automated evaluation. We call this key contribution outcome-centric evaluation. We evaluate five existing ReAct agents on WorkBench, finding they successfully complete as few as 3% of tasks (Llama2-70B), and just 43% for the best-performing (GPT-4). We further find that agents' errors can result in the wrong action being taken, such as an email being sent to the wrong person. WorkBench reveals weaknesses in agents' ability to undertake common business activities, raising questions about their use in high-stakes workplace settings. WorkBench is publicly available as a free resource at https://github.com/link_updated_upon_acceptance",
    "checked": true,
    "id": "4c6fd0ef6ff1acb313f208a98d80f454a8317234",
    "semantic_title": "workbench: a benchmark dataset for agents in a realistic workplace setting",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=EKBPn7no4y": {
    "title": "StructLM: Towards Building Generalist Models for Structured Knowledge Grounding",
    "volume": "main",
    "abstract": "Structured data sources, such as tables, graphs, and databases, are ubiquitous knowledge sources. Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35\\%. To augment the Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a comprehensive instruction tuning dataset comprising 1.1 million examples. Utilizing this dataset, we train a series of models, referred to as $\\texttt{structlm}$, based on Mistral and the CodeLlama model family, ranging from 7B to 34B parameters. Our $\\texttt{structlm}$ series surpasses task-specific models~\\citep{UnifiedSKG2022} on 16 out of 18 evaluated datasets and establishes new SoTA performance on 8 SKG tasks. Furthermore, $\\texttt{structlm}$ demonstrates strong generalization across 6 novel held-out SKG tasks, outperforming TableLlama by an average of 35\\% and Flan-UL2 20B by an average of 10\\%. Contrary to expectations, we observe that scaling model size offers marginal benefits, with $\\texttt{structlm}$-34B showing only slight improvements over $\\texttt{structlm}$-7B. This suggests that structured knowledge grounding is still a challenging task and requires more innovative design to push to a new level. We release the model weights and training dataset to the community, along with relevant code on Github",
    "checked": true,
    "id": "72c73d5fdf1e67a566cd534336d76b754bafdebd",
    "semantic_title": "structlm: towards building generalist models for structured knowledge grounding",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=xdg4CS5mkl": {
    "title": "Investigating Instruction Tuning Large Language Models on Graphs",
    "volume": "main",
    "abstract": "Inspired by the recent advancements of Large Language Models (LLMs) in NLP tasks, there's growing interest in applying LLMs to graph-related tasks. This study delves into the capabilities of instruction-following LLMs for engaging with real-world graphs, aiming to offer empirical insights into how LLMs can effectively interact with graphs and generalize across graph tasks. We begin by constructing a dataset designed for instruction tuning, which comprises a diverse collection of 79 graph-related tasks from academic and e-commerce domains, featuring 44,240 training instances and 18,960 test samples. Utilizing this benchmark, our initial investigation focuses on identifying the optimal graph representation that serves as a conduit for LLMs to understand complex graph structures. Our findings indicate that JSON format for graph representation consistently outperforms natural language and code formats across various LLMs and graph types. Furthermore, we examine the key factors that influence the generalization abilities of instruction-tuned LLMs by evaluating their performance on both in-domain and out-of-domain graph tasks",
    "checked": true,
    "id": "ede9e29755a9856b820137869f136a9b5842f43c",
    "semantic_title": "investigating instruction tuning large language models on graphs",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Uhwze2LEwq": {
    "title": "MileBench: Benchmarking MLLMs in Long Context",
    "volume": "main",
    "abstract": "Despite the rapid progression of Multimodal Large Language Models (MLLMs) and their impressive performance on various benchmarks, the applicability of these results to real-world tasks remains uncertain. This ambiguity primarily stems from the benchmarks' limited consideration for long-context and multi-image tasks, which are critical elements in real-world applications. Existing benchmarks often focus on single-image and short-text samples, and when assessing multi-image tasks, they either limit the image count or focus on time-series captioning tasks, potentially masking MLLMs' performance challenges such as hallucination in long-context situations. To address these limitations, we introduce \\textbf{\\dataset}, a pioneering benchmark designed to rigorously test the \\textbf{M}ult\\textbf{I}modal \\textbf{L}ong-cont\\textbf{E}xt capabilities of MLLMs. This benchmark comprises a mix of text and images, long contexts, multiple tasks, and tasks requiring both comprehension and generation. We establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMs' long-context adaptation capacity and their ability to complete tasks in long-context scenarios. Our experimental results, garnered from testing 19 models, revealed that while closed-source model GPT-4(Vision) outperforms others, most open-source MLLMs display inadequate performance in long-context situations. Hence, we strongly encourage an intensification of research efforts towards enhancing MLLMs' long-context capabilities, especially in scenarios involving multiple images",
    "checked": true,
    "id": "cc37925b1b117723d6fd35f9771cc22a6b73a3de",
    "semantic_title": "milebench: benchmarking mllms in long context",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=Ukf4301hXm": {
    "title": "Unforgettable Generalization in Language Models",
    "volume": "main",
    "abstract": "When language models (LMs) are trained to ``unlearn'' a skill, does this unlearning generalize? We study the behavior of LMs after fine-tuned on data for a target task (e.g. sentiment analysis) in which the labels have been randomized, a popular unlearning method. While LMs consistently learn to generate near-random predictions for individual training examples in the unlearning set, there is extreme variability across tasks in whether LM predictions change on examples outside the unlearning set. In some tasks (like sentiment analysis), unlearning generalizes robustly, and causes models to generate random outputs on all sentiment-type inputs; in other tasks (like physical commonsense reasoning and scientific question answering) unlearning produces almost no generalization at all, and models continue to perform the task accurately even for examples very similar to those that appeared in the training set. Across tasks, we find that dataset difficulty is not predictive of whether a behavior can be unlearned; instead, generalization in unlearning is (weakly) predicted by the confidence of LMs' initial task predictions and the variability of LM representations of unlearning data, with low confidence and low variability both associated with greater generalization. Finally, we show that even generalizable unlearning is shallow: linear probes trained on LMs' representations can still perform tasks reliably after unlearning. Our results highlight the difficulty and unpredictability of performing targeted skill removal from models via fine-tuning",
    "checked": true,
    "id": "f87d203d2c3a5933a6896377cd899b79329f0a36",
    "semantic_title": "unforgettable generalization in language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=BDBdblmyzY": {
    "title": "Automata-based constraints for language model decoding",
    "volume": "main",
    "abstract": "Language models (LMs) are often expected to generate strings in some formal language; for example, structured data, API calls, or code snippets. Although LMs can be tuned to improve their adherence to formal syntax, this does not *guarantee* conformance, especially with smaller LMs suitable for large-scale deployment. In addition, tuning requires significant resources, making it impractical for uncommon or task-specific formats. To prevent downstream parsing errors we would ideally *constrain* the LM to only produce valid output, but this is severely complicated by tokenization, which is typically both ambiguous and misaligned with the formal grammar. We solve these issues through the application of automata theory, deriving an efficient closed-form solution for the *regular languages*, a broad class of formal languages with many practical applications, including API calls or schema-guided JSON and YAML. We also discuss pragmatic extensions for coping with the issue of high branching factor, and extend our techniques to *deterministic context-free languages*, which similarly admit an efficient closed-form solution. Previous work on this topic (Willard and Louf, 2023) layers bespoke solutions onto automata, leading to problems with speed, correctness, and extensibility. Instead, we reformulate the entire task in terms of automata so we can leverage well-studied and well-optimized algorithms. Our system compiles constraints ~7,000x faster, is provably correct, and can be extended in a modular fashion",
    "checked": true,
    "id": "0ad80a46a44506c083b0018f831311e5f8a2ee44",
    "semantic_title": "automata-based constraints for language model decoding",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=VWWzO3ewMS": {
    "title": "Crowd-Calibrator: Can Annotator Disagreement Inform Calibration in Subjective Tasks?",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "82af358d2d17feb4f28187c73e170b3167192b59",
    "semantic_title": "crowd-calibrator: can annotator disagreement inform calibration in subjective tasks?",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=kHO2ZTa8e3": {
    "title": "The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6252b0d8ab26a4e71c82221837226fa5f41174ec",
    "semantic_title": "the n+ implementation details of rlhf with ppo: a case study on tl;dr summarization",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=5fg0VtRxgi": {
    "title": "SteP: Stacked LLM Policies for Web Actions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "268e28f8d5235031dcd7bfae0f857439e27e8564",
    "semantic_title": "step: stacked llm policies for web actions",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=nXNN0x4wbl": {
    "title": "Instruction-tuning Aligns LLMs to the Human Brain",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "dac7158541d30f9a1eb663bc8dc21f5475d47043",
    "semantic_title": "instruction-tuning aligns llms to the human brain",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=LKEJPySnlt": {
    "title": "Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training",
    "volume": "main",
    "abstract": "Mixture-of-experts (MoE) models facilitate efﬁcient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. Recently, a fully-differentiable MoE architecture SMEAR was proposed (Muqeeth et al., 2023), which softly merges experts in the parameter space. Nevertheless, its effectiveness was only demonstrated in downstream ﬁne-tuning on classiﬁcation tasks. In this paper, we present Lory, a novel approach that scales such architectures to autoregressive language model pre-training. Lory introduces two key techniques: (1) a causal segment routing strategy that achieves high efﬁciency for expert merging operations while preserving the autoregressive nature of language models; (2) a similarity-based data batching method that encourages expert specialization by grouping similar documents in training instances. We pre-train a series of Lory models from scratch on 150B tokens, with up to 32 experts and 30B (1.5B active) parameters. Experimental results show signiﬁcant performance gains over parameter-matched dense models in both perplexity (+13.9%) and a variety of downstream tasks (+1.5%-11.1%). Despite segment-level routing, Lory models achieve competitive performance compared to state-of-the-art MoE models with token-level routing. We further demonstrate that the trained experts capture domain-level specialization without supervision. Our work highlights the potential of fully-differentiable MoE architectures for language model pre-training and advocates future research in this area",
    "checked": true,
    "id": "4b879f069d023e03bf537309a99bdaeb39916ea5",
    "semantic_title": "lory: fully differentiable mixture-of-experts for autoregressive language model pre-training",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=dJMTn3QOWO": {
    "title": "Fine-grained Hallucination Detection and Editing for Language Models",
    "volume": "main",
    "abstract": "Large language models (LMs) are prone to generate factual errors, which are often called hallucinations. In this paper, we introduce a comprehensive taxonomy of hallucinations and argue that hallucinations manifest in diverse forms, each requiring varying degrees of careful assessments to verify factuality. We propose a novel task of automatic fine-grained hallucination detection and construct a new evaluation benchmark, FavaBench, that includes about one thousand fine-grained human judgments on three LM outputs across various domains. Our analysis reveals that ChatGPT and Llama2-Chat (70B, 7B) exhibit diverse types of hallucinations in the majority of their outputs in information-seeking scenarios. We train FAVA, a retrieval-augmented LM by carefully creating synthetic data to detect and correct fine-grained hallucinations. On our benchmark, our automatic and human evaluations show that FAVA significantly outperforms ChatGPT and GPT-4 on fine-grained hallucination detection, and edits suggested by FAVA improve the factuality of LM-generated text",
    "checked": true,
    "id": "028d75496e51943f52c7b2177344a3c089c18058",
    "semantic_title": "fine-grained hallucination detection and editing for language models",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=7QaEO9WYMa": {
    "title": "Poly-Visual-Expert Vision-Language Models",
    "volume": "main",
    "abstract": "Current large vision-language models (VLMs) frequently face challenges such as the limited capabilities of a single visual component and the excessive length of visual tokens. These issues can limit the model's ability to interpret complex visual information and over-lengthy contextual information accurately. Tackling these challenges is crucial for enhancing the performance and applicability of VLMs. This paper proposes leveraging the ensemble experts technique to synergize the capabilities of individual visual encoders, including those skilled in image-text matching, image segmentation, OCR, etc. This method introduces a fusion network that consolidates the outputs from different visual experts while bridging the gap between image encoders and pre-trained LLMs. In addition, we explore different positional encoding schemes to mitigate the waste of positional encoding caused by lengthy image feature sequences, effectively addressing the issue of position overflow and length limitations. For instance, in our implementation, this technique significantly reduces the positional occupancy in models like SAM, from a substantial 4096 to a more efficient 64 or even down to 1. Experimental results show that VLMs with multiple experts consistently outperform isolated visual encoders, with notable performance improvements as more experts are integrated. Our codes are available on our project website",
    "checked": false,
    "id": "ab003ca4c9479d1b155f8da9505160e8c07e83ce",
    "semantic_title": "mousi: poly-visual-expert vision-language models",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=mUlLf50Y6H": {
    "title": "Is ChatGPT a Good Sentiment Analyzer?",
    "volume": "main",
    "abstract": "Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly interested in whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a comprehensive evaluation of ChatGPT on the understanding of \\emph{opinions}, \\emph{sentiments}, and \\emph{emotions} contained in the text. Specifically, we evaluate it in three settings, including \\emph{standard} evaluation, \\emph{polarity shift} evaluation and \\emph{open-domain} evaluation. We conduct an evaluation on 7 representative sentiment analysis tasks covering 17 benchmark datasets and compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on them. We also attempt several popular prompting techniques to elicit the ability further. Moreover, we conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities",
    "checked": false,
    "id": "1aeb3239735e28c7318af096044e48d919ea500b",
    "semantic_title": "is chatgpt a good sentiment analyzer? a preliminary study",
    "citation_count": 163,
    "authors": []
  },
  "https://openreview.net/forum?id=F2yGbwXJAi": {
    "title": "Suspicion Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4",
    "volume": "main",
    "abstract": "Unlike perfect information games, where all elements are known to every player, imperfect information games emulate the real-world complexities of decision-making under uncertain or incomplete information. GPT-4, the recent breakthrough in large language models (LLMs) trained on massive passive data, is notable for its knowledge retrieval and reasoning abilities. This paper delves into the applicability of GPT-4's learned knowledge for imperfect information games. To achieve this, we introduce \\textbf{\\agentname{}}, an innovative agent that leverages GPT-4's capabilities for imperfect information games. With proper prompt engineering to achieve different functions, \\agentname{} based on GPT-4 demonstrates remarkable adaptability across a range of imperfect information card games. Importantly, GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it can understand others and intentionally impact others' behavior. Leveraging this, we design a planning strategy that enables GPT-4 to competently play against different opponents, adapting its gameplay style as needed, while requiring only the game rules and descriptions of observations as input. In the experiments, we qualitatively showcase the capabilities of \\agentname{} across three different imperfect information games and then quantitatively evaluate it in Leduc Hold'em. {As an exploration study, we show that \\agentname{} can potentially outperform traditional algorithms without any specialized training or examples, but still cannot beat Nash-Equilibrium algorithms}. In order to encourage and foster deeper insights within the community, we make our game-related data publicly available",
    "checked": false,
    "id": "c74e9642ec71c6dfaadd3b8638c110d4048ff53e",
    "semantic_title": "suspicion-agent: playing imperfect information games with theory of mind aware gpt-4",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=2cop2jmQVL": {
    "title": "Stream of Search (SoS): Learning to Search in Language",
    "volume": "main",
    "abstract": "Language models are rarely shown fruitful mistakes while training. They then struggle to look beyond the next token, suffering from a snowballing of errors and struggling to predict the consequence of their actions several steps ahead. In this paper, we show how language models can be taught to search by representing the process of search in language, as a flattened string --- stream of search (SoS). We propose a unified language for search that captures an array of different symbolic search strategies. We demonstrate our approach using the simple yet difficult game of Countdown, where the goal is to combine input numbers with arithmetic operations to reach a target number. We pretrain a transformer-based language model from scratch on a dataset of streams of search generated by heuristic solvers. We find that SoS pretraining increases search accuracy by 25\\% over models trained to predict only the optimal search trajectory. We further finetune this model with two policy improvement methods: Advantage-Induced Policy Alignment (APA) and Self-Taught Reasoner (STaR). The finetuned SoS models solve 36\\% of previously unsolved problems, including problems that cannot be solved by any of the heuristic solvers. Our results indicate that language models can learn to solve problems via search, self-improve to flexibly use different search strategies, and potentially discover new ones",
    "checked": true,
    "id": "b2688b3a0ddf190cd99b11b6bf589a6e071c5369",
    "semantic_title": "stream of search (sos): learning to search in language",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=i2oJjC0ESQ": {
    "title": "Does In-Context Learning Really Learn? Rethinking How Large Language Models Respond and Solve Tasks via In-Context Learning",
    "volume": "main",
    "abstract": "In-context Learning (ICL) has emerged as a powerful capability alongside the development of scaled-up large language models (LLMs). By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without updating millions of parameters. However, the precise contributions of demonstrations towards improving end-task performance have not been thoroughly investigated in recent analytical studies. In this paper, we empirically decompose the overall performance of ICL into three dimensions, label space, format, and discrimination, and we evaluate four general-purpose LLMs across a diverse range of tasks. Counter-intuitively, we find that the demonstrations have a marginal impact on provoking discriminative knowledge of language models. However, ICL exhibits significant efficacy in regulating the label space and format, which helps LLMs respond to desired label words. We then demonstrate that this ability functions similar to detailed instructions for LLMs to follow. We additionally provide an in-depth analysis of the mechanism of retrieval helping with ICL. Our findings demonstrate that retrieving the semantically similar examples notably boosts the model's discriminative capability. However, we also observe a trade-off in selecting good in-context examples regarding label diversity",
    "checked": true,
    "id": "6d306d582ce303c16f3a2387cfdf1c846ac82973",
    "semantic_title": "does in-context learning really learn? rethinking how large language models respond and solve tasks via in-context learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Zu8OWNUC0u": {
    "title": "Nonparametric Variational Regularisation of Pretrained Transformers",
    "volume": "main",
    "abstract": "Pretrained transformers have demonstrated impressive abilities, but tend not to generalise well out-of-domain and are very expensive to fine-tune on new domain data. Nonparametric Variational Information Bottleneck (NVIB) has been proposed as a regulariser for training cross-attention in transformers, potentially addressing this domain overfitting problem. We extend the NVIB framework to replace all types of attention functions in transformers. We show that existing pretrained transformers can be reinterpreted as nonparametric variational models using an empirical prior distribution and identity initialisation with controllable hyperparameters. We then show that changing the initialisation introduces a novel, information-theoretic post-training regularisation in the attention mechanism, which improves out-of-domain generalisation on NLP tasks without any additional training. This success supports the hypothesis that the way pretrained transformer embeddings represent information is accurately characterised by nonparametric variational Bayesian models",
    "checked": true,
    "id": "dcb19c24e61a5e4f5ece966f232597f911d624af",
    "semantic_title": "nonparametric variational regularisation of pretrained transformers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tzE7VqsaJ4": {
    "title": "RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Queries for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9% across three single-hop QA datasets, and when applied to a 8B Llama3 model, it also demonstrates enhanced performance in handling complex, multi-hop QA datasets",
    "checked": true,
    "id": "746b96ee17e329f1085a047116c05e12eaa3925a",
    "semantic_title": "rq-rag: learning to refine queries for retrieval augmented generation",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=5RdIMlGLXL": {
    "title": "LLM-Datasets: An Open Framework for Pretraining Datasets of Large Language Models",
    "volume": "main",
    "abstract": "Large language models have become the cornerstone of today's natural language processing research. To facilitate the training, evaluation, and deployment of language models, the community has developed a series of tools and frameworks and made them openly available. This joint community effort has led to more collaboration, standardization, and overall more progress in language model research. However, one crucial aspect of large language models has been neglected so far: the pretraining datasets. To address this gap, we present an open framework for the collection and systematic compilation of pretraining datasets, called LLM-Datasets. With LLM-Datasets, we make a community-effort and collaborate with experts from the individual languages to collect and systematically compile datasets suitable in terms of data quantity and quality for pretraining language models in a multilingual setting. The framework provides a unified interface to pretraining datasets enabling the download, text extraction, filtering, and sampling of the pretraining data. It is modular and extensible with new datasets and designed with high-performance-computing requirements in mind that are needed to achieve the scale of today's language models. Users of the framework can focus on the actual data composition and reuse existing datasets from the community while ensuring reproducibility. To showcase LLM-Datasets, we compiled a pretraining dataset with 2.3 trillion tokens for a large language model covering 32 European languages",
    "checked": false,
    "id": "af067c6f1c12941625e4c3b49b002c7c7c0b2542",
    "semantic_title": "bookgpt: a general framework for book recommendation empowered by large language model",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=CybBmzWBX0": {
    "title": "Length-Controlled AlpacaEval: A Simple Debiasing of Automatic Evaluators",
    "volume": "main",
    "abstract": "LLM-based auto-annotators have become a key component of the LLM development process due to their cost-effectiveness and scalability compared to human-based evaluation. However, these auto-annotators can introduce complex biases that are hard to remove. Even simple, known confounders such as preference for longer outputs remains in existing automated evaluation metrics. We propose a simple regression analysis approach for controlling biases in auto-evaluations. As a real case study, we focus on reducing the length bias of AlpacaEval, a fast and affordable benchmark for instruction-following LLMs that uses LLMs to estimate response quality. Despite being highly correlated with human preferences, AlpacaEval is known to favor models that generate longer outputs. We introduce a length-controlled AlpacaEval that aims to answer the counterfactual question: \"What would the preference be if the model's and baseline's output had the same length?\" To achieve this, we first fit a GLM to predict the biased output of interest (auto-annotator preferences) based on the mediators we want to control for (length difference) and other relevant features. We then obtain length-controlled preferences by predicting preferences while conditioning the GLM with a zero difference in lengths. Length-controlling not only improves the robustness of the metric to manipulations in model verbosity, we also find that it increases the Spearman correlation with LMSYS' Chatbot Arena from 0.94 to 0.98. We release \\thecode{} and \\leaderboard{}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3X2L2TFr0f": {
    "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies",
    "volume": "main",
    "abstract": "The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly~\\footnote{\\url{https://github.com/OpenBMB/MiniCPM}}",
    "checked": true,
    "id": "49873ee415619efd9e1e4c16f73ee066ff008c1f",
    "semantic_title": "minicpm: unveiling the potential of small language models with scalable training strategies",
    "citation_count": 412,
    "authors": []
  },
  "https://openreview.net/forum?id=6U1FEKP7Ar": {
    "title": "ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning",
    "volume": "main",
    "abstract": "Compositional visual reasoning methods, which translate a complex query into a structured composition of feasible visual tasks, have exhibited a strong potential in complicated multi-modal tasks. Empowered by recent advances in large language models (LLMs), this multi-modal challenge has been brought to a new stage by treating LLMs as few-shot/zero-shot planners, i.e., vision-language (VL) programming. Such methods, despite their numerous merits, suffer from challenges due to LLM planning mistakes or inaccuracy of visual execution modules, lagging behind the non-compositional models. In this work, we devise a \"plug-and-play\" method, ExoViP, to correct errors in both the planning and execution stages through introspective verification. We employ verification modules as \"exoskeletons\" to enhance current VL programming schemes. Specifically, our proposed verification module utilizes a mixture of three sub-verifiers to validate predictions after each reasoning step, subsequently calibrating the visual module predictions and refining the reasoning trace planned by LLMs. Experimental results on two representative VL programming methods showcase consistent improvements on five compositional reasoning tasks on standard benchmarks. In light of this, we believe that ExoViP can foster better performance and generalization on open-domain multi-modal challenges",
    "checked": true,
    "id": "96f10e92ab92048d965acfbbc25a65955f33871f",
    "semantic_title": "exovip: step-by-step verification and exploration with exoskeleton modules for compositional visual reasoning",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=MoitXWlXcS": {
    "title": "Why do small language models underperform? Studying Language Model Saturation via the Softmax Bottleneck",
    "volume": "main",
    "abstract": "Recent advances in language modeling consist in pretraining highly parameterized neural networks on extremely large web-mined text corpora. Training and inference with such models can be costly in practice, which incentivizes the use of smaller counterparts. However, it has been observed that smaller models can suffer from saturation, characterized as a drop in performance at some advanced point in training followed by a plateau. In this paper, we find that such saturation can be explained by a mismatch between the hidden dimension of smaller models and the high rank of the target contextual probability distribution. This mismatch affects the performance of the linear prediction head used in such models through the well-known softmax bottleneck phenomenon. We measure the effect of the softmax bottleneck in various settings and estimate that models based on less than roughly 1000 hidden dimensions tend to adopt degenerate latent representations in late pretraining, which leads to reduced evaluation performance",
    "checked": true,
    "id": "5df905efd0c79e9a39b98a537769cbe9f9462847",
    "semantic_title": "why do small language models underperform? studying language model saturation via the softmax bottleneck",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=3TzGD95Jw1": {
    "title": "Timo: Towards Better Temporal Reasoning for Language Models",
    "volume": "main",
    "abstract": "Reasoning about time is essential for Large Language Models (LLMs) to understand the world. Previous works focus on solving specific tasks, primarily on time-sensitive question answering. While these methods have proven effective, they cannot generalize to a wider spectrum of temporal reasoning tasks. Therefore, we propose a crucial question: Can we build a universal framework to handle a variety of temporal reasoning tasks? To that end, we systematically study 38 temporal reasoning tasks. Based on the observation that 19 tasks are directly related to mathematics, we first leverage the available mathematical dataset to set a solid foundation for temporal reasoning. However, the in-depth study indicates that focusing solely on mathematical enhancement falls short of addressing pure temporal reasoning tasks. To mitigate this limitation, we propose a simple but effective self-critic temporal optimization method to enhance the model's temporal reasoning capabilities without sacrificing general task abilities. Finally, we develop Timo, a model designed to excel in temporal reasoning at the 7B and 13B scales. Notably, Timo outperforms the counterpart LLMs by 10.0 and 7.6 in average accuracy scores and achieves the new state-of-the-art (SOTA) performance of comparable size. Extensive experiments further validate our framework's effectiveness and its generalization across diverse temporal tasks. The code is available at https://github.com/zhaochen0110/Timo",
    "checked": true,
    "id": "d78763acfc8aaba3d6deecb8fb1d5b829d7c3a11",
    "semantic_title": "timo: towards better temporal reasoning for language models",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=iMqJsQ4evS": {
    "title": "LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models",
    "volume": "main",
    "abstract": "This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic reasoning as a critical cognitive capability and offers insights into future research directions and potential improvements",
    "checked": true,
    "id": "ac8bc36c253c8593492afc2db620c01b152dc778",
    "semantic_title": "llm as a mastermind: a survey of strategic reasoning with large language models",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=8tKjqqMM5z": {
    "title": "Keep the Cost Down: A Review on Methods to Optimize LLM's KV-Cache Consumption",
    "volume": "main",
    "abstract": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022, have revolutionized various industries with their advanced language comprehension. However, their efficiency is challenged by the Transformer architecture's struggle with handling long texts. KV-Cache has emerged as a pivotal solution to this issue, converting the time complexity of token generation from quadratic to linear, albeit with increased GPU memory overhead proportional to conversation length. With the development of the LLM community and academia, various KV-Cache compression methods have been proposed. In this review, we dissect the various properties of KV-Cache and elaborate on various methods currently used to optimize the KV-Cache space usage of LLMs. These methods span the pre-training phase, deployment phase, and inference phase, and we summarize the commonalities and differences among these methods. Additionally, we list some metrics for evaluating the long-text capabilities of large language models, from both efficiency and capability perspectives. Our review thus sheds light on the evolving landscape of LLM optimization, offering insights into future advancements in this dynamic field",
    "checked": false,
    "id": "6c67e17fdd763f212e582f8de56b56dd5ed58832",
    "semantic_title": "keep the cost down: a review on methods to optimize llm' s kv-cache consumption",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=y6SqbJfCSk": {
    "title": "HGRN2: Gated Linear RNNs with State Expansion",
    "volume": "main",
    "abstract": "Hierarchically gated linear RNN (HGRN) has demonstrated competitive training speed and performance in language modeling while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, limiting its expressiveness. To address this issue, we introduce a simple outer product-based state expansion mechanism, which significantly enlarges the recurrent state size without introducing any additional parameters. This enhancement also provides a linear attention interpretation for HGRN2, enabling hardware-efficient training. Our extensive experiments verify the advantage of HGRN2 over HGRN consistently across different settings and comptetive to other recurrent models",
    "checked": true,
    "id": "46732358e98ce6be0c564ae11f71d556a64b4c35",
    "semantic_title": "hgrn2: gated linear rnns with state expansion",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=0VLBwQGWpA": {
    "title": "ReAct Meets ActRe: Autonomous Annotation of Agent Trajectories for Contrastive Self-Training",
    "volume": "main",
    "abstract": "Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotation or implementations of diverse prompting frameworks. In this work, we propose A$^\\mathbf{3}$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent executes multiple trajectories for the failed tasks, and selects the successful ones to supplement its failed trajectory for contrastive self-training. Realized by policy gradient methods with binarized rewards, the contrastive self-training with accumulated trajectories facilitates a closed loop for multiple rounds of language agent self-improvement. We conduct experiments using QLoRA fine-tuning with the open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with A$^3$T obtains a 1-shot success rate of 96\\%, and 100\\% success with 4 iterative rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human average, and 4 rounds of iterative refinement lead to the performance approaching human experts. A$^3$T agents significantly outperform existing techniques, including prompting with GPT-4, advanced agent frameworks, and fully fine-tuned LLMs",
    "checked": false,
    "id": "cd0f365fca59f303ce158c36faa7a7f430a5a698",
    "semantic_title": "react meets actre: autonomous annotations of agent trajectories for contrastive self-training",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=ndY9qFf9Sa": {
    "title": "AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts",
    "volume": "main",
    "abstract": "We introduce AdaMoLE, a novel method for fine-tuning large language models (LLMs) through an Adaptive Mixture of Low-Rank Adaptation (LoRA) Experts. Moving beyond conventional methods that employ a static top-k strategy for activating experts, AdaMoLE dynamically adjusts the activation threshold using a dedicated threshold network, adaptively responding to the varying complexities of different tasks. By replacing a single LoRA in a layer with multiple LoRA experts and integrating a gating function with the threshold mechanism, AdaMoLE effectively selects and activates the most appropriate experts based on the input context. Our extensive evaluations across a variety of commonsense reasoning and natural language processing tasks show that AdaMoLE exceeds baseline performance. This enhancement highlights the advantages of AdaMoLE's adaptive selection of LoRA experts, improving model effectiveness without a corresponding increase in the expert count. The experimental validation not only confirms AdaMoLE as a robust approach for enhancing LLMs but also suggests valuable directions for future research in adaptive expert selection mechanisms, potentially broadening the scope for optimizing model performance across diverse language processing tasks",
    "checked": true,
    "id": "f833cb351af8cf540808fc523a24a3eb405a8150",
    "semantic_title": "adamole: fine-tuning large language models with adaptive mixture of low-rank adaptation experts",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=h5umhm6mzj": {
    "title": "NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness",
    "volume": "main",
    "abstract": "Existing evaluation benchmarks of language models of code (code LMs) focus almost exclusively on whether the LMs can generate functionally-correct code. In real-world software engineering, developers think beyond functional correctness. They have requirements on \"how'' a functionality should be implemented to meet overall system design objectives like efficiency, security, and maintainability. They would also trust the code LMs more if the LMs demonstrate robust understanding of such requirements. We propose a new benchmark NoFunEval to evaluate code LMs on non-functional requirements and simple classification instances for both functional and non-functional requirements. We propose a prompting method, Coding Concepts (CoCo), as a way for a developer to communicate the domain knowledge to the LMs. We conduct an extensive evaluation of twenty-two code LMs. Our finding is that they generally falter when tested on our benchmark, hinting at fundamental blindspots in their training setups. Surprisingly, even the classification accuracy on functional-correctness instances derived from the popular HumanEval benchmark is low, calling in question the depth of their comprehension and the source of their success in generating functionally-correct code in the first place. We release our benchmark and evaluation scripts publicly at https://aka.ms/NoFunEval",
    "checked": true,
    "id": "f5f00afb894c5f3cb6decc531c9b64d49e865875",
    "semantic_title": "nofuneval: funny how code lms falter on requirements beyond functional correctness",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=jt0R50d5nk": {
    "title": "Can MLLMs Perform Text-to-Image In-Context Learning?",
    "volume": "main",
    "abstract": "The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to its multimodal counterpart. Existing such studies have primarily concentrated on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique characteristics and potential applications, remains underexplored. To address this gap, we formally define the task of T2I-ICL and present **CoBSAT**, the first T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to benchmark six state-of-the-art MLLMs, we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation, and show that strategies such as fine-tuning and Chain-of-Thought prompting help to mitigate these difficulties, leading to notable improvements in performance. Our code and dataset are available at https://github.com/UW-Madison-Lee-Lab/CoBSAT",
    "checked": true,
    "id": "3e6a9b9bb793458ca0e97eab78251ab38f8fd417",
    "semantic_title": "can mllms perform text-to-image in-context learning?",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=7iaAlIlV2H": {
    "title": "Pairwise Proximal Policy Optimization: Language Model Alignment with Comparative RL",
    "volume": "main",
    "abstract": "LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior is Reinforcement Learning with Human Feedback (RLHF). This involves training a reward model with a human-labeled ranking dataset and fine-tuning the LLM with the reward signal using RL. Despite the fact that the reward is learned from comparing different responses, the RL stage doesn't involve direct comparisons. This inconsistency between reward learning and reinforcement learning stages exacerbates RL's instability. An example would be that the well adopted RL optimizer, Proximal Policy Optimization (PPO), could perform different gradient updates even for batches with identical human preference information. To address this, we propose a new framework, reinforcement learning with comparative feedback, and a simple policy gradient algorithm, Pairwise Proximal Policy Optimization (P3O), that learns to improve from direct comparison. Theoretically, P3O has the nice property of being invariant with any reward functions that contain identical preference information, while doesn't require learning a value function. Empirical evaluations demonstrate that P3O can align with human preferences better than existing methods. This suggest that comparative RL is strong candidate for aligning LLM with preference data",
    "checked": false,
    "id": "951a11a4d247de5823dfa42affe39cf849c708b8",
    "semantic_title": "pairwise proximal policy optimization: large language models alignment via comparative rl",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=zZa7Ke7WAJ": {
    "title": "Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM",
    "volume": "main",
    "abstract": "Large language models (LLMs) have become the go-to choice for code generation tasks, with an exponential increase in the training, development, and usage of LLMs specifically for code generation. To evaluate the ability of LLMs on code, both academic and industry practitioners rely on popular handcrafted benchmarks. However, prior benchmarks contain only a very limited set of problems, both in quantity and variety. Further, due to popularity and age, many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data. Such limitations inevitably lead us to inquire: Is the leaderboard performance on existing benchmarks reliable and comprehensive enough to measure the program synthesis ability of LLMs? To address this, we introduce EvoEval – a program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains for a comprehensive evaluation of LLM coding abilities. Our study on 51 LLMs shows that compared to the high performance obtained on standard benchmarks like HumanEval, there is a significant drop in performance (on average 39.4%) when using EvoEval. Additionally, the decrease in performance can range from 19.6% to 47.7%, leading to drastic ranking changes amongst LLMs and showing potential overfitting of existing benchmarks. Furthermore, we showcase various insights including the brittleness of instruction-following models when encountering rewording or subtle changes as well as the importance of learning problem composition and decomposition. EvoEval not only provides comprehensive benchmarks, but can be used to further evolve arbitrary problems to keep up with advances and the ever-changing landscape of LLMs for code. We have open-sourced our benchmarks, tools, and all LLM-generated code at https://github.com/evo-eval/evoeval",
    "checked": true,
    "id": "af9df109e409b485e1337362431ba61623195145",
    "semantic_title": "top leaderboard ranking = top coding proficiency, always? evoeval: evolving coding benchmarks via llm",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=7VPKtz8CHN": {
    "title": "Beyond Relevance: Evaluate and Improve Retrievers on Perspective Awareness",
    "volume": "main",
    "abstract": "The task of Information Retrieval (IR) requires a system to identify relevant documents based on users' information needs. In real-world scenarios, retrievers are expected to not only rely on the semantic relevance between the documents and the queries but also recognize the nuanced intents or perspectives behind a user query. For example, when asked to verify a claim, a retrieval system is expected to identify evidence from both supporting vs. contradicting perspectives, for the downstream system to make a fair judgment call. In this work, we study whether retrievers can recognize and respond to different perspectives of the queries --- beyond finding relevant documents for a claim, can retrievers distinguish supporting vs. opposing documents? We reform and extend six existing tasks to create a benchmark for retrieval, where we have diverse perspectives described in free-form text, besides root, neutral queries. We show that current retrievers covered in our experiments have limited awareness of subtly different perspectives in queries and can also be biased toward certain perspectives. Motivated by the observation, we further explore the potential to leverage geometric features of retriever representation space to improve the perspective awareness of retrievers in a zero-shot manner. We demonstrate the efficiency and effectiveness of our projection-based methods on the same set of tasks. Further analysis also shows how perspective awareness improves performance on various downstream tasks, with 4.2% higher accuracy on AmbigQA and 29.9% more correlation with designated viewpoints on essay writing, compared to non-perspective-aware baselines",
    "checked": true,
    "id": "94b9084923eec9f7fe52450cb6ffac513b4f1580",
    "semantic_title": "beyond relevance: evaluate and improve retrievers on perspective awareness",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=T5pGDydMkS": {
    "title": "Adaptive Quantization Error Reconstruction for LLMs with Mixed Precision",
    "volume": "main",
    "abstract": "Large language models (LLMs) has demonstrated superior performance on various downstream tasks. However, their practical applications are hindered by their immense memory and computation requirements. Although recent post-training quantization methods can effectively reduce memory usage and improve computational efficiency, they often overlook the varying sensitivity of different layer weights to bit precision. Additionally, the previous methods suffer from significant accuracy loss under low-bit quantization (2-3 bits). To address these limitations, we propose Adaptive Mixed Precision and Low-Rank Quantization Error Reconstruction for LLMs (AMLQ), which achieves state-of-the-art performance under the approximate average bit precision overall. Furthermore, we introduce the low-rank decomposition to reconstruct quantization error based on the output features. Experimental results demonstrate that this method can be effectively combined with various quantization techniques and bring considerable performance gains. Our approach comprehensively considers model performance and inference efficiency, offering more than 3$\\times$ speedup over the FP16 execution",
    "checked": false,
    "id": "a2781a5583ca624d569783850d035de8a58ecf13",
    "semantic_title": "mc-moe: mixture compressor for mixture-of-experts llms gains more",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=7ysaJGs7zY": {
    "title": "IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models",
    "volume": "main",
    "abstract": "The advent of Vision Language Models (VLM) has allowed researchers to investigate the visual understanding of a neural network using natural language. Beyond object classification and detection, VLMs are capable of visual comprehension and common-sense reasoning. This naturally led to the question: How do VLMs respond when the image itself is inherently *unreasonable*? To this end, we present IllusionVQA: a diverse dataset of challenging optical illusions and hard-to-interpret scenes to test the capability of VLMs in two distinct multiple-choice VQA tasks - comprehension and soft localization. GPT4V, the best performing VLM, achieves 62.99\\% accuracy (4-shot) on the comprehension task and 49.7\\% on the localization task (4-shot and Chain-of-Thought). Human evaluation reveals that humans achieve 91.03\\% and 100\\% accuracy in comprehension and localization. We discover that In-Context Learning (ICL) and Chain-of-Thought reasoning substantially degrade the performance of Gemini-Pro on the localization task. Tangentially, we discover a potential weakness in the ICL capabilities of VLMs: they fail to locate optical illusions even when the correct answer is in the context window as a few-shot example",
    "checked": true,
    "id": "20a3c8c5d41e41acd5e672967635389578914313",
    "semantic_title": "illusionvqa: a challenging optical illusion dataset for vision language models",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=5u1GpUkKtG": {
    "title": "Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking",
    "volume": "main",
    "abstract": "Reward models play a key role in aligning language model applications towards human preferences. However, this setup creates an incentive for the language model to exploit errors in the reward model to achieve high estimated reward, a phenomenon often termed \\emph{reward hacking}. A natural mitigation is to train an ensemble of reward models, aggregating over model outputs to obtain a more robust reward estimate. We explore the application of reward ensembles to alignment at both training time (through reinforcement learning) and inference time (through reranking). First, we show that reward models are \\emph{underspecified}: reward models that perform similarly in-distribution can yield very different rewards when used in alignment, due to distribution shift. Second, underspecification results in overoptimization, where alignment to one reward model does not improve reward as measured by another reward model trained on the same data. Third, overoptimization is mitigated by the use of reward ensembles, and ensembles that vary by their \\emph{pretraining} seeds lead to better generalization than ensembles that differ only by their \\emph{fine-tuning} seeds, with both outperforming individual reward models. However, even pretrain reward ensembles do not eliminate reward hacking: we show several qualitative reward hacking phenomena that are not mitigated by ensembling because all reward models in the ensemble exhibit similar error patterns",
    "checked": true,
    "id": "b4afc7b4a6836054c1b4568ce9d49993afbb0461",
    "semantic_title": "helping or herding? reward model ensembles mitigate but do not eliminate reward hacking",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=Szp33itD10": {
    "title": "StyleTalker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation",
    "volume": "main",
    "abstract": "The rapid advancement of large language models (LLMs) has significantly propelled the development of text-based chatbots, demonstrating their capability to engage in coherent and contextually relevant dialogues. However, extending these advancements to enable end-to-end speech-to-speech conversation bots remains a formidable challenge, primarily due to the extensive dataset and computational resources required. The conventional approach of cascading automatic speech recognition (ASR), LLM, and text-to-speech (TTS) models in a pipeline, while effective, suffers from unnatural prosody because it lacks direct interactions between the input audio and its transcribed text and the output audio. These systems are also limited by their inherent latency from the ASR process for real-time applications. This paper introduces Style-Talker, an innovative framework that fine-tunes an audio LLM alongside a style-based TTS model for fast spoken dialog generation. Style-Talker takes user input audio and uses transcribed chat history and speech styles to generate both the speaking style and text for the response. Subsequently, the TTS model synthesizes the speech, which is then played back to the user. While the response speech is being played, the input speech undergoes ASR processing to extract the transcription and speaking style, serving as the context for the ensuing dialogue turn. This novel pipeline accelerates the traditional cascade ASR-LLM-TTS systems while integrating rich paralinguistic information from input speech. Our experimental results show that Style-Talker significantly outperforms the conventional cascade and speech-to-speech baselines in terms of both dialogue naturalness and coherence while being more than 50\\% faster. The demo and code are available at https://styletalker.github.io/",
    "checked": false,
    "id": "5550118041a89121e9d7274f83aef420cd9ed487",
    "semantic_title": "style-talker: finetuning audio language model and style-based text-to-speech model for fast spoken dialogue generation",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=aKwQPRjdGa": {
    "title": "Hummer: Towards Limited Competitive Preference Dataset",
    "volume": "main",
    "abstract": "Preference datasets are essential for incorporating human preferences into pre-trained language models, playing a key role in the success of Reinforcement Learning from Human Feedback. However, these datasets often demonstrate conflicting alignment objectives, leading to increased vulnerability to jailbreak attacks and challenges in adapting downstream tasks to prioritize specific alignment objectives without negatively impacting others. In this work, we introduce a novel statistical metric, Alignment Dimension Conflict, to quantify the degree of conflict within preference datasets. We then present \\texttt{Hummer} and its fine-grained variant, \\texttt{Hummer-F}, as innovative pairwise preference datasets with reduced-conflict alignment objectives. \\texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback from GPT-4, marking as the first preference dataset aimed at reducing the competition between alignment objectives. Furthermore, we develop reward models, \\texttt{HummerRM} and \\texttt{HummerRM-F}, which employ a hybrid sampling approach to balance diverse alignment objectives effectively. This sampling method positions \\texttt{HummerRM} as an ideal model for domain-specific further fine-tuning and reducing vulnerability to jailbreak attacks",
    "checked": true,
    "id": "04762cbef4a118f56d9ad8f2d6d400e0e32dc6ef",
    "semantic_title": "hummer: towards limited competitive preference dataset",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=iI1CzEhEMU": {
    "title": "Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability",
    "volume": "main",
    "abstract": "Large language models (LLMs) have emerged as powerful tools for many AI problems and exhibit remarkable in-context learning (ICL) capabilities. Compositional ability, solving unseen complex tasks that combine two or more simple tasks, is an essential reasoning ability for Artificial General Intelligence. Despite the tremendous success of LLMs, how they approach composite tasks, especially those not encountered during the pretraining phase, remains an open and largely underexplored question. In this study, we delve into the ICL capabilities of LLMs on composite tasks, with only simple tasks as in-context examples. We develop a test suite of composite tasks including linguistic and logical challenges and perform empirical studies across different LLM families. We observe that models exhibit divergent behaviors: (1) For simpler composite tasks that apply distinct mapping mechanisms to different input segments, the models demonstrate decent compositional ability, while scaling up the model enhances this ability; (2) for more complex composite tasks involving reasoning multiple steps, where each step represents one task, models typically underperform, and scaling up generally provides no improvements. We offer theoretical analysis in a simplified setting, explaining that models exhibit compositional capability when the task handles different input parts separately. We believe our work sheds new light on the capabilities of LLMs in solving composite tasks regarding the nature of the tasks and model scale. Our dataset and code are available at {\\url{https://github.com/OliverXUZY/LLM_Compose}}",
    "checked": true,
    "id": "0e177741ef1e09fcf70b4236621d7204bc619439",
    "semantic_title": "do large language models have compositional ability? an investigation into limitations and scalability",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=X1xNsuKssb": {
    "title": "MambaByte: Token-free Selective State Space Model",
    "volume": "main",
    "abstract": "Token-free language models learn directly from raw bytes and remove the inductive bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences. In this setting, standard autoregressive Transformers scale poorly as the effective memory required grows with sequence length. The recent development of the Mamba state space model (SSM) offers an appealing alternative approach with a fixed-sized memory state and efficient decoding. We propose MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences. In terms of modeling, we show MambaByte to be competitive with, and even to outperform, state-of-the-art subword Transformers on language modeling tasks while maintaining the benefits of token-free language models, such as robustness to noise. In terms of efficiency, we develop an adaptation of speculative decoding with tokenized drafting and byte-level verification. This results in a $2.6\\times$ inference speedup to the standard MambaByte implementation, showing similar decoding efficiency as the subword Mamba. These findings establish the viability of SSMs in enabling token-free language modeling",
    "checked": true,
    "id": "a6e2dca754f3dc625a9da5f10f9b7a57079bfd27",
    "semantic_title": "mambabyte: token-free selective state space model",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=TBNYjdOazs": {
    "title": "Decoupling Noise and Toxic Parameters for Language Model Detoxification by Task Vector Merging",
    "volume": "main",
    "abstract": "The goal of detoxifying language models is to reduce the chances of producing offensive or harmful output in pre-trained language models (PLMs), ensuring their safer use. A recently proposed detoxification method utilizes the task vector obtained by subtraction from the fine-tuned model on toxic datasets to the pre-trained model. This approach has shown effectiveness for detoxification but still suffers from degradation. This study focuses on further mitigating degradation while maintaining detoxification performance. To mitigate the degradation, we propose a method that detoxifies the PLMs by fine-tuning multiple models on split toxic datasets and by merging the subtracted task vectors. We conducted experiments on two toxic datasets (Civil Comments and Toxigen) with five PLMs (GPT2-small, GPT2-medium, GPT2-large, Phi-1.5, and Llama2-7b), demonstrating that our method consistently achieves a lower toxicity score while preventing the degradation compared to baseline methods. Especially, with the GPT2-small model on the Toxigen dataset, degradation was reduced by 38.9\\% compared to that of an existing task vector method while maintaining a similar toxicity score. In addition, we found that merging multiple detoxified models tends to increase the number of parameters that remained almost unchanged from the pre-trained model. We assume that by merging multiple detoxified models, \"decoupling noise and toxic parameters\" is implicitly achieved. The accidental noise in the parameter shift unrelated to detoxification disappears by averaging noise, whereas the parameter shift associated with detoxification is maintained. We hope that the findings of this study will be applied not only to detoxification but also to many other research domains that seek to suppress undesirable outputs of language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MmBQSNHKUl": {
    "title": "Are Language Models Robust Coreference Resolvers?",
    "volume": "main",
    "abstract": "Recent work on extending coreference resolution across domains and languages relies on annotated data in both the target domain and language. At the same time, pre-trained large language models (LMs) have been reported to exhibit strong zero- and few-shot learning abilities across a wide range of NLP tasks. However, prior work mostly studied this ability using artificial sentence-level datasets such as the Winograd Schema Challenge. In this paper, we assess the feasibility of prompt-based coreference resolution by evaluating instruction-tuned language models on difficult, linguistically-complex coreference benchmarks (e.g., CoNLL-2012). We show that prompting for coreference can outperform current unsupervised coreference systems, although this approach appears to be reliant on high-quality mention detectors. Further investigations reveal that instruction-tuned LMs generalize surprisingly well across domains, languages, and time periods; yet continued fine-tuning of neural models should still be preferred if small amounts of annotated examples are available",
    "checked": false,
    "id": "874287c656fb35098dfd81a3fd1a88198f8909db",
    "semantic_title": "are large language models robust coreference resolvers?",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=9Ik05cycLq": {
    "title": "Certifying LLM Safety against Adversarial Prompting",
    "volume": "main",
    "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks, which add maliciously designed token sequences to bypass the model's safety guardrails and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with certifiable safety guarantees. Given a prompt, our erase-and-check method erases tokens individually and inspects the resulting subsequences using a safety filter, declaring it harmful if any of the subsequences are detected as harmful. Our safety filters are implemented by leveraging Llama 2 and DistilBERT. We theoretically demonstrate that our method detects harmful prompts with accuracy at least as high as the safety filter. Additionally, we propose three efficient empirical defenses inspired by our erase-and-check (EC) method: i) RandEC, a randomized subsampling version of erase-and-check; ii) GreedyEC, which greedily erases tokens that maximize the softmax score of the harmful class; and iii) GradEC, which uses gradient information to optimize the tokens to erase. Extensive empirical evaluation with real-world datasets demonstrates the effectiveness of the proposed methods in defending against state-of-the-art adversarial prompting attacks",
    "checked": true,
    "id": "1ab91d6ac7afc1a0121487a9089fa70edc1634d4",
    "semantic_title": "certifying llm safety against adversarial prompting",
    "citation_count": 219,
    "authors": []
  },
  "https://openreview.net/forum?id=5B2K4LRgmz": {
    "title": "Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data",
    "volume": "main",
    "abstract": "The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when future models are trained on model-generated data? Recent investigations answered that such model-data feedback loops cause performance to progressively degrades with each model-data iteration until fitted models become useless, a phenomenon termed model collapse. However, those studies largely assumed that new data replace old data over time, where a more realistic assumption is that data accumulate over time. In this paper, we ask: what effect does accumulating data have on model collapse? We first empirically study this question by pretraining sequences of language models on text corpora. After confirming that replacing the original real data by each generation's synthetic data does indeed tend towards model collapse, we demonstrate that accumulating synthetic data with real data avoids model collapse; these results hold across a range of sizes, architectures, and hyperparameters. We obtain similar results for other deep generative models: diffusion models for molecule conformation generation and variational autoencoders for image generation. To understand why accumulating data can avoid model collapse, we use an analytically tractable framework introduced by prior work in which a sequence of linear models are fit to previous models' outputs. Previous work used this framework to show that if data are replaced, the test error increases with the number of model-fitting iterations; we extend this argument to prove that if data instead accumulate, the test error has a finite upper bound independent of the number of iterations, meaning model collapse is avoided. Our work provides consistent empirical and theoretical evidence that data accumulation avoids model collapse",
    "checked": true,
    "id": "e8815da26d4e6cac8b23b7e6aa75cec028cb66d2",
    "semantic_title": "is model collapse inevitable? breaking the curse of recursion by accumulating real and synthetic data",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=PEQFHRUFca": {
    "title": "A Reparameterized Discrete Diffusion Model for Text Generation",
    "volume": "main",
    "abstract": "This work studies discrete diffusion probabilistic models with applications to natural language generation. We derive an alternative yet equivalent formulation of the sampling from discrete diffusion processes and leverage this insight to develop a family of reparameterized discrete diffusion models. The derived generic framework is highly flexible, offers a fresh perspective of the generation process in discrete diffusion models, and features more effective training and decoding techniques. We conduct extensive experiments to evaluate the text generation capability of our model, demonstrating significant improvements over existing diffusion models",
    "checked": true,
    "id": "1f898d66acabff511a3871b82799aa73c0055402",
    "semantic_title": "a reparameterized discrete diffusion model for text generation",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=didvEO1can": {
    "title": "CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text",
    "volume": "main",
    "abstract": "Large language models (LLMs) such as ChatGPT are increasingly proficient in understanding and generating a mixture of code and text. Evaluation based on such *mixture* can lead to a more comprehensive understanding of the models' abilities in solving coding problems. However, in this context, current evaluation methods are either limited in task coverage or lack standardization. To address this issue, we propose using category theory as a framework for evaluation. Specifically, morphisms within a code category can represent code debugging and transformation, functors between two categories represent code translation, and functors between a code category and a natural language category represent code generation, explanation, and reproduction. We present an automatic evaluation framework called **CatCode** (**Cat**egory **Code**) that can comprehensively assess the coding abilities of LLMs, including ChatGPT, Text-Davinci, and CodeGeeX. Large language models (LLMs) are increasingly proficient in understanding and generating a mixture of code and text. Evaluation based on such *mixture* can lead to a more comprehensive understanding of the models' abilities in solving coding problems. However, current evaluation methods are either limited in task coverage or lack standardization. To address this issue, we propose to apply category theory as math abstraction for code-related evaluation. Specifically, morphisms within a code category can represent code debugging and transformation, functors between two categories represent code translation, and functors between a code category and a natural language category represent code generation and explanation. We present an automatic evaluation framework called **CatCode** (**Cat**egory *Code*) that can assess the coding abilities of various ChatGPT-like LLMs in a *comprehensive* and *standard* way, and further support *composite* task evaluation. The code can be found in https://github.com/scorpio-nova/CatCode",
    "checked": true,
    "id": "2d04583175cfb47b6fea99bff85930708793fe8a",
    "semantic_title": "catcode: a comprehensive evaluation framework for llms on the mixture of code and text",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hDoN0CAy5e": {
    "title": "Characterizing Multimodal Long-form Summarization: A Case Study on Financial Reports",
    "volume": "main",
    "abstract": "As large language models (LLMs) expand the power of natural language processing to handle long inputs, rigorous and systematic analyses are necessary to understand their abilities and behavior. A salient application is summarization, due to its ubiquity and controversy (e.g., researchers have declared the death of summarization). In this paper, we use financial report summarization as a case study because financial reports are not only long but also use numbers and tables extensively. We propose a computational framework for characterizing multimodal long-form summarization and investigate the behavior of Claude 2.0/2.1, GPT-4/3.5, and Cohere. We find that GPT-3.5 and Cohere fail to perform this summarization task meaningfully. For Claude 2 and GPT-4, we analyze the extractiveness of the summary and identify a position bias in LLMs. This position bias disappears after shuffling the input for Claude, which suggests that Claude seems to recognize important information. We also conduct a comprehensive investigation on the use of numeric data in LLM-generated summaries and offer a taxonomy of numeric hallucination. We employ prompt engineering to improve GPT-4's use of numbers with limited success. Overall, our analyses highlight the strong capability of Claude 2 in handling long multimodal inputs compared to GPT-4. The generated summaries and evaluation code are available at https://github.com/ChicagoHAI/characterizing-multimodal-long-form-summarization",
    "checked": true,
    "id": "b59aea21f0cc2ba9478eded78213fc448704ed58",
    "semantic_title": "characterizing multimodal long-form summarization: a case study on financial reports",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=60a1SAtH4e": {
    "title": "Measuring and Controlling Instruction (In)Stability in Language Model Dialogs",
    "volume": "main",
    "abstract": "System-prompting is a standard tool for customizing language-model chatbots, enabling them to follow a specific instruction. An implicit assumption in the use of system prompts is that they will be _stable_, so the chatbot will continue to generate text according to the stipulated instructions for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating instruction stability via self-chats between two instructed chatbots. Testing popular models like LLaMA2-chat-70B and GPT-3.5, we reveal a significant _instruction drift_ within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to _attention decay_ over long exchanges. To combat attention decay and instruction drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines. Code: [https://github.com/likenneth/persona_drift](https://github.com/likenneth/persona_drift)",
    "checked": true,
    "id": "37cd61cdc7fc9db4791cc15706704a4789e96f0c",
    "semantic_title": "measuring and controlling instruction (in)stability in language model dialogs",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=H1Edd5d2JP": {
    "title": "LLM4Causal: Democratized Causal Tools for Everyone via Large Language Model",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have shown their success in language understanding and reasoning on general topics. However, their capability to perform inference based on user-specified structured data and knowl- edge in corpus-rare concepts, such as causal decision-making is still limited. In this work, we explore the possibility of fine-tuning an open-sourced LLM into LLM4Causal, which can identify the causal task, execute a cor- responding function, and interpret its numerical results based on users' queries and the provided dataset. Meanwhile, we propose a data gen- eration process for more controllable GPT prompting and present two instruction-tuning datasets: (1) Causal-Retrieval-Bench for causal problem identification and input parameter extraction for causal function calling and (2) Causal-Interpret-Bench for in-context causal interpretation. By conducting end-to-end evaluations and two ablation studies, we showed that LLM4Causal can deliver end-to-end solutions for causal problems and provide easy-to-understand answers, which significantly outperforms the baselines",
    "checked": true,
    "id": "618052378a39fbed35d615ad70c618e6fe37e295",
    "semantic_title": "llm4causal: democratized causal tools for everyone via large language model",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=NikbrdtYvG": {
    "title": "Let's Think Dot by Dot: Hidden computation in transformer language models",
    "volume": "main",
    "abstract": "Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., ‘......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical conjecture for the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens",
    "checked": true,
    "id": "397e5015f761bc4a0d4e81daff7d6462ae7c5a98",
    "semantic_title": "let's think dot by dot: hidden computation in transformer language models",
    "citation_count": 92,
    "authors": []
  },
  "https://openreview.net/forum?id=lVOw78nYXS": {
    "title": "Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs",
    "volume": "main",
    "abstract": "Humans spontaneously use increasingly efficient language as interactions progress, by adapting and forming ad-hoc conventions. This phenomenon has been studied extensively using reference games, showing properties of human language that go beyond relaying intents. It remains unexplored whether multimodal large language models (MLLMs) similarly increase communication efficiency during interactions, and what mechanisms they may adopt for this purpose. We introduce ICCA, an automated framework to evaluate such conversational adaptation as an in-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and observe that while they may understand the increasingly efficient language of their interlocutor, they do not spontaneously make their own language more efficient over time. This latter ability can only be elicited in some models (e.g., GPT-4) with heavy-handed prompting. This shows that this property of linguistic interaction does not arise from current training regimes, even though it is a common hallmark of human language",
    "checked": true,
    "id": "767b57e134d3473af88eea675fe9c1279f680337",
    "semantic_title": "talk less, interact better: evaluating in-context conversational adaptation in multimodal llms",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=UfqzXg95I5": {
    "title": "AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs",
    "volume": "main",
    "abstract": "\\begin{center} \\textcolor{red}{Warning: This paper contains potentially offensive and harmful text.}\\end{center} As large language models (LLMs) become increasingly prevalent and integrated into autonomous systems, ensuring their safety is imperative. Despite significant strides toward safety alignment, recent work GCG~\\citep{zou2023universal} proposes a discrete tokens optimization algorithm and selects the single suffix with the lowest loss to successfully jailbreak aligned LLMs. In this work, we first discuss the drawbacks of solely picking the suffix with the lowest loss during GCG optimization for jailbreaking and uncover the missed successful suffixes during the intermediate steps. Moreover, we utilize those successful suffixes as training data to learn a generative model, named AmpleGCG, which captures the distribution of adversarial suffixes given a harmful query and enables the rapid generation of hundreds of suffixes for any harmful queries in seconds. AmpleGCG achieves near 100\\% attack success rate (ASR) on two aligned LLMs (Llama-2-7B-chat and Vicuna-7B), surpassing two strongest attack baselines. More interestingly, AmpleGCG also transfers seamlessly to attack different models, including closed-source LLMs, achieving a 99\\% ASR on the latest GPT-3.5. To summarize, our work amplifies the impact of GCG by training a generative model of adversarial suffixes that is universal to any harmful queries and transferable from attacking open-source LLMs to closed-source LLMs. Impressively, it can generate 200 adversarial suffixes for one harmful query in only 4 seconds, rendering it more challenging to defend",
    "checked": true,
    "id": "4ad33969188555b8303b375e18f5c117a68387c6",
    "semantic_title": "amplegcg: learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
    "citation_count": 112,
    "authors": []
  },
  "https://openreview.net/forum?id=FbhjirzvJG": {
    "title": "Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding",
    "volume": "main",
    "abstract": "To combat the memory bandwidth-bound nature of autoregressive LLM inference, previous research has proposed the speculative decoding framework. To perform speculative decoding, a small draft model proposes candidate continuations of the input sequence that are then verified in parallel by the base model. One way to specify the draft model, as used in the recent Medusa decoding framework, is as a collection of lightweight heads, called draft heads, that operate on the base model's hidden states. To date, all existing draft heads have been sequentially independent, meaning that they speculate tokens in the candidate continuation independently of any preceding tokens in the candidate continuation. In this work, we propose Hydra heads: a sequentially-dependent drop-in replacement for standard draft heads that significantly improves the accuracy of draft head speculation. We further explore the design space of Hydra head training objectives and architectures, and propose a carefully tuned Hydra head recipe, which we call Hydra++, that improves decoding throughput by up to 1.31x and 2.70x compared to Medusa decoding and autoregressive decoding respectively. Overall, Hydra heads are a simple and well-motivated intervention on standard draft heads that significantly improve the end-to-end speed of draft head-based speculative decoding. We make our code publicly available at https://github.com/zankner/Hydra",
    "checked": true,
    "id": "d0b049018c9e21b7b95c179d33e1e2ac9113c85b",
    "semantic_title": "hydra: sequentially-dependent draft heads for medusa decoding",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=wF6k0aWjAu": {
    "title": "Instruction Mining: Instruction Data Selection for Tuning Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) are initially pretrained for broad capabilities and then finetuned with instruction-following datasets to improve their performance in interacting with humans. Despite advances in finetuning, a standardized guideline for selecting high-quality datasets to optimize this process remains elusive. In this paper, we first propose InstructMining, an innovative method designed for automatically selecting premium instruction-following data for finetuning LLMs. Specifically, InstructMining utilizes natural language indicators as a measure of data quality, applying them to evaluate unseen datasets. During experimentation, we discover that double descent phenomenon exists in large language model finetuning. Based on this observation, we further leverage BlendSearch to help find the best subset among the entire datase. Experiment results show that InstructMining-7B achieves state-of-the-art performance on two of the most popular benchmarks: LLM-as-a-judge and OpenLLM benchmark",
    "checked": true,
    "id": "cc8dec40ee2ccce16fb70218f970515338149a09",
    "semantic_title": "instruction mining: instruction data selection for tuning large language models",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=z7FvXbyyrM": {
    "title": "Long-Form Answers to Visual Questions from Blind and Low Vision People",
    "volume": "main",
    "abstract": "Vision language models can now generate long-form answers to questions about images – long-form visual question answers (LFVQA). We contribute VizWiz-LF, a dataset of long-form answers to visual questions posed by blind and low vision (BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions, collected from human expert describers and six VQA models. We develop and annotate functional roles of sentences of LFVQA and demonstrate that long-form answers contain information beyond the question answer such as explanations and suggestions to retake photos. We further conduct automatic and human evaluations involving BLV and sighted people to evaluate long-form answers. While BLV people perceive both human-written and generated long-form answers as plausible, generated answers often hallucinate incorrect visual details, especially for unanswerable visual questions (e.g., blurry or irrelevant images). To reduce hallucinations, we evaluate VQA models on their ability to abstain from answering unanswerable questions",
    "checked": true,
    "id": "805e4497c2725117fa04e2df1d0d09c8c68f6f56",
    "semantic_title": "long-form answers to visual questions from blind and low vision people",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Aaz6R4Tlwv": {
    "title": "SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design",
    "volume": "main",
    "abstract": "Predicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient's specific tumor via biopsied cells. In this paper, we propose a novel setting and models for *in-context drug synergy learning*. We are given a small \"personalized dataset\" of 10-20 drug synergy relationships in the context of specific cancer cell targets. Our goal is to predict additional drug synergy relationships in that context. Inspired by recent work that pre-trains a GPT language model (LM) to \"in-context learn\" common function classes, we devise novel pre-training schemes that enable a GPT model to in-context learn \"drug synergy functions\". Our model---which does not use any textual corpora, molecular fingerprints, protein interaction or any other domain-specific knowledge--- is able to achieve competitive results. We further integrate our in-context approach with a genetic algorithm to optimize model prompts and select synergy candidates to test after conducting a patient biopsy. Finally, we explore a novel task of inverse drug design which can potentially enable the design of drugs that synergize specifically to target a given patient's \"personalized dataset'\". Our findings could have an important impact on precision cancer medicine, and also raise intriguing questions on non-textual pre-training for LMs",
    "checked": true,
    "id": "316a011bf461d3a96965fb9f69398888da19bd9f",
    "semantic_title": "synergpt: in-context learning for personalized drug synergy prediction and drug design",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=DbsLm2KAqP": {
    "title": "CULTURE-GEN: Revealing Global Cultural Perception in Language Models through Natural Language Prompting",
    "volume": "main",
    "abstract": "As the utilization of large language models (LLMs) has proliferated world-wide, it is crucial for them to have adequate knowledge and fair representation for diverse global cultures. In this work, we uncover culture perceptions of three SOTA models on 110 countries and regions on 8 culture-related topics through culture-conditioned generations, and extract symbols from these generations that are associated to each culture by the LLM. We discover that culture-conditioned generation consist of linguistic \"markers\" that distinguish marginalized cultures apart from default cultures. We also discover that LLMs have an uneven degree of diversity in the culture symbols, and that cultures from different geographic regions have different presence in LLMs' culture-agnostic generation. Our findings promote further research in studying the knowledge and fairness of global culture perception in LLMs",
    "checked": true,
    "id": "42b6852b2e5e8e50687a6aa34fb4655e413caa0c",
    "semantic_title": "culture-gen: revealing global cultural perception in language models through natural language prompting",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=zlw6AHwukB": {
    "title": "A Survey on Deep Learning for Theorem Proving",
    "volume": "main",
    "abstract": "Theorem proving is a fundamental aspect of mathematics, spanning from informal reasoning in natural language to rigorous derivations in formal systems. In recent years, the advancement of deep learning, especially the emergence of large language models, has sparked a notable surge of research exploring these techniques to enhance the process of theorem proving. This paper presents a comprehensive survey of deep learning for theorem proving by offering (i) a thorough review of existing approaches across various tasks such as autoformalization, premise selection, proofstep generation, and proof search; (ii) an extensive summary of curated datasets and strategies for synthetic data generation; (iii) a detailed analysis of evaluation metrics and the performance of state-of-the-art methods; and (iv) a critical discussion on the persistent challenges and the promising avenues for future exploration. Our survey aims to serve as a foundational reference for deep learning approaches in theorem proving, inspiring and catalyzing further research endeavors in this rapidly growing field. A curated list of papers is available at https://github.com/zhaoyu-li/DL4TP",
    "checked": true,
    "id": "6a4501fefaf73261dc180ff86b52208679f3fb9c",
    "semantic_title": "a survey on deep learning for theorem proving",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=Ti67584b98": {
    "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
    "volume": "main",
    "abstract": "We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65\\% accuracy (74\\% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34\\% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are \"Google-proof\"). When we released this dataset in November 2023, GPT-4 achieved 39\\% accuracy. As of March 2024, Claude 3 Opus achieves a reported score of approximately 60\\%, highlighting the rapid pace of progress in AI. If we are to use future AI systems to help us answer very hard questions—for example, when developing new scientific knowledge—we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA for skilled non-experts should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities",
    "checked": true,
    "id": "210b0a3d76e93079cc51b03c4115fde545eea966",
    "semantic_title": "gpqa: a graduate-level google-proof q&a benchmark",
    "citation_count": 982,
    "authors": []
  },
  "https://openreview.net/forum?id=QbCHlIqbDJ": {
    "title": "From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsies",
    "volume": "main",
    "abstract": "In settings where most deaths occur outside the healthcare system, verbal autopsies (VAs) are a common tool to monitor trends in causes of death (COD). VAs are interviews with a surviving caregiver or relative that are used to predict the decedent's COD. Turning VAs into actionable insights for researchers and policymakers requires two steps (i) predicting likely COD using the VA interview and (ii) performing inference with predicted CODs (e.g. modeling the breakdown of causes by demographic factors using a sample of deaths). In this paper, we develop a method for valid inference using outcomes (in our case COD) predicted from free-form text using state-of-the-art NLP techniques. This method, which we call multiPPI++, extends recent work in \"prediction-powered inference\" to multinomial classification. We leverage a suite of NLP techniques for COD prediction and, through empirical analysis of VA data, we demonstrate the effectiveness of our approach in handling transportability issues. multiPPI++ recovers ground truth estimates, regardless of which NLP model produced predictions and regardless of whether they were produced by a more accurate predictor like GPT-4-32k or a less accurate predictor like KNN. Our findings demonstrate the practical importance of inference correction for public health decision-making and suggests that if inference tasks are the end goal, having a small amount of contextually relevant, high quality labeled data is essential regardless of the NLP algorithm",
    "checked": false,
    "id": "56db8447c63c7eba8401d481dd51b4ae874d0700",
    "semantic_title": "from narratives to numbers: valid inference using language model predictions from verbal autopsy narratives",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=j3AAkO5xgr": {
    "title": "Understanding Retrieval Augmentation for Long-Form Question Answering",
    "volume": "main",
    "abstract": "How retrieved documents are used in language models (LMs) for long-form generation task is understudied. We present two controlled studies on retrieval-augmented LM for long-form question answering (LFQA): one fixing the LM and varying evidence documents and the other fixing evidence documents and varying the LMs. We study various attributes of generated answers (e.g., fluency, length, variance), with an emphasis on the attribution of generated answers to in-context evidence documents. We collect a dataset (SALAD) containing human annotations of sentence-level answer attribution in LFQA and evaluate existing methods for automatically judging attribution. We find that while LMs can leverage relevant in-context documents, the generated answer is only partially attributable towards the documents, especially for LMs trained without retrieval augmentation. Together, our analysis reveals how retrieval augmentation impacts long knowledge-rich text generation and provide directions for future work",
    "checked": true,
    "id": "ea61232aa8932f79d6151a025496dc806e4603d6",
    "semantic_title": "understanding retrieval augmentation for long-form question answering",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=N5EYQSwW26": {
    "title": "Building a Large Japanese Web Corpus for Large Language Models",
    "volume": "main",
    "abstract": "Open Japanese large language models (LLMs) have been trained on the Japanese portions of corpora such as CC-100, mC4, and OSCAR. However, these corpora were not created for the quality of Japanese texts. This study builds a large Japanese web corpus by extracting and refining text from the Common Crawl archive (21 snapshots of approximately 63.4 billion pages crawled between 2020 and 2023). This corpus consists of approximately 312.1 billion characters (approximately 173 million pages), which is the largest of all available training corpora for Japanese LLMs, surpassing CC-100 (approximately 25.8 billion characters), mC4 (approximately 239.7 billion characters) and OSCAR 23.01 (approximately 74 billion characters). To confirm the quality of the corpus, we performed continual pre-training on Llama 2 7B, 13B, 70B, Mistral 7B v0.1, and Mixtral 8x7B as base LLMs and gained consistent (6.6-8.1 points) improvements on Japanese benchmark datasets. We also demonstrate that the improvement on Llama 2 13B brought from the presented corpus was the largest among those from other existing corpora",
    "checked": true,
    "id": "ca4f899224c8232616e9a1e59aca33a4aad0675f",
    "semantic_title": "building a large japanese web corpus for large language models",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=BgvgMxY8s5": {
    "title": "Handling Open-Vocabulary Constructs in Formalizing Specifications: Retrieval Augmented Parsing with Expert Knowledge",
    "volume": "main",
    "abstract": "We study the problem of Open-vocabulary constructs (OVCs), ones that are not known beforehand, in the context of converting natural language (NL) specification sentences into formal languages (e.g., LTL or code). Models tend to fare poorly on such OVCs, since they do not have the necessary knowledge a priori. In such settings, a domain expert can provide the correct constructs based on their preference or domain knowledge at inference time. Our goal is to effectively reuse this inference-time, expert-provided knowledge in future specification sentences without having to retrain the model. To this end, we first present a new parsing setting---\\emph{dynamic knowledge-augmented parsing} (DKAP)---where, in addition to the input sentence, the model is given (dynamically growing) expert knowledge in the form of a key-value lexicon that associates NL phrases with correct OVC constructs. To address the DKAP problem, we propose ROLex, a retrieval-augmented parsing approach that uses the dynamic expert lexicon. ROLex consists of a retriever and a generator that are trained to find and use the relevant subset of the key-value store to produce the correct parse. One key challenge in realizing this solution is the lack of training data for the retrieval-augmented parsing. We show how we can make use of synthetic data generation, along with original task-level training data---i.e., the (NL sentence, FL statement) pairs---to carry out the requisite training for the retrieval-augmented parsing setting. Further, to improve training effectiveness, we have devised multiple strategies for focusing the model on the relevant subset of retrieved knowledge. Finally, we introduce a new evaluation paradigm designed to address the DKAP problem by simulating the dynamic expert-provided knowledge in three different formalization settings (NL2LTL, NL2Code, and NL2CMD). Our evaluations show that DKAP is a difficult challenge, and ROLex helps improve the performance of baseline models by using dynamic expert knowledge effectively",
    "checked": false,
    "id": "83e1ca503efc4308b1b772f7d4ba86bd888bf58a",
    "semantic_title": "handling open-vocabulary constructs in formalizing speci-fications: retrieval-augmented parsing with expert knowledge",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BaOAvPUyBO": {
    "title": "Do Language Models Plan Ahead for Future Tokens?",
    "volume": "main",
    "abstract": "Do transformers ``think ahead'' during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at time step $t$ that is then used in future forward passes $t+\\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present during training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a constructed synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis, though pre-caching increases with model scale",
    "checked": true,
    "id": "659292949b854fd485ac45215c7f9619cef46d01",
    "semantic_title": "do language models plan ahead for future tokens?",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=IA8CWtNkUr": {
    "title": "Early Weight Averaging meets High Learning Rates for LLM Pre-training",
    "volume": "main",
    "abstract": "Training Large Language Models (LLMs) incurs significant cost; hence, any strategy that accelerates model convergence is helpful. In this paper, we investigate the ability of a simple idea – checkpoint averaging along the trajectory of a training run – to improve both convergence and generalization quite early during training. Here we show that models trained with high learning rates observe higher gains due to checkpoint averaging. Furthermore, these gains are amplified when checkpoints are sampled with considerable spacing in training steps. Our training recipe outperforms conventional training and popular checkpoint averaging baselines such as exponential moving average (EMA) and stochastic moving average (SWA). We evaluate our training recipe by pre-training LLMs, where high learning rates are inherently preferred due to extremely large batch sizes. Specifically, we pre-trained nanoGPT-2 models of varying sizes—small (125M), medium (335M), and large (770M)—on the OpenWebText dataset, comprised of 9B tokens. Additionally, we present results for publicly available Pythia LLMs, ranging from 1B to 12B, which were trained on the PILE-deduped dataset containing 207B tokens",
    "checked": true,
    "id": "fa168fe11fa77c9728412e49596f8a808b67a4d8",
    "semantic_title": "early weight averaging meets high learning rates for llm pre-training",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=MkppMETE49": {
    "title": "Information Guided Regularization for Fine-tuning Language Models",
    "volume": "main",
    "abstract": "The pretraining-fine-tuning paradigm has been the de facto strategy for transfer learning in modern language modeling. With the understanding that task adaptation in LMs is often a function of parameters shared across tasks, we argue that a more surgical approach to regularization needs to exist for smoother transfer learning. Towards this end, we investigate how the pretraining loss landscape is affected by these task-sensitive parameters through an information-theoretic lens. We then leverage the findings from our investigations to devise a novel approach to dropout for improved model regularization and better downstream generalization. This approach, named guided dropout, is both task & architecture agnostic and adds no computational overhead to the fine-tuning process. Through empirical evaluations, we showcase that our approach to regularization yields consistently better performance, even in scenarios of data paucity, compared to standardized baselines",
    "checked": true,
    "id": "ab3cc3f039457324ec68827f1314f1ac33b75748",
    "semantic_title": "information guided regularization for fine-tuning language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1eg6UnpYu7": {
    "title": "Prompt Public Large Language Models to Synthesize Data for Private On-device Applications",
    "volume": "main",
    "abstract": "Pre-training on public data is an effective method to improve the performance for federated learning (FL) with differential privacy (DP). This paper investigates how large language models (LLMs) trained on public data can improve the quality of pre-training data for the on-device language models trained with DP and FL. We carefully design LLM prompts to filter and transform existing public data, and generate new data to resemble the real user data distribution. The model pre-trained on our synthetic dataset achieves relative improvement of 19.0\\% and 22.8\\% in next word prediction accuracy compared to the baseline model pre-trained on a standard public dataset, when evaluated over the real user data in Gboard (Google Keyboard, a production mobile keyboard application). Furthermore, our method achieves evaluation accuracy better than or comparable to the baseline during the DP FL fine-tuning over the user data from millions of mobile devices, and our final model outperforms the baseline in production A/B testing. Our experiments demonstrate the strengths of LLMs in synthesizing data close to the private distribution even without accessing the private data, and also suggest future research directions to further reduce the distribution gap",
    "checked": true,
    "id": "6a28f9390a2fa99e593b2b3efeccce4abec52631",
    "semantic_title": "prompt public large language models to synthesize data for private on-device applications",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=k8KS9Ps71d": {
    "title": "PRobELM: Plausibility Ranking Evaluation for Language Models",
    "volume": "main",
    "abstract": "This paper introduces PRobELM (Plausibility Ranking Evaluation for Language Models), a benchmark designed to assess language models' ability to discern more plausible from less plausible scenarios through their parametric knowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or truthfulness, and others such as COPA explore plausible scenarios without explicitly incorporating world knowledge, PRobELM seeks to bridge this gap by evaluating models' capabilities to prioritise plausible scenarios that leverage world knowledge over less plausible alternatives. This design allows us to assess the potential of language models for downstream use cases such as literature-based discovery where the focus is on identifying information that is likely but not yet known. Our benchmark is constructed from a dataset curated from Wikidata edit histories, tailored to align the temporal bounds of the training data for the evaluated models. PRobELM facilitates the evaluation of language models across multiple prompting types, including statement, text completion, and question-answering. Experiments with 10 models of various sizes and architectures on the relationship between model scales, training recency, and plausibility performance, reveal that factual accuracy does not directly correlate with plausibility performance and that up-to-date training data enhances plausibility assessment across different model architectures",
    "checked": true,
    "id": "7633d8e74d9749becc0fdc14826ee98048ec1a36",
    "semantic_title": "probelm: plausibility ranking evaluation for language models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=SGoVIC0u0f": {
    "title": "Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping",
    "volume": "main",
    "abstract": "While Transformers have enabled tremendous progress in various application settings, such architectures still struggle with solving planning and sequential decision-making tasks. In this work, we demonstrate how to train Transformers to solve complex planning tasks. This is accomplished by first designing a synthetic language that captures the computation performed by the $A^*$ search algorithm when solving a planning task. Then, an encoder-decoder Transformer model is trained to predict this language, resulting in a language model that can correctly solve novel planning tasks by generating $A^*$'s search dynamics. We fine tune this model to obtain a Searchformer, a Transformer model that optimally solves previously unseen Sokoban puzzles 93.7\\% of the time, while using up to 26.8\\% fewer search steps than our $A^*$ reference implementation. Searchformer significantly outperforms baselines that predict the optimal plan directly with a 5-10$\\times$ smaller model size and a 10$\\times$ smaller training dataset. Lastly, we demonstrate how Searchformer scales to larger and more complex decision making tasks with improved percentage of solved tasks and shortened search dynamics",
    "checked": true,
    "id": "df5bb57e03c38a439d664d3c609a1c03a9a64009",
    "semantic_title": "beyond a*: better planning with transformers via search dynamics bootstrapping",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=eGCw1UVOhk": {
    "title": "LMD3: Language Model Data Density Dependence",
    "volume": "main",
    "abstract": "We develop a methodology for analyzing language model task performance at the individual example level based on training data density estimation. Experiments with paraphrasing as a controlled intervention on finetuning data demonstrate that increasing the support in the training distribution for specific test queries results in a measurable increase in density, which is also a significant predictor of the performance increase caused by the intervention. Experiments with pretraining data demonstrate that we can explain a significant fraction of the variance in model perplexity via density measurements. We conclude that our framework can provide statistical evidence of the dependence of a target model's predictions on subsets of its training data, and can more generally be used to characterize the support (or lack thereof) in the training data for a given test task",
    "checked": true,
    "id": "edc6b26390200dfc9b375529ca774215fe13acfb",
    "semantic_title": "lmd3: language model data density dependence",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=F9tqgOPXH5": {
    "title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents",
    "volume": "main",
    "abstract": "Recent state-of-the-art approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. This begs an interesting question: Instead of directly employing LLMs as embodied agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at? In this work, we propose EnvGen, a novel framework to address this question. First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel. Concretely, the LLM is given the task description and environment simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g., different terrains, items initially given to agents, chances of finding certain objects, etc.). Next, we train a small RL agent in a mixture of the original and LLM-generated environments. Then, we enable the LLM to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the LLM in the form of the agent's performance. We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist game environments. We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster. We also show that using an LLM to adapt environments dynamically outperforms curriculum learning approaches and how the LLM adapts training environments to help improve RL agents' weaker skills over time. Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require one or more LLM calls per step (resulting in thousands of LLM calls per episode). We also present detailed analyses of EnvGen's design choices",
    "checked": true,
    "id": "6cd1b99ec6d399a682b01e6fe9096e2fcf450862",
    "semantic_title": "envgen: generating and adapting environments via llms for training embodied agents",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=FX4fUThO9H": {
    "title": "Model Autophagy Analysis to Explicate Self-consumption within Human-AI Interactions",
    "volume": "main",
    "abstract": "The increasing significance of large models and their multi-modal variants in societal information processing has ignited debates on social safety and ethics. However, there exists a paucity of comprehensive analysis for: (i) the interactions between human and artificial intelligence systems, and (ii) understanding and addressing the associated limitations. To bridge this gap, we present Model Autophagy Analysis for large models' selfconsumption explanation. We employ two distinct autophagous loops (referred to as \"self-consumption loops\") to elucidate the suppression of human-generated information in the exchange between human and AI systems. Through comprehensive experiments on diverse datasets, we evaluate the capacities of generated models as both creators and disseminators of information. Our key findings reveal (i) A progressive prevalence of model-generated synthetic information over time within training datasets compared to human-generated information; (ii) The discernible tendency of large models, when acting as information transmitters across multiple iterations, to selectively modify or prioritize specific contents; and (iii) The potential for a reduction in the diversity of socially or human-generated information, leading to bottlenecks in the performance enhancement of large models and confining them to local optima",
    "checked": false,
    "id": "27da47f159cd064d9be0e36d7d4b9fa6aa628401",
    "semantic_title": "monal: model autophagy analysis for modeling human-ai interactions",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=NV8yRJRET1": {
    "title": "DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning",
    "volume": "main",
    "abstract": "Text-to-image (T2I) generation has seen significant growth over the past few years. Despite this, there has been little work on generating diagrams with T2I models. A diagram is a symbolic/schematic representation that explains information using structurally rich and spatially complex visualizations (e.g., a dense combination of related objects, text labels, directional arrows/lines, etc.). Existing state-of-the-art T2I models often fail at diagram generation because they lack fine-grained object layout control when many objects are densely connected via complex relations such as arrows/lines, and also often fail to render comprehensible text labels. To address this gap, we present DiagrammerGPT, a novel two-stage text-to-diagram generation framework leveraging the layout guidance capabilities of LLMs to generate more accurate diagrams. In the first stage, we use LLMs to generate and iteratively refine ‘diagram plans' (in a planner-auditor feedback loop). In the second stage, we use a diagram generator, DiagramGLIGEN, and a text label rendering module to generate diagrams (with clear text labels) following the diagram plans. To benchmark the text-to-diagram generation task, we introduce AI2D-Caption, a densely annotated diagram dataset built on top of the AI2D dataset. We show that our DiagrammerGPT framework produces more accurate diagrams, outperforming existing T2I models. We also provide comprehensive analysis, including open-domain diagram generation, multi-platform vector graphic diagram generation, human-in-the-loop editing, and multimodal planner/auditor LLMs",
    "checked": true,
    "id": "ba7f09d76f465c7d5fefc87de859a491d6c5e145",
    "semantic_title": "diagrammergpt: generating open-domain, open-platform diagrams via llm planning",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=sKNIjS2brr": {
    "title": "VideoDirectorGPT: Consistent Multi-Scene Video Generation via LLM-Guided Planning",
    "volume": "main",
    "abstract": "Recent text-to-video (T2V) generation methods have seen significant advancements. However, the majority of these works focus on producing short video clips of a single event (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules. This prompts an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a ‘video plan', which includes the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency groupings of the entities. Next, guided by this video plan, our video generator, named Layout2Vid, has explicit control over spatial layouts and can maintain temporal consistency of entities across multiple scenes, while being trained only with image-level annotations. Our experiments demonstrate that our proposed VideoDirectorGPT framework substantially improves layout and movement control in both single- and multi-scene video generation and can generate multi-scene videos with consistency, while achieving competitive performance with SOTAs in open-domain single-scene T2V generation. Detailed ablation studies, including dynamic adjustment of layout control strength with an LLM and video generation with user-provided images, confirm the effectiveness of each component of our framework and its future potential",
    "checked": true,
    "id": "16753e0317730e8c1b297338300a8c6163dd06f2",
    "semantic_title": "videodirectorgpt: consistent multi-scene video generation via llm-guided planning",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=fib9qidCpY": {
    "title": "Towards Verifiable Text Generation with Symbolic References",
    "volume": "main",
    "abstract": "LLMs are vulnerable to hallucinations, and thus their outputs generally require laborious human verification for high-stakes applications. To this end, we propose symbolically grounded generation (SymGen) as a simple approach for enabling easier manual validation of an LLM's output. SymGen prompts an LLM to interleave its regular output text with explicit symbolic references to fields present in some conditioning data (e.g., a table in JSON format). The references can be used to display the provenance of different spans of text in the generation, reducing the effort required for manual verification. Across a range of data-to-text and question-answering exper- iments, we find that LLMs are able to directly output text that makes use of accurate symbolic references while maintaining fluency and factuality. In a human study we further find that such annotations can streamline human verification of machine-generated text",
    "checked": true,
    "id": "722aa3bb6e426afc40f05c42a2fc0623adb51af9",
    "semantic_title": "towards verifiable text generation with symbolic references",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=c30qeMg8dv": {
    "title": "Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations",
    "volume": "main",
    "abstract": "The widespread adoption and transformative effects of large language models (LLMs) have sparked concerns regarding their capacity to produce inaccurate and fictitious content, referred to as `hallucinations'. Given the potential risks associated with hallucinations, humans should be able to identify them. This research aims to understand the human perception of LLM hallucinations by systematically varying the degree of hallucination (genuine, minor hallucination, major hallucination) and examining its interaction with warning (i.e., a warning of potential inaccuracies: absent vs. present). Participants ($N=419$) from Prolific rated the perceived accuracy and engaged with content (e.g., like, dislike, share) in a Q/A format. Results indicate that humans rank content as truthful in the order genuine > minor hallucination > major hallucination and user engagement behaviors mirror this pattern. More importantly, we observed that warning improves hallucination detection without significantly affecting the perceived truthfulness of genuine content. We conclude by offering insights for future tools to aid human detection of hallucinations",
    "checked": true,
    "id": "d67d3e31ce219a82eb6315295f60ab663edebc96",
    "semantic_title": "fakes of varying shades: how warning affects human perception and engagement regarding llm hallucinations",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=VHhwhmtx3b": {
    "title": "Does RoBERTa Perform Better than BERT in Continual Learning: An Attention Sink Perspective",
    "volume": "main",
    "abstract": "Continual learning (CL) aims to train models that can sequentially learn new tasks without forgetting previous tasks' knowledge. Although previous works observed that pre-training can benefit CL, it remains unclear whether a pre-trained model with higher downstream capacity also performs better in CL. In this paper, we observe that pre-trained models may allocate high attention scores to some 'sink' tokens, such as [SEP] tokens, which are ubiquitous across various tasks. Such attention sinks may lead to models' over-smoothing in single-task learning and interference in sequential tasks' learning, which may compromise the models' CL performance despite their high pre-trained capabilities. To reduce these effects, we propose a pre-scaling mechanism that encourages attention diversity across all tokens. Specifically, it first scales the task's attention to the non-sink tokens in a probing stage, and then fine-tunes the model with scaling. Experiments show that pre-scaling yields substantial improvements in CL without experience replay, or progressively storing parameters from previous tasks",
    "checked": true,
    "id": "9a1f22898d95514d1d4223b04cb3e0e2feef7e15",
    "semantic_title": "does roberta perform better than bert in continual learning: an attention sink perspective",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=TRxQMpLUfD": {
    "title": "Stronger Random Baselines for In-Context Learning",
    "volume": "main",
    "abstract": "Evaluating the in-context learning classification performance of language models poses challenges due to small dataset sizes, extensive prompt-selection using the validation set, and intentionally difficult tasks that lead to near-random performance. The standard random baseline--the expected accuracy of guessing labels uniformly at random--is stable when the evaluation set is used only once or when the dataset is large. We account for the common practice of validation set reuse and existing small datasets with a stronger random baseline: the expected maximum accuracy across multiple random classifiers. When choosing the best prompt demonstrations across six quantized language models applied to 16 BIG-bench Lite tasks, more than 20% of the few-shot results that exceed the standard baseline do not exceed this stronger random baseline. When held-out test sets are available, this stronger baseline is also a better predictor of held-out performance than the standard baseline, avoiding unnecessary test set evaluations. This maximum random baseline provides an easily calculated drop-in replacement for the standard baseline",
    "checked": true,
    "id": "94abf13f6a0598dcd9239ce5d5715c19f42b15da",
    "semantic_title": "stronger random baselines for in-context learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=nqLAuMOF6n": {
    "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
    "volume": "main",
    "abstract": "We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff",
    "checked": true,
    "id": "07894aeadab9158fdb97647c4792816ede1b60b9",
    "semantic_title": "branch-train-mix: mixing expert llms into a mixture-of-experts llm",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=B41hNBoWLo": {
    "title": "TOFU: A Task of Fictitious Unlearning for LLMs",
    "volume": "main",
    "abstract": "Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of $200$ diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all",
    "checked": true,
    "id": "6ef1ab6b6775861b7f6a78343d5b90a15226c654",
    "semantic_title": "tofu: a task of fictitious unlearning for llms",
    "citation_count": 219,
    "authors": []
  },
  "https://openreview.net/forum?id=Hi8jKh4HE9": {
    "title": "What is in Your Safe Data? Identifying Benign Data that Breaks Safety",
    "volume": "main",
    "abstract": "Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Additionally, we propose a bi-directional anchoring method that, during the selection process, prioritizes data points that are close to harmful examples and far from benign ones. Our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints surprisingly leads to the fine-tuned model affirmatively responding to >70% of tested harmful requests, compared to <20% after fine-tuning on randomly selected data. We also observe that the selected data frequently appear as lists, bullet points, or math questions, indicating a systematic pattern in fine-tuning data that contributes to jailbreaking",
    "checked": true,
    "id": "0391aee67b008aee6aa12aeea67a7f90c954c113",
    "semantic_title": "what is in your safe data? identifying benign data that breaks safety",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=soz1SEiPeq": {
    "title": "Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence",
    "volume": "main",
    "abstract": "We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks",
    "checked": true,
    "id": "157ed5647da39a7f5d33a84a90414b2a9e97e301",
    "semantic_title": "eagle and finch: rwkv with matrix-valued states and dynamic recurrence",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=gUNeyiLNxr": {
    "title": "Uncovering Intermediate Variables in Transformers using Circuit Probing",
    "volume": "main",
    "abstract": "Neural network models have achieved high performance on a wide variety of complex tasks, but the algorithms that they implement are notoriously difficult to interpret. It is often necessary to hypothesize intermediate variables involved in a network's computation in order to understand these algorithms. For example, does a language model depend on particular syntactic properties when generating a sentence? Yet, existing analysis tools make it difficult to test hypotheses of this type. We propose a new analysis technique – circuit probing – that automatically uncovers low-level circuits that compute hypothesized intermediate variables. This enables causal analysis through targeted ablation at the level of model parameters. We apply this method to models trained on simple arithmetic tasks, demonstrating its effectiveness at (1) deciphering the algorithms that models have learned, (2) revealing modular structure within a model, and (3) tracking the development of circuits over training. Across these three experiments we demonstrate that circuit probing combines and extends the capabilities of existing methods, providing one unified approach for a variety of analyses. Finally, we demonstrate circuit probing on a real-world use case: uncovering circuits that are responsible for subject-verb agreement and reflexive anaphora in GPT2-Small and Medium",
    "checked": true,
    "id": "40c34e85dc4558d26bfef7fb54327dbd4a0bebc3",
    "semantic_title": "uncovering intermediate variables in transformers using circuit probing",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=3ypWPhMGhV": {
    "title": "Cohesive Conversations: Enhancing Authenticity in Multi-Agent Simulated Dialogues",
    "volume": "main",
    "abstract": "This paper investigates the quality of multi-agent dialogues in simulations powered by Large Language Models (LLMs). Analyzing dialogues and memory over multiple sessions revealed significant issues such as repetition, inconsistency, and hallucination, exacerbated by the propagation of erroneous information. To combat these challenges, we propose a novel Screening, Diagnosis, and Regeneration (SDR) framework that detects and corrects utterance errors through a comprehensive process involving immediate issue identification, evidence gathering from past dialogues, and LLM analysis for utterance revision. By incorporating our SDR framework to Generative Agents (Park et al., 2023), we enhance the diversity, consistency, and factualness of the generated dialogues. This work presents a pioneering approach to enhancing dialogue quality in multi-agent simulations, establishing a new standard for future research in the field",
    "checked": true,
    "id": "bf19d876b64922d41152a04955b309fb5ed8e929",
    "semantic_title": "cohesive conversations: enhancing authenticity in multi-agent simulated dialogues",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=eDWcNqiQWW": {
    "title": "The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models",
    "volume": "main",
    "abstract": "While large language models (LLMs) are still being adopted to new domains and utilized in novel applications, we are experiencing an influx of the new generation of foundation models, namely multi-modal large language models (MLLMs). These models integrate verbal and visual information, opening new possibilities to demonstrate more complex reasoning abilities at the intersection of the two modalities. However, despite the revolutionizing prospect of MLLMs, our understanding of their reasoning abilities is limited. In this study, we assess the nonverbal abstract reasoning abilities of open-source and closed-source MLLMs using variations of Raven's Progressive Matrices. Our experiments reveal the challenging nature of such problems for MLLMs while showcasing the immense gap between open-source and closed-source models. We also uncover critical shortcomings of visual and textual perceptions, subjecting the models to low-performance ceilings. Finally, to improve MLLMs' performance, we experiment with different methods, such as Chain-of-Thought prompting, leading to a significant (up to 100\\%) boost in performance",
    "checked": true,
    "id": "cdce4525bc94b8b72d7330f4e26775142edd0018",
    "semantic_title": "the curious case of nonverbal abstract reasoning with multi-modal large language models",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=MXLBXjQkmb": {
    "title": "Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training. LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model's utilities on other tasks. Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data. However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse --- a drastic degradation of the model's utilities. In this paper, we propose \\emph{Negative Preference Optimization} (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset. We theoretically show that the progression toward catastrophic collapse by minimizing the NPO loss is exponentially slower than GA. Through experiments on synthetic data and the benchmark TOFU dataset, we demonstrate that NPO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model's utilities. We also observe that NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the first to achieve reasonable unlearning results in forgetting 50\\% (or more) of the training data, whereas existing methods already struggle with forgetting 10\\% of training data",
    "checked": true,
    "id": "6dc1bb04eb0df303b1820ff1de15ab78f554cfff",
    "semantic_title": "negative preference optimization: from catastrophic collapse to effective unlearning",
    "citation_count": 218,
    "authors": []
  },
  "https://openreview.net/forum?id=wLQ3I0F1oj": {
    "title": "Large Language Model is not a (Multilingual) Compositional Relation Reasoner",
    "volume": "main",
    "abstract": "We present a comprehensive evaluation of large language models' capability to reason compositional relations through a benchmark encompassing 1,800 test cases in both English and Chinese, covering six distinct categories of composition relations: Positional, Comparative, Personal, Mathematical, Identity, and Other. We expand our assessment to the multilingual realm by including translations of the benchmark suite into Japanese, French, and Korean. Our Multilingual Composition Relation (MCR) benchmark aims at investigating the robustness and adaptability of LLMs in handling compositional relation reasoning across diverse linguistic contexts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xMt9kCv5YR": {
    "title": "Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game",
    "volume": "main",
    "abstract": "Large language models (LLMs) have exhibited memorable strategic behaviors in social deductive games. However, the significance of opinion leadership exhibited by LLM-based agents has been largely overlooked, which is crucial for practical applications in multi-agent and human-AI interaction settings. Opinion leaders are individuals who have a noticeable impact on the beliefs and behaviors of others within a social group. In this work, we employ the Werewolf game as a simulation platform to assess the opinion leadership of LLMs. The game includes the role of the Sheriff, tasked with summarizing arguments and recommending decision options, and therefore serves as a credible proxy for an opinion leader. We develop a framework integrating the Sheriff role and devise two novel metrics based on the critical characteristics of opinion leaders. The first metric measures the reliability of the opinion leader, and the second assesses the influence of the opinion leader on other players' decisions. We conduct extensive experiments to evaluate LLMs of different scales. In addition, we collect a Werewolf question-answering dataset (WWQA) to assess and enhance LLM's grasp of the game rules, and we also incorporate human participants for further analysis. The results suggest that the Werewolf game is a suitable test bed to evaluate the opinion leadership of LLMs, and few LLMs possess the capacity for opinion leadership",
    "checked": true,
    "id": "e7fac49adb164902cf286c777d0ca836fe2728e7",
    "semantic_title": "helmsman of the masses? evaluate the opinion leadership of large language models in the werewolf game",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=xI8C7sfN1H": {
    "title": "Factual and Tailored Recommendation Endorsements using Language Models and Reinforcement Learning",
    "volume": "main",
    "abstract": "Recommender systems (RSs) play a central role in matching candidate items to users based on their preferences. While traditional RSs rely on user feed-back signals, conversational RSs interact with users in natural language. In this work, we develop P4LM, an _aPpealing, Precise, Preference-comprehensive and Prioritized_ language model which endorses recommended items by emphasizing specific item characteristics and their coverage to a user's preferences. P4LM uses an _embedding_ representation of a user's preferences to generate responses that are appealing, factually-grounded and tailored to the user's preferences. P4LM employs a joint reward function to measure precision, appeal, preference coverage and prioritization of preferences, which are used as AI-based feedback in a reinforcement learning-based language model framework. On the MovieLens 25M and Amazon Product Review datasets, P4LM delivers more appealing and tailored endorsements to users, as determined by auto-critic and rater evaluations",
    "checked": true,
    "id": "5c59146978d415ca86d33242d893ecb647d545ff",
    "semantic_title": "factual and tailored recommendation endorsements using language models and reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1XnUsqwr7": {
    "title": "Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning",
    "volume": "main",
    "abstract": "Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning. However, previous methods often struggle to address reasoning errors in intermediate steps, which can lead to accumulative errors. In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs. Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation. Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities. Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic. Moreover, our analysis proves DBS's capability of detecting diverse and subtle reasoning errors and robustness on different model scales",
    "checked": true,
    "id": "98aa313fbe30734eb3bb50683204765bcbc607eb",
    "semantic_title": "deductive beam search: decoding deducible rationale for chain-of-thought reasoning",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=PPTrmvEnpW": {
    "title": "Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models",
    "volume": "main",
    "abstract": "Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model's internal representations using linear probes and contrastive activations. The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state. We validate these internal representations by using them to make interventions on the model's activations and edit its internal board state. Unlike Li et al's prior synthetic dataset approach, our analysis finds that the model also learns to estimate latent variables like player skill to better predict the next character. We derive a player skill vector and add it to the model, improving the model's win rate by up to 2.6 times",
    "checked": true,
    "id": "197a45da6e58d86998b81771a714caa611eae05a",
    "semantic_title": "emergent world models and latent variable estimation in chess-playing language models",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=Qmq4zqdnWh": {
    "title": "Using Natural Language Explanations to Rescale Human Judgments",
    "volume": "main",
    "abstract": "The rise of large language models (LLMs) has brought a critical need for high-quality human-labeled data, particularly for processes like human feedback and evaluation. A common practice is to label data via consensus annotation over human judgments. However, annotators' judgments for subjective tasks can differ in many ways: they may reflect different qualitative judgments about an example, and they may be mapped to a labeling scheme in different ways. We show that these nuances can be captured by natural language explanations, and propose a method to rescale ordinal annotations and explanations using LLMs. Specifically, we feed annotators' Likert ratings and corresponding explanations into an LLM and prompt it to produce a numeric score anchored in a scoring rubric. These scores should reflect the annotators' underlying assessments of the example. The rubric can be designed or modified after annotation, and include distinctions that may not have been known when the original error taxonomy was devised. We explore our technique in the context of rating system outputs for a document-grounded question answering task, where LLMs achieve near-human performance. Our method rescales the raw judgments without impacting agreement and brings the scores closer to human judgments grounded in the same scoring rubric",
    "checked": true,
    "id": "4bf3fd4859cb0d37e333ee9ed4024387e265c99e",
    "semantic_title": "using natural language explanations to rescale human judgments",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=oqYiYG8PtY": {
    "title": "Stop Reasoning! When Multimodal LLM with Chain-of-Thought Reasoning Meets Adversarial Image",
    "volume": "main",
    "abstract": "Multimodal LLMs (MLLMs) with a great ability of text and image under- standing have received great attention. To achieve better reasoning with MLLMs, Chain-of-Thought (CoT) reasoning has been widely explored, which further promotes MLLMs' explainability by giving intermediate reasoning steps. Despite the strong power demonstrated by MLLMs in multimodal reasoning, recent studies show that MLLMs still suffer from ad- versarial images. This raises the following open questions: Does CoT also enhance the adversarial robustness of MLLMs? What do the intermediate reasoning steps of CoT entail under adversarial attacks? To answer these questions, we first generalize existing attacks to CoT-based inferences by attacking the two main components, i.e., rationale and answer. We find that CoT indeed improves MLLMs' adversarial robustness against the existing attack methods by leveraging the multi-step reasoning process, but not substantially. Based on our findings, we further propose a novel attack method, termed as stop-reasoning attack, that attacks the model while by- passing the CoT reasoning process. Experiments on three MLLMs and two visual reasoning datasets verify the effectiveness of our proposed method. We show that stop-reasoning attack can result in misled predictions and outperform baseline attacks by a significant margin",
    "checked": true,
    "id": "f4f87904af97d7f567dd2f44930a73952ed4d5dd",
    "semantic_title": "stop reasoning! when multimodal llm with chain-of-thought reasoning meets adversarial image",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=RCdoMrg4I0": {
    "title": "Chinese Tiny LLM: Pretraining a Chinese-Centered Large Language Model",
    "volume": "main",
    "abstract": "In this study, we introduce $\\textbf{CT-LLM}$, a groundbreaking 2B large language model (LLM) that illustrates a pivotal shift towards prioritizing the Chinese language in the development of LLMs. Uniquely initiated from scratch, CT-LLM diverges from the conventional methodology by primarily incorporating Chinese textual data, utilizing an extensive corpus of 1,200 billion tokens, including 800 billion Chinese tokens and 400 billion English tokens. This strategic composition facilitates the model's exceptional proficiency in understanding and processing Chinese, a capability further enhanced through alignment techniques including supervised fine-tuning (SFT) and direct preference optimization (DPO). Demonstrating remarkable performance on the ChineseHardCase Benchmark, CT-LLM not only excels in Chinese language tasks but also showcases its adeptness in English through SFT. This research challenges the prevailing paradigm of training LLMs predominantly on English corpora and then adapting them to other languages, broadening the horizons for LLM training methodologies. By open-sourcing full-process of CT-LLM, we aim to foster further exploration and innovation within both the academic and industrial spheres, paving the way for more inclusive and versatile language models in the future",
    "checked": false,
    "id": "3e9141595ab2820be0b461c9ca61ed86aa9a5b15",
    "semantic_title": "chinese tiny llm: pretraining a chinese-centric large language model",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=dJfBejh478": {
    "title": "Scalable Model Editing via Customized Expert Networks",
    "volume": "main",
    "abstract": "Addressing the issues of hallucinations and outdated knowledge in large language models is critical for their reliable application. Model Editing presents a promising avenue for mitigating these challenges in a costeffective manner. However, existing methods often suffer from unsatisfactory generalization and unintended effects on non-edited samples. To overcome these limitations, we introduce a novel approach: Scalable Model Editing via Customized Expert Networks (SCEN), which is a two-stage continuous training paradigm. Specifically, in the first stage, we train lightweight expert networks individually for each piece of knowledge that needs to be updated. Subsequently, we train a corresponding indexing neuron for each expert to control the activation state of that expert. We conducted a series of experiments on the ZsRE and Hallucination benchmarks by tuning the advanced open-source LLM, Llama2, achieving state-of-theart results compared to current mainstream methods. Our code is available at https://github.com/TAL-auroraX/SCEN",
    "checked": true,
    "id": "c849ab233a65f328c2f24f4b1361dcd86a062351",
    "semantic_title": "scalable model editing via customized expert networks",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=tRxIB7y3wF": {
    "title": "LalaEval: A Holistic Human Evaluation Framework for Domain-Specific Large Language Models",
    "volume": "main",
    "abstract": "This paper introduces LalaEval, a holistic framework designed for the human evaluation of domain-specific large language models (LLMs). LalaEval proposes a comprehensive suite of end-to-end protocols that cover five main components including domain specification, criteria establishment, benchmark dataset creation, construction of evaluation rubrics, and thorough analysis and interpretation of evaluation outcomes. This initiative aims to fill a crucial research gap by providing a systematic methodology for conducting standardized human evaluations within specific domains, a practice that, despite its widespread application, lacks substantial coverage in the literature and human evaluation are often criticized to be less reliable due to subjective factors, so standardized procedures adapted to the nuanced requirements of specific domains or even individual organizations are in great need. Furthermore, the paper demonstrates the framework's application within the logistics industry and a comparative analysis of LLMs for the logistics domain use, highlighting the framework's capacity to elucidate performance differences and guide model selection and development for domain-specific LLMs. Through real-world deployment, the paper underscores the framework's effectiveness in advancing the field of domain-specific LLM evaluation, thereby contributing significantly to the ongoing discussion on LLMs' practical utility and performance in domain-specific applications",
    "checked": true,
    "id": "3fe644014c134fa9896cccb6cd62cc3629df8a9f",
    "semantic_title": "lalaeval: a holistic human evaluation framework for domain-specific large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=5Evv4tIjUI": {
    "title": "Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners",
    "volume": "main",
    "abstract": "In-context learning, which offers substantial advantages over fine-tuning, is predominantly observed in decoder-only models, while encoder-decoder (i.e., seq2seq) models excel in methods that rely on weight updates. Recently, a few studies have demonstrated the feasibility of few-shot learning with seq2seq models; however, this has been limited to tasks that align well with the seq2seq architecture, such as summarization and translation. Inspired by these initial studies, we provide a first-ever extensive experiment comparing the in-context few-shot learning capabilities of decoder-only and encoder-decoder models on a broad range of tasks. Furthermore, we propose two methods to more effectively elicit in-context learning ability in seq2seq models: objective-aligned prompting and a fusion-based approach. Remarkably, our approach outperforms a decoder-only model that is six times larger and exhibits significant performance improvements compared to conventional seq2seq models across a variety of settings. We posit that, with the right configuration and prompt design, seq2seq models can be highly effective few-shot learners for a wide spectrum of applications",
    "checked": true,
    "id": "07bc02bd16f6fe78a7ea3bb8d966fcc6e3893195",
    "semantic_title": "exploiting the potential of seq2seq models as robust few-shot learners",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EHPns3hVkj": {
    "title": "Tower: An Open Multilingual Large Language Model for Translation-Related Tasks",
    "volume": "main",
    "abstract": "While general-purpose large language models (LLMs) demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open LLMs are competitive only when specializing on a single task. In this paper, we propose a recipe for tailoring LLMs to multiple tasks present in translation workflows. We perform continued pretraining on a multilingual mixture of monolingual and parallel data, creating TowerBase, followed by finetuning on instructions relevant for translation processes, creating TowerInstruct. Our model surpasses open alternatives on several relevant tasks and is competitive with general-purpose closed LLMs. We will release the Tower models, our specialization dataset, an evaluation framework for LLMs focusing on the translation ecosystem, and a collection of model generations on our benchmark",
    "checked": true,
    "id": "cc49514a9859ae171a7cd7eaae111990605a5e4e",
    "semantic_title": "tower: an open multilingual large language model for translation-related tasks",
    "citation_count": 169,
    "authors": []
  },
  "https://openreview.net/forum?id=Zq9Dfj4nBo": {
    "title": "Redesigning Information Markets in the Era of Language Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2481c82d3ee502eb2149212b8f80023a25a78a64",
    "semantic_title": "redesigning information markets in the era of language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=lY6XTF9tPv": {
    "title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1823b8aecd62ccfca0cb6caa8e2a1159754afc5e",
    "semantic_title": "llasmol: advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=7BCmIWVT0V": {
    "title": "Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "bfeda6c7aa7899a80adb01894555b09d24756a59",
    "semantic_title": "corex: pushing the boundaries of complex reasoning through multi-model collaboration",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=HLoWN6m4fS": {
    "title": "Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a58112381efffe6f1b43f8b663a5d30a2ff27eef",
    "semantic_title": "elephants never forget: memorization and learning of tabular data in large language models",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=wi9IffRhVM": {
    "title": "Guiding Language Model Reasoning with Planning Tokens",
    "volume": "main",
    "abstract": "Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought (CoT) reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. To encourage a more structural generation of CoT steps, we propose a hierarchical generation scheme: we let the LM generate a planning token at the start of each reasoning step, intuitively serving as a high-level plan of the current step, and add their embeddings to the model parameters. Our approach requires a negligible increase in trainable parameters (0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets and one multihop QA dataset with respect to standard fine-tuning baselines",
    "checked": true,
    "id": "b29134737a0c81c13d31fc0263b3c4d4f05ccb78",
    "semantic_title": "guiding language model reasoning with planning tokens",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=3nTbuygoop": {
    "title": "StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows",
    "volume": "main",
    "abstract": "It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and external environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes as state machines. In StateFlow, we distinguish between \"process grounding\" (via state and state transitions) and \"sub-task solving\" (through actions within a state), enhancing control and interpretability of the task-solving procedure. A state represents the status of a running process. The transitions between states are controlled by heuristic rules or decisions made by the LLM, allowing for a dynamic and adaptive progression. Upon entering a state, a series of actions is executed, involving not only calling LLMs guided by different prompts, but also the utilization of external tools as needed. Our results show that StateFlow significantly enhances LLMs' efficiency. For instance, StateFlow achieves 13\\% and 28\\% higher success rates compared to ReAct in InterCode SQL and ALFWorld benchmark, with 5$\\times$ and 3$\\times$ less cost respectively. We also show that StateFlow can be combined with iterative refining methods like Reflexion to further improve performance",
    "checked": true,
    "id": "b8cc74d8c2a009cc99a8b285503d86be80d840d9",
    "semantic_title": "stateflow: enhancing llm task-solving through state-driven workflows",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=9gdZI7c6yr": {
    "title": "Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human evaluation, revealing that existing calibration methods aimed at mitigating biases of LLMs are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PairS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons locally and efficiently ranks candidate texts globally. PairS achieves state-of-the-art performance on representative evaluation tasks in long-form generations and demonstrates significant improvements over direct scoring. Furthermore, we provide insights into the role of pairwise preference in quantifying the transitivity of LLMs and demonstrate how PairS benefits from calibration using debiased pairwise evaluations",
    "checked": true,
    "id": "aae01e933690e1f060b8bc5e3ecbef785630d0f9",
    "semantic_title": "aligning with human judgement: the role of pairwise preference in large language model evaluators",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=eJ3cHNu7ss": {
    "title": "HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs",
    "volume": "main",
    "abstract": "Adapting a language model (LM) into a specific domain, *a.k.a* domain adaption, is a common practice when specialized knowledge, e.g. medicine, is not encapsulated in a general language model like Llama2. This typically involves a two-stage process including *continued pre-training* and *supervised fine-tuning*. Implementing a pipeline solution with these two stages not only introduces complexities (necessitating dual meticulous tuning) but also leads to two occurrences of data distribution shifts, exacerbating catastrophic forgetting. To mitigate these, we propose a one-stage domain adaption protocol where heterogeneous data from both the traditional pre-training and supervised stages are unified into a simple instruction-output pair format to achieve efficient knowledge injection. Subsequently, a data priority sampling strategy is introduced to adaptively adjust data mixture during training. Following this protocol, we train HuatuoGPT-II, a specialized LLM for the medical domain in Chinese. HuatuoGPT-II achieve competitive performance with GPT4 across multiple benchmarks, which especially shows the state-of-the-art (SOTA) performance in multiple Chinese medical benchmarks and the newest pharmacist licensure examinations. Furthermore, we explore the phenomenon of one-stage protocols, and the experiments reflect that the simplicity of the proposed protocol improves training stability and domain generalization",
    "checked": true,
    "id": "2a86d281bef364e2ea2d4fc61fde46ca25b955f1",
    "semantic_title": "huatuogpt-ii, one-stage training for medical adaption of llms",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=Dt6qXZsgaU": {
    "title": "Self-Guide: Better Task-Specific Instruction Following via Self-Synthetic Finetuning",
    "volume": "main",
    "abstract": "Large language models (LLMs) hold the promise of solving diverse tasks when provided with appropriate natural language prompts. However, prompting often leads models to make predictions with lower accuracy compared to finetuning a model with ample training data. On the other hand, while finetuning LLMs on task-specific data generally improves their performance, abundant annotated datasets are not available for all tasks. Previous work has explored generating task-specific data from state-of-the-art LLMs and using this data to finetune smaller models, but this approach requires access to a language model other than the one being trained, which introduces cost, scalability challenges, and legal hurdles associated with continuously relying on more powerful LLMs. In response to these, we propose Self-Guide, a multi-stage mechanism in which we synthesize task-specific input-output pairs from the student LLM, then use these input-output pairs to finetune the student LLM itself. In our empirical evaluation of the Natural Instructions V2 benchmark, we find that Self-Guide improves the performance of LLM by a substantial margin. Specifically, we report an absolute improvement of approximately 15% for classification tasks and 18% for generation tasks in the benchmark's metrics. This sheds light on the promise of self-synthesized data guiding LLMs towards becoming task-specific experts without any external learning signals",
    "checked": true,
    "id": "31d9e19005b99dba626d66e1da7b9967e579210c",
    "semantic_title": "self-guide: better task-specific instruction following via self-synthetic finetuning",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=DMUGTMWrKZ": {
    "title": "Enhancing Adversarial Robustness of LLMs with Analytic Hierarchy Process",
    "volume": "main",
    "abstract": "With the increasing impact of large language models (LLMs) across diverse applications, ensuring the robustness of LLMs has become a pressing concern. Existing defense strategies are tailored to specific attack scenarios, which typically require high-cost model training and cannot rapidly respond to new threats. To tackle this issue, we conceptualize the defense strategy in LLMs as a cognitive process for dealing with complex user queries. Intuitively, faced with a spectrum of queries that potentially contain malicious perturbations, LLMs need human-like discernment to avoid being misled. Drawing inspiration from cognitive theory, we introduce an innovative Analytic Hierarchy Process (AHP) inference framework. Our methodology involves decomposing intricate tasks into manageable subtasks, prioritizing them, and systematically addressing each step. Our framework is based on AI feedback, eliminating the necessity for training and optimization. We evaluate the effectiveness of our framework in jailbreak attacks and robustness in downstream tasks using representative LLMs, including GPT-3.5 and Llama2. The experimental results demonstrate that our proposed framework significantly enhances the adversarial robustness of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KqK5XcgEhR": {
    "title": "Empowering Large Language Model Agents through Action Learning",
    "volume": "main",
    "abstract": "Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents. While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations across Robotic Planning and Alfworld environments reveal that after learning on a few training task instances, our approach to open-action learning markedly improves agent performance for the type of task (by 32 percent in AlfWorld compared to ReAct+Reflexion, for instance) highlighting the importance of experiential action learning in the development of more intelligent LLM agents",
    "checked": true,
    "id": "9eaa62a2a45634c6e922ed1a3f2e44b4424398fb",
    "semantic_title": "empowering large language model agents through action learning",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=Nd950RAcCW": {
    "title": "Multi-hop Question Answering under Temporal Knowledge Editing",
    "volume": "main",
    "abstract": "Multi-hop question answering (MQA) under knowledge editing (KE) has garnered significant attention in the era of large language models. However, existing models for MQA under KE exhibit poor performance when dealing with questions containing explicit temporal contexts. To address this limitation, we propose a novel framework, namely TEMPoral knowLEdge augmented Multi-hop Question Answering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a time-aware graph (TAG) to store edit knowledge in a structured manner. Then, through our proposed inference path, structural retrieval, and joint reasoning stages, TEMPLE-MQA effectively discerns temporal contexts within the question query. Experiments on benchmark datasets demonstrate that TEMPLE-MQA significantly outperforms baseline models. Additionally, we contribute a new dataset, namely TKEMQA, which serves as the inaugural benchmark tailored specifically for MQA with temporal scopes",
    "checked": true,
    "id": "c7d6597da713c85306040b6037c9e6365a9119de",
    "semantic_title": "multi-hop question answering under temporal knowledge editing",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=7jSMMvXLri": {
    "title": "Measuring Taiwanese Mandarin Language Understanding",
    "volume": "main",
    "abstract": "The evaluation of large language models (LLMs) has drawn substantial attention in the field recently. This work focuses on evaluating LLMs in a Chinese context, specifically, for Traditional Chinese which has been largely underrepresented in existing benchmarks. We present TMLU, a comprehensive evaluation suit tailored for assessing the advanced knowledge and reasoning capability in LLMs, under the context of Taiwanese Mandarin. TMLU consists of an array of 37 subjects across social science, STEM, humanities, Taiwan-specific content, and others, ranging from middle school to professional levels. In addition, we curate chain-of-thought-like few-shot explanations for each subject to facilitate the evaluation of complex reasoning skills. To establish a comprehensive baseline, we conduct extensive experiments and analysis on 24 advanced LLMs. The results suggest that Chinese open-weight models demonstrate inferior performance comparing to multilingual proprietary ones, and open-weight models tailored for Taiwanese Mandarin lag behind the Simplified-Chinese counterparts. The findings indicate great headrooms for improvement, and emphasize the goal of TMLU to foster the development of localized Taiwanese-Mandarin LLMs. We release the benchmark and evaluation scripts for the community to promote future research",
    "checked": true,
    "id": "2a0bd79de0973ce130da32c2c5a7a2e0952834d7",
    "semantic_title": "measuring taiwanese mandarin language understanding",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=dj9x6JuiD5": {
    "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation",
    "volume": "main",
    "abstract": "Long text generation, such as novel writing and discourse-level translation with extremely long contexts, presents significant challenges to current language models. Existing methods mainly focus on extending the model's context window through strategies like length extrapolation. However, these approaches demand substantial hardware resources during the training and/or inference phases. Our proposed method, Temp-Lora, introduces an alternative concept. Instead of relying on the KV cache to store all context information, we embeds this information directly into a temporary Lora module. In the process of long text generation, this module is progressively trained with text generated previously. This approach not only efficiently preserves contextual knowledge but also prevents any permanent alteration to the model's parameters given that the module is discarded post-generation. Extensive experiments on the PG19 language modeling benchmark and the GuoFeng discourse-level translation benchmark validate the effectiveness of Temp-Lora. Our results show that: 1) Temp-Lora substantially enhances generation quality for long text, as indicated by a 13.2\\% decrease in perplexity (PPL) on a subset of PG19, and a 29.3\\% decrease in PPL along with a 113.2\\% increase in BLEU score on a subset of GuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text generation methods, and 3) Temp-Lora can greatly reduce computational costs by shortening the context window. For example, we can ensure a moderate improvement in generation quality (a decrease of 3.8\\% in PPL) while enabling a 51.5\\% memory usage reduction and a 60.0\\% decrease in latency for inference",
    "checked": true,
    "id": "5215a3cfd67fdc6eb0201822dd0004bd4b830f91",
    "semantic_title": "with greater text comes greater necessity: inference-time training helps long text generation",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=xm8zYRfrqE": {
    "title": "Studying Large Language Model Behaviors Under Context-Memory Conflicts With Real Documents",
    "volume": "main",
    "abstract": "Retrieval-augmented generation (RAG) mitigates many problems of fully parametric language models, such as temporal degradation, hallucinations, and lack of grounding. In RAG, the model's knowledge can be updated from documents provided in context. This leads to cases of conflict between the model's parametric knowledge and the contextual information, where the model may not always update its knowledge. Previous work studied context-memory knowledge conflicts by creating synthetic documents that contradict the model's correct parametric answers. We present a framework for studying such knowledge conflicts in a realistic setup. We update incorrect parametric knowledge using real conflicting documents. This reflects how knowledge conflicts arise in practice. In this realistic scenario, we find that knowledge updates fail less often than previously reported. In cases where the models still fail to update their answers, we find a parametric bias: the incorrect parametric answer appearing in context makes the knowledge update likelier to fail. These results suggest that the factual parametric knowledge of LLMs can negatively influence their reading abilities and behaviors. Our code is available at https://github.com/kortukov/realistic_knowledge_conflicts/",
    "checked": true,
    "id": "faaaf13276a8c25e093dd5ad198425cbad2cf250",
    "semantic_title": "studying large language model behaviors under context-memory conflicts with real documents",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=dkpeWQRmlc": {
    "title": "HDT: Hierarchical Document Transformer",
    "volume": "main",
    "abstract": "In this paper, we propose the Hierarchical Document Transformer (HDT), a novel sparse Transformer architecture tailored for structured hierarchical documents. Such documents are extremely important in numerous domains, including science, law or medicine. However, most existing solutions are inefficient and fail to make use of the structure inherent to documents. HDT exploits document structure by introducing auxiliary anchor tokens and redesigning the attention mechanism into a sparse multi-level hierarchy. This approach facilitates information exchange between tokens at different levels while maintaining sparsity, thereby enhancing computational and memory efficiency while exploiting the document structure as an inductive bias. We address the technical challenge of implementing HDT's sample-dependent hierarchical attention pattern by developing a novel sparse attention kernel that considers the hierarchical structure of documents. As demonstrated by our experiments, utilizing structural information present in documents leads to faster convergence, higher sample efficiency and better performance on downstream tasks",
    "checked": true,
    "id": "0793b0ff0bebf68a2b2fa5296fae6c850e25c2dc",
    "semantic_title": "hdt: hierarchical document transformer",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=kzzwTrt04Z": {
    "title": "AI-generated text boundary detection with RoFT",
    "volume": "main",
    "abstract": "Due to the rapid development of large language models, people increasingly often encounter texts that may start as written by a human but continue as machine-generated. Detecting the boundary between human-written and machine-generated parts of such texts is a challenging problem that has not received much attention in literature. We attempt to bridge this gap and examine several ways to adapt state of the art artificial text detection classifiers to the boundary detection setting. We push all detectors to their limits, using the Real or Fake text benchmark that contains short texts on several topics and includes generations of various language models. We use this diversity to deeply examine the robustness of all detectors in cross-domain and cross-model settings to provide baselines and insights for future research. In particular, we find that perplexity-based approaches to boundary detection tend to be more robust to peculiarities of domain-specific data than supervised fine-tuning of the RoBERTa model; we also find which features of the text confuse boundary detection algorithms and negatively influence their performance in cross-domain settings",
    "checked": true,
    "id": "6c5d2469c71fdfec4367fad5b8bd113ba7ba2910",
    "semantic_title": "ai-generated text boundary detection with roft",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=YDZ7GeFLxq": {
    "title": "Scattered Mixture-of-Experts Implementation",
    "volume": "main",
    "abstract": "ScatterMoE is an implementation of Sparse Mixture-of-Experts (SMoE) on GPUs. ScatterMoE builds upon techniques in existing implementations, and overcoming some of the current limitations to improve batched inference, training speed, and memory footprint. This implementation achieves this by avoiding padding and making excessive copies of the input. We also fuse expert linear transforms and reordering operations with ParallelLinear, a module that can be used to extend the concept of SMoEs. We benchmark our implementation against Megablocks, and show that it enables a higher throughput and lower memory footprint. We also show how ParallelLinear enables extension of the Mixture-of-Experts concept by demonstrating with an implementation of Mixture-of-Attention",
    "checked": true,
    "id": "d8599a04f738a42f5d7a2c9bc598c2c7b74669c5",
    "semantic_title": "scattered mixture-of-experts implementation",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=TZ0CCGDcuT": {
    "title": "Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms",
    "volume": "main",
    "abstract": "Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM's circuit for a task by performing causal interventions on each edge independently, but this scales poorly with model size. As a solution, recent work has proposed edge attribution patching (EAP), a scalable gradient-based approximation to interventions. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to efficiently find circuits while better maintaining one of their core properties: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model's behavior on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP-IG are more faithful than those found using EAP, even though both have high node overlap with reference circuits found using causal interventions. We conclude more generally that when comparing circuits, measuring overlap is no substitute for measuring faithfulness",
    "checked": true,
    "id": "0d8f8092ea3ee98a4ab5b797a7b5c0287c73ce36",
    "semantic_title": "have faith in faithfulness: going beyond circuit overlap when finding model mechanisms",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=taThoOlDNQ": {
    "title": "Exploring the Mystery of Influential Data for Mathematical Reasoning",
    "volume": "main",
    "abstract": "Selecting influential data for fine-tuning on downstream tasks is a key factor for both performance and computation efficiency. Recent works have shown that training with only limited data can show a superior performance on general tasks. However, the feasibility on mathematical reasoning tasks has not been validated. To go further, there exist two open questions for mathematical reasoning: how to select influential data and what is an influential data composition. For the former one, we propose a Quality-aware Diverse Selection (QaDS) strategy adaptable for mathematical reasoning. A comparison with other selection strategies validates the superiority of QaDS. For the latter one, we first enlarge our setting and explore the influential data composition. We conduct a series of experiments and highlight: scaling up reasoning data, and training with general data selected by QaDS is helpful. Then, we define our optimal mixture as OpenMathMix, an influential data mixture with open-source data selected by QaDS. With OpenMathMix, we achieve a state-of-the-art 48.8% accuracy on MATH with 7B base model. Additionally, we showcase the use of QaDS in creating efficient fine-tuning mixtures with various selection ratios, and analyze the quality of a wide range of open-source datasets, which can perform as a reference for future works on mathematical reasoning tasks",
    "checked": true,
    "id": "c00c3fd40e4ab89faa14c0d2d34e0ea5de8e3608",
    "semantic_title": "exploring the mystery of influential data for mathematical reasoning",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=yfyHxvVzZT": {
    "title": "Does Incomplete Syntax Influence Korean Language Model? Focusing on Word Order and Case Markers",
    "volume": "main",
    "abstract": "Syntactic elements, such as word order and case markers, are fundamental in natural language processing. Recent studies show that syntactic information boosts language model performance and offers clues for people to understand their learning mechanisms. Unlike languages with a fixed word order such as English, Korean allows for varied word sequences, despite its canonical structure, due to case markers that indicate the functions of sentence components. This study explores whether Korean language models can accurately capture this flexibility. We note that incomplete word orders and omitted case markers frequently appear in ordinary Korean communication. To investigate this further, we introduce the Syntactically Incomplete Korean (SIKO) dataset. Through SIKO, we assessed Korean language models' flexibility with incomplete syntax and confirmed the dataset's training value. Results indicate these models reflect Korean's inherent flexibility, accurately handling incomplete inputs. Moreover, fine-tuning with SIKO enhances the ability to handle common incomplete Korean syntactic forms. The dataset's simple construction process, coupled with significant performance enhancements, solidifies its standing as an effective data augmentation technique. The SIKO will become accessible post-publication",
    "checked": true,
    "id": "c62bfe4f68a26e68d1fe6fc3da8f201c700c2ab9",
    "semantic_title": "does incomplete syntax influence korean language model? focusing on word order and case markers",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=wS7PxDjy6m": {
    "title": "Dated Data: Tracing Knowledge Cutoffs in Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are often paired with a reported cutoff date, the time at which training data was gathered. Such information is crucial for applications where the LLM must provide up-to-date information. However, a reported cutoff only scratches the surface. Do all sub-resources in the training data share the same cutoff? Does the model's demonstrated knowledge for these sub-resources closely align to their cutoff? We define the notion of an effective cutoff, which is distinct from the LLM's reported cutoff and differs between sub-resources. We propose a simple approach to estimate effective cutoffs of an LLM on the resource-level by probing across versions of the data. Crucially, our method does not require access to a model's pre-training data. Through our analysis, we find that effective cutoffs often drastically differ from reported cutoffs. To understand the root cause of this observation, we conduct a large-scale analysis on open pre-training datasets. Our analysis reveals two reasons for these inconsistencies: (1) temporal misalignments of CommonCrawl data due to non-trivial amounts of old data in new dumps; and (2) complications in LLM deduplication schemes involving semantic duplicates and lexical near-duplicates. Overall, our results show that cutoffs are not as simple as they have seemed and that care must be taken both by LLM dataset curators as well as practitioners who seek to use these models",
    "checked": true,
    "id": "5ee0c8975b965a413b27332b5cbfb2745251dc52",
    "semantic_title": "dated data: tracing knowledge cutoffs in large language models",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=0UK8c2kg7c": {
    "title": "InstructAV: Instruction Fine-tuning Large Language Models for Authorship Verification",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in a wide range of NLP tasks. However, when it comes to authorship verification (AV) tasks, which involve determining whether two given texts share the same authorship, even advanced models like ChatGPT exhibit notable limitations. This paper introduces a novel approach, termed InstructAV, for authorship verification. This approach utilizes LLMs in conjunction with a parameter-efficient fine-tuning (PEFT) method to simultaneously improve accuracy and explainability. The distinctiveness of InstructAV lies in its ability to align classification decisions with transparent and understandable explanations, representing a significant progression in the field of authorship verification. Through comprehensive experiments conducted across various datasets, InstructAV demonstrates its state-of-the-art performance on the AV task, offering high classification accuracy coupled with enhanced explanation reliability",
    "checked": true,
    "id": "330bb7ce9fb150f04b1bd671ecdec2acadc430e9",
    "semantic_title": "instructav: instruction fine-tuning large language models for authorship verification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Wmdk94oKF": {
    "title": "CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs",
    "volume": "main",
    "abstract": "Businesses and software platforms are increasingly utilizing Large Language Models (LLMs) like GPT-3.5, GPT-4, GLM-3, and LLaMa-2 as chat assistants with file access or as reasoning agents for custom service. Current LLM-based customer service models exhibit limited integration with customer profiles and lack operational capabilities, while existing API integrations prioritize diversity over precision and error avoidance that are crucial in real-world scenarios for Customer Service. We propose an LLMs agent called **CHOPS** (**CH**at with cust**O**mer **P**rofile in existing **S**ystem) that: (1) efficiently utilizes existing databases or systems to access user information or interact with these systems based on existing guidance; (2) provides accurate and reasonable responses or executing required operations in the system while avoiding harmful operations; and (3) leverages the combination of small and large LLMs together to provide satisfying performance while having decent inference cost. We introduce a practical dataset, *CPHOS-dataset*, including a database, some guiding files, and QA pairs collected from *CPHOS*, which employs an online platform to facilitate the organization of simulated Physics Olympiads for high school teachers and students. We conduct extensive experiments to validate the performance of our proposed **CHOPS** architecture using the *CPHOS-dataset*, aiming to demonstrate how LLMs can enhance or serve as alternatives to human customer service",
    "checked": true,
    "id": "e262b485df299a0388bcb1a4f2780504ac7eb8f9",
    "semantic_title": "chops: chat with customer profile systems for customer service with llms",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=95TayIeqJ4": {
    "title": "TMMLU+: An Improved Traditional Chinese Evaluation Suite for Foundation Models",
    "volume": "main",
    "abstract": "We present TMMLU+, a new benchmark designed for Traditional Chinese language understanding. TMMLU+ is a multi-choice question-answering dataset with 66 subjects from elementary to professional level. It is six times larger and boasts a more balanced subject distribution than its predecessor, Taiwan Massive Multitask Language Understanding (TMMLU). We also benchmark closed-source models and 26 open-weight Chinese large language models (LLMs) of parameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings reveal that (1.) Traditional Chinese models still trail behind their Simplified Chinese counterparts, highlighting a need for more focused advancements in LLMs catering to Traditional Chinese. (2.) Current LLMs still fall short of human performance in average scores, indicating a potential need for future research to delve deeper into social science and humanities subjects. (3.) Among all the tokenization compression metrics examined, we identify that only the fertility score uniquely demonstrates strong correlations with our benchmark results. We foresee that TMMLU+ will pinpoint areas for future model improvement, thereby narrowing the gap between machine and human linguistic capabilities and supporting researchers in developing Traditional Chinese LLMs. Our dataset, along with the benchmark source code, is accessible at huggingface.co/datasets/ikala/tmmluplus",
    "checked": false,
    "id": "14394ae5ee9153b78266cbc53a04dd6ecc4d1a4e",
    "semantic_title": "an improved traditional chinese evaluation suite for foundation model",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=1ba209BACA": {
    "title": "Agent-DocEdit: Language-Instructed LLM Agent for Content-Rich Document Editing",
    "volume": "main",
    "abstract": "Editing content-rich and multimodal documents, such as posters, flyers, and slides, can be tedious if the edits are complex, repetitive, or require subtle skills and deep knowledge of the editing software. Motivated by recent advancements in both Large Language Model (LLM) agents and multimodal modeling, we propose a framework that automates document editing which takes as input a linguistic edit request from the user and then performs sequential editing actions to the document the satisfy the request. Our proposed method, Agent-DocEdit, first grounds the edit request directly in the underlying document structure to identify the elements that need to be manipulated. Then, we rely on the agent capabilities of LLMs to generate an edit program which calls a set of pre-defined APIs to modify the underlying structure of the document. To improve the generated edit program, we leverage a feedback mechanism incorporating a deterministic code executor and a multimodal LLM. We demonstrate the effectiveness of our proposed modularized LLM editing agent on the DocEdit dataset, where Agent-DocEdit outperforms existing state-of-the-art methods by 70+% in document element grounding and 16+% on final rendition generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HVK6nl3i97": {
    "title": "TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding",
    "volume": "main",
    "abstract": "With large language models (LLMs) widely deployed in long content generation recently, there has emerged an increasing demand for efficient long-sequence inference support. However, key-value (KV) cache, which is stored to avoid re-computation, has emerged as a critical bottleneck by growing linearly in size with the sequence length. Due to the auto-regressive nature of LLMs, the entire KV cache will be loaded for every generated token, resulting in low utilization of computational cores and high latency. While various compression methods for KV cache have been proposed to alleviate this issue, they suffer from degradation in generation quality. We introduce TriForce, a hierarchical speculative decoding system that is scalable for long sequence generation. This approach leverages the original model weights and dynamic sparse KV cache via retrieval as a draft model, which serves as an intermediate layer in the hierarchy and is further speculated by a smaller model to reduce its drafting latency. TriForce not only facilitates impressive speedups for Llama2-7B-128K, achieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in handling even longer contexts. For the offloading setting on two RTX 4090 GPUs, TriForce achieves 0.108s/token—only half as slow as the auto-regressive baseline on an A100, which attains 7.78$\\times$ on our optimized offloading system. Additionally, TriForce performs 4.86$\\times$ than DeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is highlighted by its consistently outstanding performance across various temperatures. The code is available at https://github.com/Infini-AI-Lab/TriForce",
    "checked": true,
    "id": "d0deaec3e1f74701ac43600d9e64c5c969be7391",
    "semantic_title": "triforce: lossless acceleration of long sequence generation with hierarchical speculative decoding",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=oSG6qGkt1I": {
    "title": "Reasoning about concepts with LLMs: Inconsistencies abound",
    "volume": "main",
    "abstract": "The ability to summarize and organize knowledge into abstract concepts is key to learning and reasoning. Many industrial applications rely on the consistent and systematic use of concepts, especially when dealing with decision-critical knowledge. However, we demonstrate that, when methodically questioned, large language models (LLMs) often display and demonstrate significant inconsistencies in their knowledge. Computationally, the basic aspects of the conceptualization of a given domain can be represented as Is-A hierarchies in a knowledge graph (KG) or ontology, together with a few properties or axioms that enable straightforward reasoning. We show that even simple ontologies can be used to reveal conceptual inconsistencies across several LLMs. We also propose strategies that domain experts can use to evaluate and improve the coverage of key domain concepts in LLMs of various sizes. In particular, we have been able to significantly enhance the performance of LLMs of various sizes with openly available weights using simple knowledge-graph (KG) based prompting strategies",
    "checked": true,
    "id": "8ac8eff3ecdc06619c1d412197ddaf8d90fc2cb3",
    "semantic_title": "reasoning about concepts with llms: inconsistencies abound",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=CrzAj0kZjR": {
    "title": "STaR-GATE: Teaching Language Models to Ask Clarifying Questions",
    "volume": "main",
    "abstract": "When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity (GATE; Li et al., 2023), models often struggle to ask good questions. We explore a language model's ability to self-improve (STaR; Zelikman et al., 2022) by rewarding the model for generating useful questions—a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model—the $\\texttt{Questioner}$—and a $\\texttt{Roleplayer}$ whose preferences are unknown to the $\\texttt{Questioner}$. By asking questions, the $\\texttt{Questioner}$ elicits preferences from the $\\texttt{Roleplayer}$. The $\\texttt{Questioner}$ is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an $\\texttt{Oracle}$ with access to the $\\texttt{Roleplayer}$'s latent preferences. After two iterations of self-improvement, the $\\texttt{Questioner}$ asks better questions, allowing it to generate responses that are preferred over responses from the initial model on $\\textbf{72}$% of tasks. Our results indicate that teaching a language model to ask better questions leads to better personalized responses",
    "checked": true,
    "id": "61be5b2d4b73cde6b11a8b988aea95c22364c86e",
    "semantic_title": "star-gate: teaching language models to ask clarifying questions",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=UfWwBaLuXV": {
    "title": "List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs",
    "volume": "main",
    "abstract": "Set-of-Mark (SoM) Prompting unleashes the visual grounding capability of GPT-4V, by enabling the model to associate visual objects with tags inserted on the image. These tags, marked with alphanumerics, can be indexed via text tokens for easy reference. Despite the extraordinary performance from GPT-4V, we observe that other Multimodal Large Language Models (MLLMs) struggle to understand these visual tags. To promote the learning of SoM prompting for open-source models, we propose a new learning paradigm: list items one by one, which asks the model to enumerate and describe all visual tags placed on the image following the alphanumeric order of tags. By integrating our synthetic dataset with other visual instruction tuning datasets, we are able to equip existing MLLMs with the SoM prompting ability. Furthermore, we evaluate our finetuned SoM models on seven MLLM benchmarks. We find that this new dataset, even in a relatively small size (10k-30k images with tags), significantly enhances visual reasoning capabilities and reduces hallucinations for MLLMs. Perhaps surprisingly, these improvements persist even when the visual tags are omitted from input images during inference. This suggests the potential of ``list items one by one'' as a new paradigm for training MLLMs, which strengthens the object-text alignment through the use of visual tags in the training stage. Finally, we conduct analyses by probing trained models to understand the working mechanism of SoM. Our code and data are available at https://github.com/zzxslp/SoM-LLaVA",
    "checked": true,
    "id": "7d0d587ca37ee09247886684a220bb1625e77910",
    "semantic_title": "list items one by one: a new data source and learning paradigm for multimodal llms",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=MLD1cwfjUb": {
    "title": "Your Context Is Not an Array: Unveiling Random Access Limitations in Transformers",
    "volume": "main",
    "abstract": "Despite their recent successes, Transformer-based large language models show surprising failure modes. A well-known example of such failure modes is their inability to length-generalize: solving problem instances at inference time that are longer than those seen during training. In this work, we further explore the root cause of this failure by performing a detailed analysis of model behaviors on the simple parity task. Our analysis suggests that length generalization failures are intricately related to a model's inability to perform random memory accesses within its context window. We present supporting evidence for this hypothesis by demonstrating the effectiveness of methodologies that circumvent the need for indexing or that enable random token access indirectly, through content-based addressing. We further show where and how the failure to perform random memory access manifests through attention map visualizations",
    "checked": true,
    "id": "debd2a77ede19998e9745d215553c48b4027924a",
    "semantic_title": "your context is not an array: unveiling random access limitations in transformers",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=v3w2a7EInO": {
    "title": "CATS: Context-Aware Thresholding for Sparsity in Large Language Models",
    "volume": "main",
    "abstract": "The dramatic improvements in Large Language Models (LLMs) come at the cost of increased computational resources for inference. Recent studies ameliorate the computational costs of LLMs by increasing their activation sparsity but suffer from significant performance degradation on downstream tasks. In this work, we introduce a new framework for sparsifying the activations of LLMs and reducing inference costs, dubbed $\\underline{C}$ontextually $\\underline{A}$ware $\\underline{T}$hresholding for $\\underline{S}$parsity (CATS). CATS is a relatively simple algorithm that is easy to implement and highly effective. At the heart of our framework is a new non-linear activation function. We demonstrate that CATS can be applied to various models, including Mistral-7B and Llama2-7B \\& 13B, and outperforms existing sparsification techniques across multiple tasks. More precisely, CATS-based models achieve downstream task performance within $\\sim$ 99\\% of their base models at activation sparsity levels of 50\\%, even without any fine-tuning. Moreover, with fine-tuning that targets only 1\\% of the parameters, CATS-based models not only converge faster but also achieve better task performance than competing techniques. Finally, we develop a custom GPU kernel for the efficient implementation of CATS that translates the activation sparsity of CATS to real wall-clock time speedups. Our custom kernel implementation of CATS results in a $\\sim$15\\% improvement in wall-clock inference latency of token generation. We release our code, experiments, and datasets at https://github.com/ScalingIntelligence/CATS",
    "checked": false,
    "id": "3d776d7a659795cbc719cf80019cae0dd6bfe69e",
    "semantic_title": "cats: contextually-aware thresholding for sparsity in large language models",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=YwrNePfb3E": {
    "title": "Prompt Exploration with Prompt Regression",
    "volume": "main",
    "abstract": "In the advent of democratized usage of large language models (LLMs), there is a growing desire to systematize LLM prompt creation and selection processes beyond iterative trial-and-error. Prior works majorly focus on searching the space of prompts without accounting for relations between prompt variations. Here we propose a framework, Prompt Exploration with Prompt Regression (PEPR), to predict the effect of prompt combinations given results for individual prompt elements as well as a simple method to select an effective prompt for a given use-case. We evaluate our approach with open-source LLMs of different sizes on several different tasks",
    "checked": true,
    "id": "b218b84d4137befe61d9a096c3e9a6c3637c68e3",
    "semantic_title": "prompt exploration with prompt regression",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JXcXnJJSuL": {
    "title": "Information-Theoretic Distillation for Reference-less Summarization",
    "volume": "main",
    "abstract": "The current winning recipe for automatic summarization is using proprietary large-scale language models (LLMs) such as ChatGPT as is, or imitation learning from them as teacher models. While increasingly ubiquitous dependence on such large-scale language models is convenient, there remains an important question of whether small-scale models could have achieved competitive results, if we were to seek an alternative learning method---that allows for a more cost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a novel framework to distill a powerful summarizer based on the information-theoretic objective for summarization, without relying on either the LLM's capability or human-written references. To achieve this, we first propose a novel formulation of the desiderata of summarization (saliency, faithfulness and brevity) through the lens of mutual information between the original document and the summary. Based on this formulation, we start off from Pythia-2.8B as the teacher model, which is not yet capable of summarization, then self-train the model to optimize for the information-centric measures of ideal summaries. Distilling from the improved teacher, we arrive at a compact but powerful summarizer with only 568M parameters that performs competitively against ChatGPT, without ever relying on ChatGPT's capabilities. Extensive analysis demonstrates that our approach outperforms in-domain supervised models in human evaluation, let alone state-of-the-art unsupervised methods, and wins over ChatGPT in controllable summarization",
    "checked": true,
    "id": "5df18deac55a1ce90a58b0401034d47594faa078",
    "semantic_title": "information-theoretic distillation for reference-less summarization",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=nMAaCsCTCI": {
    "title": "Impact of Preference Noise on the Alignment Performance of Generative Language Models",
    "volume": "main",
    "abstract": "A key requirement in developing Generative Language Models (GLMs) is to have their values aligned with human's values. Preference-based alignment is a widely used paradigm for this purpose, in which preferences over generation pairs are first elicited from human annotators or AI systems, and then fed into some alignment techniques, e.g., Direct Preference Optimization. However, a substantial percent (up to 42%) of the preference pairs used in GLM alignment are noisy, and it remains unclear how the noise affect the alignment performance and how to mitigate their negative impact. In this paper, we propose a framework to inject desirable amounts and types of noise to the preferences, and systematically study the impact of preference noise on the alignment performance in two tasks (summarization and dialogue generation). We find that the alignment performance can be highly sensitive to the noise rates in the preference data: e.g., a 10 percentage points (pp) increase of the noise rate can lead to 30 pp drop in the alignment performance (in win rate). To mitigate the impact of noise, confidence-based data filtering shows significant benefit when certain types of noise are present. We hope our work can help the community better understand and mitigate the impact of preference noise in GLM alignment",
    "checked": true,
    "id": "865bf47dedfec416221504708cd1f039050b0a2d",
    "semantic_title": "impact of preference noise on the alignment performance of generative language models",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=8TdcXwfNRB": {
    "title": "PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models",
    "volume": "main",
    "abstract": "We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training), a method which associates astronomical observations imaged by telescopes with natural language using a neural network model. The model is fine-tuned from a pre-trained Contrastive Language-Image Pre-training (CLIP) model using successful observing proposal abstracts and corresponding downstream observations, with the abstracts optionally summarized via guided generation using large language models (LLMs). Using observations from the Hubble Space Telescope (HST) as an example, we show that the fine-tuned model embodies a meaningful joint representation between observations and natural language through tests targeting image retrieval (i.e., finding the most relevant observations using natural language queries) and description retrieval (i.e., querying for astrophysical object classes and use cases most relevant to a given observation). Our study demonstrates the potential for using generalist foundation models rather than task-specific models for interacting with astronomical data by leveraging text as an interface",
    "checked": true,
    "id": "d3234875f7a5e70cc9210004a57b625fa1299d58",
    "semantic_title": "paperclip: associating astronomical observations and natural language with multi-modal models",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=KidynPuLNW": {
    "title": "On Limitations of the Transformer Architecture",
    "volume": "main",
    "abstract": "What are the root causes of hallucinations in large language models (LLMs)? We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true",
    "checked": true,
    "id": "bbe0e4cc9b052e960362fdc18b6805043b81ca6b",
    "semantic_title": "on limitations of the transformer architecture",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=DRffhKBVlE": {
    "title": "LITE: Modeling Environmental Ecosystems with Multimodal Large Language Models",
    "volume": "main",
    "abstract": "The modeling of environmental ecosystems plays a pivotal role in the sustainable management of our planet. Accurate prediction of key environmental variables over space and time can aid in informed policy and decision-making, thus improving people's livelihood. Recently, deep learning-based methods have shown promise in modeling the spatial-temporal relationships for predicting environmental variables. However, these approaches often fall short in handling incomplete features and distribution shifts, which are commonly observed in environmental data due to the substantial cost of data collection and malfunctions in measuring instruments. To address these issues, we propose LITE -- a multimodal large language model for environmental ecosystems modeling. Specifically, LITE unifies different environmental variables by transforming them into natural language descriptions and line graph images. Then, LITE utilizes unified encoders to capture spatial-temporal dynamics and correlations in different modalities. During this step, the incomplete features are imputed by a sparse Mixture-of-Experts framework, and the distribution shift is handled by incorporating multi-granularity information from past observations. Finally, guided by domain instructions, a language model is employed to fuse the multimodal representations for the prediction. Our experiments demonstrate that LITE significantly enhances performance in environmental spatial-temporal prediction across different domains compared to the best baseline, with a 41.25\\% reduction in prediction error. This justifies its effectiveness",
    "checked": true,
    "id": "64feb7d1bbaee85fece0933b7276e0ac5af8b051",
    "semantic_title": "lite: modeling environmental ecosystems with multimodal large language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=CI7D2kiih1": {
    "title": "Should We Attend More or Less? Modulating Attention for Fairness",
    "volume": "main",
    "abstract": "The advances in natural language processing (NLP) pose both opportunities and challenges. While recent progress enables the development of high-performing models for a variety of tasks, it also poses the risk of models learning harmful biases from the data, such as gender stereotypes. In this work, we investigate the role of attention, a widely-used technique in current state-of-the-art NLP models, in the propagation of social biases. Specifically, we study the relationship between the entropy of the attention distribution and the model's performance and fairness. We then propose a novel method for modulating attention weights to improve model fairness after training. Since our method is only applied post-training and pre-inference, it is an intra-processing method and is, therefore, less computationally expensive than existing in-processing and pre-processing approaches. Our results show an increase in fairness and minimal performance loss on different text classification and generation tasks using language models of varying sizes",
    "checked": true,
    "id": "6927a5b0152433a199ab4974ad85e787454d6a30",
    "semantic_title": "should we attend more or less? modulating attention for fairness",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=QJvfpWSpWm": {
    "title": "The Larger the Better? Improved LLM Code-Generation via Budget Reallocation",
    "volume": "main",
    "abstract": "It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model. We consider a standard unit-test setup, which can be used to select the correct output from the smaller model. Our findings reveal that the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the importance of studying approaches for ranking LLM outputs",
    "checked": true,
    "id": "bd598f1b113891cbaccd5b7b96008a9bf2759712",
    "semantic_title": "the larger the better? improved llm code-generation via budget reallocation",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=Lmjgl2n11u": {
    "title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey",
    "volume": "main",
    "abstract": "Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rather than on sophisticated reasoning abilities. Additionally, we identify the need for further research that delineates the key differences between human and LLM-based reasoning. Through this survey, we aim to shed light on the complex reasoning processes within LLMs",
    "checked": true,
    "id": "d0255ce4c897bea6578955de64fc40324f991878",
    "semantic_title": "beyond accuracy: evaluating the reasoning behavior of large language models - a survey",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=bo4pauxnIR": {
    "title": "Tabular Transfer Learning via Prompting LLMs",
    "volume": "main",
    "abstract": "Learning with a limited number of labeled data is a central problem in real-world applications of machine learning, as it is often expensive to obtain annotations. To deal with the scarcity of labeled data, transfer learning is a conventional approach; it suggests to learn a transferable knowledge by training a neural network from multiple other sources. In this paper, we investigate transfer learning of tabular tasks, which has been less studied and successful in the literature, compared to other domains, e.g., vision and language. This is because tables are inherently heterogeneous, i.e., they contain different columns and feature spaces, making transfer learning difficult. On the other hand, recent advances in natural language processing suggest that the label scarcity issue can be mitigated by utilizing in-context learning capability of large language models (LLMs). Inspired by this and the fact that LLMs can also process tables within a unified language space, we ask whether LLMs can be effective for tabular transfer learning, in particular, under the scenarios where the source and target datasets are of different format. As a positive answer, we propose a novel tabular transfer learning framework, coined Prompt to Transfer (P2T), that utilizes unlabeled (or heterogeneous) source data with LLMs. Specifically, P2T identifies a column feature in a source dataset that is strongly correlated with a target task feature to create examples relevant to the target task, thus creating pseudo-demonstrations for prompts. Experimental results demonstrate that P2T outperforms previous methods on various tabular learning benchmarks, showing good promise for the important, yet underexplored tabular transfer learning problem. Code is available at https://github.com/jaehyun513/P2T",
    "checked": true,
    "id": "45236255e2960596628d1e6f21f91e382f2c863f",
    "semantic_title": "tabular transfer learning via prompting llms",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=qHdSA85GyZ": {
    "title": "Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think",
    "volume": "main",
    "abstract": "Multiple choice questions (MCQs) are commonly used to evaluate the capabilities of large language models (LLMs). One common way to evaluate the model response is to rank the candidate answers based on the log probability of the first token prediction. An alternative way is to examine the text output. Prior work has shown that first token probabilities lack robustness to changes in MCQ phrasing, and that first token probabilities do not match text answers for instruction-tuned models. Therefore, in this paper, we investigate the robustness of text answers. We show that the text answers are more robust to question perturbations than the first token probabilities, when the first token answers mismatch the text answers. The difference in robustness increases as the mismatch rate becomes greater. As the mismatch reaches over 50%, the text answer is more robust to option order changes than the debiased first token probabilities using state-of-the-art debiasing methods such as PriDe. Our findings provide further evidence for the benefits of text answer evaluation over first token probability evaluation",
    "checked": true,
    "id": "6c3b832b98c2236b7e652f2d41573db20ca053d1",
    "semantic_title": "look at the text: instruction-tuned language models are more robust multiple choice selectors than you think",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=UyNIH6CWHH": {
    "title": "Efficient Parallelization Layouts for Large-Scale Distributed Model Training",
    "volume": "main",
    "abstract": "Efficiently training large language models requires parallelizing across hundreds of hardware accelerators and invoking various compute and memory optimizations. When combined, many of these strategies have complex interactions regarding the final training efficiency. Prior work tackling this problem did not have access to the latest set of optimizations, such as FlashAttention or sequence parallelism. In this work, we conduct a comprehensive ablation study of possible training configurations for large language models. We distill this large study into several key recommendations for the most efficient training. For instance, we find that using a micro-batch size of 1 usually enables the most efficient training layouts. Larger micro-batch sizes necessitate activation checkpointing or higher degrees of model parallelism and also lead to larger pipeline bubbles. Our most efficient configurations enable us to achieve state-of-the-art training efficiency results over a range of model sizes, most notably a Model FLOPs utilization of 70.5% when training a LLaMA 13B model",
    "checked": true,
    "id": "165de9784c6ebee03a5bf81e754fb99c4532bf1c",
    "semantic_title": "efficient parallelization layouts for large-scale distributed model training",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=dcbNzhVVQj": {
    "title": "Learning From Correctness Without Prompting Makes LLM Efficient Reasoner",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content. One potential approach to mitigate these issues is learning from human or external feedback (e.g. tools). In this paper, we introduce an intrinsic self-correct reasoning framework for LLMs that eliminates the need for human feedback, external tools, and handcraft prompts. The proposed framework, based on a multi-step reasoning paradigm \\textbf{Le}arning from \\textbf{Co}rrectness (\\textsc{LeCo}), improves reasoning performance without needing to learn from errors. This paradigm prioritizes learning from correct reasoning steps, and a unique method to measure confidence for each reasoning step based on generation logits. Experimental results across various multi-step reasoning tasks demonstrate the effectiveness of the framework in improving reasoning performance with reduced token consumption",
    "checked": true,
    "id": "4bfb6070f6a80de6a85b9edab1fb332fd0a56e69",
    "semantic_title": "learning from correctness without prompting makes llm efficient reasoner",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=3HTVP34WWE": {
    "title": "Bot or Human? Detecting ChatGPT Imposters with A Single Question",
    "volume": "main",
    "abstract": "Large language models (LLMs) like GPT-4 have recently demonstrated impressive capabilities in natural language understanding and generation. However, there is a concern that they can be misused for malicious purposes, such as fraud or denial-of-service attacks. Therefore, it is crucial to develop methods for detecting whether the party involved in a conversation is a bot or a human. In this paper, we propose a framework named **FLAIR**, Finding Large Language Model Authenticity via a Single Inquiry and Response, to detect conversational bots in an online manner. Specifically, we target a single question scenario that can effectively differentiate human users from bots. The questions are divided into two categories: those that are easy for humans but difficult for bots (e.g., counting, substitution, searching, and ASCII art reasoning), and those that are easy for bots but difficult for humans (e.g., memorization and computation). Our approach shows different strengths of these questions in their effectiveness, providing a new way for online service providers to protect themselves against nefarious activities. Our code and question set are available at https://github.com/hongwang600/FLAIR",
    "checked": true,
    "id": "f0d7ce7b514e7f3f06700d9fca0a662aa625ca2d",
    "semantic_title": "bot or human? detecting chatgpt imposters with a single question",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=nGCMLATBit": {
    "title": "Eliciting Latent Knowledge from \"Quirky\" Language Models",
    "volume": "main",
    "abstract": "Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations that robustly track the true state of the world, especially in hard-to-verify cases where the model's output is untrusted. To further ELK research, we introduce 12 datasets and a corresponding suite of \"quirky\" language models (LMs) that are finetuned to make systematic errors when answering questions *if and only if* the keyword \"Bob\" is present in the prompt. We find that, especially in middle layers, linear probes usually report an LM's knowledge independently of what the LM outputs, enabling us to elicit the correct answer despite the model's untruthful output. The best probing method (logistic regression on contrast pairs) recovers 89% of the gap in AUROC between truthful and untruthful contexts, and 75% for questions harder than those used to train the probe. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 0.95 AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods",
    "checked": false,
    "id": "6902fd1ed5b6da79ed3fa7842b8a8474dd0931d1",
    "semantic_title": "eliciting latent knowledge from quirky language models",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=D06yk3DBas": {
    "title": "Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions",
    "volume": "main",
    "abstract": "A significant amount of research is focused on developing and evaluating large language models for a variety of code synthesis tasks. These include synthesizing code from natural language, synthesizing tests from code, and synthesizing explanations of code. In contrast, the behavior of instructional code editing with LLMs is understudied. These are tasks in which the model is provided a block of code and an instruction to modify the code. The editing instruction may ask for a feature to be added or removed, describe a bug and ask for a fix, or ask for a different kind of solution. We introduce a carefully crafted benchmark of code editing tasks and use it to evaluate several cutting edge LLMs. Our evaluation exposes a significant gap between the capabilities of state-of-the-art open and closed models. For example, even GPT-3.5-Turbo is better than the best open model at code editing tasks. We also introduce a new, carefully curated, permissively licensed training dataset of code editing tasks coupled with natural language instructions. Using this training dataset, we show that we can fine-tune open Code LLMs to significantly improve their code editing capabilities, closing the gap between open and closed models. All code, data, and models are available at https://github.com/nuprl/CanItEdit",
    "checked": true,
    "id": "807cb0b27e078de76f27a36c5bfecc93de3abcea",
    "semantic_title": "can it edit? evaluating the ability of large language models to follow code editing instructions",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=Xh1B90iBSR": {
    "title": "What Are Tools Anyway? A Survey from the Language Model Perspective",
    "volume": "main",
    "abstract": "Language models (LMs) are powerful yet mostly for text generation tasks. Tools have substantially enhanced their performance for tasks that require complex skills. However, many works adopt the term \"tool\" in different ways, raising the question: What is a tool anyway? Subsequently, where and how do tools help LMs? In this survey, we provide a unified definition of tools as external programs used by LMs, and perform a systematic review of LM tooling scenarios and approaches. Grounded on this review, we empirically study the efficiency of various tooling methods by measuring their required compute and performance gains on various benchmarks, and highlight some challenges and potential future research in the field",
    "checked": true,
    "id": "1819a53eddb4a5334937561bb57542d7f11c8308",
    "semantic_title": "what are tools anyway? a survey from the language model perspective",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=q36rpGlG9X": {
    "title": "Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation",
    "volume": "main",
    "abstract": "The rapid growth of biomedical knowledge has outpaced our ability to efficiently extract insights and generate novel hypotheses. Large language models (LLMs) have emerged as a promising tool to revolutionize knowledge interaction and potentially accelerate biomedical discovery. In this paper, we present a comprehensive evaluation of LLMs as biomedical hypothesis generators. We construct a dataset of background-hypothesis pairs from biomedical literature, carefully partitioned into training, seen, and unseen test sets based on publication date to mitigate data contamination. Using this dataset, we assess the hypothesis generation capabilities of top-tier instructed models in zero-shot, few-shot, and fine-tuning settings. To enhance the exploration of uncertainty, a crucial aspect of scientific discovery, we incorporate tool use and multi-agent interactions in our evaluation framework. Furthermore, we propose four novel metrics grounded in extensive literature review to evaluate the quality of generated hypotheses, considering both LLM-based and human assessments. Our experiments yield two key findings: 1) LLMs can generate novel and validated hypotheses, even when tested on literature unseen during training, and 2) Increasing uncertainty through multi-agent interactions and tool use can facilitate diverse candidate generation and improve zero-shot hypothesis generation performance. However, we also observe that the integration of additional knowledge through few-shot learning and tool use may not always lead to performance gains, highlighting the need for careful consideration of the type and scope of external knowledge incorporated. These findings underscore the potential of LLMs as powerful aids in biomedical hypothesis generation and provide valuable insights to guide further research in this area",
    "checked": true,
    "id": "2171d913a9845de4da95a90b1ca3bbcb8194b72a",
    "semantic_title": "large language models as biomedical hypothesis generators: a comprehensive evaluation",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=sKATR2O1Y0": {
    "title": "OpenAgents: An Open Platform for Language Agents in the Wild",
    "volume": "main",
    "abstract": "Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs). Current language agent frameworks aim to facilitate the construction of proof-of-concept language agents while neglecting the non-expert user access to agents and paying little attention to application-level designs. We present OpenAgents, an open platform for using and hosting language agents in the wild of everyday life. OpenAgents includes three agents: (1) Data Agent for data analysis with Python/SQL and data tools; (2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web browsing. OpenAgents enables general users to interact with agent functionalities through a web user interface optimized for swift responses and common failures while offering developers and researchers a seamless deployment experience on local setups, providing a foundation for crafting innovative language agents and facilitating real-world evaluations. We elucidate the challenges and opportunities, aspiring to set a foundation for future research and development of real-world language agents",
    "checked": true,
    "id": "f0227a0500f2875d9af3d62b5afb3bb93c2b4561",
    "semantic_title": "openagents: an open platform for language agents in the wild",
    "citation_count": 113,
    "authors": []
  },
  "https://openreview.net/forum?id=S7NVVfuRv8": {
    "title": "How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?",
    "volume": "main",
    "abstract": "By leveraging the retrieval of information from external knowledge databases, Large Language Models (LLMs) exhibit enhanced capabilities for accomplishing many knowledge-intensive tasks. However, due to the inherent flaws of current retrieval systems, there might exist irrelevant information within those retrieving top-ranked passages. In this work, we present a comprehensive investigation into the robustness of LLMs to different types of irrelevant information under various conditions. We initially introduce a framework to construct high-quality irrelevant information that ranges from semantically unrelated, partially related, and related to questions. Furthermore, our analysis demonstrates that the constructed irrelevant information not only scores highly on similarity metrics, being highly retrieved by existing systems, but also bears semantic connections to the context. Our investigation reveals that current LLMs still face challenges in discriminating highly semantically related information and can be easily distracted by these irrelevant yet misleading content. Besides, we also find that current solutions for handling irrelevant information have limitations in improving the robustness of LLMs to such distractions. All the resources are available on [GitHub](https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information)",
    "checked": true,
    "id": "5168d3595acaf7528039ee542eb57d0dc3ad2fd8",
    "semantic_title": "how easily do irrelevant inputs skew the responses of large language models?",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=pYEnhZ6NAv": {
    "title": "How Far Are We from Intelligent Visual Deductive Reasoning?",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective when applied to LLMs do not seamlessly translate to the challenges presented by visual reasoning tasks. A detailed analysis reveals that VLMs struggle to solve these tasks mainly because they are unable to perceive and comprehend multiple, confounding abstract patterns in RPM examples",
    "checked": true,
    "id": "672ad7c1bd1a6e4e47e4748b878a448225f07a10",
    "semantic_title": "how far are we from intelligent visual deductive reasoning?",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=egVSgtJJAx": {
    "title": "VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?",
    "volume": "main",
    "abstract": "Multimodal Large Language models (MLLMs) have shown promise in web-related tasks, but evaluating their performance in the web domain remains a challenge due to the lack of comprehensive benchmarks. Existing benchmarks are either designed for general multimodal tasks, failing to capture the unique characteristics of web pages, or focus on end-to-end web agent tasks, unable to measure fine-grained abilities such as OCR, understanding, and grounding. In this paper, we introduce VisualWebBench, a multimodal benchmark designed to assess the capabilities of MLLMs across a variety of web tasks. VisualWebBench consists of seven tasks, and comprises 1.5K human-curated instances from 139 real websites, covering 87 sub-domains. We evaluate 16 open-source MLLMs, Gemini Pro, Claude-3 series, and GPT-4V(ision) on VisualWebBench, revealing significant challenges and performance gaps. Further analysis highlights the limitations of current MLLMs, including inadequate grounding in text-rich environments and subpar performance with low-resolution image inputs. We believe VisualWebBench will serve as a valuable resource for the research community and contribute to the creation of more powerful and versatile MLLMs for web-related applications",
    "checked": true,
    "id": "8484a2af6c1276f9fa11d4091fcab415808ced1d",
    "semantic_title": "visualwebbench: how far have multimodal llms evolved in web page understanding and grounding?",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=oRXPiSOGH9": {
    "title": "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking",
    "volume": "main",
    "abstract": "When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult questions. In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9%→10.9%) and CommonsenseQA (36.3%→47.2%) and observe a perplexity improvement of difficult tokens in natural text. Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way",
    "checked": true,
    "id": "b5bffe41155052a43010ec7197f832e81c546268",
    "semantic_title": "quiet-star: language models can teach themselves to think before speaking",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=nT6fQIidrQ": {
    "title": "Learning to Plan for Language Modeling from Unlabeled Data",
    "volume": "main",
    "abstract": "By training to predict the next token in an unlabeled corpus, large language models learn to perform many tasks without any labeled data. However, their next-token-prediction objective arguably limits their performance in scenarios that require planning, such as writing a coherent article. In this paper, we train a module for planning the future writing process via a self-supervised learning objective. Given the textual context, this planning module learns to predict future abstract writing actions, which correspond to centroids in a clustered text embedding space. By conditioning on these actions, our model extends the successful language model formula to more abstract planning in an unsupervised way. Empirically, we demonstrate that our method improves language modeling performance in general, particularly with respect to the text structure. Because our framework uses a planner module that is unsupervised and external to the language model, new planner modules can be trained at large scale and easily be shared with the community",
    "checked": true,
    "id": "fc8eb088e82e9b0f3d82c1ab0382cbd16be9b877",
    "semantic_title": "learning to plan for language modeling from unlabeled data",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=t3z6UlV09o": {
    "title": "How bad is training on synthetic data? A statistical analysis of language model collapse",
    "volume": "main",
    "abstract": "Model collapse, as introduced in (Shumailov et al., 2023), refers to the phenomenon where training models on synthetic data generated from previously trained models leads to a deterioration in performance. This recursive training loop makes the tails of the original distribution disappear, thereby making future-generation models forget about the initial (real) distribution. With the aim of rigorously understanding model collapse in language models, we consider in this paper a statistical model that allows us to characterize the impact of various recursive training scenarios. Specifically, we demonstrate that model collapse cannot be avoided when training solely on synthetic data. However, when mixing both real and synthetic data, we provide an estimate of a maximal amount of synthetic data below which model collapse can eventually be avoided. Our theoretical conclusions are further supported by empirical validations",
    "checked": true,
    "id": "1f71820adfe5eaa344494b1158cbe46ca2d00fc3",
    "semantic_title": "how bad is training on synthetic data? a statistical analysis of language model collapse",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=MI52iXSSNy": {
    "title": "Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?",
    "volume": "main",
    "abstract": "We present a novel task and benchmark for evaluating the ability of text-to-image(T2I) generation models to produce images that align with commonsense in real life, which we call Commonsense-T2I. Given two adversarial text prompts containing an identical set of action words with minor differences, such as *a lightbulb without electricity* vs. *a lightbulb with electricity*, we evaluate whether T2I models can conduct visual-commonsense reasoning, e.g. produce images that fit *The lightbulb is unlit* vs. *The lightbulb is lit* correspondingly. Commonsense-T2I presents an adversarial challenge, providing pairwise text prompts along with expected outputs. The dataset is carefully hand-curated by experts and annotated with fine-grained labels, such as commonsense type and likelihood of the expected outputs, to assist analyzing model behavior. We benchmark a variety of state-of-the-art (sota) T2I models and surprisingly find that, there is still a large gap between image synthesis and real life photos--even the DALL-E 3 model could only achieve 48.92% on Commonsense-T2I, and the stable diffusion XL model only achieves 24.92% accuracy. Our experiments show that GPT-enriched prompts cannot solve this challenge, and we include a detailed analysis about possible reasons for such deficiency. We aim for Commonsense-T2I to serve as a high-quality evaluation benchmark for T2I commonsense checking, fostering advancements in real life image generation",
    "checked": true,
    "id": "64a9a997d796678edc9d5693424d9feb2e9d3777",
    "semantic_title": "commonsense-t2i challenge: can text-to-image generation models understand commonsense?",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=6vEfyp0o68": {
    "title": "MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models",
    "volume": "main",
    "abstract": "Large language models such as ChatGPT and GPT-4 have recently achieved astonishing performance on a variety of natural language processing tasks. In this paper, we propose MANGO, a benchmark to evaluate their capabilities to perform text-based mapping and navigation. Our benchmark includes 53 mazes taken from a suite of textgames: each maze is paired with a walkthrough that visits every location but does not cover all possible paths. The task is question-answering: for each maze, a large language model reads the walkthrough and answers hundreds of mapping and navigation questions such as \"How should you go to Attic from West of House?\" and \"Where are we if we go north and east from Cellar?\". Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them. Further, our experiments suggest that a strong mapping and navigation ability would benefit large language models in performing relevant downstream tasks, such as playing textgames. Our MANGO benchmark will facilitate future research on methods that improve the mapping and navigation capabilities of language models. We host our leaderboard, data, code, and evaluation program at https://mango.ttic.edu and https://github.com/oaklight/mango/",
    "checked": true,
    "id": "e9b40e2cf481ebe5bf40b6958eebaf5ab1274481",
    "semantic_title": "mango: a benchmark for evaluating mapping and navigation abilities of large language models",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=ZZzXpyv65G": {
    "title": "Language Models as Critical Thinking Tools: A Case Study of Philosophers",
    "volume": "main",
    "abstract": "Current work in language models (LMs) helps us speed up or even skip thinking by accelerating and automating cognitive work. But can LMs help us with critical thinking -- thinking in deeper, more reflective ways which challenge assumptions, clarify ideas, and engineer new concepts? We treat philosophy as a case study in critical thinking, and interview 21 professional philosophers about how they engage in critical thinking and on their experiences with LMs. We find that philosophers do not find LMs to be useful because they lack a sense of selfhood (memory, beliefs, consistency) and initiative (curiosity, proactivity). We propose the selfhood-initiative model for critical thinking tools to characterize this gap. Using the model, we formulate three roles LMs could play as critical thinking tools: the Interlocutor, the Monitor, and the Respondent. We hope that our work inspires LM researchers to further develop LMs as critical thinking tools and philosophers and other `critical thinkers' to imagine intellectually substantive uses of LMs",
    "checked": true,
    "id": "4d4092fe29c8798db2cdc25fa86c192ea3998db3",
    "semantic_title": "language models as critical thinking tools: a case study of philosophers",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=IBCBMeAhmC": {
    "title": "Evaluating Language Models for Efficient Code Generation",
    "volume": "main",
    "abstract": "We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. Traditional coding benchmarks often fail to provide reliable insights into code efficiency, due to their reliance on simplistic test inputs and the absence of effective compound metrics. DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation. DPE operates in two phases: To curate efficiency datasets, it selects efficiency-demanding tasks from existing coding benchmarks and generates computationally expensive inputs to stress the efficiency of LLM solutions. To assess the code efficiency, DPE profiles the new solution and compares it globally against a set of reference solutions that exhibit distinct efficiency levels, where the matched level defines its efficiency score. As a proof of concept, we use DPE to create EvalPerf, a benchmark with 121 performance-challenging coding tasks. Our comprehensive evaluation draws interesting findings on the efficiency impact of model sizes, instruction tuning, and prompting. For example, while the scaling law fails to account for code efficiency, general instruction tuning benefits both code correctness and efficiency. We also evaluate the evaluation by examining the effectiveness of DPE, showing that EvalPerf is reliable and convenient to use even across platforms",
    "checked": true,
    "id": "47e0ded22e3f446af96b41ec25c3b38f533cb489",
    "semantic_title": "evaluating language models for efficient code generation",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=UPE6WYE8vg": {
    "title": "A Language Agent for Autonomous Driving",
    "volume": "main",
    "abstract": "Human-level driving is an ultimate goal of autonomous driving. Conventional approaches formulate autonomous driving as a perception-prediction-planning framework, yet their systems do not capitalize on the inherent reasoning ability and experiential knowledge of humans. In this paper, we propose a fundamental paradigm shift from current pipelines, exploiting Large Language Models (LLMs) as a cognitive agent to integrate human-like intelligence into autonomous driving systems. Our system, termed Agent-Driver, transforms the traditional autonomous driving pipeline by introducing a versatile tool library accessible via function calls, a cognitive memory of common sense and experiential knowledge for decision-making, and a reasoning engine capable of chain-of-thought reasoning, task planning, motion planning, and self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive common sense and robust reasoning capabilities, thus enabling a more nuanced, human-like approach to autonomous driving. We evaluate our system on both open-loop and close-loop driving challenges, and extensive experiments substantiate that our Agent-Driver significantly outperforms the state-of-the-art driving methods by a large margin. Our approach also demonstrates superior interpretability and few-shot learning ability to these methods",
    "checked": true,
    "id": "357e182a38219625dd37cba526befe5f8429aa4b",
    "semantic_title": "a language agent for autonomous driving",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=W8Rv1jVycX": {
    "title": "Description-Based Text Similarity",
    "volume": "main",
    "abstract": "Identifying texts with a given semantics is central for many information seeking scenarios. Similarity search over vector embeddings appear to be central to this ability, yet the similarity reflected in current text embeddings is corpus-driven, and is inconsistent and sub-optimal for many use cases. What, then, is a good notion of similarity for effective retrieval of text? We identify the need to search for texts based on abstract descriptions of their content, and the corresponding notion of \\emph{description based similarity}. We demonstrate the inadequacy of current text embeddings and propose an alternative model that significantly improves when used in standard nearest neighbor search. The model is trained using positive and negative pairs sourced through prompting a LLM, demonstrating how data from LLMs can be used for creating new capabilities not immediately possible using the original model",
    "checked": true,
    "id": "87cc05515253c060ee6381b4617f86c963a28b92",
    "semantic_title": "description-based text similarity",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=t4eB3zYWBK": {
    "title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries",
    "volume": "main",
    "abstract": "Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models for retrieving evidence for multi-hop queries. In the second experiment, we examine the capabilities of various state-of-the-art LLMs, including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop queries given the evidence. Both experiments reveal that existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable resource for the community in developing effective RAG systems, thereby facilitating greater adoption of LLMs in practice. We make the dataset and benchmarking code publicly available via GitHub",
    "checked": true,
    "id": "4e71624e90960cb003e311a0fe3b8be4c2863239",
    "semantic_title": "multihop-rag: benchmarking retrieval-augmented generation for multi-hop queries",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=C0j44uRPcl": {
    "title": "On Robustness-Accuracy Characterization of Language Models using Synthetic Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": "ab0a24c4cb8f8630d79f1082064ecee232aca5dc",
    "semantic_title": "on robustness-accuracy characterization of large language models using synthetic datasets",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=QdWhj0QZFw": {
    "title": "LLM360: Towards Fully Transparent Open-Source LLMs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9266dc3c65334e36a12fef7e4b231091d346b8a4",
    "semantic_title": "llm360: towards fully transparent open-source llms",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=dribhnhm1i": {
    "title": "Tuning Language Models by Proxy",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f6d7482bb5baf33f22b862ad4997f5c8cd13db21",
    "semantic_title": "tuning language models by proxy",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=PKfAq8N4fK": {
    "title": "AgentKit: Structured LLM Reasoning with Dynamic Graphs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5c7a5bbcac94eb4ee97782a8708d8c2d85fc5984",
    "semantic_title": "agentkit: structured llm reasoning with dynamic graphs",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=DOMP5AgwQz": {
    "title": "CTIKG: LLM-Powered Knowledge Graph Construction from Cyber Threat Intelligence",
    "volume": "main",
    "abstract": "To gain visibility into evolving threat landscape, knowledge of cyber threats has been aggressively collected across organizations and is often shared through Cyber Threat Intelligence (CTI). While knowledge of CTI can be shared via structured format such as Indicators of Compromise (IOC), articles in technical blogs and posts in forums (referred to as CTI articles) provide more comprehensive descriptions of the observed real-world at- tacks. However, existing works can only analyze standard texts from mainstream cyber threat knowledge bases such as CVE and NVD, and lack of the capability to link multiple CTI articles to uncover the relationships among security-related entities such as vulnerabilities. In this paper, we propose a novel approach, CTIKG, that utilizes prompt engineering to efficiently build a security-oriented knowledge graph from CTI articles based on LLMs. To mitigate the challenges of LLMs in randomness, hallucinations and tokens limitation, CTIKG divides an article into segments and employs multiple LLM agents with dual memory design to (1) process each text segment separately and (2) summarize the results of the text segments to generate more accurate results. We evaluate CTIKG on two representative benchmarks built from real world CTI articles, and the results show that CTIKG achieves 86.88% precision in building security-oriented knowledge graphs, achieving at least 30% improvements over the state-of-the-art techniques. We also demonstrate that the retry mechanism makes open source language models outperform GPT4 for building knowledge graphs",
    "checked": false,
    "id": "8a7f2cda8f95aac35660c60a2f92d81a0e308041",
    "semantic_title": "attackg+:boosting attack knowledge graph construction with large language models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Pvn1dKreZW": {
    "title": "Merge Conflicts!'\" Exploring the Impacts of External Knowledge Distractors to Parametric Knowledge Graphs",
    "volume": "main",
    "abstract": "Large language models (LLMs) acquire extensive knowledge during pre-training, known as their parametric knowledge. However, to remain up-to-date and align with human instructions, LLMs inevitably require external knowledge during interactions. This raises a crucial question: How will LLMs respond when external knowledge interferes with their parametric knowledge? To uncover the impacts systematically, we construct parametric knowledge graphs to reveal different LLM knowledge structures, and introduce external information through external knowledge distractors of varying degrees, methods, positions, and formats. Experiments on both closed and open-source models demonstrate that LLMs tend to believe in external knowledge sources, particularly when they direct conflict or make confounding changes within detailed contexts. We also discover while LLMs are sensitive to external knowledge veracity, they still get distracted by unrelated information. These findings highlight the mechanisms behind LLM's integration of external knowledge, even indirectly, during model-user interactions",
    "checked": false,
    "id": "7bf607c643a1f7f4607db84dfc0f4b63ff0bde70",
    "semantic_title": "merge conflicts!\" exploring the impacts of external distractors to parametric knowledge graphs",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=soGxskHGox": {
    "title": "Linearizing Large Language Models",
    "volume": "main",
    "abstract": "Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed recurrent state. However, they suffer from poor scaling and under-perform compute-matched transformers. Prior models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. In this paper, we propose Scalable UPtraining for Recurrent Attention (SUPRA), an alternative to pre-training linear transformers. We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre- training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify a persistent in-context learning shortfall for even the largest linear models",
    "checked": true,
    "id": "862479f7bd78a69c52a0691766848caa8eb4660f",
    "semantic_title": "linearizing large language models",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=kh9Zt2Ldmn": {
    "title": "Don't throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding",
    "volume": "main",
    "abstract": "Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the *value network*, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the *policy network*. More concretely, we present a novel *value-guided* decoding algorithm called **PPO-MCTS**, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS greatly improves the preferability of generated text compared to the standard practice of using only the PPO policy. Our results demonstrate the promise of search algorithms even on top of the aligned language models from PPO, and the under-explored benefit of the value network",
    "checked": true,
    "id": "1671d70a135b1e28b3a9cbc830feaa9b0c57df32",
    "semantic_title": "don't throw away your value model! generating more preferable text with value-guided monte-carlo tree search decoding",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=u2vAyMeLMm": {
    "title": "Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens",
    "volume": "main",
    "abstract": "Are $n$-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is *yes*, and we showcase their values in both text analysis and improving neural LLMs. This was done by modernizing $n$-gram LMs in two aspects. First, we train them at the same data scale as neural LLMs -- **5 trillion tokens**. This is one of the largest $n$-gram LMs ever built. Second, existing $n$-gram LMs use small $n$ which hinders their performance; we instead allow $n$ to be arbitrarily large, by introducing a new **$\\infty$-gram LM** with backoff. Instead of pre-computing $n$-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\\infty$-gram (as well as $n$-gram with arbitrary $n$) probabilities with **millisecond-level latency**. The $\\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\\infty$-gram LM has fairly high accuracy for next-token prediction (47%), and can complement neural LLMs to greatly reduce their perplexity. When analyzing machine-generated text, we also observe irregularities in the machine: $\\infty$-gram agreement level with respect to the suffix length, which indicates deficiencies in neural LLM pretraining and the positional embeddings of Transformers",
    "checked": true,
    "id": "1c143a753d4eb82da5526132149f6e4ed5271b63",
    "semantic_title": "infini-gram: scaling unbounded n-gram language models to a trillion tokens",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=2nTzomzjjb": {
    "title": "ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction",
    "volume": "main",
    "abstract": "The prediction of protein-protein interactions (PPIs) is crucial for understanding biological functions and diseases. Previous machine learning approaches to PPI prediction mainly focus on direct physical interactions, ignoring the broader context of nonphysical connections through intermediate proteins, thus limiting their effectiveness. The emergence of Large Language Models (LLMs) provides a new opportunity for addressing this complex biological challenge. By transforming structured data into natural language prompts, we can map the relationships between proteins into texts. This approach allows LLMs to identify indirect connections between proteins, tracing the path from upstream to downstream. Therefore, we propose a novel framework ProLLM that employs an LLM tailored for PPI for the first time. Specifically, we propose Protein Chain of Thought (ProCoT), which replicates the biological mechanism of signaling pathways as natural language prompts. ProCoT considers a signaling pathway as a protein reasoning process, which starts from upstream proteins and passes through several intermediate proteins to transmit biological signals to downstream proteins. Thus, we can use ProCoT to predict the interaction between upstream proteins and downstream proteins. The training of ProLLM employs the ProCoT format, which enhances the model's understanding of complex biological problems. In addition to ProCoT, this paper also contributes to the exploration of embedding replacement of protein sites in natural language prompts, and instruction fine-tuning in protein knowledge datasets. We demonstrate the efficacy of ProLLM through rigorous validation against benchmark datasets, showing significant improvement over existing methods in terms of prediction accuracy and generalizability. Our results highlight the potential of LLMs to transform the field of PPI, serving as a robust potential tool for various categories of biological and medical research. The code is available at: https://anonymous.4open.science/r/ProLLM-AB04",
    "checked": true,
    "id": "a9086398e27327e5cc664cbabf8a7bbb25c2c50f",
    "semantic_title": "prollm: protein chain-of-thoughts enhanced llm for protein-protein interaction prediction",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=EEPBOB2Xww": {
    "title": "Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models",
    "volume": "main",
    "abstract": "While Ferret seamlessly integrates regional understanding into the Large Language Model (LLM) to facilitate its referring and grounding capability, it poses certain limitations: constrained by the pre-trained fixed visual encoder and failed to perform well on broader tasks. In this work, we unveil Ferret-v2, a significant upgrade to Ferret, with three key designs. (1) Any resolution grounding and referring: A flexible approach that effortlessly handles higher image resolution, improving the model's ability to process and understand images in greater detail. (2) Multi-granularity visual encoding: By integrating the additional DINOv2 encoder, the model learns better and diverse underlying contexts for global and fine-grained visual information. (3) A three-stage training paradigm: Besides image-caption alignment, an additional stage is proposed for high-resolution dense alignment before the final instruction tuning. Experiments show that Ferret-v2 provides substantial improvements over Ferret and other state-of-the-art methods, thanks to its high-resolution scaling and fine-grained visual processing",
    "checked": true,
    "id": "a39ed048aa86296b16f9d2d33011f94e3d5ac402",
    "semantic_title": "ferret-v2: an improved baseline for referring and grounding with large language models",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=HDkNbfLQgu": {
    "title": "Reverse Training to Nurse the Reversal Curse",
    "volume": "main",
    "abstract": "Large language models (LLMs) have a surprising failure: when trained on ``A has a feature B``, they do not generalize to ``B is a feature of A``, which is termed the Reversal Curse. Even when training with trillions of tokens this issue still appears due to Zipf's law -- hence even if we train on the entire internet. This work proposes an alternative training scheme, called $reverse$ $training$, whereby all words are used twice, doubling the amount of available tokens. The LLM is trained in both forward and reverse directions by reversing training strings while preserving (i.e., not reversing) chosen substrings, such as entities. We show that data matched reverse-trained models provide superior performance to standard models on standard tasks, and compute matched reverse-trained models provide far superior performance on reversal tasks, helping resolve the reversal curse issue",
    "checked": true,
    "id": "e2c50a18b92713c89be991af6cc1a652adfee362",
    "semantic_title": "reverse training to nurse the reversal curse",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=KZd1EErRJ1": {
    "title": "IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations",
    "volume": "main",
    "abstract": "Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose **IsoBench**, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple **isomorphic representations** of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.66 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.71 points worse and Gemini Pro is 14.87 points worse. Finally, we present two prompting techniques, *IsoCombination* and *IsoScratchPad*, which improve model performance by considering combinations of, and translations between, different input representations",
    "checked": true,
    "id": "89d857fdd842e2a632b76a8f38cf8d0f03aef46f",
    "semantic_title": "isobench: benchmarking multimodal foundation models on isomorphic representations",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=K1M3gLW0MX": {
    "title": "On Fairness of Low-Rank Adaptation of Large Models",
    "volume": "main",
    "abstract": "Low-rank adaptation of large models, particularly LoRA, has gained traction due to its computational efficiency. This efficiency, contrasted with the prohibitive costs of full-model fine-tuning, means that practitioners often turn to LoRA without a complete understanding of its ramifications. In this study, we focus on fairness and ask whether LoRA has an unexamined impact on utility, calibration, and resistance to membership inference across different subgroups (e.g., genders, races, religions) compared to a full-model fine-tuning baseline. We present extensive experiments across vision and language domains and across classification and generation tasks using ViT-Base, Swin-v2-Large, Llama-2 7B, and Mistral 7B. Intriguingly, experiments suggest that while one can isolate cases where LoRA exacerbates model bias across subgroups, the pattern is inconsistent---in many cases, LoRA has equivalent or even improved fairness compared to the base model or its full fine-tuning baseline. We also examine the complications of evaluating fine-tuning fairness relating to task design and model token bias, calling for more careful fairness evaluations in future work",
    "checked": true,
    "id": "9d059ae691953578017f1361742c4b36046b92e8",
    "semantic_title": "on fairness of low-rank adaptation of large models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=3GhOWfSLrD": {
    "title": "Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs",
    "volume": "main",
    "abstract": "Although large language models (LLMs) have demonstrated remarkable proficiency in modeling text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example, versus considering broader facts or statistical evidence. This research investigates the impact of the representativeness heuristic on LLM reasoning. We created ReHeAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to ReHeAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps are often incorrectly based on a stereotype rather than on the problem's description. Interestingly, the performance improves when adding a hint in the prompt to remind the model to use its knowledge. This suggests the uniqueness of the representativeness heuristic compared to traditional biases. It can occur even when LLMs possess the correct knowledge while falling into a cognitive trap. This highlights the importance of future research focusing on the representativeness heuristic in model reasoning and decision-making and on developing solutions to address it",
    "checked": true,
    "id": "d4e3aa50b3d822021628d7c8d89519ec0389fbd8",
    "semantic_title": "will the real linda please stand up...to large language models? examining the representativeness heuristic in llms",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=dnwRScljXr": {
    "title": "Evaluating LLMs at Detecting Errors in LLM Responses",
    "volume": "main",
    "abstract": "With Large Language Models (LLMs) being widely used across various tasks, detecting errors in their responses is increasingly crucial. However, little research has been conducted on error detection of LLM responses. Collecting error annotations on LLM responses is challenging due to the subjective nature of many NLP tasks, and thus previous research focuses on tasks of little practical value (e.g., word sorting) or limited error types (e.g., faithfulness in summarization). This work introduces ReaLMistake, the first error detection benchmark consisting of objective, realistic, and diverse errors made by LLMs. ReaLMistake contains three challenging and meaningful tasks that introduce objectively assessable errors in four categories (reasoning correctness, instruction-following, context-faithfulness, and parameterized knowledge), eliciting naturally observed and diverse errors in responses of GPT-4 and Llama 2 70B annotated by experts. We use ReaLMistake to evaluate error detectors based on 12 LLMs. Our findings show: 1) Top LLMs like GPT-4 and Claude 3 detect errors made by LLMs at very low recall, and all LLM-based error detectors perform much worse than humans. 2) Explanations by LLM-based error detectors lack reliability. 3) LLMs-based error detection is sensitive to small changes in prompts but remains challenging to improve. 4) Popular approaches to improving LLMs, including self-consistency and majority vote, do not improve the error detection performance. Our benchmark and code are provided at https://github.com/psunlpgroup/ReaLMistake",
    "checked": true,
    "id": "d5e9532d12a26124f019de5f1a1d6a9a6eab3d03",
    "semantic_title": "evaluating llms at detecting errors in llm responses",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=F7aAhfitX6": {
    "title": "Massive Activations in Large Language Models",
    "volume": "main",
    "abstract": "We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers",
    "checked": true,
    "id": "aa085ed9a77d815d3c1d19a468be1d9a8d3efd13",
    "semantic_title": "massive activations in large language models",
    "citation_count": 94,
    "authors": []
  }
}