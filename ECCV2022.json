{
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/19_ECCV_2022_paper.php": {
    "title": "Learning Depth from Focus in the Wild",
    "abstract": "For better photography, most recent commercial cameras including smartphones have either adopted large-aperture lens to collect more light or used a burst mode to take multiple images within short times. These interesting features lead us to examine depth from focus/defocus. In this work, we present a convolutional neural network-based depth estimation from single focal stacks. Our method differs from relevant state-of-the-art works with three unique features. First, our method allows depth maps to be inferred in an end-to-end manner even with image alignment. Second, we propose a sharp region detection module to reduce blur ambiguities in subtle focus changes and weakly texture-less regions. Third, we design an effective downsampling module to ease flows of focal information in feature extractions. In addition, for the generalization of the proposed network, we develop a simulator to realistically reproduce the features of commercial cameras, such as changes in field of view, focal length and principal points. By effectively incorporating these three unique features, our network achieves the top rank in the DDFF 12-Scene benchmark on most metrics. We also demonstrate the effectiveness of the proposed method on various quantitative evaluations and real-world images taken from various off-the-shelf cameras compared with state-of-the-art methods. Our source code is publicly available at https://github.com/wcy199705/DfFintheWild",
    "volume": "main",
    "checked": true,
    "id": "da4eb6f3b84be058148e0d0134037d43c083129a",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/69_ECCV_2022_paper.php": {
    "title": "Learning-Based Point Cloud Registration for 6D Object Pose Estimation in the Real World",
    "abstract": "In this work, we tackle the task of estimating the 6D pose of an object from point cloud data. While recent learning-based approaches to addressing this task have shown great success on synthetic datasets, we have observed them to fail in the presence of real-world data. We thus analyze the causes of these failures, which we trace back to the difference between the feature distributions of the source and target point clouds, and the sensitivity of the widely-used SVD-based loss function to the range of rotation between the two point clouds. We address the first challenge by introducing a new normalization strategy, Match Normalization, and the second via the use of a loss function based on the negative log likelihood of point correspondences. Our two contributions are general and can be applied to many existing learning-based 3D object registration frameworks, which we illustrate by implementing them in two of them, DCP and IDAM. Our experiments on the real-scene TUD-L, LINEMOD and Occluded-LINEMOD datasets evidence the benefits of our strategies. They allow for the first time learning-based 3D object registration methods to achieve meaningful results on real-world data. We therefore expect them to be key to the future development of point cloud registration methods",
    "volume": "main",
    "checked": true,
    "id": "3107318426e3f6f28882f31937aa95aabc358c94",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/127_ECCV_2022_paper.php": {
    "title": "An End-to-End Transformer Model for Crowd Localization",
    "abstract": "Crowd localization, predicting head positions, is a more practical and high-level task than simply counting. Existing methods employ pseudo-bounding boxes or pre-designed localization maps, relying on complex post-processing to obtain the head positions. In this paper, we propose an elegant, end-to-end Crowd Localization TRansformer named CLTR that solves the task in the regression-based paradigm. The proposed method views the crowd localization as a direct set prediction problem, taking extracted features and trainable embeddings as input of the transformer-decoder. To reduce the ambiguous points and generate more reasonable matching results, we introduce a KMO-based Hungarian matcher, which adopts the nearby context as the external matching cost. Extensive experiments conducted on five datasets in various data settings show the effectiveness of our method. In particular, the proposed method achieves the best localization performance on the NWPU-Crowd, UCF-QNRF, and ShanghaiTech Part A datasets",
    "volume": "main",
    "checked": true,
    "id": "d1aa3961270de9c8740d1255c08152d862572314",
    "citation_count": 3
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/192_ECCV_2022_paper.php": {
    "title": "Few-Shot Single-View 3D Reconstruction with Memory Prior Contrastive Network",
    "abstract": "3D reconstruction of novel categories based on few-shot learning is appealing in real-world applications and attracts increasing research interests. Previous approaches mainly focus on how to design shape prior models for different categories. Their performance on unseen categories is not very competitive. In this paper, we present a Memory Prior Contrastive Network (MPCN) that can store shape prior knowledge in a few-shot learning based 3D reconstruction framework. With the shape memory, a multi-head attention module is proposed to capture different parts of a candidate shape prior and fuse these parts together to guide 3D reconstruction of novel categories. Besides, we introduce a 3D-aware contrastive learning method, which can not only complement the retrieval accuracy of memory network, but also better organize image features for downstream tasks. Compared with previous few-shot 3D reconstruction methods, MPCN can handle the inter-class variability without category annotations. Experimental results on a benchmark synthetic dataset and the Pascal3D+ real-world dataset show that our model outperforms the current state-of-the-art methods significantly",
    "volume": "main",
    "checked": true,
    "id": "95ceb387384bf11a5582632cc2479d0250ddfe75",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/343_ECCV_2022_paper.php": {
    "title": "DID-M3D: Decoupling Instance Depth for Monocular 3D Object Detection",
    "abstract": "Monocular 3D detection has drawn much attention from the community due to its low cost and setup simplicity. It takes an RGB image as input and predicts 3D boxes in the 3D space. The most challenging sub-task lies in the instance depth estimation. Previous works usually use a direct estimation method. However, in this paper we point out that the instance depth on the RGB image is non-intuitive. It is coupled by visual depth clues and instance attribute clues, making it hard to be directly learned in the network. Therefore, we propose to reformulate the instance depth to the combination of the instance visual surface depth (visual depth) and the instance attribute depth (attribute depth). The visual depth is related to objects’ appearances and positions on the image. By contrast, the attribute depth relies on objects’ inherent attributes, which are invariant to the object affine transformation on the image. Correspondingly, we decouple the 3D location uncertainty into visual depth uncertainty and attribute depth uncertainty. By combining different types of depths and associated uncertainties, we can obtain the final instance depth. Furthermore, data augmentation in monocular 3D detection is usually limited due to the physical nature, hindering the boost of performance. Based on the proposed instance depth disentanglement strategy, we can alleviate this problem. Evaluated on KITTI, our method achieves new state-of-the-art results, and extensive ablation studies validate the effectiveness of each component in our method. The codes are released at https://github.com/SPengLiang/DID-M3D",
    "volume": "main",
    "checked": true,
    "id": "e0e97f307d6526a6b1682575378b2882121d7f24",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/405_ECCV_2022_paper.php": {
    "title": "Adaptive Co-Teaching for Unsupervised Monocular Depth Estimation",
    "abstract": "Unsupervised depth estimation using photometric losses suffers from local minimum and training instability. We address this issue by proposing an adaptive co-teaching framework to distill the learned knowledge from unsupervised teacher networks to a student network. We design an ensemble architecture for our teacher networks, integrating a depth basis decoder with multiple depth coefficient decoders. Depth prediction can then be formulated as a combination of the predicted depth bases weighted by coefficients. By further constraining their correlations, multiple coefficient decoders can yield a diversity of depth predictions, serving as the ensemble teachers. During the co-teaching step, our method allows different supervision sources from not only ensemble teachers but also photometric losses to constantly compete with each other, and adaptively select the optimal ones to teach the student, which effectively improves the ability of the student to jump out of the local minimum. Our method is shown to significantly benefit unsupervised depth estimation and sets new state of the art on both KITTI and Nuscenes datasets",
    "volume": "main",
    "checked": true,
    "id": "74695c2b17b1636f8afd5ecc9f1343c78aa9b91a",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/444_ECCV_2022_paper.php": {
    "title": "Fusing Local Similarities for Retrieval-Based 3D Orientation Estimation of Unseen Objects",
    "abstract": "In this paper, we tackle the task of estimating the 3D orientation of previously-unseen objects from monocular images. This task contrasts with the one considered by most existing deep learning methods which typically assume that the testing objects have been observed during training. To handle the unseen objects, we follow a retrieval-based strategy and prevent the network from learning object-specific features by computing multi-scale local similarities between the query image and synthetically-generated reference images. We then introduce an adaptive fusion module that robustly aggregates the local similarities into a global similarity score of pairwise images. Furthermore, we speed up the retrieval process by developing a fast retrieval strategy. Our experiments on the LineMOD, LineMOD-Occluded, and T-LESS datasets show that our method yields a significantly better generalization to unseen objects than previous works. Our code and pre-trained models are available at https://sailor-z.github.io/projects/Unseen_Object_Pose.html",
    "volume": "main",
    "checked": true,
    "id": "3f627eb4fe07353d38a8de4c66b7f8bbc2940504",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/655_ECCV_2022_paper.php": {
    "title": "Lidar Point Cloud Guided Monocular 3D Object Detection",
    "abstract": "Monocular 3D object detection is a challenging task in the self-driving and computer vision community. As a common practice, most previous works use manually annotated 3D box labels, where the annotating process is expensive. In this paper, we find that the precisely and carefully annotated labels may be unnecessary in monocular 3D detection, which is an interesting and counterintuitive finding. Using rough labels that are randomly disturbed, the detector can achieve very close accuracy compared to the one using the ground-truth labels. We delve into this underlying mechanism and then empirically find that: concerning the label accuracy, the 3D location part in the label is preferred compared to other parts of labels. Motivated by the conclusions above and considering the precise LiDAR 3D measurement, we propose a simple and effective framework, dubbed LiDAR point cloud guided monocular 3D object detection (LPCG). This framework is capable of either reducing the annotation costs or considerably boosting the detection accuracy without introducing extra annotation costs. Specifically, It generates pseudo labels from unlabeled LiDAR point clouds. Thanks to accurate LiDAR 3D measurements in 3D space, such pseudo labels can replace manually annotated labels in the training of monocular 3D detectors, since their 3D location information is precise. LPCG can be applied into any monocular 3D detector to fully use massive unlabeled data in a self-driving system. As a result, in KITTI benchmark, we take the first place on both monocular 3D and BEV (bird’s-eye-view) detection with a significant margin. In Waymo benchmark, our method using 10% labeled data achieves comparable accuracy to the baseline detector using 100% labeled data. The codes are released at https://github.com/SPengLiang/LPCG",
    "volume": "main",
    "checked": true,
    "id": "671da12c7f419eea0d8410832c908286f1242361",
    "citation_count": 15
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/656_ECCV_2022_paper.php": {
    "title": "Structural Causal 3D Reconstruction",
    "abstract": "This paper considers the problem of unsupervised 3D object reconstruction from in-the-wild single-view images. Due to ambiguity and intrinsic ill-posedness, this problem is inherently difficult to solve and therefore requires strong regularization to achieve disentanglement of different latent factors. Unlike existing works that introduce explicit regularizations into objective functions, we look into a different space for implicit regularization -- the structure of latent space. Specifically, we restrict the structure of latent space to capture a topological causal ordering of latent factors (i.e., representing causal dependency as a directed acyclic graph). We first show that different causal orderings matter for 3D reconstruction, and then explore several approaches to find a task-dependent causal factor ordering. Our experiments demonstrate that the latent space structure indeed serves as an implicit regularization and introduces an inductive bias beneficial for reconstruction",
    "volume": "main",
    "checked": true,
    "id": "73cc10f3ebce9a73d39be3eb180c74a57130f564",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1049_ECCV_2022_paper.php": {
    "title": "3D Human Pose Estimation Using Möbius Graph Convolutional Networks",
    "abstract": "3D human pose estimation is fundamental to understanding human behavior. Recently, promising results have been achieved by graph convolutional networks(GCNs), which achieve state-of-the-art performance and provide rather light-weight architectures. However, a major limitation of GCNs is their inability to encode all the transformations between joints explicitly. To address this issue, we propose a novel spectral GCN using the Möbius transformation (MöbiusGCN). In particular, this allows us to directly and explicitly encode the transformation between joints, resulting in a significantly more compact representation. Compared to even the lightest architectures so far, our novel approach requires 90-98% fewer parameters, i.e. our lightest MöbiusGCN uses only 0.042M trainable parameters. Besides the drastic parameter reduction, explicitly encoding the transformation of joints also enables us to achieve state-of-the-art results. We evaluate our approach on the two challenging pose estimation benchmarks, Human3.6M and MPI-INF-3DHP, demonstrating both state-of-the-art results and the generalization capabilities of MöbiusGCN",
    "volume": "main",
    "checked": true,
    "id": "54c8120fa83da978700b9064978ef2e25ad640a0",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1235_ECCV_2022_paper.php": {
    "title": "Learning to Train a Point Cloud Reconstruction Network without Matching",
    "abstract": "Reconstruction networks for well-ordered data such as 2D images and 1D continuous signals are easy to optimize through element-wised squared errors, while permutation-arbitrary point clouds cannot be constrained directly because their points permutations are not fixed. Though existing works design algorithms to match two point clouds and evaluate shape errors based on matched results, they are limited by pre-defined matching processes. In this work, we propose a novel framework named PCLossNet which learns to train a point cloud reconstruction network without any matching. By training through an adversarial process together with the reconstruction network, PCLossNet can better explore the differences between point clouds and create more precise reconstruction results. Experiments on multiple datasets prove the superiority of our method, where PCLossNet can help networks achieve much lower reconstruction errors and extract more representative features, with about 4 times faster training efficiency than the commonly-used EMD loss. Our codes can be found in https://github.com/Tianxinhuang/PCLossNet",
    "volume": "main",
    "checked": true,
    "id": "e71603fea9822974c5d720d23288dcfa7b470470",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1300_ECCV_2022_paper.php": {
    "title": "PanoFormer: Panorama Transformer for Indoor 360° Depth Estimation",
    "abstract": "Existing panoramic depth estimation methods based on convolutional neural networks (CNNs) focus on removing panoramic distortions, failing to perceive panoramic structures efficiently due to the fixed receptive field in CNNs. This paper proposes the panorama Transformer (named PanoFormer) to estimate the depth in panorama images, with tangent patches from spherical domain, learnable token flows, and panorama specific metrics. In particular, we divide patches on the spherical tangent domain into tokens to reduce the negative effect of panoramic distortions. Since the geometric structures are essential for depth estimation, a self-attention module is redesigned with an additional learnable token flow. In addition, considering the characteristic of the spherical domain, we present two panorama-specific metrics to comprehensively evaluate the panoramic depth estimation models’ performance. Extensive experiments demonstrate that our approach significantly outperforms the state-of-the-art methods. At last, the proposed method can be effectively extended to solve semantic panorama segmentation, a similar pixel2pixel task. Code will be released upon acceptance",
    "volume": "main",
    "checked": false,
    "id": "b09c550d7bf183eb47a45ea3969d4d31c412bf05",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1534_ECCV_2022_paper.php": {
    "title": "Self-supervised Human Mesh Recovery with Cross-Representation Alignment",
    "abstract": "Fully supervised human mesh recovery methods are data-hungry and have poor generalizability due to the limited availability and diversity of 3D-annotated benchmark datasets. Recent progress in self-supervised human mesh recovery has been made using synthetic-data-driven training paradigms where the model is trained from synthetic paired 2D representation (e.g., 2D keypoints and segmentation masks) and 3D mesh. However, on synthetic dense correspondence maps (i.e., IUV) few have been explored since the domain gap between synthetic training data and real testing data is hard to address for 2D dense representation. To alleviate this domain gap on IUV, we propose cross-representation alignment utilizing the complementary information from the robust but sparse representation (2D keypoints). Specifically, the alignment errors between initial mesh estimation and both 2D representations are forwarded into regressor and dynamically corrected in the following mesh regression. This adaptive cross-representation alignment explicitly learns from the deviations and captures complementary information: robustness from sparse representation and richness from dense representation. We conduct extensive experiments on multiple standard benchmark datasets and demonstrate competitive results, helping take a step towards reducing the annotation effort needed to produce state-of-the-art models in human mesh estimation",
    "volume": "main",
    "checked": true,
    "id": "1bb0277fd9a7a41f9d96da2c37e546760c9c3859",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1549_ECCV_2022_paper.php": {
    "title": "AlignSDF: Pose-Aligned Signed Distance Fields for Hand-Object Reconstruction",
    "abstract": "Recent work achieved impressive progress towards joint reconstruction of hands and manipulated objects from monocular color images. Existing methods focus on two alternative representations in terms of either parametric meshes or signed distance fields (SDFs). On one side, parametric models can benefit from prior knowledge at the cost of limited shape deformations and mesh resolutions. Mesh models, hence, may fail to precisely reconstruct details such as contact surfaces of hands and objects. SDF-based methods, on the other side, can represent arbitrary details but are lacking explicit priors. In this work we aim to improve SDF models using priors provided by parametric representations. In particular, we propose a joint learning framework that disentangles the pose and the shape. We obtain hand and object poses from parametric models and use them to align SDFs in 3D space. We show that such aligned SDFs better focus on reconstructing shape details and improve reconstruction accuracy both for hands and objects. We evaluate our method and demonstrate significant improvements over the state of the art on the challenging ObMan and DexYCB benchmarks",
    "volume": "main",
    "checked": true,
    "id": "469807b841e3c46acfea9cf78d111c8741430053",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1737_ECCV_2022_paper.php": {
    "title": "A Reliable Online Method for Joint Estimation of Focal Length and Camera Rotation",
    "abstract": "Linear perspective cues deriving from regularities of the built environment can be used to recalibrate both intrinsic and extrinsic camera parameters online, but these estimates can be unreliable due to irregularities in the scene, uncertainties in line segment estimation and background clutter. Here we address this challenge through four initiatives. First, we use the PanoContext panoramic image dataset to curate a novel and realistic dataset of planar projections over a broad range of scenes, focal lengths and camera poses. Second, we use this novel dataset and the YorkUrbanDB to systematically evaluate the linear perspective deviation measures frequently found in the literature and show that the choice of deviation measure and likelihood model has a huge impact on reliability. Third, we use these findings to create a novel system for online camera calibration we call fR, and show that it outperforms the prior state of the art, substantially reducing error in estimated camera rotation and focal length. Our fourth contribution is a novel and efficient approach to estimating uncertainty that can dramatically improve online reliability for performance-critical applications by strategically selecting which frames to use for recalibration",
    "volume": "main",
    "checked": true,
    "id": "a4929b0c5c9f47259ef1412a00344adb9b949823",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1832_ECCV_2022_paper.php": {
    "title": "PS-NeRF: Neural Inverse Rendering for Multi-View Photometric Stereo",
    "abstract": "Traditional multi-view photometric stereo (MVPS) methods are often composed of multiple disjoint stages, resulting in noticeable accumulated errors. In this paper, we present a neural inverse rendering method for MVPS based on implicit representation. Given multi-view images of a non-Lambertian object illuminated by multiple unknown directional lights, our method jointly estimates the geometry, materials, and lights. Our method first employs multi-light images to estimate per-view surface normal maps, which are used to regularize the normals derived from the neural radiance field. It then jointly optimizes the surface normals, spatially-varying BRDFs, and lights based on a shadow-aware differentiable rendering layer. After optimization, the reconstructed object can be used for novel-view rendering, relighting, and material editing. Experiments on both synthetic and real datasets demonstrate that our method achieves far more accurate shape reconstruction than existing MVPS and neural rendering methods. Our code and model can be found at https://ywq.github.io/psnerf",
    "volume": "main",
    "checked": true,
    "id": "ec26727f05900617fdefbc302f8549ed5d72301d",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1851_ECCV_2022_paper.php": {
    "title": "Share with Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency",
    "abstract": "Approaches for single-view reconstruction typically rely on viewpoint annotations, silhouettes, the absence of background, multiple views of the same instance, a template shape, or symmetry. We avoid all such supervision and assumptions by explicitly leveraging the consistency between images of different object instances. As a result, our method can learn from large collections of unlabelled images depicting the same object category. Our main contributions are two ways for leveraging cross-instance consistency: (i) progressive conditioning, a training strategy to gradually specialize the model from category to instances in a curriculum learning fashion; and (ii) neighbor reconstruction, a loss enforcing consistency between instances having similar shape or texture. Also critical to the success of our method are: our structured autoencoding architecture decomposing an image into explicit shape, texture, pose, and background; an adapted formulation of differential rendering; and a new optimization scheme alternating between 3D and pose learning. We compare our approach, UNICORN, both on the diverse synthetic ShapeNet dataset - the classical benchmark for methods requiring multiple views as supervision - and on standard real-image benchmarks (Pascal3D+ Car, CUB) for which most methods require known templates and silhouette annotations. We also showcase applicability to more challenging real-world collections (CompCars, LSUN), where silhouettes are not available and images are not cropped around the object",
    "volume": "main",
    "checked": true,
    "id": "fb57a98b39488c1f6c1e2a5575c5e82cc7e28a6d",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1925_ECCV_2022_paper.php": {
    "title": "Towards Comprehensive Representation Enhancement in Semantics-Guided Self-Supervised Monocular Depth Estimation",
    "abstract": "Semantics-guided self-supervised monocular depth estimation has been widely researched, owing to the strong cross-task correlation of depth and semantics. However, since depth estimation and semantic segmentation are fundamentally two types of tasks: one is regression while the other is classification, the distribution of depth feature and semantic feature are naturally different. Previous works that leverage semantic information in depth estimation mostly neglect such representational discrimination, which leads to insufficient representation enhancement of depth feature. In this work, we propose an attention-based module to enhance task-specific feature by addressing their feature uniqueness within instances. Additionally, we propose a metric learning based approach to accomplish comprehensive enhancement on depth feature by creating a separation between instances in feature space. Extensive experiments and analysis demonstrate the effectiveness of our proposed method. In the end, our method achieves the state-of-the-art performance on KITTI dataset",
    "volume": "main",
    "checked": true,
    "id": "e5a2734907136dcdfabb8cf4b7b30b34cdd3e315",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2057_ECCV_2022_paper.php": {
    "title": "AvatarCap: Animatable Avatar Conditioned Monocular Human Volumetric Capture",
    "abstract": "To address the ill-posed problem caused by partial observations in monocular human volumetric capture, we present AvatarCap, a novel framework that introduces animatable avatars into the capture pipeline for high-fidelity reconstruction in both visible and invisible regions. Our method firstly creates an animatable avatar for the subject from a small number ( 20) of 3D scans as a prior. Then given a monocular RGB video of this subject, our method integrates information from both the image observation and the avatar prior, and accordingly reconstructs high-fidelity 3D textured models with dynamic details regardless of the visibility. To learn an effective avatar for volumetric capture from only few samples, we propose GeoTexAvatar, which leverages both geometry and texture supervisions to constrain the pose-dependent dynamics in a decomposed implicit manner. An avatar-conditioned volumetric capture method that involves a canonical normal fusion and a reconstruction network is further proposed to integrate both image observations and avatar dynamics for high-fidelity reconstruction in both observed and invisible regions. Overall, our method enables monocular human volumetric capture with detailed and pose-dependent dynamics, and the experiments show that our method outperforms state of the art",
    "volume": "main",
    "checked": true,
    "id": "3546f6db7f4cfd1a1925555ee765d5bb09c6cb23",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2116_ECCV_2022_paper.php": {
    "title": "Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers",
    "abstract": "Transformer encoder architectures have recently achieved state-of-the-art results on monocular 3D human mesh reconstruction, but they require a substantial number of parameters and expensive computations. Due to the large memory overhead and slow inference speed, it is difficult to deploy such models for practical use. In this paper, we propose a novel transformer encoder-decoder architecture for 3D human mesh reconstruction from a single image, called FastMETRO. We identify the performance bottleneck in the encoder-based transformers is caused by the token design which introduces high complexity interactions among input tokens. We disentangle the interactions via an encoder-decoder architecture, which allows our model to demand much fewer parameters and shorter inference time. In addition, we impose the prior knowledge of human body’s morphological relationship via attention masking and mesh upsampling operations, which leads to faster convergence with higher accuracy. Our FastMETRO improves the Pareto-front of accuracy and efficiency, and clearly outperforms image-based methods on Human3.6M and 3DPW. Furthermore, we validate its generalizability on FreiHAND",
    "volume": "main",
    "checked": true,
    "id": "d81052a7f94b6e732ff482eb145ca32de57e638f",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2124_ECCV_2022_paper.php": {
    "title": "GeoRefine: Self-Supervised Online Depth Refinement for Accurate Dense Mapping",
    "abstract": "We present a robust and accurate depth refinement system, named GeoRefine, for geometrically-consistent dense mapping from monocular sequences. GeoRefine consists of three modules: a hybrid SLAM module using learning-based priors, an online depth refinement module leveraging self-supervision, and a global mapping module via TSDF fusion. The proposed system is online by design and achieves great robustness and accuracy via: (i) a robustified hybrid SLAM that incorporates learning-based optical flow and/or depth; (ii) self-supervised losses that leverage SLAM outputs and enforce long-term geometric consistency; (iii) careful system design that avoids degenerate cases in online depth refinement. We extensively evaluate GeoRefine on multiple public datasets and reach as low as 5% absolute relative depth errors",
    "volume": "main",
    "checked": true,
    "id": "67969b851e9b003e9e64c7e0cea728ebed4dd687",
    "citation_count": 4
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2269_ECCV_2022_paper.php": {
    "title": "Multi-modal Masked Pre-training for Monocular Panoramic Depth Completion",
    "abstract": "In this paper, we formulate a potentially valuable panoramic depth completion (PDC) task as panoramic 3D cameras often produce 360Â° depth with missing data in complex scenes. Its goal is to recover dense panoramic depths from raw sparse ones and panoramic RGB images. To deal with the PDC task, we train a deep network that takes both depth and image as inputs for the dense panoramic depth recovery. However, it needs to face a challenging optimization problem of the network parameters due to its non-convex objective function. To address this problem, we propose a simple yet effective approach termed MÂ³PT: multi-modal masked pre-training. Specifically, during pre-training, we simultaneously cover up patches of the panoramic RGB image and sparse depth by shared random mask, then reconstruct the sparse depth in the masked regions. To our best knowledge, it is the first time that we show the effectiveness of masked pre-training in a multi-modal vision task, instead of the single-modal task resolved by masked autoencoders (MAE). Different from MAE where fine-tuning completely discards the decoder part of pre-training, there is no architectural difference between the pre-training and fine-tuning stages in our MÂ³PT as they only differ in the prediction density, which potentially makes the transfer learning more convenient and effective. Extensive experiments verify the effectiveness of MÂ³PT on three panoramic datasets. Notably, we improve the state-of-the-art baselines by averagely 29.2% in RMSE, 51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "4259c7706ad1b2f993e33d824f208ffe68313656",
    "citation_count": 3
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2449_ECCV_2022_paper.php": {
    "title": "GitNet: Geometric Prior-Based Transformation for Birds-Eye-View Segmentation",
    "abstract": "Birds-eye-view (BEV) semantic segmentation is critical for autonomous driving for its powerful spatial representation ability. It is challenging to estimate the BEV semantic maps from monocular images due to the spatial gap, since it is implicitly required to realize both the perspective-to-BEV transformation and segmentation. We present a novel two-stage Geometry PrIor-based Transformation framework named GitNet, consisting of (i) the geometry-guided pre-alignment and (ii) ray-based transformer. In the first stage, we decouple the BEV segmentation into the perspective image segmentation and geometric prior-based mapping, with explicit supervision by projecting the BEV semantic labels onto the image plane to learn visibility-aware features and learnable geometry to translate into BEV space. Second, the pre-aligned coarse BEV features are further deformed by ray-based transformers to take visibility knowledge into account. GitNet achieves the leading performance on the challenging nuScenes and Argoverse Datasets. The code will be publicly available",
    "volume": "main",
    "checked": true,
    "id": "24361f5422ac288e9ab30bfc45f3c53f1378ce57",
    "citation_count": 3
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2568_ECCV_2022_paper.php": {
    "title": "Learning Visibility for Robust Dense Human Body Estimation",
    "abstract": "Estimating 3D human pose and shape from 2D images is a crucial yet challenging task. While prior methods with model-based representations can perform reasonably well on whole-body images, they often fail when parts of the body are occluded or outside the frame. Moreover, these results usually do not faithfully capture the human silhouettes due to their limited representation power of deformable models (e.g., representing only the naked body). An alternative approach is to estimate dense vertices of a predefined template body in the image space. Such representations are effective in localizing vertices within an image but cannot handle out-of-frame body parts. In this work, we learn dense human body estimation that is robust to partial observations. We explicitly model the visibility of human joints and vertices in the x, y, and z axes separately. The visibility in x and y axes help distinguishing out-of-frame cases, and the visibility in depth axis corresponds to occlusions (either self-occlusions or occlusions by other objects). We obtain pseudo ground-truths of visibility labels from dense UV correspondences and train a neural network to predict visibility along with 3D coordinates. We show that visibility can serve as 1) an additional signal to resolve depth ordering ambiguities of self-occluded vertices and 2) a regularization term when fitting a human body model to the predictions. Extensive experiments on multiple 3D human datasets demonstrate that visibility modeling significantly improves the accuracy of human body estimation, especially for partial-body cases. Our project page with code is at: https://github.com/chhankyao/visdb",
    "volume": "main",
    "checked": true,
    "id": "d4b798f51606e6c02b1943ced70b05defbde5028",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2747_ECCV_2022_paper.php": {
    "title": "Towards High-Fidelity Single-View Holistic Reconstruction of Indoor Scenes",
    "abstract": "We present a new framework to reconstruct holistic 3D indoor scenes including both room background and indoor objects from single-view images. Existing methods can only produce 3D shapes of indoor objects with limited geometry quality because of the heavy occlusion of indoor scenes. To solve this, we propose an instance-aligned implicit function (InstPIFu) for detailed object reconstruction. Combining with instance-aligned attention module, our method is empowered to decouple mixed local features toward the occluded instances. Additionally, unlike previous methods that simply represents the room background as a 3D bounding box, depth map or a set of planes, we recover the fine geometry of the background via implicit representation. Extensive experiments on the SUN RGB-D, Pix3D, 3D-FUTURE, and 3D-FRONT datasets demonstrate that our method outperforms existing approaches in both background and foreground object reconstruction. Our code and model will be made publicly available",
    "volume": "main",
    "checked": true,
    "id": "2d85b29ee105674011ac84d3df99146fff78c436",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2786_ECCV_2022_paper.php": {
    "title": "CompNVS: Novel View Synthesis with Scene Completion",
    "abstract": "We introduce a scalable framework for novel view synthesis from RGB-D images with largely incomplete scene coverage. While generative neural approaches have demonstrated spectacular results on 2D images, they have not yet achieved similar photorealistic results in combination with scene completion where a spatial 3D scene understanding is essential. To this end, we propose a generative pipeline performing on a sparse grid-based neural scene representation to complete unobserved scene parts via a learned distribution of scenes in a 2.5D-3D-2.5D manner. We process encoded image features in 3D space with a geometry completion network and a subsequent texture inpainting network to extrapolate the missing area. Photorealistic image sequences can be finally obtained via consistency-relevant differentiable rendering. Comprehensive experiments show that the graphical outputs of our method outperform the state of the art, especially within unobserved scene parts",
    "volume": "main",
    "checked": true,
    "id": "bec9032be7105bf759a4c7aae0fec67fb3a6a0fc",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2822_ECCV_2022_paper.php": {
    "title": "SketchSampler: Sketch-Based 3D Reconstruction via View-Dependent Depth Sampling",
    "abstract": "Reconstructing a 3D shape based on a single sketch image is challenging due to the large domain gap between a sparse, irregular sketch and a regular, dense 3D shape. Existing works try to employ the global feature extracted from sketch to directly predict the 3D coordinates, but they usually suffer from losing fine details that are not faithful to the input sketch. Through analyzing the 3D-to-2D projection process, we notice that the density map that characterizes the distribution of 2D point clouds (i.e., the probability of points projected at each location of the projection plane) can be used as a proxy to facilitate the reconstruction process. To this end, we first translate a sketch via an image translation network to a more informative 2D representation that can be used to generate a density map. Next, a 3D point cloud is reconstructed via a two-stage probabilistic sampling process: first recovering the 2D points (i.e., the x and y coordinates) by sampling the density map; and then predicting the depth (i.e., the z coordinate) by sampling the depth values at the ray determined by each 2D point. Extensive experiments are conducted, and both quantitative and qualitative results show that our proposed approach significantly outperforms other baseline methods",
    "volume": "main",
    "checked": true,
    "id": "ad70b35e78058f3002975f536056f5a6d5f1545b",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2871_ECCV_2022_paper.php": {
    "title": "LocalBins: Improving Depth Estimation by Learning Local Distributions",
    "abstract": "We propose a novel architecture for depth estimation from a single image. The architecture itself is based on the popular encoder-decoder architecture that is frequently used as a starting point for all dense regression tasks. We build on AdaBins which estimates a global distribution of depth values for the input image and evolve the architecture in two ways. First, instead of predicting global depth distributions, we predict depth distributions of local neighborhoods at every pixel. Second, instead of predicting depth distributions only towards the end of the decoder, we involve all layers of the decoder. We call this new architecture LocalBins. Our results demonstrate a clear improvement over the state-of-the-art in all metrics on the NYU-Depth V2 dataset. Code and pretrained models will be made publicly available",
    "volume": "main",
    "checked": true,
    "id": "e10b065cac96fbbf0a660b569919ed59b2a30c9b",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2888_ECCV_2022_paper.php": {
    "title": "2D GANs Meet Unsupervised Single-View 3D Reconstruction",
    "abstract": "Recent research has shown that controllable image generation based on pre-trained GANs can benefit a wide range of computer vision tasks. However, less attention has been devoted to 3D vision tasks. In light of this, we propose a novel image-conditioned neural implicit field, which can leverage 2D supervisions from GAN-generated multi-view images and perform the single-view reconstruction of generic objects. Firstly, a novel offline StyleGAN-based generator is presented to generate plausible pseudo images with full control over the viewpoint. Then, we propose to utilize a neural implicit function, along with a differentiable renderer to learn 3D geometry from pseudo images with object masks and rough pose initializations. To further detect the unreliable supervisions, we introduce a novel uncertainty module to predict uncertainty maps, which remedy the negative effect of uncertain regions in pseudo images, leading to a better reconstruction performance. The effectiveness of our approach is demonstrated through superior single-view 3D reconstruction results of generic objects",
    "volume": "main",
    "checked": true,
    "id": "762d9b045fd2e6fe521720de57d8fc441fff7746",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2911_ECCV_2022_paper.php": {
    "title": "InfiniteNature-Zero: Learning Perpetual View Generation of Natural Scenes from Single Images",
    "abstract": "We present a method for learning to generate unbounded flythrough videos of natural scenes starting from a single view. This capability is learned from a collection of single photographs, without requiring camera poses or even multiple views of each scene. To achieve this, we propose a novel self-supervised view generation training paradigm where we sample and render virtual camera trajectories, including cyclic camera paths, allowing our model to learn stable view generation from a collection of single views. At test time, despite never having seen a video, our approach can take a single image and generate long camera trajectories comprised of hundreds of new views with realistic and diverse content. We compare our approach with recent state-of-the-art supervised view generation methods that require posed multi-view videos and demonstrate superior performance and synthesis quality. Our project webpage, including video results, is at infinite-nature-zero.github.io",
    "volume": "main",
    "checked": true,
    "id": "df6987f29e53e5f95aec8fe520c16ac1428986b2",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3139_ECCV_2022_paper.php": {
    "title": "Semi-Supervised Single-View 3D Reconstruction via Prototype Shape Priors",
    "abstract": "The performance of existing single-view 3D reconstruction methods heavily relies on large-scale of 3D annotations. However, such annotations are tedious and expensive to collect. Semi-supervised learning serves as an alternative way to mitigate the need for manual labels, but remains unexplored in 3D reconstruction. Inspired by the recent success of self-ensembling method in semi-supervised image classification task, we first propose SSP3D, a semi-supervised framework for 3D reconstruction. In particular, we introduce an attention-guided prototype shape prior module for guiding realistic object reconstruction. we further introduce a discriminator-guided module to incentivize better shape generation, as well as a regularizer to tolerate noisy training samples. On the ShapeNet benchmark, the proposed approach outperforms previous supervised methods by clear margins margin under various labeling ratios, ( i.e., 1%, 5%, 10% and 20%). Moreover, our approach also performs well when transferring to real-world Pix3D datasets under labeling ratios of 10%. We also demonstrate our method could transfer to novel categories with few novel supervised data. Experiments on the popular ShapeNet dataset show that our method outperforms the zero-shot baseline by over 12% and the current state-of-the-art by over 7% in the few-shot setting",
    "volume": "main",
    "checked": true,
    "id": "9f5b12757b3ae4e4d3b5bf5baf6b953ce342f6b1",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3202_ECCV_2022_paper.php": {
    "title": "Bilateral Normal Integration",
    "abstract": "This paper studies the discontinuity preservation problem in recovering a surface from its surface normal map. To model discontinuities, we introduce the assumption that the surface to be recovered is semi-smooth, i.e., the surface is one-sided differentiable (hence one-sided continuous) everywhere in the horizontal and vertical directions. Under the semi-smooth surface assumption, we propose a bilaterally weighted functional for discontinuity preserving normal integration. The key idea is to relatively weight the one-sided differentiability at each point’s two sides based on the definition of one-sided depth discontinuity. As a result, our method effectively preserves discontinuities and alleviates the under- or over-segmentation artifacts in the recovered surfaces compared to existing methods. Further, we unify the normal integration problem in the orthographic and perspective cases in a new way and show effective discontinuity preservation results in both cases",
    "volume": "main",
    "checked": true,
    "id": "e9c5390ca0f6dd25fb80a212ec96be8a58324f24",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3351_ECCV_2022_paper.php": {
    "title": "S$^2$Contact: Graph-Based Network for 3D Hand-Object Contact Estimation with Semi-Supervised Learning",
    "abstract": "Being able to reason about the physical contacts between hands and objects is crucial in understanding hand-object manipulation. However, despite the efforts in accurate 3D annotations in hand and object datasets, there still exist gaps in 3D hand and object reconstructions. Recent works leverage contact maps to refine inaccurate hand-object pose estimations and generate grasps given object models. However, they require explicit 3D supervision which is seldom available and therefore, are limited to constrained settings, e.g., where thermal cameras observe residual heat left on manipulated objects. In this paper, we propose a novel semi-supervised framework that allows us to learn contact from monocular videos. Specifically, we leverage visual and geometric consistency constraints in large-scale datasets for generating pseudo-labels in semi-supervised learning and propose an efficient graph-based network to infer contact. Our semi-supervised learning framework achieves a favourable improvement over the existing supervised learning methods trained on data with ‘limited’ annotations. Notably, our proposed model is able to achieve superior results with less than half the network parameters and memory access cost when compared with the commonly-used PointNet-based approach. We show benefits from using a contact map that rules hand-object interactions to produce more accurate reconstructions. We further demonstrate that training with pseudo-labels can extend contact map estimations to out-of-domain objects and generalise better across multiple datasets",
    "volume": "main",
    "checked": true,
    "id": "b07fd669164e69dda3d9f714809995998f5db3d8",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3498_ECCV_2022_paper.php": {
    "title": "SC-wLS: Towards Interpretable Feed-Forward Camera Re-localization",
    "abstract": "Visual re-localization aims to recover camera poses in a known environment, which is vital for applications like robotics or augmented reality. Feed-forward absolute camera pose regression methods directly output poses by a network, but suffer from low accuracy. Meanwhile, scene coordinate based methods are accurate, but need iterative RANSAC post-processing, which brings challenges to efficient end-to-end training and inference. In order to have the best of both worlds, we propose a feed-forward method termed SC-wLS that exploits all scene coordinate estimates for weighted least squares pose regression. This differentiable formulation exploits a weight network imposed on 2D-3D correspondences, and requires pose supervision only. Qualitative results demonstrate the interpretability of learned weights. Evaluations on 7Scenes and Cambridge datasets show significantly promoted performance when compared with former feed-forward counterparts. Moreover, our SC-wLS method enables a new capability: self-supervised test-time adaptation on the weight network. Codes and models are publicly available",
    "volume": "main",
    "checked": true,
    "id": "a058da4e3b06e201bab4d91992e7ef326fcc787f",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3503_ECCV_2022_paper.php": {
    "title": "FloatingFusion: Depth from ToF and Image-Stabilized Stereo Cameras",
    "abstract": "High-accuracy per-pixel depth is vital for computational photography, so smartphones now have multimodal camera systems with time-of-flight (ToF) depth sensors and multiple color cameras. However, producing accurate high-resolution depth is still challenging due to the low resolution and limited active illumination power of ToF sensors. Fusing RGB stereo and ToF information is a promising direction to overcome these issues, but a key problem remains: to provide high-quality 2D RGB images, the main smartphone color sensor’s lens is optically stabilized, resulting in an unknown pose for the floating lens that breaks the geometric relationships between the multimodal image sensors. Leveraging ToF depth estimates and a wide-angle RGB camera, we design an automatic calibration technique based on dense 2D/3D matching that can estimate camera pose intrinsic and distortion parameters of a stabilized main RGB sensor from a single snapshot. This lets us fuse stereo and ToF cues via a correlation volume. For fusion, we apply deep learning via a real-world training dataset with depth supervision estimated by a neural reconstruction method. For evaluation, we acquire a test dataset using a commercial high-power depth camera and show that our approach achieves higher accuracy than existing baselines",
    "volume": "main",
    "checked": true,
    "id": "f0b43d6b734d5e42c766076a6489b6122ee0d3b8",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3514_ECCV_2022_paper.php": {
    "title": "DELTAR: Depth Estimation from a Light-Weight ToF Sensor and RGB Image",
    "abstract": "Light-weight time-of-flight (ToF) depth sensors are small, cheap, low-energy and have been massively deployed on mobile devices for the purposes like autofocus, obstacle detection, etc. However, due to their specific measurements (depth distribution in a region instead of the depth value at a certain pixel) and extremely low resolution, they are insufficient for applications requiring high-fidelity depth such as 3D reconstruction. In this paper, we propose DELTAR, a novel method to empower light-weight ToF sensors with the capability of measuring high resolution and accurate depth by cooperating with a color image. As the core of DELTAR, a feature extractor customized for depth distribution and an attention-based neural architecture is proposed to fuse the information from the color and ToF domain efficiently. To evaluate our system in real-world scenarios, we design a data collection device and propose a new approach to calibrate the RGB camera and ToF sensor. Experiments show that our method produces more accurate depth than existing frameworks designed for depth completion and depth super-resolution and achieves on par performance with a commodity-level RGB-D sensor. Code and data are available on the project webpage: https://zju3dv.github.io/deltar",
    "volume": "main",
    "checked": true,
    "id": "02f0afe1f456811c4b59ca90e211a6c320e9cc8a",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3606_ECCV_2022_paper.php": {
    "title": "3D Room Layout Estimation from a Cubemap of Panorama Image via Deep Manhattan Hough Transform",
    "abstract": "Significant geometric structures can be compactly described by global wireframes in the estimation of 3D room layout from a single panoramic image. Based on this observation, we present an alternative approach to estimate the walls in 3D space by modeling long-range geometric patterns in a learnable Hough Transform block. We transform the image feature from a cubemap tile to the Hough space of a Manhattan world and directly map the feature to the geometric output. The convolutional layers not only learn the local gradient-like line features, but also utilize the global information to successfully predict occluded walls with a simple network structure. Unlike most previous work, the predictions are performed individually on each cubemap tile, and then assembled to get the layout estimation. Experimental results show that we achieve comparable results with recent state-of-the-art in prediction accuracy and performance. Code is available at https://github.com/Starrah/DMH-Net",
    "volume": "main",
    "checked": true,
    "id": "63df46ace36c030fa45c64b0d219df8ba8d2692c",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3809_ECCV_2022_paper.php": {
    "title": "RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation",
    "abstract": "Category-level object pose estimation aims to predict the 6D pose as well as the 3D metric size of previously unseen objects from a known set of categories. Recent methods harness shape prior adaptation to map the observed point cloud into the canonical space and apply Umeyama’s algorithm to recover the pose and size. However, their shape prior integration strategy boosts pose estimation indirectly, which leads to insufficient pose-sensitive feature extraction and slow inference speed. To tackle this problem, in this paper, we propose a novel geometry-guided Residual Object Bounding Box Projection network RBP-Pose that jointly predicts object pose and residual vectors describing the displacements from the shape-prior-indicated object surface projections on the bounding box towards real surface projections. Such definition of residual vectors is inherently zero-mean and relatively small, and explicitly encapsulates spatial cues of the 3D object for robust and accurate pose regression. We enforce geometry-aware consistency terms to align the predicted pose and residual vectors to further boost performance. Finally, to avoid overfitting and enhance the generalization ability of RBP-Pose, we propose an online non-linear shape augmentation scheme to promote shape diversity during training. Extensive experiments on NOCS datasets demonstrate that RBP-Pose surpasses all existing methods by a large margin, whilst achieving a real-time inference speed",
    "volume": "main",
    "checked": true,
    "id": "675c22a877b785a77b0bde5076dd1cb81b1f4d6b",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3999_ECCV_2022_paper.php": {
    "title": "Monocular 3D Object Reconstruction with GAN Inversion",
    "abstract": "Recovering a textured 3D mesh from a monocular image is highly challenging, particularly for in-the-wild objects that lack 3D ground truths. In this work, we present MeshInversion, a novel framework to improve the reconstruction by exploiting the generative prior of a 3D GAN pre-trained for 3D textured mesh synthesis. Reconstruction is achieved by searching for a latent space in the 3D GAN that best resembles the target mesh in accordance with the single view observation. Since the pre-trained GAN encapsulates rich 3D semantics in terms of mesh geometry and texture, searching within the GAN manifold thus naturally regularizes the realness and fidelity of the reconstruction. Importantly, such regularization is directly applied in the 3D space, providing crucial guidance of mesh parts that are unobserved in the 2D space. Experiments on standard benchmarks show that our framework obtains faithful 3D reconstructions with consistent geometry and texture across both observed and unobserved parts. Moreover, it generalizes well to meshes that are less commonly seen, such as the extended articulation of deformable objects. Code is released at https://github.com/junzhezhang/mesh-inversion",
    "volume": "main",
    "checked": true,
    "id": "3bcf42780f142e0c5d7efc6741018a34ed161c3c",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4029_ECCV_2022_paper.php": {
    "title": "Map-Free Visual Relocalization: Metric Pose Relative to a Single Image",
    "abstract": "Can we relocalize in a scene represented by a single reference image? Standard visual relocalization requires hundreds of images and scale calibration to build a scene-specific 3D map. In contrast, we propose Map-free Relocalization, i.e., using only one photo of a scene to enable instant, metric scaled relocalization. Existing datasets are not suitable to benchmark map-free relocalization, due to their focus on large scenes or their limited variability. Thus, we have constructed a new dataset of 655 small places of interest, such as sculptures, murals and fountains, collected worldwide. Each place comes with a reference image to serve as a relocalization anchor, and dozens of query images with known, metric camera poses. The dataset features changing conditions, stark viewpoint changes, high variability across places, and queries with low to no visual overlap with the reference image.We identify two viable families of existing methods to provide baseline results: relative pose regression, and feature matching combined with single-image depth prediction. While these methods show reasonable performance on some favorable scenes in our dataset, map-free relocalization proves to be a challenge that requires new, innovative solutions",
    "volume": "main",
    "checked": true,
    "id": "5de2d5f0db4f6a869ccfb8a3a497cfe4c719a18a",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4073_ECCV_2022_paper.php": {
    "title": "Self-Distilled Feature Aggregation for Self-Supervised Monocular Depth Estimation",
    "abstract": "Self-supervised monocular depth estimation has received much attention recently in computer vision. Most of the existing works in literature aggregate multi-scale features for depth prediction via either straightforward concatenation or element-wise addition, however, such feature aggregation operations generally neglect the contextual consistency between multi-scale features. Addressing this problem, we propose the Self-Distilled Feature Aggregation (SDFA) module for simultaneously aggregating a pair of low-scale and high-scale features and maintaining their contextual consistency. The SDFA employs three branches to learn three feature offset maps respectively: one offset map for refining the input low-scale feature and the other two for refining the input high-scale feature under a designed self-distillation manner. Then, we propose an SDFA-based network for self-supervised monocular depth estimation, and design a self-distilled training strategy to train the proposed network with the SDFA module. Experimental results on the KITTI dataset demonstrate that the proposed method outperforms the comparative state-of-the-art methods in most cases. The code is available at https://github.com/ZM-Zhou/SDFA-Net_pytorch",
    "volume": "main",
    "checked": true,
    "id": "079bfc68b7d6ce57f49719584d842a2746371c90",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4231_ECCV_2022_paper.php": {
    "title": "Planes vs. Chairs: Category-Guided 3D Shape Learning without Any 3D Cues",
    "abstract": "We present a novel 3D shape reconstruction method which learns to predict an implicit 3D shape representation from a single RGB image. Our approach uses a set of single-view images of multiple object categories without viewpoint annotation, forcing the model to learn across multiple object categories without 3D supervision. To facilitate learning with such minimal supervision, we use category labels to guide shape learning with a novel categorical metric learning approach. We also utilize adversarial and viewpoint regularization techniques to further disentangle the effects of viewpoint and shape. We obtain the first results for large-scale (more than 50 categories) single-viewpoint shape prediction using a single model. We are also the first to examine and quantify the benefit of class information in single-view supervised 3D shape reconstruction. Our method achieves superior performance over state-of-the-art methods on ShapeNet-13, ShapeNet-55 and Pascal3D+",
    "volume": "main",
    "checked": true,
    "id": "bf6e79f528f94ae15c10b1d79d7b3e89670c7213",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4241_ECCV_2022_paper.php": {
    "title": "MHR-Net: Multiple-Hypothesis Reconstruction of Non-rigid Shapes from 2D Views",
    "abstract": "We propose MHR-Net, a novel method for recovering Non-Rigid Shapes from Motion (NRSfM). MHR-Net aims to find a set of reasonable reconstructions for a 2D view, and it also selects the most likely reconstruction from the set. To deal with the challenging unsupervised generation of non-rigid shapes, we develop a new Deterministic Basis and Stochastic Deformation scheme in MHR-Net. The non-rigid shape is first expressed as the sum of a coarse shape basis and a flexible shape deformation, then multiple hypotheses are generated with uncertainty modeling of the deformation part. MHR-Net is optimized with reprojection loss on the basis and the best hypothesis. Furthermore, we design a new Procrustean Residual Loss, which reduces the rigid rotations between similar shapes and further improves the performance. Experiments show that MHR-Net achieves state-of-the-art reconstruction accuracy on Human3.6M, SURREAL and 300-VW datasets",
    "volume": "main",
    "checked": true,
    "id": "050dedbeeb1af878389cf57e44e47c166f737211",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4247_ECCV_2022_paper.php": {
    "title": "Depth Map Decomposition for Monocular Depth Estimation",
    "abstract": "We propose a novel algorithm for monocular depth estimation that decomposes a metric depth map into a normalized depth map and scale features. The proposed network is composed of a shared encoder and three decoders, called G-Net, N-Net, and M-Net, which estimate gradient maps, a normalized depth map, and a metric depth map, respectively. M-Net learns to estimate metric depths more accurately using relative depth features extracted by G-Net and N-Net. The proposed algorithm has the advantage that it can use datasets without metric depth labels to improve the performance of metric depth estimation. Experimental results on various datasets demonstrate that the proposed algorithm not only provides competitive performance to state-of-the-art algorithms but also yields acceptable results even when only a small amount of metric depth data is available for its training",
    "volume": "main",
    "checked": true,
    "id": "43df0f83ab390814690048741c0b9dd64dfa616e",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4288_ECCV_2022_paper.php": {
    "title": "Monitored Distillation for Positive Congruent Depth Completion",
    "abstract": "We propose a method to infer a dense depth map from a single image, its calibration, and the associated sparse point cloud. In order to leverage existing models (teachers) that produce putative depth maps, we propose an adaptive knowledge distillation approach that yields a positive congruent training process, wherein a student model avoids learning the error modes of the teachers. In the absence of ground truth for model selection and training, our method, termed Monitored Distillation, allows a student to exploit a blind ensemble of teachers by selectively learning from predictions that best minimize the reconstruction error for a given image. Monitored Distillation yields a distilled depth map and a confidence map, or “monitor\"\"\"\", for how well a prediction from a particular teacher fits the observed image. The monitor adaptively weights the distilled depth where if all of the teachers exhibit high residuals, the standard unsupervised image reconstruction loss takes over as the supervisory signal. On indoor scenes (VOID), we outperform blind ensembling baselines by 17.53% and unsupervised methods by 24.25%; we boast a 79% model size reduction while maintaining comparable performance to the best supervised method. For outdoors (KITTI), we tie for 5th overall on the benchmark despite not using ground truth. Code available at: https://github.com/alexklwong/mondi-python",
    "volume": "main",
    "checked": true,
    "id": "3a7cde2f64e5937b5ed2a2e1899b02b59504bbd0",
    "citation_count": 3
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4326_ECCV_2022_paper.php": {
    "title": "Resolution-Free Point Cloud Sampling Network with Data Distillation",
    "abstract": "Down-sampling algorithms are adopted to simplify the point clouds and save the computation cost on subsequent tasks. Existing learning-based sampling methods often need to train a big sampling network to support sampling under different resolutions, which must generate sampled points with the costly maximum resolution even if only low-resolution points need to be sampled. In this work, we propose a novel resolution-free point clouds sampling network to directly sample the original point cloud to different resolutions, which is conducted by optimizing non-learning-based initial sampled points to better positions. Besides, we introduce data distillation to assist the training process by considering the differences between task network outputs from original point clouds and sampled points. Experiments on point cloud reconstruction and recognition tasks demonstrate that our method can achieve SOTA performances with lower time and memory cost than existing learning-based sampling strategies. Codes are available at https://github.com/Tianxinhuang/PCDNet",
    "volume": "main",
    "checked": true,
    "id": "c1046d98a33b82d0a5f498730e59a2af68c69903",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4720_ECCV_2022_paper.php": {
    "title": "Organic Priors in Non-rigid Structure from Motion",
    "abstract": "This paper advocates the use of organic priors in classical non-rigid structure from motion (NRSfM). By organic priors, we mean invaluable intermediate prior information intrinsic to the NRSfM matrix factorization theory. It is shown that such priors reside in the factorized matrices, and quite surprisingly, existing methods generally disregard them. The paper’s main contribution is to put forward a simple, methodical, and practical method that can effectively exploit such organic priors to solve NRSfM. The proposed method does not make assumptions other than the popular one on the low-rank shape and offers a reliable solution to NRSfM under orthographic projection. Our work reveals that the accessibility of organic priors is independent of the camera motion and shape deformation type. Besides that, the paper provides insights into the NRSfM factorization---both in terms of shape and motion---and is the first approach to show the benefit of single rotation averaging for NRSfM. Furthermore, we outline how to effectively recover motion and non-rigid 3D shape using the proposed organic prior based approach and demonstrate results that outperform prior-free NRSfM performance by a significant margin. Finally, we present the benefits of our method via extensive experiments and evaluations on several benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "11b395c8fb0f58c5057cb4187d15c98923b627f9",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4807_ECCV_2022_paper.php": {
    "title": "Perspective Flow Aggregation for Data-Limited 6D Object Pose Estimation",
    "abstract": "Most recent 6D object pose estimation methods, including unsupervised ones, require many real training images. Unfortunately, for some applications, such as those in space or deep under water, acquiring real images, even unannotated, is virtually impossible. In this paper, we propose a method that can be trained solely on synthetic images, or optionally using a few additional real ones. Given a rough pose estimate obtained from a first network, it uses a second network to predict a dense 2D correspondence field between the image rendered using the rough pose and the real image and infers the required pose correction. This approach is much less sensitive to the domain shift between synthetic and real images than state-of-the-art methods. It performs on par with methods that require annotated real images for training when not using any, and outperforms them considerably when using as few as twenty real images",
    "volume": "main",
    "checked": true,
    "id": "75d3671ea8fc6af70359f4cc4cad6471470df6b1",
    "citation_count": 2
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4883_ECCV_2022_paper.php": {
    "title": "DANBO: Disentangled Articulated Neural Body Representations via Graph Neural Networks",
    "abstract": "Deep learning greatly improved the realism of animatable human models by learning geometry and appearance from collections of 3D scans, template meshes, and multi-view imagery. High-resolution models enable photo-realistic avatars but at the cost of requiring studio settings not available to end users. Our goal is to create avatars directly from raw images without relying on expensive studio setups and surface tracking. While a few such approaches exist, those have limited generalization capabilities and are prone to learning spurious (chance) correlations between irrelevant body parts, resulting in implausible deformations and missing body parts on unseen poses. We introduce a three-stage method that induces two inductive biases to better disentangled pose-dependent deformation. First, we model correlations of body parts explicitly with a graph neural network. Second, to further reduce the effect of chance correlations, we introduce localized per-bone features that use a factorized volumetric representation and a new aggregation function. We demonstrate that our model produces realistic body shapes under challenging unseen poses and shows high-quality image synthesis. Our proposed representation strikes a better trade-off between model capacity, expressiveness, and robustness than competing methods. Project website: https://lemonatsu.github.io/danbo",
    "volume": "main",
    "checked": true,
    "id": "48c73c60c84f863e27d00dde04ced658e7449196",
    "citation_count": 2
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4894_ECCV_2022_paper.php": {
    "title": "CHORE: Contact, Human and Object REconstruction from a Single RGB Image",
    "abstract": "Most prior works in perceiving 3D humans from images reason human in isolation without their surroundings. However, humans are constantly interacting with the surrounding objects, thus calling for models that can reason about not only the human but also the object and their interaction. The problem is extremely challenging due to heavy occlusions between humans and objects, diverse interaction types and depth ambiguity. In this paper, we introduce CHORE, a novel method that learns to jointly reconstruct the human and the object from a single RGB image. CHORE takes inspiration from recent advances in implicit surface learning and classical model-based fitting. We compute a neural reconstruction of human and object represented implicitly with two unsigned distance fields, a correspondence field to a parametric body and an object pose field. This allows us to robustly fit a parametric body model and a 3D object template, while reasoning about interactions. Furthermore, prior pixel-aligned implicit learning methods use synthetic data and make assumptions that are not met in the real data. We propose a elegant depth-aware scaling that allows more efficient shape learning on real data. Experiments show that our joint reconstruction learned with the proposed strategy significantly outperforms the SOTA. Our code and models are available at https://virtualhumans.mpi-inf.mpg.de/chore",
    "volume": "main",
    "checked": true,
    "id": "6c01c2a74a1e98dc7bfa18553b5c786b9fc9342f",
    "citation_count": 6
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4918_ECCV_2022_paper.php": {
    "title": "Learned Vertex Descent: A New Direction for 3D Human Model Fitting",
    "abstract": "We propose a novel optimization-based paradigm for 3D human shape fitting on images. In contrast to existing approaches that directly regress the parameters of a low-dimensional statistical body model (e.g. SMPL) from input images, we propose training a deep network that, given solely image features and an unfit mesh, predicts the directions of the vertices towards the 3D body mesh. At inference, we employ this network, dubbed LVD, within a gradient-descent optimization pipeline until its convergence, which typically occurs in a fraction of a second even when initializing all vertices into a single point. An exhaustive evaluation demonstrates that our approach is able to capture the underlying body of clothed people with very different body shapes, achieving a significant improvement compared to state-of-the-art. Additionally, the proposed formulation can generalize to other sources of input data, which we experimentally show on fitting 3D scans of full bodies and hands",
    "volume": "main",
    "checked": true,
    "id": "f43d2710dfe14bd884eea6eda6997fc81f1436c4",
    "citation_count": 2
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5007_ECCV_2022_paper.php": {
    "title": "Self-Calibrating Photometric Stereo by Neural Inverse Rendering",
    "abstract": "This paper tackles the task of uncalibrated photometric stereo for 3D object reconstruction, where both the object shape, object reflectance, and lighting directions are unknown. This is an extremely difficult task, and the challenge is further compounded with the existence of the well-known generalized bas-relief (GBR) ambiguity in photometric stereo. Previous methods to resolve this ambiguity either rely on an overly simplified reflectance model, or assume special light distribution. We propose a new method that jointly optimizes object shape, light directions, and light intensities, all under general surfaces and lights assumptions. The specularities are used explicitly to resolve the GBR ambiguity via a neural inverse rendering process. We gradually fit specularities from shiny to rough using novel progressive specular bases. Our method leverages a physically based rendering equation by minimizing the reconstruction error on a per-object-basis. Our method demonstrates state-of-the-art accuracy in light estimation and shape recovery on real-world datasets",
    "volume": "main",
    "checked": true,
    "id": "87653b4c81db28256f6d88e0069ba8dc123e21f8",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5036_ECCV_2022_paper.php": {
    "title": "3D Clothed Human Reconstruction in the Wild",
    "abstract": "Although much progress has been made in 3D clothed human reconstruction, most of the existing methods fail to produce robust results from in-the-wild images, which contain diverse human poses and appearances. This is mainly due to the large domain gap between training datasets and in-the-wild datasets. The training datasets are usually synthetic ones, which contain rendered images from GT 3D scans. However, such datasets contain simple human poses and less natural image appearances compared to those of real in-the-wild datasets, which makes generalization of it to in-the-wild images extremely challenging. To resolve this issue, in this work, we propose ClothWild, a 3D clothed human reconstruction framework that firstly addresses the robustness on in-thewild images. First, for the robustness to the domain gap, we propose a weakly supervised pipeline that is trainable with 2D supervision targets of in-the-wild datasets. Second, we design a DensePose-based loss function to reduce ambiguities of the weak supervision. Extensive empirical tests on several public in-the-wild datasets demonstrate that our proposed ClothWild produces much more accurate and robust results than the state-of-the-art methods. The codes are available in here: https://github.com/hygenie1228/ClothWild_RELEASE",
    "volume": "main",
    "checked": true,
    "id": "fbd060ab36e8781609b0986c256d3f9612870ce9",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5180_ECCV_2022_paper.php": {
    "title": "Directed Ray Distance Functions for 3D Scene Reconstruction",
    "abstract": "We present an approach for full 3D scene reconstruction from a single new image that can be trained on realistic non-watertight scans. Our approach uses a predicted distance function, since these have shown promise in handling complex topologies and large spaces. We identify and analyze two key challenges for predicting these implicit functions from an image that have prevented their success on 3D scenes from a single image. First, we show that predicting a conventional scene distance from an image requires reasoning over a large receptive field. Second, we analytically show that the optimal output of a network that predicts these distance functions is often not a distance function. We propose an alternate approach, the Direct Ray Distance Function (DRDF), that avoids both challenges. We show that a deep network trained to predict DRDFs outperforms all other methods quantitatively and qualitatively on 3D reconstruction on Matterport3D, 3DFront, and ScanNet",
    "volume": "main",
    "checked": true,
    "id": "511ad5e192dd7699a7e95e198ce59f338d129dd2",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5287_ECCV_2022_paper.php": {
    "title": "Object Level Depth Reconstruction for Category Level 6D Object Pose Estimation from Monocular RGB Image",
    "abstract": "Recently, RGBD-based category-level 6D object pose estimation has achieved promising improvement in performance, however, the requirement of depth information prohibits broader applications. In order to relieve this problem, this paper proposes a novel approach named Object Level Depth reconstruction Network (OLD-Net) taking only RGB images as input for category-level 6D object pose estimation. We propose to directly predict object-level depth from a monocular RGB image by deforming the category-level shape prior into object-level depth and the canonical NOCS representation. Two novel modules named Normalized Global Position Hints (NGPH) and Shape-aware Decoupled Depth Reconstruction (SDDR) module are introduced to learn high fidelity object-level depth and delicate shape representations. At last, the 6D object pose is solved by aligning the predicted canonical representation with the back-projected object-level depth. Extensive experiments on the challenging CAMERA25 and REAL275 datasets indicate that our model, though simple, achieves state-of-the-art performance",
    "volume": "main",
    "checked": true,
    "id": "278e196c155f3b4f35fac473685191695d15366f",
    "citation_count": 2
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5351_ECCV_2022_paper.php": {
    "title": "Uncertainty Quantification in Depth Estimation via Constrained Ordinal Regression",
    "abstract": "Monocular Depth Estimation (MDE) is a task to predict a dense depth map from a single image. Despite the recent progress brought by deep learning, existing methods are still prone to errors due to the ill-posed nature of MDE. Hence depth estimation systems must be self-aware of possible mistakes to avoid disastrous consequences. This paper provides an uncertainty quantification method for supervised MDE models. From a frequentist view, we capture the uncertainty by predictive variance that consists of two terms: error variance and estimation variance. The former represents the noise of a depth value, and the latter measures the randomness in the depth regression model due to training on finite data. To estimate error variance, we perform constrained ordinal regression (ConOR) on discretized depth to estimate the conditional distribution of depth given image, and then compute the corresponding conditional mean and variance as the predicted depth and error variance estimator, respectively. Our work also leverages bootstrapping methods to infer estimation variance from re-sampled data. We perform experiments on both simulated and real data to validate the effectiveness of the proposed method. The results show that our approach produces accurate uncertainty estimates while maintaining high depth prediction accuracy",
    "volume": "main",
    "checked": true,
    "id": "42c0df4e2f3e9b089ffa5476e0a0256530e9961b",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5688_ECCV_2022_paper.php": {
    "title": "CostDCNet: Cost Volume Based Depth Completion for a Single RGB-D Image",
    "abstract": "Successful depth completion from a single RGB-D image requires both extracting plentiful 2D and 3D features and merging these heterogeneous features appropriately. We propose a novel depth completion framework, CostDCNet, based on the cost volume-based depth estimation approach that has been successfully employed for multi-view stereo (MVS). The key to high-quality depth map estimation in the approach is constructing an accurate cost volume. To produce a quality cost volume tailored to single-view depth completion, we present a simple but effective architecture that can fully exploit the 3D information, three options to make an RGB-D feature volume, and per-plane pixel shuffle for efficient volume upsampling. Our CostDCNet framework consists of lightweight deep neural networks ( 1.8M parameters), running in real time ( 30ms). Nevertheless, thanks to our simple but effective design, CostDCNet demonstrates depth completion results comparable to or better than the state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "f6cafadb66ac71765c12e00787c0ecf2a1669b63",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5770_ECCV_2022_paper.php": {
    "title": "ShAPO: Implicit Representations for Multi-Object Shape, Appearance, and Pose Optimization",
    "abstract": "Our method studies the complex task of object-centric 3D understanding from a single RGB-D observation. As it is an ill-posed problem, existing methods suffer from low performance for both 3D shape and 6D pose and size estimation in complex multi-object scenarios with occlusions. We present ShAPO, a method for joint multi-object detection, 3D textured reconstruction, 6D object pose and size estimation. Key to ShAPO is a single-shot pipeline to regress shape, appearance and pose latent codes along with the masks of each object instance, which is then further refined in a sparse-to-dense fashion. A novel disentangled shape and appearance database of priors is first learned to embed objects in their respective shape and appearance space. We also propose a novel, octree-based differentiable optimization step, allowing us to further improve object shape, pose and appearance simultaneously under the learned latent space, in an analysis-by synthesis fashion. Our novel joint implicit textured object representation allows us to accurately identify and reconstruct novel unseen objects without having access to their 3D meshes. Through extensive experiments, we show that our method, trained on simulated indoor scenes, accurately regresses the shape, appearance and pose of novel objects in the real-world with minimal fine-tuning. Our method significantly out-performs all baselines on the NOCS dataset with an 8% absolute improvement in mAP for 6D pose estimation",
    "volume": "main",
    "checked": true,
    "id": "0fdf57ff32ea5fc52d80c81105ba759c61789787",
    "citation_count": 2
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5829_ECCV_2022_paper.php": {
    "title": "3D Siamese Transformer Network for Single Object Tracking on Point Clouds",
    "abstract": "Siamese network based trackers formulate 3D single object tracking as cross-correlation learning between point features of a template and a search area. Due to the large appearance variation between the template and search area during tracking, how to learn the robust cross correlation between them for identifying the potential target in the search area is still a challenging problem. In this paper, we explicitly use Transformer to form a 3D Siamese Transformer network for learning robust cross correlation between the template and the search area of point clouds. Specifically, we develop a Siamese point Transformer network to learn shape context information of the target. Its encoder uses self-attention to capture non-local information of point clouds to characterize the shape information of the object, and the decoder utilizes cross-attention to upsample discriminative point features. After that, we develop an iterative coarse-to-fine correlation network to learn the robust cross correlation between the template and the search area. It formulates the cross-feature augmentation to associate the template with the potential target in the search area via cross attention. To further enhance the potential target, it employs the ego-feature augmentation that applies self-attention to the local k-NN graph of the feature space to aggregate target features. Experiments on the KITTI, nuScenes, and Waymo datasets show that our method achieves state-of-the-art performance on the 3D single object tracking task",
    "volume": "main",
    "checked": true,
    "id": "2d17afe7f7a84e6ac2d2b8bed3a4bdebba6d1a18",
    "citation_count": 2
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5901_ECCV_2022_paper.php": {
    "title": "Object Wake-Up: 3D Object Rigging from a Single Image",
    "abstract": "Given a single chair image, could we wake it up by reconstructing its 3D shape and skeleton, as well as animating its plausible articulations and motions, similar to that of human modeling? It is a new problem that not only goes beyond image-based object reconstruction but also involves articulated animation of generic objects in 3D, which could give rise to numerous downstream augmented and virtual reality applications. In this paper, we propose an automated approach to tackle the entire process of reconstruct such generic 3D objects, rigging and animation, all from single images. A two-stage pipeline has thus been proposed, which specifically contains a multi-head structure to utilize the deep implicit functions for skeleton prediction. Two in-house 3D datasets of generic objects with high-fidelity rendering and annotated skeletons have also been constructed. Empirically our approach demonstrated promising results; when evaluated on the related sub-tasks of 3D reconstruction and skeleton prediction, our results surpass those of the state-of-the-arts by a noticeable margin. Our code and datasets are made publicly available at the dedicated project website",
    "volume": "main",
    "checked": true,
    "id": "cde49fa490a766f062d3d658385d376280d036e0",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5915_ECCV_2022_paper.php": {
    "title": "IntegratedPIFu: Integrated Pixel Aligned Implicit Function for Single-View Human Reconstruction",
    "abstract": "We propose IntegratedPIFu, a new pixel-aligned implicit model that builds on the foundation set by PIFuHD. IntegratedPIFu shows how depth and human parsing information can be predicted and capitalized upon in a pixel-aligned implicit model. In addition, IntegratedPIFu introduces depth-oriented sampling, a novel training scheme that improve any pixel-aligned implicit model’s ability to reconstruct important human features without noisy artefacts. Lastly, IntegratedPIFu presents a new architecture that, despite using less model parameters than PIFuHD, is able to improves the structural correctness of reconstructed meshes. Our results show that IntegratedPIFu significantly outperforms existing state-of-the-arts methods on single-view human reconstruction. We provide the code in our supplementary materials. Our code is available at https://github.com/kcyt/IntegratedPIFu",
    "volume": "main",
    "checked": true,
    "id": "24b9ceb67b48c5d85658031342cc1431bc4d4b92",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6023_ECCV_2022_paper.php": {
    "title": "Realistic One-Shot Mesh-Based Head Avatars",
    "abstract": "We present a system for the creation of realistic one-shot mesh-based (ROME) human head avatars. From a single photograph, our system estimates the head mesh (with person-specific details in both the facial and non-facial head parts) as well as the neural texture encoding local photometric and geometric details. The resulting avatars are rigged and can be rendered using a deep rendering network, which is trained alongside the mesh and texture estimators on a dataset of in-the-wild videos. In the experiments, we observe that our system performs competitively both in terms of head geometry recovery and the quality of renders, especially for strong pose and expression changes",
    "volume": "main",
    "checked": true,
    "id": "62338f4697f2cdc35d7b19fe4893256dc61c8cd8",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6090_ECCV_2022_paper.php": {
    "title": "A Kendall Shape Space Approach to 3D Shape Estimation from 2D Landmarks",
    "abstract": "3D shapes provide substantially more information than 2D images. However, the acquisition of 3D shapes is sometimes very difficult or even impossible in comparison with acquiring 2D images, making it necessary to derive the 3D shape from 2D images. Although this is, in general, a mathematically ill-posed problem, it might be solved by constraining the problem formulation using prior information. Here, we present a new approach based on Kendall’s shape space to reconstruct 3D shapes from single monocular 2D images. The work is motivated by an application to study the feeding behavior of the basking shark, an endangered species whose massive size and mobility render 3D shape data nearly impossible to obtain, hampering understanding of their feeding behaviors and ecology. 2D images of these animals in feeding position, however, are readily available. We compare our approach with state-of-the-art shape-based approaches, both on human stick models and on shark head skeletons. Using a small set of training shapes, we show that the Kendall shape space approach is substantially more robust than previous methods and results in plausible shapes. This is essential for the motivating application in which specimens are rare and therefore only few training shapes are available",
    "volume": "main",
    "checked": true,
    "id": "b432f630e4d7860869a3103d5cf5bb8ea1f10ff6",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6516_ECCV_2022_paper.php": {
    "title": "Neural Light Field Estimation for Street Scenes with Differentiable Virtual Object Insertion",
    "abstract": "We consider the challenging problem of outdoor lighting estimation for the goal of photorealistic virtual object insertion into photographs. Existing works on outdoor lighting estimation typically simplify the scene lighting into an environment map which cannot capture the spatially-varying lighting effects in outdoor scenes. In this work, we propose a neural approach that estimates the 5D HDR light field from a single image, and a differentiable object insertion formulation that enables end-to-end training with image-based losses that encourage realism. Specifically, we design a hybrid lighting representation tailored to outdoor scenes, which contains an HDR sky dome that handles the extreme intensity of the sun, and a volumetric lighting representation that models the spatially-varying appearance of the surrounding scene. With the estimated lighting, our shadow-aware object insertion is fully differentiable, which enables adversarial training over the composited image to provide additional supervisory signal to the lighting prediction. We experimentally demonstrate that our hybrid lighting representation is more performant than existing outdoor lighting estimation methods. We further show the benefits of our AR object insertion in an autonomous driving application, where we obtain performance gains for a 3D object detector when trained on our augmented data",
    "volume": "main",
    "checked": true,
    "id": "3781830e2c55f3fba6d4745f4cb673b71b7a22b4",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6667_ECCV_2022_paper.php": {
    "title": "Perspective Phase Angle Model for Polarimetric 3D Reconstruction",
    "abstract": "Current polarimetric 3D reconstruction methods, including those in the well-established shape from polarization literature, are all developed under the orthographic projection assumption. In the case of a large field of view, however, this assumption does not hold and may result in significant reconstruction errors in methods that make this assumption. To address this problem, we present the perspective phase angle (PPA) model that is applicable to perspective cameras. Compared with the orthographic model, the proposed PPA model accurately describes the relationship between polarization phase angle and surface normal under perspective projection. In addition, the PPA model makes it possible to estimate surface normals from only one single-view phase angle map and does not suffer from the so-called Ï€-ambiguity problem. Experiments on real data show that the PPA model is more accurate for surface normal estimation with a perspective camera than the orthographic model",
    "volume": "main",
    "checked": true,
    "id": "86c0d17790501d0c65a1b5f6c1afe0c8f7e15e95",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7476_ECCV_2022_paper.php": {
    "title": "DeepShadow: Neural Shape from Shadow",
    "abstract": "This paper presents ‘DeepShadow’, a one-shot method for recovering the depth map and surface normals from photometric stereo shadow maps. Previous works that try to recover the surface normals from photometric stereo images treat cast shadows as a disturbance. We show that the self and cast shadows not only do not disturb 3D reconstruction, but can be used alone, as a strong learning signal, to recover the depth map and surface normals. We demonstrate that 3D reconstruction from shadows can even outperform shape-from-shading in certain cases. To the best of our knowledge, our method is the first to reconstruct 3D shape-from-shadows using neural networks. The method does not require any pre-training or expensive labeled data, and is optimized during inference time",
    "volume": "main",
    "checked": true,
    "id": "8f53d33f8305fdcf1769eb124e80a1d24629f362",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7605_ECCV_2022_paper.php": {
    "title": "Camera Auto-Calibration from the Steiner Conic of the Fundamental Matrix",
    "abstract": "This paper addresses the problem of camera auto-calibration from the fundamental matrix under general motion. The fundamental matrix can be decomposed into a symmetric part (a Steiner conic) and a skew-symmetric part (a fixed point), which we find useful for fully calibrating camera parameters. We first obtain a fixed line from the image of the symmetric, skew-symmetric parts of the fundamental matrix and the image of the absolute conic. Then the properties of this fixed line are presented and proved, from which new constraints on general eigenvectors between the Steiner conic and the image of the absolute conic are derived. We thus propose a method to fully calibrate the camera. First, the three camera intrinsic parameters, i.e., the two focal lengths and the skew, can be solved from our new constraints on the imaged absolute conic obtained from at least three images. On this basis, we can initialize and then iteratively restore the optimal pair of projection centers of the Steiner conic, thereby obtaining the corresponding vanishing lines and images of circular points. Finally, all five camera parameters are fully calibrated using images of circular points obtained from at least three images. Experimental results on synthetic and real data demonstrate that our method achieves state-of-the-art performance in terms of accuracy",
    "volume": "main",
    "checked": true,
    "id": "59583f70910f3754ffac1282c51c44c27214633a",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7765_ECCV_2022_paper.php": {
    "title": "Super-Resolution 3D Human Shape from a Single Low-Resolution Image",
    "abstract": "We propose a novel framework to reconstruct super-resolution human shape from a single low-resolution input image. The approach overcomes limitations of existing approaches that reconstruct 3D human shape from a single image, which require high-resolution images together with auxiliary data such as surface normal or a parametric model to reconstruct high-detail shape. The proposed framework represents the reconstructed shape with a high-detail implicit function. Analogous to the objective of 2D image super-resolution, the approach learns the mapping from a low-resolution shape to its high-resolution counterpart and it is applied to reconstruct 3D shape detail from low-resolution images. The approach is trained end-to-end employing a novel loss function which estimates the information lost between a low and high-resolution representation of the same 3D surface shape. Evaluation for single image reconstruction of clothed people demonstrates that our method achieves high-detail surface reconstruction from low-resolution images without auxiliary data. Extensive experiments show that the proposed approach can estimate super-resolution human geometries with a significantly higher level of detail than that obtained with previous approaches when applied to low-resolution images",
    "volume": "main",
    "checked": true,
    "id": "7505e5f76474793b82b81c9c6c6cfafde9631d4a",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/107_ECCV_2022_paper.php": {
    "title": "Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal Charts and Distortion",
    "abstract": "Explicit neural surface representations allow for exact and efficient extraction of the encoded surface at arbitrary precision, as well as analytic derivation of differential geometric properties such as surface normal and curvature. Such desirable properties, which are absent in its implicit counterpart, makes it ideal for various applications in computer vision, graphics and robotics. However, SOTA works are limited in terms of the topology it can effectively describe, distortion it introduces to reconstruct complex surfaces and model efficiency. In this work, we present Minimal Neural Atlas, a novel atlas-based explicit neural surface representation. At its core is a fully learnable parametric domain, given by an implicit probabilistic occupancy field defined on an open square of the parametric space. In contrast, prior works generally predefine the parametric domain. The added flexibility enables charts to admit arbitrary topology and boundary. Thus, our representation can learn a minimal atlas of 3 charts with distortion-minimal parameterization for surfaces of arbitrary topology, including closed and open surfaces with arbitrary connected components. Our experiments support the hypotheses and show that our reconstructions are more accurate in terms of the overall geometry, due to the separation of concerns on topology and geometry",
    "volume": "main",
    "checked": true,
    "id": "76763df598ff61b622a04603dbb5f3946f20d7e6",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/194_ECCV_2022_paper.php": {
    "title": "ExtrudeNet: Unsupervised Inverse Sketch-and-Extrude for Shape Parsing",
    "abstract": "Sketch-and-extrude is a common and intuitive modeling process in computer aided design. This paper studies the problem of learning the shape given in the form of point clouds by “inverse” sketch-and-extrude. We present ExtrudeNet, an unsupervised end-to-end network for discovering sketch and extrude from point clouds. Behind ExtrudeNet are two new technical components: 1) the use of a specially-designed rational Bézier representation for sketch and extrude, which can model extrusion with freeform sketches and conventional cylinder and box primitives as well; and 2) a numerical method for computing the signed distance field which is used in the network learning. This is the first attempt that uses machine learning to reverse engineer the sketch-and-extrude modeling process of a shape in an unsupervised fashion. ExtrudeNet not only outputs a compact, editable and interpretable representation of the shape that can be seamlessly integrated into modern CAD software, but also aligns with the standard CAD modeling process facilitating various editing applications, which distinguishes our work from existing shape parsing research. Code will be open-sourced upon acceptance",
    "volume": "main",
    "checked": true,
    "id": "8192dbf95de2a5ca37afe5d5c6bf0f679ba92bc1",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/326_ECCV_2022_paper.php": {
    "title": "CATRE: Iterative Point Clouds Alignment for Category-Level Object Pose Refinement",
    "abstract": "While category-level 9DoF object pose estimation has emerged recently, previous correspondence-based or direct regression methods are both limited in accuracy due to the huge intra-category variances in object shape and color, etc. Orthogonal to them, this work presents a category-level object pose and size refiner CATRE, which is able to iteratively enhance pose estimate from point clouds to produce accurate results. Given an initial pose estimate, CATRE predicts a relative transformation between the initial pose and ground truth by means of aligning the partially observed point cloud and an abstract shape prior. In specific, we propose a novel disentangled architecture being aware of the inherent distinctions between rotation and translation/size estimation. Extensive experiments show that our approach remarkably outperforms state-of-the-art methods on REAL275, CAMERA25, and LM benchmarks up to a speed of approximately 85.32Hz, and achieves competitive results on category-level tracking. We further demonstrate that CATRE can perform pose refinement on unseen category. Code and trained models are available",
    "volume": "main",
    "checked": true,
    "id": "f9d32a7b7cdc84f119e63631cd570a81f9e3b6c5",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/361_ECCV_2022_paper.php": {
    "title": "Optimization over Disentangled Encoding: Unsupervised Cross-Domain Point Cloud Completion via Occlusion Factor Manipulation",
    "abstract": "Recently, studies considering domain gaps in shape completion attracted more attention, due to the undesirable performance of supervised methods on real scans. They only noticed the gap in input scans, but ignored the gap in output prediction, which is specific for completion. In this paper, we disentangle partial scans into three (domain, shape, and occlusion) factors to handle the output gap in cross-domain completion. For factor learning, we design view-point prediction and domain classification tasks in a self-supervised manner and bring a factor permutation consistency regularization to ensure factor independence. Thus, scans can be completed by simply manipulating occlusion factors while preserving domain and shape information. To further adapt to instances in the target domain, we introduce an optimization stage to maximize the consistency between completed shapes and input scans. Extensive experiments on real scans and synthetic datasets show that ours outperforms previous methods by a large margin and is encouraging for the following works. Code is available at https://github.com/azuki-miho/OptDE",
    "volume": "main",
    "checked": true,
    "id": "9b5720e78c4ab908100d18de4ea0505139c7df8c",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/463_ECCV_2022_paper.php": {
    "title": "Unsupervised Learning of 3D Semantic Keypoints with Mutual Reconstruction",
    "abstract": "Semantic 3D keypoints are category-level semantic consistent points on 3D objects. Detecting 3D semantic keypoints is a foundation for a number of 3D vision tasks but remains challenging, due to the ambiguity of semantic information, especially when the objects are represented by unordered 3D point clouds. Existing unsupervised methods tend to generate category-level keypoints in implicit manners, making it difficult to extract high-level information, such as semantic labels and topology. From a novel mutual reconstruction perspective, we present an unsupervised method to generate consistent semantic keypoints from point clouds explicitly. To achieve this, we train our unsupervised model to reconstruct both the input object and other objects from the same category based on predicted keypoints. To the best of our knowledge, the proposed method is the first to mine 3D semantic consistent keypoints from a mutual reconstruction view. Experiments under various evaluation metrics as well as comparisons with the state-of-the-arts have verified the efficacy of our new solution to mining semantic consistent keypoints with mutual reconstruction",
    "volume": "main",
    "checked": true,
    "id": "db41d6bb8f23afe8eb22318229d5d6971ece7f8a",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/525_ECCV_2022_paper.php": {
    "title": "MvDeCor: Multi-View Dense Correspondence Learning for Fine-Grained 3D Segmentation",
    "abstract": "We propose to utilize self-supervised techniques in the 2D domain for fine-grained 3D shape segmentation tasks. This is inspired by the observation that view-based surface representations are more effective at modeling high-resolution surface details and texture than their 3D counterparts based on point clouds or voxel occupancy. Specifically, given a 3D shape, we render it from multiple views, and set up a dense correspondence learning task within the contrastive learning framework. As a result, the learned 2D representations are view-invariant and geometrically consistent, leading to better generalization when trained on a limited number of labeled shapes than alternatives based on self-supervision in 2D or 3D alone. Experiments on textured (RenderPeople) and untextured (PartNet) 3D datasets show that our method outperforms state-of-the-art alternatives in fine-grained part segmentation. The improvements over baselines are greater when only a sparse set of views is available for training or when shapes are textured, indicating that \\mvd benefits from both 2D processing and 3D geometric reasoning",
    "volume": "main",
    "checked": true,
    "id": "e0644acbdd437226711b58135c82494caf5b8933",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/570_ECCV_2022_paper.php": {
    "title": "SUPR: A Sparse Unified Part-Based Human Representation",
    "abstract": "Statistical 3D shape models of the head, hands, and full body are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important information about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet deform due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts’ full range of motion. SUPR is publicly available for research purposes",
    "volume": "main",
    "checked": true,
    "id": "569d109c0b63f9d5a6a7e83b3bf6e9bf85a6d5cb",
    "citation_count": 0
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/584_ECCV_2022_paper.php": {
    "title": "Revisiting Point Cloud Simplification: A Learnable Feature Preserving Approach",
    "abstract": "The recent advances in 3D sensing technology have made possible the capture of point clouds in significantly high resolution. However, increased detail usually comes at the expense of high storage, as well as computational costs in terms of processing and visualization operations. Mesh and Point Cloud simplification methods aim to reduce the complexity of 3D models while retaining visual quality and relevant salient features. Traditional simplification techniques usually rely on solving a time-consuming optimization problem, hence they are impractical for large-scale datasets. In an attempt to alleviate this computational burden, we propose a fast point cloud simplification method by learning to sample salient points. The proposed method relies on a graph neural network architecture trained to select an arbitrary, user-defined, number of points according to their latent encodings and re-arrange their positions so as to minimize the visual perception error. The approach is extensively evaluated on various datasets using several perceptual metrics. Importantly, our method is able to generalize to out-of-distribution shapes, hence demonstrating zero-shot capabilities",
    "volume": "main",
    "checked": true,
    "id": "f16779f22ecfa5530b5a0d20089a3ee76a2a35ba",
    "citation_count": 3
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/800_ECCV_2022_paper.php": {
    "title": "Masked Autoencoders for Point Cloud Self-Supervised Learning",
    "abstract": "As a promising scheme of self-supervised learning, masked autoencoding has significantly advanced natural language processing and computer vision. Inspired by this, we propose a neat scheme of masked autoencoders for point cloud self-supervised learning, addressing the challenges posed by point cloud’s properties, including leakage of location information and uneven information density. Concretely, we divide the input point cloud into irregular point patches and randomly mask them at a high ratio. Then, a standard Transformer based autoencoder, with an asymmetric design and a shifting mask tokens operation, learns high-level latent features from unmasked point patches, aiming to reconstruct the masked point patches. Extensive experiments show that our approach is efficient during pre-training and generalizes well on various downstream tasks. The pre-trained models achieve 85.18% accuracy on ScanObjectNN and 94.04% accuracy on ModelNet40, outperforming all the other self-supervised learning methods. We show with our scheme, a simple architecture entirely based on standard Transformers can surpass dedicated Transformer models from supervised learning. Our approach also advances state-of-the-art accuracies by 1.5%-2.3% in the few-shot classification. Furthermore, our work inspires the feasibility of applying unified architectures from languages and images to the point cloud. Codes are available at https://github.com/Pang-Yatian/Point-MAE",
    "volume": "main",
    "checked": true,
    "id": "a601e71127d95bdae30fd818d2a0cc34b80b13f7",
    "citation_count": 21
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/927_ECCV_2022_paper.php": {
    "title": "Intrinsic Neural Fields: Learning Functions on Manifolds",
    "abstract": "Neural fields have gained significant attention in the computer vision community due to their excellent performance in novel view synthesis, geometry reconstruction, and generative modeling. Some of their advantages are a sound theoretic foundation and an easy implementation in current deep learning frameworks. While neural fields have been applied to signals on manifolds, e.g., for texture reconstruction, their representation has been limited to extrinsically embedding the shape into Euclidean space. The extrinsic embedding ignores known intrinsic manifold properties and is inflexible wrt. transfer of the learned function. To overcome these limitations, this work introduces intrinsic neural fields, a novel and versatile representation for neural fields on manifolds. Intrinsic neural fields combine the advantages of neural fields with the spectral properties of the Laplace-Beltrami operator. We show theoretically that intrinsic neural fields inherit many desirable properties of the extrinsic neural field framework but exhibit additional intrinsic qualities, like isometry invariance. In experiments, we show intrinsic neural fields can reconstruct high-fidelity textures from images with state-of-the-art quality and are robust to the discretization of the underlying manifold. We demonstrate the versatility of intrinsic neural fields by tackling various applications: texture transfer between deformed shapes & different shapes, texture reconstruction from real-world images with view dependence, and discretization-agnostic learning on meshes and point clouds",
    "volume": "main",
    "checked": true,
    "id": "14a28553f553e130655d8e7e899ae37c47def0ed",
    "citation_count": 3
  },
  "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1103_ECCV_2022_paper.php": {
    "title": "Skeleton-Free Pose Transfer for Stylized 3D Characters",
    "abstract": "We present the first method that automatically transfers poses between stylized 3D characters without skeletal rigging. In contrast to previous attempts to learn pose transformations on fixed or topology-equivalent skeleton templates, our method focuses on a novel scenario to handle skeleton-free characters with diverse shapes, topologies, and mesh connectivities. The key idea of our method is to represent the characters in a unified articulation model so that the pose can be transferred through the correspondent parts. To achieve this, we propose a novel pose transfer network that predicts the character skinning weights and deformation transformations jointly to articulate the target character to match the desired pose. Our method is trained in a semi-supervised manner absorbing all existing character data with paired/unpaired poses and stylized shapes. It generalizes well to unseen stylized characters and inanimate objects. We conduct extensive experiments and demonstrate the effectiveness of our method on this novel task",
    "volume": "main",
    "checked": true,
    "id": "1089b0872f8435a4da244dc4c03241305452738a",
    "citation_count": 1
  }
}