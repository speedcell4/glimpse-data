{
  "https://www.isca-speech.org/archive/interspeech_2018/atal18_interspeech.html": {
    "title": "From Vocoders to Code-Excited Linear Prediction: Learning How We Hear What We Hear",
    "volume": "main",
    "abstract": "It all started almost a century ago, in 1920s. A new undersea transatlantic telegraph cable had been laid. The idea of transmitting speech over the new telegraph cable caught the fancy of Homer Dudley, a young engineer who had just joined Bell Telephone Laboratories. This led to the invention of Vocoder - its close relative Voder was showcased as the first machine to create human speech at the 1939 New York World's Fair. However, the voice quality of vocoders was not good enough for use in commercial telephony. During the time speech scientists were busy with vocoders, several major developments took place outside speech research. Norbert Wiener developed a mathematical theory for calculating the best filters and predictors for detecting signals hidden in noise. Linear Prediction or Linear Predictive Coding became a major tool for speech processing. Claude Shannon established that the highest bit rate in a communication channel in presence of noise is achieved when the transmitted signal resembles random white Gaussian noise. Shannon's theory led to the invention of Code-Excited Linear Prediction (CELP). Nearly all digital cellular standards as well as standards for digital voice communication over the Internet use CELP coders. The success in speech coding came with understanding of what we hear and what we do not. Speech encoding at low bit rates introduce errors and these errors must be hidden under the speech signal to become inaudible. More and more, speech technologies are being used in different acoustic environments raising questions about the robustness of the technology. Human listeners handle situations well when the signal at our ears is not just one signal, but also a superposition of many acoustic signals. We need new research to develop signal-processing methods that can separate the mixed acoustic signal into individual components and provide performance similar or superior to that of human listeners",
    "checked": true,
    "id": "555b2345a62f4426c11eccd8174a184257fbc5b7",
    "semantic_title": "from vocoders to code-excited linear prediction: learning how we hear what we hear",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/karita18_interspeech.html": {
    "title": "Semi-Supervised End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "We propose a novel semi-supervised method for end-to-end automatic speech recognition (ASR). It can exploit large unpaired speech and text datasets, which require much less human effort to create paired speech-to-text datasets. Our semi-supervised method targets the extraction of an intermediate representation between speech and text data using a shared encoder network. Autoencoding of text data with this shared encoder improves the feature extraction of text data as well as that of speech data when the intermediate representations of speech and text are similar to each other as an inter-domain feature. In other words, by combining speech-to-text and text-to-text mappings through the shared network, we can improve speech-to-text mapping by learning to reconstruct the unpaired text data in a semi-supervised end-to-end manner. We investigate how to design suitable inter-domain loss, which minimizes the dissimilarity between the encoded speech and text sequences, which originally belong to quite different domains. The experimental results we obtained with our proposed semi-supervised training shows a larger character error rate reduction from 15.8% to 14.4% than a conventional language model integration on the Wall Street Journal dataset",
    "checked": true,
    "id": "933396f5b9111f6acdd76710ee6ab4d24e8673dd",
    "semantic_title": "semi-supervised end-to-end speech recognition",
    "citation_count": 46
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zeyer18_interspeech.html": {
    "title": "Improved Training of End-to-end Attention Models for Speech Recognition",
    "volume": "main",
    "abstract": "Sequence-to-sequence attention-based models on subword units allow simple open-vocabulary end-to-end speech recognition. In this work, we show that such models can achieve competitive results on the Switchboard 300h and LibriSpeech 1000h tasks. In particular, we report the state-of-the-art word error rates (WER) of 3.54% on the dev-clean and 3.82% on the test-clean evaluation subsets of LibriSpeech. We introduce a new pretraining scheme by starting with a high time reduction factor and lowering it during training, which is crucial both for convergence and final performance. In some experiments, we also use an auxiliary CTC loss function to help the convergence. In addition, we train long short-term memory (LSTM) language models on subword units. By shallow fusion, we report up to 27% relative improvements in WER over the attention baseline without a language model",
    "checked": true,
    "id": "f0ead45e8c1e4cc390ff6603bc0738b8c57f99ec",
    "semantic_title": "improved training of end-to-end attention models for speech recognition",
    "citation_count": 252
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hadian18_interspeech.html": {
    "title": "End-to-end Speech Recognition Using Lattice-free MMI",
    "volume": "main",
    "abstract": "We present our work on end-to-end training of acoustic models using the lattice-free maximum mutual information (LF-MMI) objective function in the context of hidden Markov models. By end-to-end training, we mean flat-start training of a single DNN in one stage without using any previously trained models, forced alignments, or building state-tying decision trees. We use full biphones to enable context-dependent modeling without trees and show that our end-to-end LF-MMI approach can achieve comparable results to regular LF-MMI on well-known large vocabulary tasks. We also compare with other end-to-end methods such as CTC in character-based and lexicon-free settings and show 5 to 25 percent relative reduction in word error rates on different large vocabulary tasks while using significantly smaller models",
    "checked": true,
    "id": "dcaeb29ad3307e2bdab2218416c81cb0c4e548b2",
    "semantic_title": "end-to-end speech recognition using lattice-free mmi",
    "citation_count": 147
  },
  "https://www.isca-speech.org/archive/interspeech_2018/braun18_interspeech.html": {
    "title": "Multi-channel Attention for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Recent end-to-end models for automatic speech recognition use sensory attention to integrate multiple input channels within a single neural network. However, these attention models are sensitive to the ordering of the channels used during training. This work proposes a sensory attention mechanism that is invariant to the channel ordering and only increases the overall parameter count by 0.09%. We demonstrate that even without re-training, our attention-equipped end-to-end model is able to deal with arbitrary numbers of input channels during inference. In comparison to a recent related model with sensory attention, our model when tested on the real noisy recordings from the multi-channel CHiME-4 dataset, achieves a relative character error rate (CER) improvement of 40.3% to 42.9%. In a two-channel configuration experiment, the attention signal allows the lower signal-to-noise ratio (SNR) sensor to be identified with 97.7% accuracy",
    "checked": true,
    "id": "8dd8b663af46a9be99181e92201a7e0b35bdf6f7",
    "semantic_title": "multi-channel attention for end-to-end speech recognition",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2018/parcollet18_interspeech.html": {
    "title": "Quaternion Convolutional Neural Networks for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Recently, the connectionist temporal classification (CTC) model coupled with recurrent (RNN) or convolutional neural networks (CNN), made it easier to train speech recognition systems in an end-to-end fashion. However in real-valued models, time frame components such as mel-filter-bank energies and the cepstral coefficients obtained from them, together with their first and second order derivatives, are processed as individual elements, while a natural alternative is to process such components as composed entities. We propose to group such elements in the form of quaternions and to process these quaternions using the established quaternion algebra. Quaternion numbers and quaternion neural networks have shown their efficiency to process multidimensional inputs as entities, to encode internal dependencies and to solve many tasks with less learning parameters than real-valued models. This paper proposes to integrate multiple feature views in quaternion-valued convolutional neural network (QCNN), to be used for sequence-to-sequence mapping with the CTC model. Promising results are reported using simple QCNNs in phoneme recognition experiments with the TIMIT corpus. More precisely, QCNNs obtain a lower phoneme error rate (PER) with less learning parameters than a competing model based on real-valued CNNs",
    "checked": true,
    "id": "45fdc73a239e9c6ea65e98c96f6a2d6dc35d6f72",
    "semantic_title": "quaternion convolutional neural networks for end-to-end automatic speech recognition",
    "citation_count": 78
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pang18_interspeech.html": {
    "title": "Compression of End-to-End Models",
    "volume": "main",
    "abstract": "End-to-end models, which output text directly given speech using a single neural network, have been shown to be competitive with conventional speech recognition models containing separate acoustic, pronunciation and language model components. Such models do not require additional resources for decoding and are typically much smaller than conventional models. This makes them particularly attractive in the context of on-device speech recognition where both small memory footprint and low power consumption are critical. This work explores the problem of compressing end-to-end models with the goal of satisfying device constraints without sacrificing model accuracy. We evaluate matrix factorization, knowledge distillation and parameter sparsity to determine the most effective methods given constraints such as a fixed parameter budget",
    "checked": true,
    "id": "7e05eb04d83b07014e7b2018666358ff5b9432a7",
    "semantic_title": "compression of end-to-end models",
    "citation_count": 46
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hodari18_interspeech.html": {
    "title": "Learning Interpretable Control Dimensions for Speech Synthesis by Using External Data",
    "volume": "main",
    "abstract": "There are many aspects of speech that we might want to control when creating text-to-speech (TTS) systems. We present a general method that enables control of arbitrary aspects of speech, which we demonstrate on the task of emotion control. Current TTS systems use supervised machine learning and are therefore heavily reliant on labelled data. If no labels are available for a desired control dimension, then creating interpretable control becomes challenging. We introduce a method that uses external, labelled data (i.e. not the original data used to train the acoustic model) to enable the control of dimensions that are not labelled in the original data. Adding interpretable control allows the voice to be manually controlled to produce more engaging speech, for applications such as audiobooks. We evaluate our method using a listening test",
    "checked": true,
    "id": "8451392694cb685e6f7741fe01ee312ab94ae846",
    "semantic_title": "learning interpretable control dimensions for speech synthesis by using external data",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2018/luong18_interspeech.html": {
    "title": "Investigating Accuracy of Pitch-accent Annotations in Neural Network-based Speech Synthesis and Denoising Effects",
    "volume": "main",
    "abstract": "We investigated the impact of noisy linguistic features on the performance of a Japanese speech synthesis system based on neural network that uses WaveNet vocoder. We compared an ideal system that uses manually corrected linguistic features including phoneme and prosodic information in training and test sets against a few other systems that use corrupted linguistic features. Both subjective and objective results demonstrate that corrupted linguistic features, especially those in the test set, affected the ideal system's performance significantly in a statistical sense due to a mismatched condition between the training and test sets. Interestingly, while an utterance-level Turing test showed that listeners had a difficult time differentiating synthetic speech from natural speech, it further indicated that adding noise to the linguistic features in the training set can partially reduce the effect of the mismatch, regularize the model and help the system perform better when linguistic features of the test set are noisy",
    "checked": true,
    "id": "a3746413aa9fff40c2b9b967f594c7f10cd3b07f",
    "semantic_title": "investigating accuracy of pitch-accent annotations in neural network-based speech synthesis and denoising effects",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liou18_interspeech.html": {
    "title": "An Exploration of Local Speaking Rate Variations in Mandarin Read Speech",
    "volume": "main",
    "abstract": "This paper explores speaking rate variation in Mandarin read speech. In contrast to assuming that each utterance is generated in a constant or global speaking rate, this study seeks to estimate local speaking rate for each prosodic unit in an utterance. The exploration is based on the existing speaking rate-dependent hierarchical prosodic model (SR-HPM). The main idea is to first use the SR-HPM to explore the prosodic structures of utterances and extract the prosodic units. Then, local speaking rate is estimated for each prosodic unit (prosodic phrase in this study). Some major influence factors including tone, base syllable type, prosodic structure and speaking rate of the higher prosodic units (utterance and BG/PG) are compensated in the local SR estimation. A syntactic-local SR model is constructed and use in the prosody generation of Mandarin TTS. Experimental results on a large read speech corpus generated by a professional female announcer showed that the generated prosody with local speaking rate variations is proved to be more vivid than the one with a constant speaking rate",
    "checked": true,
    "id": "acf807496fafea77ef0b2a4ed028a1b7f22ab61f",
    "semantic_title": "an exploration of local speaking rate variations in mandarin read speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zheng18_interspeech.html": {
    "title": "BLSTM-CRF Based End-to-End Prosodic Boundary Prediction with Context Sensitive Embeddings in a Text-to-Speech Front-End",
    "volume": "main",
    "abstract": "In this paper, we propose a language-independent end-to-end architecture for prosodic boundary prediction based on BLSTM-CRF. The proposed architecture has three components, word embedding layer, BLSTM layer and CRF layer. The word embedding layer is employed to learn the task-specific embeddings for prosodic boundary prediction. The BLSTM layer can efficiently use both past and future input features, while the CRF layer can efficiently use sentence level information. We integrate these three components and learn the whole process end-to-end. In addition, we investigate both character-level embeddings and context sensitive embeddings to this model and employ an attention mechanism for combining alternative word-level embeddings. By using an attention mechanism, the model is able to decide how much information to use from each level of embeddings. Objective evaluation results show the proposed BLSTM-CRF architecture achieves the best results on both Mandarin and English datasets, with an absolute improvement of 3.21% and 3.74% in F1 score, respectively, for intonational phrase prediction, compared to previous state-of-the-art method (BLSTM). The subjective evaluation results further indicate the effectiveness of the proposed methods",
    "checked": true,
    "id": "b8a5516ce4a5d114203ef2b0dda2cdcf7a07c128",
    "semantic_title": "blstm-crf based end-to-end prosodic boundary prediction with context sensitive embeddings in a text-to-speech front-end",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sisman18b_interspeech.html": {
    "title": "Wavelet Analysis of Speaker Dependent and Independent Prosody for Voice Conversion",
    "volume": "main",
    "abstract": "Thus far, voice conversion studies are mainly focused on the conversion of spectrum. However, speaker identity is also characterized by its prosody features, such as fundamental frequency (F0) and energy contour. We believe that with a better understanding of speaker dependent/independent prosody features, we can devise an analytic approach that addresses voice conversion in a better way. We consider that speaker dependent features reflect speaker's individuality, while speaker independent features reflect the expression of linguistic content. Therefore, the former is to be converted while the latter is to be carried over from source to target during the conversion. To achieve this, we provide an analysis of speaker dependent and speaker independent prosody patterns in different temporal scales by using wavelet transform. The centrepiece of this paper is based on the understanding that a speech utterance can be characterized by speaker dependent and independent features in its prosodic manifestations. Experiments show that the proposed prosody analysis scheme improves the prosody conversion performance consistently under the sparse representation framework",
    "checked": true,
    "id": "264db56d26e1fa2304c656485d15fb786567ddf7",
    "semantic_title": "wavelet analysis of speaker dependent and independent prosody for voice conversion",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18f_interspeech.html": {
    "title": "Improving Mongolian Phrase Break Prediction by Using Syllable and Morphological Embeddings with BiLSTM Model",
    "volume": "main",
    "abstract": "In the speech synthesis systems, the phrase break (PB) prediction is the first and most important step. Recently, the state-of-the-art PB prediction systems mainly rely on word embeddings. However this method is not fully applicable to Mongolian language, because its word embeddings are inadequate trained, owing to the lack of resources. In this paper, we introduce a bidirectional Long Short Term Memory (BiLSTM) model which combined word embeddings with syllable and morphological embedding representations to provide richer and multi-view information which leverages the agglutinative property. Experimental results show the proposed method outperforms compared systems which only used the word embeddings. In addition, further analysis shows that it is quite robust to the Out-of-Vocabulary (OOV) problem owe to the refined word embedding. The proposed method achieves the state-of-the-art performance in the Mongolian PB prediction",
    "checked": true,
    "id": "fcb19ca59aee54f64b179b1072ea29af8573e9e3",
    "semantic_title": "improving mongolian phrase break prediction by using syllable and morphological embeddings with bilstm model",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2018/you18_interspeech.html": {
    "title": "Improved Supervised Locality Preserving Projection for I-vector Based Speaker Verification",
    "volume": "main",
    "abstract": "A Supervised Locality Preserving Projection (SLPP) method is employed for channel compensation in an i-vector based speaker verification system. SLPP preserves more important local information by weighing both the within- and between-speaker nearby data pairs based on the similarity matrices. In this paper, we propose an improved SLPP (P-SLPP) to enhance the channel compensation ability. First, the conventional Euclidean distance in conventional SLPP is replaced with Probabilistic Linear Discriminant Analysis (PLDA) scores. Furthermore, the weight matrices of P-SLPP are generated by using the relative PLDA scores of within- and between-speaker pairs. Experiments are carried out on the five common conditions of NIST 2012 speaker recognition evaluation (SRE) core sets. The results show that SLPP and the proposed P-SLPP outperform all other state-of-the-art channel compensation methods. Among these methods, P-SLPP achieves the best performance",
    "checked": true,
    "id": "6d16338ec327ed00631790202f0146df102ca333",
    "semantic_title": "improved supervised locality preserving projection for i-vector based speaker verification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shi18b_interspeech.html": {
    "title": "Double Joint Bayesian Modeling of DNN Local I-Vector for Text Dependent Speaker Verification with Random Digit Strings",
    "volume": "main",
    "abstract": "Double joint Bayesian is a recently introduced analysis method that models and explores multiple information explicitly from the samples to improve the verification performance. It was recently applied to voice pass phrase verification, result in better results on text dependent speaker verification task. However little is known about its effectiveness in other challenging situations such as speaker verification for short, text-constrained test utterances, e.g. random digit strings. Contrary to conventional joint Bayesian method that cannot make full use of multi-view information, double joint Bayesian can incorporate both intra-speaker/digit and inter-speaker/digit variation and calculated the likelihood to describe whether the features having all labels consistent or not. We show that double joint Bayesian outperforms conventional method on modeling DNN local (digit-dependent) i-vectors for speaker verification with random prompted digit strings. Since the strength of both double joint Bayesian and conventional DNN local i-vector appear complementary, the combination significantly outperforms either of its components",
    "checked": true,
    "id": "27b767dd96d1ef0fb778f5131e5091ab12bf1acd",
    "semantic_title": "double joint bayesian modeling of dnn local i-vector for text dependent speaker verification with random digit strings",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/silnova18_interspeech.html": {
    "title": "Fast Variational Bayes for Heavy-tailed PLDA Applied to i-vectors and x-vectors",
    "volume": "main",
    "abstract": "The standard state-of-the-art backend for text-independent speaker recognizers that use i-vectors or x-vectors is Gaussian PLDA (G-PLDA), assisted by a Gaussianization step involving length normalization. G-PLDA can be trained with both gener- ative or discriminative methods. It has long been known that heavy-tailed PLDA (HT-PLDA), applied without length nor- malization, gives similar accuracy, but at considerable extra computational cost. We have recently introduced a fast scor- ing algorithm for a discriminatively trained HT-PLDA back- end. This paper extends that work by introducing a fast, vari- ational Bayes, generative training algorithm. We compare old and new backends, with and without length-normalization, with i-vectors and x-vectors, on SRE'10, SRE'16 and SITW",
    "checked": true,
    "id": "a229084bf0f268a345b20e5361e41e128b19b48b",
    "semantic_title": "fast variational bayes for heavy-tailed plda applied to i-vectors and x-vectors",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2018/todisco18_interspeech.html": {
    "title": "Integrated Presentation Attack Detection and Automatic Speaker Verification: Common Features and Gaussian Back-end Fusion",
    "volume": "main",
    "abstract": "The vulnerability of automatic speaker verification (ASV) systems to spoofing is widely acknowledged. Recent years have seen an intensification in research efforts to develop spoofing countermeasures, also known as presentation attack detection (PAD) systems. Much of this work has involved the exploration of features that discriminate reliably between bona fide and spoofed speech. While there are grounds to use different front-ends for ASV and PAD systems (they are different tasks) the use of a single front-end has obvious benefits, not least convenience and computational efficiency, especially when ASV and PAD are combined. This paper investigates the performance of a variety of different features used previously for both ASV and PAD and assesses their performance when combined for both tasks. The paper also presents a Gaussian back-end fusion approach to system combination. In contrast to cascaded architectures, it relies upon the modelling of the two-dimensional score distribution stemming from the combination of ASV and PAD in parallel. This approach to combination is shown to generalise particularly well across independent ASVspoof 2017 v2.0 development and evaluation datasets",
    "checked": true,
    "id": "2b5d9c131d5f45216303cad5407d5ac9a4ab69f5",
    "semantic_title": "integrated presentation attack detection and automatic speaker verification: common features and gaussian back-end fusion",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ferrer18_interspeech.html": {
    "title": "A Generalization of PLDA for Joint Modeling of Speaker Identity and Multiple Nuisance Conditions",
    "volume": "main",
    "abstract": "Probabilistic linear discriminant analysis (PLDA) is the leading method for computing scores in speaker recognition systems. The method models the vectors representing each audio sample as a sum of three terms: one that depends on the speaker identity, one that models the within-speaker variability and one that models any remaining variability. The last two terms are assumed to be independent across samples. We recently proposed an extension of the PLDA method, which we termed Joint PLDA (JPLDA), where the second term is considered dependent on the type of nuisance condition present in the data (e.g., the language or channel). The proposed method led to significant gains for multilanguage speaker recognition when taking language as the nuisance condition. In this paper, we present a generalization of this approach that allows for multiple nuisance terms. We show results using language and several nuisance conditions describing the acoustic characteristics of the sample and demonstrate that jointly including all these factors in the model leads to better results than including only language or acoustic condition factors. Overall, we obtain relative improvements in detection cost function between 5% and 47% for various systems and test conditions with respect to standard PLDA approaches",
    "checked": true,
    "id": "dc77b0a0611d6e7c940a237decc2a9db2c16b3ce",
    "semantic_title": "a generalization of plda for joint modeling of speaker identity and multiple nuisance conditions",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18j_interspeech.html": {
    "title": "An Investigation of Non-linear i-vectors for Speaker Verification",
    "volume": "main",
    "abstract": "Speaker verification becomes increasingly important due to the popularity of speech assistants and smart home. i-vectors are used broadly for this topic, which use factor analysis to model the shift of average parameter in Gaussian Mixture Models. Recently by the progress of deep learning, high-level non-linearity improves results in many areas. In this paper we proposed a new framework of i-vectors which uses stochastic gradient descent to solve the problem of i-vectors. From our preliminary results stochastic gradient descent can get same performance as expectation-maximization algorithm. However, by backpropagation the assumption can be more flexible, so both linear and non-linear assumption is possible in our framework. From our result, both maximum a posteriori estimation and maximum likelihood lead to slightly better result than conventional i-vectors and both linear and non-linear system has similar performance",
    "checked": true,
    "id": "813f21bcc64ccf2a4744e8eb14b65298bbd769dc",
    "semantic_title": "an investigation of non-linear i-vectors for speaker verification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ram18_interspeech.html": {
    "title": "CNN Based Query by Example Spoken Term Detection",
    "volume": "main",
    "abstract": "In this work, we address the problem of query by example spoken term detection (QbE-STD) in zero-resource scenario. State of the art solutions usually rely on dynamic time warping (DTW) based template matching. In contrast, we propose here to tackle the problem as binary classification of images. Similar to the DTW approach, we rely on deep neural network (DNN) based posterior probabilities as feature vectors. The posteriors from a spoken query and a test utterance are used to compute frame-level similarities in a matrix form. This matrix contains somewhere a quasi-diagonal pattern if the query occurs in the test utterance. We propose to use this matrix as an image and train a convolutional neural network (CNN) for identifying the pattern and make a decision about the occurrence of the query. This language independent system is evaluated on SWS 2013 and is shown to give 10% relative improvement over a highly competitive baseline system based on DTW. Experiments on QUESST 2014 database gives similar improvements showing that the approach generalizes to other database as well",
    "checked": true,
    "id": "55337b94a7d2f3a16c71d7852cff290a45637e22",
    "semantic_title": "cnn based query by example spoken term detection",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yuan18_interspeech.html": {
    "title": "Learning Acoustic Word Embeddings with Temporal Context for Query-by-Example Speech Search",
    "volume": "main",
    "abstract": "We propose to learn acoustic word embeddings with temporal context for query-by-example (QbE) speech search. The temporal context includes the leading and trailing word sequences of a word. We assume that there exist spoken word pairs in the training database. We pad the word pairs with their original temporal context to form fixed-length speech segment pairs. We obtain the acoustic word embeddings through a deep convolutional neural network (CNN) which is trained on the speech segment pairs with a triplet loss. By shifting a fixed-length analysis window through the search content, we obtain a running sequence of embeddings. In this way, searching for the spoken query is equivalent to the matching of acoustic word embeddings. The experiments show that our proposed acoustic word embeddings learned with temporal context are effective in QbE speech search. They outperform the state-of-the-art frame-level feature representations and reduce run-time computation since no dynamic time warping is required in QbE speech search. We also find that it is important to have sufficient speech segment pairs to train the deep CNN for effective acoustic word embeddings",
    "checked": true,
    "id": "b3852b76ccfc1fd311f6afd5ac82dd2862f03011",
    "semantic_title": "learning acoustic word embeddings with temporal context for query-by-example speech search",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhu18b_interspeech.html": {
    "title": "Siamese Recurrent Auto-Encoder Representation for Query-by-Example Spoken Term Detection",
    "volume": "main",
    "abstract": "With the explosive development of human-computer speech interaction, spoken term detection is widely required and has attracted increasing interest. In this paper, we propose a weak supervised approach using Siamese recurrent auto-encoder (RAE) to represent speech segments for query-by-example spoken term detection (QbyE-STD). The proposed approach exploits word pairs that contain different instances of the same/different word content as input to train the Siamese RAE. The encoder last hidden state vector of Siamese RAE is used as the feature for QbyE-STD, which is a fixed dimensional embedding feature containing mostly semantic content related information. The advantages of the proposed approach are: 1) extracting more compact feature with fixed dimension while keeping the semantic information for STD; 2) the extracted feature can describe the sequential phonetic structure of similar sounds to degree, which can be applied for zero-resource QbyE-STD. Evaluations on real scene Chinese speech interaction data and TIMIT confirm the effectiveness and efficiency of the proposed approach compared to the conventional ones",
    "checked": true,
    "id": "6ab41db11281b6b62023a6db3540e40f1e2ae252",
    "semantic_title": "siamese recurrent auto-encoder representation for query-by-example spoken term detection",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18g_interspeech.html": {
    "title": "Fast Derivation of Cross-lingual Document Vectors from Self-attentive Neural Machine Translation Model",
    "volume": "main",
    "abstract": "A universal cross-lingual representation of documents, which can capture the underlying semantics is very useful in many natural language processing tasks. In this paper, we develop a new document vectorization method which effectively selects the most salient sequential patterns from the inputs to create document vectors via a self-attention mechanism using a neural machine translation (NMT) model. The model used by our method can be trained with parallel corpora that are unrelated to the task at hand. During testing, our method will take a monolingual document and convert it into a \"Neural machine Translation framework based cross-lingual Document Vector\" (NTDV). NTDV has two comparative advantages. Firstly, the NTDV can be produced by the forward pass of the encoder in the NMT and the process is very fast and does not require any training/optimization. Secondly, our model can be conveniently adapted from a pair of existing attention based NMT models and the training requirement on parallel corpus can be reduced significantly. In a cross-lingual document classification task, our NTDV embeddings surpass the previous state-of-the-art performance in the English-to-German classification test and, to our best knowledge, it also achieves the best performance among the fast decoding methods in the German-to-English classification test",
    "checked": true,
    "id": "c82820a991b1ce9a9a8f20a9032d86fd83257b27",
    "semantic_title": "fast derivation of cross-lingual document vectors from self-attentive neural machine translation model",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pandey18_interspeech.html": {
    "title": "LSTM Based Attentive Fusion of Spectral and Prosodic Information for Keyword Spotting in Hindi Language",
    "volume": "main",
    "abstract": "In this paper, a DNN based keyword spotting framework, that utilizes both spectral as well as prosodic information present in the speech signal, is proposed. A DNN is first trained to learn a set of hierarchical non-linear transformation parameters that project the original spectral and prosodic feature vectors onto a feature space where the distance between similar syllable pairs is small and between dissimilar syllable pairs is large. These transformed features are then fused using an attention-based long short-term memory (LSTM) network. As a side result, a deep denoising autoencoder based fine-tuning technique is used to improve the performance of sequence predictions. A sequence matching method called the sliding syllable protocol is also developed for keyword spotting. Syllable recognition and keyword spotting (KWS) experiments are conducted specifically for the Hindi language which is one of the widely spoken languages across the globe but is not addressed significantly by the speech processing community. The proposed framework indicates reasonable improvements when compared to baseline methods available in the literature",
    "checked": true,
    "id": "dd9f2c3c1e8fbe4ec8f08aac61c3b4ea43371b27",
    "semantic_title": "lstm based attentive fusion of spectral and prosodic information for keyword spotting in hindi language",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shankar18_interspeech.html": {
    "title": "Spoken Keyword Detection Using Joint DTW-CNN",
    "volume": "main",
    "abstract": "A method to detect spoken keywords in a given speech utterance is proposed, called as joint Dynamic Time Warping (DTW)- Convolution Neural Network (CNN). It is a combination of DTW approach with a strong classifier like CNN. Both these methods have independently shown significant results in solving problems related to optimal sequence alignment and object recognition, respectively. The proposed method modifies the original DTW formulation and converts the warping matrix into a gray scale image. A CNN is trained on these images to classify the presence or absence of keyword by identifying the texture of warping matrix. The TIMIT corpus has been used for conducting experiments and our method shows significant improvement over other existing techniques",
    "checked": true,
    "id": "f613d3c84d6df49d762560bc900fa82e2dcfc601",
    "semantic_title": "spoken keyword detection using joint dtw-cnn",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2018/schuller18_interspeech.html": {
    "title": "The INTERSPEECH 2018 Computational Paralinguistics Challenge: Atypical & Self-Assessed Affect, Crying & Heart Beats",
    "volume": "main",
    "abstract": "The INTERSPEECH 2018 Computational Paralinguistics Challenge addresses four different problems for the first time in a research competition under well-defined conditions: In the Atypical Affect Sub-Challenge, four basic emotions annotated in the speech of handicapped subjects have to be classified; in the Self-Assessed Affect Sub-Challenge, valence scores given by the speakers themselves are used for a three-class classification problem; in the Crying Sub-Challenge, three types of infant vocalisations have to be told apart; and in the Heart Beats Sub-Challenge, three different types of heart beats have to be determined. We describe the Sub-Challenges, their conditions and baseline feature extraction and classifiers, which include data-learnt (supervised) feature representations by end-to-end learning, the â€˜usual' ComParE and BoAW features and deep unsupervised representation learning using the AUDEEP toolkit for the first time in the challenge series",
    "checked": true,
    "id": "783d2bd2820b35ce7e398be412569e9d5c6f5880",
    "semantic_title": "the interspeech 2018 computational paralinguistics challenge: atypical & self-assessed affect, crying & heart beats",
    "citation_count": 114
  },
  "https://www.isca-speech.org/archive/interspeech_2018/humayun18_interspeech.html": {
    "title": "An Ensemble of Transfer, Semi-supervised and Supervised Learning Methods for Pathological Heart Sound Classification",
    "volume": "main",
    "abstract": "In this work, we propose an ensemble of classifiers to distinguish between various degrees of abnormalities of the heart using Phonocardiogram (PCG) signals acquired using digital stethoscopes in a clinical setting, for the INTERSPEECH 2018 Computational Paralinguistics (ComParE) Heart Beats Sub-Challenge. Our primary classification framework constitutes a convolutional neural network with 1D-CNN time-convolution (tConv) layers, which uses features transferred from a model trained on the 2016 Physionet Heart Sound Database. We also employ a Representation Learning (RL) approach to generate features in an unsupervised manner using Deep Recurrent Autoencoders and use Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) classifiers. Finally, we utilize an SVM classifier on a high-dimensional segment-level feature extracted using various functionals on short-term acoustic features, i.e., Low-Level Descriptors (LLD). An ensemble of the three different approaches provides a relative improvement of 11.13% compared to our best single sub-system in terms of the Unweighted Average Recall (UAR) performance metric on the evaluation dataset",
    "checked": true,
    "id": "1578dd4dc568ebc2f3a1bff093b17e3ea53d2073",
    "semantic_title": "an ensemble of transfer, semi-supervised and supervised learning methods for pathological heart sound classification",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2018/turan18_interspeech.html": {
    "title": "Monitoring Infant's Emotional Cry in Domestic Environments Using the Capsule Network Architecture",
    "volume": "main",
    "abstract": "Automated recognition of an infant's cry from audio can be considered as a preliminary step for the applications like remote baby monitoring. In this paper, we implemented a recently introduced deep learning topology called capsule network (CapsNet) for the cry recognition problem. A capsule in the CapsNet, which is defined as a new representation, is a group of neurons whose activity vector represents the probability that the entity exists. Active capsules at one level make predictions, via transformation matrices, for the parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We employed spectrogram representations from the short segments of an audio signal as an input of the CapsNet. For experimental evaluations, we apply the proposed method on INTERSPEECH 2018 computational paralinguistics challenge (ComParE), crying sub-challenge, which is a three-class classification task using an annotated database (CRIED). Provided audio samples contains recordings from 20 healthy infants and categorized into the three classes namely neutral, fussing and crying. We show that multi-layer CapsNet outperforms baseline performance on CRIED corpus and is considerably better than a conventional convolutional net",
    "checked": true,
    "id": "b3383950b3162e7f651ce9c30f2eba7efacd4e28",
    "semantic_title": "monitoring infant's emotional cry in domestic environments using the capsule network architecture",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/huckvale18_interspeech.html": {
    "title": "Neural Network Architecture That Combines Temporal and Summative Features for Infant Cry Classification in the Interspeech 2018 Computational Paralinguistics Challenge",
    "volume": "main",
    "abstract": "This paper describes the application of a novel deep neural network architecture to the classification of infant vocalisations as part of the Interspeech 2018 Computational Paralinguistics Challenge. Previous approaches to infant cry classification have either applied a statistical classifier to summative features of the whole cry, or applied a syntactic pattern recognition technique to a temporal sequence of features. In this work we explore a deep neural network architecture that exploits both temporal and summative features to make a joint classification. The temporal input comprises centi-second frames of low-level signal features which are input to LSTM nodes, while the summative vector comprises a large set of statistical functionals of the same frames that are input to MLP nodes. The combined network is jointly optimized and evaluated using leave-one-speaker-out cross-validation on the challenge training set. Results are compared to independently-trained temporal and summative networks and to a baseline SVM classifier. The combined model outperforms the other models and the challenge baseline on the training set. While problems remain in finding the best configuration and training protocol for such networks, the approach seems promising for future signal classification tasks",
    "checked": true,
    "id": "7baa89b0658e5e4bf54136f5a7bc07af948ff2ad",
    "semantic_title": "neural network architecture that combines temporal and summative features for infant cry classification in the interspeech 2018 computational paralinguistics challenge",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18l_interspeech.html": {
    "title": "Evolving Learning for Analysing Mood-Related Infant Vocalisation",
    "volume": "main",
    "abstract": "Infant vocalisation analysis plays an important role in the study of the development of pre-speech capability of infants, while machine-based approaches nowadays emerge with an aim to advance such an analysis. However, conventional machine learning techniques require heavy feature-engineering and refined architecture designing. In this paper, we present an evolving learning framework to automate the design of neural network structures for infant vocalisation analysis. In contrast to manually searching by trial and error, we aim to automate the search process in a given space with less interference. This framework consists of a controller and its child networks, where the child networks are built according to the controller's estimation. When applying the framework to the Interspeech 2018 Computational Paralinguistics (ComParE) Crying Sub-challenge, we discover several deep recurrent neural network structures, which are able to deliver competitive results to the best ComParE baseline method",
    "checked": true,
    "id": "ba398cb0c7406237caf5bf511cbf66fa08b5e2c0",
    "semantic_title": "evolving learning for analysing mood-related infant vocalisation",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wagner18_interspeech.html": {
    "title": "Deep Learning in Paralinguistic Recognition Tasks: Are Hand-crafted Features Still Relevant?",
    "volume": "main",
    "abstract": "In the past, the performance of machine learning algorithms depended heavily on the representation of the data. Well-designed features therefore played a key role in speech and paralinguistic recognition tasks. Consequently, engineers have put a great deal of work into manually designing large and complex acoustic feature sets. With the emergence of Deep Neural Networks (DNNs), however, it is now possible to automatically infer higher abstractions from simple spectral representations or even learn directly from raw waveforms. This raises the question if (complex) hand-crafted features will still be needed in the future. We take this year's INTERSPEECH Computational Paralinguistic Challenge as an opportunity to approach this issue by means of two corpora - Atypical Affect and Crying. At first, we train a Recurrent Neural Network (RNN) to evaluate the performance of several hand-crafted feature sets of varying complexity. Afterwards, we make the network do the feature engineering all on its own by prefixing a stack of convolutional layers. Our results show that there is no clear winner (yet). This creates room to discuss chances and limits of either approach",
    "checked": true,
    "id": "db622af9b0ba2b691af53ba0fcb46ad279592c3c",
    "semantic_title": "deep learning in paralinguistic recognition tasks: are hand-crafted features still relevant?",
    "citation_count": 44
  },
  "https://www.isca-speech.org/archive/interspeech_2018/luo18_interspeech.html": {
    "title": "Investigation on Joint Representation Learning for Robust Feature Extraction in Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) is a challenging task due to its difficulty in finding proper representations for emotion embedding in speech. Recently, Convolutional Recurrent Neural Network (CRNN), which is combined by convolution neural network and recurrent neural network, is popular in this field and achieves state-of-art on related corpus. However, most of work on CRNN only utilizes simple spectral information, which is not capable to capture enough emotion characteristics for the SER task. In this work, we investigate two joint representation learning structures based on CRNN aiming at capturing richer emotional information from speech. Cooperating the handcrafted high-level statistic features with CRNN, a two-channel SER system (HSF-CRNN) is developed to jointly learn the emotion-related features with better discriminative property. Furthermore, considering that the time duration of speech segment significantly affects the accuracy of emotion recognition, another two-channel SER system is proposed where CRNN features extracted from different time scale of spectrogram segment are used for joint representation learning. The systems are evaluated over Atypical Affect Challenge of ComParE2018 and IEMOCAP corpus. Experimental results show that our proposed systems outperform the plain CRNN",
    "checked": true,
    "id": "2574dd6f8965e0b4adf73272f93f236b5152ca55",
    "semantic_title": "investigation on joint representation learning for robust feature extraction in speech emotion recognition",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2018/park18c_interspeech.html": {
    "title": "Using Voice Quality Supervectors for Affect Identification",
    "volume": "main",
    "abstract": "The voice quality of speech sounds often conveys perceivable information about the speaker's affect. This study proposes perceptually important voice quality features to recognize affect represented in speech excerpts from individuals with mental, neurological and/or physical disabilities. The voice quality feature set consists of F0, harmonic amplitude differences between the first, second, fourth harmonics and the harmonic near 2 kHz, the center frequency and amplitudes of the first 3 formants and cepstral peak prominence. The feature distribution of each utterance was represented with a supervector and the Gaussian mixture model and support vector machine classifiers were used for affect classification. Similar classification systems using the MFCCs and ComParE16 feature set were implemented. The systems were fused by taking the confidence mean of the classifiers. Applying the fused system to the Interspeech 2018 Atypical Affect subchallenge task resulted in unweighted average recalls of 43.9% and 41.0% on the development and test dataset, respectively. Additionally, we investigated clusters obtained by unsupervised learning to address gender-related differences",
    "checked": true,
    "id": "c3fde8b1abe3efa03a03485d827f272f1dc4b3ee",
    "semantic_title": "using voice quality supervectors for affect identification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tang18c_interspeech.html": {
    "title": "An End-to-End Deep Learning Framework for Speech Emotion Recognition of Atypical Individuals",
    "volume": "main",
    "abstract": "The goal of the ongoing ComParE 2018 Atypical Affect sub-challenge is to recognize the emotional states of atypical individuals. In this work, we present three modeling methods under the end-to-end learning framework, namely CNN combined with extended features, CNN+RNN and ResNet, respectively. Furthermore, we investigate multiple data augmentation, balancing and sampling methods to further enhance the system performance. The experimental results show that data balancing and augmentation increase the unweighted accuracy (UAR) by 10% absolutely. After score level fusion, our proposed system achieves 48.8% UAR on the develop dataset",
    "checked": false,
    "id": "dfadf827301d924f0bef0a445bac68e1f90904c3",
    "semantic_title": "an end-to-end deep learning framework with speech emotion recognition of atypical individuals",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2018/koller18_interspeech.html": {
    "title": "DialogOS: Simple and Extensible Dialogue Modeling",
    "volume": "main",
    "abstract": "We present the open-source extensible dialog manager DialogOS. DialogOS features simple finite-state based dialog management (which can be expanded to more complex DM strategies via a full-fledged scripting language) in combination with integrated speech recognition and synthesis in multiple languages. DialogOS runs on all major platforms, provides a simple-to-use graphical interface and can easily be extended via well-defined plugin and client interfaces, or can be integrated server-side into larger existing software infrastructures. We hope that DialogOS will help foster research and teaching given that it lowers the bar of entry into building and testing spoken dialog systems and provides paths to extend one's system as development progresses",
    "checked": true,
    "id": "67e0af648094c65b78f326d0397e6be9fc870112",
    "semantic_title": "dialogos: simple and extensible dialogue modeling",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dernoncourt18_interspeech.html": {
    "title": "A Framework for Speech Recognition Benchmarking",
    "volume": "main",
    "abstract": "Over the past few years, the number of APIs for automated speech recognition (ASR) has significantly increased. It is often time-consuming to evaluate how the performance of these ASR systems compare with each other and against newly proposed algorithms. In this paper, we present a lightweight, open source framework that allows users to easily benchmark ASR APIs on the corpora of their choice. The framework currently supports 7 ASR APIs and is easily extendable to more APIs",
    "checked": true,
    "id": "de2dc71ef3333c3ca53f500b2ee169d4d5668141",
    "semantic_title": "a framework for speech recognition benchmarking",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2018/arai18_interspeech.html": {
    "title": "Flexible Tongue Housed in a Static Model of the Vocal Tract With Jaws, Lips and Teeth",
    "volume": "main",
    "abstract": "Physical models of the human vocal tract with a moveable tongue have been reported in past literature. In this study, we developed a new model with a flexible tongue. As with previous models by the author, the flexible tongue is made of gel material. The shape of this model's tongue is still an abstraction, although it is more realistic than previous models. Apart from the tongue, the model is static and solid; the gel tongue is the main part that can be manipulated. The static portion of the model is an extension of our recent static model with lips, teeth and tongue. The entire model looks like a sagittal splice taken from an artificial human head. Because the thin, acrylic plates on the outside are transparent, the interior of the oral and pharyngeal cavities are visible. When we feed a glottal sound through a hole in the laryngeal region on the bottom of the model, different vowels are produced, dependent upon the shape of the tongue. This model is the most useful and realistic looking of the models we've made for speech science education so far",
    "checked": true,
    "id": "301fe62464796e0e476c7e10447c5c1142b4cacd",
    "semantic_title": "flexible tongue housed in a static model of the vocal tract with jaws, lips and teeth",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mathew18_interspeech.html": {
    "title": "Voice Analysis Using Acoustic and Throat Microphones for Speech Therapy",
    "volume": "main",
    "abstract": "Diagnosis of voice disorders by a speech therapist involves the process of voice recording with the patient, followed by software-aided analysis. In this paper, we propose a novel voice diagnosis system which gives voice report information based on Praat software, using voice samples from a throat microphone and an acoustic microphone, making the diagnosis near real-time, as well as robust to background noise. Results show that throat microphones give reliable Jitter and Shimmer values in ambient noise levels of 47~50 dB, while acoustic microphones show high variance in these parameters",
    "checked": true,
    "id": "7a0badb7d33d1d895d7cf59cfefbf79d4cb7a7e0",
    "semantic_title": "voice analysis using acoustic and throat microphones for speech therapy",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rayner18_interspeech.html": {
    "title": "A Robust Context-Dependent Speech-to-Speech Phraselator Toolkit for Alexa",
    "volume": "main",
    "abstract": "We present an open source toolkit for creating robust speech-to-speech phraselators, suitable for medical and other safety-critical domains, that can be hosted on the Amazon Alexa platform. Supported functionality includes context-dependent translation of incomplete utterances. We describe a preliminary evaluation on an English medical examination grammar",
    "checked": true,
    "id": "4b6451b128cfc272afe447f48fffedb6a7ed14a0",
    "semantic_title": "a robust context-dependent speech-to-speech phraselator toolkit for alexa",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/prasad18_interspeech.html": {
    "title": "Discriminating Nasals and Approximants in English Language Using Zero Time Windowing",
    "volume": "main",
    "abstract": "Nasals and approximants consonants are often confused with each other. Despite the distinction in the production mechanism, these two sound classes exhibit a similar low frequency behavior and lack significant high frequency content. The present study uses a spectral representation obtained using the zero time windowing (ZTW) analysis of speech, for the task of distinction between these two. The instantaneous spectral representation has good resolution at resonances, which helps to highlight the difference in the acoustic vocal tract system response for these sounds. The ZTW spectra around the regions of glottal closure instants are averaged to derive parameters for their classification in continuous speech. A set of parameters based on the dominant resonances, center of gravity, band energy ratio and cumulative spectral sum in low frequencies, is derived from the average spectrum. The paper proposes classification using a knowledge-based approach and training a support vector machine. These classifiers are tested on utterances from different English speakers in the TIMIT dataset. The proposed methods result in an average classification accuracy of 90% between the two classes in continuous speech",
    "checked": true,
    "id": "310e798e0df83b65908fcc31872f455e6c27a9d6",
    "semantic_title": "discriminating nasals and approximants in english language using zero time windowing",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/howson18_interspeech.html": {
    "title": "Gestural Lenition of Rhotics Captures Variation in Brazilian Portuguese",
    "volume": "main",
    "abstract": "The goal of this study is to examine the rhotics in Brazilian Portuguese (BP), /É¾,Ê/ and the â€˜archetypal' coda /R/, to determine if: (1) they can be characterized as a coordination of the tongue dorsum and tongue body or tip and (2) manipulation of the gestural settings accounts for rhotic allophony in BP. Six native speakers of BP participated in an ultrasound experiment and produced target phonemes in #CV, VCV and VC# environments with the vowels /i, e, a, o/. Tongue contours for the rhotics were compared using Smoothing Spline ANOVAs. /É¾, Ê/ were produced with a tongue body and dorsum gesture, while /É¾/ also had an apical gesture. Archetypal /R/ was realized variably, as any of [É¾, É», É¹, Ï‡]. BP rhotics can be described as the coordination of a tongue dorsum and a tongue body or tip gesture. â€˜Archetypal' /R/ is posited to be /É¾/. Allophony between /É¾/ and [É», É¹, Ï‡] is due to tongue tip lenition. Allophony between /Ê/ and [h] is due to weakening of the tongue dorsum and body gestures. This analysis suggests synchronic and diachronic changes of rhotics result from lenition. It also captures the rarity of diachronic changes from uvulars to alveolars",
    "checked": true,
    "id": "22d223b8210f3b731132468f7ef416383c79e209",
    "semantic_title": "gestural lenition of rhotics captures variation in brazilian portuguese",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/prasad18b_interspeech.html": {
    "title": "Identification and Classification of Fricatives in Speech Using Zero Time Windowing Method",
    "volume": "main",
    "abstract": "Fricatives are produced by creating a turbulence in the air-flow by passing it through a stricture in the vocal tract cavity. Fricatives are characterized by their noise-like behavior, which makes it difficult to analyze. Difference in the place of articulation leads to different classes of fricatives. Identification of fricative segment boundaries in speech helps in improving the performance of several applications. The present study attempts towards the identification and classification of fricative segments in continuous speech, based on the statistical behavior of instantaneous spectral characteristics. The proposed method uses parameters such as the dominant resonance frequencies, the center of gravity along with the statistical moments of the spectrum obtained using the zero time windowing (ZTW) method. The ZTW spectra exhibits a high temporal resolution and therefore gives accurate segment boundaries in speech. The proposed algorithm is tested on the TIMIT dataset for English language. A high identification rate of 97.5% is achieved for segment boundaries of the sibilant fricative class. Voiced nonsibilants show a lower identification rate than their voiceless counterparts due to their vowel-like spectral characteristics. A high classification rate of 93.2% is achieved between sibilants and nonsibilants",
    "checked": true,
    "id": "fa2fbdc42068ba8fae439825fdb3bb71627f6250",
    "semantic_title": "identification and classification of fricatives in speech using zero time windowing method",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chanchaochai18_interspeech.html": {
    "title": "GlobalTIMIT: Acoustic-Phonetic Datasets for the World's Languages",
    "volume": "main",
    "abstract": "Although the TIMIT acoustic-phonetic dataset ([1], [2]) was created three decades ago, it remains in wide use, with more than 20000 Google Scholar references and more than 1000 since 2017. Despite TIMIT's antiquity and relatively small size, inspection of these references shows that it is still used in many research areas: speech recognition, speaker recognition, speech synthesis, speech coding, speech enhancement, voice activity detection, speech perception, overlap detection and source separation, diagnosis of speech and language disorders and linguistic phonetics, among others. Nevertheless, comparable datasets are not available even for other widely-studied languages, much less for under-documented languages and varieties. Therefore, we have developed a method for creating TIMIT-like datasets in new languages with modest effort and cost and we have applied this method in standard Thai, standard Mandarin Chinese, English from Chinese L2 learners, the Guanzhong dialect of Mandarin Chinese and the Ga language of West Africa. Other collections are planned or underway. The resulting datasets will be published through the LDC, along with instructions and open-source tools for replicating this method in other languages, covering the steps of sentence selection and assignment to speakers, speaker recruiting and recording, proof-listening and forced alignment",
    "checked": true,
    "id": "f74c70d2727f0014ca85152667e1b494b32c50ef",
    "semantic_title": "globaltimit: acoustic-phonetic datasets for the world's languages",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hermes18_interspeech.html": {
    "title": "Structural Effects on Properties of Consonantal Gestures in Tashlhiyt",
    "volume": "main",
    "abstract": "Tashlhiyt Berber is a language in which every consonant can take up the nucleus position in a syllable. The present study investigates how gestural properties are modified when the consonants occur in different syllable positions (onset, nucleus, coda). Furthermore, the effect of higher structural components such as morphology on the respective gestural organization patterns are examined. Therefore, we collected articulographic data for different consonantal roots, such as /bdg/ and /gzm/ with varying affixes, entailing different syllabification patterns in Tashlhiyt. Consonantal properties in different syllable positions are investigated with respect to their intragestural properties and intergestural properties, i.e. bonding strength. Furthermore, gestural coherence with respect to prefixation were examined. Results reveal that consonantal gestures were not modified on the intragestural level in terms of duration, velocity, stiffness or displacement, when the morphological structure was kept constant. However, on the intergestural level syllable relation was encoded, revealing a tighter bonding for onset-nucleus relations than for heterosyllabic sequences. Furthermore, when changing the morphological marker, modifications of intragestural parameters occur, inducing temporal changes of consonantal gestures. We conclude that higher structural components should be taken into account when investigating syllable internal timing patterns",
    "checked": true,
    "id": "5b659e2827b2eb7f28eed0076334d0cea5c4cd0e",
    "semantic_title": "structural effects on properties of consonantal gestures in tashlhiyt",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kochetov18_interspeech.html": {
    "title": "The Retroflex-dental Contrast in Punjabi Stops and Nasals: A Principal Component Analysis of Ultrasound Images",
    "volume": "main",
    "abstract": "Many languages of South Asia show a phonemic contrast between retroflexes and dentals across different manners of articulation. This contrast, however, tends to be less phonetically distinct and more variable in nasals. The goal of this paper is to examine the overall similarity of the retroflex-dental contrasts in Punjabi stops and nasals. Ultrasound tongue imaging recordings were obtained from 14 Punjabi speakers producing /Êˆ,É³,t,n/ in the /ba_ab/ nonsense word context. Selected video frames were fed to a principal component analysis (PCA); the output was used for (1) training a linear discriminant model on one manner that discriminates place and (2) testing it on the other manner. The results showed 100% correct classification of the contrast (retroflex or dental) in stops and 92% correct classification in nasals in the training data. The classification was much poorer across different manners: on average 67% of stops and 57% of nasals were classified correctly based on training sets with nasals or stops, respectively. In both cases, retroflex responses were more common. These results suggest that the tongue configurations for Punjabi retroflex and dental consonants differ by manner of articulation. The contrast is also overall less robust in nasals than in stops, confirming previous reports",
    "checked": true,
    "id": "6b5cdf8e82820afe77dd1a7f483a3a702f2153c7",
    "semantic_title": "the retroflex-dental contrast in punjabi stops and nasals: a principal component analysis of ultrasound images",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yue18_interspeech.html": {
    "title": "Vowels and Diphthongs in Hangzhou Wu Chinese Dialect",
    "volume": "main",
    "abstract": "This paper gives an acoustic phonetic description of the vowels and diphthongs in Hangzhou Wu Chinese dialect. Data from 12 speakers, 6 male and 6 female, were measured and analyzed. Monophthongs were investigated in CV, CVN and CVC syllables; diphthongs were examined in terms of temporal organization, spectral properties and dynamic aspects. Results suggest that falling diphthongs tend to have a single dynamic target, while rising diphthongs have two static spectral targets",
    "checked": true,
    "id": "591a1fe2afd4787ec2ef7bc3847df084b9e3c3ec",
    "semantic_title": "vowels and diphthongs in hangzhou wu chinese dialect",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/m18b_interspeech.html": {
    "title": "Resyllabification in Indian Languages and Its Implications in Text-to-speech Systems",
    "volume": "main",
    "abstract": "Resyllabification is a phonological process in continuous speech in which the coda of a syllable is converted into the onset of the following syllable, either in the same word or in the subsequent word. This paper presents an analysis of resyllabification across words in different Indian languages and its implications in Indian language text-to-speech (TTS) synthesis systems. The evidence for resyllabification is evaluated based on the acoustic analysis of a read speech corpus of the corresponding language. This study shows that the resyllabification obeys the maximum onset principle and introduces the notion of prominence resyllabification in Indian languages. This paper finds acoustic evidence for total resyllabification. The resyllabification rules obtained are applied to TTS systems. The correctness of the rules is evaluated quantitatively by comparing the acoustic log-likelihood scores of the speech utterances with the original and resyllabified texts and by performing a pair comparison (PC) listening test on the synthesized speech output. An improvement in the log-likelihood score with the resyllabified text is observed and the synthesized speech with the resyllabified text is preferred 3 times over those without resyllabification",
    "checked": true,
    "id": "db9d1e365d8186c846a9dd52d9129e04259e9992",
    "semantic_title": "resyllabification in indian languages and its implications in text-to-speech systems",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/murphy18_interspeech.html": {
    "title": "Voice Source Contribution to Prominence Perception: Rd Implementation",
    "volume": "main",
    "abstract": "This paper explores the contribution of voice source modulation to the perception of prominence, following on previous analyses of accentuation, focus and deaccentuation. A listening test was carried out on a sentence of Irish with three accented, prominent syllables (P1, P2, P3). Using inverse filtering and resynthesis, a â€˜flattened' version was generated, with only slight declination of f0 and other voice source parameters. The global waveshape parameter Rd was modulated to provide (i) source boosting (tenser phonation) on either P1 or P2 and/or (ii) source attenuation (laxer phonation) following (Post-attenuation) or preceding (Pre-attenuation) P1 or P2. Rd variation was achieved in two different ways to generate two series of stimuli. f0 was not varied in either series. Twenty-nine listeners rated the prominence level of all syllables in the utterance. Results show that the phrasal position (P1 vs. P2) makes a large difference to prominence judgements. P1 emerged as overall more prominent and more readily â€˜enhanced' by the source modifications. Post-attenuation was particularly important for P1, with effects equal to or greater than local P-boosting. In the case of P2, Pre-attenuation was much more important than Post-attenuation",
    "checked": true,
    "id": "ae6f6b5d903588b7ad837ebf1460231a0941502e",
    "semantic_title": "voice source contribution to prominence perception: rd implementation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gobl18_interspeech.html": {
    "title": "On the Relationship between Glottal Pulse Shape and Its Spectrum: Correlations of Open Quotient, Pulse Skew and Peak Flow with Source Harmonic Amplitudes",
    "volume": "main",
    "abstract": "This paper explores the relationship between the glottal pulse amplitude (Up) and the amplitude of the first harmonic (H1), as well as the combined effects of Up, the open quotient (Oq) and degree of pulse asymmetry/skew (Rk) on the low end of the source spectrum. This serves to elucidate their relationship to the H1-H2 estimate, widely used to make inferences on changes in Oq and voice quality. It has been suggested that H1 is mainly determined by Up and that the pulse shape has a relatively small impact. To investigate this, a series of glottal pulses were generated using the LF model, where Up was kept constant, while Oq and Rk were systematically varied. The resulting harmonic amplitudes of these pulses show that Up is not the sole determinant of H1. Rather, H1 is highly dependent on Oq and to a certain degree also on Rk. Although the effects of these parameters on the lowest harmonics is rather complex, we find that the H1-H2 measure is broadly correlated with Oq. However, there is also a strong effect of differences in glottal skew, particularly at high Oq values, which could invalidate inferences on Oq and voice quality from estimates of H1-H2",
    "checked": true,
    "id": "8f7217566f9bb29cef79201e37df8f852d450e26",
    "semantic_title": "on the relationship between glottal pulse shape and its spectrum: correlations of open quotient, pulse skew and peak flow with source harmonic amplitudes",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hughes18_interspeech.html": {
    "title": "The Individual and the System: Assessing the Stability of the Output of a Semi-automatic Forensic Voice Comparison System",
    "volume": "main",
    "abstract": "Semi-automatic systems based on traditional linguistic-phonetic features are increasingly being used for forensic voice comparison (FVC) casework. In this paper, we examine the stability of the output of a semi-automatic system, based on the long-term formant distributions (LTFDs) of F1, F2 and F3, as the channel quality of the input recordings decreases. Cross-validated, calibrated GMM-UBM log likelihood-ratios (LLRs) were computed for 97 Standard Southern British English speakers under four conditions. In each condition the same speech material was used, but the technical properties of the recordings changed (high quality studio recording, landline telephone recording, high bit-rate GSM mobile telephone recording and low bit-rate GSM mobile telephone recording). Equal error rate (EER) and the log LR cost function (Cllr) were compared across conditions. System validity was found to decrease with poorer technical quality, with the largest differences in EER (21.66%) and Cllr (0.46) found between the studio and the low bit-rate GSM conditions. However, importantly, performance for individual speakers was affected differently by channel quality. Speakers that produced stronger evidence overall were found to be more variable. Mean F3 was also found to be a predictor of LLR variability, however no effects were found based on speakers' voice quality profiles",
    "checked": true,
    "id": "2a5c0514deeeb3396d6df0dd3ddce95d69b507b7",
    "semantic_title": "the individual and the system: assessing the stability of the output of a semi-automatic forensic voice comparison system",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kadiri18b_interspeech.html": {
    "title": "Breathy to Tense Voice Discrimination using Zero-Time Windowing Cepstral Coefficients (ZTWCCs)",
    "volume": "main",
    "abstract": "In this paper, we consider breathy to tense voices, which are often considered to be opposite ends of a voice quality continuum. Along with these, other aspects of a speaker's voice play an important role to convey the information to the listener such as mood, attitude and emotional state. The glottal pulse characteristics in different phonation types vary due to the tension of laryngeal muscles together with the respiratory effort. In the present study, we are deriving the features that can capture effects of excitation on the vocal tract system through a signal processing method, called as zero-time windowing (ZTW) method. The ZTW method gives the instantaneous spectrum which captures the changes in the speech production mechanism, providing higher spectral resolution. The cepstral coefficients derived from ZTW method are used for the classification of phonation types. Along with zero-time windowing cepstral coefficients (ZTWCCs), we use the excitation source features derived from zero frequency filtering (ZFF) method. The excitation features used are: strength of excitation, energy of excitation, loudness measure and ZFF signal energy. Classification experiments using ZTWCC and excitation features reveal a significant improvement in the detection of phonation type compared to the existing voice quality features and MFCC features",
    "checked": true,
    "id": "1a560f8a3ce211f0a3c219f6ca6f0600d9d1c0b7",
    "semantic_title": "breathy to tense voice discrimination using zero-time windowing cepstral coefficients (ztwccs)",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gogoi18_interspeech.html": {
    "title": "Analysis of Breathiness in Contextual Vowel of Voiceless Nasals in Mizo",
    "volume": "main",
    "abstract": "This study analyses the source characteristics of voiced and voiceless nasals in Mizo, a Tibeto-Burman language spoken in North-East India. Mizo is one of the few languages that has voiced and voiceless nasals in its phoneme inventory. This analysis is motivated by the interaction between breathiness and nasality reported in a number of speech perception studies using synthetic stimuli. However, there are no studies examining this interaction in vowels after voiced and voiceless nasals. Existing research has also documented the interaction between breathy phonation and vowel height. The current study is an acoustic analysis of breathiness in high and low vowels following voiced and voiceless nasals in Mizo. The acoustic parameter measures are: H1H2 ratio, spectral balance (SB), strength of excitation (SoE) and waveform peak factor (WPF). The values obtained for all the four acoustic measures suggest that vowels following voiceless nasals exhibit stronger acoustic characteristics associated with breathy phonation than vowels following voiced nasals. In addition, the degree of acoustic breathiness is affected by vowel height",
    "checked": true,
    "id": "1ccacd5a22b975b7c75797ec9bd6e318d1a67aea",
    "semantic_title": "analysis of breathiness in contextual vowel of voiceless nasals in mizo",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xu18c_interspeech.html": {
    "title": "Infant Emotional Outbursts Detection in Infant-parent Spoken Interactions",
    "volume": "main",
    "abstract": "Detection of infant emotional outbursts, such as crying, in large corpora of recorded infant speech, is essential to the study of dyadic social process, by which infants learn to identify and regulate their own emotions. Such large corpora now exist with the advent of LENA speech monitoring systems, but are not labeled for emotional outbursts. This paper reports on our efforts to manually code child utterances as being of type \"laugh\", \"cry\", \"fuss\", \"babble\" and \"hiccup\" and to develop algorithms capable of performing the same task automatically. Human labelers achieve much higher rates of inter-coder agreement for some of these categories than for others. Linear discriminant analysis (LDA) achieves better accuracy on tokens that have been coded by two human labelers than on tokens that have been coded by only one labeler, but the difference is not as much as we expected, suggesting that the acoustic and contextual features being used by human labelers are not yet available to the LDA. Convolutional neural network and hidden markov model achieve better accuracy than LDA, but worse F-score, because they over-weight the prior. Discounting the transition probability does not solve the problem",
    "checked": true,
    "id": "09688091a1ed5198a56c013add1cffca48d110cc",
    "semantic_title": "infant emotional outbursts detection in infant-parent spoken interactions",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cho18_interspeech.html": {
    "title": "Deep Neural Networks for Emotion Recognition Combining Audio and Transcripts",
    "volume": "main",
    "abstract": "In this paper, we propose to improve emotion recognition by combining acoustic information and conversation transcripts. On the one hand, an LSTM network was used to detect emotion from acoustic features like f0, shimmer, jitter, MFCC, etc. On the other hand, a multi-resolution CNN was used to detect emotion from word sequences. This CNN consists of several parallel convolutions with different kernel sizes to exploit contextual information at different levels. A temporal pooling layer aggregates the hidden representations of different words into a unique sequence level embedding, from which we compute the emotion posteriors. We optimized a weighted sum of classification and verification losses. The verification loss tries to bring embeddings from same emotions closer while it separates embeddings for different emotions. We also compared our CNN with state-of-the-art text-based hand-crafted features (e-vector). We evaluated our approach on the USC-IEMOCAP dataset as well as the dataset consisting of US English telephone speech. In the former, we used human transcripts while in the latter, we used ASR transcripts. The results showed fusing audio and transcript information improved unweighted accuracy by relative 24% for IEMOCAP and relative 3.4% for the telephone data compared to a single acoustic system",
    "checked": true,
    "id": "75b2843539dc8567b1502a19b3788adf6a015eb6",
    "semantic_title": "deep neural networks for emotion recognition combining audio and transcripts",
    "citation_count": 78
  },
  "https://www.isca-speech.org/archive/interspeech_2018/parthasarathy18b_interspeech.html": {
    "title": "Preference-Learning with Qualitative Agreement for Sentence Level Emotional Annotations",
    "volume": "main",
    "abstract": "The perceptual evaluation of emotional attributes is noisy due to inconsistencies between annotators. The low inter-evaluator agreement arises due to the complex nature of emotions. Conventional approaches average scores provided by multiple annotators. While this approach reduces the influence of dissident annotations, previous studies have showed the value of considering individual evaluations to better capture the underlying ground-truth. One of these approaches is the qualitative agreement (QA) method, which provides an alternative framework that captures the inherent trends amongst the annotators. While previous studies have focused on using the QA method for time-continuous annotations from a fixed number of annotators, most emotional databases are annotated with attributes at the sentence-level (e.g., one global score per sentence). This study proposes a novel formulation based on the QA framework to estimate reliable sentence-level annotations for preference-learning. The proposed relative labels between pairs of sentences capture consistent trends across evaluators. The experimental evaluation shows that preference-learning methods to rank-order emotional attributes trained with the proposed QA-based labels achieve significantly better performance than the same algorithms trained with relative scores obtained by averaging absolute scores across annotators. These results show the benefits of QA-based labels for preference-learning using sentence-level annotations",
    "checked": true,
    "id": "ab93dda9af122c2d0efdaff1feaff54d09b96687",
    "semantic_title": "preference-learning with qualitative agreement for sentence level emotional annotations",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2018/latif18b_interspeech.html": {
    "title": "Transfer Learning for Improving Speech Emotion Classification Accuracy",
    "volume": "main",
    "abstract": "The majority of existing speech emotion recognition research focuses on automatic emotion detection using training and testing data from same corpus collected under the same conditions. The performance of such systems has been shown to drop significantly in cross-corpus and cross-language scenarios. To address the problem, this paper exploits a transfer learning technique to improve the performance of speech emotion recognition systems that is novel in cross-language and cross-corpus scenarios. Evaluations on five different corpora in three different languages show that Deep Belief Networks (DBNs) offer better accuracy than previous approaches on cross-corpus emotion recognition, relative to a Sparse Autoencoder and Support Vector Machine (SVM) baseline system. Results also suggest that using a large number of languages for training and using a small fraction of the target data in training can significantly boost accuracy compared with baseline also for the corpus with limited training examples",
    "checked": true,
    "id": "766f677f4a42dcd43a856fad554c6678f642355b",
    "semantic_title": "transfer learning for improving speech emotion classification accuracy",
    "citation_count": 91
  },
  "https://www.isca-speech.org/archive/interspeech_2018/meyer18_interspeech.html": {
    "title": "What Do Classifiers Actually Learn? a Case Study on Emotion Recognition Datasets",
    "volume": "main",
    "abstract": "In supervised learning, a typical method to ensure that a classifier has desirable generalization properties, is to split the available data into training, validation and test subsets. Given a proper data split, we typically then trust our results on the test data. But what do classifiers actually learn? In this case study we show how important it is to analyze precisely the available data, its inherent dependencies w.r.t. class labels and present an example of a popular database for speech emotion recognition, where a minor change of the data split results in an accuracy decrease of about 55% absolute, leading to the conclusion that linguistic content has been learned instead of the desired speech emotions",
    "checked": true,
    "id": "6ec26ab014959cf6c62e5b79d69334767ef91909",
    "semantic_title": "what do classifiers actually learn? a case study on emotion recognition datasets",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rathner18b_interspeech.html": {
    "title": "State of Mind: Classification through Self-reported Affect and Word Use in Speech",
    "volume": "main",
    "abstract": "Human stateâ€“of-mind (SOM; e.g.: perception, cognition, attention) constantly shifts due to internal and external demands. Mental health is influenced by the habitual use of either adaptive or maladaptive SOM. Therefore, the training of conscious regulation of SOM could be promising in self-help (e- and m-health), blended care and psychotherapy. The presented study indicates that SOM can be influenced by telling personal narratives. Furthermore, SOM and narrative sentiment (positive vs. negative) can be predicted through word use. Such results lay the groundwork for the development of applications that analyse text and speech for: i) the early detection of mental health; ii) the early detection of maladaptive changes in emotion dynamics; (iii) the use of personal narratives to improve emotion regulation skills; iv) the distribution of tailored interventions; and finally, v) evaluation of therapy outcome",
    "checked": true,
    "id": "069ae43eab55150e2b54785cfdbe12b04b80ce9f",
    "semantic_title": "state of mind: classification through self-reported affect and word use in speech",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhao18c_interspeech.html": {
    "title": "Exploring Spatio-Temporal Representations by Integrating Attention-based Bidirectional-LSTM-RNNs and FCNs for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Automatic emotion recognition from speech, which is an important and challenging task in the field of affective computing, heavily relies on the effectiveness of the speech features for classification. Previous approaches to emotion recognition have mostly focused on the extraction of carefully hand-crafted features. How to model spatio-temporal dynamics for speech emotion recognition effectively is still under active investigation. In this paper, we propose a method to tackle the problem of emotional relevant feature extraction from speech by leveraging Attention-based Bidirectional Long Short-Term Memory Recurrent Neural Networks with fully convolutional networks in order to automatically learn the best spatio-temporal representations of speech signals. The learned high-level features are then fed into a deep neural network (DNN) to predict the final emotion. The experimental results on the Chinese Natural Audio-Visual Emotion Database (CHEAVD) and the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpora show that our method provides more accurate predictions compared with other existing emotion recognition algorithms",
    "checked": true,
    "id": "460c6277a9ad97b10c161c56aacc25ef71b56832",
    "semantic_title": "exploring spatio-temporal representations by integrating attention-based bidirectional-lstm-rnns and fcns for speech emotion recognition",
    "citation_count": 59
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ghahremani18b_interspeech.html": {
    "title": "End-to-end Deep Neural Network Age Estimation",
    "volume": "main",
    "abstract": "In this paper, we apply the recently proposed x-vector neural network architecture for the task of age estimation. This architecture maps a variable length utterance into a fixed dimensional embedding which retains the relevant sequence level information. This is achieved by a temporal pooling layer. From the embedding, a series of layers is applied to make predictions. The full network is trained end-to-end in a discriminative fashion. This kind of network is starting to outperform the state-of-the-art i-vector embeddings in tasks like speaker and language recognition. Motivated by this, we investigated the optimum way to train x-vectors for the age estimation task. Despite that a regression objective is typical for this task, we found that optimizing a mixture of classification and regression losses provides better results. We trained our models on the NIST SRE08 dataset and evaluated on SRE10. The proposed approach improved mean absolute error (MAE) by 12% w.r.t the i-vector baseline",
    "checked": true,
    "id": "c936edddcb803b9eb065b6128c6d0e28d5234db1",
    "semantic_title": "end-to-end deep neural network age estimation",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hebbar18_interspeech.html": {
    "title": "Improving Gender Identification in Movie Audio Using Cross-Domain Data",
    "volume": "main",
    "abstract": "Gender identification from audio is an important task for quantitative gender analysis in multimedia and to improve tasks like speech recognition. Robust gender identification requires speech segmentation that relies on accurate voice activity detection (VAD). These tasks are challenging in movie audio due to diverse and often noisy acoustic conditions. In this work, we acquire VAD labels for movie audio by aligning it with subtitle text and train a recurrent neural network model for VAD. Subsequently, we apply transfer learning to predict gender using feature embeddings obtained from a model pre-trained for large-scale audio classification. In order to account for the diverse acoustic conditions in movie audio, we use audio clips from YouTube labeled for gender. We compare the performance of our proposed method with baseline experiments that were setup to assess the importance of feature embeddings and training data used for gender identification task. For systematic evaluation, we extend an existing benchmark dataset for movie VAD, to include precise gender labels. The VAD system shows comparable results to state-of-the-art in movie domain. The proposed gender identification system outperforms existing baselines, achieving an accuracy of 85% for movie audio. We have made the data and related code publicly available",
    "checked": true,
    "id": "c21b8b1de04a7f06a4be7a5afd78e31c105aa09a",
    "semantic_title": "improving gender identification in movie audio using cross-domain data",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kabil18_interspeech.html": {
    "title": "On Learning to Identify Genders from Raw Speech Signal Using CNNs",
    "volume": "main",
    "abstract": "Automatic Gender Recognition (AGR) is the task of identifying the gender of a speaker given a speech signal. Standard approaches extract features like fundamental frequency and cepstral features from the speech signal and train a binary classifier. Inspired from recent works in the area of automatic speech recognition (ASR), speaker recognition and presentation attack detection, we present a novel approach where relevant features and classifier are jointly learned from the raw speech signal in end-to-end manner. We propose a convolutional neural networks (CNN) based gender classifier that consists of: (1) convolution layers, which can be interpreted as a feature learning stage and (2) a multilayer perceptron (MLP), which can be interpreted as a classification stage. The system takes raw speech signal as input and outputs gender posterior probabilities. Experimental studies conducted on two datasets, namely AVspoof and ASVspoof 2015, with different architectures show that with simple architectures the proposed approach yields better system than standard acoustic features based approach. Further analysis of the CNNs show that the CNNs learn formant and fundamental frequency information for gender identification",
    "checked": true,
    "id": "69dcf97ad7d0918bbace3537db6ec5ab2ae03bb6",
    "semantic_title": "on learning to identify genders from raw speech signal using cnns",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sebastian18_interspeech.html": {
    "title": "Denoising and Raw-waveform Networks for Weakly-Supervised Gender Identification on Noisy Speech",
    "volume": "main",
    "abstract": "This paper presents a raw-waveform neural network and uses it along with a denoising network for clustering in weakly-supervised learning scenarios under extreme noise conditions. Specifically, we consider language independent gender identification on a set of varied noise conditions and signal to noise ratios (SNRs). We formulate the denoising problem as a source separation task and train the system using a discriminative criterion in order to enhance output SNRs. A denoising recurrent neural network (RNN) is first trained on a small subset (roughly one-fifth) of the data for learning a speech-specific mask. The denoised speech signal is then directly fed as input to a raw-waveform convolutional neural network (CNN) trained with denoised speech. We evaluate the standalone performance of denoiser in terms of various signal-to-noise measures and discuss its contribution towards robust gender identification. An absolute improvement of 11.06% and 13.33% is achieved by the combined pipeline over the i-vector SVM baseline system for 0 dB and -5 dB SNR conditions, respectively. We further analyse the information captured by the first CNN layer in both noisy and denoised speech",
    "checked": true,
    "id": "2b99bf3b749fad4379cdcffdad999ed8cbec2dbd",
    "semantic_title": "denoising and raw-waveform networks for weakly-supervised gender identification on noisy speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/williamson18_interspeech.html": {
    "title": "The Effect of Exposure to High Altitude and Heat on Speech Articulatory Coordination",
    "volume": "main",
    "abstract": "The effects of altitude and heat on speech articulatory coordination following exercise and approximately three hours of exposure are explored. Recordings of read speech and free response speech before and after exercise in moderate altitude, moderate heat and both moderate altitude and heat are analyzed using features that characterize articulatory coordination. It is found that 1) moderate altitude causes small changes and moderate heat negligible changes to articulatory coordination features after brief exposure prior to exercise; 2) moderate altitude and heat produce similar large feature changes following exercise and longer exposure; 3) moderate altitude and heat produce larger feature changes in combination than individually immediately following exercise. Finally, using cross-validation training of a statistical classifiers, the features are sufficient to classify the four experimental conditions with an overall accuracy of 0.50 and to detect the presence of any one of the experimental conditions with an accuracy of 0.90",
    "checked": true,
    "id": "3dee55cea99f341f7440fabcc388eb7c88d6b248",
    "semantic_title": "the effect of exposure to high altitude and heat on speech articulatory coordination",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18i_interspeech.html": {
    "title": "Permutation Invariant Training of Generative Adversarial Network for Monaural Speech Separation",
    "volume": "main",
    "abstract": "We explore generative adversarial networks (GANs) for speech separation, particularly with permutation invariant training (SSGAN-PIT). Prior work demonstrates that GANs can be implemented for suppressing additive noise in noisy speech waveform and improving perceptual speech quality. In this work, we train GANs for speech separation which enhances multiple speech sources simultaneously with the permutation issue addressed by the utterance level PIT in the training of the generator network. We propose operating GANs on the power spectrum domain instead of waveforms to reduce computation. To better explore time dependencies, recurrent neural networks (RNNs) with long short-term memory (LSTM) are adopted for both generator and discriminator in this study. We evaluated SSGAN-PIT on the WSJ0 two-talker mixed speech separation task and found that SSGAN-PIT outperforms SSGAN without PIT and the neural networks based speech separation with or without PIT. The evaluation confirms the feasibility of the proposed model and training approach for efficient speech separation. The convergence behavior of permutation invariant training and adversarial training are analyzed",
    "checked": true,
    "id": "7d00ac1dc541fc8a2b5535cb7986967a508bea9d",
    "semantic_title": "permutation invariant training of generative adversarial network for monaural speech separation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18d_interspeech.html": {
    "title": "Deep Extractor Network for Target Speaker Recovery from Single Channel Speech Mixtures",
    "volume": "main",
    "abstract": "Speaker-aware source separation methods are promising workarounds for major difficulties such as arbitrary source permutation and unknown number of sources. However, it remains challenging to achieve satisfying performance provided a very short available target speaker utterance (anchor). Here we present a novel \"deep extractor network\" which creates an extractor point for the target speaker in a canonical high dimensional embedding space and pulls together the time-frequency bins corresponding to the target speaker. The proposed model is different from prior works that the carnonical embedding space encodes knowledges of both the anchor and the mixture during training phase: first, embeddings for the anchor and mixture speech are separately constructed in a primary embedding space and then combined as an input to feed-forward layers to transform to a carnonical embedding space which we discover more stable than the primary one. Experimental results show that given a very short utterance, the proposed model can efficiently recover high quality target speech from a mixture, which outperforms various baseline models, with 5.2% and 6.6% relative improvements in SDR and PESQ respectively compared with a baseline oracle deep attracor model. Meanwhile, we show it can be generalized well to more than one interfering speaker",
    "checked": true,
    "id": "792b700e1a04e9b9b4ec8b2b2bee66a6cf8ebe3e",
    "semantic_title": "deep extractor network for target speaker recovery from single channel speech mixtures",
    "citation_count": 82
  },
  "https://www.isca-speech.org/archive/interspeech_2018/he18b_interspeech.html": {
    "title": "Joint Localization and Classification of Multiple Sound Sources Using a Multi-task Neural Network",
    "volume": "main",
    "abstract": "We propose a novel multi-task neural network-based approach for joint sound source localization and speech/non-speech classification in noisy environments. The network takes raw short time Fourier transform as input and outputs the likelihood values for the two tasks, which are used for the simultaneous detection, localization and classification of an unknown number of overlapping sound sources, Tested with real recorded data, our method achieves significantly better performance in terms of speech/non-speech classification and localization of speech sources, compared to method that performs localization and classification separately. In addition, we demonstrate that incorporating the temporal context can further improve the performance",
    "checked": true,
    "id": "8ddc1c11c94259cd8e7d85dfdb7337155c627fb6",
    "semantic_title": "joint localization and classification of multiple sound sources using a multi-task neural network",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yang18_interspeech.html": {
    "title": "Detection of Glottal Closure Instants from Speech Signals: A Convolutional Neural Network Based Method",
    "volume": "main",
    "abstract": "Most conventional methods to detect glottal closure instants (GCI) are based on signal processing technologies and different GCI candidate selection methods. This paper proposes a classification method to detect glottal closure instants from speech waveforms using convolutional neural network (CNN). The procedure is divided into two successive steps. Firstly, a low-pass filtered signal is computed, whose negative peaks are taken as candidates for GCI placement. Secondly, a CNN-based classification model determines for each peak whether it corresponds to a GCI or not. The method is compared with three existing GCI detection algorithms on two publicly available databases. For the proposed method, the detection accuracy in terms of F1-score is 98.23%. Additional experiment indicates that the model can perform better after trained with the speech data from the speakers who are the same as those in the test set",
    "checked": true,
    "id": "22cae7f1a4918f66bbe076896e1f05af8f810346",
    "semantic_title": "detection of glottal closure instants from speech signals: a convolutional neural network based method",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18g_interspeech.html": {
    "title": "Robust TDOA Estimation Based on Time-Frequency Masking and Deep Neural Networks",
    "volume": "main",
    "abstract": "Deep learning based time-frequency (T-F) masking has dramatically advanced monaural speech separation and enhancement. This study investigates its potential for robust time difference of arrival (TDOA) estimation in noisy and reverberant environments. Three novel algorithms are proposed to improve the robustness of conventional cross-correlation-, beamforming- and subspace-based algorithms for speaker localization. The key idea is to leverage the power of deep neural networks (DNN) to accurately identify T-F units that are relatively clean for TDOA estimation. All of the proposed algorithms exhibit strong robustness for TDOA estimation in environments with low input SNR, high reverberation and low direction-to-reverberant energy ratio",
    "checked": true,
    "id": "0c4603e0bc53a133560b81b5f76e58a9819c359f",
    "semantic_title": "robust tdoa estimation based on time-frequency masking and deep neural networks",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kato18_interspeech.html": {
    "title": "Waveform to Single Sinusoid Regression to Estimate the F0 Contour from Noisy Speech Using Recurrent Deep Neural Networks",
    "volume": "main",
    "abstract": "The fundamental frequency (F0) represents pitch in speech that determines prosodic characteristics of speech and is needed in various tasks for speech analysis and synthesis. Despite decades of research on this topic, F0 estimation at low signal-to-noise ratios (SNRs) in unexpected noise conditions remains difficult. This work proposes a new approach to noise robust F0 estimation using a recurrent neural network (RNN) trained in a supervised manner. Recent studies employ deep neural networks (DNNs) for F0 tracking as a frame-by-frame classification task into quantised frequency states but we propose waveform-to-sinusoid regression instead to achieve both noise robustness and accurate estimation with increased frequency resolution. Experimental results with PTDB-TUG corpus contaminated by additive noise (NOISEX-92) demonstrate that the proposed method improves gross pitch error (GPE) rate and fine pitch error (FPE) by more than 35% at SNRs between -10 dB and +10 dB compared with well-known noise robust F0 tracker, PEFAC. Furthermore, the proposed method also outperforms state-of-the-art DNN-based approaches by more than 15% in terms of both FPE and GPE rate over the preceding SNR range",
    "checked": true,
    "id": "3f6fb909f4d167c7d1fab2cf331ff32d652b084d",
    "semantic_title": "waveform to single sinusoid regression to estimate the f0 contour from noisy speech using recurrent deep neural networks",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2018/magron18b_interspeech.html": {
    "title": "Reducing Interference with Phase Recovery in DNN-based Monaural Singing Voice Separation",
    "volume": "main",
    "abstract": "State-of-the-art methods for monaural singing voice separation consist in estimating the magnitude spectrum of the voice in the short-time Fourier transform (STFT) domain by means of deep neural networks (DNNs). The resulting magnitude estimate is then combined with the mixture's phase to retrieve the complex-valued STFT of the voice, which is further synthesized into a time-domain signal. However, when the sources overlap in time and frequency, the STFT phase of the voice differs from the mixture's phase, which results in interference and artifacts in the estimated signals. In this paper, we investigate on recent phase recovery algorithms that tackle this issue and can further enhance the separation quality. These algorithms exploit phase constraints that originate from a sinusoidal model or from consistency, a property that is a direct consequence of the STFT redundancy. Experiments conducted on real music songs show that those algorithms are efficient for reducing interference in the estimated voice compared to the baseline approach",
    "checked": true,
    "id": "67c5f07208a9216b9e9e503067bd75b2559891ff",
    "semantic_title": "reducing interference with phase recovery in dnn-based monaural singing voice separation",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hua18_interspeech.html": {
    "title": "Nebula: F0 Estimation and Voicing Detection by Modeling the Statistical Properties of Feature Extractors",
    "volume": "main",
    "abstract": "A F0 and voicing status estimation algorithm for high quality speech analysis/synthesis is proposed. This problem is approached from a different perspective that models the behavior of feature extractors under noise, instead of directly modeling speech signals. Under time-frequency locality assumptions, the joint distribution of extracted features and target F0 can be characterized by training a bank of Gaussian mixture models (GMM) on artificial data generated from Monte-Carlo simulations. The trained GMMs can then be used to generate a set of conditional distributions on the predicted F0, which are then combined and post-processed by Viterbi algorithm to give a final F0 trajectory. Evaluation on CSTR and CMU Arctic speech databases shows that the proposed method, trained on fully synthetic data, achieves lower gross error rates than state-of-the-art methods",
    "checked": true,
    "id": "b00d4652ee4ff92d495a869c65e7ebc06d306e35",
    "semantic_title": "nebula: f0 estimation and voicing detection by modeling the statistical properties of feature extractors",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/luo18c_interspeech.html": {
    "title": "Real-time Single-channel Dereverberation and Separation with Time-domain Audio Separation Network",
    "volume": "main",
    "abstract": "We investigate the recently proposed Time-domain Audio Separation Network (TasNet) in the task of real-time single-channel speech dereverberation. Unlike systems that take time-frequency representation of the audio as input, TasNet learns an adaptive front-end in replacement of the time-frequency representation by a time-domain convolutional non-negative autoencoder. We show that by formulating the dereverberation problem as a denoising problem where the direct path is separated from the reverberations, a TasNet denoising autoencoder can outperform a deep LSTM baseline on log-power magnitude spectrogram input in both causal and non-causal settings. We further show that adjusting the stride size in the convolutional autoencoder helps both the dereverberation and separation performance",
    "checked": true,
    "id": "219eb2745a272f29c1b417eb247ef0bc5df9e3cd",
    "semantic_title": "real-time single-channel dereverberation and separation with time-domain audio separation network",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kumar18c_interspeech.html": {
    "title": "Music Source Activity Detection and Separation Using Deep Attractor Network",
    "volume": "main",
    "abstract": "In music signal processing, singing voice detection and music source separation are widely researched topics. Recent progress in deep neural network based source separation has advanced the state of the performance in the problem of vocal and instrument separation, while the problem of joint source activity detection and separation remains unexplored. In this paper, we propose an approach to perform source activity detection using the high-dimensional embedding generated by Deep Attractor Network (DANet) when trained for music source separation. By defining both tasks together, DANet is able to dynamically estimate the number of outputs depending on the active sources. We propose an Expectation-Maximization (EM) training paradigm for DANet which further improves the separation performance of the original DANet. Experiments show that our network achieves higher source separation and comparable source activity detection against a baseline system",
    "checked": true,
    "id": "1f3d1f81880e5d8a5a69a23e61fef9467f9f639e",
    "semantic_title": "music source activity detection and separation using deep attractor network",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yang18d_interspeech.html": {
    "title": "Improving Mandarin Tone Recognition Using Convolutional Bidirectional Long Short-Term Memory with Attention",
    "volume": "main",
    "abstract": "Automatic tone recognition is useful for Mandarin spoken language processing. However, the complex F0 variations from the tone co-articulations and the interplay effects among tonality make it rather difficult to perform tone recognition of Chinese continuous speech. This paper explored the application of Bidirectional Long Short-Term Memory (BLSTM), which had the capability of modeling time series, to Mandarin tone recognition to handle the tone variations in continuous speech. In addition, we introduced attention mechanism to guide the model to select the suitable context information. The experimental results showed that the performance of proposed CNN-BLSTM with attention mechanism was the best and it achieved the tone error rate (TER) of 9.30% with a 17.6% relative error reduction from the DNN baseline system with TER of 11.28%. It demonstrated that our proposed model was more effective to handle the complex F0 variations than other models",
    "checked": true,
    "id": "6786522b75e6dfb27f2fd117166fa45d6b55036e",
    "semantic_title": "improving mandarin tone recognition using convolutional bidirectional long short-term memory with attention",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vanson18_interspeech.html": {
    "title": "Vowel Space as a Tool to Evaluate Articulation Problems",
    "volume": "main",
    "abstract": "Treatment for oral tumors can lead to long term changes in the anatomy and physiology of the vocal tract and result in problems with articulation. There are currently no readily available automatic methods to evaluate changes in articulation. We developed a Praat script which plots and measures vowel space coverage. The script reproduces speaker specific vowel space use and speaking-style dependent vowel reduction in normal speech from a Dutch corpus. Speaker identity and speaking style explain more than 60% of the variance in the measured area of the vowel triangle. In recordings of patients treated for oral tumors, vowel space use before and after treatment is still significantly correlated. Articulation before and after treatment is evaluated in a listening experiment and from a maximal articulation speed task. Linear models can explain 50-75% of variance in perceptual ratings and relative articulation rate from values at previous recordings and vowel space measures",
    "checked": true,
    "id": "36d22629fa4a3ef064f1ef86a9eb566fa20bda13",
    "semantic_title": "vowel space as a tool to evaluate articulation problems",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2018/delvaux18_interspeech.html": {
    "title": "Towards a Better Characterization of Parkinsonian Speech: A Multidimensional Acoustic Study",
    "volume": "main",
    "abstract": "This paper reports on a first attempt at adopting a new perspective in characterizing speech disorders in Parkinson's Disease (PD) based on individual patient profiles. Acoustic data were collected on 13 Belgian French speakers with PD, 6 male, 7 female, aged 45-81 and 50 healthy controls (HC) using the \"MonPaGe\" protocol (Fougeron et al., LREC18). In this protocol, various kinds of linguistic material are recorded in different speech conditions, in order to assess multiple speech dimensions for each speaker. First, we compared a variety of voice and speech parameters across groups (HC vs. PD patients). Second, we examined individual profiles of PD patients. Results showed that as a group PD participants most systematically differed from HC in terms of speech tempo and rythm. Moreover, the analysis of individual profiles revealed that other parameters, related to pneumophonatory control and linguistic prosody, were valuable to describe the speech specificities of several PD patients",
    "checked": true,
    "id": "26c2a63a20c59522a3837d19d31df783c45bb1c7",
    "semantic_title": "towards a better characterization of parkinsonian speech: a multidimensional acoustic study",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kalita18_interspeech.html": {
    "title": "Self-similarity Matrix Based Intelligibility Assessment of Cleft Lip and Palate Speech",
    "volume": "main",
    "abstract": "This work presents a comparison based framework by exploiting the self-similarity matrices matching technique to estimate the speech intelligibility of cleft lip and palate (CLP) children. Self-similarity matrix (SSM) of a feature sequence is a square matrix, which encodes the acoustic-phonetic composition of the underlying speech signal. Deviations in the acoustic characteristics of underlying sound units due to the degradation of intelligibility will deviate the CLP speech's SSM structure from that of normal. This degree of deviations in CLP speech's SSM from the corresponding normal speech's SSM may provide information about the severity profile of speech intelligibility. The degree of deviations is quantified using the structural similarity (SSIM) index, which is considered as the representative of objective intelligibility score. The proposed method is evaluated using two parameterizations of speech signals: Mel-frequency cepstral coefficients and Gaussian posteriorgrams and compared with dynamic time warping (DTW) based intelligibility assessment method. The proposed SSM based method shows the better correlation with the perceptual ratings of intelligibility when compared to the DTW based method",
    "checked": true,
    "id": "32f9eace2f896fcabc64a0a0ab52a718ea5ea618",
    "semantic_title": "self-similarity matrix based intelligibility assessment of cleft lip and palate speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dubey18b_interspeech.html": {
    "title": "Pitch-Adaptive Front-end Feature for Hypernasality Detection",
    "volume": "main",
    "abstract": "Hypernasality in cleft palate (CP) children is due to the velopharyngeal insufficiency. The vowels get nasalized in hypernasal speech and the nasality evidence are mainly present in low-frequency region around the first formant (F1) of vowels. The detection of hypernasality using Mel-frequency cepstral coefficient (MFCC) feature may get affected because the feature might not be able to capture the nasality evidence present in the low-frequency region. This is due to the fact that the MFCC feature extracted from high pitched children speech contains the pitch harmonics effect of magnitude spectrum. The pitch harmonics effect results in high variance for the higher dimensions of MFCC coefficients. This problem may increase due to high perturbation in pitch of CP speech. So in this work, a pitch-adaptive MFCC feature is used for hypernasality detection. The feature is derived from the cepstral smooth spectrum instead of magnitude spectrum. A pitch-adaptive low time liftering is done to smooth out the pitch harmonics. This feature when used for the detection of hypernasality using support vector machine (SVM) gives an accuracy of 83.45%, 88.04% and 85.58% for /a/, /i/ and /u/ vowels respectively, which is better than the accuracy of MFCC feature",
    "checked": true,
    "id": "247506fff04119ab323dd8627d7fa25dd93ea99a",
    "semantic_title": "pitch-adaptive front-end feature for hypernasality detection",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/norel18_interspeech.html": {
    "title": "Detection of Amyotrophic Lateral Sclerosis (ALS) via Acoustic Analysis",
    "volume": "main",
    "abstract": "ALS is a fatal neurodegenerative disease with no cure. Experts typically measure disease progression via the ALSFRS-R score, which includes measurements of various abilities known to decline. We propose instead the use of speech analysis as a proxy for ALS progression. This technique enables 1) frequent non-invasive, inexpensive, longitudinal analysis, 2) analysis of data recorded in the wild and 3) creation of an extensive ALS databank for future analysis. Patients and trained medical professionals need not be co-located, enabling more frequent monitoring of more patients from the convenience of their own homes. The goals of this study are the identification of acoustic speech features in naturalistic contexts which characterize disease progression and development of machine models which can recognize the presence and severity of the disease. We evaluated subjects from the Prize4Life Israel dataset, using a variety of frequency, spectral and voice quality features. The dataset was generated using the ALS Mobile Analyzer, a cell-phone app that collects data regarding disease progress using a self-reported ALSFRS-R questionnaire and several active tasks that measure speech and motor skills. Classification via leave-five-subjects-out cross-validation resulted in an accuracy rate of 79% (61% chance) for males and 83% (52% chance) for females",
    "checked": true,
    "id": "1aee4a79913fb155d7da1fb78e1b91b4292034c4",
    "semantic_title": "detection of amyotrophic lateral sclerosis (als) via acoustic analysis",
    "citation_count": 35
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vikram18c_interspeech.html": {
    "title": "Detection of Glottal Activity Errors in Production of Stop Consonants in Children with Cleft Lip and Palate",
    "volume": "main",
    "abstract": "Individuals with cleft lip and palate (CLP) alter the glottal activity characteristics during the production of stop consonants. The presence/absence of glottal vibrations during the production of unvoiced/voiced stops is referred as glottal activity error (GAE). In this work, acoustic-phonetic and production based knowledge of stop consonants are exploited to propose an algorithm for the automatic detection of GAE. The algorithm uses zero frequency filtered and band-pass (500-4000 Hz) filtered speech signals to identify the syllable nuclei positions, followed by the detection of glottal activity characteristics of consonant present within the syllable. Based on the identified glottal activity characteristics of consonant and a priori voicing information of target stop consonant, the presence or absence of GAE is detected. The algorithm is evaluated over the database containing the responses of normal children and children with repaired CLP for the target consonant-vowel-consonant-vowel words with stop consonants",
    "checked": true,
    "id": "0feee566d7c56c2cad94d9cee9ec2d4717fb319f",
    "semantic_title": "detection of glottal activity errors in production of stop consonants in children with cleft lip and palate",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sriram18_interspeech.html": {
    "title": "Cold Fusion: Training Seq2Seq Models Together with Language Models",
    "volume": "main",
    "abstract": "Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold Fusion are able to better utilize language information enjoying i) faster convergence and better generalization and ii) almost complete transfer to a new domain while using less than 10% of the labeled training data",
    "checked": true,
    "id": "33125ec92a0b4b1687ccd153762d6275668e3d09",
    "semantic_title": "cold fusion: training seq2seq models together with language models",
    "citation_count": 241
  },
  "https://www.isca-speech.org/archive/interspeech_2018/irie18_interspeech.html": {
    "title": "Investigation on Estimation of Sentence Probability by Combining Forward, Backward and Bi-directional LSTM-RNNs",
    "volume": "main",
    "abstract": "A combination of forward and backward long short-term memory (LSTM) recurrent neural network (RNN) language models is a popular model combination approach to improve the estimation of the sequence probability in the second pass N-best list rescoring in automatic speech recognition (ASR). In this work, we further push such an idea by proposing a combination of three models: a forward LSTM language model, a backward LSTM language model and a bi-directional LSTM based gap completion model. We derive such a combination method from a forward backward decomposition of the sequence probability. We carry out experiments on the Switchboard speech recognition task. While we empirically find that such a combination gives slight improvements in perplexity over the combination of forward and backward models, we finally show that a combination of the same number of forward models gives the best perplexity and word error rate (WER) overall",
    "checked": true,
    "id": "5ceb3cf83107c22180d862362bde260d032a1cfe",
    "semantic_title": "investigation on estimation of sentence probability by combining forward, backward and bi-directional lstm-rnns",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zenkel18_interspeech.html": {
    "title": "Subword and Crossword Units for CTC Acoustic Models",
    "volume": "main",
    "abstract": "This paper proposes a novel approach to create a unit set for CTC-based speech recognition systems. By using Byte-Pair Encoding we learn a unit set of an arbitrary size on a given training text. In contrast to using characters or words as units this allows us to find a good trade-off between the size of our unit set and the available training data. We investigate both Crossword units, that may span multiple word and Subword units. By evaluating these unit sets with decodings methods using a separate language model we are able to show improvements over a purely character-based unit set",
    "checked": true,
    "id": "223d74b4f401cdb0bcb31467c8cfecbf46f65795",
    "semantic_title": "subword and crossword units for ctc acoustic models",
    "citation_count": 34
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tanaka18_interspeech.html": {
    "title": "Neural Error Corrective Language Models for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "We present novel neural network based language models that can correct automatic speech recognition (ASR) errors by using speech recognizer output as a context. These models, called neural error corrective language models (NECLMs), utilizes ASR hypotheses of a target utterance as a context for estimating the generative probability of words. NECLMs are expressed as conditional generative models composed of an encoder network and a decoder network. In the models, the encoder network constructs context vectors from N-best lists and ASR confidence scores generated in a speech recognizer. The decoder network rescores recognition hypotheses by computing a generative probability of words using the context vectors so as to correct ASR errors. We evaluate the proposed models in Japanese lecture ASR tasks. Experimental results show that NECLM achieve better ASR performance than a state-of-the-art ASR system that incorporate a convolutional neural network acoustic model and a long short-term memory recurrent neural network language model",
    "checked": true,
    "id": "8ffc6d3c3af7a2c3489773db85beb61f01357a51",
    "semantic_title": "neural error corrective language models for automatic speech recognition",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rasooli18_interspeech.html": {
    "title": "Entity-Aware Language Model as an Unsupervised Reranker",
    "volume": "main",
    "abstract": "In language modeling, it is difficult to incorporate entity relationships from a knowledge-base. One solution is to use a reranker trained with global features, in which global features are derived from n-best lists. However, training such a reranker requires manually annotated n-best lists, which is expensive to obtain. We propose a method based on the contrastive estimation method that alleviates the need for such data. Experiments in the music domain demonstrate that global features, as well as features extracted from an external knowledge-base, can be incorporated into our reranker. Our final model, a simple ensemble of a language model and reranker, achieves a 0.44% absolute word error rate improvement over an LSTM language model on the blind test data",
    "checked": true,
    "id": "04c477a0e2ca828620753be89d1ffb465c851755",
    "semantic_title": "entity-aware language model as an unsupervised reranker",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/choi18_interspeech.html": {
    "title": "Character-level Language Modeling with Gated Hierarchical Recurrent Neural Networks",
    "volume": "main",
    "abstract": "Recurrent neural network (RNN)-based language models are widely used for speech recognition and translation applications. We propose a gated hierarchical recurrent neural network (GHRNN) and apply it to the character-level language modeling. GHRNN consists of multiple RNN units that operate with different time scales and the frequency of operation at each unit is controlled by the learned gates from training data. In our model, GHRNN learns the hierarchical structure of character, sub-word and word. Timing gates are included in the hierarchical connections to control the operating frequency of these units. The performance was measured for Penn Treebank and Wikitext-2 datasets. Experimental results showed lower bit per character (BPC) when compared to simply layered or skip-connected RNN models. Also, when a continuous cache model is added, the BPC of 1.192 is recorded, which is comparable to the state of the art result",
    "checked": true,
    "id": "852ffd44950f8a9433f4c31ddb3c843031188166",
    "semantic_title": "character-level language modeling with gated hierarchical recurrent neural networks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/levitan18_interspeech.html": {
    "title": "Acoustic-Prosodic Indicators of Deception and Trust in Interview Dialogues",
    "volume": "main",
    "abstract": "We analyze a set of acoustic-prosodic features in both truthful and deceptive responses to interview questions, identifying differences between truthful and deceptive speech. We also study the perception of deception, identifying acoustic-prosodic characteristics of speech that is perceived as truthful or deceptive by interviewers. In addition to studying differences across all speakers, we identify individual variations in deception production and perception across gender and native language. We conduct machine learning classification experiments aimed at distinguishing between truthful and deceptive speech, using acoustic-prosodic features. We also explore methods of leveraging individual traits for deception classification. Our results show that acoustic-prosodic features are highly effective at classifying deceptive speech. Our best classifier achieved an F1-score of 72.77, well above both the random baseline and above human performance at this task. This work advances our understanding of deception production and perception and has implications for automatic deception detection and the development of synthesized speech that is trustworthy",
    "checked": true,
    "id": "662c68980546ba71f379ed0f3f5b064204edc6ef",
    "semantic_title": "acoustic-prosodic indicators of deception and trust in interview dialogues",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2018/an18b_interspeech.html": {
    "title": "Deep Personality Recognition for Deception Detection",
    "volume": "main",
    "abstract": "Researchers in both psychology and computer science have suggested that modeling individual differences may improve the performance of automatic deception detection systems. In this study, we fuse a personality classifier with a deception classifier and evaluate various ways to combine the two tasks, either as a single network with shared layers, or by feeding the results of the personality classifier into the deception classifier. We show that including personality recognition improves the performance of deception detection on the Columbia X-Cultural Deception (CXD) corpus by more than 6% relative, achieving new state-of-the-art results on classification of phrase-like units in this corpus",
    "checked": true,
    "id": "8f09d590dbc8f11b8f2e3be7888f83479c59ee59",
    "semantic_title": "deep personality recognition for deception detection",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mixdorff18_interspeech.html": {
    "title": "Cross-cultural (A)symmetries in Audio-visual Attitude Perception",
    "volume": "main",
    "abstract": "This paper evaluates results from a cross-cultural and cross-language experiment series employing short audio-visual utterances produced with varying attitudinal expressions. German and Cantonese-speaking participants freely labeled such utterances in the two languages and assigned to each stimulus a verbal label. Based on the results of the four experiments we were able to establish to what degree the attitudinal frames of reference of the two groups overlap and how they differ. Verbal labels were assessed regarding their emotional content in terms of valence, activation and dominance and for the linguistic opposition between assertive and interrogative speech act and hence permit to abstract from the language of the rater and ultimately even abstract from the attitudinal categories used when eliciting the stimuli. Instead we regard each utterance as a data-point in the emotional space. We found that the judgments of the two rater groups agree well with respect to the valence of attitudinal expressions and diverge most as to the perceived activation of the stimulus presenter. Cantonese speaking participants seem to mirror Germans' ratings of German stimuli better than vice versa, which suggests an interesting asymmetry of attitudinal perception. As for the modality of presentation, the audio channel primarily transmits linguistically relevant information regarding the opposition of assertion and interrogation while the visual information signals the emotional content",
    "checked": true,
    "id": "197c76cfb2d456c590f5e381f4a83c4b42034cf1",
    "semantic_title": "cross-cultural (a)symmetries in audio-visual attitude perception",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/haider18_interspeech.html": {
    "title": "An Active Feature Transformation Method for Attitude Recognition of Video Bloggers",
    "volume": "main",
    "abstract": "Video blogging is a form of unidirectional communication where a video blogger expresses his/her opinion about different issues. The success of a video blog is measured using metrics like the number of views and comments by online viewers. Researchers have highlighted the importance of non-verbal behaviours (e.g. attitudes) in the context of video blogging and showed that it correlates with the level of attention (number of views) gained by a video blog. Therefore, an automatic attitude recognition system can help potential video bloggers to train their attitudes. It can also be useful in developing video blogs summarization and search tools. This study proposes a novel Active Feature Transformation (AFT) method for automatic recognition of attitudes (a form of non-verbal behaviour) in video blogs. The proposed method transforms the Mel-frequency Cepstral Coefficient (MFCC) features for the classification task. The Principal Component Analysis (PCA) transformation is also used for comparison. Our results show that AFT outperforms PCA in terms of accuracy and dimensionality reduction for attitude recognition using linear discrimination analysis, 1-nearest neighbour and decision tree classifiers",
    "checked": true,
    "id": "6b8e35afd732788fad11e5907e15820e99569e91",
    "semantic_title": "an active feature transformation method for attitude recognition of video bloggers",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tsai18_interspeech.html": {
    "title": "Automatic Assessment of Individual Culture Attribute of Power Distance Using a Social Context-Enhanced Prosodic Network Representation",
    "volume": "main",
    "abstract": "Culture is a collective social norm of human societies that often influences a person's values, thoughts and social behaviors during interactions at an individual level. In this work, we present a computational analysis toward automatic assessing an individual's culture attribute of power distance, i.e., a measure of his/her belief about status, authority and power in organizations, by modeling their expressive prosodic structures during social encounters with people of different power status. Specifically, we propose a center-loss embedded network architecture to jointly consider the effect of social interaction contexts on individuals' prosodic manifestations in order to learn an enhanced representation for power distance recognition. Our proposed prosodic network achieves an overall accuracy of 78.6% in binary classification task of recognizing high versus low power distance. Our experiment demonstrates an improved discriminability (17.6% absolute improvement) over prosodic neural network without social context enhancement. Further visualization reveals that the diversity in the prosodic manifestation for individuals with low power distance seems to be higher than those of high power distance",
    "checked": true,
    "id": "b6208c79cb90792bce4a2d00b1d1138bd91e681e",
    "semantic_title": "automatic assessment of individual culture attribute of power distance using a social context-enhanced prosodic network representation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kadiri18c_interspeech.html": {
    "title": "Analysis and Detection of Phonation Modes in Singing Voice using Excitation Source Features and Single Frequency Filtering Cepstral Coefficients (SFFCC)",
    "volume": "main",
    "abstract": "In this study, classification of the phonation modes in singing voice is carried out. Phonation modes in singing voice can be described using four categories: breathy, neutral, flow and pressed phonations. Previous studies on the classification of phonation modes use voice quality features derived from inverse filtering which lack in accuracy. This is due to difficulty in deriving the excitation source features using inverse filtering from singing voice. We propose to use the excitation source features that are derived directly from the signal. It is known that, the characteristics of the excitation source vary in different phonation types due to the vibration of the vocal folds together with the respiratory effort (lungs effort). In the present study, we are exploring excitation source features derived from the modified zero frequency filtering (ZFF) method. Apart from excitation source features, we also explore cepstral coefficients derived from single frequency filtering (SFF) method for the analysis and classification of phonation types in singing voice",
    "checked": true,
    "id": "31c004ae166b17349a366030b1fa255fe90cc4ed",
    "semantic_title": "analysis and detection of phonation modes in singing voice using excitation source features and single frequency filtering cepstral coefficients (sffcc)",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wu18b_interspeech.html": {
    "title": "A Deep Learning Method for Pathological Voice Detection Using Convolutional Deep Belief Networks",
    "volume": "main",
    "abstract": "Automatically detecting pathological voice disorders such as vocal cord paralysis or Reinke's edema is an important medical classification problem. While deep learning techniques have achieved significant progress in the speech recognition field, there has been less research work in the area of pathological voice disorders detection. A novel system for pathological voice detection using Convolutional Neural Network (CNN) as the basic architecture is presented in this work. The novel system uses spectrograms of normal and pathological speech recordings as the input to the network. Initially Convolutional Deep Belief Network (CDBN) are used to pre-train the weights of CNN system. This acts as a generative model to explore the structure of the input data using statistical methods. Then a CNN is trained using supervised back-propagation learning algorithm to fine tune the weights. Results show that a small amount of data can be used to achieve good results in classification with this deep learning approach. A performance analysis of the novel method is provided using real data from the Saarbrucken Voice database",
    "checked": true,
    "id": "e298e2eb7d731b6335581e52ba8e2f4db8c4a8d8",
    "semantic_title": "a deep learning method for pathological voice detection using convolutional deep belief networks",
    "citation_count": 48
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bhat18_interspeech.html": {
    "title": "Dysarthric Speech Recognition Using Time-delay Neural Network Based Denoising Autoencoder",
    "volume": "main",
    "abstract": "Dysarthria is a manisfestation of the disruption in the neuro-muscular physiology resulting in uneven, slow, slurred, harsh or quiet speech. Dysarthric speech poses serious challenges to automatic speech recognition, considering this speech is difficult to decipher for both humans and machines. The objective of this work is to enhance dysarthric speech features to match that of healthy control speech. We use a Time-Delay Neural Network based Denoising Autoencoder (TDNN-DAE) to enhance the dysarthric speech features. The dysarthric speech thus enhanced is recognized using a DNN-HMM based Automatic Speech Recognition (ASR) engine. This methodology was evaluated for speaker-independent (SI) and speaker-adapted (SA) systems. Absolute improvements of 13% and 3% was observed in the ASR performance for SI and SA systems respectively as compared with unenhanced dysarthric speech recognition",
    "checked": true,
    "id": "aa678e23f0b6c2b23528f6871a3d07a4c3aa1e61",
    "semantic_title": "dysarthric speech recognition using time-delay neural network based denoising autoencoder",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vasquezcorrea18_interspeech.html": {
    "title": "A Multitask Learning Approach to Assess the Dysarthria Severity in Patients with Parkinson's Disease",
    "volume": "main",
    "abstract": "Parkinson's disease is a neurodegenerative disorder characterized by a variety of motor and non-motor symptoms. Particularly, several speech impairments appear in the initial stages of the disease, which affect aspects related to respiration and the movement of muscles and limbs in the vocal tract. Most of the studies in the literature aim to assess only one specific task from the patients, such as the classification of patients vs. healthy speakers, or the assessment of the neurological state of the patients. This study proposes a multitask learning approach based on convolutional neural networks to assess at the same time several speech deficits of the patients. A total of eleven speech aspects are considered, including difficulties of the patients to move articulators such as lips, palate, tongue and larynx. According to the results, the proposed approach improves the generalization of the convolutional network, producing more representative feature maps to assess the different speech symptoms of the patients. The multitask learning scheme improves in of up to 4% the average accuracy relative to single networks trained to assess each individual speech aspect",
    "checked": true,
    "id": "71305f454431447d68384c42189858eb4fb2f770",
    "semantic_title": "a multitask learning approach to assess the dysarthria severity in patients with parkinson's disease",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lilley18_interspeech.html": {
    "title": "The Use of Machine Learning and Phonetic Endophenotypes to Discover Genetic Variants Associated with Speech Sound Disorder",
    "volume": "main",
    "abstract": "Thirty-four (34) children with reported speech sound disorders (SSD) were recruited for a prior study, as well as 31 of their siblings, many of whom also showed SSD. Using data-clustering techniques, we assigned each child to one or more endophenotypes defined by the number and type of speech errors made on the GFTA-2. The genetic samples of 53 of the participants underwent whole exome sequencing. Variant alleles were detected, filtered and annotated from the sequences and the data were filtered using quality checks, annotations and phenotypes. We then used Random Forest classification to search for associations between variants and endophenotypes. In this preliminary report, we highlight one promising association with a common variant of COMT, a dopamine metabolizer in the brain",
    "checked": true,
    "id": "61dc37b1f5e589222a6dde03e181aa124fad8199",
    "semantic_title": "the use of machine learning and phonetic endophenotypes to discover genetic variants associated with speech sound disorder",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/moore18_interspeech.html": {
    "title": "Whistle-blowing ASRs: Evaluating the Need for More Inclusive Speech Recognition Systems",
    "volume": "main",
    "abstract": "Speech is a complex process that can break in many different ways and lead to a variety of voice disorders. Dysarthria is a voice disorder where individuals are unable to control one or more of the aspects of speechâ€”the articulation, breathing, voicing, or prosodyâ€”leading to less intelligible speech. In this paper, we evaluate the accuracy of state-of-the-art automatic speech recognition systems (ASRs) on two dysarthric speech datasets and compare the results to ASR performance on control speech. The limits of ASR performance using different voices have not been explored since the field has shifted from generative models of speech recognition to deep neural network architectures. To test how far the field has come in recognizing disordered speech, we test two different ASR systems: (1) Carnegie Mellon University's Sphinx Open Source Recognition and (2) GoogleÂ®Speech Recognition. While (1) uses generative models of speech recognition, (2) uses deep neural networks. As expected, while (2) achieved lower word error rates (WER) on dysarthric speech than (1), control speech had a WER 59% lower than dysarthric speech. Future studies should be focused not only on making ASRs robust to environmental noise, but also more robust to different voices",
    "checked": true,
    "id": "0fdb6e1bece1ae627ed3d112b62f099daeb36ff2",
    "semantic_title": "whistle-blowing asrs: evaluating the need for more inclusive speech recognition systems",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vachhani18_interspeech.html": {
    "title": "Data Augmentation Using Healthy Speech for Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "Dysarthria refers to a speech disorder caused by trauma to the brain areas concerned with motor aspects of speech giving rise to effortful, slow, slurred or prosodically abnormal speech. Traditional Automatic Speech Recognizers (ASR) perform poorly on dysarthric speech recognition tasks, owing mostly to insufficient dysarthric speech data. Speaker related challenges complicates data collection process for dysarthric speech. In this paper, we explore data augmentation using temporal and speed modifications of healthy speech to simulate dysarthric speech. DNN-HMM based Automatic Speech Recognition (ASR) and Random Forest based classification were used for evaluation of the proposed method. Dysarthric speech generated synthetically is classified for severity using a Random Forest classifier that is trained on actual dysarthric speech. ASR trained on healthy speech augmented with simulated dysarthric speech is evaluated for dysarthric speech recognition. All evaluations were carried out using Universal Access dysarthric speech corpus. An absolute improvement of 4.24% and 2% was achieved using tempo based and speed based data augmentation respectively as compared to ASR performance using healthy speech alone for training",
    "checked": true,
    "id": "e98ea9dc73bf87e5509e987addf56b7006593ad7",
    "semantic_title": "data augmentation using healthy speech for dysarthric speech recognition",
    "citation_count": 61
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ding18b_interspeech.html": {
    "title": "Improving Sparse Representations in Exemplar-Based Voice Conversion with a Phoneme-Selective Objective Function",
    "volume": "main",
    "abstract": "The acoustic quality of exemplar-based voice conversion (VC) degrades whenever the phoneme labels of the selected exemplars do not match the phonetic content of the frame being represented. To address this issue, we propose a Phoneme-Selective Objective Function (PSOF) that promotes a sparse representation of each speech frame with exemplars from a few phoneme classes. Namely, PSOF enforces group sparsity on the representation, where each group corresponds to a phoneme class. The sparse representation for exemplars within a phoneme class tends to activate or suppress simultaneously using the proposed objective function. We conducted two sets of experiments on the ARCTIC corpus to evaluate the proposed method. First, we evaluated the ability of PSOF to reduce phoneme mismatches. Then, we assessed its performance on a VC task and compared it against three baseline methods from previous studies. Results from objective measurements and subjective listening tests show that the proposed method effectively reduces phoneme mismatches and significantly improves VC acoustic quality while retaining the voice identity of the target speaker",
    "checked": true,
    "id": "aa2f467408c57f9105472e0d7bd6ba9d28cc91a4",
    "semantic_title": "improving sparse representations in exemplar-based voice conversion with a phoneme-selective objective function",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ding18c_interspeech.html": {
    "title": "Learning Structured Dictionaries for Exemplar-based Voice Conversion",
    "volume": "main",
    "abstract": "Incorporating phonetic information has been shown to improve the performance of exemplar-based voice conversion. A standard approach is to build a phonetically structured dictionary, where exemplars are categorized into sub-dictionaries according to their phoneme labels. However, acquiring phoneme labels can be expensive and the phoneme labels can have inaccuracies. The latter problem becomes more salient when the speakers are non-native speakers. This paper presents an iterative dictionary-learning algorithm that avoids the need for phoneme labels and instead learns the structured dictionaries in an unsupervised fashion. At each iteration, two steps are alternatively performed: cluster update and dictionary update. In the cluster update step, each training frame is assigned to a cluster whose sub-dictionary represents it with the lowest residual. In the dictionary update step, the sub-dictionary for a cluster is updated using all the speech frames in the cluster. We evaluate the proposed algorithm through objective and subjective experiments on a new corpus of non-native English speech. Compared to previous studies, the proposed algorithm improves the acoustic quality of voice-converted speech while retaining the target speaker's identity",
    "checked": true,
    "id": "b86a258a6bc54be04cd7981768af950328c763ed",
    "semantic_title": "learning structured dictionaries for exemplar-based voice conversion",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/peng18_interspeech.html": {
    "title": "Exemplar-Based Spectral Detail Compensation for Voice Conversion",
    "volume": "main",
    "abstract": "Most voice conversion (VC) systems are established under the vocoder-based VC framework. When performing spectral conversion (SC) under this framework, the low-dimensional spectral features, such as mel-ceptral coefficients (MCCs), are often adopted to represent the high-dimensional spectral envelopes. The joint density Gaussian mixture model (GMM)-based SC method with the STRAIGHT vocoder is a well-known representative. Although it is reasonably effective, the loss of spectral details in the converted spectral envelopes inevitably deteriorates speech quality and similarity. To overcome this problem, we propose a novel exemplar-based spectral detail compensation method for VC. In the offline stage, the paired dictionaries of source spectral envelopes and target spectral details are constructed. In the online stage, the locally linear embedding (LLE) algorithm is applied to predict the target spectral details from the source spectral envelopes and then, the predicted spectral details are used to compensate the converted spectral envelopes obtained by a baseline GMM-based SC method with the STRAIGHT vocoder. Experimental results show that the proposed method can notably improve the baseline system in terms of objective and subjective tests",
    "checked": true,
    "id": "05c15d7710d6cff56680e3f04feb2db8e3c0c640",
    "semantic_title": "exemplar-based spectral detail compensation for voice conversion",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/meenakshi18_interspeech.html": {
    "title": "Whispered Speech to Neutral Speech Conversion Using Bidirectional LSTMs",
    "volume": "main",
    "abstract": "We propose a bidirectional long short-term memory (BLSTM) based whispered speech to neutral speech conversion system that employs the STRAIGHT speech synthesizer. We use a BLSTM to map the spectral features of whispered speech to those of neutral speech. Three other BLSTMs are employed to predict the pitch, periodicity levels and the voiced/unvoiced phoneme decisions from the spectral features of whispered speech. We use objective measures to quantify the quality of the predicted spectral features and excitation parameters, using data recorded from six subjects, in a four fold setup. We find that the temporal smoothness of the spectral features predicted using the proposed BLSTM based system is statistically more compared to that predicted using deep neural network based baseline schemes. We also observe that while the performance of the proposed system is comparable to the baseline scheme for pitch prediction, it is superior in terms of classifying voicing decisions and predicting periodicity levels. From subjective evaluation via listening test, we find that the proposed method is chosen as the best performing scheme 26.61% (absolute) more often than the best baseline scheme. This reveals that the proposed method yields a more natural sounding neutral speech from whispered speech",
    "checked": true,
    "id": "4d918bb31e3b63829ae7a28468bfa6a288544cf6",
    "semantic_title": "whispered speech to neutral speech conversion using bidirectional lstms",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18d_interspeech.html": {
    "title": "Voice Conversion Across Arbitrary Speakers Based on a Single Target-Speaker Utterance",
    "volume": "main",
    "abstract": "Developing a voice conversion (VC) system for a particular speaker typically requires considerable data from both the source and target speakers. This paper aims to effectuate VC across arbitrary speakers, which we call any-to-any VC, with only a single target-speaker utterance. Two systems are studied: (1) the i-vector-based VC (IVC) system and (2) the speaker-encoder-based VC (SEVC) system. Phonetic PosteriorGrams are adopted as speaker-independent linguistic features extracted from speech samples. Both systems train a multi-speaker deep bidirectional long-short term memory (DBLSTM) VC model, taking in additional inputs that encode speaker identities, in order to generate the outputs. In the IVC system, the speaker identity of a new target speaker is represented by i-vectors. In the SEVC system, the speaker identity is represented by speaker embedding predicted from a separately trained model. Experiments verify the effectiveness of both systems in achieving VC based only on a single target-speaker utterance. Furthermore, the IVC approach is superior to SEVC, in terms of the quality of the converted speech and its similarity to the utterance produced by the genuine target speaker",
    "checked": true,
    "id": "d1e16c12c33aef26cd71fa6f27a9b615c60a7e41",
    "semantic_title": "voice conversion across arbitrary speakers based on a single target-speaker utterance",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chou18_interspeech.html": {
    "title": "Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations",
    "volume": "main",
    "abstract": "Recently, cycle-consistent adversarial network (Cycle-GAN) has been successfully applied to voice conversion to a different speaker without parallel data, although in those approaches an individual model is needed for each target speaker. In this paper, we propose an adversarial learning framework for voice conversion, with which a single model can be trained to convert the voice to many different speakers, all without parallel data, by separating the speaker characteristics from the linguistic content in speech signals. An autoencoder is first trained to extract speaker-independent latent representations and speaker embedding separately using another auxiliary speaker classifier to regularize the latent representation. The decoder then takes the speaker-independent latent representation and the target speaker embedding as the input to generate the voice of the target speaker with the linguistic content of the source utterance. The quality of decoder output is further improved by patching with the residual signal produced by another pair of generator and discriminator. A target speaker set size of 20 was tested in the preliminary experiments and very good voice quality was obtained. Conventional voice conversion metrics are reported. We also show that the speaker information has been properly reduced from the latent representations",
    "checked": true,
    "id": "c04051cef693c9a41269f425a88f769599745d04",
    "semantic_title": "multi-target voice conversion without parallel data by adversarially learning disentangled audio representations",
    "citation_count": 120
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gorrostieta18_interspeech.html": {
    "title": "Attention-based Sequence Classification for Affect Detection",
    "volume": "main",
    "abstract": "This paper presents the Cogito submission to the Interspeech Computational Paralinguistics Challenge (ComParE), for the second sub-challenge. The aim of this second sub-challenge is to recognize self-assessed affect from short clips of speech-containing audio data. We adopt a sequence classification-based approach where we use a long-short term memory (LSTM) network for modeling the evolution of low-level spectral coefficients, with added attention mechanism to emphasize salient regions of the audio clip. Additionally to deal with the underrepresentation of the negative valence class we use a combination of mitigation strategies including oversampling and loss function weighting. Our experiments demonstrate improvements in detection accuracy when including the attention mechanism and class balancing strategies in combination, with the best models outperforming the best single challenge baseline model",
    "checked": true,
    "id": "023e96e57a26a801f5a8d5eed6ab53f0d0d190d9",
    "semantic_title": "attention-based sequence classification for affect detection",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2018/syed18_interspeech.html": {
    "title": "Computational Paralinguistics: Automatic Assessment of Emotions, Mood and Behavioural State from Acoustics of Speech",
    "volume": "main",
    "abstract": "Paralinguistic analysis of speech remains a challenging task due to the many confounding factors which affect speech production. In this paper, we address the Interspeech 2018 Computational Paralinguistics Challenge (ComParE) which aims to push the boundaries of sensitivity to non-textual information that is conveyed in the acoustics of speech. We attack the problem on several fronts. We posit that a substantial amount of paralinguistic information is contained in spectral features alone. To this end, we use a large ensemble of Extreme Learning Machines for classification of spectral features. We further investigate the applicability of (an ensemble of) CNN-GRUs networks to model temporal variations therein. We report on the details of the experiments and the results for three ComParE sub-challenges: Atypical Affect, Self-Assessed Affect and Crying. Our results compare favourably and in some cases exceed the published state-of-the-art performance",
    "checked": true,
    "id": "d5df475bc3b37bc66e1bf16d0c6a17c9060cfe1a",
    "semantic_title": "computational paralinguistics: automatic assessment of emotions, mood and behavioural state from acoustics of speech",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rallabandi18_interspeech.html": {
    "title": "Investigating Utterance Level Representations for Detecting Intent from Acoustics",
    "volume": "main",
    "abstract": "Recognizing paralinguistic cues from speech has applications in varied domains of speech processing. In this paper we present approaches to identify the expressed intent from acoustics in the context of INTERSPEECH 2018 ComParE challenge. We have made submissions in three sub-challenges: prediction of 1) self-assessed affect and 2) atypical affect 3) Crying Sub challenge. Since emotion and intent are perceived at suprasegmental levels, we explore a variety of utterance level embeddings. The work includes experiments with both automatically derived as well as knowledge-inspired features that capture spoken intent at various acoustic levels. Incorporation of utterance level embeddings at the text level using an off the shelf phone decoder has also been investigated. The experiments impose constraints and manipulate the training procedure using heuristics from the data distribution. We conclude by presenting the preliminary results on the development and blind test sets",
    "checked": true,
    "id": "403195c53f82f3c499a9b096dd1cf0735d1063d6",
    "semantic_title": "investigating utterance level representations for detecting intent from acoustics",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kaya18_interspeech.html": {
    "title": "LSTM Based Cross-corpus and Cross-task Acoustic Emotion Recognition",
    "volume": "main",
    "abstract": "Acoustic emotion recognition is a popular and central research direction in paralinguistic analysis, due its relation to a wide range of affective states/traits and manifold applications. Developing highly generalizable models still remains as a challenge for researchers and engineers, because of multitude of nuisance factors. To assert generalization, deployed models need to handle spontaneous speech recorded under different acoustic conditions compared to the training set. This requires that the models are tested for cross-corpus robustness. In this work, we first investigate the suitability of Long-Short-Term-Memory (LSTM) models trained with time- and space-continuously annotated affective primitives for cross-corpus acoustic emotion recognition. We next employ an effective approach to use the frame level valence and arousal predictions of LSTM models for utterance level affect classification and apply this approach on the ComParE 2018 challenge corpora. The proposed method alone gives motivating results both on development and test set of the Self-Assessed Affect Sub-Challenge. On the development set, the cross-corpus prediction based method gives a boost to performance when fused with top components of the baseline system. Results indicate the suitability of the proposed method for both time-continuous and utterance level cross-corpus acoustic emotion recognition tasks",
    "checked": true,
    "id": "d35701317664851f2bf50b7bd60bd38234e1b047",
    "semantic_title": "lstm based cross-corpus and cross-task acoustic emotion recognition",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vlasenko18_interspeech.html": {
    "title": "Implementing Fusion Techniques for the Classification of Paralinguistic Information",
    "volume": "main",
    "abstract": "This work tests several classification techniques and acoustic features and further combines them using late fusion to classify paralinguistic information for the ComParE 2018 challenge. We use Multiple Linear Regression (MLR) with Ordinary Least Squares (OLS) analysis to select the most informative features for Self-Assessed Affect (SSA) sub-Challenge. We also propose to use raw-waveform convolutional neural networks (CNN) in the context of three paralinguistic sub-challenges. By using combined evaluation split for estimating codebook, we obtain better representation for Bag-of-Audio-Words approach. We preprocess the speech to vocalized segments to improve classification performance. For fusion of our leading classification techniques, we use weighted late fusion approach applied for confidence scores. We use two mismatched evaluation phases by exchanging the training and development sets and this estimates the optimal fusion weight. Weighted late fusion provides better performance on development sets in comparison with baseline techniques. Raw-waveform techniques perform comparable to the baseline",
    "checked": true,
    "id": "89bcf386733d68976c90d70299b3cb08362e8ff9",
    "semantic_title": "implementing fusion techniques for the classification of paralinguistic information",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gosztolya18_interspeech.html": {
    "title": "General Utterance-Level Feature Extraction for Classifying Crying Sounds, Atypical & Self-Assessed Affect and Heart Beats",
    "volume": "main",
    "abstract": "In the area of computational paralinguistics, there is a growing need for general techniques that can be applied in a variety of tasks and which can be easily realized using standard and publicly available tools. In our contribution to the 2018 Interspeech Computational Paralinguistic Challenge (ComParE), we test four general ways of extracting features. Besides the standard ComParE feature set consisting of 6373 diverse attributes, we experiment with two variations of Bag-of-Audio-Words representations, and define a simple feature set inspired by Gaussian Mixture Models. Our results indicate that the UAR scores obtained via the different approaches vary among the tasks. In our view, this is mainly because most feature sets tested were local by nature and they could not properly represent the utterances of the Atypical Affect and Self-Assessed Affect Sub- Challenges. On the Crying Sub-Challenge, however, a simple combination of all four feature sets proved to be effective",
    "checked": true,
    "id": "8458d17eba5d9586acdcce720e159ee633c235da",
    "semantic_title": "general utterance-level feature extraction for classifying crying sounds, atypical & self-assessed affect and heart beats",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/su18d_interspeech.html": {
    "title": "Self-Assessed Affect Recognition Using Fusion of Attentional BLSTM and Static Acoustic Features",
    "volume": "main",
    "abstract": "In this study, we present a computational framework to participate in the Self-Assessed Affect Sub-Challenge in the INTERSPEECH 2018 Computation Paralinguistics Challenge. The goal of this sub-challenge is to classify the valence scores given by the speaker themselves into three different levels, i.e., low, medium and high. We explore fusion of Bi-directional LSTM with baseline SVM models to improve the recognition accuracy. In specifics, we extract frame-level acoustic LLDs as input to the BLSTM with a modified attention mechanism and separate SVMs are trained using the standard ComParE_16 baseline feature sets with minority class upsampling. These diverse prediction results are then further fused using a decision-level score fusion scheme to integrate all of the developed models. Our proposed approach achieves a 62.94% and 67.04% unweighted average recall (UAR), which is an 6.24% and 1.04% absolute improvement over the best baseline provided by the challenge organizer. We further provide a detailed comparison analysis between different models",
    "checked": true,
    "id": "5ceb28f9769b5af8086067b22d71dbb743cc7c13",
    "semantic_title": "self-assessed affect recognition using fusion of attentional blstm and static acoustic features",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/montacie18_interspeech.html": {
    "title": "Vocalic, Lexical and Prosodic Cues for the INTERSPEECH 2018 Self-Assessed Affect Challenge",
    "volume": "main",
    "abstract": "The INTERSPEECH 2018 Self-Assessed Affect Challenge consists in the prediction of the affective state of mind from speech. Experiments were conducted on the Ulm State-of-Mind in Speech database (USoMS) where subjects self-report their affective state. Dimensional representation of emotion (valence) is used for labeling. We have investigated cues related to the perception of the emotional valence according to three main relevant linguistic levels: phonetics, lexical and prosodic. For this purpose we studied: the degree-of-articulation, the voice quality, an affect lexicon and the expressive prosodic contours. For the phonetics level, a set of gender-dependent audio-features was computed on vowel analysis (voice quality and speech articulation measurements). At the lexical level, an affect lexicon was extracted from the automatic transcription of the USoMS database. This lexicon has been assessed for the Challenge task comparatively to a reference polarity lexicon. In order to detect expressive prosody, N-gram models of the prosodic contours were computed from an intonation labeling system. At last, an emotional valence classifier was designed combining ComParE and eGeMAPS feature sets with other phonetic, prosodic and lexical features. Experiments have shown an improvement of 2.4% on the Test set, compared to the baseline performance of the Challenge",
    "checked": true,
    "id": "e9fa2037c6c2b757d0e438854ebac25402f1f871",
    "semantic_title": "vocalic, lexical and prosodic cues for the interspeech 2018 self-assessed affect challenge",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pa18_interspeech.html": {
    "title": "Intonation tutor by SPIRE (In-SPIRE): An Online Tool for an Automatic Feedback to the Second Language Learners in Learning Intonation",
    "volume": "main",
    "abstract": "In spoken communication, intonation often conveys meaning of an utterance. Thus, incorrect intonation, typically made by second language (L2) learners, could result in miscommunication. We demonstrate In-SPIRE tool that helps the L2 learners to learn intonation in a self-learning manner. For this, we design an interactive self-explanatory front end, which is also used to send learner`s audio and hand-shake signals to the back-end. At the back-end, we implement a system that takes the learner`s audio against a specific stimuli and computes pitch patterns representing the intonation. For this, we apply pitch stylization on each syllable segment in the audio. Further, we compute a quality score using the learner`s patterns and the respective ground-truth patterns. Finally, the score, the patterns of the learners and the ground-truth are sent to the front-end for display as a feedback to the learners. Thus, the learner could correct any mismatch in his/her intonation with respect to the ground-truth. The proposed tool benefits the learners who do not have access to effective spoken language training",
    "checked": true,
    "id": "26ff4677371592e9364de855622de25a8afef2dd",
    "semantic_title": "intonation tutor by spire (in-spire): an online tool for an automatic feedback to the second language learners in learning intonation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/evanini18b_interspeech.html": {
    "title": "Game-based Spoken Dialog Language Learning Applications for Young Students",
    "volume": "main",
    "abstract": "This demo presents three different spoken dialog applications that were developed to provide young learners of English an opportunity to practice speaking and to receive feedback on particular aspects of their English speaking ability. The speaking tasks were designed as game-based interactions in order to engage young students and they provide feedback about grammar yes/no question formation and simple past tense verb formation) and vocabulary. A pilot study with 27 primary-level English as a foreign language (EFL) learners investigated the usefulness of these applications",
    "checked": true,
    "id": "98a47a5ce498ff6fda50e37031d1a8cd02284d9a",
    "semantic_title": "game-based spoken dialog language learning applications for young students",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sorin18_interspeech.html": {
    "title": "The IBM Virtual Voice Creator",
    "volume": "main",
    "abstract": "The IBM Virtual Voice Creator (IVVC) is an end-to-end cloud-based solution for TTS voice customization and voiceover generation in games and animated movies. The solution is based on the IBM expressive TTS technology with built-in online voice transformation capabilities. It is endowed with an interactive web GUI studio. IVVC lets the users create unique voice personas according to their needs and imagination and control the vocal performance of the virtual speakers. IVVC provides a powerful set of controls over the voice characteristics, including the vocal tract, glottal pulse, breathiness, pitch, rate and special voice effects. IVVC also allows the user to control emotional style and emphasis in the synthesized speech. The virtual voice design and performance controls are interactive, intuitive, fast and do not require any special skills",
    "checked": true,
    "id": "673e160743f91ad6e14616f39d103856c0872c52",
    "semantic_title": "the ibm virtual voice creator",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/g18_interspeech.html": {
    "title": "Mobile Application for Learning Languages for the Unlettered",
    "volume": "main",
    "abstract": "Mobile based technologies have become ubiquitous and various applications from games to readers are mobile oriented. In this paper, we propose development of speech based language learning app. Conventionally language learning tools start with words, followed by sentences. The fundamental assumption is that the person is literate. In the Indian context, the literacy levels are as low as 65%. In addition, each Indian language has its own scripts. The objective of this work is to develop a mobile app that starts from the script to teach the language to a person who can speak the language but is unlettered. Since the focus is on the unlettered, writing should be easy. A script centric approach is used to learn a language. The application starts with teaching a simple letter of the alphabet, followed by another letter that can be obtained by simple modification to the previously learned letter, followed by words using the letters that are learned, followed by sentences using the learned words. At every step, a text-to-speech system is used which articulates the letters and words. The learning app is based on a book called Tamil Karpom (P Nannan). The ideas from the book are adapted for learning Hindi",
    "checked": true,
    "id": "1bdf04e121c80c2be46182be4ab51f43acf8ecbf",
    "semantic_title": "mobile application for learning languages for the unlettered",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xu18d_interspeech.html": {
    "title": "Mandarin-English Code-switching Speech Recognition",
    "volume": "main",
    "abstract": "This work presents the development of a Mandarin-English code-switching speech recognition system. We demonstrate three key novelties in our system. First, we increase our lexicon coverage to 360K words, where phone sets of different languages are maintained separately. Secondly, we used over 1000 hours of training data combining both mono-lingual and code-switch corpus to develop the acoustic model. Finally, for language modelling, we applied context-aware text normalization and word-class language model. When testing on our internal code-switch close talk microphone recording, the system achieves recognition performance that can support real applications",
    "checked": true,
    "id": "e01e83530a082815bd55f88088c4e826801ded5c",
    "semantic_title": "mandarin-english code-switching speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kim18c_interspeech.html": {
    "title": "Joint Learning of Domain Classification and Out-of-Domain Detection with Dynamic Class Weighting for Satisficing False Acceptance Rates",
    "volume": "main",
    "abstract": "In domain classification for spoken dialog systems, correct detection of out-of-domain (OOD) utterances is crucial because it reduces confusion and unnecessary interaction costs between users and the systems. Previous work usually utilizes OOD detectors that are trained separately from in-domain (IND) classifiers and confidence thresholding for OOD detection given target evaluation scores. In this paper, we introduce a neural joint learning model for domain classification and OOD detection, where dynamic class weighting is used during the model training to satisfice a given OOD false acceptance rate (FAR) while maximizing the domain classification accuracy. Evaluating on two domain classification tasks for the utterances from a large spoken dialogue system, we show that our approach significantly improves the domain classification performance with satisficing given target FARs",
    "checked": true,
    "id": "799ebfe2c0a645cf50b024823a4f14b7b3501a7e",
    "semantic_title": "joint learning of domain classification and out-of-domain detection with dynamic class weighting for satisficing false acceptance rates",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mukherjee18_interspeech.html": {
    "title": "Analyzing Vocal Tract Movements During Speech Accommodation",
    "volume": "main",
    "abstract": "When two people engage in verbal interaction, they tend to accommodate on a variety of linguistic levels. Although recent attention has focused on to the acoustic characteristics of convergence in speech, the underlying articulatory mechanisms remain to be explored. Using 3D electromagnetic articulography (EMA), we simultaneously recorded articulatory movements in two speakers engaged in an interactive verbal game, the domino task. In this task, the two speakers take turn in chaining bi-syllabic words according to a rhyming rule. By using a robust speaker identification strategy, we identified for which specific words speakers converged or diverged. Then, we explored the different vocal tract features characterizing speech accommodation. Our results suggest that tongue movements tend to slow down during convergence whereas maximal jaw opening during convergence and divergence differs depending on syllable position",
    "checked": true,
    "id": "8898e55e911a63c9572f05e99d2413e5623dafa8",
    "semantic_title": "analyzing vocal tract movements during speech accommodation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18_interspeech.html": {
    "title": "Cross-Lingual Multi-Task Neural Architecture for Spoken Language Understanding",
    "volume": "main",
    "abstract": "Cross-lingual spoken language understanding (SLU) systems traditionally require machine translation services for language portability and liberation from human supervision. However, restriction exists in parallel corpora and model architectures. Assuming reliable data are provided with human-supervision, which encourages non-parallel corpora and alleviate translation errors, this paper aims to explore cross-lingual knowledge transfer from multiple levels by taking advantage of neural architectures. We first investigate a joint model of slot filling and intent determination for SLU, which alleviates the out-of-vocabulary problem and explicitly models dependencies between output labels by combining character and word representations, bidirectional Long Short-Term Memory and conditional random fields together, while attention-based classifier is introduced for intent determination. Knowledge transfer is further operated on character-level and sequence-level, aiming to share morphological and phonological information between languages with similar alphabets by sharing character representations and characterize the sequence with language-general and language-specific knowledge adaptively acquired by separate encoders. Experimental results on the MIT-Restaurant-Corpus and the ATIS corpora in different languages demonstrate the effectiveness of the proposed methods",
    "checked": true,
    "id": "9fd1ec7261d872ad5748ee16a9b024af57559874",
    "semantic_title": "cross-lingual multi-task neural architecture for spoken language understanding",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/strimel18_interspeech.html": {
    "title": "Statistical Model Compression for Small-Footprint Natural Language Understanding",
    "volume": "main",
    "abstract": "In this paper we investigate statistical model compression applied to natural language understanding (NLU) models. Small-footprint NLU models are important for enabling offline systems on hardware restricted devices and for decreasing on-demand model loading latency in cloud-based systems. To compress NLU models, we present two main techniques, parameter quantization and perfect feature hashing. These techniques are complementary to existing model pruning strategies such as L1 regularization. We performed experiments on a large scale NLU system. The results show that our approach achieves 14-fold reduction in memory usage compared to the original models with minimal predictive performance impact",
    "checked": true,
    "id": "271ec0e36a058e70fd5fdd9c5ea05d4cf2bd6a6d",
    "semantic_title": "statistical model compression for small-footprint natural language understanding",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2018/braunschweiler18_interspeech.html": {
    "title": "Comparison of an End-to-end Trainable Dialogue System with a Modular Statistical Dialogue System",
    "volume": "main",
    "abstract": "This paper presents a comparison of two dialogue systems: one is end-to-end trainable and the other uses a more traditional, modular architecture. End-to-end trainable dialogue systems recently attracted a lot of attention because they offer several advantages over traditional systems. One of them is the avoidance to train each system module independently, by creating a single network architecture which maps an input to the corresponding output without the need for intermediate representations. While the end-to-end system investigated here had been tested in a text-in/out scenario it remained an open question how the system would perform in a speech-in/out scenario, with noisy input from a speech recognizer and output speech generated by a speech synthesizer. To evaluate this, both dialogue systems were trained on the same corpus, including human-human dialogues in the Cambridge restaurant domain, and then compared in both scenarios by human evaluation. The results show, that in both interfaces the end-to-end system receives significantly higher ratings on all metrics than the traditional modular system, an indication that it enables users to reach their goals faster and experience both a more natural system response and a better comprehension by the dialogue system",
    "checked": true,
    "id": "621eb0d40a497d0b229b7d1b63ba3d1bed330ec3",
    "semantic_title": "comparison of an end-to-end trainable dialogue system with a modular statistical dialogue system",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/willi18_interspeech.html": {
    "title": "A Discriminative Acoustic-Prosodic Approach for Measuring Local Entrainment",
    "volume": "main",
    "abstract": "Acoustic-prosodic entrainment describes the tendency of humans to align or adapt their speech acoustics to each other in conversation. This alignment of spoken behavior has important implications for conversational success. However, modeling the subtle nature of entrainment in spoken dialogue continues to pose a challenge. In this paper, we propose a straightforward definition for local entrainment in the speech domain and operationalize an algorithm based on this: acoustic-prosodic features that capture entrainment should be maximally different between real conversations involving two partners and sham conversations generated by randomly mixing the speaking turns from the original two conversational partners. We propose an approach for measuring local entrainment that quantifies alignment of behavior on a turn-by-turn basis, projecting the differences between interlocutors' acoustic-prosodic features for a given turn onto a discriminative feature subspace that maximizes the difference between real and sham conversations. We evaluate the method using the derived features to drive a classifier aiming to predict an objective measure of conversational success (i.e., low versus high), on a corpus of task-oriented conversations. The proposed entrainment approach achieves 72% classification accuracy using a Naive Bayes classifier, outperforming three previously established approaches evaluated on the same conversational corpus",
    "checked": true,
    "id": "dda8cfdce08b83dde3abebb388adc443aaa3f74f",
    "semantic_title": "a discriminative acoustic-prosodic approach for measuring local entrainment",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/roddy18_interspeech.html": {
    "title": "Investigating Speech Features for Continuous Turn-Taking Prediction Using LSTMs",
    "volume": "main",
    "abstract": "For spoken dialog systems to conduct fluid conversational interactions with users, the systems must be sensitive to turn-taking cues produced by a user. Models should be designed so that effective decisions can be made as to when it is appropriate, or not, for the system to speak. Traditional end-of-turn models, where decisions are made at utterance end-points, are limited in their ability to model fast turn-switches and overlap. A more flexible approach is to model turn-taking in a continuous manner using RNNs, where the system predicts speech probability scores for discrete frames within a future window. The continuous predictions represent generalized turn-taking behaviors observed in the training data and can be applied to make decisions that are not just limited to end-of-turn detection. In this paper, we investigate optimal speech-related feature sets for making predictions at pauses and overlaps in conversation. We find that while traditional acoustic features perform well, part-of-speech features generally perform worse than word features. We show that our current models outperform previously reported baselines",
    "checked": true,
    "id": "ede28e0b075c294c5d66a75711c960c84f2fd234",
    "semantic_title": "investigating speech features for continuous turn-taking prediction using lstms",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kraljevski18_interspeech.html": {
    "title": "Classification of Correction Turns in Multilingual Dialogue Corpus",
    "volume": "main",
    "abstract": "This paper presents a multiclass classification of correction dialog turns using machine learning. The classes are determined by the type of the introduced recognition errors while performing WOz trials and creating the multilingual corpus. Three datasets were obtained using different sets of acoustic-prosodic features on the multilingual dialogue corpus. The classification experiments were done using different machine learning paradigms: Decision Trees, Support Vector Machines and Deep Learning. After careful experiments setup and optimization on the hyper-parameter space, the obtained classification results were analyzed and compared in the terms of accuracy, precision, recall and F1 score. The achieved results are comparable with those obtained in similar experiments on different tasks and speech databases",
    "checked": true,
    "id": "1abb3adfb4f5676fa72a73cd540ea88099b1e6a6",
    "semantic_title": "classification of correction turns in multilingual dialogue corpus",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/naik18_interspeech.html": {
    "title": "Contextual Slot Carryover for Disparate Schemas",
    "volume": "main",
    "abstract": "In the slot-filling paradigm, where a user can refer back to slots in the context during the conversation, the goal of the contextual understanding system is to resolve the referring expressions to the appropriate slots in the context. In large-scale multi-domain systems, this presents two challenges - scaling to a very large and potentially unbounded set of slot values and dealing with diverse schemas. We present a neural network architecture that addresses the slot value scalability challenge by reformulating the contextual interpretation as a decision to carryover a slot from a set of possible candidates. To deal with heterogenous schemas, we introduce a simple data-driven method for transforming the candidate slots. Our experiments show that our approach can scale to multiple domains and provides competitive results over a strong baseline",
    "checked": true,
    "id": "a1f7f3604870fb9797973fbb8109abe21df8ecea",
    "semantic_title": "contextual slot carryover for disparate schemas",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2018/renkens18_interspeech.html": {
    "title": "Capsule Networks for Low Resource Spoken Language Understanding",
    "volume": "main",
    "abstract": "Designing a spoken language understanding system for command-and-control applications can be challenging because of a wide variety of domains and users or because of a lack of training data. In this paper we discuss a system that learns from scratch from user demonstrations. This method has the advantage that the same system can be used for many domains and users without modifications and that no training data is required prior to deployment. The user is required to train the system, so for a user friendly experience it is crucial to minimize the required amount of data. In this paper we investigate whether a capsule network can make efficient use of the limited amount of available training data. We compare the proposed model to an approach based on Non-negative Matrix Factorisation which is the state-of-the-art in this setting and another deep learning approach that was recently introduced for end-to-end spoken language understanding. We show that the proposed model outperforms the baseline models for three command-and-control applications: controlling a small robot, a vocally guided card game and a home automation task",
    "checked": true,
    "id": "2ca546b0e2aac4578df293ce35af4b1164e11a2a",
    "semantic_title": "capsule networks for low resource spoken language understanding",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2018/padmasundari18_interspeech.html": {
    "title": "Intent Discovery Through Unsupervised Semantic Text Clustering",
    "volume": "main",
    "abstract": "Conversational systems need to understand spoken language to be able to converse with a human in a meaningful coherent manner. This understanding (Spoken Language understanding - SLU) of the human language is operationalized through identifying intents and entities. While classification methods that rely on labeled data are often used for SLU, creating large supervised data sets is extremely tedious and time consuming. This paper presents a practical approach to automate the process of intent discovery on unlabeled data sets of human language text through clustering techniques. We explore a range of representations for the texts and various clustering methods to validate the clustering stability through quantitative metrics like Adjusted Random Index (ARI). A final alignment of the clusters to the semantic intent is determined through consensus labelling. Our experiments on public datasets demonstrate the effectiveness of our approach generating homogeneous clusters with 89% cluster accuracy, leading to better semantic intent alignments. Furthermore, we illustrate that the clustering offer an alternate and effective way to mine sentence variants that can aid the bootstrapping of SLU models",
    "checked": true,
    "id": "29a6ecd69146a65463fbd622ca164f2569ab63cd",
    "semantic_title": "intent discovery through unsupervised semantic text clustering",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2018/du18b_interspeech.html": {
    "title": "Multimodal Polynomial Fusion for Detecting Driver Distraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/inoue18b_interspeech.html": {
    "title": "Engagement Recognition in Spoken Dialogue via Neural Network by Aggregating Different Annotators' Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/buanzur18_interspeech.html": {
    "title": "A First Investigation of the Timing of Turn-taking in Ruuli",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhao18_interspeech.html": {
    "title": "Spoofing Detection Using Adaptive Weighting Framework and Clustering Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jelil18_interspeech.html": {
    "title": "Exploration of Compressed ILPR Features for Replay Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gunendradasan18_interspeech.html": {
    "title": "Detection of Replay-Spoofing Attacks Using Frequency Modulation Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kamble18_interspeech.html": {
    "title": "Effectiveness of Speech Demodulation-Based Features for Replay Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kamble18b_interspeech.html": {
    "title": "Novel Variable Length Energy Separation Algorithm Using Instantaneous Amplitude Features for Replay Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yang18b_interspeech.html": {
    "title": "Feature with Complementarity of Statistics and Principal Information for Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18n_interspeech.html": {
    "title": "Multiple Phase Information Combination for Replay Attacks Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wickramasinghe18_interspeech.html": {
    "title": "Frequency Domain Linear Prediction Features for Replay Spoofing Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sailor18c_interspeech.html": {
    "title": "Auditory Filterbank Learning for Temporal Modulation Features in Replay Spoof Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sriskandaraja18_interspeech.html": {
    "title": "Deep Siamese Architecture Based Replay Detection for Secure Voice Biometric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gomezalanis18_interspeech.html": {
    "title": "A Deep Identity Representation for Noise Robust Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tom18_interspeech.html": {
    "title": "End-To-End Audio Replay Attack Detection Using Deep Convolutional Networks with Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ms18_interspeech.html": {
    "title": "Decision-level Feature Switching as a Paradigm for Replay Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/suthokumar18_interspeech.html": {
    "title": "Modulation Dynamic Features for the Detection of Replay Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/loweimi18_interspeech.html": {
    "title": "On the Usefulness of the Speech Phase Spectrum for Pitch Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/airaksinen18_interspeech.html": {
    "title": "Time-regularized Linear Prediction for Noise-robust Extraction of the Spectral Envelope of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sailor18_interspeech.html": {
    "title": "Auditory Filterbank Learning Using ConvRBM for Infant Cry Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shah18_interspeech.html": {
    "title": "Effectiveness of Dynamic Features in INCA and Temporal Context-INCA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gong18_interspeech.html": {
    "title": "Singing Voice Phoneme Segmentation by Hierarchically Inferring Syllable and Phoneme Onset Positions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tapkir18_interspeech.html": {
    "title": "Novel Empirical Mode Decomposition Cepstral Features for Replay Spoof Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tak18_interspeech.html": {
    "title": "Novel Linear Frequency Residual Cepstral Features for Replay Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tripathi18_interspeech.html": {
    "title": "Analysis of sparse representation based feature on speech mode classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dhiman18_interspeech.html": {
    "title": "Multicomponent 2-D AM-FM Modeling of Speech Spectrograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sainathan18_interspeech.html": {
    "title": "An Optimization Framework for Recovery of Speech from Phase-Encoded Spectrograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xia18b_interspeech.html": {
    "title": "Speaker Recognition with Nonlinear Distortion: Clipping Analysis and Impact",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/singh18_interspeech.html": {
    "title": "Linear Prediction Residual based Short-term Cepstral Features for Replay Attacks Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sakshi18_interspeech.html": {
    "title": "Analysis of Variational Mode Functions for Robust Detection of Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/weng18_interspeech.html": {
    "title": "Improving Attention Based Sequence-to-Sequence Models for End-to-End English Conversational Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/beck18_interspeech.html": {
    "title": "Segmental Encoder-Decoder Models for Large Vocabulary Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18b_interspeech.html": {
    "title": "Acoustic Modeling with DFSMN-CTC and Joint CTC-CE Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bae18_interspeech.html": {
    "title": "End-to-End Speech Command Recognition with Capsule Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zeghidour18_interspeech.html": {
    "title": "End-to-End Speech Recognition from the Raw Waveform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yu18_interspeech.html": {
    "title": "A Multistage Training Framework for Acoustic-to-Word Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhou18_interspeech.html": {
    "title": "Syllable-Based Sequence-to-Sequence Speech Recognition with the Transformer in Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/han18b_interspeech.html": {
    "title": "Densely Connected Networks for Conversational Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hayashi18_interspeech.html": {
    "title": "Multi-Head Decoder for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mori18_interspeech.html": {
    "title": "Compressing End-to-end ASR Networks by Tensor-Train Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chung18c_interspeech.html": {
    "title": "Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dong18_interspeech.html": {
    "title": "Extending Recurrent Neural Aligner for Streaming End-to-End Speech Recognition in Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18c_interspeech.html": {
    "title": "Joint Noise and Reverberation Adaptive Learning for Robust Speaker DOA Estimation with an Acoustic Vector Sensor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18c_interspeech.html": {
    "title": "Multiple Concurrent Sound Source Tracking Based on Observation-Guided Adaptive Particle Filter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/m18c_interspeech.html": {
    "title": "Harmonic-Percussive Source Separation of Polyphonic Music by Suppressing Impulsive Noise Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ceolini18_interspeech.html": {
    "title": "Speaker Activity Detection and Minimum Variance Beamforming for Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/qi18_interspeech.html": {
    "title": "Sparsity-Constrained Weight Mapping for Head-Related Transfer Functions Individualization from Anthropometric Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sai18_interspeech.html": {
    "title": "Speech Source Separation Using ICA in Constant Q Transform Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yin18_interspeech.html": {
    "title": "Multi-talker Speech Separation Based on Permutation Invariant Training and Beamforming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/magron18_interspeech.html": {
    "title": "Expectation-Maximization Algorithms for Itakura-Saito Nonnegative Matrix Factorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/girijaramesan18_interspeech.html": {
    "title": "Subband Weighting for Binaural Speech Source Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vaissiere18_interspeech.html": {
    "title": "Universal Tendencies for Cross-Linguistic Prosodic Tendencies: A Review and Some New Proposals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/klejch18_interspeech.html": {
    "title": "Learning to Adapt: A Meta-learning Approach for Speaker Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18o_interspeech.html": {
    "title": "Speaker Adaptation and Adaptive Training for Jointly Optimised Tandem Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kitza18_interspeech.html": {
    "title": "Comparison of BLSTM-Layer-Specific Affine Transformations for Speaker Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sharon18_interspeech.html": {
    "title": "Correlational Networks for Speaker Normalization in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tjandra18_interspeech.html": {
    "title": "Machine Speech Chain with One-shot Speaker Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sim18_interspeech.html": {
    "title": "Domain Adaptation Using Factorized Hidden Layer for Robust Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wan18_interspeech.html": {
    "title": "Waveform-Based Speaker Representations for Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yanagita18_interspeech.html": {
    "title": "Incremental TTS for Japanese Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fu18_interspeech.html": {
    "title": "Transfer Learning Based Progressive Neural Networks for Acoustic Modeling in Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hwang18_interspeech.html": {
    "title": "A Unified Framework for the Generation of Glottal Signals in Deep Learning-based Parametric Speech Synthesis Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lee18c_interspeech.html": {
    "title": "Acoustic Modeling Using Adversarially Trained Variational Recurrent Neural Network for Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zheng18b_interspeech.html": {
    "title": "On the Application and Compression of Deep Time Delay Neural Network for Embedded Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tzinis18_interspeech.html": {
    "title": "Integrating Recurrence Dynamics for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/han18c_interspeech.html": {
    "title": "Towards Temporal Modelling of Categorical Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kim18_interspeech.html": {
    "title": "Emotion Recognition from Human Speech Using Temporal Information and Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sridhar18_interspeech.html": {
    "title": "Role of Regularization in the Prediction of Valence from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mangalam18_interspeech.html": {
    "title": "Learning Spontaneity to Improve Emotion Recognition in Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lotfian18_interspeech.html": {
    "title": "Predicting Categorical Emotions by Jointly Learning Primary and Secondary Emotions through Multitask Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/caudrelier18_interspeech.html": {
    "title": "Picture Naming or Word Reading: Does the Modality Affect Speech Motor Adaptation and Its Transfer?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/du18_interspeech.html": {
    "title": "Measuring the Band Importance Function for Mandarin Chinese with a Bayesian Adaptive Procedure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shafaeibajestan18_interspeech.html": {
    "title": "Wide Learning for Auditory Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tenbosch18_interspeech.html": {
    "title": "Analyzing Reaction Time Sequences from Human Participants in Auditory Experiments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ooster18_interspeech.html": {
    "title": "Prediction of Perceived Speech Quality Using Deep Machine Listening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kranzusch18_interspeech.html": {
    "title": "Prediction of Subjective Listening Effort from Acoustic Data with Non-Intrusive Deep Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kotti18_interspeech.html": {
    "title": "A Case Study on the Importance of Belief State Representation for Dialogue Policy Management",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hara18_interspeech.html": {
    "title": "Prediction of Turn-taking Using Multitask Learning with Prediction of Backchannels and Fillers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bothe18_interspeech.html": {
    "title": "Conversational Analysis Using Utterance-level Attention-based Bidirectional Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ohsugi18_interspeech.html": {
    "title": "A Comparative Study of Statistical Conversion of Face to Voice Based on Their Subjective Impressions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/su18_interspeech.html": {
    "title": "Follow-up Question Generation Using Pattern-based Seq2seq with a Small Corpus for Interview Coaching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cervone18_interspeech.html": {
    "title": "Coherence Models for Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ke18_interspeech.html": {
    "title": "Indian Languages ASR: A Multilingual Phone Recognition Framework with IPA Based Common Phone-set, Predicted Articulatory Features and Feature fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/raza18_interspeech.html": {
    "title": "Rapid Collection of Spontaneous Speech Corpora Using Telephonic Community Forums",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/murthy18_interspeech.html": {
    "title": "Effect of TTS Generated Audio on OOV Detection and Word Error Rate in ASR for Low-resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/patel18_interspeech.html": {
    "title": "Development of Large Vocabulary Speech Recognition System with Keyword Search for Manipuri",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dey18_interspeech.html": {
    "title": "Robust Mizo Continuous Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chellapriyadharshini18_interspeech.html": {
    "title": "Semi-supervised and Active-learning Scenarios: Efficient Acoustic Model Refinement for a Low Resource Indian Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dash18_interspeech.html": {
    "title": "Automatic Speech Recognition with Articulatory Information and a Unified Dictionary for Hindi, Marathi, Bengali and Oriya",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rouhe18_interspeech.html": {
    "title": "Captaina: Integrated Pronunciation Practice and Data Collection Portal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sachdev18_interspeech.html": {
    "title": "auMinaâ„¢ - Enterprise Speech Analytics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/naresh18_interspeech.html": {
    "title": "HoloCompanion: An MR Friend for EveryOne",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sachdev18b_interspeech.html": {
    "title": "akeiraâ„¢ - Virtual Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/maruthachalam18_interspeech.html": {
    "title": "Brain-Computer Interface using Electroencephalogram Signatures of Eye Blinks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ajili18_interspeech.html": {
    "title": "Voice Comparison and Rhythm: Behavioral Differences between Target and Non-target Comparisons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xu18b_interspeech.html": {
    "title": "Co-whitening of I-vectors for Short and Long Duration Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bahmaninezhad18_interspeech.html": {
    "title": "Compensation for Domain Mismatch in Text-independent Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shi18d_interspeech.html": {
    "title": "Joint Learning of J-Vector Extractor and Joint Bayesian Model for Text Dependent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shi18c_interspeech.html": {
    "title": "Latent Factor Analysis of Deep Bottleneck Features for Speaker Verification with Random Digit Strings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chung18b_interspeech.html": {
    "title": "VoxCeleb2: Deep Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ramoji18_interspeech.html": {
    "title": "Supervised I-vector Modeling - Theory and Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dmitriev18_interspeech.html": {
    "title": "LOCUST - Longitudinal Corpus and Toolset for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/madikeri18_interspeech.html": {
    "title": "Analysis of Language Dependent Front-End for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nandwana18b_interspeech.html": {
    "title": "Robust Speaker Recognition from Distant Speech under Real Reverberant Environments Using Speaker Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nidadavolu18_interspeech.html": {
    "title": "Investigation on Bandwidth Extension for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/muckenhirn18_interspeech.html": {
    "title": "On Learning Vocal Tract System Related Speaker Discriminative Information from Raw Signal Using CNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kumar18b_interspeech.html": {
    "title": "On Convolutional LSTM Modeling for Joint Wake-Word Detection and Text Dependent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bai18_interspeech.html": {
    "title": "Cosine Metric Learning for Speaker Verification in the I-vector Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jati18_interspeech.html": {
    "title": "An Unsupervised Neural Prediction Framework for Learning Speaker Embeddings Using Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pandey18c_interspeech.html": {
    "title": "A New Framework for Supervised Speech Enhancement in the Time Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sadasivan18_interspeech.html": {
    "title": "Speech Enhancement Using the Minimum-probability-of-error Criterion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/papadopoulos18_interspeech.html": {
    "title": "Exploring the Relationship between Conic Affinity of NMF Dictionaries and Speech Enhancement Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18e_interspeech.html": {
    "title": "Using Shifted Real Spectrum Mask as Training Target for Supervised Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/srinivas18_interspeech.html": {
    "title": "Enhancement of Noisy Speech Signal by Non-Local Means Estimation of Variational Mode Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pallavi18_interspeech.html": {
    "title": "Phase-locked Loop (PLL) Based Phase Estimation in Single Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/meng18b_interspeech.html": {
    "title": "Cycle-Consistent Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gabbay18_interspeech.html": {
    "title": "Visual Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sharma18c_interspeech.html": {
    "title": "Implementation of Digital Hearing Aid as a Smartphone Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lee18_interspeech.html": {
    "title": "Bone-Conduction Sensor Assisted Noise Estimation for Improved Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bachhav18_interspeech.html": {
    "title": "Artificial Bandwidth Extension with Memory Inclusion Using Semi-supervised Stacked Auto-encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/maiti18_interspeech.html": {
    "title": "Large Vocabulary Concatenative Resynthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/syed18b_interspeech.html": {
    "title": "Concatenative Resynthesis with Improved Training Signals for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rasanen18_interspeech.html": {
    "title": "Comparison of Syllabification Algorithms and Training Strategies for Robust Word Count Estimation across Different Languages and Recording Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kelley18_interspeech.html": {
    "title": "A Comparison of Input Types to a Deep Neural Network-based Forced Aligner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jung18_interspeech.html": {
    "title": "Joint Learning Using Denoising Variational Autoencoders for Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dawalatabad18_interspeech.html": {
    "title": "Information Bottleneck Based Percussion Instrument Diarization System for Taniavartanam Segments of Carnatic Music Concerts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ghosh18_interspeech.html": {
    "title": "Robust Voice Activity Detection Using Frequency Domain Long-Term Differential Entropy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mallidi18_interspeech.html": {
    "title": "Device-directed Utterance Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ma18_interspeech.html": {
    "title": "Acoustic-Prosodic Features of Tabla Bol Recitation and Correspondence with the Tabla Imitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/krikke18_interspeech.html": {
    "title": "Who Said That? a Comparative Study of Non-negative Matrix Factorization Techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chaudhuri18_interspeech.html": {
    "title": "AVA-Speech: A Densely Labeled Dataset of Speech Activity in Movies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tao18_interspeech.html": {
    "title": "Audiovisual Speech Activity Detection with Advanced Long Short-Term Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/saha18_interspeech.html": {
    "title": "Towards Automatic Speech Identification from Vocal Tract Shape Dynamics in Real-time MRI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shi18_interspeech.html": {
    "title": "Structured Word Embedding for Low Memory Neural Network Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/masumura18_interspeech.html": {
    "title": "Role Play Dialogue Aware Language Models Based on Conditional Hierarchical Recurrent Encoder-Decoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/myer18_interspeech.html": {
    "title": "Efficient Keyword Spotting Using Time Delay Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yoshida18_interspeech.html": {
    "title": "Automatic DNN Node Pruning Using Mixture Distribution-based Group Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tavarone18_interspeech.html": {
    "title": "Conditional-Computation-Based Recurrent Neural Networks for Computationally Efficient Acoustic Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/anastasopoulos18_interspeech.html": {
    "title": "Leveraging Translations for Speech Transcription in Low-resource Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bruguier18_interspeech.html": {
    "title": "Sequence-to-sequence Neural Network Model with 2D Attention for Learning Japanese Pitch Accents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ghannay18_interspeech.html": {
    "title": "Task Specific Sentence Embeddings for ASR Error Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/niehues18_interspeech.html": {
    "title": "Low-Latency Neural Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bansal18_interspeech.html": {
    "title": "Low-Resource Speech-to-Text Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/brasser18_interspeech.html": {
    "title": "VoiceGuard: Secure and Private Speech Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hakkanitur18_interspeech.html": {
    "title": "Deep Learning based Situated Goal-oriented Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18b_interspeech.html": {
    "title": "Single-channel Speech Dereverberation via Generative Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mack18_interspeech.html": {
    "title": "Single-Channel Dereverberation Using Direct MMSE Optimization and Bidirectional LSTM Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kodrasi18_interspeech.html": {
    "title": "Single-channel Late Reverberation Power Spectral Density Estimation Using Denoising Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mohanan18_interspeech.html": {
    "title": "A Non-convolutive NMF Model for Speech Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/guzewich18_interspeech.html": {
    "title": "Cross-Corpora Convolutional Deep Neural Network Dereverberation Preprocessing for Speaker Verification and Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mosner18_interspeech.html": {
    "title": "Dereverberation and Beamforming in Robust Far-Field Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18_interspeech.html": {
    "title": "Comparing the Max and Noisy-Or Pooling Functions in Multiple Instance Learning for Weakly Supervised Sequence Learning Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18n_interspeech.html": {
    "title": "A Simple Model for Detection of Rare Sound Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18e_interspeech.html": {
    "title": "Temporal Transformer Networks for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lu18_interspeech.html": {
    "title": "Temporal Attentive Pooling for Acoustic Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kao18_interspeech.html": {
    "title": "R-CRNN: Region-based Convolutional Recurrent Neural Network for Audio Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/papayiannis18_interspeech.html": {
    "title": "Detecting Media Sound Presence in Acoustic Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/broux18_interspeech.html": {
    "title": "S4D: Speaker Diarization Toolkit in Python",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/park18b_interspeech.html": {
    "title": "Multimodal Speaker Segmentation and Diarization Using Lexical and Acoustic Cues via Sequence to Sequence Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/flemotomos18b_interspeech.html": {
    "title": "Combined Speaker Clustering and Role Recognition in Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lefranc18_interspeech.html": {
    "title": "The ACLEW DiViMe: An Easy-to-use Diarization Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kazimirova18_interspeech.html": {
    "title": "Automatic Detection of Multi-speaker Fragments with High Time Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yin18b_interspeech.html": {
    "title": "Neural Speech Turn Segmentation and Affinity Propagation for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18i_interspeech.html": {
    "title": "Pitch or Phonation: on the Glottalization in Tone Productions in the Ruokeng Hui Chinese Dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hullebus18_interspeech.html": {
    "title": "Speaker-specific Structure in German Voiceless Stop Voice Onset Times",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/aare18_interspeech.html": {
    "title": "Creak in the Respiratory Cycle",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18p_interspeech.html": {
    "title": "Acoustic Analysis of Whispery Voice Disguise in Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/maurer18_interspeech.html": {
    "title": "The Zurich Corpus of Vowel and Voice Quality, Version 1.0",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/penney18_interspeech.html": {
    "title": "Weighting of Coda Voicing Cues: Glottalisation and Vowel Duration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhao18d_interspeech.html": {
    "title": "Revealing Spatiotemporal Brain Dynamics of Speech Production Based on EEG and Eye Movement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bollavetisyan18_interspeech.html": {
    "title": "Neural Response Development During Distributional Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/maggu18b_interspeech.html": {
    "title": "Learning Two Tone Languages Enhances the Brainstem Encoding of Lexical Tones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/williams18b_interspeech.html": {
    "title": "Perceptual Sensitivity to Spectral Change in Australian English Close Front Vowels: An Electroencephalographic Investigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nixon18_interspeech.html": {
    "title": "Effective Acoustic Cue Learning Is Not Just Statistical, It Is Discriminative",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mulder18_interspeech.html": {
    "title": "Analyzing EEG Signals in Auditory Speech Comprehension Using Temporal Response Functions and Generalized Additive Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tenbosch18b_interspeech.html": {
    "title": "Information Encoding by Deep Neural Networks: What Can We Learn?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hsu18_interspeech.html": {
    "title": "Scalable Factorized Hierarchical Variational Autoencoder Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/verwimp18_interspeech.html": {
    "title": "State Gradients for RNN Memory Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bai18b_interspeech.html": {
    "title": "Exploring How Phone Classification Neural Networks Learn Phonetic Information by Visualising and Interpreting Bottleneck Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zegers18_interspeech.html": {
    "title": "Memory Time Span in LSTMs for Multi-Speaker Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/scharenborg18b_interspeech.html": {
    "title": "Visualizing Phoneme Category Adaptation in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kasthuri18_interspeech.html": {
    "title": "Early Vocabulary Development Through Picture-based Software Solutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sabu18_interspeech.html": {
    "title": "Automatic Detection of Expressiveness in Oral Reading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pal18_interspeech.html": {
    "title": "PannoMulloKathan: Voice Enabled Mobile App for Agricultural Commodity Price Dissemination in Bengali Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/oktem18_interspeech.html": {
    "title": "Visualizing Punctuation Restoration in Speech Transcripts with Prosograph",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mathivanan18_interspeech.html": {
    "title": "CACTAS - Collaborative Audio Categorization and Transcription for ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/parrell18_interspeech.html": {
    "title": "FACTS: A Hierarchical Task-based Control Model of Speech Incorporating Sensory Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/katz18_interspeech.html": {
    "title": "Sensorimotor Response to Tongue Displacement Imagery by Talkers with Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gupta18_interspeech.html": {
    "title": "Automatic Pronunciation Evaluation of Singing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bouserhal18_interspeech.html": {
    "title": "Classification of Nonverbal Human Produced Audio Events: A Pilot Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/spreafico18_interspeech.html": {
    "title": "UltraFit: A Speaker-friendly Headset for Ultrasound Recordings in Speech Science",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cortes18_interspeech.html": {
    "title": "Articulatory Consequences of Vocal Effort Elicitation Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hermes18b_interspeech.html": {
    "title": "Age-related Effects on Sensorimotor Control of Speech Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/percival18_interspeech.html": {
    "title": "An Ultrasound Study of Gemination in Coronal Stops in Eastern Oromo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nomosudro18_interspeech.html": {
    "title": "Processing Transition Regions of Glottal Stop Substituted /S/ for Intelligibility Enhancement of Cleft Palate Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/n18_interspeech.html": {
    "title": "Reconstructing Neutral Speech from Tracheoesophageal Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ochi18_interspeech.html": {
    "title": "Automatic Evaluation of Soft Articulatory Contact for Stuttering Treatment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kim18b_interspeech.html": {
    "title": "Korean Singing Voice Synthesis Based on an LSTM Recurrent Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xuanda18_interspeech.html": {
    "title": "The Trajectory of Voice Onset Time with Vocal Aging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/barker18_interspeech.html": {
    "title": "The Fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset, Task and Baselines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/richey18_interspeech.html": {
    "title": "Voices Obscured in Complex Environmental Settings (VOiCES) Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18d_interspeech.html": {
    "title": "Building State-of-the-art Distant Speech Recognition Using the CHiME-4 Challenge with a Setup of Speech Enhancement Baseline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hsu18b_interspeech.html": {
    "title": "Unsupervised Adaptation with Interpretable Disentangled Representations for Distant Conversational Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18i_interspeech.html": {
    "title": "Investigating Generative Adversarial Networks Based Speech Dereverberation for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chang18_interspeech.html": {
    "title": "Monaural Multi-Talker Speech Recognition with Attention Mechanism and Gated Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/do18_interspeech.html": {
    "title": "Weighting Time-Frequency Representation of Speech Using Auditory Saliency for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ghahremani18_interspeech.html": {
    "title": "Acoustic Modeling from Frequency Domain Representations of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yadav18b_interspeech.html": {
    "title": "Non-Uniform Spectral Smoothing for Robust Children's Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nicolson18_interspeech.html": {
    "title": "Bidirectional Long-Short Term Memory Network-based Estimation of Reliable Spectral Component Locations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/guo18c_interspeech.html": {
    "title": "Speech Emotion Recognition by Combining Amplitude and Phase Information Using Convolutional Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/trinh18_interspeech.html": {
    "title": "Bubble Cooperative Networks for Identifying Important Speech Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cheng18_interspeech.html": {
    "title": "Real-Time Scoring of an Oral Reading Assessment on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kyriakopoulos18_interspeech.html": {
    "title": "A Deep Learning Approach to Assessing Non-native Pronunciation of English Using Phone Distances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xiao18b_interspeech.html": {
    "title": "Paired Phone-Posteriors Approach to ESL Pronunciation Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tu18_interspeech.html": {
    "title": "Investigating the Role of L1 in Automatic Pronunciation Evaluation of L2 Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/knill18_interspeech.html": {
    "title": "Impact of ASR Performance on Free Speaking Language Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hong18_interspeech.html": {
    "title": "Automatic Miscue Detection Using RNN Based Models with Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/inoue18_interspeech.html": {
    "title": "A Study of Objective Measurement of Comprehensibility through Native Speakers' Shadowing of Learners' Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/luo18b_interspeech.html": {
    "title": "Factorized Deep Neural Network Adaptation for Automatic Scoring of L2 Speech in English Speaking Tests",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yeung18_interspeech.html": {
    "title": "On the Difficulties of Automatic Speech Recognition for Kindergarten-Aged Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nicolao18_interspeech.html": {
    "title": "Improved Acoustic Modelling for Automatic Literacy Assessment of Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shahin18_interspeech.html": {
    "title": "Anomaly Detection Approach for Pronunciation Verification of Disordered Speech Using Speech Attribute Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/afshan18_interspeech.html": {
    "title": "Effectiveness of Voice Quality Features in Detecting Depression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kothalkar18_interspeech.html": {
    "title": "Fusing Text-dependent Word-level i-Vector Models to Screen â€˜at Risk' Child Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chandrashekar18_interspeech.html": {
    "title": "Testing Paradigms for Assistive Hearing Devices in Diverse Acoustic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ujiro18_interspeech.html": {
    "title": "Detection of Dementia from Responses to Atypical Questions Asked by Embodied Conversational Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18h_interspeech.html": {
    "title": "Acoustic Features Associated with Sustained Vowel and Continuous Speech Productions by Chinese Children with Functional Articulation Disorders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vikram18b_interspeech.html": {
    "title": "Estimation of Hypernasality Scores from Cleft Lip and Palate Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/warnita18_interspeech.html": {
    "title": "Detecting Alzheimer's Disease Using Gated Convolutional Neural Network from Audio Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bandini18_interspeech.html": {
    "title": "Automatic Detection of Orofacial Impairment in Stroke",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/alhanai18_interspeech.html": {
    "title": "Detecting Depression with Audio/Text Sequence Modeling of Interviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18l_interspeech.html": {
    "title": "Discourse Marker Detection for Hesitation Events on Mandarin Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/geng18_interspeech.html": {
    "title": "Acoustic and Perceptual Characteristics of Mandarin Speech in Homosexual and Heterosexual Male Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ando18_interspeech.html": {
    "title": "Automatic Question Detection from Acoustic and Phonetic Features Using Feature-wise Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/haider18b_interspeech.html": {
    "title": "Improving Response Time of Active Speaker Detection Using Visual Prosody Information Prior to Articulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/turker18_interspeech.html": {
    "title": "Audio-Visual Prediction of Head-Nod and Turn-Taking Events in Dyadic Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wu18c_interspeech.html": {
    "title": "Analyzing Effect of Physical Expression on English Proficiency for Multimodal Computer-Assisted Language Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dumpala18_interspeech.html": {
    "title": "Analysis of the Effect of Speech-Laugh on Speaker Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sloboda18_interspeech.html": {
    "title": "Vocal Biomarkers for Cognitive Performance Estimation in a Working Memory Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/an18_interspeech.html": {
    "title": "Lexical and Acoustic Deep Learning Model for Personality Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ramabhadran18_interspeech.html": {
    "title": "Open Problems in Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bourlard18_interspeech.html": {
    "title": "Evolution of Neural Network Architectures for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18i_interspeech.html": {
    "title": "Layer Trajectory LSTM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18n_interspeech.html": {
    "title": "Semi-tied Units for Efficient Gating in LSTM and Highway Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lam18_interspeech.html": {
    "title": "Gaussian Process Neural Networks for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tang18_interspeech.html": {
    "title": "Acoustic Modeling with Densely Connected Residual Network for Multichannel Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18k_interspeech.html": {
    "title": "Gated Recurrent Unit Based Acoustic Modeling with Future Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cheng18b_interspeech.html": {
    "title": "Output-Gate Projected Gated Recurrent Unit for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sadjadi18_interspeech.html": {
    "title": "Performance Analysis of the 2017 NIST Language Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mateju18_interspeech.html": {
    "title": "Using Deep Neural Networks for Identification of Slavic Languages from Acoustic Signal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/taitelbaum18_interspeech.html": {
    "title": "Adding New Classes without Access to the Original Training Data with Applications to Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shen18b_interspeech.html": {
    "title": "Feature Representation of Short Utterances Based on Knowledge Distillation for Spoken Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fernando18_interspeech.html": {
    "title": "Sub-band Envelope Features Using Frequency Domain Linear Prediction for Short Duration Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/frederiksen18_interspeech.html": {
    "title": "Effectiveness of Single-Channel BLSTM Enhancement for Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gold18b_interspeech.html": {
    "title": "Articulation Rate as a Speaker Discriminant in British English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yu18d_interspeech.html": {
    "title": "Truncation and Compression in Southern German and Australian English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kallio18_interspeech.html": {
    "title": "Prominence-based Evaluation of L2 Prosody",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ridouane18_interspeech.html": {
    "title": "Length Contrast and Covarying Features: Whistled Speech as a Case Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chodroff18_interspeech.html": {
    "title": "Information Structure, Affect and Prenuclear Prominence in American English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/novakiii18_interspeech.html": {
    "title": "Effects of User Controlled Speech Rate on Intelligibility in Noisy Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kondo18_interspeech.html": {
    "title": "Binaural Speech Intelligibility Estimation Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yamamoto18_interspeech.html": {
    "title": "Multi-resolution Gammachirp Envelope Distortion Index for Intelligibility Prediction of Noisy Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pv18_interspeech.html": {
    "title": "Speech Intelligibility Enhancement Based on a Non-causal Wavenet-like Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fu18c_interspeech.html": {
    "title": "Quality-Net: An End-to-End Non-intrusive Speech Quality Assessment Model Based on BLSTM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/aralikatti18_interspeech.html": {
    "title": "Global SNR Estimation of Speech Signals Using Entropy and Uncertainty Estimates from Dropout Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mittag18_interspeech.html": {
    "title": "Detecting Packet-Loss Concealment Using Formant Features and Decision Tree Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/eshky18_interspeech.html": {
    "title": "UltraSuite: A Repository of Ultrasound and Acoustic Data from Child Speech Therapy Sessions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mirheidari18_interspeech.html": {
    "title": "Detecting Signs of Dementia Using Word Vector Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/perez18_interspeech.html": {
    "title": "Classification of Huntington Disease Using Acoustic and Lexical Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/khorram18_interspeech.html": {
    "title": "The PRIORI Emotion Dataset: Linking Mood to Emotion Detected In-the-Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/flemotomos18_interspeech.html": {
    "title": "Language Features for Automated Evaluation of Cognitive Behavior Psychotherapy Sessions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/an18c_interspeech.html": {
    "title": "Automatic Early Detection of Amyotrophic Lateral Sclerosis from Intelligible Speech Using Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rao18_interspeech.html": {
    "title": "A Study of Lexical and Prosodic Cues to Segmentation in a Hindi-English Code-switched Discourse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ylmaz18c_interspeech.html": {
    "title": "Building a Unified Code-Switching ASR System for South African Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/guo18b_interspeech.html": {
    "title": "Study of Semi-supervised Approaches to Improving English-Mandarin Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ylmaz18_interspeech.html": {
    "title": "Acoustic and Textual Data Augmentation for Improved ASR of Code-Switching Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/soto18_interspeech.html": {
    "title": "The Role of Cognate Words, POS Tags and Entrainment in Code-Switching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/srivastava18_interspeech.html": {
    "title": "Homophone Identification and Merging for Code-switched Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/thomas18_interspeech.html": {
    "title": "Code-switching in Indic Speech Synthesisers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ganji18_interspeech.html": {
    "title": "A Novel Approach for Effective Recognition of the Code-Switched Data on Monolingual Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/viswanathan18_interspeech.html": {
    "title": "Hierarchical Accent Determination and Application in a Large Scale ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ramanarayanan18_interspeech.html": {
    "title": "Toward Scalable Dialog Technology for Conversational Language Learning: Case Study of the TOEFLÂ® MOOC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/freitas18_interspeech.html": {
    "title": "Machine Learning Powered Data Platform for High-Quality Speech and NLP Workflows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cohen18_interspeech.html": {
    "title": "Fully Automatic Speaker Separation System, with Automatic Enrolling of Recurrent Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ayyavu18_interspeech.html": {
    "title": "Online Speech Translation System for Tamil",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shah18c_interspeech.html": {
    "title": "Unsupervised Vocal Tract Length Warped Posterior Features for Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhou18b_interspeech.html": {
    "title": "Voice Conversion with Conditional SampleRNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sisman18_interspeech.html": {
    "title": "A Voice Conversion Framework with Tandem Feature Sparse Representation and Speaker-Adapted WaveNet Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18_interspeech.html": {
    "title": "WaveNet Vocoder with Limited Training Data for Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wu18_interspeech.html": {
    "title": "Collapsed Speech Segment Detection and Suppression for WaveNet Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18h_interspeech.html": {
    "title": "High-quality Voice Conversion Using Spectrogram-Based WaveNet Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bonafonte18_interspeech.html": {
    "title": "Spanish Statistical Parametric Speech Synthesis Using a Neural Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/podsiado18_interspeech.html": {
    "title": "Experiments with Training Corpora for Statistical Text-to-speech Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gu18_interspeech.html": {
    "title": "Multi-task WaveNet: A Multi-task Generative Model for Statistical Parametric Speech Synthesis without Fundamental Frequency Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/juvela18_interspeech.html": {
    "title": "Speaker-independent Raw Waveform Model for Glottal Excitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cui18_interspeech.html": {
    "title": "A New Glottal Neural Vocoder for Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/watts18_interspeech.html": {
    "title": "Exemplar-based Speech Waveform Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kawahara18_interspeech.html": {
    "title": "Frequency Domain Variants of Velvet Noise and Their Application to Speech Processing and Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chung18_interspeech.html": {
    "title": "Joint Learning of Interactive Spoken Content Retrieval and Trainable User Simulator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shan18_interspeech.html": {
    "title": "Attention-based End-to-End Models for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/m18_interspeech.html": {
    "title": "Prediction of Aesthetic Elements in Karnatic Music: A Machine Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18e_interspeech.html": {
    "title": "Topic and Keyword Identification for Low-resourced Speech Using Cross-Language Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wiesner18_interspeech.html": {
    "title": "Automatic Speech Recognition and Topic Identification from Speech for Almost-Zero-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xiao18_interspeech.html": {
    "title": "Play Duration Based User-Entity Affinity Modeling in Spoken Dialog System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lee18e_interspeech.html": {
    "title": "Empirical Analysis of Score Fusion Application to Combined Neural Networks for Open Vocabulary Spoken Term Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/asaei18_interspeech.html": {
    "title": "Phonological Posterior Hashing for Query by Example Spoken Term Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kucza18_interspeech.html": {
    "title": "Term Extraction via Neural Sequence Labeling a Comparative Evaluation of Strategies Using Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kannan18_interspeech.html": {
    "title": "Semi-supervised Learning for Information Extraction from Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shin18_interspeech.html": {
    "title": "Slot Filling with Delexicalized Sentence Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ghosal18_interspeech.html": {
    "title": "Music Genre Recognition Using Deep Neural Networks and Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sigtia18_interspeech.html": {
    "title": "Efficient Voice Trigger Detection for Low Resource Hardware",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lin18_interspeech.html": {
    "title": "A Novel Normalization Method for Autocorrelation Function for Pitch Detection and for Speech Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ananthapadmanabha18_interspeech.html": {
    "title": "Estimation of the Vocal Tract Length of Vowel Sounds Based on the Frequency of the Significant Spectral Valley",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/himawan18_interspeech.html": {
    "title": "Deep Learning Techniques for Koala Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/matousek18_interspeech.html": {
    "title": "Glottal Closure Instant Detection from Speech Signal Using Voting Classifier and Recursive Feature Elimination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yousefi18_interspeech.html": {
    "title": "Assessing Speaker Engagement in 2-Person Debates: Overlap Detection in United States Presidential Debates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pankajakshan18_interspeech.html": {
    "title": "All-Conv Net for Bird Activity Detection: Significance of Learned Pooling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/thakur18_interspeech.html": {
    "title": "Deep Convex Representations: Feature Representations for Bioacoustics Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dasgupta18_interspeech.html": {
    "title": "Detection of Glottal Excitation Epochs in Speech Signal Using Hilbert Envelope",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18m_interspeech.html": {
    "title": "Analyzing Thai Tone Distribution through Functional Data Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/merkx18_interspeech.html": {
    "title": "Articulatory Feature Classification Using Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lin18c_interspeech.html": {
    "title": "A New Frequency Coverage Metric and a New Subband Encoding Model, with an Application in Pitch Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gowri18_interspeech.html": {
    "title": "Improved Epoch Extraction from Telephonic Speech Using Chebfun and Zero Frequency Filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kohn18_interspeech.html": {
    "title": "An Empirical Analysis of the Correlation of Syntax and Prosody",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/baumann18_interspeech.html": {
    "title": "Analysing the Focus of a Hierarchical Attention Network: the Importance of Enjambments When Classifying Post-modern Poetry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kocharov18_interspeech.html": {
    "title": "Language-Dependent Melody Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jia18_interspeech.html": {
    "title": "Stress Distribution of Given Information in Chinese Reading Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cabarrao18_interspeech.html": {
    "title": "Acoustic-prosodic Entrainment in Structural Metadata Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tabain18_interspeech.html": {
    "title": "Formant Measures of Vowels Adjacent to Alveolar and Retroflex Consonants in Arrernte: Stressed and Unstressed Position",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/truong18_interspeech.html": {
    "title": "Automatic Assessment of L2 English Word Prosody Using Weighted Distances of F0 and Intensity Contours",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/maxwell18_interspeech.html": {
    "title": "Homogeneity vs Heterogeneity in Indian English: Investigating Influences of L1 on f0 Range",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18_interspeech.html": {
    "title": "Emotional Prosody Perception in Mandarin-speaking Congenital Amusics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shochi18_interspeech.html": {
    "title": "Cultural Differences in Pattern Matching: Multisensory Recognition of Socio-affective Prosody",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mesgarani18_interspeech.html": {
    "title": "Speech Processing in the Human Brain Meets Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/watanabe18_interspeech.html": {
    "title": "ESPnet: End-to-End Speech Processing Toolkit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18f_interspeech.html": {
    "title": "A GPU-based WFST Decoder with Exact Lattice Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ragni18_interspeech.html": {
    "title": "Automatic Speech Recognition System Development in the \"Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/velikovich18_interspeech.html": {
    "title": "Semantic Lattice Processing in Contextual Automatic Speech Recognition for Google Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/williams18_interspeech.html": {
    "title": "Contextual Speech Recognition in End-to-end Neural Network Systems Using Beam Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mimura18_interspeech.html": {
    "title": "Forward-Backward Attention Decoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yadav18_interspeech.html": {
    "title": "Learning Discriminative Features for Speaker Identification and Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/novoselov18_interspeech.html": {
    "title": "Triplet Loss Based Cosine Similarity Metric Learning for Text-independent Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18b_interspeech.html": {
    "title": "Speaker Embedding Extraction with Phonetic Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/okabe18_interspeech.html": {
    "title": "Attentive Statistics Pooling for Deep Speaker Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/le18_interspeech.html": {
    "title": "Robust and Discriminative Speaker Embedding via Intra-Class Distance Variance Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18l_interspeech.html": {
    "title": "Deep Discriminative Embeddings for Duration Robust Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/simantiraki18_interspeech.html": {
    "title": "Impact of Different Speech Types on Listening Effort",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/huet18_interspeech.html": {
    "title": "Who Are You Listening to? Towards a Dynamic Measure of Auditory Attention to Speech-on-speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kim18d_interspeech.html": {
    "title": "Investigating the Role of Familiar Face and Voice Cues in Speech Processing in Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/scharenborg18_interspeech.html": {
    "title": "The Conversation Continues: the Effect of Lyrics and Music Complexity of Background Music on Spoken-Word Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/meyer18b_interspeech.html": {
    "title": "Loud and Shouted Speech Perception at Variable Distances in a Forest",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/docarmoblanco18_interspeech.html": {
    "title": "Phoneme Resistance and Phoneme Confusion in Noise: Impact of Dyslexia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/haque18_interspeech.html": {
    "title": "Conditional End-to-End Audio Transforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/aneeja18_interspeech.html": {
    "title": "Detection of Glottal Closure Instants in Degraded Speech Using Single Frequency Filtering Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lugosch18_interspeech.html": {
    "title": "Tone Recognition Using Lifters and CTC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vikram18_interspeech.html": {
    "title": "Epoch Extraction from Pathological Children Speech Using Single Pole Filtering Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bt18_interspeech.html": {
    "title": "Automated Classification of Vowel-Gesture Parameters Using External Broadband Excitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kadiri18_interspeech.html": {
    "title": "Estimation of Fundamental Frequency from Singing Voice Using Harmonics of Impulse-like Excitation Source",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/weiner18_interspeech.html": {
    "title": "Investigating the Effect of Audio Duration on Dementia Detection Using Acoustic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lin18b_interspeech.html": {
    "title": "An Interlocutor-Modulated Attentional LSTM for Differentiating between Subgroups of Autism Spectrum Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/amiriparian18_interspeech.html": {
    "title": "Recognition of Echolalic Autistic Child Vocalisations Utilising Convolutional Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nallanchakravarthula18_interspeech.html": {
    "title": "Modeling Interpersonal Influence of Verbal Behavior in Couples Therapy Dyadic Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ramakrishna18_interspeech.html": {
    "title": "Computational Modeling of Conversational Humor in Psychotherapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/garcia18_interspeech.html": {
    "title": "Multimodal I-vectors to Detect and Evaluate Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/baur18_interspeech.html": {
    "title": "Overview of the 2018 Spoken CALL Shared Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/julg18_interspeech.html": {
    "title": "The CSU-K Rule-Based System for the 2nd Edition Spoken CALL Shared Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nguyen18_interspeech.html": {
    "title": "Liulishuo's System for the Spoken CALL Shared Task 2018",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ateeq18_interspeech.html": {
    "title": "An Optimization Based Approach for Solving Spoken CALL Shared Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/qian18_interspeech.html": {
    "title": "The University of Birmingham 2018 Spoken CALL Shared Task Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/evanini18_interspeech.html": {
    "title": "Improvements to an Automated Content Scoring System for Spoken CALL Responses: the ETS Submission to the Second Spoken CALL Shared Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/goel18_interspeech.html": {
    "title": "Extracting Speaker's Gender, Accent, Age and Emotional State from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/narayanamurthy18_interspeech.html": {
    "title": "Determining Speaker Location from Speech in a Practical Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/patel18b_interspeech.html": {
    "title": "An Automatic Speech Transcription System for Manipuri Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yarra18_interspeech.html": {
    "title": "SPIRE-SST: An Automatic Web-based Self-learning Tool for Syllable Stress Tutoring (SST) to the Second Language Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chakraborty18_interspeech.html": {
    "title": "Glotto Vibrato Graph: A Device and Method for Recording, Analysis and Visualization of Glottal Activity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/renduchintala18_interspeech.html": {
    "title": "Multi-Modal Data Augmentation for End-to-end ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/moriya18_interspeech.html": {
    "title": "Multi-task Learning with Augmentation Strategy for Acoustic-to-word Attention-based Encoder-decoder Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sun18_interspeech.html": {
    "title": "Training Augmentation with Adversarial Examples for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fukuda18_interspeech.html": {
    "title": "Data Augmentation Improves Recognition of Foreign Accented Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tomashenko18_interspeech.html": {
    "title": "Speaker Adaptive Training and Mixup Regularization for Neural Network Acoustic Models in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/muller18_interspeech.html": {
    "title": "Neural Language Codes for Multilingual Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ueno18_interspeech.html": {
    "title": "Encoder Transfer for Attention-based Acoustic-to-word Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18j_interspeech.html": {
    "title": "Empirical Evaluation of Speaker Adaptation on DNN Based Acoustic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/das18c_interspeech.html": {
    "title": "Improving DNNs Trained with Non-Native Transcriptions Using Knowledge Distillation and Target Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/feng18b_interspeech.html": {
    "title": "Improving Cross-Lingual Knowledge Transferability Using Multilingual TDNN-BLSTM with Language-Dependent Pre-Final Layer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/delcroix18_interspeech.html": {
    "title": "Auxiliary Feature Based Adaptation of End-to-end ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ghorbani18_interspeech.html": {
    "title": "Leveraging Native Language Information for Improved Accented Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jain18_interspeech.html": {
    "title": "Improved Accented Speech Recognition Using Accent Embeddings and Multi-task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tong18_interspeech.html": {
    "title": "Fast Language Adaptation Using Phonological Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/murakami18_interspeech.html": {
    "title": "Naturalness Improvement Algorithm for Reconstructed Glossectomy Patient's Speech Using Spectral Differential Modification in Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tamura18_interspeech.html": {
    "title": "Audio-visual Voice Conversion Using Deep Canonical Correlation Analysis for Deep Bottleneck Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/baljekar18_interspeech.html": {
    "title": "An Investigation of Convolution Attention Based Models for Multilingual Speech Synthesis of Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/websdale18_interspeech.html": {
    "title": "The Effect of Real-Time Constraints on Automatic Speech Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/greenwood18_interspeech.html": {
    "title": "Joint Learning of Facial Expression and Head Pose from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vythelingum18_interspeech.html": {
    "title": "Acoustic-dependent Phonemic Transcription for Text-to-speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/luong18b_interspeech.html": {
    "title": "Multimodal Speech Synthesis Architecture for Unsupervised Speaker Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/taguchi18_interspeech.html": {
    "title": "Articulatory-to-speech Conversion Using Bi-directional Long Short-term Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tanihara18_interspeech.html": {
    "title": "Implementation of Respiration in Articulatory Synthesis Using a Pressure-Volume Lung Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhou18c_interspeech.html": {
    "title": "Learning and Modeling Unit Embeddings for Improving HMM-based Unit Selection Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fu18b_interspeech.html": {
    "title": "Deep Metric Learning for the Target Cost in Unit-Selection Speech Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sone18_interspeech.html": {
    "title": "DNN-based Speech Synthesis for Small Data Sets Considering Bidirectional Speech-Text Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gerazov18_interspeech.html": {
    "title": "A Weighted Superposition of Functional Contours Model for Modelling Contextual Prominence of Elementary Prosodic Contours",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nakashika18_interspeech.html": {
    "title": "LSTBM: A Novel Sequence Representation of Speech Spectra Using Restricted Boltzmann Machine with Long Short-Term Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bullock18_interspeech.html": {
    "title": "Should Code-switching Models Be Asymmetric?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tsukada18_interspeech.html": {
    "title": "Cross-language Perception of Mandarin Lexical Tones by Mongolian-speaking Bilinguals in the Inner Mongolia Autonomous Region, China",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fontan18_interspeech.html": {
    "title": "Automatically Measuring L2 Speech Fluency without the Need of ASR: A Proof-of-concept Study with Japanese Learners of French",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sun18c_interspeech.html": {
    "title": "Analysis of L2 Learners' Progress of Distinguishing Mandarin Tone 2 and Tone 3",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18o_interspeech.html": {
    "title": "Unsupervised Discovery of Non-native Phonetic Patterns in L2 English Speech for Mispronunciation Detection and Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18m_interspeech.html": {
    "title": "Wuxi Speakers' Production and Perception of Coda Nasals in Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dyrenko18_interspeech.html": {
    "title": "The Diphthongs of Formal Nigerian English: A Preliminary Acoustic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/davis18_interspeech.html": {
    "title": "Characterizing Rhythm Differences between Strong and Weak Accented L2 Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fringi18_interspeech.html": {
    "title": "Analysis of Phone Errors Attributable to Phonological Effects Associated With Language Acquisition Through Bottleneck Feature Visualisations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/koreman18_interspeech.html": {
    "title": "Category Similarity in Multilingual Pronunciation Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cristia18_interspeech.html": {
    "title": "Talker Diarization in the Wild: the Case of Child-centered Daylong Audio-recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18o_interspeech.html": {
    "title": "Automated Classification of Children's Linguistic versus Non-Linguistic Vocalisations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yuan18b_interspeech.html": {
    "title": "Pitch Characteristics of L2 English Speech by Chinese Speakers: A Large-scale Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/garg18_interspeech.html": {
    "title": "Dual Language Models for Code Switched Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/biswas18_interspeech.html": {
    "title": "Multilingual Neural Network Acoustic Modelling for ASR of Under-Resourced English-isiZulu Code-Switched Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/menon18_interspeech.html": {
    "title": "Fast ASR-free and Almost Zero-resource Keyword Spotting Using DTW and CNNs for Humanitarian Monitoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yu18c_interspeech.html": {
    "title": "Text-Dependent Speech Enhancement for Small-Footprint Robust Keyword Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/he18_interspeech.html": {
    "title": "Improved ASR for Under-resourced Languages through Multi-task Learning with Acoustic Landmarks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chibuye18_interspeech.html": {
    "title": "Cross-language Phoneme Mapping for Low-resource Languages: An Exploration of Benefits and Trade-offs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tundik18_interspeech.html": {
    "title": "User-centric Evaluation of Automatic Punctuation in ASR Closed Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zelasko18_interspeech.html": {
    "title": "Punctuation Prediction Model for Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/karafiat18_interspeech.html": {
    "title": "BUT OpenSAT 2017 Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18h_interspeech.html": {
    "title": "Visual Recognition of Continuous Cued Speech Using a Tandem CNN-HMM Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/thangthai18_interspeech.html": {
    "title": "Building Large-vocabulary Speaker-independent Lipreading Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gupta18b_interspeech.html": {
    "title": "CRIM's System for the MGB-3 English Multi-Genre Broadcast Media Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/riad18_interspeech.html": {
    "title": "Sampling Strategies in Siamese Networks for Unsupervised Speech Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18c_interspeech.html": {
    "title": "Compact Feedforward Sequential Memory Networks for Small-footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hermann18_interspeech.html": {
    "title": "Multilingual Bottleneck Features for Subword Modeling in Zero-resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/feng18_interspeech.html": {
    "title": "Exploiting Speaker and Phonetic Diversity of Mismatched Language Resources for Unsupervised Subword Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/godard18_interspeech.html": {
    "title": "Unsupervised Word Segmentation from Speech with Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/holzenberger18_interspeech.html": {
    "title": "Learning Word Embeddings: Unsupervised Methods for Fixed-size Representations of Variable-length Speech Segments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/glarner18_interspeech.html": {
    "title": "Full Bayesian Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/milde18_interspeech.html": {
    "title": "Unspeech: Unsupervised Speech Context Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gong18b_interspeech.html": {
    "title": "Impact of Aliasing on Deep CNN-Based End-to-End Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sivasankaran18_interspeech.html": {
    "title": "Keyword Based Speaker Localization: Localizing a Target Speaker in a Multi-speaker Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18f_interspeech.html": {
    "title": "End-to-End Speech Separation with Unfolded Iterative Phase Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/takahashi18_interspeech.html": {
    "title": "PhaseNet: Discretized Phase Modeling with Deep Neural Networks for Audio Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18k_interspeech.html": {
    "title": "Integrating Spectral and Spatial Features for Multi-Channel Speaker Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gogate18_interspeech.html": {
    "title": "DNN Driven Speaker Independent Audio-Visual Mask Estimation for Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vasilescu18_interspeech.html": {
    "title": "Exploring Temporal Reduction in Dialectal Spanish: A Large-scale Study of Lenition of Voiced Stops and Coda-s",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rose18_interspeech.html": {
    "title": "Dialect-geographical Acoustic-Tonetics: Five Disyllabic Tone Sandhi Patterns in Cognate Words from the Wu Dialects of ZhÃ¨JiÄNg Province",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/leemann18_interspeech.html": {
    "title": "Regional Variation of /r/ in Swiss German Dialects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/earnshaw18_interspeech.html": {
    "title": "Variation in the FACE Vowel across West Yorkshire: Implications for Forensic Speaker Comparisons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gold18_interspeech.html": {
    "title": "The â€˜West Yorkshire Regional English Database': Investigations into the Generalizability of Reference Populations for Forensic Speaker Comparison Casework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wottawa18_interspeech.html": {
    "title": "Studying Vowel Variation in French-Algerian Arabic Code-switched Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hansen18_interspeech.html": {
    "title": "Fearless Steps: Apollo-11 Corpus Advancements for Speech Technologies from Earth to the Moon",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kumar18_interspeech.html": {
    "title": "A Knowledge Driven Structural Segmentation Approach for Play-Talk Classification During Autism Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/james18_interspeech.html": {
    "title": "An Open Source Emotional Speech Corpus for Human Robot Interaction Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lapidot18_interspeech.html": {
    "title": "Speech Database and Protocol Validation Using Waveform Entropy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/terissi18_interspeech.html": {
    "title": "A French-Spanish Multimodal Speech Communication Corpus Incorporating Acoustic Data, Facial, Hands and Arms Gestures Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhao18b_interspeech.html": {
    "title": "L2-ARCTIC: A Non-native English Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zajic18_interspeech.html": {
    "title": "ZCU-NTIS Speaker Diarization System for the DIHARD 2018 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sun18b_interspeech.html": {
    "title": "Speaker Diarization with Enhancing Speech for the First DIHARD Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/diez18_interspeech.html": {
    "title": "BUT System for DIHARD Speech Diarization Challenge 2018",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vinals18_interspeech.html": {
    "title": "Estimation of the Number of Speakers with Variational Bayesian PLDA in the DIHARD Diarization Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sell18_interspeech.html": {
    "title": "Diarization is Hard: Some Experiences and Lessons Learned for the JHU Team in the Inaugural DIHARD Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/patino18_interspeech.html": {
    "title": "The EURECOM Submission to the First DIHARD Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/miasatofilho18_interspeech.html": {
    "title": "Joint Discriminative Embedding Learning, Speech Activity and Overlap Detection for the DIHARD Speaker Diarization Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ni18_interspeech.html": {
    "title": "Multilingual Grapheme-to-Phoneme Conversion with Global Character Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/roy18_interspeech.html": {
    "title": "A Hybrid Approach to Grapheme to Phoneme Conversion in Assamese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mohammadi18_interspeech.html": {
    "title": "Investigation of Using Disentangled and Interpretable Representations for One-shot Cross-lingual Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/govender18_interspeech.html": {
    "title": "Using Pupillometry to Measure the Cognitive Load of Synthetic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/govender18b_interspeech.html": {
    "title": "Measuring the Cognitive Load of Synthetic Speech Using a Dual Task Paradigm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/orife18_interspeech.html": {
    "title": "Attentive Sequence-to-Sequence Learning for Diacritic Restoration of YorÃ¹BÃ¡ Language Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18_interspeech.html": {
    "title": "Gated Convolutional Neural Network for Sentence Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sharma18b_interspeech.html": {
    "title": "On Training and Evaluation of Grapheme-to-Phoneme Mappings with Limited Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/baird18_interspeech.html": {
    "title": "The Perception and Analysis of the Likeability and Human Likeness of Synthesized Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mass18_interspeech.html": {
    "title": "Word Emphasis Prediction for Expressive Text to Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lee18b_interspeech.html": {
    "title": "A Comparison of Speaker-based and Utterance-based Data Selection for Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/toman18_interspeech.html": {
    "title": "Data Requirements, Selection and Augmentation for DNN-based Speech Synthesis from Crowdsourced Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vesely18_interspeech.html": {
    "title": "Lightly Supervised vs. Semi-supervised Training of Acoustic Model on Luxembourgish for Low-resource Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wenjie18_interspeech.html": {
    "title": "Investigation on the Combination of Batch Normalization and Dropout in BLSTM-based Acoustic Modeling for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/suzuki18_interspeech.html": {
    "title": "Inference-Invariant Transformation of Batch Normalization for Domain Adaptation of Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/long18_interspeech.html": {
    "title": "Active Learning for LF-MMI Trained Neural Networks in ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/medennikov18_interspeech.html": {
    "title": "An Investigation of Mixup Training Strategies for Acoustic Models in ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/agrawal18_interspeech.html": {
    "title": "Comparison of Unsupervised Modulation Filter Learning Methods for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kim18f_interspeech.html": {
    "title": "Improved Training for Online End-to-end Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/haider18c_interspeech.html": {
    "title": "Combining Natural Gradient with Hessian Free Methods for Sequence Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kanda18_interspeech.html": {
    "title": "Lattice-free State-level Minimum Bayes Risk Training of Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tang18b_interspeech.html": {
    "title": "A Study of Enhancement, Augmentation and Autoencoder Methods for Domain Adaptation in Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kirkedal18_interspeech.html": {
    "title": "Multilingual Deep Neural Network Training Using Cyclical Learning Rate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yu18b_interspeech.html": {
    "title": "Development of the CUHK Dysarthric Speech Recognition System for the UA Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/laaridh18b_interspeech.html": {
    "title": "Automatic Evaluation of Speech Intelligibility Based on I-vectors in the Context of Head and Neck Cancers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kim18e_interspeech.html": {
    "title": "Dysarthric Speech Recognition Using Convolutional LSTM Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/laaridh18_interspeech.html": {
    "title": "Perceptual and Automatic Evaluations of the Intelligibility of Speech Degraded by Noise Induced Hearing Loss Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ylmaz18b_interspeech.html": {
    "title": "Articulatory Features for ASR of Pathological Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/correia18_interspeech.html": {
    "title": "Mining Multimodal Repositories for Speech Affecting Diseases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/qin18_interspeech.html": {
    "title": "Long Distance Voice Channel Diagnosis Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chiu18_interspeech.html": {
    "title": "Speech Recognition for Medical Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/farah18_interspeech.html": {
    "title": "Prosodic Focus Acquisition in French Early Cochlear Implanted Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fezza18_interspeech.html": {
    "title": "The Role of Temporal Variation in Narrative Organization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/murtola18_interspeech.html": {
    "title": "Interaction Mechanisms between Glottal Source and Vocal Tract in Pitch Glides",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/singh18b_interspeech.html": {
    "title": "Relating Articulatory Motions in Different Speaking Rates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cabral18_interspeech.html": {
    "title": "Estimation of the Asymmetry Parameter of the Glottal Flow Waveform Using the Electroglottographic Signal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mandal18_interspeech.html": {
    "title": "Classification of Disorders in Vocal Folds Using Electroglottographic Signal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/raomv18_interspeech.html": {
    "title": "Automatic Glottis Localization and Segmentation in Stroboscopic Videos Using Deep Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/iseijaakkola18_interspeech.html": {
    "title": "Respiratory and Respiratory Muscular Control in JL1's and JL2's Text Reading Utilizing 4-RSTs and a Soft Respiratory Mask with a Two-Way Bulb",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hao18_interspeech.html": {
    "title": "A Preliminary Study on Tonal Coarticulation in Continuous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/meng18d_interspeech.html": {
    "title": "Speech and Language Processing for Learning and Wellbeing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ganapathy18_interspeech.html": {
    "title": "Far-Field Speech Recognition Using Multivariate Autoregressive Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kim18g_interspeech.html": {
    "title": "Efficient Implementation of the Room Simulator for Training Deep Neural Network Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18b_interspeech.html": {
    "title": "Stream Attention for Distributed Multi-Microphone Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yoshioka18_interspeech.html": {
    "title": "Recognizing Overlapped Speech in Meetings: A Multichannel Separation Approach Using Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/drude18_interspeech.html": {
    "title": "Integrating Neural Network Based Beamforming and Weighted Prediction Error Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bu18_interspeech.html": {
    "title": "A Probability Weighted Beamformer for Noise Robust ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yokoyama18_interspeech.html": {
    "title": "Effects of Dimensional Input on Paralinguistic Information Perceived from Synthesized Dialogue Speech with Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/oraby18_interspeech.html": {
    "title": "Neural MultiVoice Models for Expressing Novel Personalities in Dialog",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jauk18_interspeech.html": {
    "title": "Expressive Speech Synthesis Using Sentiment Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/akuzawa18_interspeech.html": {
    "title": "Expressive Speech Synthesis via Modeling Expressions with Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wu18d_interspeech.html": {
    "title": "Rapid Style Adaptation Using Residual Error Embedding for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18j_interspeech.html": {
    "title": "EMPHASIS: An Emotional Phoneme-based Acoustic Model for Speech Synthesis System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/han18_interspeech.html": {
    "title": "Bags in Bag: Generating Context-Aware Bags for Tracking Emotions from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18c_interspeech.html": {
    "title": "An Attention Pooling Based Representation Learning Method for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yang18c_interspeech.html": {
    "title": "Predicting Arousal and Valence from Waveforms and Spectrograms Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sarma18_interspeech.html": {
    "title": "Emotion Identification from Raw Speech Signals Using DNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18f_interspeech.html": {
    "title": "Encoding Individual Acoustic Features Using Dyad-Augmented Deep Variational Representations for Dialog-level Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/latif18_interspeech.html": {
    "title": "Variational Autoencoders for Learning Latent Representations of Speech Emotion: A Preliminary Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/biasuttolervat18_interspeech.html": {
    "title": "Phoneme-to-Articulatory Mapping Using Bidirectional Gated RNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/su18c_interspeech.html": {
    "title": "Tongue Segmentation with Geometrically Constrained Snake Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/illa18_interspeech.html": {
    "title": "Low Resource Acoustic-to-articulatory Inversion Using Bi-directional Long Short Term Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/s18_interspeech.html": {
    "title": "Automatic Visual Augmentation for Concatenation Based Synthesized Articulatory Videos from Real-time MRI Data for Spoken Language Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ca18_interspeech.html": {
    "title": "Air-Tissue Boundary Segmentation in Real-Time Magnetic Resonance Imaging Video Using Semantic Segmentation with Fully Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/seneviratne18_interspeech.html": {
    "title": "Noise Robust Acoustic to Articulatory Speech Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ahmadi18_interspeech.html": {
    "title": "Designing a Pneumatic Bionic Voice Prosthesis - A Statistical Approach for Source Excitation Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/schnell18_interspeech.html": {
    "title": "A Neural Model to Predict Parameters for a Generalized Command Response Model of Intonation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cao18b_interspeech.html": {
    "title": "Articulation-to-Speech Synthesis Using Articulatory Flesh Point Sensors' Orientation Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shah18b_interspeech.html": {
    "title": "Effectiveness of Generative Adversarial Network for Non-Audible Murmur-to-Whisper Speech Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/diener18_interspeech.html": {
    "title": "Investigating Objective Intelligibility in Real-Time EMG-to-Speech Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wand18_interspeech.html": {
    "title": "Domain-Adversarial Training for Session Independent EMG-based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/toth18_interspeech.html": {
    "title": "Multi-Task Learning of Speech Recognition and Speech Synthesis Parameters for Ultrasound-based Silent Speech Interfaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jprakash18_interspeech.html": {
    "title": "Transcription Correction for Indian Languages Using Acoustic Signatures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pulugundla18_interspeech.html": {
    "title": "BUT System for Low Resource Indian Language ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sailor18b_interspeech.html": {
    "title": "DA-IICT/IIITV System for Low Resource Speech Recognition Challenge 2018",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vydana18_interspeech.html": {
    "title": "An Exploration towards Joint Acoustic Modeling for Indian Languages: IIIT-H Submission for Low Resource Speech Recognition Challenge for Indian Languages, INTERSPEECH 2018",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fathima18_interspeech.html": {
    "title": "TDNN-based Multilingual Speech Recognition System for Low Resource Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shetty18_interspeech.html": {
    "title": "Articulatory and Stacked Bottleneck Features for Low Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/billa18_interspeech.html": {
    "title": "ISI ASR System for the Low Resource Speech Recognition Challenge for Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/finley18_interspeech.html": {
    "title": "An Automated Assistant for Medical Scribes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dey18c_interspeech.html": {
    "title": "AGROASSAM: A Web Based Assamese Speech Recognition Application for Retrieving Agricultural Commodity Price and Weather Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/aharon18_interspeech.html": {
    "title": "Voice-powered Solutions with Cloud AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sivaraman18_interspeech.html": {
    "title": "Speech Synthesis in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nie18_interspeech.html": {
    "title": "Deep Noise Tracking Network: A Hybrid Signal Processing/Deep Learning Approach to Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ouyang18_interspeech.html": {
    "title": "A Deep Neural Network Based Harmonic Noise Model for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tan18_interspeech.html": {
    "title": "A Convolutional Recurrent Neural Network for Real-Time Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18h_interspeech.html": {
    "title": "All-Neural Multi-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18g_interspeech.html": {
    "title": "Deep Learning for Acoustic Echo Cancellation in Noisy and Double-Talk Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/afouras18_interspeech.html": {
    "title": "The Conversation: Deep Audio-Visual Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/subramanian18_interspeech.html": {
    "title": "Student-Teacher Learning for BLSTM Mask-based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/karjol18_interspeech.html": {
    "title": "Speech Enhancement Using Deep Mixture of Experts Based on Hard Expectation Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/meng18c_interspeech.html": {
    "title": "Adversarial Feature-Mapping for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/baby18_interspeech.html": {
    "title": "Biophysically-inspired Features Improve the Generalizability of Neural Network-based Speech Enhancement Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chai18_interspeech.html": {
    "title": "Error Modeling via Asymmetric Laplace Distribution for Deep Neural Network Based Single-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xia18_interspeech.html": {
    "title": "A Priori SNR Estimation Based on a Recurrent Neural Network for Robust Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tseng18_interspeech.html": {
    "title": "Multiple Instance Deep Learning for Weakly Supervised Small-Footprint Audio Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18f_interspeech.html": {
    "title": "Unsupervised Temporal Feature Learning Based on Sparse Coding Embedded BoAW for Acoustic Event Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/teng18_interspeech.html": {
    "title": "Data Independent Sequence Augmentation Method for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/song18_interspeech.html": {
    "title": "A Compact and Discriminative Feature Based on Auditory Summary Statistics for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sharma18_interspeech.html": {
    "title": "ASe: Acoustic Scene Embedding Using Deep Archetypal Analysis and GMM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18g_interspeech.html": {
    "title": "Deep Convolutional Neural Network with Scalogram for Audio Scene Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/joshi18_interspeech.html": {
    "title": "Time Aggregation Operators for Multi-label Audio Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mcloughlin18_interspeech.html": {
    "title": "Early Detection of Continuous and Partial Audio Events Using CNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mulimani18_interspeech.html": {
    "title": "Robust Acoustic Event Classification Using Bag-of-Visual-Words",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/waldekar18_interspeech.html": {
    "title": "Wavelet Transform Based Mel-scaled Features for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18d_interspeech.html": {
    "title": "Multi-modal Attention Mechanisms in LSTM and Its Application to Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/raju18_interspeech.html": {
    "title": "Contextual Language Model Adaptation for Conversational Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18b_interspeech.html": {
    "title": "Active Memory Networks for Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/khassanov18_interspeech.html": {
    "title": "Unsupervised and Efficient Vocabulary Expansion for Recurrent Neural Network Language Models in ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18c_interspeech.html": {
    "title": "Improving Language Modeling with an Adversarial Critic for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/deng18_interspeech.html": {
    "title": "Training Recurrent Neural Network through Moment Matching for NLP Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tuske18_interspeech.html": {
    "title": "Investigation on LSTM Recurrent N-gram Language Models for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hu18_interspeech.html": {
    "title": "Online Incremental Learning for Speaker-Adaptive Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/andresferrer18_interspeech.html": {
    "title": "Efficient Language Model Adaptation with Noise Contrastive Estimation and Kullback-Leibler Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18e_interspeech.html": {
    "title": "Recurrent Neural Network Language Model Adaptation for Conversational Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/levit18_interspeech.html": {
    "title": "What to Expect from Expected Kneser-Ney Smoothing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/benes18_interspeech.html": {
    "title": "i-Vectors in Language Modeling: An Efficient Way of Domain Adaptation for Feed-Forward Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rathner18_interspeech.html": {
    "title": "How Did You like 2017? Detection of Language Markers of Depression and Narcissism in Personal Narratives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/huang18e_interspeech.html": {
    "title": "Depression Detection from Short Utterances via Diverse Smartphones in Natural Environmental Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ozkanca18_interspeech.html": {
    "title": "Multi-Lingual Depression-Level Assessment from Conversational Speech Using Acoustic and Text Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/np18_interspeech.html": {
    "title": "Dysarthric Speech Classification Using Glottal Features Computed from Non-words, Words and Sentences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gosztolya18b_interspeech.html": {
    "title": "Identifying Schizophrenia Based on Temporal Parameters in Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/singla18_interspeech.html": {
    "title": "Using Prosodic and Lexical Information for Learning Utterance-level Behaviors in Psychotherapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/qin18b_interspeech.html": {
    "title": "Automatic Speech Assessment for People with Aphasia Using TDNN-BLSTM with Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nasir18_interspeech.html": {
    "title": "Towards an Unsupervised Entrainment Distance in Conversational Speech Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/teixeira18_interspeech.html": {
    "title": "Patient Privacy in Paralinguistic Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/alharbi18_interspeech.html": {
    "title": "A Lightly Supervised Approach to Detect Stuttering in Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18d_interspeech.html": {
    "title": "Learning Conditional Acoustic Latent Representation with Gender and Age Attributes for Automatic Pain Level Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ganapathy18b_interspeech.html": {
    "title": "Speaker and Language Recognition -- From Laboratory Technologies to the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18e_interspeech.html": {
    "title": "A Deep Reinforcement Learning Based Multimodal Coaching Model (DCM) for Slot Filling in Spoken Language Understanding(SLU)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bechet18_interspeech.html": {
    "title": "Is ATIS Too Shallow to Go Deeper for Benchmarking Spoken Language Understanding Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ray18_interspeech.html": {
    "title": "Robust Spoken Language Understanding via Paraphrasing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lee18d_interspeech.html": {
    "title": "Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shen18_interspeech.html": {
    "title": "User Information Augmented Semantic Frame Parsing Using Progressive Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gupta18c_interspeech.html": {
    "title": "An Efficient Approach to Encoding Context for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hetherly18_interspeech.html": {
    "title": "Deep Speech Denoising with Vector Space Projections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xu18_interspeech.html": {
    "title": "A Shifted Delta Coefficient Objective for Monaural Speech Separation Using Multi-task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tan18b_interspeech.html": {
    "title": "A Two-Stage Approach to Noisy Cochannel Speech Separation with Gated Residual Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pandey18b_interspeech.html": {
    "title": "Monoaural Audio Source Separation Using Variational Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gang18_interspeech.html": {
    "title": "Towards Automated Single Channel Source Separation Using Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/erdogan18_interspeech.html": {
    "title": "Investigations on Data Augmentation and Loss Functions for Deep Learning Based Speech-Background Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hantke18_interspeech.html": {
    "title": "Annotator Trustability-based Cooperative Learning Solutions for Intelligent Audio Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/su18b_interspeech.html": {
    "title": "Semi-supervised Cross-domain Visual Feature Learning for Audio-Visual Broadcast Speech Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/afouras18b_interspeech.html": {
    "title": "Deep Lip Reading: A Comparison of Models and an Online Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/srinivasamurthy18_interspeech.html": {
    "title": "Iterative Learning of Speech Recognition Models for Air Traffic Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sari18_interspeech.html": {
    "title": "Speaker Adaptive Audio-Visual Fusion for the Open-Vocabulary Section of AVICAR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hruz18_interspeech.html": {
    "title": "Multimodal Name Recognition in Live TV Subtitling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/backstrom18_interspeech.html": {
    "title": "Dithered Quantization for Frequency-Domain Speech and Audio Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/das18_interspeech.html": {
    "title": "Postfiltering with Complex Spectral Correlations for Speech and Audio Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/das18b_interspeech.html": {
    "title": "Postfiltering Using Log-Magnitude Spectrum for Speech and Audio Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/biswas18b_interspeech.html": {
    "title": "Temporal Noise Shaping with Companding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18p_interspeech.html": {
    "title": "Multi-frame Quantization of LSF Parameters Using a Deep Autoencoder and Pyramid Vector Quantizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18q_interspeech.html": {
    "title": "Multi-frame Coding of LSF Parameters Using Block-Constrained Trellis Coded Vector Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/park18_interspeech.html": {
    "title": "Training Utterance-level Embedding Networks for Speaker Identification and Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nandwana18_interspeech.html": {
    "title": "Analysis of Complementary Information Sources in the Speaker Embeddings Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhu18_interspeech.html": {
    "title": "Self-Attentive Speaker Embeddings for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gao18_interspeech.html": {
    "title": "An Improved Deep Embedding Learning Method for Short Duration Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jung18b_interspeech.html": {
    "title": "Avoiding Speaker Overfitting in End-to-End DNNs Using Raw Waveform for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bhattacharya18_interspeech.html": {
    "title": "Deeply Fused Speaker Embeddings for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rahman18_interspeech.html": {
    "title": "Employing Phonetic Information in DNN Speaker Embeddings to Improve Speaker Recognition Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dey18b_interspeech.html": {
    "title": "End-to-end Text-dependent Speaker Verification Using Novel Distance Measures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dubey18_interspeech.html": {
    "title": "Robust Speaker Clustering using Mixtures of von Mises-Fisher Distributions for Naturalistic Audio Streams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/song18b_interspeech.html": {
    "title": "Triplet Network with Attention for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18j_interspeech.html": {
    "title": "I-vector Transformation Using Conditional Generative Adversarial Networks for Short Utterance Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cai18_interspeech.html": {
    "title": "Analysis of Length Normalization in End-to-End Speaker Verification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/huang18c_interspeech.html": {
    "title": "Angular Softmax for Short-Duration Text-independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ji18_interspeech.html": {
    "title": "An End-to-End Text-Independent Speaker Identification System on Short Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ding18_interspeech.html": {
    "title": "MTGAN: Speaker Verification through Multitasking Triplet Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/paradacabaleiro18_interspeech.html": {
    "title": "Categorical vs Dimensional Perception of Italian Emotional Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18m_interspeech.html": {
    "title": "A Three-Layer Emotion Perception Model for Valence and Arousal-Based Detection from Multilingual Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/desplanques18_interspeech.html": {
    "title": "Cross-lingual Speech Emotion Recognition through Factor Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cheng18c_interspeech.html": {
    "title": "Modeling Self-Reported and Observed Affect from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/huang18_interspeech.html": {
    "title": "Stochastic Shake-Shake Regularization for Affective Learning from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/avila18_interspeech.html": {
    "title": "Investigating Speech Enhancement and Perceptual Quality for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/atcheson18_interspeech.html": {
    "title": "Demonstrating and Modelling Systematic Time-varying Annotator Disagreement in Continuous Emotion Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/huang18b_interspeech.html": {
    "title": "Speech Emotion Recognition from Variable-Length Inputs with Triplet Loss Function",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18k_interspeech.html": {
    "title": "Imbalance Learning-based Framework for Fear Recognition in the MediaEval Emotional Impact of Movies Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ma18b_interspeech.html": {
    "title": "Emotion Recognition from Variable-Length Speech Segments Using Deep Learning on Spectrograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yenigalla18_interspeech.html": {
    "title": "Speech Emotion Recognition Using Spectrogram & Phoneme Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sahu18_interspeech.html": {
    "title": "On Enhancing Speech Emotion Recognition Using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/parthasarathy18_interspeech.html": {
    "title": "Ladder Networks for Emotion Recognition: Using Unsupervised Auxiliary Tasks to Improve Predictions of Emotional Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/huang18d_interspeech.html": {
    "title": "Knowledge Distillation for Sequence Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18h_interspeech.html": {
    "title": "Improving CTC-based Acoustic Model with Very Deep Residual Time-delay Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/guo18_interspeech.html": {
    "title": "Filter Sampling and Combination CNN (FSC-CNN): A Compact CNN Model for Small-footprint ASR Acoustic Modeling Using Raw Waveforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ravanelli18_interspeech.html": {
    "title": "Twin Regularization for Online Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sperber18_interspeech.html": {
    "title": "Self-Attentional Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/park18d_interspeech.html": {
    "title": "Hierarchical Recurrent Neural Networks for Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bruguier18b_interspeech.html": {
    "title": "Dictionary Augmented Sequence-to-Sequence Neural Network for Grapheme to Phoneme Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/raj18_interspeech.html": {
    "title": "Leveraging Second-Order Log-Linear Model for Improved Deep Learning Based ASR Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/povey18_interspeech.html": {
    "title": "Semi-Orthogonal Low-Rank Matrix Factorization for Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18g_interspeech.html": {
    "title": "Completely Unsupervised Phoneme Recognition by Adversarially Learning Mapping Relationships from Audio Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/qian18b_interspeech.html": {
    "title": "Phone Recognition Using a Non-Linear Manifold with Broad Phone Class Dependent DNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hosseiniasl18_interspeech.html": {
    "title": "A Multi-Discriminator CycleGAN for Unsupervised Non-Parallel Speech Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cao18_interspeech.html": {
    "title": "Interactions between Vowels and Nasal Codas in Mandarin Speakers' Perception of Nasal Finals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/meng18_interspeech.html": {
    "title": "Weighting Pitch Contour and Loudness Contour in Mandarin Tone Perception in Cochlear Implant Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nenadic18_interspeech.html": {
    "title": "Implementing DIANA to Model Isolated Auditory Word Recognition in English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sharma18d_interspeech.html": {
    "title": "Effects of Homophone Density on Spoken Word Recognition in Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xie18_interspeech.html": {
    "title": "Visual Timing Information in Audiovisual Speech Perception: Evidence from Lexical Tone Contour",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/barnaud18_interspeech.html": {
    "title": "COSMO SylPhon: A Bayesian Perceptuo-motor Model to Assess Phonological Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/maggu18_interspeech.html": {
    "title": "Experience-dependent Influence of Music and Language on Lexical Pitch Learning Is Not Additive",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dellwo18_interspeech.html": {
    "title": "Influences of Fundamental Oscillation on Speaker Identification in Vocalic Utterances by Humans and Computers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}