{
  "https://www.isca-speech.org/archive/interspeech_2018/atal18_interspeech.html": {
    "title": "From Vocoders to Code-Excited Linear Prediction: Learning How We Hear What We Hear",
    "volume": "main",
    "abstract": "It all started almost a century ago, in 1920s. A new undersea transatlantic telegraph cable had been laid. The idea of transmitting speech over the new telegraph cable caught the fancy of Homer Dudley, a young engineer who had just joined Bell Telephone Laboratories. This led to the invention of Vocoder - its close relative Voder was showcased as the first machine to create human speech at the 1939 New York World's Fair. However, the voice quality of vocoders was not good enough for use in commercial telephony. During the time speech scientists were busy with vocoders, several major developments took place outside speech research. Norbert Wiener developed a mathematical theory for calculating the best filters and predictors for detecting signals hidden in noise. Linear Prediction or Linear Predictive Coding became a major tool for speech processing. Claude Shannon established that the highest bit rate in a communication channel in presence of noise is achieved when the transmitted signal resembles random white Gaussian noise. Shannon's theory led to the invention of Code-Excited Linear Prediction (CELP). Nearly all digital cellular standards as well as standards for digital voice communication over the Internet use CELP coders. The success in speech coding came with understanding of what we hear and what we do not. Speech encoding at low bit rates introduce errors and these errors must be hidden under the speech signal to become inaudible. More and more, speech technologies are being used in different acoustic environments raising questions about the robustness of the technology. Human listeners handle situations well when the signal at our ears is not just one signal, but also a superposition of many acoustic signals. We need new research to develop signal-processing methods that can separate the mixed acoustic signal into individual components and provide performance similar or superior to that of human listeners",
    "checked": true,
    "id": "555b2345a62f4426c11eccd8174a184257fbc5b7",
    "semantic_title": "from vocoders to code-excited linear prediction: learning how we hear what we hear",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/karita18_interspeech.html": {
    "title": "Semi-Supervised End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "We propose a novel semi-supervised method for end-to-end automatic speech recognition (ASR). It can exploit large unpaired speech and text datasets, which require much less human effort to create paired speech-to-text datasets. Our semi-supervised method targets the extraction of an intermediate representation between speech and text data using a shared encoder network. Autoencoding of text data with this shared encoder improves the feature extraction of text data as well as that of speech data when the intermediate representations of speech and text are similar to each other as an inter-domain feature. In other words, by combining speech-to-text and text-to-text mappings through the shared network, we can improve speech-to-text mapping by learning to reconstruct the unpaired text data in a semi-supervised end-to-end manner. We investigate how to design suitable inter-domain loss, which minimizes the dissimilarity between the encoded speech and text sequences, which originally belong to quite different domains. The experimental results we obtained with our proposed semi-supervised training shows a larger character error rate reduction from 15.8% to 14.4% than a conventional language model integration on the Wall Street Journal dataset",
    "checked": true,
    "id": "933396f5b9111f6acdd76710ee6ab4d24e8673dd",
    "semantic_title": "semi-supervised end-to-end speech recognition",
    "citation_count": 46
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zeyer18_interspeech.html": {
    "title": "Improved Training of End-to-end Attention Models for Speech Recognition",
    "volume": "main",
    "abstract": "Sequence-to-sequence attention-based models on subword units allow simple open-vocabulary end-to-end speech recognition. In this work, we show that such models can achieve competitive results on the Switchboard 300h and LibriSpeech 1000h tasks. In particular, we report the state-of-the-art word error rates (WER) of 3.54% on the dev-clean and 3.82% on the test-clean evaluation subsets of LibriSpeech. We introduce a new pretraining scheme by starting with a high time reduction factor and lowering it during training, which is crucial both for convergence and final performance. In some experiments, we also use an auxiliary CTC loss function to help the convergence. In addition, we train long short-term memory (LSTM) language models on subword units. By shallow fusion, we report up to 27% relative improvements in WER over the attention baseline without a language model",
    "checked": true,
    "id": "f0ead45e8c1e4cc390ff6603bc0738b8c57f99ec",
    "semantic_title": "improved training of end-to-end attention models for speech recognition",
    "citation_count": 252
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hadian18_interspeech.html": {
    "title": "End-to-end Speech Recognition Using Lattice-free MMI",
    "volume": "main",
    "abstract": "We present our work on end-to-end training of acoustic models using the lattice-free maximum mutual information (LF-MMI) objective function in the context of hidden Markov models. By end-to-end training, we mean flat-start training of a single DNN in one stage without using any previously trained models, forced alignments, or building state-tying decision trees. We use full biphones to enable context-dependent modeling without trees and show that our end-to-end LF-MMI approach can achieve comparable results to regular LF-MMI on well-known large vocabulary tasks. We also compare with other end-to-end methods such as CTC in character-based and lexicon-free settings and show 5 to 25 percent relative reduction in word error rates on different large vocabulary tasks while using significantly smaller models",
    "checked": true,
    "id": "dcaeb29ad3307e2bdab2218416c81cb0c4e548b2",
    "semantic_title": "end-to-end speech recognition using lattice-free mmi",
    "citation_count": 147
  },
  "https://www.isca-speech.org/archive/interspeech_2018/braun18_interspeech.html": {
    "title": "Multi-channel Attention for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Recent end-to-end models for automatic speech recognition use sensory attention to integrate multiple input channels within a single neural network. However, these attention models are sensitive to the ordering of the channels used during training. This work proposes a sensory attention mechanism that is invariant to the channel ordering and only increases the overall parameter count by 0.09%. We demonstrate that even without re-training, our attention-equipped end-to-end model is able to deal with arbitrary numbers of input channels during inference. In comparison to a recent related model with sensory attention, our model when tested on the real noisy recordings from the multi-channel CHiME-4 dataset, achieves a relative character error rate (CER) improvement of 40.3% to 42.9%. In a two-channel configuration experiment, the attention signal allows the lower signal-to-noise ratio (SNR) sensor to be identified with 97.7% accuracy",
    "checked": true,
    "id": "8dd8b663af46a9be99181e92201a7e0b35bdf6f7",
    "semantic_title": "multi-channel attention for end-to-end speech recognition",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2018/parcollet18_interspeech.html": {
    "title": "Quaternion Convolutional Neural Networks for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Recently, the connectionist temporal classification (CTC) model coupled with recurrent (RNN) or convolutional neural networks (CNN), made it easier to train speech recognition systems in an end-to-end fashion. However in real-valued models, time frame components such as mel-filter-bank energies and the cepstral coefficients obtained from them, together with their first and second order derivatives, are processed as individual elements, while a natural alternative is to process such components as composed entities. We propose to group such elements in the form of quaternions and to process these quaternions using the established quaternion algebra. Quaternion numbers and quaternion neural networks have shown their efficiency to process multidimensional inputs as entities, to encode internal dependencies and to solve many tasks with less learning parameters than real-valued models. This paper proposes to integrate multiple feature views in quaternion-valued convolutional neural network (QCNN), to be used for sequence-to-sequence mapping with the CTC model. Promising results are reported using simple QCNNs in phoneme recognition experiments with the TIMIT corpus. More precisely, QCNNs obtain a lower phoneme error rate (PER) with less learning parameters than a competing model based on real-valued CNNs",
    "checked": true,
    "id": "45fdc73a239e9c6ea65e98c96f6a2d6dc35d6f72",
    "semantic_title": "quaternion convolutional neural networks for end-to-end automatic speech recognition",
    "citation_count": 78
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pang18_interspeech.html": {
    "title": "Compression of End-to-End Models",
    "volume": "main",
    "abstract": "End-to-end models, which output text directly given speech using a single neural network, have been shown to be competitive with conventional speech recognition models containing separate acoustic, pronunciation and language model components. Such models do not require additional resources for decoding and are typically much smaller than conventional models. This makes them particularly attractive in the context of on-device speech recognition where both small memory footprint and low power consumption are critical. This work explores the problem of compressing end-to-end models with the goal of satisfying device constraints without sacrificing model accuracy. We evaluate matrix factorization, knowledge distillation and parameter sparsity to determine the most effective methods given constraints such as a fixed parameter budget",
    "checked": true,
    "id": "7e05eb04d83b07014e7b2018666358ff5b9432a7",
    "semantic_title": "compression of end-to-end models",
    "citation_count": 46
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hodari18_interspeech.html": {
    "title": "Learning Interpretable Control Dimensions for Speech Synthesis by Using External Data",
    "volume": "main",
    "abstract": "There are many aspects of speech that we might want to control when creating text-to-speech (TTS) systems. We present a general method that enables control of arbitrary aspects of speech, which we demonstrate on the task of emotion control. Current TTS systems use supervised machine learning and are therefore heavily reliant on labelled data. If no labels are available for a desired control dimension, then creating interpretable control becomes challenging. We introduce a method that uses external, labelled data (i.e. not the original data used to train the acoustic model) to enable the control of dimensions that are not labelled in the original data. Adding interpretable control allows the voice to be manually controlled to produce more engaging speech, for applications such as audiobooks. We evaluate our method using a listening test",
    "checked": true,
    "id": "8451392694cb685e6f7741fe01ee312ab94ae846",
    "semantic_title": "learning interpretable control dimensions for speech synthesis by using external data",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2018/luong18_interspeech.html": {
    "title": "Investigating Accuracy of Pitch-accent Annotations in Neural Network-based Speech Synthesis and Denoising Effects",
    "volume": "main",
    "abstract": "We investigated the impact of noisy linguistic features on the performance of a Japanese speech synthesis system based on neural network that uses WaveNet vocoder. We compared an ideal system that uses manually corrected linguistic features including phoneme and prosodic information in training and test sets against a few other systems that use corrupted linguistic features. Both subjective and objective results demonstrate that corrupted linguistic features, especially those in the test set, affected the ideal system's performance significantly in a statistical sense due to a mismatched condition between the training and test sets. Interestingly, while an utterance-level Turing test showed that listeners had a difficult time differentiating synthetic speech from natural speech, it further indicated that adding noise to the linguistic features in the training set can partially reduce the effect of the mismatch, regularize the model and help the system perform better when linguistic features of the test set are noisy",
    "checked": true,
    "id": "a3746413aa9fff40c2b9b967f594c7f10cd3b07f",
    "semantic_title": "investigating accuracy of pitch-accent annotations in neural network-based speech synthesis and denoising effects",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liou18_interspeech.html": {
    "title": "An Exploration of Local Speaking Rate Variations in Mandarin Read Speech",
    "volume": "main",
    "abstract": "This paper explores speaking rate variation in Mandarin read speech. In contrast to assuming that each utterance is generated in a constant or global speaking rate, this study seeks to estimate local speaking rate for each prosodic unit in an utterance. The exploration is based on the existing speaking rate-dependent hierarchical prosodic model (SR-HPM). The main idea is to first use the SR-HPM to explore the prosodic structures of utterances and extract the prosodic units. Then, local speaking rate is estimated for each prosodic unit (prosodic phrase in this study). Some major influence factors including tone, base syllable type, prosodic structure and speaking rate of the higher prosodic units (utterance and BG/PG) are compensated in the local SR estimation. A syntactic-local SR model is constructed and use in the prosody generation of Mandarin TTS. Experimental results on a large read speech corpus generated by a professional female announcer showed that the generated prosody with local speaking rate variations is proved to be more vivid than the one with a constant speaking rate",
    "checked": true,
    "id": "acf807496fafea77ef0b2a4ed028a1b7f22ab61f",
    "semantic_title": "an exploration of local speaking rate variations in mandarin read speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zheng18_interspeech.html": {
    "title": "BLSTM-CRF Based End-to-End Prosodic Boundary Prediction with Context Sensitive Embeddings in a Text-to-Speech Front-End",
    "volume": "main",
    "abstract": "In this paper, we propose a language-independent end-to-end architecture for prosodic boundary prediction based on BLSTM-CRF. The proposed architecture has three components, word embedding layer, BLSTM layer and CRF layer. The word embedding layer is employed to learn the task-specific embeddings for prosodic boundary prediction. The BLSTM layer can efficiently use both past and future input features, while the CRF layer can efficiently use sentence level information. We integrate these three components and learn the whole process end-to-end. In addition, we investigate both character-level embeddings and context sensitive embeddings to this model and employ an attention mechanism for combining alternative word-level embeddings. By using an attention mechanism, the model is able to decide how much information to use from each level of embeddings. Objective evaluation results show the proposed BLSTM-CRF architecture achieves the best results on both Mandarin and English datasets, with an absolute improvement of 3.21% and 3.74% in F1 score, respectively, for intonational phrase prediction, compared to previous state-of-the-art method (BLSTM). The subjective evaluation results further indicate the effectiveness of the proposed methods",
    "checked": true,
    "id": "b8a5516ce4a5d114203ef2b0dda2cdcf7a07c128",
    "semantic_title": "blstm-crf based end-to-end prosodic boundary prediction with context sensitive embeddings in a text-to-speech front-end",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sisman18b_interspeech.html": {
    "title": "Wavelet Analysis of Speaker Dependent and Independent Prosody for Voice Conversion",
    "volume": "main",
    "abstract": "Thus far, voice conversion studies are mainly focused on the conversion of spectrum. However, speaker identity is also characterized by its prosody features, such as fundamental frequency (F0) and energy contour. We believe that with a better understanding of speaker dependent/independent prosody features, we can devise an analytic approach that addresses voice conversion in a better way. We consider that speaker dependent features reflect speaker's individuality, while speaker independent features reflect the expression of linguistic content. Therefore, the former is to be converted while the latter is to be carried over from source to target during the conversion. To achieve this, we provide an analysis of speaker dependent and speaker independent prosody patterns in different temporal scales by using wavelet transform. The centrepiece of this paper is based on the understanding that a speech utterance can be characterized by speaker dependent and independent features in its prosodic manifestations. Experiments show that the proposed prosody analysis scheme improves the prosody conversion performance consistently under the sparse representation framework",
    "checked": true,
    "id": "264db56d26e1fa2304c656485d15fb786567ddf7",
    "semantic_title": "wavelet analysis of speaker dependent and independent prosody for voice conversion",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18f_interspeech.html": {
    "title": "Improving Mongolian Phrase Break Prediction by Using Syllable and Morphological Embeddings with BiLSTM Model",
    "volume": "main",
    "abstract": "In the speech synthesis systems, the phrase break (PB) prediction is the first and most important step. Recently, the state-of-the-art PB prediction systems mainly rely on word embeddings. However this method is not fully applicable to Mongolian language, because its word embeddings are inadequate trained, owing to the lack of resources. In this paper, we introduce a bidirectional Long Short Term Memory (BiLSTM) model which combined word embeddings with syllable and morphological embedding representations to provide richer and multi-view information which leverages the agglutinative property. Experimental results show the proposed method outperforms compared systems which only used the word embeddings. In addition, further analysis shows that it is quite robust to the Out-of-Vocabulary (OOV) problem owe to the refined word embedding. The proposed method achieves the state-of-the-art performance in the Mongolian PB prediction",
    "checked": true,
    "id": "fcb19ca59aee54f64b179b1072ea29af8573e9e3",
    "semantic_title": "improving mongolian phrase break prediction by using syllable and morphological embeddings with bilstm model",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2018/you18_interspeech.html": {
    "title": "Improved Supervised Locality Preserving Projection for I-vector Based Speaker Verification",
    "volume": "main",
    "abstract": "A Supervised Locality Preserving Projection (SLPP) method is employed for channel compensation in an i-vector based speaker verification system. SLPP preserves more important local information by weighing both the within- and between-speaker nearby data pairs based on the similarity matrices. In this paper, we propose an improved SLPP (P-SLPP) to enhance the channel compensation ability. First, the conventional Euclidean distance in conventional SLPP is replaced with Probabilistic Linear Discriminant Analysis (PLDA) scores. Furthermore, the weight matrices of P-SLPP are generated by using the relative PLDA scores of within- and between-speaker pairs. Experiments are carried out on the five common conditions of NIST 2012 speaker recognition evaluation (SRE) core sets. The results show that SLPP and the proposed P-SLPP outperform all other state-of-the-art channel compensation methods. Among these methods, P-SLPP achieves the best performance",
    "checked": true,
    "id": "6d16338ec327ed00631790202f0146df102ca333",
    "semantic_title": "improved supervised locality preserving projection for i-vector based speaker verification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shi18b_interspeech.html": {
    "title": "Double Joint Bayesian Modeling of DNN Local I-Vector for Text Dependent Speaker Verification with Random Digit Strings",
    "volume": "main",
    "abstract": "Double joint Bayesian is a recently introduced analysis method that models and explores multiple information explicitly from the samples to improve the verification performance. It was recently applied to voice pass phrase verification, result in better results on text dependent speaker verification task. However little is known about its effectiveness in other challenging situations such as speaker verification for short, text-constrained test utterances, e.g. random digit strings. Contrary to conventional joint Bayesian method that cannot make full use of multi-view information, double joint Bayesian can incorporate both intra-speaker/digit and inter-speaker/digit variation and calculated the likelihood to describe whether the features having all labels consistent or not. We show that double joint Bayesian outperforms conventional method on modeling DNN local (digit-dependent) i-vectors for speaker verification with random prompted digit strings. Since the strength of both double joint Bayesian and conventional DNN local i-vector appear complementary, the combination significantly outperforms either of its components",
    "checked": true,
    "id": "27b767dd96d1ef0fb778f5131e5091ab12bf1acd",
    "semantic_title": "double joint bayesian modeling of dnn local i-vector for text dependent speaker verification with random digit strings",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/silnova18_interspeech.html": {
    "title": "Fast Variational Bayes for Heavy-tailed PLDA Applied to i-vectors and x-vectors",
    "volume": "main",
    "abstract": "The standard state-of-the-art backend for text-independent speaker recognizers that use i-vectors or x-vectors is Gaussian PLDA (G-PLDA), assisted by a Gaussianization step involving length normalization. G-PLDA can be trained with both gener- ative or discriminative methods. It has long been known that heavy-tailed PLDA (HT-PLDA), applied without length nor- malization, gives similar accuracy, but at considerable extra computational cost. We have recently introduced a fast scor- ing algorithm for a discriminatively trained HT-PLDA back- end. This paper extends that work by introducing a fast, vari- ational Bayes, generative training algorithm. We compare old and new backends, with and without length-normalization, with i-vectors and x-vectors, on SRE'10, SRE'16 and SITW",
    "checked": true,
    "id": "a229084bf0f268a345b20e5361e41e128b19b48b",
    "semantic_title": "fast variational bayes for heavy-tailed plda applied to i-vectors and x-vectors",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2018/todisco18_interspeech.html": {
    "title": "Integrated Presentation Attack Detection and Automatic Speaker Verification: Common Features and Gaussian Back-end Fusion",
    "volume": "main",
    "abstract": "The vulnerability of automatic speaker verification (ASV) systems to spoofing is widely acknowledged. Recent years have seen an intensification in research efforts to develop spoofing countermeasures, also known as presentation attack detection (PAD) systems. Much of this work has involved the exploration of features that discriminate reliably between bona fide and spoofed speech. While there are grounds to use different front-ends for ASV and PAD systems (they are different tasks) the use of a single front-end has obvious benefits, not least convenience and computational efficiency, especially when ASV and PAD are combined. This paper investigates the performance of a variety of different features used previously for both ASV and PAD and assesses their performance when combined for both tasks. The paper also presents a Gaussian back-end fusion approach to system combination. In contrast to cascaded architectures, it relies upon the modelling of the two-dimensional score distribution stemming from the combination of ASV and PAD in parallel. This approach to combination is shown to generalise particularly well across independent ASVspoof 2017 v2.0 development and evaluation datasets",
    "checked": true,
    "id": "2b5d9c131d5f45216303cad5407d5ac9a4ab69f5",
    "semantic_title": "integrated presentation attack detection and automatic speaker verification: common features and gaussian back-end fusion",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ferrer18_interspeech.html": {
    "title": "A Generalization of PLDA for Joint Modeling of Speaker Identity and Multiple Nuisance Conditions",
    "volume": "main",
    "abstract": "Probabilistic linear discriminant analysis (PLDA) is the leading method for computing scores in speaker recognition systems. The method models the vectors representing each audio sample as a sum of three terms: one that depends on the speaker identity, one that models the within-speaker variability and one that models any remaining variability. The last two terms are assumed to be independent across samples. We recently proposed an extension of the PLDA method, which we termed Joint PLDA (JPLDA), where the second term is considered dependent on the type of nuisance condition present in the data (e.g., the language or channel). The proposed method led to significant gains for multilanguage speaker recognition when taking language as the nuisance condition. In this paper, we present a generalization of this approach that allows for multiple nuisance terms. We show results using language and several nuisance conditions describing the acoustic characteristics of the sample and demonstrate that jointly including all these factors in the model leads to better results than including only language or acoustic condition factors. Overall, we obtain relative improvements in detection cost function between 5% and 47% for various systems and test conditions with respect to standard PLDA approaches",
    "checked": true,
    "id": "dc77b0a0611d6e7c940a237decc2a9db2c16b3ce",
    "semantic_title": "a generalization of plda for joint modeling of speaker identity and multiple nuisance conditions",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18j_interspeech.html": {
    "title": "An Investigation of Non-linear i-vectors for Speaker Verification",
    "volume": "main",
    "abstract": "Speaker verification becomes increasingly important due to the popularity of speech assistants and smart home. i-vectors are used broadly for this topic, which use factor analysis to model the shift of average parameter in Gaussian Mixture Models. Recently by the progress of deep learning, high-level non-linearity improves results in many areas. In this paper we proposed a new framework of i-vectors which uses stochastic gradient descent to solve the problem of i-vectors. From our preliminary results stochastic gradient descent can get same performance as expectation-maximization algorithm. However, by backpropagation the assumption can be more flexible, so both linear and non-linear assumption is possible in our framework. From our result, both maximum a posteriori estimation and maximum likelihood lead to slightly better result than conventional i-vectors and both linear and non-linear system has similar performance",
    "checked": true,
    "id": "813f21bcc64ccf2a4744e8eb14b65298bbd769dc",
    "semantic_title": "an investigation of non-linear i-vectors for speaker verification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ram18_interspeech.html": {
    "title": "CNN Based Query by Example Spoken Term Detection",
    "volume": "main",
    "abstract": "In this work, we address the problem of query by example spoken term detection (QbE-STD) in zero-resource scenario. State of the art solutions usually rely on dynamic time warping (DTW) based template matching. In contrast, we propose here to tackle the problem as binary classification of images. Similar to the DTW approach, we rely on deep neural network (DNN) based posterior probabilities as feature vectors. The posteriors from a spoken query and a test utterance are used to compute frame-level similarities in a matrix form. This matrix contains somewhere a quasi-diagonal pattern if the query occurs in the test utterance. We propose to use this matrix as an image and train a convolutional neural network (CNN) for identifying the pattern and make a decision about the occurrence of the query. This language independent system is evaluated on SWS 2013 and is shown to give 10% relative improvement over a highly competitive baseline system based on DTW. Experiments on QUESST 2014 database gives similar improvements showing that the approach generalizes to other database as well",
    "checked": true,
    "id": "55337b94a7d2f3a16c71d7852cff290a45637e22",
    "semantic_title": "cnn based query by example spoken term detection",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yuan18_interspeech.html": {
    "title": "Learning Acoustic Word Embeddings with Temporal Context for Query-by-Example Speech Search",
    "volume": "main",
    "abstract": "We propose to learn acoustic word embeddings with temporal context for query-by-example (QbE) speech search. The temporal context includes the leading and trailing word sequences of a word. We assume that there exist spoken word pairs in the training database. We pad the word pairs with their original temporal context to form fixed-length speech segment pairs. We obtain the acoustic word embeddings through a deep convolutional neural network (CNN) which is trained on the speech segment pairs with a triplet loss. By shifting a fixed-length analysis window through the search content, we obtain a running sequence of embeddings. In this way, searching for the spoken query is equivalent to the matching of acoustic word embeddings. The experiments show that our proposed acoustic word embeddings learned with temporal context are effective in QbE speech search. They outperform the state-of-the-art frame-level feature representations and reduce run-time computation since no dynamic time warping is required in QbE speech search. We also find that it is important to have sufficient speech segment pairs to train the deep CNN for effective acoustic word embeddings",
    "checked": true,
    "id": "b3852b76ccfc1fd311f6afd5ac82dd2862f03011",
    "semantic_title": "learning acoustic word embeddings with temporal context for query-by-example speech search",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhu18b_interspeech.html": {
    "title": "Siamese Recurrent Auto-Encoder Representation for Query-by-Example Spoken Term Detection",
    "volume": "main",
    "abstract": "With the explosive development of human-computer speech interaction, spoken term detection is widely required and has attracted increasing interest. In this paper, we propose a weak supervised approach using Siamese recurrent auto-encoder (RAE) to represent speech segments for query-by-example spoken term detection (QbyE-STD). The proposed approach exploits word pairs that contain different instances of the same/different word content as input to train the Siamese RAE. The encoder last hidden state vector of Siamese RAE is used as the feature for QbyE-STD, which is a fixed dimensional embedding feature containing mostly semantic content related information. The advantages of the proposed approach are: 1) extracting more compact feature with fixed dimension while keeping the semantic information for STD; 2) the extracted feature can describe the sequential phonetic structure of similar sounds to degree, which can be applied for zero-resource QbyE-STD. Evaluations on real scene Chinese speech interaction data and TIMIT confirm the effectiveness and efficiency of the proposed approach compared to the conventional ones",
    "checked": true,
    "id": "6ab41db11281b6b62023a6db3540e40f1e2ae252",
    "semantic_title": "siamese recurrent auto-encoder representation for query-by-example spoken term detection",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18g_interspeech.html": {
    "title": "Fast Derivation of Cross-lingual Document Vectors from Self-attentive Neural Machine Translation Model",
    "volume": "main",
    "abstract": "A universal cross-lingual representation of documents, which can capture the underlying semantics is very useful in many natural language processing tasks. In this paper, we develop a new document vectorization method which effectively selects the most salient sequential patterns from the inputs to create document vectors via a self-attention mechanism using a neural machine translation (NMT) model. The model used by our method can be trained with parallel corpora that are unrelated to the task at hand. During testing, our method will take a monolingual document and convert it into a \"Neural machine Translation framework based cross-lingual Document Vector\" (NTDV). NTDV has two comparative advantages. Firstly, the NTDV can be produced by the forward pass of the encoder in the NMT and the process is very fast and does not require any training/optimization. Secondly, our model can be conveniently adapted from a pair of existing attention based NMT models and the training requirement on parallel corpus can be reduced significantly. In a cross-lingual document classification task, our NTDV embeddings surpass the previous state-of-the-art performance in the English-to-German classification test and, to our best knowledge, it also achieves the best performance among the fast decoding methods in the German-to-English classification test",
    "checked": true,
    "id": "c82820a991b1ce9a9a8f20a9032d86fd83257b27",
    "semantic_title": "fast derivation of cross-lingual document vectors from self-attentive neural machine translation model",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pandey18_interspeech.html": {
    "title": "LSTM Based Attentive Fusion of Spectral and Prosodic Information for Keyword Spotting in Hindi Language",
    "volume": "main",
    "abstract": "In this paper, a DNN based keyword spotting framework, that utilizes both spectral as well as prosodic information present in the speech signal, is proposed. A DNN is first trained to learn a set of hierarchical non-linear transformation parameters that project the original spectral and prosodic feature vectors onto a feature space where the distance between similar syllable pairs is small and between dissimilar syllable pairs is large. These transformed features are then fused using an attention-based long short-term memory (LSTM) network. As a side result, a deep denoising autoencoder based fine-tuning technique is used to improve the performance of sequence predictions. A sequence matching method called the sliding syllable protocol is also developed for keyword spotting. Syllable recognition and keyword spotting (KWS) experiments are conducted specifically for the Hindi language which is one of the widely spoken languages across the globe but is not addressed significantly by the speech processing community. The proposed framework indicates reasonable improvements when compared to baseline methods available in the literature",
    "checked": true,
    "id": "dd9f2c3c1e8fbe4ec8f08aac61c3b4ea43371b27",
    "semantic_title": "lstm based attentive fusion of spectral and prosodic information for keyword spotting in hindi language",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shankar18_interspeech.html": {
    "title": "Spoken Keyword Detection Using Joint DTW-CNN",
    "volume": "main",
    "abstract": "A method to detect spoken keywords in a given speech utterance is proposed, called as joint Dynamic Time Warping (DTW)- Convolution Neural Network (CNN). It is a combination of DTW approach with a strong classifier like CNN. Both these methods have independently shown significant results in solving problems related to optimal sequence alignment and object recognition, respectively. The proposed method modifies the original DTW formulation and converts the warping matrix into a gray scale image. A CNN is trained on these images to classify the presence or absence of keyword by identifying the texture of warping matrix. The TIMIT corpus has been used for conducting experiments and our method shows significant improvement over other existing techniques",
    "checked": true,
    "id": "f613d3c84d6df49d762560bc900fa82e2dcfc601",
    "semantic_title": "spoken keyword detection using joint dtw-cnn",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2018/schuller18_interspeech.html": {
    "title": "The INTERSPEECH 2018 Computational Paralinguistics Challenge: Atypical & Self-Assessed Affect, Crying & Heart Beats",
    "volume": "main",
    "abstract": "The INTERSPEECH 2018 Computational Paralinguistics Challenge addresses four different problems for the first time in a research competition under well-defined conditions: In the Atypical Affect Sub-Challenge, four basic emotions annotated in the speech of handicapped subjects have to be classified; in the Self-Assessed Affect Sub-Challenge, valence scores given by the speakers themselves are used for a three-class classification problem; in the Crying Sub-Challenge, three types of infant vocalisations have to be told apart; and in the Heart Beats Sub-Challenge, three different types of heart beats have to be determined. We describe the Sub-Challenges, their conditions and baseline feature extraction and classifiers, which include data-learnt (supervised) feature representations by end-to-end learning, the ‘usual' ComParE and BoAW features and deep unsupervised representation learning using the AUDEEP toolkit for the first time in the challenge series",
    "checked": true,
    "id": "783d2bd2820b35ce7e398be412569e9d5c6f5880",
    "semantic_title": "the interspeech 2018 computational paralinguistics challenge: atypical & self-assessed affect, crying & heart beats",
    "citation_count": 114
  },
  "https://www.isca-speech.org/archive/interspeech_2018/humayun18_interspeech.html": {
    "title": "An Ensemble of Transfer, Semi-supervised and Supervised Learning Methods for Pathological Heart Sound Classification",
    "volume": "main",
    "abstract": "In this work, we propose an ensemble of classifiers to distinguish between various degrees of abnormalities of the heart using Phonocardiogram (PCG) signals acquired using digital stethoscopes in a clinical setting, for the INTERSPEECH 2018 Computational Paralinguistics (ComParE) Heart Beats Sub-Challenge. Our primary classification framework constitutes a convolutional neural network with 1D-CNN time-convolution (tConv) layers, which uses features transferred from a model trained on the 2016 Physionet Heart Sound Database. We also employ a Representation Learning (RL) approach to generate features in an unsupervised manner using Deep Recurrent Autoencoders and use Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) classifiers. Finally, we utilize an SVM classifier on a high-dimensional segment-level feature extracted using various functionals on short-term acoustic features, i.e., Low-Level Descriptors (LLD). An ensemble of the three different approaches provides a relative improvement of 11.13% compared to our best single sub-system in terms of the Unweighted Average Recall (UAR) performance metric on the evaluation dataset",
    "checked": true,
    "id": "1578dd4dc568ebc2f3a1bff093b17e3ea53d2073",
    "semantic_title": "an ensemble of transfer, semi-supervised and supervised learning methods for pathological heart sound classification",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2018/turan18_interspeech.html": {
    "title": "Monitoring Infant's Emotional Cry in Domestic Environments Using the Capsule Network Architecture",
    "volume": "main",
    "abstract": "Automated recognition of an infant's cry from audio can be considered as a preliminary step for the applications like remote baby monitoring. In this paper, we implemented a recently introduced deep learning topology called capsule network (CapsNet) for the cry recognition problem. A capsule in the CapsNet, which is defined as a new representation, is a group of neurons whose activity vector represents the probability that the entity exists. Active capsules at one level make predictions, via transformation matrices, for the parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We employed spectrogram representations from the short segments of an audio signal as an input of the CapsNet. For experimental evaluations, we apply the proposed method on INTERSPEECH 2018 computational paralinguistics challenge (ComParE), crying sub-challenge, which is a three-class classification task using an annotated database (CRIED). Provided audio samples contains recordings from 20 healthy infants and categorized into the three classes namely neutral, fussing and crying. We show that multi-layer CapsNet outperforms baseline performance on CRIED corpus and is considerably better than a conventional convolutional net",
    "checked": true,
    "id": "b3383950b3162e7f651ce9c30f2eba7efacd4e28",
    "semantic_title": "monitoring infant's emotional cry in domestic environments using the capsule network architecture",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/huckvale18_interspeech.html": {
    "title": "Neural Network Architecture That Combines Temporal and Summative Features for Infant Cry Classification in the Interspeech 2018 Computational Paralinguistics Challenge",
    "volume": "main",
    "abstract": "This paper describes the application of a novel deep neural network architecture to the classification of infant vocalisations as part of the Interspeech 2018 Computational Paralinguistics Challenge. Previous approaches to infant cry classification have either applied a statistical classifier to summative features of the whole cry, or applied a syntactic pattern recognition technique to a temporal sequence of features. In this work we explore a deep neural network architecture that exploits both temporal and summative features to make a joint classification. The temporal input comprises centi-second frames of low-level signal features which are input to LSTM nodes, while the summative vector comprises a large set of statistical functionals of the same frames that are input to MLP nodes. The combined network is jointly optimized and evaluated using leave-one-speaker-out cross-validation on the challenge training set. Results are compared to independently-trained temporal and summative networks and to a baseline SVM classifier. The combined model outperforms the other models and the challenge baseline on the training set. While problems remain in finding the best configuration and training protocol for such networks, the approach seems promising for future signal classification tasks",
    "checked": true,
    "id": "7baa89b0658e5e4bf54136f5a7bc07af948ff2ad",
    "semantic_title": "neural network architecture that combines temporal and summative features for infant cry classification in the interspeech 2018 computational paralinguistics challenge",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18l_interspeech.html": {
    "title": "Evolving Learning for Analysing Mood-Related Infant Vocalisation",
    "volume": "main",
    "abstract": "Infant vocalisation analysis plays an important role in the study of the development of pre-speech capability of infants, while machine-based approaches nowadays emerge with an aim to advance such an analysis. However, conventional machine learning techniques require heavy feature-engineering and refined architecture designing. In this paper, we present an evolving learning framework to automate the design of neural network structures for infant vocalisation analysis. In contrast to manually searching by trial and error, we aim to automate the search process in a given space with less interference. This framework consists of a controller and its child networks, where the child networks are built according to the controller's estimation. When applying the framework to the Interspeech 2018 Computational Paralinguistics (ComParE) Crying Sub-challenge, we discover several deep recurrent neural network structures, which are able to deliver competitive results to the best ComParE baseline method",
    "checked": true,
    "id": "ba398cb0c7406237caf5bf511cbf66fa08b5e2c0",
    "semantic_title": "evolving learning for analysing mood-related infant vocalisation",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wagner18_interspeech.html": {
    "title": "Deep Learning in Paralinguistic Recognition Tasks: Are Hand-crafted Features Still Relevant?",
    "volume": "main",
    "abstract": "In the past, the performance of machine learning algorithms depended heavily on the representation of the data. Well-designed features therefore played a key role in speech and paralinguistic recognition tasks. Consequently, engineers have put a great deal of work into manually designing large and complex acoustic feature sets. With the emergence of Deep Neural Networks (DNNs), however, it is now possible to automatically infer higher abstractions from simple spectral representations or even learn directly from raw waveforms. This raises the question if (complex) hand-crafted features will still be needed in the future. We take this year's INTERSPEECH Computational Paralinguistic Challenge as an opportunity to approach this issue by means of two corpora - Atypical Affect and Crying. At first, we train a Recurrent Neural Network (RNN) to evaluate the performance of several hand-crafted feature sets of varying complexity. Afterwards, we make the network do the feature engineering all on its own by prefixing a stack of convolutional layers. Our results show that there is no clear winner (yet). This creates room to discuss chances and limits of either approach",
    "checked": true,
    "id": "db622af9b0ba2b691af53ba0fcb46ad279592c3c",
    "semantic_title": "deep learning in paralinguistic recognition tasks: are hand-crafted features still relevant?",
    "citation_count": 44
  },
  "https://www.isca-speech.org/archive/interspeech_2018/luo18_interspeech.html": {
    "title": "Investigation on Joint Representation Learning for Robust Feature Extraction in Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) is a challenging task due to its difficulty in finding proper representations for emotion embedding in speech. Recently, Convolutional Recurrent Neural Network (CRNN), which is combined by convolution neural network and recurrent neural network, is popular in this field and achieves state-of-art on related corpus. However, most of work on CRNN only utilizes simple spectral information, which is not capable to capture enough emotion characteristics for the SER task. In this work, we investigate two joint representation learning structures based on CRNN aiming at capturing richer emotional information from speech. Cooperating the handcrafted high-level statistic features with CRNN, a two-channel SER system (HSF-CRNN) is developed to jointly learn the emotion-related features with better discriminative property. Furthermore, considering that the time duration of speech segment significantly affects the accuracy of emotion recognition, another two-channel SER system is proposed where CRNN features extracted from different time scale of spectrogram segment are used for joint representation learning. The systems are evaluated over Atypical Affect Challenge of ComParE2018 and IEMOCAP corpus. Experimental results show that our proposed systems outperform the plain CRNN",
    "checked": true,
    "id": "2574dd6f8965e0b4adf73272f93f236b5152ca55",
    "semantic_title": "investigation on joint representation learning for robust feature extraction in speech emotion recognition",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2018/park18c_interspeech.html": {
    "title": "Using Voice Quality Supervectors for Affect Identification",
    "volume": "main",
    "abstract": "The voice quality of speech sounds often conveys perceivable information about the speaker's affect. This study proposes perceptually important voice quality features to recognize affect represented in speech excerpts from individuals with mental, neurological and/or physical disabilities. The voice quality feature set consists of F0, harmonic amplitude differences between the first, second, fourth harmonics and the harmonic near 2 kHz, the center frequency and amplitudes of the first 3 formants and cepstral peak prominence. The feature distribution of each utterance was represented with a supervector and the Gaussian mixture model and support vector machine classifiers were used for affect classification. Similar classification systems using the MFCCs and ComParE16 feature set were implemented. The systems were fused by taking the confidence mean of the classifiers. Applying the fused system to the Interspeech 2018 Atypical Affect subchallenge task resulted in unweighted average recalls of 43.9% and 41.0% on the development and test dataset, respectively. Additionally, we investigated clusters obtained by unsupervised learning to address gender-related differences",
    "checked": true,
    "id": "c3fde8b1abe3efa03a03485d827f272f1dc4b3ee",
    "semantic_title": "using voice quality supervectors for affect identification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tang18c_interspeech.html": {
    "title": "An End-to-End Deep Learning Framework for Speech Emotion Recognition of Atypical Individuals",
    "volume": "main",
    "abstract": "The goal of the ongoing ComParE 2018 Atypical Affect sub-challenge is to recognize the emotional states of atypical individuals. In this work, we present three modeling methods under the end-to-end learning framework, namely CNN combined with extended features, CNN+RNN and ResNet, respectively. Furthermore, we investigate multiple data augmentation, balancing and sampling methods to further enhance the system performance. The experimental results show that data balancing and augmentation increase the unweighted accuracy (UAR) by 10% absolutely. After score level fusion, our proposed system achieves 48.8% UAR on the develop dataset",
    "checked": false,
    "id": "dfadf827301d924f0bef0a445bac68e1f90904c3",
    "semantic_title": "an end-to-end deep learning framework with speech emotion recognition of atypical individuals",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2018/koller18_interspeech.html": {
    "title": "DialogOS: Simple and Extensible Dialogue Modeling",
    "volume": "main",
    "abstract": "We present the open-source extensible dialog manager DialogOS. DialogOS features simple finite-state based dialog management (which can be expanded to more complex DM strategies via a full-fledged scripting language) in combination with integrated speech recognition and synthesis in multiple languages. DialogOS runs on all major platforms, provides a simple-to-use graphical interface and can easily be extended via well-defined plugin and client interfaces, or can be integrated server-side into larger existing software infrastructures. We hope that DialogOS will help foster research and teaching given that it lowers the bar of entry into building and testing spoken dialog systems and provides paths to extend one's system as development progresses",
    "checked": true,
    "id": "67e0af648094c65b78f326d0397e6be9fc870112",
    "semantic_title": "dialogos: simple and extensible dialogue modeling",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dernoncourt18_interspeech.html": {
    "title": "A Framework for Speech Recognition Benchmarking",
    "volume": "main",
    "abstract": "Over the past few years, the number of APIs for automated speech recognition (ASR) has significantly increased. It is often time-consuming to evaluate how the performance of these ASR systems compare with each other and against newly proposed algorithms. In this paper, we present a lightweight, open source framework that allows users to easily benchmark ASR APIs on the corpora of their choice. The framework currently supports 7 ASR APIs and is easily extendable to more APIs",
    "checked": true,
    "id": "de2dc71ef3333c3ca53f500b2ee169d4d5668141",
    "semantic_title": "a framework for speech recognition benchmarking",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2018/arai18_interspeech.html": {
    "title": "Flexible Tongue Housed in a Static Model of the Vocal Tract With Jaws, Lips and Teeth",
    "volume": "main",
    "abstract": "Physical models of the human vocal tract with a moveable tongue have been reported in past literature. In this study, we developed a new model with a flexible tongue. As with previous models by the author, the flexible tongue is made of gel material. The shape of this model's tongue is still an abstraction, although it is more realistic than previous models. Apart from the tongue, the model is static and solid; the gel tongue is the main part that can be manipulated. The static portion of the model is an extension of our recent static model with lips, teeth and tongue. The entire model looks like a sagittal splice taken from an artificial human head. Because the thin, acrylic plates on the outside are transparent, the interior of the oral and pharyngeal cavities are visible. When we feed a glottal sound through a hole in the laryngeal region on the bottom of the model, different vowels are produced, dependent upon the shape of the tongue. This model is the most useful and realistic looking of the models we've made for speech science education so far",
    "checked": true,
    "id": "301fe62464796e0e476c7e10447c5c1142b4cacd",
    "semantic_title": "flexible tongue housed in a static model of the vocal tract with jaws, lips and teeth",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mathew18_interspeech.html": {
    "title": "Voice Analysis Using Acoustic and Throat Microphones for Speech Therapy",
    "volume": "main",
    "abstract": "Diagnosis of voice disorders by a speech therapist involves the process of voice recording with the patient, followed by software-aided analysis. In this paper, we propose a novel voice diagnosis system which gives voice report information based on Praat software, using voice samples from a throat microphone and an acoustic microphone, making the diagnosis near real-time, as well as robust to background noise. Results show that throat microphones give reliable Jitter and Shimmer values in ambient noise levels of 47~50 dB, while acoustic microphones show high variance in these parameters",
    "checked": true,
    "id": "7a0badb7d33d1d895d7cf59cfefbf79d4cb7a7e0",
    "semantic_title": "voice analysis using acoustic and throat microphones for speech therapy",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rayner18_interspeech.html": {
    "title": "A Robust Context-Dependent Speech-to-Speech Phraselator Toolkit for Alexa",
    "volume": "main",
    "abstract": "We present an open source toolkit for creating robust speech-to-speech phraselators, suitable for medical and other safety-critical domains, that can be hosted on the Amazon Alexa platform. Supported functionality includes context-dependent translation of incomplete utterances. We describe a preliminary evaluation on an English medical examination grammar",
    "checked": true,
    "id": "4b6451b128cfc272afe447f48fffedb6a7ed14a0",
    "semantic_title": "a robust context-dependent speech-to-speech phraselator toolkit for alexa",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/prasad18_interspeech.html": {
    "title": "Discriminating Nasals and Approximants in English Language Using Zero Time Windowing",
    "volume": "main",
    "abstract": "Nasals and approximants consonants are often confused with each other. Despite the distinction in the production mechanism, these two sound classes exhibit a similar low frequency behavior and lack significant high frequency content. The present study uses a spectral representation obtained using the zero time windowing (ZTW) analysis of speech, for the task of distinction between these two. The instantaneous spectral representation has good resolution at resonances, which helps to highlight the difference in the acoustic vocal tract system response for these sounds. The ZTW spectra around the regions of glottal closure instants are averaged to derive parameters for their classification in continuous speech. A set of parameters based on the dominant resonances, center of gravity, band energy ratio and cumulative spectral sum in low frequencies, is derived from the average spectrum. The paper proposes classification using a knowledge-based approach and training a support vector machine. These classifiers are tested on utterances from different English speakers in the TIMIT dataset. The proposed methods result in an average classification accuracy of 90% between the two classes in continuous speech",
    "checked": true,
    "id": "310e798e0df83b65908fcc31872f455e6c27a9d6",
    "semantic_title": "discriminating nasals and approximants in english language using zero time windowing",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/howson18_interspeech.html": {
    "title": "Gestural Lenition of Rhotics Captures Variation in Brazilian Portuguese",
    "volume": "main",
    "abstract": "The goal of this study is to examine the rhotics in Brazilian Portuguese (BP), /ɾ,ʁ/ and the ‘archetypal' coda /R/, to determine if: (1) they can be characterized as a coordination of the tongue dorsum and tongue body or tip and (2) manipulation of the gestural settings accounts for rhotic allophony in BP. Six native speakers of BP participated in an ultrasound experiment and produced target phonemes in #CV, VCV and VC# environments with the vowels /i, e, a, o/. Tongue contours for the rhotics were compared using Smoothing Spline ANOVAs. /ɾ, ʁ/ were produced with a tongue body and dorsum gesture, while /ɾ/ also had an apical gesture. Archetypal /R/ was realized variably, as any of [ɾ, ɻ, ɹ, χ]. BP rhotics can be described as the coordination of a tongue dorsum and a tongue body or tip gesture. ‘Archetypal' /R/ is posited to be /ɾ/. Allophony between /ɾ/ and [ɻ, ɹ, χ] is due to tongue tip lenition. Allophony between /ʁ/ and [h] is due to weakening of the tongue dorsum and body gestures. This analysis suggests synchronic and diachronic changes of rhotics result from lenition. It also captures the rarity of diachronic changes from uvulars to alveolars",
    "checked": true,
    "id": "22d223b8210f3b731132468f7ef416383c79e209",
    "semantic_title": "gestural lenition of rhotics captures variation in brazilian portuguese",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/prasad18b_interspeech.html": {
    "title": "Identification and Classification of Fricatives in Speech Using Zero Time Windowing Method",
    "volume": "main",
    "abstract": "Fricatives are produced by creating a turbulence in the air-flow by passing it through a stricture in the vocal tract cavity. Fricatives are characterized by their noise-like behavior, which makes it difficult to analyze. Difference in the place of articulation leads to different classes of fricatives. Identification of fricative segment boundaries in speech helps in improving the performance of several applications. The present study attempts towards the identification and classification of fricative segments in continuous speech, based on the statistical behavior of instantaneous spectral characteristics. The proposed method uses parameters such as the dominant resonance frequencies, the center of gravity along with the statistical moments of the spectrum obtained using the zero time windowing (ZTW) method. The ZTW spectra exhibits a high temporal resolution and therefore gives accurate segment boundaries in speech. The proposed algorithm is tested on the TIMIT dataset for English language. A high identification rate of 97.5% is achieved for segment boundaries of the sibilant fricative class. Voiced nonsibilants show a lower identification rate than their voiceless counterparts due to their vowel-like spectral characteristics. A high classification rate of 93.2% is achieved between sibilants and nonsibilants",
    "checked": true,
    "id": "fa2fbdc42068ba8fae439825fdb3bb71627f6250",
    "semantic_title": "identification and classification of fricatives in speech using zero time windowing method",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chanchaochai18_interspeech.html": {
    "title": "GlobalTIMIT: Acoustic-Phonetic Datasets for the World's Languages",
    "volume": "main",
    "abstract": "Although the TIMIT acoustic-phonetic dataset ([1], [2]) was created three decades ago, it remains in wide use, with more than 20000 Google Scholar references and more than 1000 since 2017. Despite TIMIT's antiquity and relatively small size, inspection of these references shows that it is still used in many research areas: speech recognition, speaker recognition, speech synthesis, speech coding, speech enhancement, voice activity detection, speech perception, overlap detection and source separation, diagnosis of speech and language disorders and linguistic phonetics, among others. Nevertheless, comparable datasets are not available even for other widely-studied languages, much less for under-documented languages and varieties. Therefore, we have developed a method for creating TIMIT-like datasets in new languages with modest effort and cost and we have applied this method in standard Thai, standard Mandarin Chinese, English from Chinese L2 learners, the Guanzhong dialect of Mandarin Chinese and the Ga language of West Africa. Other collections are planned or underway. The resulting datasets will be published through the LDC, along with instructions and open-source tools for replicating this method in other languages, covering the steps of sentence selection and assignment to speakers, speaker recruiting and recording, proof-listening and forced alignment",
    "checked": true,
    "id": "f74c70d2727f0014ca85152667e1b494b32c50ef",
    "semantic_title": "globaltimit: acoustic-phonetic datasets for the world's languages",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hermes18_interspeech.html": {
    "title": "Structural Effects on Properties of Consonantal Gestures in Tashlhiyt",
    "volume": "main",
    "abstract": "Tashlhiyt Berber is a language in which every consonant can take up the nucleus position in a syllable. The present study investigates how gestural properties are modified when the consonants occur in different syllable positions (onset, nucleus, coda). Furthermore, the effect of higher structural components such as morphology on the respective gestural organization patterns are examined. Therefore, we collected articulographic data for different consonantal roots, such as /bdg/ and /gzm/ with varying affixes, entailing different syllabification patterns in Tashlhiyt. Consonantal properties in different syllable positions are investigated with respect to their intragestural properties and intergestural properties, i.e. bonding strength. Furthermore, gestural coherence with respect to prefixation were examined. Results reveal that consonantal gestures were not modified on the intragestural level in terms of duration, velocity, stiffness or displacement, when the morphological structure was kept constant. However, on the intergestural level syllable relation was encoded, revealing a tighter bonding for onset-nucleus relations than for heterosyllabic sequences. Furthermore, when changing the morphological marker, modifications of intragestural parameters occur, inducing temporal changes of consonantal gestures. We conclude that higher structural components should be taken into account when investigating syllable internal timing patterns",
    "checked": true,
    "id": "5b659e2827b2eb7f28eed0076334d0cea5c4cd0e",
    "semantic_title": "structural effects on properties of consonantal gestures in tashlhiyt",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kochetov18_interspeech.html": {
    "title": "The Retroflex-dental Contrast in Punjabi Stops and Nasals: A Principal Component Analysis of Ultrasound Images",
    "volume": "main",
    "abstract": "Many languages of South Asia show a phonemic contrast between retroflexes and dentals across different manners of articulation. This contrast, however, tends to be less phonetically distinct and more variable in nasals. The goal of this paper is to examine the overall similarity of the retroflex-dental contrasts in Punjabi stops and nasals. Ultrasound tongue imaging recordings were obtained from 14 Punjabi speakers producing /ʈ,ɳ,t,n/ in the /ba_ab/ nonsense word context. Selected video frames were fed to a principal component analysis (PCA); the output was used for (1) training a linear discriminant model on one manner that discriminates place and (2) testing it on the other manner. The results showed 100% correct classification of the contrast (retroflex or dental) in stops and 92% correct classification in nasals in the training data. The classification was much poorer across different manners: on average 67% of stops and 57% of nasals were classified correctly based on training sets with nasals or stops, respectively. In both cases, retroflex responses were more common. These results suggest that the tongue configurations for Punjabi retroflex and dental consonants differ by manner of articulation. The contrast is also overall less robust in nasals than in stops, confirming previous reports",
    "checked": true,
    "id": "6b5cdf8e82820afe77dd1a7f483a3a702f2153c7",
    "semantic_title": "the retroflex-dental contrast in punjabi stops and nasals: a principal component analysis of ultrasound images",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yue18_interspeech.html": {
    "title": "Vowels and Diphthongs in Hangzhou Wu Chinese Dialect",
    "volume": "main",
    "abstract": "This paper gives an acoustic phonetic description of the vowels and diphthongs in Hangzhou Wu Chinese dialect. Data from 12 speakers, 6 male and 6 female, were measured and analyzed. Monophthongs were investigated in CV, CVN and CVC syllables; diphthongs were examined in terms of temporal organization, spectral properties and dynamic aspects. Results suggest that falling diphthongs tend to have a single dynamic target, while rising diphthongs have two static spectral targets",
    "checked": true,
    "id": "591a1fe2afd4787ec2ef7bc3847df084b9e3c3ec",
    "semantic_title": "vowels and diphthongs in hangzhou wu chinese dialect",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/m18b_interspeech.html": {
    "title": "Resyllabification in Indian Languages and Its Implications in Text-to-speech Systems",
    "volume": "main",
    "abstract": "Resyllabification is a phonological process in continuous speech in which the coda of a syllable is converted into the onset of the following syllable, either in the same word or in the subsequent word. This paper presents an analysis of resyllabification across words in different Indian languages and its implications in Indian language text-to-speech (TTS) synthesis systems. The evidence for resyllabification is evaluated based on the acoustic analysis of a read speech corpus of the corresponding language. This study shows that the resyllabification obeys the maximum onset principle and introduces the notion of prominence resyllabification in Indian languages. This paper finds acoustic evidence for total resyllabification. The resyllabification rules obtained are applied to TTS systems. The correctness of the rules is evaluated quantitatively by comparing the acoustic log-likelihood scores of the speech utterances with the original and resyllabified texts and by performing a pair comparison (PC) listening test on the synthesized speech output. An improvement in the log-likelihood score with the resyllabified text is observed and the synthesized speech with the resyllabified text is preferred 3 times over those without resyllabification",
    "checked": true,
    "id": "db9d1e365d8186c846a9dd52d9129e04259e9992",
    "semantic_title": "resyllabification in indian languages and its implications in text-to-speech systems",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/murphy18_interspeech.html": {
    "title": "Voice Source Contribution to Prominence Perception: Rd Implementation",
    "volume": "main",
    "abstract": "This paper explores the contribution of voice source modulation to the perception of prominence, following on previous analyses of accentuation, focus and deaccentuation. A listening test was carried out on a sentence of Irish with three accented, prominent syllables (P1, P2, P3). Using inverse filtering and resynthesis, a ‘flattened' version was generated, with only slight declination of f0 and other voice source parameters. The global waveshape parameter Rd was modulated to provide (i) source boosting (tenser phonation) on either P1 or P2 and/or (ii) source attenuation (laxer phonation) following (Post-attenuation) or preceding (Pre-attenuation) P1 or P2. Rd variation was achieved in two different ways to generate two series of stimuli. f0 was not varied in either series. Twenty-nine listeners rated the prominence level of all syllables in the utterance. Results show that the phrasal position (P1 vs. P2) makes a large difference to prominence judgements. P1 emerged as overall more prominent and more readily ‘enhanced' by the source modifications. Post-attenuation was particularly important for P1, with effects equal to or greater than local P-boosting. In the case of P2, Pre-attenuation was much more important than Post-attenuation",
    "checked": true,
    "id": "ae6f6b5d903588b7ad837ebf1460231a0941502e",
    "semantic_title": "voice source contribution to prominence perception: rd implementation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gobl18_interspeech.html": {
    "title": "On the Relationship between Glottal Pulse Shape and Its Spectrum: Correlations of Open Quotient, Pulse Skew and Peak Flow with Source Harmonic Amplitudes",
    "volume": "main",
    "abstract": "This paper explores the relationship between the glottal pulse amplitude (Up) and the amplitude of the first harmonic (H1), as well as the combined effects of Up, the open quotient (Oq) and degree of pulse asymmetry/skew (Rk) on the low end of the source spectrum. This serves to elucidate their relationship to the H1-H2 estimate, widely used to make inferences on changes in Oq and voice quality. It has been suggested that H1 is mainly determined by Up and that the pulse shape has a relatively small impact. To investigate this, a series of glottal pulses were generated using the LF model, where Up was kept constant, while Oq and Rk were systematically varied. The resulting harmonic amplitudes of these pulses show that Up is not the sole determinant of H1. Rather, H1 is highly dependent on Oq and to a certain degree also on Rk. Although the effects of these parameters on the lowest harmonics is rather complex, we find that the H1-H2 measure is broadly correlated with Oq. However, there is also a strong effect of differences in glottal skew, particularly at high Oq values, which could invalidate inferences on Oq and voice quality from estimates of H1-H2",
    "checked": true,
    "id": "8f7217566f9bb29cef79201e37df8f852d450e26",
    "semantic_title": "on the relationship between glottal pulse shape and its spectrum: correlations of open quotient, pulse skew and peak flow with source harmonic amplitudes",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hughes18_interspeech.html": {
    "title": "The Individual and the System: Assessing the Stability of the Output of a Semi-automatic Forensic Voice Comparison System",
    "volume": "main",
    "abstract": "Semi-automatic systems based on traditional linguistic-phonetic features are increasingly being used for forensic voice comparison (FVC) casework. In this paper, we examine the stability of the output of a semi-automatic system, based on the long-term formant distributions (LTFDs) of F1, F2 and F3, as the channel quality of the input recordings decreases. Cross-validated, calibrated GMM-UBM log likelihood-ratios (LLRs) were computed for 97 Standard Southern British English speakers under four conditions. In each condition the same speech material was used, but the technical properties of the recordings changed (high quality studio recording, landline telephone recording, high bit-rate GSM mobile telephone recording and low bit-rate GSM mobile telephone recording). Equal error rate (EER) and the log LR cost function (Cllr) were compared across conditions. System validity was found to decrease with poorer technical quality, with the largest differences in EER (21.66%) and Cllr (0.46) found between the studio and the low bit-rate GSM conditions. However, importantly, performance for individual speakers was affected differently by channel quality. Speakers that produced stronger evidence overall were found to be more variable. Mean F3 was also found to be a predictor of LLR variability, however no effects were found based on speakers' voice quality profiles",
    "checked": true,
    "id": "2a5c0514deeeb3396d6df0dd3ddce95d69b507b7",
    "semantic_title": "the individual and the system: assessing the stability of the output of a semi-automatic forensic voice comparison system",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kadiri18b_interspeech.html": {
    "title": "Breathy to Tense Voice Discrimination using Zero-Time Windowing Cepstral Coefficients (ZTWCCs)",
    "volume": "main",
    "abstract": "In this paper, we consider breathy to tense voices, which are often considered to be opposite ends of a voice quality continuum. Along with these, other aspects of a speaker's voice play an important role to convey the information to the listener such as mood, attitude and emotional state. The glottal pulse characteristics in different phonation types vary due to the tension of laryngeal muscles together with the respiratory effort. In the present study, we are deriving the features that can capture effects of excitation on the vocal tract system through a signal processing method, called as zero-time windowing (ZTW) method. The ZTW method gives the instantaneous spectrum which captures the changes in the speech production mechanism, providing higher spectral resolution. The cepstral coefficients derived from ZTW method are used for the classification of phonation types. Along with zero-time windowing cepstral coefficients (ZTWCCs), we use the excitation source features derived from zero frequency filtering (ZFF) method. The excitation features used are: strength of excitation, energy of excitation, loudness measure and ZFF signal energy. Classification experiments using ZTWCC and excitation features reveal a significant improvement in the detection of phonation type compared to the existing voice quality features and MFCC features",
    "checked": true,
    "id": "1a560f8a3ce211f0a3c219f6ca6f0600d9d1c0b7",
    "semantic_title": "breathy to tense voice discrimination using zero-time windowing cepstral coefficients (ztwccs)",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gogoi18_interspeech.html": {
    "title": "Analysis of Breathiness in Contextual Vowel of Voiceless Nasals in Mizo",
    "volume": "main",
    "abstract": "This study analyses the source characteristics of voiced and voiceless nasals in Mizo, a Tibeto-Burman language spoken in North-East India. Mizo is one of the few languages that has voiced and voiceless nasals in its phoneme inventory. This analysis is motivated by the interaction between breathiness and nasality reported in a number of speech perception studies using synthetic stimuli. However, there are no studies examining this interaction in vowels after voiced and voiceless nasals. Existing research has also documented the interaction between breathy phonation and vowel height. The current study is an acoustic analysis of breathiness in high and low vowels following voiced and voiceless nasals in Mizo. The acoustic parameter measures are: H1H2 ratio, spectral balance (SB), strength of excitation (SoE) and waveform peak factor (WPF). The values obtained for all the four acoustic measures suggest that vowels following voiceless nasals exhibit stronger acoustic characteristics associated with breathy phonation than vowels following voiced nasals. In addition, the degree of acoustic breathiness is affected by vowel height",
    "checked": true,
    "id": "1ccacd5a22b975b7c75797ec9bd6e318d1a67aea",
    "semantic_title": "analysis of breathiness in contextual vowel of voiceless nasals in mizo",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xu18c_interspeech.html": {
    "title": "Infant Emotional Outbursts Detection in Infant-parent Spoken Interactions",
    "volume": "main",
    "abstract": "Detection of infant emotional outbursts, such as crying, in large corpora of recorded infant speech, is essential to the study of dyadic social process, by which infants learn to identify and regulate their own emotions. Such large corpora now exist with the advent of LENA speech monitoring systems, but are not labeled for emotional outbursts. This paper reports on our efforts to manually code child utterances as being of type \"laugh\", \"cry\", \"fuss\", \"babble\" and \"hiccup\" and to develop algorithms capable of performing the same task automatically. Human labelers achieve much higher rates of inter-coder agreement for some of these categories than for others. Linear discriminant analysis (LDA) achieves better accuracy on tokens that have been coded by two human labelers than on tokens that have been coded by only one labeler, but the difference is not as much as we expected, suggesting that the acoustic and contextual features being used by human labelers are not yet available to the LDA. Convolutional neural network and hidden markov model achieve better accuracy than LDA, but worse F-score, because they over-weight the prior. Discounting the transition probability does not solve the problem",
    "checked": true,
    "id": "09688091a1ed5198a56c013add1cffca48d110cc",
    "semantic_title": "infant emotional outbursts detection in infant-parent spoken interactions",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cho18_interspeech.html": {
    "title": "Deep Neural Networks for Emotion Recognition Combining Audio and Transcripts",
    "volume": "main",
    "abstract": "In this paper, we propose to improve emotion recognition by combining acoustic information and conversation transcripts. On the one hand, an LSTM network was used to detect emotion from acoustic features like f0, shimmer, jitter, MFCC, etc. On the other hand, a multi-resolution CNN was used to detect emotion from word sequences. This CNN consists of several parallel convolutions with different kernel sizes to exploit contextual information at different levels. A temporal pooling layer aggregates the hidden representations of different words into a unique sequence level embedding, from which we compute the emotion posteriors. We optimized a weighted sum of classification and verification losses. The verification loss tries to bring embeddings from same emotions closer while it separates embeddings for different emotions. We also compared our CNN with state-of-the-art text-based hand-crafted features (e-vector). We evaluated our approach on the USC-IEMOCAP dataset as well as the dataset consisting of US English telephone speech. In the former, we used human transcripts while in the latter, we used ASR transcripts. The results showed fusing audio and transcript information improved unweighted accuracy by relative 24% for IEMOCAP and relative 3.4% for the telephone data compared to a single acoustic system",
    "checked": true,
    "id": "75b2843539dc8567b1502a19b3788adf6a015eb6",
    "semantic_title": "deep neural networks for emotion recognition combining audio and transcripts",
    "citation_count": 78
  },
  "https://www.isca-speech.org/archive/interspeech_2018/parthasarathy18b_interspeech.html": {
    "title": "Preference-Learning with Qualitative Agreement for Sentence Level Emotional Annotations",
    "volume": "main",
    "abstract": "The perceptual evaluation of emotional attributes is noisy due to inconsistencies between annotators. The low inter-evaluator agreement arises due to the complex nature of emotions. Conventional approaches average scores provided by multiple annotators. While this approach reduces the influence of dissident annotations, previous studies have showed the value of considering individual evaluations to better capture the underlying ground-truth. One of these approaches is the qualitative agreement (QA) method, which provides an alternative framework that captures the inherent trends amongst the annotators. While previous studies have focused on using the QA method for time-continuous annotations from a fixed number of annotators, most emotional databases are annotated with attributes at the sentence-level (e.g., one global score per sentence). This study proposes a novel formulation based on the QA framework to estimate reliable sentence-level annotations for preference-learning. The proposed relative labels between pairs of sentences capture consistent trends across evaluators. The experimental evaluation shows that preference-learning methods to rank-order emotional attributes trained with the proposed QA-based labels achieve significantly better performance than the same algorithms trained with relative scores obtained by averaging absolute scores across annotators. These results show the benefits of QA-based labels for preference-learning using sentence-level annotations",
    "checked": true,
    "id": "ab93dda9af122c2d0efdaff1feaff54d09b96687",
    "semantic_title": "preference-learning with qualitative agreement for sentence level emotional annotations",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2018/latif18b_interspeech.html": {
    "title": "Transfer Learning for Improving Speech Emotion Classification Accuracy",
    "volume": "main",
    "abstract": "The majority of existing speech emotion recognition research focuses on automatic emotion detection using training and testing data from same corpus collected under the same conditions. The performance of such systems has been shown to drop significantly in cross-corpus and cross-language scenarios. To address the problem, this paper exploits a transfer learning technique to improve the performance of speech emotion recognition systems that is novel in cross-language and cross-corpus scenarios. Evaluations on five different corpora in three different languages show that Deep Belief Networks (DBNs) offer better accuracy than previous approaches on cross-corpus emotion recognition, relative to a Sparse Autoencoder and Support Vector Machine (SVM) baseline system. Results also suggest that using a large number of languages for training and using a small fraction of the target data in training can significantly boost accuracy compared with baseline also for the corpus with limited training examples",
    "checked": true,
    "id": "766f677f4a42dcd43a856fad554c6678f642355b",
    "semantic_title": "transfer learning for improving speech emotion classification accuracy",
    "citation_count": 91
  },
  "https://www.isca-speech.org/archive/interspeech_2018/meyer18_interspeech.html": {
    "title": "What Do Classifiers Actually Learn? a Case Study on Emotion Recognition Datasets",
    "volume": "main",
    "abstract": "In supervised learning, a typical method to ensure that a classifier has desirable generalization properties, is to split the available data into training, validation and test subsets. Given a proper data split, we typically then trust our results on the test data. But what do classifiers actually learn? In this case study we show how important it is to analyze precisely the available data, its inherent dependencies w.r.t. class labels and present an example of a popular database for speech emotion recognition, where a minor change of the data split results in an accuracy decrease of about 55% absolute, leading to the conclusion that linguistic content has been learned instead of the desired speech emotions",
    "checked": true,
    "id": "6ec26ab014959cf6c62e5b79d69334767ef91909",
    "semantic_title": "what do classifiers actually learn? a case study on emotion recognition datasets",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rathner18b_interspeech.html": {
    "title": "State of Mind: Classification through Self-reported Affect and Word Use in Speech",
    "volume": "main",
    "abstract": "Human state–of-mind (SOM; e.g.: perception, cognition, attention) constantly shifts due to internal and external demands. Mental health is influenced by the habitual use of either adaptive or maladaptive SOM. Therefore, the training of conscious regulation of SOM could be promising in self-help (e- and m-health), blended care and psychotherapy. The presented study indicates that SOM can be influenced by telling personal narratives. Furthermore, SOM and narrative sentiment (positive vs. negative) can be predicted through word use. Such results lay the groundwork for the development of applications that analyse text and speech for: i) the early detection of mental health; ii) the early detection of maladaptive changes in emotion dynamics; (iii) the use of personal narratives to improve emotion regulation skills; iv) the distribution of tailored interventions; and finally, v) evaluation of therapy outcome",
    "checked": true,
    "id": "069ae43eab55150e2b54785cfdbe12b04b80ce9f",
    "semantic_title": "state of mind: classification through self-reported affect and word use in speech",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhao18c_interspeech.html": {
    "title": "Exploring Spatio-Temporal Representations by Integrating Attention-based Bidirectional-LSTM-RNNs and FCNs for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Automatic emotion recognition from speech, which is an important and challenging task in the field of affective computing, heavily relies on the effectiveness of the speech features for classification. Previous approaches to emotion recognition have mostly focused on the extraction of carefully hand-crafted features. How to model spatio-temporal dynamics for speech emotion recognition effectively is still under active investigation. In this paper, we propose a method to tackle the problem of emotional relevant feature extraction from speech by leveraging Attention-based Bidirectional Long Short-Term Memory Recurrent Neural Networks with fully convolutional networks in order to automatically learn the best spatio-temporal representations of speech signals. The learned high-level features are then fed into a deep neural network (DNN) to predict the final emotion. The experimental results on the Chinese Natural Audio-Visual Emotion Database (CHEAVD) and the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpora show that our method provides more accurate predictions compared with other existing emotion recognition algorithms",
    "checked": true,
    "id": "460c6277a9ad97b10c161c56aacc25ef71b56832",
    "semantic_title": "exploring spatio-temporal representations by integrating attention-based bidirectional-lstm-rnns and fcns for speech emotion recognition",
    "citation_count": 59
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ghahremani18b_interspeech.html": {
    "title": "End-to-end Deep Neural Network Age Estimation",
    "volume": "main",
    "abstract": "In this paper, we apply the recently proposed x-vector neural network architecture for the task of age estimation. This architecture maps a variable length utterance into a fixed dimensional embedding which retains the relevant sequence level information. This is achieved by a temporal pooling layer. From the embedding, a series of layers is applied to make predictions. The full network is trained end-to-end in a discriminative fashion. This kind of network is starting to outperform the state-of-the-art i-vector embeddings in tasks like speaker and language recognition. Motivated by this, we investigated the optimum way to train x-vectors for the age estimation task. Despite that a regression objective is typical for this task, we found that optimizing a mixture of classification and regression losses provides better results. We trained our models on the NIST SRE08 dataset and evaluated on SRE10. The proposed approach improved mean absolute error (MAE) by 12% w.r.t the i-vector baseline",
    "checked": true,
    "id": "c936edddcb803b9eb065b6128c6d0e28d5234db1",
    "semantic_title": "end-to-end deep neural network age estimation",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hebbar18_interspeech.html": {
    "title": "Improving Gender Identification in Movie Audio Using Cross-Domain Data",
    "volume": "main",
    "abstract": "Gender identification from audio is an important task for quantitative gender analysis in multimedia and to improve tasks like speech recognition. Robust gender identification requires speech segmentation that relies on accurate voice activity detection (VAD). These tasks are challenging in movie audio due to diverse and often noisy acoustic conditions. In this work, we acquire VAD labels for movie audio by aligning it with subtitle text and train a recurrent neural network model for VAD. Subsequently, we apply transfer learning to predict gender using feature embeddings obtained from a model pre-trained for large-scale audio classification. In order to account for the diverse acoustic conditions in movie audio, we use audio clips from YouTube labeled for gender. We compare the performance of our proposed method with baseline experiments that were setup to assess the importance of feature embeddings and training data used for gender identification task. For systematic evaluation, we extend an existing benchmark dataset for movie VAD, to include precise gender labels. The VAD system shows comparable results to state-of-the-art in movie domain. The proposed gender identification system outperforms existing baselines, achieving an accuracy of 85% for movie audio. We have made the data and related code publicly available",
    "checked": true,
    "id": "c21b8b1de04a7f06a4be7a5afd78e31c105aa09a",
    "semantic_title": "improving gender identification in movie audio using cross-domain data",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kabil18_interspeech.html": {
    "title": "On Learning to Identify Genders from Raw Speech Signal Using CNNs",
    "volume": "main",
    "abstract": "Automatic Gender Recognition (AGR) is the task of identifying the gender of a speaker given a speech signal. Standard approaches extract features like fundamental frequency and cepstral features from the speech signal and train a binary classifier. Inspired from recent works in the area of automatic speech recognition (ASR), speaker recognition and presentation attack detection, we present a novel approach where relevant features and classifier are jointly learned from the raw speech signal in end-to-end manner. We propose a convolutional neural networks (CNN) based gender classifier that consists of: (1) convolution layers, which can be interpreted as a feature learning stage and (2) a multilayer perceptron (MLP), which can be interpreted as a classification stage. The system takes raw speech signal as input and outputs gender posterior probabilities. Experimental studies conducted on two datasets, namely AVspoof and ASVspoof 2015, with different architectures show that with simple architectures the proposed approach yields better system than standard acoustic features based approach. Further analysis of the CNNs show that the CNNs learn formant and fundamental frequency information for gender identification",
    "checked": true,
    "id": "69dcf97ad7d0918bbace3537db6ec5ab2ae03bb6",
    "semantic_title": "on learning to identify genders from raw speech signal using cnns",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sebastian18_interspeech.html": {
    "title": "Denoising and Raw-waveform Networks for Weakly-Supervised Gender Identification on Noisy Speech",
    "volume": "main",
    "abstract": "This paper presents a raw-waveform neural network and uses it along with a denoising network for clustering in weakly-supervised learning scenarios under extreme noise conditions. Specifically, we consider language independent gender identification on a set of varied noise conditions and signal to noise ratios (SNRs). We formulate the denoising problem as a source separation task and train the system using a discriminative criterion in order to enhance output SNRs. A denoising recurrent neural network (RNN) is first trained on a small subset (roughly one-fifth) of the data for learning a speech-specific mask. The denoised speech signal is then directly fed as input to a raw-waveform convolutional neural network (CNN) trained with denoised speech. We evaluate the standalone performance of denoiser in terms of various signal-to-noise measures and discuss its contribution towards robust gender identification. An absolute improvement of 11.06% and 13.33% is achieved by the combined pipeline over the i-vector SVM baseline system for 0 dB and -5 dB SNR conditions, respectively. We further analyse the information captured by the first CNN layer in both noisy and denoised speech",
    "checked": true,
    "id": "2b99bf3b749fad4379cdcffdad999ed8cbec2dbd",
    "semantic_title": "denoising and raw-waveform networks for weakly-supervised gender identification on noisy speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/williamson18_interspeech.html": {
    "title": "The Effect of Exposure to High Altitude and Heat on Speech Articulatory Coordination",
    "volume": "main",
    "abstract": "The effects of altitude and heat on speech articulatory coordination following exercise and approximately three hours of exposure are explored. Recordings of read speech and free response speech before and after exercise in moderate altitude, moderate heat and both moderate altitude and heat are analyzed using features that characterize articulatory coordination. It is found that 1) moderate altitude causes small changes and moderate heat negligible changes to articulatory coordination features after brief exposure prior to exercise; 2) moderate altitude and heat produce similar large feature changes following exercise and longer exposure; 3) moderate altitude and heat produce larger feature changes in combination than individually immediately following exercise. Finally, using cross-validation training of a statistical classifiers, the features are sufficient to classify the four experimental conditions with an overall accuracy of 0.50 and to detect the presence of any one of the experimental conditions with an accuracy of 0.90",
    "checked": true,
    "id": "3dee55cea99f341f7440fabcc388eb7c88d6b248",
    "semantic_title": "the effect of exposure to high altitude and heat on speech articulatory coordination",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18i_interspeech.html": {
    "title": "Permutation Invariant Training of Generative Adversarial Network for Monaural Speech Separation",
    "volume": "main",
    "abstract": "We explore generative adversarial networks (GANs) for speech separation, particularly with permutation invariant training (SSGAN-PIT). Prior work demonstrates that GANs can be implemented for suppressing additive noise in noisy speech waveform and improving perceptual speech quality. In this work, we train GANs for speech separation which enhances multiple speech sources simultaneously with the permutation issue addressed by the utterance level PIT in the training of the generator network. We propose operating GANs on the power spectrum domain instead of waveforms to reduce computation. To better explore time dependencies, recurrent neural networks (RNNs) with long short-term memory (LSTM) are adopted for both generator and discriminator in this study. We evaluated SSGAN-PIT on the WSJ0 two-talker mixed speech separation task and found that SSGAN-PIT outperforms SSGAN without PIT and the neural networks based speech separation with or without PIT. The evaluation confirms the feasibility of the proposed model and training approach for efficient speech separation. The convergence behavior of permutation invariant training and adversarial training are analyzed",
    "checked": true,
    "id": "7d00ac1dc541fc8a2b5535cb7986967a508bea9d",
    "semantic_title": "permutation invariant training of generative adversarial network for monaural speech separation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18d_interspeech.html": {
    "title": "Deep Extractor Network for Target Speaker Recovery from Single Channel Speech Mixtures",
    "volume": "main",
    "abstract": "Speaker-aware source separation methods are promising workarounds for major difficulties such as arbitrary source permutation and unknown number of sources. However, it remains challenging to achieve satisfying performance provided a very short available target speaker utterance (anchor). Here we present a novel \"deep extractor network\" which creates an extractor point for the target speaker in a canonical high dimensional embedding space and pulls together the time-frequency bins corresponding to the target speaker. The proposed model is different from prior works that the carnonical embedding space encodes knowledges of both the anchor and the mixture during training phase: first, embeddings for the anchor and mixture speech are separately constructed in a primary embedding space and then combined as an input to feed-forward layers to transform to a carnonical embedding space which we discover more stable than the primary one. Experimental results show that given a very short utterance, the proposed model can efficiently recover high quality target speech from a mixture, which outperforms various baseline models, with 5.2% and 6.6% relative improvements in SDR and PESQ respectively compared with a baseline oracle deep attracor model. Meanwhile, we show it can be generalized well to more than one interfering speaker",
    "checked": true,
    "id": "792b700e1a04e9b9b4ec8b2b2bee66a6cf8ebe3e",
    "semantic_title": "deep extractor network for target speaker recovery from single channel speech mixtures",
    "citation_count": 82
  },
  "https://www.isca-speech.org/archive/interspeech_2018/he18b_interspeech.html": {
    "title": "Joint Localization and Classification of Multiple Sound Sources Using a Multi-task Neural Network",
    "volume": "main",
    "abstract": "We propose a novel multi-task neural network-based approach for joint sound source localization and speech/non-speech classification in noisy environments. The network takes raw short time Fourier transform as input and outputs the likelihood values for the two tasks, which are used for the simultaneous detection, localization and classification of an unknown number of overlapping sound sources, Tested with real recorded data, our method achieves significantly better performance in terms of speech/non-speech classification and localization of speech sources, compared to method that performs localization and classification separately. In addition, we demonstrate that incorporating the temporal context can further improve the performance",
    "checked": true,
    "id": "8ddc1c11c94259cd8e7d85dfdb7337155c627fb6",
    "semantic_title": "joint localization and classification of multiple sound sources using a multi-task neural network",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yang18_interspeech.html": {
    "title": "Detection of Glottal Closure Instants from Speech Signals: A Convolutional Neural Network Based Method",
    "volume": "main",
    "abstract": "Most conventional methods to detect glottal closure instants (GCI) are based on signal processing technologies and different GCI candidate selection methods. This paper proposes a classification method to detect glottal closure instants from speech waveforms using convolutional neural network (CNN). The procedure is divided into two successive steps. Firstly, a low-pass filtered signal is computed, whose negative peaks are taken as candidates for GCI placement. Secondly, a CNN-based classification model determines for each peak whether it corresponds to a GCI or not. The method is compared with three existing GCI detection algorithms on two publicly available databases. For the proposed method, the detection accuracy in terms of F1-score is 98.23%. Additional experiment indicates that the model can perform better after trained with the speech data from the speakers who are the same as those in the test set",
    "checked": true,
    "id": "22cae7f1a4918f66bbe076896e1f05af8f810346",
    "semantic_title": "detection of glottal closure instants from speech signals: a convolutional neural network based method",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18g_interspeech.html": {
    "title": "Robust TDOA Estimation Based on Time-Frequency Masking and Deep Neural Networks",
    "volume": "main",
    "abstract": "Deep learning based time-frequency (T-F) masking has dramatically advanced monaural speech separation and enhancement. This study investigates its potential for robust time difference of arrival (TDOA) estimation in noisy and reverberant environments. Three novel algorithms are proposed to improve the robustness of conventional cross-correlation-, beamforming- and subspace-based algorithms for speaker localization. The key idea is to leverage the power of deep neural networks (DNN) to accurately identify T-F units that are relatively clean for TDOA estimation. All of the proposed algorithms exhibit strong robustness for TDOA estimation in environments with low input SNR, high reverberation and low direction-to-reverberant energy ratio",
    "checked": true,
    "id": "0c4603e0bc53a133560b81b5f76e58a9819c359f",
    "semantic_title": "robust tdoa estimation based on time-frequency masking and deep neural networks",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kato18_interspeech.html": {
    "title": "Waveform to Single Sinusoid Regression to Estimate the F0 Contour from Noisy Speech Using Recurrent Deep Neural Networks",
    "volume": "main",
    "abstract": "The fundamental frequency (F0) represents pitch in speech that determines prosodic characteristics of speech and is needed in various tasks for speech analysis and synthesis. Despite decades of research on this topic, F0 estimation at low signal-to-noise ratios (SNRs) in unexpected noise conditions remains difficult. This work proposes a new approach to noise robust F0 estimation using a recurrent neural network (RNN) trained in a supervised manner. Recent studies employ deep neural networks (DNNs) for F0 tracking as a frame-by-frame classification task into quantised frequency states but we propose waveform-to-sinusoid regression instead to achieve both noise robustness and accurate estimation with increased frequency resolution. Experimental results with PTDB-TUG corpus contaminated by additive noise (NOISEX-92) demonstrate that the proposed method improves gross pitch error (GPE) rate and fine pitch error (FPE) by more than 35% at SNRs between -10 dB and +10 dB compared with well-known noise robust F0 tracker, PEFAC. Furthermore, the proposed method also outperforms state-of-the-art DNN-based approaches by more than 15% in terms of both FPE and GPE rate over the preceding SNR range",
    "checked": true,
    "id": "3f6fb909f4d167c7d1fab2cf331ff32d652b084d",
    "semantic_title": "waveform to single sinusoid regression to estimate the f0 contour from noisy speech using recurrent deep neural networks",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2018/magron18b_interspeech.html": {
    "title": "Reducing Interference with Phase Recovery in DNN-based Monaural Singing Voice Separation",
    "volume": "main",
    "abstract": "State-of-the-art methods for monaural singing voice separation consist in estimating the magnitude spectrum of the voice in the short-time Fourier transform (STFT) domain by means of deep neural networks (DNNs). The resulting magnitude estimate is then combined with the mixture's phase to retrieve the complex-valued STFT of the voice, which is further synthesized into a time-domain signal. However, when the sources overlap in time and frequency, the STFT phase of the voice differs from the mixture's phase, which results in interference and artifacts in the estimated signals. In this paper, we investigate on recent phase recovery algorithms that tackle this issue and can further enhance the separation quality. These algorithms exploit phase constraints that originate from a sinusoidal model or from consistency, a property that is a direct consequence of the STFT redundancy. Experiments conducted on real music songs show that those algorithms are efficient for reducing interference in the estimated voice compared to the baseline approach",
    "checked": true,
    "id": "67c5f07208a9216b9e9e503067bd75b2559891ff",
    "semantic_title": "reducing interference with phase recovery in dnn-based monaural singing voice separation",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hua18_interspeech.html": {
    "title": "Nebula: F0 Estimation and Voicing Detection by Modeling the Statistical Properties of Feature Extractors",
    "volume": "main",
    "abstract": "A F0 and voicing status estimation algorithm for high quality speech analysis/synthesis is proposed. This problem is approached from a different perspective that models the behavior of feature extractors under noise, instead of directly modeling speech signals. Under time-frequency locality assumptions, the joint distribution of extracted features and target F0 can be characterized by training a bank of Gaussian mixture models (GMM) on artificial data generated from Monte-Carlo simulations. The trained GMMs can then be used to generate a set of conditional distributions on the predicted F0, which are then combined and post-processed by Viterbi algorithm to give a final F0 trajectory. Evaluation on CSTR and CMU Arctic speech databases shows that the proposed method, trained on fully synthetic data, achieves lower gross error rates than state-of-the-art methods",
    "checked": true,
    "id": "b00d4652ee4ff92d495a869c65e7ebc06d306e35",
    "semantic_title": "nebula: f0 estimation and voicing detection by modeling the statistical properties of feature extractors",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/luo18c_interspeech.html": {
    "title": "Real-time Single-channel Dereverberation and Separation with Time-domain Audio Separation Network",
    "volume": "main",
    "abstract": "We investigate the recently proposed Time-domain Audio Separation Network (TasNet) in the task of real-time single-channel speech dereverberation. Unlike systems that take time-frequency representation of the audio as input, TasNet learns an adaptive front-end in replacement of the time-frequency representation by a time-domain convolutional non-negative autoencoder. We show that by formulating the dereverberation problem as a denoising problem where the direct path is separated from the reverberations, a TasNet denoising autoencoder can outperform a deep LSTM baseline on log-power magnitude spectrogram input in both causal and non-causal settings. We further show that adjusting the stride size in the convolutional autoencoder helps both the dereverberation and separation performance",
    "checked": true,
    "id": "219eb2745a272f29c1b417eb247ef0bc5df9e3cd",
    "semantic_title": "real-time single-channel dereverberation and separation with time-domain audio separation network",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kumar18c_interspeech.html": {
    "title": "Music Source Activity Detection and Separation Using Deep Attractor Network",
    "volume": "main",
    "abstract": "In music signal processing, singing voice detection and music source separation are widely researched topics. Recent progress in deep neural network based source separation has advanced the state of the performance in the problem of vocal and instrument separation, while the problem of joint source activity detection and separation remains unexplored. In this paper, we propose an approach to perform source activity detection using the high-dimensional embedding generated by Deep Attractor Network (DANet) when trained for music source separation. By defining both tasks together, DANet is able to dynamically estimate the number of outputs depending on the active sources. We propose an Expectation-Maximization (EM) training paradigm for DANet which further improves the separation performance of the original DANet. Experiments show that our network achieves higher source separation and comparable source activity detection against a baseline system",
    "checked": true,
    "id": "1f3d1f81880e5d8a5a69a23e61fef9467f9f639e",
    "semantic_title": "music source activity detection and separation using deep attractor network",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yang18d_interspeech.html": {
    "title": "Improving Mandarin Tone Recognition Using Convolutional Bidirectional Long Short-Term Memory with Attention",
    "volume": "main",
    "abstract": "Automatic tone recognition is useful for Mandarin spoken language processing. However, the complex F0 variations from the tone co-articulations and the interplay effects among tonality make it rather difficult to perform tone recognition of Chinese continuous speech. This paper explored the application of Bidirectional Long Short-Term Memory (BLSTM), which had the capability of modeling time series, to Mandarin tone recognition to handle the tone variations in continuous speech. In addition, we introduced attention mechanism to guide the model to select the suitable context information. The experimental results showed that the performance of proposed CNN-BLSTM with attention mechanism was the best and it achieved the tone error rate (TER) of 9.30% with a 17.6% relative error reduction from the DNN baseline system with TER of 11.28%. It demonstrated that our proposed model was more effective to handle the complex F0 variations than other models",
    "checked": true,
    "id": "6786522b75e6dfb27f2fd117166fa45d6b55036e",
    "semantic_title": "improving mandarin tone recognition using convolutional bidirectional long short-term memory with attention",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vanson18_interspeech.html": {
    "title": "Vowel Space as a Tool to Evaluate Articulation Problems",
    "volume": "main",
    "abstract": "Treatment for oral tumors can lead to long term changes in the anatomy and physiology of the vocal tract and result in problems with articulation. There are currently no readily available automatic methods to evaluate changes in articulation. We developed a Praat script which plots and measures vowel space coverage. The script reproduces speaker specific vowel space use and speaking-style dependent vowel reduction in normal speech from a Dutch corpus. Speaker identity and speaking style explain more than 60% of the variance in the measured area of the vowel triangle. In recordings of patients treated for oral tumors, vowel space use before and after treatment is still significantly correlated. Articulation before and after treatment is evaluated in a listening experiment and from a maximal articulation speed task. Linear models can explain 50-75% of variance in perceptual ratings and relative articulation rate from values at previous recordings and vowel space measures",
    "checked": true,
    "id": "36d22629fa4a3ef064f1ef86a9eb566fa20bda13",
    "semantic_title": "vowel space as a tool to evaluate articulation problems",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2018/delvaux18_interspeech.html": {
    "title": "Towards a Better Characterization of Parkinsonian Speech: A Multidimensional Acoustic Study",
    "volume": "main",
    "abstract": "This paper reports on a first attempt at adopting a new perspective in characterizing speech disorders in Parkinson's Disease (PD) based on individual patient profiles. Acoustic data were collected on 13 Belgian French speakers with PD, 6 male, 7 female, aged 45-81 and 50 healthy controls (HC) using the \"MonPaGe\" protocol (Fougeron et al., LREC18). In this protocol, various kinds of linguistic material are recorded in different speech conditions, in order to assess multiple speech dimensions for each speaker. First, we compared a variety of voice and speech parameters across groups (HC vs. PD patients). Second, we examined individual profiles of PD patients. Results showed that as a group PD participants most systematically differed from HC in terms of speech tempo and rythm. Moreover, the analysis of individual profiles revealed that other parameters, related to pneumophonatory control and linguistic prosody, were valuable to describe the speech specificities of several PD patients",
    "checked": true,
    "id": "26c2a63a20c59522a3837d19d31df783c45bb1c7",
    "semantic_title": "towards a better characterization of parkinsonian speech: a multidimensional acoustic study",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kalita18_interspeech.html": {
    "title": "Self-similarity Matrix Based Intelligibility Assessment of Cleft Lip and Palate Speech",
    "volume": "main",
    "abstract": "This work presents a comparison based framework by exploiting the self-similarity matrices matching technique to estimate the speech intelligibility of cleft lip and palate (CLP) children. Self-similarity matrix (SSM) of a feature sequence is a square matrix, which encodes the acoustic-phonetic composition of the underlying speech signal. Deviations in the acoustic characteristics of underlying sound units due to the degradation of intelligibility will deviate the CLP speech's SSM structure from that of normal. This degree of deviations in CLP speech's SSM from the corresponding normal speech's SSM may provide information about the severity profile of speech intelligibility. The degree of deviations is quantified using the structural similarity (SSIM) index, which is considered as the representative of objective intelligibility score. The proposed method is evaluated using two parameterizations of speech signals: Mel-frequency cepstral coefficients and Gaussian posteriorgrams and compared with dynamic time warping (DTW) based intelligibility assessment method. The proposed SSM based method shows the better correlation with the perceptual ratings of intelligibility when compared to the DTW based method",
    "checked": true,
    "id": "32f9eace2f896fcabc64a0a0ab52a718ea5ea618",
    "semantic_title": "self-similarity matrix based intelligibility assessment of cleft lip and palate speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dubey18b_interspeech.html": {
    "title": "Pitch-Adaptive Front-end Feature for Hypernasality Detection",
    "volume": "main",
    "abstract": "Hypernasality in cleft palate (CP) children is due to the velopharyngeal insufficiency. The vowels get nasalized in hypernasal speech and the nasality evidence are mainly present in low-frequency region around the first formant (F1) of vowels. The detection of hypernasality using Mel-frequency cepstral coefficient (MFCC) feature may get affected because the feature might not be able to capture the nasality evidence present in the low-frequency region. This is due to the fact that the MFCC feature extracted from high pitched children speech contains the pitch harmonics effect of magnitude spectrum. The pitch harmonics effect results in high variance for the higher dimensions of MFCC coefficients. This problem may increase due to high perturbation in pitch of CP speech. So in this work, a pitch-adaptive MFCC feature is used for hypernasality detection. The feature is derived from the cepstral smooth spectrum instead of magnitude spectrum. A pitch-adaptive low time liftering is done to smooth out the pitch harmonics. This feature when used for the detection of hypernasality using support vector machine (SVM) gives an accuracy of 83.45%, 88.04% and 85.58% for /a/, /i/ and /u/ vowels respectively, which is better than the accuracy of MFCC feature",
    "checked": true,
    "id": "247506fff04119ab323dd8627d7fa25dd93ea99a",
    "semantic_title": "pitch-adaptive front-end feature for hypernasality detection",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/norel18_interspeech.html": {
    "title": "Detection of Amyotrophic Lateral Sclerosis (ALS) via Acoustic Analysis",
    "volume": "main",
    "abstract": "ALS is a fatal neurodegenerative disease with no cure. Experts typically measure disease progression via the ALSFRS-R score, which includes measurements of various abilities known to decline. We propose instead the use of speech analysis as a proxy for ALS progression. This technique enables 1) frequent non-invasive, inexpensive, longitudinal analysis, 2) analysis of data recorded in the wild and 3) creation of an extensive ALS databank for future analysis. Patients and trained medical professionals need not be co-located, enabling more frequent monitoring of more patients from the convenience of their own homes. The goals of this study are the identification of acoustic speech features in naturalistic contexts which characterize disease progression and development of machine models which can recognize the presence and severity of the disease. We evaluated subjects from the Prize4Life Israel dataset, using a variety of frequency, spectral and voice quality features. The dataset was generated using the ALS Mobile Analyzer, a cell-phone app that collects data regarding disease progress using a self-reported ALSFRS-R questionnaire and several active tasks that measure speech and motor skills. Classification via leave-five-subjects-out cross-validation resulted in an accuracy rate of 79% (61% chance) for males and 83% (52% chance) for females",
    "checked": true,
    "id": "1aee4a79913fb155d7da1fb78e1b91b4292034c4",
    "semantic_title": "detection of amyotrophic lateral sclerosis (als) via acoustic analysis",
    "citation_count": 35
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vikram18c_interspeech.html": {
    "title": "Detection of Glottal Activity Errors in Production of Stop Consonants in Children with Cleft Lip and Palate",
    "volume": "main",
    "abstract": "Individuals with cleft lip and palate (CLP) alter the glottal activity characteristics during the production of stop consonants. The presence/absence of glottal vibrations during the production of unvoiced/voiced stops is referred as glottal activity error (GAE). In this work, acoustic-phonetic and production based knowledge of stop consonants are exploited to propose an algorithm for the automatic detection of GAE. The algorithm uses zero frequency filtered and band-pass (500-4000 Hz) filtered speech signals to identify the syllable nuclei positions, followed by the detection of glottal activity characteristics of consonant present within the syllable. Based on the identified glottal activity characteristics of consonant and a priori voicing information of target stop consonant, the presence or absence of GAE is detected. The algorithm is evaluated over the database containing the responses of normal children and children with repaired CLP for the target consonant-vowel-consonant-vowel words with stop consonants",
    "checked": true,
    "id": "0feee566d7c56c2cad94d9cee9ec2d4717fb319f",
    "semantic_title": "detection of glottal activity errors in production of stop consonants in children with cleft lip and palate",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sriram18_interspeech.html": {
    "title": "Cold Fusion: Training Seq2Seq Models Together with Language Models",
    "volume": "main",
    "abstract": "Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold Fusion are able to better utilize language information enjoying i) faster convergence and better generalization and ii) almost complete transfer to a new domain while using less than 10% of the labeled training data",
    "checked": true,
    "id": "33125ec92a0b4b1687ccd153762d6275668e3d09",
    "semantic_title": "cold fusion: training seq2seq models together with language models",
    "citation_count": 241
  },
  "https://www.isca-speech.org/archive/interspeech_2018/irie18_interspeech.html": {
    "title": "Investigation on Estimation of Sentence Probability by Combining Forward, Backward and Bi-directional LSTM-RNNs",
    "volume": "main",
    "abstract": "A combination of forward and backward long short-term memory (LSTM) recurrent neural network (RNN) language models is a popular model combination approach to improve the estimation of the sequence probability in the second pass N-best list rescoring in automatic speech recognition (ASR). In this work, we further push such an idea by proposing a combination of three models: a forward LSTM language model, a backward LSTM language model and a bi-directional LSTM based gap completion model. We derive such a combination method from a forward backward decomposition of the sequence probability. We carry out experiments on the Switchboard speech recognition task. While we empirically find that such a combination gives slight improvements in perplexity over the combination of forward and backward models, we finally show that a combination of the same number of forward models gives the best perplexity and word error rate (WER) overall",
    "checked": true,
    "id": "5ceb3cf83107c22180d862362bde260d032a1cfe",
    "semantic_title": "investigation on estimation of sentence probability by combining forward, backward and bi-directional lstm-rnns",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zenkel18_interspeech.html": {
    "title": "Subword and Crossword Units for CTC Acoustic Models",
    "volume": "main",
    "abstract": "This paper proposes a novel approach to create a unit set for CTC-based speech recognition systems. By using Byte-Pair Encoding we learn a unit set of an arbitrary size on a given training text. In contrast to using characters or words as units this allows us to find a good trade-off between the size of our unit set and the available training data. We investigate both Crossword units, that may span multiple word and Subword units. By evaluating these unit sets with decodings methods using a separate language model we are able to show improvements over a purely character-based unit set",
    "checked": true,
    "id": "223d74b4f401cdb0bcb31467c8cfecbf46f65795",
    "semantic_title": "subword and crossword units for ctc acoustic models",
    "citation_count": 34
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tanaka18_interspeech.html": {
    "title": "Neural Error Corrective Language Models for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "We present novel neural network based language models that can correct automatic speech recognition (ASR) errors by using speech recognizer output as a context. These models, called neural error corrective language models (NECLMs), utilizes ASR hypotheses of a target utterance as a context for estimating the generative probability of words. NECLMs are expressed as conditional generative models composed of an encoder network and a decoder network. In the models, the encoder network constructs context vectors from N-best lists and ASR confidence scores generated in a speech recognizer. The decoder network rescores recognition hypotheses by computing a generative probability of words using the context vectors so as to correct ASR errors. We evaluate the proposed models in Japanese lecture ASR tasks. Experimental results show that NECLM achieve better ASR performance than a state-of-the-art ASR system that incorporate a convolutional neural network acoustic model and a long short-term memory recurrent neural network language model",
    "checked": true,
    "id": "8ffc6d3c3af7a2c3489773db85beb61f01357a51",
    "semantic_title": "neural error corrective language models for automatic speech recognition",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rasooli18_interspeech.html": {
    "title": "Entity-Aware Language Model as an Unsupervised Reranker",
    "volume": "main",
    "abstract": "In language modeling, it is difficult to incorporate entity relationships from a knowledge-base. One solution is to use a reranker trained with global features, in which global features are derived from n-best lists. However, training such a reranker requires manually annotated n-best lists, which is expensive to obtain. We propose a method based on the contrastive estimation method that alleviates the need for such data. Experiments in the music domain demonstrate that global features, as well as features extracted from an external knowledge-base, can be incorporated into our reranker. Our final model, a simple ensemble of a language model and reranker, achieves a 0.44% absolute word error rate improvement over an LSTM language model on the blind test data",
    "checked": true,
    "id": "04c477a0e2ca828620753be89d1ffb465c851755",
    "semantic_title": "entity-aware language model as an unsupervised reranker",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/choi18_interspeech.html": {
    "title": "Character-level Language Modeling with Gated Hierarchical Recurrent Neural Networks",
    "volume": "main",
    "abstract": "Recurrent neural network (RNN)-based language models are widely used for speech recognition and translation applications. We propose a gated hierarchical recurrent neural network (GHRNN) and apply it to the character-level language modeling. GHRNN consists of multiple RNN units that operate with different time scales and the frequency of operation at each unit is controlled by the learned gates from training data. In our model, GHRNN learns the hierarchical structure of character, sub-word and word. Timing gates are included in the hierarchical connections to control the operating frequency of these units. The performance was measured for Penn Treebank and Wikitext-2 datasets. Experimental results showed lower bit per character (BPC) when compared to simply layered or skip-connected RNN models. Also, when a continuous cache model is added, the BPC of 1.192 is recorded, which is comparable to the state of the art result",
    "checked": true,
    "id": "852ffd44950f8a9433f4c31ddb3c843031188166",
    "semantic_title": "character-level language modeling with gated hierarchical recurrent neural networks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/levitan18_interspeech.html": {
    "title": "Acoustic-Prosodic Indicators of Deception and Trust in Interview Dialogues",
    "volume": "main",
    "abstract": "We analyze a set of acoustic-prosodic features in both truthful and deceptive responses to interview questions, identifying differences between truthful and deceptive speech. We also study the perception of deception, identifying acoustic-prosodic characteristics of speech that is perceived as truthful or deceptive by interviewers. In addition to studying differences across all speakers, we identify individual variations in deception production and perception across gender and native language. We conduct machine learning classification experiments aimed at distinguishing between truthful and deceptive speech, using acoustic-prosodic features. We also explore methods of leveraging individual traits for deception classification. Our results show that acoustic-prosodic features are highly effective at classifying deceptive speech. Our best classifier achieved an F1-score of 72.77, well above both the random baseline and above human performance at this task. This work advances our understanding of deception production and perception and has implications for automatic deception detection and the development of synthesized speech that is trustworthy",
    "checked": true,
    "id": "662c68980546ba71f379ed0f3f5b064204edc6ef",
    "semantic_title": "acoustic-prosodic indicators of deception and trust in interview dialogues",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2018/an18b_interspeech.html": {
    "title": "Deep Personality Recognition for Deception Detection",
    "volume": "main",
    "abstract": "Researchers in both psychology and computer science have suggested that modeling individual differences may improve the performance of automatic deception detection systems. In this study, we fuse a personality classifier with a deception classifier and evaluate various ways to combine the two tasks, either as a single network with shared layers, or by feeding the results of the personality classifier into the deception classifier. We show that including personality recognition improves the performance of deception detection on the Columbia X-Cultural Deception (CXD) corpus by more than 6% relative, achieving new state-of-the-art results on classification of phrase-like units in this corpus",
    "checked": true,
    "id": "8f09d590dbc8f11b8f2e3be7888f83479c59ee59",
    "semantic_title": "deep personality recognition for deception detection",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mixdorff18_interspeech.html": {
    "title": "Cross-cultural (A)symmetries in Audio-visual Attitude Perception",
    "volume": "main",
    "abstract": "This paper evaluates results from a cross-cultural and cross-language experiment series employing short audio-visual utterances produced with varying attitudinal expressions. German and Cantonese-speaking participants freely labeled such utterances in the two languages and assigned to each stimulus a verbal label. Based on the results of the four experiments we were able to establish to what degree the attitudinal frames of reference of the two groups overlap and how they differ. Verbal labels were assessed regarding their emotional content in terms of valence, activation and dominance and for the linguistic opposition between assertive and interrogative speech act and hence permit to abstract from the language of the rater and ultimately even abstract from the attitudinal categories used when eliciting the stimuli. Instead we regard each utterance as a data-point in the emotional space. We found that the judgments of the two rater groups agree well with respect to the valence of attitudinal expressions and diverge most as to the perceived activation of the stimulus presenter. Cantonese speaking participants seem to mirror Germans' ratings of German stimuli better than vice versa, which suggests an interesting asymmetry of attitudinal perception. As for the modality of presentation, the audio channel primarily transmits linguistically relevant information regarding the opposition of assertion and interrogation while the visual information signals the emotional content",
    "checked": true,
    "id": "197c76cfb2d456c590f5e381f4a83c4b42034cf1",
    "semantic_title": "cross-cultural (a)symmetries in audio-visual attitude perception",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/haider18_interspeech.html": {
    "title": "An Active Feature Transformation Method for Attitude Recognition of Video Bloggers",
    "volume": "main",
    "abstract": "Video blogging is a form of unidirectional communication where a video blogger expresses his/her opinion about different issues. The success of a video blog is measured using metrics like the number of views and comments by online viewers. Researchers have highlighted the importance of non-verbal behaviours (e.g. attitudes) in the context of video blogging and showed that it correlates with the level of attention (number of views) gained by a video blog. Therefore, an automatic attitude recognition system can help potential video bloggers to train their attitudes. It can also be useful in developing video blogs summarization and search tools. This study proposes a novel Active Feature Transformation (AFT) method for automatic recognition of attitudes (a form of non-verbal behaviour) in video blogs. The proposed method transforms the Mel-frequency Cepstral Coefficient (MFCC) features for the classification task. The Principal Component Analysis (PCA) transformation is also used for comparison. Our results show that AFT outperforms PCA in terms of accuracy and dimensionality reduction for attitude recognition using linear discrimination analysis, 1-nearest neighbour and decision tree classifiers",
    "checked": true,
    "id": "6b8e35afd732788fad11e5907e15820e99569e91",
    "semantic_title": "an active feature transformation method for attitude recognition of video bloggers",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tsai18_interspeech.html": {
    "title": "Automatic Assessment of Individual Culture Attribute of Power Distance Using a Social Context-Enhanced Prosodic Network Representation",
    "volume": "main",
    "abstract": "Culture is a collective social norm of human societies that often influences a person's values, thoughts and social behaviors during interactions at an individual level. In this work, we present a computational analysis toward automatic assessing an individual's culture attribute of power distance, i.e., a measure of his/her belief about status, authority and power in organizations, by modeling their expressive prosodic structures during social encounters with people of different power status. Specifically, we propose a center-loss embedded network architecture to jointly consider the effect of social interaction contexts on individuals' prosodic manifestations in order to learn an enhanced representation for power distance recognition. Our proposed prosodic network achieves an overall accuracy of 78.6% in binary classification task of recognizing high versus low power distance. Our experiment demonstrates an improved discriminability (17.6% absolute improvement) over prosodic neural network without social context enhancement. Further visualization reveals that the diversity in the prosodic manifestation for individuals with low power distance seems to be higher than those of high power distance",
    "checked": true,
    "id": "b6208c79cb90792bce4a2d00b1d1138bd91e681e",
    "semantic_title": "automatic assessment of individual culture attribute of power distance using a social context-enhanced prosodic network representation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kadiri18c_interspeech.html": {
    "title": "Analysis and Detection of Phonation Modes in Singing Voice using Excitation Source Features and Single Frequency Filtering Cepstral Coefficients (SFFCC)",
    "volume": "main",
    "abstract": "In this study, classification of the phonation modes in singing voice is carried out. Phonation modes in singing voice can be described using four categories: breathy, neutral, flow and pressed phonations. Previous studies on the classification of phonation modes use voice quality features derived from inverse filtering which lack in accuracy. This is due to difficulty in deriving the excitation source features using inverse filtering from singing voice. We propose to use the excitation source features that are derived directly from the signal. It is known that, the characteristics of the excitation source vary in different phonation types due to the vibration of the vocal folds together with the respiratory effort (lungs effort). In the present study, we are exploring excitation source features derived from the modified zero frequency filtering (ZFF) method. Apart from excitation source features, we also explore cepstral coefficients derived from single frequency filtering (SFF) method for the analysis and classification of phonation types in singing voice",
    "checked": true,
    "id": "31c004ae166b17349a366030b1fa255fe90cc4ed",
    "semantic_title": "analysis and detection of phonation modes in singing voice using excitation source features and single frequency filtering cepstral coefficients (sffcc)",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wu18b_interspeech.html": {
    "title": "A Deep Learning Method for Pathological Voice Detection Using Convolutional Deep Belief Networks",
    "volume": "main",
    "abstract": "Automatically detecting pathological voice disorders such as vocal cord paralysis or Reinke's edema is an important medical classification problem. While deep learning techniques have achieved significant progress in the speech recognition field, there has been less research work in the area of pathological voice disorders detection. A novel system for pathological voice detection using Convolutional Neural Network (CNN) as the basic architecture is presented in this work. The novel system uses spectrograms of normal and pathological speech recordings as the input to the network. Initially Convolutional Deep Belief Network (CDBN) are used to pre-train the weights of CNN system. This acts as a generative model to explore the structure of the input data using statistical methods. Then a CNN is trained using supervised back-propagation learning algorithm to fine tune the weights. Results show that a small amount of data can be used to achieve good results in classification with this deep learning approach. A performance analysis of the novel method is provided using real data from the Saarbrucken Voice database",
    "checked": true,
    "id": "e298e2eb7d731b6335581e52ba8e2f4db8c4a8d8",
    "semantic_title": "a deep learning method for pathological voice detection using convolutional deep belief networks",
    "citation_count": 48
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bhat18_interspeech.html": {
    "title": "Dysarthric Speech Recognition Using Time-delay Neural Network Based Denoising Autoencoder",
    "volume": "main",
    "abstract": "Dysarthria is a manisfestation of the disruption in the neuro-muscular physiology resulting in uneven, slow, slurred, harsh or quiet speech. Dysarthric speech poses serious challenges to automatic speech recognition, considering this speech is difficult to decipher for both humans and machines. The objective of this work is to enhance dysarthric speech features to match that of healthy control speech. We use a Time-Delay Neural Network based Denoising Autoencoder (TDNN-DAE) to enhance the dysarthric speech features. The dysarthric speech thus enhanced is recognized using a DNN-HMM based Automatic Speech Recognition (ASR) engine. This methodology was evaluated for speaker-independent (SI) and speaker-adapted (SA) systems. Absolute improvements of 13% and 3% was observed in the ASR performance for SI and SA systems respectively as compared with unenhanced dysarthric speech recognition",
    "checked": true,
    "id": "aa678e23f0b6c2b23528f6871a3d07a4c3aa1e61",
    "semantic_title": "dysarthric speech recognition using time-delay neural network based denoising autoencoder",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vasquezcorrea18_interspeech.html": {
    "title": "A Multitask Learning Approach to Assess the Dysarthria Severity in Patients with Parkinson's Disease",
    "volume": "main",
    "abstract": "Parkinson's disease is a neurodegenerative disorder characterized by a variety of motor and non-motor symptoms. Particularly, several speech impairments appear in the initial stages of the disease, which affect aspects related to respiration and the movement of muscles and limbs in the vocal tract. Most of the studies in the literature aim to assess only one specific task from the patients, such as the classification of patients vs. healthy speakers, or the assessment of the neurological state of the patients. This study proposes a multitask learning approach based on convolutional neural networks to assess at the same time several speech deficits of the patients. A total of eleven speech aspects are considered, including difficulties of the patients to move articulators such as lips, palate, tongue and larynx. According to the results, the proposed approach improves the generalization of the convolutional network, producing more representative feature maps to assess the different speech symptoms of the patients. The multitask learning scheme improves in of up to 4% the average accuracy relative to single networks trained to assess each individual speech aspect",
    "checked": true,
    "id": "71305f454431447d68384c42189858eb4fb2f770",
    "semantic_title": "a multitask learning approach to assess the dysarthria severity in patients with parkinson's disease",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lilley18_interspeech.html": {
    "title": "The Use of Machine Learning and Phonetic Endophenotypes to Discover Genetic Variants Associated with Speech Sound Disorder",
    "volume": "main",
    "abstract": "Thirty-four (34) children with reported speech sound disorders (SSD) were recruited for a prior study, as well as 31 of their siblings, many of whom also showed SSD. Using data-clustering techniques, we assigned each child to one or more endophenotypes defined by the number and type of speech errors made on the GFTA-2. The genetic samples of 53 of the participants underwent whole exome sequencing. Variant alleles were detected, filtered and annotated from the sequences and the data were filtered using quality checks, annotations and phenotypes. We then used Random Forest classification to search for associations between variants and endophenotypes. In this preliminary report, we highlight one promising association with a common variant of COMT, a dopamine metabolizer in the brain",
    "checked": true,
    "id": "61dc37b1f5e589222a6dde03e181aa124fad8199",
    "semantic_title": "the use of machine learning and phonetic endophenotypes to discover genetic variants associated with speech sound disorder",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/moore18_interspeech.html": {
    "title": "Whistle-blowing ASRs: Evaluating the Need for More Inclusive Speech Recognition Systems",
    "volume": "main",
    "abstract": "Speech is a complex process that can break in many different ways and lead to a variety of voice disorders. Dysarthria is a voice disorder where individuals are unable to control one or more of the aspects of speech—the articulation, breathing, voicing, or prosody—leading to less intelligible speech. In this paper, we evaluate the accuracy of state-of-the-art automatic speech recognition systems (ASRs) on two dysarthric speech datasets and compare the results to ASR performance on control speech. The limits of ASR performance using different voices have not been explored since the field has shifted from generative models of speech recognition to deep neural network architectures. To test how far the field has come in recognizing disordered speech, we test two different ASR systems: (1) Carnegie Mellon University's Sphinx Open Source Recognition and (2) Google®Speech Recognition. While (1) uses generative models of speech recognition, (2) uses deep neural networks. As expected, while (2) achieved lower word error rates (WER) on dysarthric speech than (1), control speech had a WER 59% lower than dysarthric speech. Future studies should be focused not only on making ASRs robust to environmental noise, but also more robust to different voices",
    "checked": true,
    "id": "0fdb6e1bece1ae627ed3d112b62f099daeb36ff2",
    "semantic_title": "whistle-blowing asrs: evaluating the need for more inclusive speech recognition systems",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vachhani18_interspeech.html": {
    "title": "Data Augmentation Using Healthy Speech for Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "Dysarthria refers to a speech disorder caused by trauma to the brain areas concerned with motor aspects of speech giving rise to effortful, slow, slurred or prosodically abnormal speech. Traditional Automatic Speech Recognizers (ASR) perform poorly on dysarthric speech recognition tasks, owing mostly to insufficient dysarthric speech data. Speaker related challenges complicates data collection process for dysarthric speech. In this paper, we explore data augmentation using temporal and speed modifications of healthy speech to simulate dysarthric speech. DNN-HMM based Automatic Speech Recognition (ASR) and Random Forest based classification were used for evaluation of the proposed method. Dysarthric speech generated synthetically is classified for severity using a Random Forest classifier that is trained on actual dysarthric speech. ASR trained on healthy speech augmented with simulated dysarthric speech is evaluated for dysarthric speech recognition. All evaluations were carried out using Universal Access dysarthric speech corpus. An absolute improvement of 4.24% and 2% was achieved using tempo based and speed based data augmentation respectively as compared to ASR performance using healthy speech alone for training",
    "checked": true,
    "id": "e98ea9dc73bf87e5509e987addf56b7006593ad7",
    "semantic_title": "data augmentation using healthy speech for dysarthric speech recognition",
    "citation_count": 61
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ding18b_interspeech.html": {
    "title": "Improving Sparse Representations in Exemplar-Based Voice Conversion with a Phoneme-Selective Objective Function",
    "volume": "main",
    "abstract": "The acoustic quality of exemplar-based voice conversion (VC) degrades whenever the phoneme labels of the selected exemplars do not match the phonetic content of the frame being represented. To address this issue, we propose a Phoneme-Selective Objective Function (PSOF) that promotes a sparse representation of each speech frame with exemplars from a few phoneme classes. Namely, PSOF enforces group sparsity on the representation, where each group corresponds to a phoneme class. The sparse representation for exemplars within a phoneme class tends to activate or suppress simultaneously using the proposed objective function. We conducted two sets of experiments on the ARCTIC corpus to evaluate the proposed method. First, we evaluated the ability of PSOF to reduce phoneme mismatches. Then, we assessed its performance on a VC task and compared it against three baseline methods from previous studies. Results from objective measurements and subjective listening tests show that the proposed method effectively reduces phoneme mismatches and significantly improves VC acoustic quality while retaining the voice identity of the target speaker",
    "checked": true,
    "id": "aa2f467408c57f9105472e0d7bd6ba9d28cc91a4",
    "semantic_title": "improving sparse representations in exemplar-based voice conversion with a phoneme-selective objective function",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ding18c_interspeech.html": {
    "title": "Learning Structured Dictionaries for Exemplar-based Voice Conversion",
    "volume": "main",
    "abstract": "Incorporating phonetic information has been shown to improve the performance of exemplar-based voice conversion. A standard approach is to build a phonetically structured dictionary, where exemplars are categorized into sub-dictionaries according to their phoneme labels. However, acquiring phoneme labels can be expensive and the phoneme labels can have inaccuracies. The latter problem becomes more salient when the speakers are non-native speakers. This paper presents an iterative dictionary-learning algorithm that avoids the need for phoneme labels and instead learns the structured dictionaries in an unsupervised fashion. At each iteration, two steps are alternatively performed: cluster update and dictionary update. In the cluster update step, each training frame is assigned to a cluster whose sub-dictionary represents it with the lowest residual. In the dictionary update step, the sub-dictionary for a cluster is updated using all the speech frames in the cluster. We evaluate the proposed algorithm through objective and subjective experiments on a new corpus of non-native English speech. Compared to previous studies, the proposed algorithm improves the acoustic quality of voice-converted speech while retaining the target speaker's identity",
    "checked": true,
    "id": "b86a258a6bc54be04cd7981768af950328c763ed",
    "semantic_title": "learning structured dictionaries for exemplar-based voice conversion",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/peng18_interspeech.html": {
    "title": "Exemplar-Based Spectral Detail Compensation for Voice Conversion",
    "volume": "main",
    "abstract": "Most voice conversion (VC) systems are established under the vocoder-based VC framework. When performing spectral conversion (SC) under this framework, the low-dimensional spectral features, such as mel-ceptral coefficients (MCCs), are often adopted to represent the high-dimensional spectral envelopes. The joint density Gaussian mixture model (GMM)-based SC method with the STRAIGHT vocoder is a well-known representative. Although it is reasonably effective, the loss of spectral details in the converted spectral envelopes inevitably deteriorates speech quality and similarity. To overcome this problem, we propose a novel exemplar-based spectral detail compensation method for VC. In the offline stage, the paired dictionaries of source spectral envelopes and target spectral details are constructed. In the online stage, the locally linear embedding (LLE) algorithm is applied to predict the target spectral details from the source spectral envelopes and then, the predicted spectral details are used to compensate the converted spectral envelopes obtained by a baseline GMM-based SC method with the STRAIGHT vocoder. Experimental results show that the proposed method can notably improve the baseline system in terms of objective and subjective tests",
    "checked": true,
    "id": "05c15d7710d6cff56680e3f04feb2db8e3c0c640",
    "semantic_title": "exemplar-based spectral detail compensation for voice conversion",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/meenakshi18_interspeech.html": {
    "title": "Whispered Speech to Neutral Speech Conversion Using Bidirectional LSTMs",
    "volume": "main",
    "abstract": "We propose a bidirectional long short-term memory (BLSTM) based whispered speech to neutral speech conversion system that employs the STRAIGHT speech synthesizer. We use a BLSTM to map the spectral features of whispered speech to those of neutral speech. Three other BLSTMs are employed to predict the pitch, periodicity levels and the voiced/unvoiced phoneme decisions from the spectral features of whispered speech. We use objective measures to quantify the quality of the predicted spectral features and excitation parameters, using data recorded from six subjects, in a four fold setup. We find that the temporal smoothness of the spectral features predicted using the proposed BLSTM based system is statistically more compared to that predicted using deep neural network based baseline schemes. We also observe that while the performance of the proposed system is comparable to the baseline scheme for pitch prediction, it is superior in terms of classifying voicing decisions and predicting periodicity levels. From subjective evaluation via listening test, we find that the proposed method is chosen as the best performing scheme 26.61% (absolute) more often than the best baseline scheme. This reveals that the proposed method yields a more natural sounding neutral speech from whispered speech",
    "checked": true,
    "id": "4d918bb31e3b63829ae7a28468bfa6a288544cf6",
    "semantic_title": "whispered speech to neutral speech conversion using bidirectional lstms",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18d_interspeech.html": {
    "title": "Voice Conversion Across Arbitrary Speakers Based on a Single Target-Speaker Utterance",
    "volume": "main",
    "abstract": "Developing a voice conversion (VC) system for a particular speaker typically requires considerable data from both the source and target speakers. This paper aims to effectuate VC across arbitrary speakers, which we call any-to-any VC, with only a single target-speaker utterance. Two systems are studied: (1) the i-vector-based VC (IVC) system and (2) the speaker-encoder-based VC (SEVC) system. Phonetic PosteriorGrams are adopted as speaker-independent linguistic features extracted from speech samples. Both systems train a multi-speaker deep bidirectional long-short term memory (DBLSTM) VC model, taking in additional inputs that encode speaker identities, in order to generate the outputs. In the IVC system, the speaker identity of a new target speaker is represented by i-vectors. In the SEVC system, the speaker identity is represented by speaker embedding predicted from a separately trained model. Experiments verify the effectiveness of both systems in achieving VC based only on a single target-speaker utterance. Furthermore, the IVC approach is superior to SEVC, in terms of the quality of the converted speech and its similarity to the utterance produced by the genuine target speaker",
    "checked": true,
    "id": "d1e16c12c33aef26cd71fa6f27a9b615c60a7e41",
    "semantic_title": "voice conversion across arbitrary speakers based on a single target-speaker utterance",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chou18_interspeech.html": {
    "title": "Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations",
    "volume": "main",
    "abstract": "Recently, cycle-consistent adversarial network (Cycle-GAN) has been successfully applied to voice conversion to a different speaker without parallel data, although in those approaches an individual model is needed for each target speaker. In this paper, we propose an adversarial learning framework for voice conversion, with which a single model can be trained to convert the voice to many different speakers, all without parallel data, by separating the speaker characteristics from the linguistic content in speech signals. An autoencoder is first trained to extract speaker-independent latent representations and speaker embedding separately using another auxiliary speaker classifier to regularize the latent representation. The decoder then takes the speaker-independent latent representation and the target speaker embedding as the input to generate the voice of the target speaker with the linguistic content of the source utterance. The quality of decoder output is further improved by patching with the residual signal produced by another pair of generator and discriminator. A target speaker set size of 20 was tested in the preliminary experiments and very good voice quality was obtained. Conventional voice conversion metrics are reported. We also show that the speaker information has been properly reduced from the latent representations",
    "checked": true,
    "id": "c04051cef693c9a41269f425a88f769599745d04",
    "semantic_title": "multi-target voice conversion without parallel data by adversarially learning disentangled audio representations",
    "citation_count": 120
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gorrostieta18_interspeech.html": {
    "title": "Attention-based Sequence Classification for Affect Detection",
    "volume": "main",
    "abstract": "This paper presents the Cogito submission to the Interspeech Computational Paralinguistics Challenge (ComParE), for the second sub-challenge. The aim of this second sub-challenge is to recognize self-assessed affect from short clips of speech-containing audio data. We adopt a sequence classification-based approach where we use a long-short term memory (LSTM) network for modeling the evolution of low-level spectral coefficients, with added attention mechanism to emphasize salient regions of the audio clip. Additionally to deal with the underrepresentation of the negative valence class we use a combination of mitigation strategies including oversampling and loss function weighting. Our experiments demonstrate improvements in detection accuracy when including the attention mechanism and class balancing strategies in combination, with the best models outperforming the best single challenge baseline model",
    "checked": true,
    "id": "023e96e57a26a801f5a8d5eed6ab53f0d0d190d9",
    "semantic_title": "attention-based sequence classification for affect detection",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2018/syed18_interspeech.html": {
    "title": "Computational Paralinguistics: Automatic Assessment of Emotions, Mood and Behavioural State from Acoustics of Speech",
    "volume": "main",
    "abstract": "Paralinguistic analysis of speech remains a challenging task due to the many confounding factors which affect speech production. In this paper, we address the Interspeech 2018 Computational Paralinguistics Challenge (ComParE) which aims to push the boundaries of sensitivity to non-textual information that is conveyed in the acoustics of speech. We attack the problem on several fronts. We posit that a substantial amount of paralinguistic information is contained in spectral features alone. To this end, we use a large ensemble of Extreme Learning Machines for classification of spectral features. We further investigate the applicability of (an ensemble of) CNN-GRUs networks to model temporal variations therein. We report on the details of the experiments and the results for three ComParE sub-challenges: Atypical Affect, Self-Assessed Affect and Crying. Our results compare favourably and in some cases exceed the published state-of-the-art performance",
    "checked": true,
    "id": "d5df475bc3b37bc66e1bf16d0c6a17c9060cfe1a",
    "semantic_title": "computational paralinguistics: automatic assessment of emotions, mood and behavioural state from acoustics of speech",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rallabandi18_interspeech.html": {
    "title": "Investigating Utterance Level Representations for Detecting Intent from Acoustics",
    "volume": "main",
    "abstract": "Recognizing paralinguistic cues from speech has applications in varied domains of speech processing. In this paper we present approaches to identify the expressed intent from acoustics in the context of INTERSPEECH 2018 ComParE challenge. We have made submissions in three sub-challenges: prediction of 1) self-assessed affect and 2) atypical affect 3) Crying Sub challenge. Since emotion and intent are perceived at suprasegmental levels, we explore a variety of utterance level embeddings. The work includes experiments with both automatically derived as well as knowledge-inspired features that capture spoken intent at various acoustic levels. Incorporation of utterance level embeddings at the text level using an off the shelf phone decoder has also been investigated. The experiments impose constraints and manipulate the training procedure using heuristics from the data distribution. We conclude by presenting the preliminary results on the development and blind test sets",
    "checked": true,
    "id": "403195c53f82f3c499a9b096dd1cf0735d1063d6",
    "semantic_title": "investigating utterance level representations for detecting intent from acoustics",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kaya18_interspeech.html": {
    "title": "LSTM Based Cross-corpus and Cross-task Acoustic Emotion Recognition",
    "volume": "main",
    "abstract": "Acoustic emotion recognition is a popular and central research direction in paralinguistic analysis, due its relation to a wide range of affective states/traits and manifold applications. Developing highly generalizable models still remains as a challenge for researchers and engineers, because of multitude of nuisance factors. To assert generalization, deployed models need to handle spontaneous speech recorded under different acoustic conditions compared to the training set. This requires that the models are tested for cross-corpus robustness. In this work, we first investigate the suitability of Long-Short-Term-Memory (LSTM) models trained with time- and space-continuously annotated affective primitives for cross-corpus acoustic emotion recognition. We next employ an effective approach to use the frame level valence and arousal predictions of LSTM models for utterance level affect classification and apply this approach on the ComParE 2018 challenge corpora. The proposed method alone gives motivating results both on development and test set of the Self-Assessed Affect Sub-Challenge. On the development set, the cross-corpus prediction based method gives a boost to performance when fused with top components of the baseline system. Results indicate the suitability of the proposed method for both time-continuous and utterance level cross-corpus acoustic emotion recognition tasks",
    "checked": true,
    "id": "d35701317664851f2bf50b7bd60bd38234e1b047",
    "semantic_title": "lstm based cross-corpus and cross-task acoustic emotion recognition",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vlasenko18_interspeech.html": {
    "title": "Implementing Fusion Techniques for the Classification of Paralinguistic Information",
    "volume": "main",
    "abstract": "This work tests several classification techniques and acoustic features and further combines them using late fusion to classify paralinguistic information for the ComParE 2018 challenge. We use Multiple Linear Regression (MLR) with Ordinary Least Squares (OLS) analysis to select the most informative features for Self-Assessed Affect (SSA) sub-Challenge. We also propose to use raw-waveform convolutional neural networks (CNN) in the context of three paralinguistic sub-challenges. By using combined evaluation split for estimating codebook, we obtain better representation for Bag-of-Audio-Words approach. We preprocess the speech to vocalized segments to improve classification performance. For fusion of our leading classification techniques, we use weighted late fusion approach applied for confidence scores. We use two mismatched evaluation phases by exchanging the training and development sets and this estimates the optimal fusion weight. Weighted late fusion provides better performance on development sets in comparison with baseline techniques. Raw-waveform techniques perform comparable to the baseline",
    "checked": true,
    "id": "89bcf386733d68976c90d70299b3cb08362e8ff9",
    "semantic_title": "implementing fusion techniques for the classification of paralinguistic information",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gosztolya18_interspeech.html": {
    "title": "General Utterance-Level Feature Extraction for Classifying Crying Sounds, Atypical & Self-Assessed Affect and Heart Beats",
    "volume": "main",
    "abstract": "In the area of computational paralinguistics, there is a growing need for general techniques that can be applied in a variety of tasks and which can be easily realized using standard and publicly available tools. In our contribution to the 2018 Interspeech Computational Paralinguistic Challenge (ComParE), we test four general ways of extracting features. Besides the standard ComParE feature set consisting of 6373 diverse attributes, we experiment with two variations of Bag-of-Audio-Words representations, and define a simple feature set inspired by Gaussian Mixture Models. Our results indicate that the UAR scores obtained via the different approaches vary among the tasks. In our view, this is mainly because most feature sets tested were local by nature and they could not properly represent the utterances of the Atypical Affect and Self-Assessed Affect Sub- Challenges. On the Crying Sub-Challenge, however, a simple combination of all four feature sets proved to be effective",
    "checked": true,
    "id": "8458d17eba5d9586acdcce720e159ee633c235da",
    "semantic_title": "general utterance-level feature extraction for classifying crying sounds, atypical & self-assessed affect and heart beats",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/su18d_interspeech.html": {
    "title": "Self-Assessed Affect Recognition Using Fusion of Attentional BLSTM and Static Acoustic Features",
    "volume": "main",
    "abstract": "In this study, we present a computational framework to participate in the Self-Assessed Affect Sub-Challenge in the INTERSPEECH 2018 Computation Paralinguistics Challenge. The goal of this sub-challenge is to classify the valence scores given by the speaker themselves into three different levels, i.e., low, medium and high. We explore fusion of Bi-directional LSTM with baseline SVM models to improve the recognition accuracy. In specifics, we extract frame-level acoustic LLDs as input to the BLSTM with a modified attention mechanism and separate SVMs are trained using the standard ComParE_16 baseline feature sets with minority class upsampling. These diverse prediction results are then further fused using a decision-level score fusion scheme to integrate all of the developed models. Our proposed approach achieves a 62.94% and 67.04% unweighted average recall (UAR), which is an 6.24% and 1.04% absolute improvement over the best baseline provided by the challenge organizer. We further provide a detailed comparison analysis between different models",
    "checked": true,
    "id": "5ceb28f9769b5af8086067b22d71dbb743cc7c13",
    "semantic_title": "self-assessed affect recognition using fusion of attentional blstm and static acoustic features",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/montacie18_interspeech.html": {
    "title": "Vocalic, Lexical and Prosodic Cues for the INTERSPEECH 2018 Self-Assessed Affect Challenge",
    "volume": "main",
    "abstract": "The INTERSPEECH 2018 Self-Assessed Affect Challenge consists in the prediction of the affective state of mind from speech. Experiments were conducted on the Ulm State-of-Mind in Speech database (USoMS) where subjects self-report their affective state. Dimensional representation of emotion (valence) is used for labeling. We have investigated cues related to the perception of the emotional valence according to three main relevant linguistic levels: phonetics, lexical and prosodic. For this purpose we studied: the degree-of-articulation, the voice quality, an affect lexicon and the expressive prosodic contours. For the phonetics level, a set of gender-dependent audio-features was computed on vowel analysis (voice quality and speech articulation measurements). At the lexical level, an affect lexicon was extracted from the automatic transcription of the USoMS database. This lexicon has been assessed for the Challenge task comparatively to a reference polarity lexicon. In order to detect expressive prosody, N-gram models of the prosodic contours were computed from an intonation labeling system. At last, an emotional valence classifier was designed combining ComParE and eGeMAPS feature sets with other phonetic, prosodic and lexical features. Experiments have shown an improvement of 2.4% on the Test set, compared to the baseline performance of the Challenge",
    "checked": true,
    "id": "e9fa2037c6c2b757d0e438854ebac25402f1f871",
    "semantic_title": "vocalic, lexical and prosodic cues for the interspeech 2018 self-assessed affect challenge",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pa18_interspeech.html": {
    "title": "Intonation tutor by SPIRE (In-SPIRE): An Online Tool for an Automatic Feedback to the Second Language Learners in Learning Intonation",
    "volume": "main",
    "abstract": "In spoken communication, intonation often conveys meaning of an utterance. Thus, incorrect intonation, typically made by second language (L2) learners, could result in miscommunication. We demonstrate In-SPIRE tool that helps the L2 learners to learn intonation in a self-learning manner. For this, we design an interactive self-explanatory front end, which is also used to send learner`s audio and hand-shake signals to the back-end. At the back-end, we implement a system that takes the learner`s audio against a specific stimuli and computes pitch patterns representing the intonation. For this, we apply pitch stylization on each syllable segment in the audio. Further, we compute a quality score using the learner`s patterns and the respective ground-truth patterns. Finally, the score, the patterns of the learners and the ground-truth are sent to the front-end for display as a feedback to the learners. Thus, the learner could correct any mismatch in his/her intonation with respect to the ground-truth. The proposed tool benefits the learners who do not have access to effective spoken language training",
    "checked": true,
    "id": "26ff4677371592e9364de855622de25a8afef2dd",
    "semantic_title": "intonation tutor by spire (in-spire): an online tool for an automatic feedback to the second language learners in learning intonation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/evanini18b_interspeech.html": {
    "title": "Game-based Spoken Dialog Language Learning Applications for Young Students",
    "volume": "main",
    "abstract": "This demo presents three different spoken dialog applications that were developed to provide young learners of English an opportunity to practice speaking and to receive feedback on particular aspects of their English speaking ability. The speaking tasks were designed as game-based interactions in order to engage young students and they provide feedback about grammar yes/no question formation and simple past tense verb formation) and vocabulary. A pilot study with 27 primary-level English as a foreign language (EFL) learners investigated the usefulness of these applications",
    "checked": true,
    "id": "98a47a5ce498ff6fda50e37031d1a8cd02284d9a",
    "semantic_title": "game-based spoken dialog language learning applications for young students",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sorin18_interspeech.html": {
    "title": "The IBM Virtual Voice Creator",
    "volume": "main",
    "abstract": "The IBM Virtual Voice Creator (IVVC) is an end-to-end cloud-based solution for TTS voice customization and voiceover generation in games and animated movies. The solution is based on the IBM expressive TTS technology with built-in online voice transformation capabilities. It is endowed with an interactive web GUI studio. IVVC lets the users create unique voice personas according to their needs and imagination and control the vocal performance of the virtual speakers. IVVC provides a powerful set of controls over the voice characteristics, including the vocal tract, glottal pulse, breathiness, pitch, rate and special voice effects. IVVC also allows the user to control emotional style and emphasis in the synthesized speech. The virtual voice design and performance controls are interactive, intuitive, fast and do not require any special skills",
    "checked": true,
    "id": "673e160743f91ad6e14616f39d103856c0872c52",
    "semantic_title": "the ibm virtual voice creator",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/g18_interspeech.html": {
    "title": "Mobile Application for Learning Languages for the Unlettered",
    "volume": "main",
    "abstract": "Mobile based technologies have become ubiquitous and various applications from games to readers are mobile oriented. In this paper, we propose development of speech based language learning app. Conventionally language learning tools start with words, followed by sentences. The fundamental assumption is that the person is literate. In the Indian context, the literacy levels are as low as 65%. In addition, each Indian language has its own scripts. The objective of this work is to develop a mobile app that starts from the script to teach the language to a person who can speak the language but is unlettered. Since the focus is on the unlettered, writing should be easy. A script centric approach is used to learn a language. The application starts with teaching a simple letter of the alphabet, followed by another letter that can be obtained by simple modification to the previously learned letter, followed by words using the letters that are learned, followed by sentences using the learned words. At every step, a text-to-speech system is used which articulates the letters and words. The learning app is based on a book called Tamil Karpom (P Nannan). The ideas from the book are adapted for learning Hindi",
    "checked": true,
    "id": "1bdf04e121c80c2be46182be4ab51f43acf8ecbf",
    "semantic_title": "mobile application for learning languages for the unlettered",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xu18d_interspeech.html": {
    "title": "Mandarin-English Code-switching Speech Recognition",
    "volume": "main",
    "abstract": "This work presents the development of a Mandarin-English code-switching speech recognition system. We demonstrate three key novelties in our system. First, we increase our lexicon coverage to 360K words, where phone sets of different languages are maintained separately. Secondly, we used over 1000 hours of training data combining both mono-lingual and code-switch corpus to develop the acoustic model. Finally, for language modelling, we applied context-aware text normalization and word-class language model. When testing on our internal code-switch close talk microphone recording, the system achieves recognition performance that can support real applications",
    "checked": true,
    "id": "e01e83530a082815bd55f88088c4e826801ded5c",
    "semantic_title": "mandarin-english code-switching speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kim18c_interspeech.html": {
    "title": "Joint Learning of Domain Classification and Out-of-Domain Detection with Dynamic Class Weighting for Satisficing False Acceptance Rates",
    "volume": "main",
    "abstract": "In domain classification for spoken dialog systems, correct detection of out-of-domain (OOD) utterances is crucial because it reduces confusion and unnecessary interaction costs between users and the systems. Previous work usually utilizes OOD detectors that are trained separately from in-domain (IND) classifiers and confidence thresholding for OOD detection given target evaluation scores. In this paper, we introduce a neural joint learning model for domain classification and OOD detection, where dynamic class weighting is used during the model training to satisfice a given OOD false acceptance rate (FAR) while maximizing the domain classification accuracy. Evaluating on two domain classification tasks for the utterances from a large spoken dialogue system, we show that our approach significantly improves the domain classification performance with satisficing given target FARs",
    "checked": true,
    "id": "799ebfe2c0a645cf50b024823a4f14b7b3501a7e",
    "semantic_title": "joint learning of domain classification and out-of-domain detection with dynamic class weighting for satisficing false acceptance rates",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mukherjee18_interspeech.html": {
    "title": "Analyzing Vocal Tract Movements During Speech Accommodation",
    "volume": "main",
    "abstract": "When two people engage in verbal interaction, they tend to accommodate on a variety of linguistic levels. Although recent attention has focused on to the acoustic characteristics of convergence in speech, the underlying articulatory mechanisms remain to be explored. Using 3D electromagnetic articulography (EMA), we simultaneously recorded articulatory movements in two speakers engaged in an interactive verbal game, the domino task. In this task, the two speakers take turn in chaining bi-syllabic words according to a rhyming rule. By using a robust speaker identification strategy, we identified for which specific words speakers converged or diverged. Then, we explored the different vocal tract features characterizing speech accommodation. Our results suggest that tongue movements tend to slow down during convergence whereas maximal jaw opening during convergence and divergence differs depending on syllable position",
    "checked": true,
    "id": "8898e55e911a63c9572f05e99d2413e5623dafa8",
    "semantic_title": "analyzing vocal tract movements during speech accommodation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18_interspeech.html": {
    "title": "Cross-Lingual Multi-Task Neural Architecture for Spoken Language Understanding",
    "volume": "main",
    "abstract": "Cross-lingual spoken language understanding (SLU) systems traditionally require machine translation services for language portability and liberation from human supervision. However, restriction exists in parallel corpora and model architectures. Assuming reliable data are provided with human-supervision, which encourages non-parallel corpora and alleviate translation errors, this paper aims to explore cross-lingual knowledge transfer from multiple levels by taking advantage of neural architectures. We first investigate a joint model of slot filling and intent determination for SLU, which alleviates the out-of-vocabulary problem and explicitly models dependencies between output labels by combining character and word representations, bidirectional Long Short-Term Memory and conditional random fields together, while attention-based classifier is introduced for intent determination. Knowledge transfer is further operated on character-level and sequence-level, aiming to share morphological and phonological information between languages with similar alphabets by sharing character representations and characterize the sequence with language-general and language-specific knowledge adaptively acquired by separate encoders. Experimental results on the MIT-Restaurant-Corpus and the ATIS corpora in different languages demonstrate the effectiveness of the proposed methods",
    "checked": true,
    "id": "9fd1ec7261d872ad5748ee16a9b024af57559874",
    "semantic_title": "cross-lingual multi-task neural architecture for spoken language understanding",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/strimel18_interspeech.html": {
    "title": "Statistical Model Compression for Small-Footprint Natural Language Understanding",
    "volume": "main",
    "abstract": "In this paper we investigate statistical model compression applied to natural language understanding (NLU) models. Small-footprint NLU models are important for enabling offline systems on hardware restricted devices and for decreasing on-demand model loading latency in cloud-based systems. To compress NLU models, we present two main techniques, parameter quantization and perfect feature hashing. These techniques are complementary to existing model pruning strategies such as L1 regularization. We performed experiments on a large scale NLU system. The results show that our approach achieves 14-fold reduction in memory usage compared to the original models with minimal predictive performance impact",
    "checked": true,
    "id": "271ec0e36a058e70fd5fdd9c5ea05d4cf2bd6a6d",
    "semantic_title": "statistical model compression for small-footprint natural language understanding",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2018/braunschweiler18_interspeech.html": {
    "title": "Comparison of an End-to-end Trainable Dialogue System with a Modular Statistical Dialogue System",
    "volume": "main",
    "abstract": "This paper presents a comparison of two dialogue systems: one is end-to-end trainable and the other uses a more traditional, modular architecture. End-to-end trainable dialogue systems recently attracted a lot of attention because they offer several advantages over traditional systems. One of them is the avoidance to train each system module independently, by creating a single network architecture which maps an input to the corresponding output without the need for intermediate representations. While the end-to-end system investigated here had been tested in a text-in/out scenario it remained an open question how the system would perform in a speech-in/out scenario, with noisy input from a speech recognizer and output speech generated by a speech synthesizer. To evaluate this, both dialogue systems were trained on the same corpus, including human-human dialogues in the Cambridge restaurant domain, and then compared in both scenarios by human evaluation. The results show, that in both interfaces the end-to-end system receives significantly higher ratings on all metrics than the traditional modular system, an indication that it enables users to reach their goals faster and experience both a more natural system response and a better comprehension by the dialogue system",
    "checked": true,
    "id": "621eb0d40a497d0b229b7d1b63ba3d1bed330ec3",
    "semantic_title": "comparison of an end-to-end trainable dialogue system with a modular statistical dialogue system",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/willi18_interspeech.html": {
    "title": "A Discriminative Acoustic-Prosodic Approach for Measuring Local Entrainment",
    "volume": "main",
    "abstract": "Acoustic-prosodic entrainment describes the tendency of humans to align or adapt their speech acoustics to each other in conversation. This alignment of spoken behavior has important implications for conversational success. However, modeling the subtle nature of entrainment in spoken dialogue continues to pose a challenge. In this paper, we propose a straightforward definition for local entrainment in the speech domain and operationalize an algorithm based on this: acoustic-prosodic features that capture entrainment should be maximally different between real conversations involving two partners and sham conversations generated by randomly mixing the speaking turns from the original two conversational partners. We propose an approach for measuring local entrainment that quantifies alignment of behavior on a turn-by-turn basis, projecting the differences between interlocutors' acoustic-prosodic features for a given turn onto a discriminative feature subspace that maximizes the difference between real and sham conversations. We evaluate the method using the derived features to drive a classifier aiming to predict an objective measure of conversational success (i.e., low versus high), on a corpus of task-oriented conversations. The proposed entrainment approach achieves 72% classification accuracy using a Naive Bayes classifier, outperforming three previously established approaches evaluated on the same conversational corpus",
    "checked": true,
    "id": "dda8cfdce08b83dde3abebb388adc443aaa3f74f",
    "semantic_title": "a discriminative acoustic-prosodic approach for measuring local entrainment",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/roddy18_interspeech.html": {
    "title": "Investigating Speech Features for Continuous Turn-Taking Prediction Using LSTMs",
    "volume": "main",
    "abstract": "For spoken dialog systems to conduct fluid conversational interactions with users, the systems must be sensitive to turn-taking cues produced by a user. Models should be designed so that effective decisions can be made as to when it is appropriate, or not, for the system to speak. Traditional end-of-turn models, where decisions are made at utterance end-points, are limited in their ability to model fast turn-switches and overlap. A more flexible approach is to model turn-taking in a continuous manner using RNNs, where the system predicts speech probability scores for discrete frames within a future window. The continuous predictions represent generalized turn-taking behaviors observed in the training data and can be applied to make decisions that are not just limited to end-of-turn detection. In this paper, we investigate optimal speech-related feature sets for making predictions at pauses and overlaps in conversation. We find that while traditional acoustic features perform well, part-of-speech features generally perform worse than word features. We show that our current models outperform previously reported baselines",
    "checked": true,
    "id": "ede28e0b075c294c5d66a75711c960c84f2fd234",
    "semantic_title": "investigating speech features for continuous turn-taking prediction using lstms",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kraljevski18_interspeech.html": {
    "title": "Classification of Correction Turns in Multilingual Dialogue Corpus",
    "volume": "main",
    "abstract": "This paper presents a multiclass classification of correction dialog turns using machine learning. The classes are determined by the type of the introduced recognition errors while performing WOz trials and creating the multilingual corpus. Three datasets were obtained using different sets of acoustic-prosodic features on the multilingual dialogue corpus. The classification experiments were done using different machine learning paradigms: Decision Trees, Support Vector Machines and Deep Learning. After careful experiments setup and optimization on the hyper-parameter space, the obtained classification results were analyzed and compared in the terms of accuracy, precision, recall and F1 score. The achieved results are comparable with those obtained in similar experiments on different tasks and speech databases",
    "checked": true,
    "id": "1abb3adfb4f5676fa72a73cd540ea88099b1e6a6",
    "semantic_title": "classification of correction turns in multilingual dialogue corpus",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/naik18_interspeech.html": {
    "title": "Contextual Slot Carryover for Disparate Schemas",
    "volume": "main",
    "abstract": "In the slot-filling paradigm, where a user can refer back to slots in the context during the conversation, the goal of the contextual understanding system is to resolve the referring expressions to the appropriate slots in the context. In large-scale multi-domain systems, this presents two challenges - scaling to a very large and potentially unbounded set of slot values and dealing with diverse schemas. We present a neural network architecture that addresses the slot value scalability challenge by reformulating the contextual interpretation as a decision to carryover a slot from a set of possible candidates. To deal with heterogenous schemas, we introduce a simple data-driven method for transforming the candidate slots. Our experiments show that our approach can scale to multiple domains and provides competitive results over a strong baseline",
    "checked": true,
    "id": "a1f7f3604870fb9797973fbb8109abe21df8ecea",
    "semantic_title": "contextual slot carryover for disparate schemas",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2018/renkens18_interspeech.html": {
    "title": "Capsule Networks for Low Resource Spoken Language Understanding",
    "volume": "main",
    "abstract": "Designing a spoken language understanding system for command-and-control applications can be challenging because of a wide variety of domains and users or because of a lack of training data. In this paper we discuss a system that learns from scratch from user demonstrations. This method has the advantage that the same system can be used for many domains and users without modifications and that no training data is required prior to deployment. The user is required to train the system, so for a user friendly experience it is crucial to minimize the required amount of data. In this paper we investigate whether a capsule network can make efficient use of the limited amount of available training data. We compare the proposed model to an approach based on Non-negative Matrix Factorisation which is the state-of-the-art in this setting and another deep learning approach that was recently introduced for end-to-end spoken language understanding. We show that the proposed model outperforms the baseline models for three command-and-control applications: controlling a small robot, a vocally guided card game and a home automation task",
    "checked": true,
    "id": "2ca546b0e2aac4578df293ce35af4b1164e11a2a",
    "semantic_title": "capsule networks for low resource spoken language understanding",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2018/padmasundari18_interspeech.html": {
    "title": "Intent Discovery Through Unsupervised Semantic Text Clustering",
    "volume": "main",
    "abstract": "Conversational systems need to understand spoken language to be able to converse with a human in a meaningful coherent manner. This understanding (Spoken Language understanding - SLU) of the human language is operationalized through identifying intents and entities. While classification methods that rely on labeled data are often used for SLU, creating large supervised data sets is extremely tedious and time consuming. This paper presents a practical approach to automate the process of intent discovery on unlabeled data sets of human language text through clustering techniques. We explore a range of representations for the texts and various clustering methods to validate the clustering stability through quantitative metrics like Adjusted Random Index (ARI). A final alignment of the clusters to the semantic intent is determined through consensus labelling. Our experiments on public datasets demonstrate the effectiveness of our approach generating homogeneous clusters with 89% cluster accuracy, leading to better semantic intent alignments. Furthermore, we illustrate that the clustering offer an alternate and effective way to mine sentence variants that can aid the bootstrapping of SLU models",
    "checked": true,
    "id": "29a6ecd69146a65463fbd622ca164f2569ab63cd",
    "semantic_title": "intent discovery through unsupervised semantic text clustering",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2018/du18b_interspeech.html": {
    "title": "Multimodal Polynomial Fusion for Detecting Driver Distraction",
    "volume": "main",
    "abstract": "Distracted driving is deadly, claiming 3,477 lives in the U.S. in 2015 alone. Although there has been a considerable amount of research on modeling the distracted behavior of drivers under various conditions, accurate automatic detection using multiple modalities and especially the contribution of using the speech modality to improve accuracy has received little attention. This paper introduces a new multimodal dataset for distracted driving behavior and discusses automatic distraction detection using features from three modalities: facial expression, speech and car signals. Detailed multimodal feature analysis shows that adding more modalities monotonically increases the predictive accuracy of the model. Finally, a simple and effective multimodal fusion technique using a polynomial fusion layer shows superior distraction detection results compared to the baseline SVM and neural network models",
    "checked": true,
    "id": "92fc125693604de5ec0542953e17aec85fe1be11",
    "semantic_title": "multimodal polynomial fusion for detecting driver distraction",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2018/inoue18b_interspeech.html": {
    "title": "Engagement Recognition in Spoken Dialogue via Neural Network by Aggregating Different Annotators' Models",
    "volume": "main",
    "abstract": "This paper addresses engagement recognition based on four multimodal listener behaviors - backchannels, laughing, eye-gaze and head nodding. Engagement is an indicator of how much a user is interested in the current dialogue. Multiple third-party annotators give ground truth labels of engagement in a human-robot interaction corpus. Since perception of engagement is subjective, the annotations are sometimes different between individual annotators. Conventional methods directly use integrated labels, such as those generated through simple majority voting and do not consider each annotator's recognition. We propose a two-step engagement recognition where each annotator's recognition is modeled and the different annotators' models are aggregated to recognize the integrated label. The proposed neural network consists of two parts. The first part corresponds to each annotator's model which is trained with the corresponding labels independently. The second part aggregates the different annotators' models to obtain one integrated label. After each part is pre-trained, the whole network is fine-tuned through back-propagation of prediction errors. Experimental results show that the proposed network outperforms baseline models which directly recognize the integrated label without considering differing annotations",
    "checked": true,
    "id": "aef93f38419b6ed605130298106509c76349cf97",
    "semantic_title": "engagement recognition in spoken dialogue via neural network by aggregating different annotators' models",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2018/buanzur18_interspeech.html": {
    "title": "A First Investigation of the Timing of Turn-taking in Ruuli",
    "volume": "main",
    "abstract": "Turn-taking behavior in conversation is reported to be universal among cultures, although the language-specific means used to accomplish smooth turn-taking are likely to differ. Previous studies investigating turn-taking have primarily focused on languages which are already heavily-studied. The current work investigates the timing of turn-taking in question-response sequences in naturalistic conversations in Ruuli, an under-studied Bantu language spoken in Uganda. We extracted sequences involving wh-questions and polar questions and measured the duration of the gap or overlap between questions and their following responses, additionally differentiating between different response types such as affirmative (i.e. type-conforming) or negative (i.e. non-type-conforming) responses to polar questions. We find that the timing of responses to various question types in Ruuli is consistent with timings that have been reported for a variety of other languages, with a mean gap duration between questions and responses of around 259 ms. Our findings thus emphasize the universal nature of turn-taking behavior in human interaction, despite Ruuli's substantial structural differences from languages in which turn-taking has been previously studied",
    "checked": true,
    "id": "89b86b894eaccd66e1faa9bb87edefe6052d21d6",
    "semantic_title": "a first investigation of the timing of turn-taking in ruuli",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhao18_interspeech.html": {
    "title": "Spoofing Detection Using Adaptive Weighting Framework and Clustering Analysis",
    "volume": "main",
    "abstract": "Security of Automatic Speaker Verification (ASV) systems against imposters are now focusing on anti-spoofing countermeasures. Under the severe threat of various speech spoofing techniques, ASV systems can easily be 'fooled' by spoofed speech which sounds as real as human-beings. As two effective solutions, the Constant Q Cepstral Coefficients (CQCC) and the Scattering Cepstral Coefficients (SCC) perform well on the detection of artificial speech signals, especially for attacks from speech synthesis (SS) and voice conversion (VC). However, for spoofing subsets generated by different approaches, a low Equal Error Rate (EER) cannot be maintained. In this paper, an adaptive weighting based standalone detector is proposed to address the selective detection degradation. The clustering property of the genuine and the spoofed subsets are analysed for the selection of suitable weighting factors. With a Gaussian Mixture Model (GMM) classifier as the back-end, the proposed detector is evaluated on the ASVspoof 2015 database. The EERs of 0.01% and 0.20% are obtained on the known and the unknown attacks, respectively. This presents an essential complementation between the CQCC and the SCC and also promotes the future research on generalized countermeasures",
    "checked": true,
    "id": "966427f242401fa5854c8661846640ac8ccab73c",
    "semantic_title": "spoofing detection using adaptive weighting framework and clustering analysis",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jelil18_interspeech.html": {
    "title": "Exploration of Compressed ILPR Features for Replay Attack Detection",
    "volume": "main",
    "abstract": "This paper deals with the problem of detecting replay attacks on speaker verification systems. In literature, apart from the acoustic features, source features have also been successfully used for this task. In existing source features, only the information around glottal closure instants (GCIs) have been utilized. We hypothesize that the feature derived by capturing the temporal dynamics between two GCIs would be more discriminative for such task. Motivated by that, in this work we explore the use of discrete cosine transform compressed integrated linear prediction residual (ILPR) features for discriminating between genuine and replayed signals. A spoof detection system is built using the compressed ILPR feature and a Gaussian mixture model (GMM) classifier. A baseline system is also built using constant-Q cepstral coefficient feature with GMM back-end. These systems are tested on the ASVSpoof 2017 Version 2.0 database. On fusing the systems developed using acoustic and proposed source features an equal error rate of 9.41% is achieved on the evaluation set",
    "checked": true,
    "id": "0397a4837aa9eb6ba407c47077c62ecad7f69dd2",
    "semantic_title": "exploration of compressed ilpr features for replay attack detection",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gunendradasan18_interspeech.html": {
    "title": "Detection of Replay-Spoofing Attacks Using Frequency Modulation Features",
    "volume": "main",
    "abstract": "Prevention of malicious spoofing attacks is currently acknowledged as a priority area of investigation for the deployment of automatic speaker verification systems. Various features of speech signals have been used to fight counterfeit attacks. Among the different spoofing attack variants, replay attacks pose a significant threat as they do not require any expert knowledge and are difficult to detect. This paper proposes the use of a spectral centroid based frequency modulation (FM) features that we term spectral centroid deviation (SCD) for replay attack detection. Spectral centroid frequency (SCF) and spectral centroid magnitude coefficient (SCMC) features extracted from the same front-end as SCD are also investigated as complementary features. Evaluations on the ASVspoof 2017 dataset indicate that the proposed SCD features with a Gaussian Mixture Model (GMM) back-end is highly capable of discriminating genuine from replay spoofed speech, providing an equal error rate improvement greater than 60% relative to the CQCC baseline system from the ASVspoof 2017 challenge. Interestingly, experiments also reveal that the proposed SCD features exhibit an increased variance for replay spoofed speech relative to genuine speech, particularly for the lowest and highest frequency subbands",
    "checked": true,
    "id": "6d24d62eb9bc3dd5b7bd2cbd35703c5387f8308f",
    "semantic_title": "detection of replay-spoofing attacks using frequency modulation features",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kamble18_interspeech.html": {
    "title": "Effectiveness of Speech Demodulation-Based Features for Replay Detection",
    "volume": "main",
    "abstract": "Replay attack presents a great threat to Automatic Speaker Verification (ASV) system. The speech can be modeled as amplitude and frequency modulated (AM-FM) signals. In this paper, we explore speech demodulation-based features using Hilbert transform (HT) and Teager Energy Operator (TEO) for replay detection. In particular, we propose features, namely, HT-based Instantaneous Amplitude (IA) and Instantaneous Frequency (IF) Cosine Coefficients (i.e., HT-IACC and HT-IFCC) and Energy Separation Algorithm (ESA)-based features (i.e., ESA-IACC and ESA-IFCC). For adapting instantaneous energy w.r.t given sampling frequency, ESA requires 3 samples whereas HT requires relatively large number of samples and thus, ESA gives high time resolution.The experiments were performed on ASV spoof 2017 Challenge database for replay spoof speech detection (SSD).The experimental results shows that ESA-based features gave lower EER. In addition, linearly-spaced Gabor filterbank gave lower EER than Butterworth filterbank. To explore possible complementary information using amplitude and frequency, we have used score-level fusion of IA and IF. With HT-based feature set, the score-level fusion gave EER of 5.24% (dev) and 10.03% (eval), whereas ESA-based feature set reduced the EER to 2.01% (dev) and 9.64% (eval)",
    "checked": true,
    "id": "5dee41b72783481a2cb82c3d70b3bdb4abfdd23d",
    "semantic_title": "effectiveness of speech demodulation-based features for replay detection",
    "citation_count": 46
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kamble18b_interspeech.html": {
    "title": "Novel Variable Length Energy Separation Algorithm Using Instantaneous Amplitude Features for Replay Detection",
    "volume": "main",
    "abstract": "Voice-based speaker authentication or Automatic Speaker Verification (ASV) system is now becoming practical reality after several decades of research. However, still this technology is very much susceptible to various spoofing attacks. Among various spoofing attacks, replay is the most challenging attack. In this paper, we propose a novel feature set based on our recently introduced Variable length Energy Separation Algorithm (VESA) during INTERSPEECH 2017. The key idea of this paper is to capture the Instantaneous Amplitude (IA) obtained from the instantaneous energy fluctuations. The replay speech is affected by acoustic environment and distortions of intermediate device. Thus, the noise added in replayed speech is important to detect. The Amplitude Modulations (AM) are more susceptible to noise and multipath interferences that may result due to replay mechanism. The experiments are performed on various dependency index (DI) and lower EER of 6.12% and 11.94% is found on dev and eval set, respectively, of ASV Spoof 2017 Challenge database. Furthermore, we compare our results with CQCC, LFCC, MFCC and VESA-IFCC feature sets. The score-level fusion VESA-IFCC and proposed feature set further reduced the EER to 0.19% and 7.11% on dev and eval set, respectively",
    "checked": true,
    "id": "30d314545377b0b00c949aee52ac494e59b79912",
    "semantic_title": "novel variable length energy separation algorithm using instantaneous amplitude features for replay detection",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yang18b_interspeech.html": {
    "title": "Feature with Complementarity of Statistics and Principal Information for Spoofing Detection",
    "volume": "main",
    "abstract": "Constant-Q transform (CQT) has demonstrated its effectiveness in anti-spoofing feature analysis for automatic speaker verification. This paper introduces a statistics-plus-principal information feature where a short-term spectral statistics information (STSSI), octave-band principal information (OPI) and full-band principal information (FPI) are proposed on the basis of CQT. Firstly, in contrast to conventional utterance-level long-term statistic information, STSSI reveals the spectral statistics at frame-level, moreover it provides a feasibility condition for model training while only small training database is available. Secondly, OPI emphasizes the principal information for octave-bands, STSSI and OPI creates a strong complementarity to enhance the anti-spoofing feature. Thirdly, FPI is also of complementary effect with OPI. With the statistical property over CQT spectral domain and the principal information through discrete cosine transform (DCT), the proposed statistics-plus-principal feature shows reasonable advantage of the complementary trait for spoofing detection. In this paper, we setup deep neural network (DNN) classifiers for evaluation of the features. Experiments show the effectiveness of the proposed feature as compared to many conventional features on ASVspoof 2017 and ASVspoof 2015 corpus",
    "checked": true,
    "id": "613967849d0546628a7c223b1ebe2d152b2db158",
    "semantic_title": "feature with complementarity of statistics and principal information for spoofing detection",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18n_interspeech.html": {
    "title": "Multiple Phase Information Combination for Replay Attacks Detection",
    "volume": "main",
    "abstract": "In recent years, the performance of Automatic Speaker Verification (ASV) systems has been improved significantly. However, they are still affected by different kind of spoofing attacks. In this paper, we propose a method that fused different phase features and amplitude features to detect replay attacks. We propose the mel-scale relative phase feature and apply source-filter vocal tract feature in phase domain for replay attacks detection. These two phase-based features are combined to get complementary information. In addition to these phase haracteristics, constant Q cepstral coefficients (CQCCs) are used. The proposed methods are evaluated using the ASVspoof 2017 challenge database and Gaussian mixture model was used as the back-end model. The proposed approach achieved 55.6% relative error reduction rate than the conventional magnitude-based feature",
    "checked": true,
    "id": "04a9311e29bd8728104689b2804efae0dfc718e0",
    "semantic_title": "multiple phase information combination for replay attacks detection",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wickramasinghe18_interspeech.html": {
    "title": "Frequency Domain Linear Prediction Features for Replay Spoofing Attack Detection",
    "volume": "main",
    "abstract": "Automatic speaker verification (ASV) systems are vulnerable to various types of spoofing attacks such as speech synthesis, voice conversion and replay attacks. Recent research has highlighted the need for more effective countermeasures for replay attacks, which can be very challenging to detect, however replayed speech has previously shown frequency band-specific differences when compared with genuine speech. In this paper, we propose the use of long-term temporal envelopes of subband signals using a frequency domain linear prediction (FDLP) framework. This flexible framework makes use of temporal envelope information, which has not previously been investigated for replay spoofing detection. Evaluations of the proposed system and its fusion with other subsystems were carried out on the ASVspoof 2017 database. Interestingly, smoother temporal envelopes, based on very long windows of up to 1 second, seem to be most successful and show good prospects for performance improvements via fusion",
    "checked": true,
    "id": "a03170e9851c0278a17dbfcb026fa873d6b01169",
    "semantic_title": "frequency domain linear prediction features for replay spoofing attack detection",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sailor18c_interspeech.html": {
    "title": "Auditory Filterbank Learning for Temporal Modulation Features in Replay Spoof Speech Detection",
    "volume": "main",
    "abstract": "In this paper, we present a standalone replay spoof speech detection (SSD) system to classify the natural vs. replay speech. The replay speech spectrum is known to be affected in the higher frequency range. In this context, we propose to exploit an auditory filterbank learning using Convolutional Restricted Boltzmann Machine (ConvRBM) with the pre-emphasized speech signals. Temporal modulations in amplitude (AM) and frequency (FM) are extracted from the ConvRBM subbands using the Energy Separation Algorithm (ESA). ConvRBM-based short-time AM and FM features are developed using cepstral processing, denoted as AM-ConvRBM-CC and FM-ConvRBM-CC. Proposed temporal modulation features performed better than the baseline Constant-Q Cepstral Coefficients (CQCC) features. On the evaluation set, an absolute reduction of 7.48% and 5.28% in Equal Error Rate (EER) is obtained using AM-ConvRBM-CC and FM-ConvRBM-CC, respectively compared to our CQCC baseline. The best results are achieved by combining scores from AM and FM cues (0.82% and 8.89% EER for development and evaluation set, respectively). The statistics of AM-FM features are analyzed to understand the performance gap and complementary information in both the features",
    "checked": true,
    "id": "b75e1a7755f87cad10a1460ea27fee910cc3064c",
    "semantic_title": "auditory filterbank learning for temporal modulation features in replay spoof speech detection",
    "citation_count": 36
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sriskandaraja18_interspeech.html": {
    "title": "Deep Siamese Architecture Based Replay Detection for Secure Voice Biometric",
    "volume": "main",
    "abstract": "Replay attacks are the simplest and the most easily accessible form of spoofing attacks on voice biometric systems and can be hard to detect by systems designed to identify spoofing attacks based on synthesised speech. In this paper, we propose a novel approach to evaluate the similarities between pairs of speech samples to detect replayed speech based on a suitable embedding learned by deep Siamese architectures. Specifically, we train a deep Siamese network to identify pairs of genuine speech samples and pairs of replayed speech samples as being ‘similar' and mixed pairs of genuine and replayed speech to be identified as ‘dissimilar'. Siamese networks are particularly suited to this task and have been shown to be effective in problems where intra-class variability is large and the number of training samples per class is relatively small. The internal low-dimensional embedding learnt by the Siamese network to accomplish this task is then used as the basis for replay detection. The proposed approach outperforms state-of-the-art systems when evaluated on the ASVspoof 2017 challenge corpus without relying on fusion with other sub-systems",
    "checked": true,
    "id": "f96cf7380492a408abd29112156f0c7749fd68d5",
    "semantic_title": "deep siamese architecture based replay detection for secure voice biometric",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gomezalanis18_interspeech.html": {
    "title": "A Deep Identity Representation for Noise Robust Spoofing Detection",
    "volume": "main",
    "abstract": "The issue of the spoofing attacks which may affect automatic speaker verification systems (ASVs) has recently received an increased attention, so that a number of countermeasures have been developed for detecting high technology attacks such as speech synthesis and voice conversion. However, the performance of anti-spoofing systems degrades significantly in noisy conditions. To address this issue, we propose a deep learning framework to extract spoofing identity vectors, as well as the use of soft missing-data masks. The proposed feature extraction employs a convolutional neural network (CNN) plus a recurrent neural network (RNN) in order to provide a single deep feature vector per utterance. Thus, the CNN is treated as a convolutional feature extractor that operates at the frame level. On top of the CNN outputs, the RNN is employed to obtain a single spoofing identity representation of the whole utterance. Experimental evaluation is carried out on both a clean and a noisy version of the ASVSpoof2015 corpus. The experimental results show that our proposals clearly outperforms other methods recently proposed such as the popular CQCC+GMM system or other similar deep feature systems for both seen and unseen noisy conditions",
    "checked": true,
    "id": "14cef141fac680f7c386c95d78c1412516fd2a54",
    "semantic_title": "a deep identity representation for noise robust spoofing detection",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tom18_interspeech.html": {
    "title": "End-To-End Audio Replay Attack Detection Using Deep Convolutional Networks with Attention",
    "volume": "main",
    "abstract": "With automatic speaker verification (ASV) systems becoming increasingly popular, the development of robust countermeasures against spoofing is needed. Replay attacks pose a significant threat to the reliability of ASV systems because of the relative difficulty in detecting replayed speech and the ease with which such attacks can be mounted. In this paper, we propose an end-to-end deep learning framework for audio replay attack detection. Our proposed approach uses a novel visual attention mechanism on time-frequency representations of utterances based on group delay features, via deep residual learning (an adaptation of ResNet-18 architecture). Using a single model system, we achieve a perfect Equal Error Rate (EER) of 0% on both the development as well as the evaluation set of the ASVspoof 2017 dataset, against a previous best of 0.12% on the development set and 2.76% on the evaluation set reported in the literature. This highlights the efficacy of our feature representation and attention-based architecture in tackling the challenging task of audio replay attack detection",
    "checked": true,
    "id": "fcf7c71e98a833ed1af4da28c24ad63d7df6841f",
    "semantic_title": "end-to-end audio replay attack detection using deep convolutional networks with attention",
    "citation_count": 77
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ms18_interspeech.html": {
    "title": "Decision-level Feature Switching as a Paradigm for Replay Attack Detection",
    "volume": "main",
    "abstract": "A pre-recorded audio sample of an authentic speaker presented to a voice-based biometric system is termed as a replay attack. Such attacks can be detected by identifying the characteristics of the recording device and environment. An analysis of different recording devices indicates that each recording device affects the spectrum differently. It is also observed that each feature captures specific characteristics of recording devices. In particular, Mel Filterbank Slope (MFS) captures low-frequency information corresponding to that of the low-quality recording devices, while Linear Filterbank Slope (LFS) captures high-frequency information which corresponds to that of a high-quality recording device. The proposed approach uses MFS and LFS along with Mel Frequency Cepstral Coefficients (MFCC) and Constant-Q Cepstral Coefficients (CQCC) in a Decision-level Feature Switching (DLFS) paradigm to determine whether a given utterance is spoofed. The obtained results surpass the state-of-the-art Light Convolutional Neural Network (LCNN) based replay detection system with a relative improvement of 7.43% on the ASV-spoof-2017 evaluation dataset",
    "checked": true,
    "id": "badc3e20bca1c071f0730ce41975a471f3384121",
    "semantic_title": "decision-level feature switching as a paradigm for replay attack detection",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2018/suthokumar18_interspeech.html": {
    "title": "Modulation Dynamic Features for the Detection of Replay Attacks",
    "volume": "main",
    "abstract": "The development of automatic systems that can detect replayed speech has emerged as a significant research challenge for securing voice biometric systems and is the focus of this paper. Specifically, this paper proposes two novel features to capture the static and dynamic characteristics of the signal from the modulation spectrum, which complement short term spectral features for use in replay detection. The modulation spectral centroid frequency feature is proposed as a vector representation of the first order spectral moments of the modulation spectrum. In conjunction to this, the long term spectral average serves to capture the static characteristics of the modulation spectrum. The proposed system, employing a GMM back-end, was evaluated on the ASVSpoof 2017 dataset and found to yield an EER of 6.54%",
    "checked": true,
    "id": "90da0f6fbd8c601aeef37e2ee252323d3e7b86de",
    "semantic_title": "modulation dynamic features for the detection of replay attacks",
    "citation_count": 42
  },
  "https://www.isca-speech.org/archive/interspeech_2018/loweimi18_interspeech.html": {
    "title": "On the Usefulness of the Speech Phase Spectrum for Pitch Extraction",
    "volume": "main",
    "abstract": "Most frequency domain techniques for pitch extraction such as cepstrum, harmonic product spectrum (HPS) and summation residual harmonics (SRH) operate on the magnitude spectrum and turn it into a function in which the fundamental frequency emerges as argmax. In this paper, we investigate the extension of these three techniques to the phase and group delay (GD) domains. Our extensions exploit the observation that the bin at which F (magnitude) becomes maximum, for some monotonically increasing function F, is equivalent to bin at which F (phase) has maximum negative slope and F (group delay) has the maximum value. To extract the pitch track from speech phase spectrum, these techniques were coupled with the source-filter model in the phase domain that we proposed in earlier publications and a novel voicing detection algorithm proposed here. The accuracy and robustness of the phase-based pitch extraction techniques are illustrated and compared with their magnitude-based counterparts using six pitch evaluation metrics. On average, it is observed that the phase spectrum can be successfully employed in pitch tracking with comparable accuracy and robustness to the speech magnitude spectrum",
    "checked": true,
    "id": "ba118f66bf0992d16178266efa1e9e22efa2f8fc",
    "semantic_title": "on the usefulness of the speech phase spectrum for pitch extraction",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/airaksinen18_interspeech.html": {
    "title": "Time-regularized Linear Prediction for Noise-robust Extraction of the Spectral Envelope of Speech",
    "volume": "main",
    "abstract": "Feature extraction of speech signals is typically performed in short-time frames by assuming that the signal is stationary within each frame. For the extraction of the spectral envelope of speech, which conveys the formant frequencies produced by the resonances of the slowly varying vocal tract, an often used frame length is within 20-30 ms. However, this kind of conventional frame-based spectral analysis is oblivious of the broader temporal context of the signal and is prone to degradation by, for example, environmental noise. In this paper, we propose a new frame-based linear prediction (LP) analysis method that includes a regularization term that penalizes energy differences in consecutive frames of an all-pole spectral envelope model. This integrates the slowly varying nature of the vocal tract as a part of the analysis. Objective evaluations related to feature distortion and phonetic representational capability were performed by studying the properties of the mel-frequency cepstral coefficient (MFCC) representations computed from different spectral estimation methods under noisy conditions using the TIMIT database. The results show that the proposed time-regularized LP approach exhibits superior MFCC distortion behavior while simultaneously having the greatest average separability of different phoneme categories in comparison to the other methods",
    "checked": true,
    "id": "fa295e8fdf46c0b1d387feedc634aacac0893159",
    "semantic_title": "time-regularized linear prediction for noise-robust extraction of the spectral envelope of speech",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sailor18_interspeech.html": {
    "title": "Auditory Filterbank Learning Using ConvRBM for Infant Cry Classification",
    "volume": "main",
    "abstract": "The infant cry classification is a socially-relevant problem where the task is to classify the normal vs. pathological cry signals. Since the cry signals are very different from the speech signals in terms of temporal and spectral content, there is a need for better feature representation for infant cry signals. In this paper, we propose to use unsupervised auditory filterbank learning using Convolutional Restricted Boltzmann Machine (ConvRBM). Analysis of the subband filters shows that most of the subband filters are Fourier-like basis functions. The infant cry classification experiments were performed on the two databases, namely, DA-IICT Cry and Baby Chillanto. The experimental results show that the proposed features perform better than the standard Mel Frequency Cepstral Coefficients (MFCC) using various statistically meaningful performance measures. In particular, our proposed ConvRBM-based features obtained an absolute improvement of 2% and 0.58% in the classification accuracy on the DA-IICT Cry and the Baby Chillanto database, respectively",
    "checked": true,
    "id": "fdba23acaaab48e097ceacae87685268a2f715fd",
    "semantic_title": "auditory filterbank learning using convrbm for infant cry classification",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shah18_interspeech.html": {
    "title": "Effectiveness of Dynamic Features in INCA and Temporal Context-INCA",
    "volume": "main",
    "abstract": "Non-parallel Voice Conversion (VC) has gained significant attention since last one decade. Obtaining corresponding speech frames from both the source and target speakers before learning the mapping function in the non-parallel VC is a key step in the standalone VC task. Obtaining such corresponding pairs, is more challenging due to the fact that both the speakers may have uttered different utterances from same or the different languages. Iterative combination of a Nearest Neighbor search step and a Conversion step Alignment (INCA) and its variant Temporal Context (TC)-INCA are popular unsupervised alignment algorithms. The INCA and TC-INCA iteratively learn the mapping function after getting the Nearest Neighbor (NN) aligned pairs from the intermediate converted and the target spectral features. In this paper, we propose to use dynamic features along with static features to calculate the NN aligned pairs in both the INCA and TC-INCA algorithms (since the dynamic features are known to play a key role to differentiate major phonetic categories). We obtained on an average relative improvement of 13.75% and 5.39% with our proposed Dynamic INCA and Dynamic TC-INCA, respectively. This improvement is also positively reflected in the quality of converted voices",
    "checked": true,
    "id": "a8373dcf7477f82f16942f71062f16e8481bc816",
    "semantic_title": "effectiveness of dynamic features in inca and temporal context-inca",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gong18_interspeech.html": {
    "title": "Singing Voice Phoneme Segmentation by Hierarchically Inferring Syllable and Phoneme Onset Positions",
    "volume": "main",
    "abstract": "In this paper, we tackle the singing voice phoneme segmentation problem in the singing training scenario by using language-independent information - onset and prior coarse duration. We propose a two-step method. In the first step, we jointly calculate the syllable and phoneme onset detection functions (ODFs) using a convolutional neural network (CNN). In the second step, the syllable and phoneme boundaries and labels are inferred hierarchically by using a duration-informed hidden Markov model (HMM). To achieve the inference, we incorporate the a priori duration model as the transition probabilities and the ODFs as the emission probabilities into the HMM. The proposed method is designed in a language-independent way such that no phoneme class labels are used. For the model training and algorithm evaluation, we collect a new jingju (also known as Beijing or Peking opera) solo singing voice dataset and manually annotate the boundaries and labels at phrase, syllable and phoneme levels. The dataset is publicly available. The proposed method is compared with a baseline method based on hidden semi-Markov model (HSMM) forced alignment. The evaluation results show that the proposed method outperforms the baseline by a large margin regarding both segmentation and onset detection tasks",
    "checked": true,
    "id": "c1558a889999816e7cadd3d3790256bcbf985c26",
    "semantic_title": "singing voice phoneme segmentation by hierarchically inferring syllable and phoneme onset positions",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tapkir18_interspeech.html": {
    "title": "Novel Empirical Mode Decomposition Cepstral Features for Replay Spoof Detection",
    "volume": "main",
    "abstract": "The advances in Automatic Speaker Verification (ASV) system for voice biometric purpose comes with the danger of spoofing attacks. The replay attack is the most accessible attack, where the attacker imitates speaker's identity by replaying the pre-recorded speech samples of the target speaker. Most of the conventional features, such as Mel Frequency Cepstral Coefficients (MFCC), Instantaneous Frequency Cepstral Coefficients (IFCC), etc. uses filterbank structure for feature extraction purpose. In this paper, we propose a novel Empirical Mode Decomposition Cepstral Coefficient (EMDCC) feature set, where the filterbank in MFCC is replaced with the Empirical Mode Decomposition (EMD) to obtain the subband signals. The proposed feature set takes an advantage of using EMD that acts as a dyadic filterbank and handles the nonlinear and non-stationary nature of the speech signal. The stand-alone EMDCC feature set gives the Equal Error Rate (EER) of 28.06% compared to the baseline CQCC and MFCC system with EER of 29.18% and 31.3%, respectively on the evaluation set of ASV Spoof 2017 Challenge database. Furthermore, the proposed feature set is fused with the Linear Frequency Modified Group Delay Cepstral Coefficient (LFMGDCC) at score-level and we obtain a reduced EER of 18.36% on evaluation set",
    "checked": true,
    "id": "7d48968de4c35f46bb292ad95b775f2f77ca6bee",
    "semantic_title": "novel empirical mode decomposition cepstral features for replay spoof detection",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tak18_interspeech.html": {
    "title": "Novel Linear Frequency Residual Cepstral Features for Replay Attack Detection",
    "volume": "main",
    "abstract": "Replay attack poses the most difficult challenge for the development of countermeasures for spoofed speech detection (SSD) system. Earlier researchers mainly used vocal tract-based (segmental) information for replay detection. However, during replay, excitation source-based information also gets affected (in particular, degradation in pitch source harmonics at higher frequency regions) due to recording environment and replay devices. Hence, in addition to the vocal tract-based system information, we have also explored the excitation source-based informations for SSD. In particular, we have used Linear Frequency Residual Cepstral Coefficients (LFRCC) for replay detection. The objective of this paper is to explore possible complementary excitation (glottal) source information present in the Linear Prediction residual-based features. Experiments performed on the ASV Spoof 2017 Challenge database with Gaussian Mixture Model (GMM) and Convolutional Neural Network (CNN) classifiers. When we combined the source and system-based information, we obtained on an average 28.77% and 42.72% relative decrease in Equal Error Rate (EER) on development and evaluation set, respectively. Furthermore, when we perform score-level fusion of feature sets (for a fixed classifier) followed by a classifier-level fusion of GMM and CNN (for a fixed feature set), we obtained reduced EER of 2.40% and 9.06% on dev and eval set, respectively",
    "checked": true,
    "id": "d6489a87efe823e2251f5319383093c2f62a331d",
    "semantic_title": "novel linear frequency residual cepstral features for replay attack detection",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tripathi18_interspeech.html": {
    "title": "Analysis of sparse representation based feature on speech mode classification",
    "volume": "main",
    "abstract": "Traditional phone recognition systems are developed using read speech. But, in reality, the speech that needs to be processed by machine is not always in read mode. Therefore to handle the phone recognition in realistic scenarios, three broad modes of speech: read, conversation and extempore are considered in this study. The conversation mode includes informal communication in an unconstrained environment between two or more individuals. In the extempore mode, a person speaks with confidence without the help of notes. Read mode is a formal type of speech in a rigid environment. In this work, we have proposed a sparse based feature for speech mode classification. The effectiveness of sparse representation depends on the dictionary. Therefore, we have learned multiple overcomplete dictionaries by using parallel atom-update dictionary learning (PAU-DL) technique to capture the discrimination characteristics present in the considered speech modes. Further, sparse features correspond to the sequence of speech frames are derived using the learned dictionary by applying the orthogonal matching pursuit (OMP) algorithm. The proposed sparse features are evaluated on speech corpora consisting of six Indian languages by performing classification of speech modes. The results with the proposed sparse features outperform the standard spectral, excitation source and prosodic features",
    "checked": true,
    "id": "078707629eb001292a33d92131823fbd9816a438",
    "semantic_title": "analysis of sparse representation based feature on speech mode classification",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dhiman18_interspeech.html": {
    "title": "Multicomponent 2-D AM-FM Modeling of Speech Spectrograms",
    "volume": "main",
    "abstract": "In contrast to 1-D short-time analysis of speech, 2-D modeling of spectrograms provides a characterization of speech attributes directly in the joint time-frequency plane. Building on existing 2-D models to analyze a spectrogram patch, we propose a multicomponent 2-D AM-FM representation for spectrogram decomposition. The components of the proposed representation comprise a DC, a fundamental frequency carrier and its harmonics and a spectrotemporal envelope, all in 2-D. The number of harmonics required is patch-dependent. The estimation of the AM and FM is done using the Riesz transform and the component weights are estimated using a least-squares approach. The proposed representation provides an improvement over existing state-of-the-art approaches, for both male and female speakers. This is quantified using reconstruction SNR and perceptual evaluation of speech quality (PESQ) metric. Further, we perform an overlap-add on the DC component, pooling all the patches and obtain a time-frequency (t-f) aperiodicity map for the speech signal. We verify its effectiveness in improving speech synthesis quality by using it in an existing state-of-the-art vocoder",
    "checked": true,
    "id": "b35dd0bbaa8a3da6f77d854dd3a73e6c20bcc0d0",
    "semantic_title": "multicomponent 2-d am-fm modeling of speech spectrograms",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sainathan18_interspeech.html": {
    "title": "An Optimization Framework for Recovery of Speech from Phase-Encoded Spectrograms",
    "volume": "main",
    "abstract": "In general, reconstruction of a speech signal from the spectrogram is non-unique because of the unavailability of the phase spectrum. Considering zero phase would result in a minimum-phase reconstruction. This limitation is overcome by computing the recently introduced phase-encoded spectrogram. In this approach, one modifies each frame of a speech signal to possess the causal, delta-dominant (CDD) property prior to computing the spectrogram. In an earlier publication, we showed that finite-length CDD sequences can be retrieved exactly from their magnitude spectra using a cepstrum technique. Although exactness is guaranteed in principle, practical implementations result in a limited, but high, reconstruction accuracy. In this paper, we focus on increasing the reconstruction accuracy. We formulate the reconstruction problem within an optimization framework and deploy a recently proposed iterative, alternating direction method of multipliers (ADMM) algorithm called autocorrelation retrieval—Kolmogorov factorization (CoRK). Experimental validations show that the CoRK algorithm results in a reconstruction accurate up to machine precision. We also show that both CoRK and cepstrum techniques are robust and invariant to the choice of the window duration, the amount of overlap between consecutive speech frames, the strength of the delta used to impart the CDD property and the presence of noise",
    "checked": true,
    "id": "d232df2c8d4736c33a0f07b056657042de5b14f4",
    "semantic_title": "an optimization framework for recovery of speech from phase-encoded spectrograms",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xia18b_interspeech.html": {
    "title": "Speaker Recognition with Nonlinear Distortion: Clipping Analysis and Impact",
    "volume": "main",
    "abstract": "Speech, speaker and language systems have traditionally relied on carefully collected speech material for training acoustic models. There is an overwhelming abundance of publicly accessible audio material available for training. A major challenge, however, is that such found data is not professionally recorded and therefore may contain a wide diversity of background noise, nonlinear distortions, or other unknown environmental based contamination or mismatch. There is a critical need for automatic analysis to screen such unknown data sets before acoustic model development, or to perform input audio purity screening prior to classification. In this study, we propose a waveform based clipping detection algorithm for naturalistic audio streams and analyze the impact of clipping at different severities on speech quality measures and automatic speaker recognition systems. We use the TIMIT and NIST SRE-08 corpora as case studies. The results show, as expected, that clipping introduces a nonlinear distortion into clean speech data, which reduces both speech quality and speaker recognition performance. We also investigate what degree of clipping can be present to sustain effective speech system performance. The proposed detection system, which will be released, could contribute to massive new audio collections for speech and language technology development",
    "checked": true,
    "id": "0a48edb9a55d1f880ea69dc86140a7639153c917",
    "semantic_title": "speaker recognition with nonlinear distortion: clipping analysis and impact",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/singh18_interspeech.html": {
    "title": "Linear Prediction Residual based Short-term Cepstral Features for Replay Attacks Detection",
    "volume": "main",
    "abstract": "Modern automatic speaker verification (ASV) systems are highly vulnerable to spoof attacks and developing ASV anti-spoofing algorithms to protect ASV systems form these attacks is currently a part of active research. Contrarily to current trends on development of stand-alone spoof detection system, this work aims detection of replay attacks directly on the ASV system. The claim made through replay spoofing trials is rejected as impostors directly by ASV system. The objective here is to model the changes in the excitation signal characteristics caused by playback devices for replay detection. Accordingly, two linear prediction (LP) residual based source features are proposed for rejecting replay spoofing trials namely, RMFCC (residual mel-frequency cepstral coefficients) and LPRHEMFCC (LP residual Hilbert envelope MFCC). A comparative analysis between these two source features has been performed through speaker verification experiments to evaluate their effectiveness for ASV anti-spoofing applications. The comparison between the two has been made in the form of (source feature + MFCC) combination. The experiments are conducted using self-developed IITG-MV replay database. From the experimental results, it has been observed that 'LPRHEMFCC+MFCC' combination outperforms 'RMFCC+MFCC' combination, under replay attacks. Finally, the experiments are repeated on ASVspoof2017 database to validate the efficacy of proposed work",
    "checked": true,
    "id": "b2ba604548e479be5771e527d4cd0a202ae7d9ce",
    "semantic_title": "linear prediction residual based short-term cepstral features for replay attacks detection",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sakshi18_interspeech.html": {
    "title": "Analysis of Variational Mode Functions for Robust Detection of Vowels",
    "volume": "main",
    "abstract": "In this work, initially the speech signal is decomposed into variational mode functions (VMFs) with the aid of variational mode decomposition (VMD). Each decomposed VMF represents different frequency band of the input speech signal. An approximate speech signal is then reconstructed by using a set of selected VMFs whose center frequency predominantly corresponds to the frequency range of the vowels. In the reconstructed speech signal, energy due to the high frequency unvoiced sound units and noises is suppressed. Consequently, over an analysis frame, the mean of the square magnitude (MSM) of the sample points is significantly higher for the vowels than other sound units. Further, the MSM at each time instant is non-linearly mapped (NLM) using a negative exponential functions to enhance the transitions at the onset and the offset points of vowels and suppress small fluctuations. The NLM-MSM is used as a front-end feature for discriminating vowels in a given speech signal. The experiments conducted on TIMIT database show that, the proposed approach outperforms the existing methods for the task of detecting vowels in a given speech signal under clean and noisy test scenarios",
    "checked": true,
    "id": "cbe45f0a45d1ae11d24c782168372e77bb15ced4",
    "semantic_title": "analysis of variational mode functions for robust detection of vowels",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/weng18_interspeech.html": {
    "title": "Improving Attention Based Sequence-to-Sequence Models for End-to-End English Conversational Speech Recognition",
    "volume": "main",
    "abstract": "In this work, we propose two improvements to attention based sequence-to-sequence models for end-to-end speech recognition systems. For the first improvement, we propose to use an input-feeding architecture which feeds not only the previous context vector but also the previous decoder hidden state information as inputs to the decoder. The second improvement is based on a better hypothesis generation scheme for sequential minimum Bayes risk (MBR) training of sequence-to-sequence models where we introduce softmax smoothing into N-best generation during MBR training. We conduct the experiments on both Switchboard-300hrs and Switchboard+Fisher-2000hrs datasets and observe significant gains from both proposed improvements. Together with other training strategies such as dropout and scheduled sampling, our best model achieved WERs of 8.3%/15.5% on the Switchboard/CallHome subsets of Eval2000 without any external language models which is highly competitive among state-of-the-art English conversational speech recognition systems",
    "checked": true,
    "id": "dc418fdc9c93ece1d51277d12ace537aecfbe913",
    "semantic_title": "improving attention based sequence-to-sequence models for end-to-end english conversational speech recognition",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2018/beck18_interspeech.html": {
    "title": "Segmental Encoder-Decoder Models for Large Vocabulary Automatic Speech Recognition",
    "volume": "main",
    "abstract": "It has been known for a long time that the classic Hidden-Markov-Model (HMM) derivation for speech recognition contains assumptions such as independence of observation vectors and weak duration modeling that are practical but unrealistic. When using the hybrid approach this is amplified by trying to fit a discriminative model into a generative one. Hidden Conditional Random Fields (CRFs) and segmental models (e.g. Semi-Markov CRFs / Segmental CRFs) have been proposed as an alternative, but for a long time have failed to get traction until recently. In this paper we explore different length modeling approaches for segmental models, their relation to attention-based systems. Furthermore we show experimental results on a handwriting recognition task and to the best of our knowledge the first reported results on the Switchboard 300h speech recognition corpus using this approach",
    "checked": true,
    "id": "0d3c436a4236daa236617e0e01d2ff031a8459e1",
    "semantic_title": "segmental encoder-decoder models for large vocabulary automatic speech recognition",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18b_interspeech.html": {
    "title": "Acoustic Modeling with DFSMN-CTC and Joint CTC-CE Learning",
    "volume": "main",
    "abstract": "Recently, the connectionist temporal classification (CTC) based acoustic models have achieved comparable or even better performance, with much higher decoding efficiency, than the conventional hybrid systems in LVCSR tasks. For CTC-based models, it usually uses the LSTM-type networks as acoustic models. However, LSTMs are computationally expensive and sometimes difficult to train with CTC criterion. In this paper, inspired by the recent DFSMN works, we propose to replace the LSTMs with DFSMN in CTC-based acoustic modeling and explore how this type of non-recurrent models behave when trained with CTC loss. We have evaluated the performance of DFSMN-CTC using both context-independent (CI) and context-dependent (CD) phones as target labels in many LVCSR tasks with various amount of training data. Experimental results shown that DFSMN-CTC acoustic models using either CI-Phones or CD-Phones can significantly outperform the conventional hybrid models that trained with CD-Phones and cross-entropy (CE) criterion. Moreover, a novel joint CTC and CE training method is proposed, which enables to improve the stability of CTC training and performance. In a 20000 hours Mandarin recognition task, joint CTC-CE trained DFSMN can achieve a 11.0% and 30.1% relative performance improvement compared to DFSMN-CE models in a normal and fast speed test set respectively",
    "checked": true,
    "id": "fb3fec089bfa13f93002936f0835b71491cbc7cf",
    "semantic_title": "acoustic modeling with dfsmn-ctc and joint ctc-ce learning",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bae18_interspeech.html": {
    "title": "End-to-End Speech Command Recognition with Capsule Network",
    "volume": "main",
    "abstract": "In recent years, neural networks have become one of the common approaches used in speech recognition(SR), with SR systems based on Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) achieving the state-of-the-art results in various SR benchmarks. Especially, since CNNs are capable of capturing the local features effectively, they are applied to tasks which have relatively short-term dependencies, such as keyword spotting or phoneme-level sequence recognition. However, one limitation of CNNs is that, with max-pooling, they do not consider the pose relationship between low-level features. Motivated by this problem, we apply the capsule network to capture the spatial relationship and pose information of speech spectrogram features in both frequency and time axes. We show that our proposed end-to-end SR system with capsule networks on one-second speech commands dataset achieves better results on both clean and noise-added test than baseline CNN models",
    "checked": true,
    "id": "42e77433f695441876e16b678914b69f0a00910b",
    "semantic_title": "end-to-end speech command recognition with capsule network",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zeghidour18_interspeech.html": {
    "title": "End-to-End Speech Recognition from the Raw Waveform",
    "volume": "main",
    "abstract": "State-of-the-art speech recognition systems rely on fixed, hand-crafted features such as mel-filterbanks to preprocess the waveform before the training pipeline. In this paper, we study end-to-end systems trained directly from the raw waveform, building on two alternatives for trainable replacements of mel-filterbanks that use a convolutional architecture. The first one is inspired by gammatone filterbanks (Hoshen et al., 2015; Sainath et al, 2015) and the second one by the scattering transform (Zeghidour et al., 2017). We propose two modifications to these architectures and systematically compare them to mel-filterbanks, on the Wall Street Journal dataset. The first modification is the addition of an instance normalization layer, which greatly improves on the gammatone-based trainable filterbanks and speeds up the training of the scattering-based filterbanks. The second one relates to the low-pass filter used in these approaches. These modifications consistently improve performances for both approaches and remove the need for a careful initialization in scattering-based trainable filterbanks. In particular, we show a consistent improvement in word error rate of the trainable filterbanks relatively to comparable mel-filterbanks. It is the first time end-to-end models trained from the raw signal significantly outperform mel-filterbanks on a large vocabulary task with clean recording conditions",
    "checked": true,
    "id": "967fd4bb4eed76f1209401eca0a96029f1171c4e",
    "semantic_title": "end-to-end speech recognition from the raw waveform",
    "citation_count": 81
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yu18_interspeech.html": {
    "title": "A Multistage Training Framework for Acoustic-to-Word Model",
    "volume": "main",
    "abstract": "Acoustic-to-word (A2W) prediction model based on Connectionist Temporal Classification (CTC) criterion has gained increasing interest in recent studies. Although previous studies have shown that A2W system could achieve competitive Word Error Rate (WER), there is still performance gap compared with the conventional speech recognition system when the amount of training data is not exceptionally large. In this study, we empirically investigate advanced model initializations and training strategies to achieve competitive speech recognition performance on 300 hour subset of the Switchboard task (SWB-300Hr). We first investigate the use of hierarchical CTC pretraining for improved model initialization. We also explore curriculum training strategy to gradually increase the target vocabulary size from 10k to 20k. Finally, joint CTC and Cross Entropy (CE) training techniques are studied to further improve the performance of A2W system. The combination of hierarchical-CTC model initialization, curriculum training and joint CTC-CE training translates to a relative of 12.1% reduction in WER. Our final A2W system evaluated on Hub5-2000 test sets achieves a WER of 11.4/20.8 for Switchboard and CallHome parts without using language model and decoder",
    "checked": true,
    "id": "eae5e14f2996d6795731ecc8d6998343dcbc6a80",
    "semantic_title": "a multistage training framework for acoustic-to-word model",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhou18_interspeech.html": {
    "title": "Syllable-Based Sequence-to-Sequence Speech Recognition with the Transformer in Mandarin Chinese",
    "volume": "main",
    "abstract": "Sequence-to-sequence attention-based models have recently shown very promising results on automatic speech recognition (ASR) tasks, which integrate an acoustic, pronunciation and language model into a single neural network. In these models, the Transformer, a new sequence-to-sequence attentionbased model relying entirely on self-attention without using RNNs or convolutions, achieves a new single-model state-of-the- art BLEU on neural machine translation (NMT) tasks. Since the outstanding performance of the Transformer, we extend it to speech and concentrate on it as the basic architecture of sequence-to-sequence attention-based model on Mandarin Chinese ASR tasks. Furthermore, we investigate a comparison between syllable based model and context-independent phoneme (CI-phoneme) based model with the Transformer in Mandarin Chinese. Additionally, a greedy cascading decoder with the Transformer is proposed for mapping CI-phoneme sequences and syllable sequences into word sequences. Experiments on HKUST datasets demonstrate that syllable based model with the Transformer performs better than CI-phoneme based counterpart, and achieves a character error rate (CER) of 28.77%, which is competitive to the state-of-the-art CER of 28.0% by the joint CTC-attention based encoder-decoder network",
    "checked": true,
    "id": "53dcd6068586d50169877d145df550ff3f568221",
    "semantic_title": "syllable-based sequence-to-sequence speech recognition with the transformer in mandarin chinese",
    "citation_count": 106
  },
  "https://www.isca-speech.org/archive/interspeech_2018/han18b_interspeech.html": {
    "title": "Densely Connected Networks for Conversational Speech Recognition",
    "volume": "main",
    "abstract": "In this paper we show how we have achieved the state-of-the-art performance on the industry-standard NIST 2000 Hub5 English evaluation set. We propose densely connected LSTMs (namely, dense LSTMs), inspired by the densely connected convolutional neural networks recently introduced for image classification tasks. It is shown that the proposed dense LSTMs would provide more reliable performance as compared to the conventional, residual LSTMs as more LSTM layers are stacked in neural networks. With RNN-LM rescoring and lattice combination on the 5 systems (including 2 dense LSTM based systems) trained across three different phone sets, Capio's conversational speech recognition system has obtained 5.0% and 9.1% on Switchboard and CallHome, respectively",
    "checked": true,
    "id": "8794dcfd6dfa71eba63f7c6452622b1f560fcc17",
    "semantic_title": "densely connected networks for conversational speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hayashi18_interspeech.html": {
    "title": "Multi-Head Decoder for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "This paper presents a new network architecture called multi-head decoder for end-to-end speech recognition as an extension of a multi-head attention model. In the multi-head attention model, multiple attentions are calculated and then, they are integrated into a single attention. On the other hand, instead of the integration in the attention level, our proposed method uses multiple decoders for each attention and integrates their outputs to generate a final output. Furthermore, in order to make each head to capture the different modalities, different attention functions are used for each head, leading to the improvement of the recognition performance with an ensemble effect. To evaluate the effectiveness of our proposed method, we conduct an experimental evaluation using Corpus of Spontaneous Japanese. Experimental results demonstrate that our proposed method outperforms the conventional methods such as location-based and multi-head attention models and that it can capture different speech/linguistic contexts within the attention-based encoder-decoder framework",
    "checked": true,
    "id": "ff1a1e39a94b9ca31e6013d12bc2d27f7a31567c",
    "semantic_title": "multi-head decoder for end-to-end speech recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mori18_interspeech.html": {
    "title": "Compressing End-to-end ASR Networks by Tensor-Train Decomposition",
    "volume": "main",
    "abstract": "End-to-end deep learning has become a popular framework for automatic speech recognition (ASR) tasks and it has proven itself to be a powerful solution. Unfortunately, network structures commonly have millions of parameters and large computational resources are required to make this approach feasible for training and running such networks. Moreover, many applications still prefer lightweight models of ASR that can run efficiently on mobile or wearable devices. To address this challenge, we propose an approach that can reduce the number of ASR parameters. Specifically, we perform Tensor-Train decomposition on the weight matrix of the gated recurrent unit (TT-GRU) in the end-to-end ASR framework. Experimental results on LibriSpeech data reveal that the compressed ASR with TT-GRU can maintain good performance while greatly reducing the number of parameters",
    "checked": true,
    "id": "bbd623f8989d1e0b70f4b143038494626973d5c5",
    "semantic_title": "compressing end-to-end asr networks by tensor-train decomposition",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chung18c_interspeech.html": {
    "title": "Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech",
    "volume": "main",
    "abstract": "In this paper, we propose a novel deep neural network architecture, Speech2Vec, for learning fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the underlying spoken words and are close to other vectors in the embedding space if their corresponding underlying spoken words are semantically similar. The proposed model can be viewed as a speech version of Word2Vec. Its design is based on a RNN Encoder-Decoder framework and borrows the methodology of skipgrams or continuous bag-of-words for training. Learning word embeddings directly from speech enables Speech2Vec to make use of the semantic information carried by speech that does not exist in plain text. The learned word embeddings are evaluated and analyzed on 13 widely used word similarity benchmarks and outperform word embeddings learned by Word2Vec from the transcriptions",
    "checked": true,
    "id": "083ec74cc10f96fbb64322ba23450666fd4df6cd",
    "semantic_title": "speech2vec: a sequence-to-sequence framework for learning word embeddings from speech",
    "citation_count": 166
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dong18_interspeech.html": {
    "title": "Extending Recurrent Neural Aligner for Streaming End-to-End Speech Recognition in Mandarin",
    "volume": "main",
    "abstract": "End-to-end models have been showing superiority in Automatic Speech Recognition (ASR). At the same time, the capacity of streaming recognition has become a growing requirement for end-to-end models. Following these trends, an encoder-decoder recurrent neural network called Recurrent Neural Aligner (RNA) has been freshly proposed and shown its competitiveness on two English ASR tasks. However, it is not clear if RNA can be further improved and applied to other spoken language. In this work, we explore the applicability of RNA in Mandarin Chinese and present four effective extensions: In the encoder, we redesign the temporal down-sampling and introduce a powerful convolutional structure. In the decoder, we utilize a regularizer to smooth the output distribution and conduct joint training with a language model. On two Mandarin Chinese conversational telephone speech recognition (MTS) datasets, our Extended-RNA obtains promising performance. Particularly, it achieves 27.7% character error rate (CER), which is superior to current state-of-the-art result on the popular HKUST task",
    "checked": true,
    "id": "eb82d0a93471cba5dc203df7a82dd8eaa845ff9a",
    "semantic_title": "extending recurrent neural aligner for streaming end-to-end speech recognition in mandarin",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18c_interspeech.html": {
    "title": "Joint Noise and Reverberation Adaptive Learning for Robust Speaker DOA Estimation with an Acoustic Vector Sensor",
    "volume": "main",
    "abstract": "Deep neural network (DNN) based DOA estimation (DNN-DOAest) methods report superior performance but the degradation is observed under stronger additive noise and room reverberation conditions. Motivated by our previous work with an acoustic vector sensor (AVS) and the great success of DNN based speech denoising and dereverberation (DNN-SDD), a unified DNN framework for robust DOA estimation task is thoroughly investigated in this paper. First, a novel DOA cue termed as sub-band inter-sensor data ratio (Sb-ISDR) is proposed to efficiently represent DOA information for training a DNN-DOAest model. Second, a speech-aware DNN-SDD is presented, where coherence vectors denoting the probability of time-frequency points dominated by speech signals are used as additional input to facilitate the training to predict complex ideal ratio masks. Last, by stacking the DNN-DOAest on the DNN-SDD with a joint part, the unified network is jointly fine-tuned, which enables DNN-SDD to serve as a pre-processing front-end to adaptively generate ‘clean' speech features that are easier to be correctly classified by the following DNN-DOAest for robust DOA estimation. Experimental results on simulated and recorded data confirm the effectiveness and superiority of our proposed methods under different noise and reverberations compared with baseline methods",
    "checked": true,
    "id": "856230a85d9b0f1fc0b098534c09fdf60d2d8fc3",
    "semantic_title": "joint noise and reverberation adaptive learning for robust speaker doa estimation with an acoustic vector sensor",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18c_interspeech.html": {
    "title": "Multiple Concurrent Sound Source Tracking Based on Observation-Guided Adaptive Particle Filter",
    "volume": "main",
    "abstract": "Particle filter (PF) has been proved to be an effective tool to track sound sources. In traditional PF, a pre-defined dynamic model is used to model source motion, which tends to be mismatched due to the uncertainty of source motion. Besides, non-stationary interferences pose a severe challenge to source tracking. To this end, an observation-guided adaptive particle filter (OAPF) is proposed for multiple concurrent sound source tracking. Firstly, sensor signals are processed in the time-frequency domain to obtain the direction of arrival (DOA) observations of sources. Then, by updating particle states with these DOA observations, angular distances between particles and observations are reduced to guide particles to directions of sources. Thirdly, particle weights are updated by an interference-adaptive likelihood function to reduce the impacts of interferences. At last, with the updated particles and the corresponding weights, OAPF is utilized to determine the final DOAs of sources. Experimental results demonstrate that our method achieves favorable performance for multiple concurrent sound source tracking in noisy environments",
    "checked": true,
    "id": "406e9e438b9e6daa94ce2dfd665d7bb99c921d6c",
    "semantic_title": "multiple concurrent sound source tracking based on observation-guided adaptive particle filter",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/m18c_interspeech.html": {
    "title": "Harmonic-Percussive Source Separation of Polyphonic Music by Suppressing Impulsive Noise Events",
    "volume": "main",
    "abstract": "In recent years, harmonic-percussive source separation methods are gaining importance because of their potential applications in many music information retrieval tasks. The goal of the decomposition methods is to achieve near real-time separation, distortion and artifact free component spectrograms and their equivalent time domain signals for potential music applications. In this paper, we propose a decomposition method based on filtering/suppressing the impulsive interference of percussive source on the harmonic components and impulsive interference of the harmonic source on the percussive components by modified moving average filter in the Fourier frequency domain. The significant advantage of the proposed method is that it minimizes the artifacts in the separated signal spectrograms. In this work, we have proposed Affine and Gain masking methods to separate the harmonic and percussive components to achieve minimal spectral leakage. The objective measures and separated spectrograms showed that the proposed method is better than the existing rank-order filtering based harmonic-percussive separation methods",
    "checked": true,
    "id": "39da86dd4780a670fcbfe63ad3b4b4af698f1062",
    "semantic_title": "harmonic-percussive source separation of polyphonic music by suppressing impulsive noise events",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ceolini18_interspeech.html": {
    "title": "Speaker Activity Detection and Minimum Variance Beamforming for Source Separation",
    "volume": "main",
    "abstract": "This work proposes a framework that renders minimum variance beamforming blind allowing for source separation in real world environments with an ad-hoc multi-microphone setup using no assumptions other than knowing the number of speakers. The framework allows for multiple active speakers at the same time and estimates the activity of every single speaker at flexible time resolution. These estimated speaker activities are subsequently used for the calibration of the beamforming algorithm. This framework is tested with three different speaker activity detection (SAD) methods, two of which use classical algorithms and one that is event-driven. Our methods, when tested in real world reverberant scenarios, can achieve very high signal-to-interference ratio (SIR) of around 20 dB and sound quality of 0.85 in short-time objective intelligibility (STOI) close to optimal beamforming results of 22 dB SIR and 0.89 in STOI",
    "checked": true,
    "id": "c26f301a195c11f2615b5e854169c2551cd8207c",
    "semantic_title": "speaker activity detection and minimum variance beamforming for source separation",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/qi18_interspeech.html": {
    "title": "Sparsity-Constrained Weight Mapping for Head-Related Transfer Functions Individualization from Anthropometric Features",
    "volume": "main",
    "abstract": "Head-related transfer functions (HRTFs) describe the propagation of sound waves from the sound source to ear drums, which contain most of information for localization. However, HRTFs are highly individual-dependent and thus because of the difference of anthropometric features between subjects, individualization of HRTFs is a great challenge for accurate localization perception in virtual auditory displays (VAD). In this paper, we propose a sparsity-constrained weight mapping method termed SWM to obtain individual HRTFs. The key idea behind SWM is to obtain optimal weights to combine HRTFs from the training subjects based on the relationship of anthropometric features between the target subject and the training subjects. To this end, SWM learns two sparse representations between the target subject and the training subjects in terms of anthropometric features and HRTFs, respectively. A non-negative sparse model is used for this purpose when considering the non-negative property of the anthropometric features. Then, we build a mapping between the two weight vectors using a nonlinear regression. Furthermore, an iterative data extension method is proposed in order to increase training samples for mapping model. The objective and subjective experimental results show that the proposed method outperforms other methods in terms of log-spectral distortion (LSD) and localization accuracy",
    "checked": true,
    "id": "a70581ca2478e6ef71c6e4a3c9b30759eb1880fb",
    "semantic_title": "sparsity-constrained weight mapping for head-related transfer functions individualization from anthropometric features",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sai18_interspeech.html": {
    "title": "Speech Source Separation Using ICA in Constant Q Transform Domain",
    "volume": "main",
    "abstract": "In order to separate individual sources from convoluted speech mixtures, complex-domain independent component analysis (ICA) is employed on the individual frequency bins of time frequency representations of the speech mixtures, obtained using short-time Fourier transform (STFT). The frequency components computed using STFT are separated by constant frequency difference with a constant frequency resolution. However, it is well known that the human auditory mechanism offers better resolution at lower frequencies. Hence, the perceptual quality of the extracted sources critically depends on the separation achieved in the lower frequency components. In this paper, we propose to perform source separation on the time-frequency representation computed though constant Q transform (CQT), which offers non uniform logarithmic binning in the frequency domain. Complex-domain ICA is performed on the individual bins of the CQT in order to get separated components in each frequency bin which are suitably scaled and permuted to obtain separated sources in the CQT domain. The estimated sources are obtained by applying inverse constant Q transform to the scaled and permuted sources. In comparison with the STFT based frequency domain ICA methods, there has been a consistent improvement of 3 dB or more in the Signal to Interference Ratios of the extracted sources",
    "checked": true,
    "id": "90ff2e389652c2537162737561a10be226a29d4b",
    "semantic_title": "speech source separation using ica in constant q transform domain",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yin18_interspeech.html": {
    "title": "Multi-talker Speech Separation Based on Permutation Invariant Training and Beamforming",
    "volume": "main",
    "abstract": "The recently proposed Permutation Invariant Training (PIT) technique addresses the label permutation problem for multi-talker speech separation. It has shown to be effective for the single-channel separation case. In this paper, we propose to extend the PIT-based technique to the multichannel multi-talker speech separation scenario. PIT is used to train a neural network that outputs masks for each separate speaker which is followed by a Minimum Variance Distortionless Response (MVDR) beamformer. The beamformer utilizes the spatial information of different speakers and alleviates the performance degradation due to misaligned labels. Experimental results show that the proposed PIT-MVDR-based technique leads to higher Signal-to-Distortion Ratios (SDRs) compared to the single-channel speech separation method when tested on two-speaker and three-speaker mixtures",
    "checked": true,
    "id": "67259db263d29f07540b23b3284675e2e406e0e0",
    "semantic_title": "multi-talker speech separation based on permutation invariant training and beamforming",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2018/magron18_interspeech.html": {
    "title": "Expectation-Maximization Algorithms for Itakura-Saito Nonnegative Matrix Factorization",
    "volume": "main",
    "abstract": "This paper presents novel expectation-maximization (EM) algorithms for estimating the nonnegative matrix factorization model with Itakura-Saito divergence. Indeed, the common EM-based approach exploits the space-alternating generalized EM (SAGE) variant of EM but it usually performs worse than the conventional multiplicative algorithm. We propose to explore more exhaustively those algorithms, in particular the choice of the methodology (standard EM or SAGE variant) and the latent variable set (full or reduced). We then derive four EM-based algorithms, among which three are novel. Speech separation experiments show that one of those novel algorithms using a standard EM methodology and a reduced set of latent variables outperforms its SAGE variants and competes with the conventional multiplicative algorithm",
    "checked": true,
    "id": "a4db89bf6663c4575c8b5cb9ac656b9bbbb96a67",
    "semantic_title": "expectation-maximization algorithms for itakura-saito nonnegative matrix factorization",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/girijaramesan18_interspeech.html": {
    "title": "Subband Weighting for Binaural Speech Source Localization",
    "volume": "main",
    "abstract": "We consider the task of speech source localization from a binaural recording using interaural time difference (ITD). A typical approach is to process binaural speech using gammatone filters and calculate frame-level ITD in each subband. The ITDs in each gammatone subband are statistically modelled using Gaussian mixture models (GMMs) for every direction during training. Given a binaural test-speech, the source is localized using maximum likelihood (ML) criterion. In this work, we propose a subband weighting scheme where subband likelihoods are weighted based on their reliability. We measure the reliability of a subband using the average frame level localization error obtained for the respective subbands. These reliability values are used as the weights for each subband likelihood prior to combining the likelihoods for ML estimation. We also introduce non-linear warping of these weights to accommodate and analyse a larger space of possible subband weights. Experiments on Subject_003 from the CIPIC database reveal that weighting the subbands is better than the unweighted scheme of combining likelihoods",
    "checked": true,
    "id": "4d8fb0dfc6b75d879a1be42b4972787e859b6eda",
    "semantic_title": "subband weighting for binaural speech source localization",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vaissiere18_interspeech.html": {
    "title": "Universal Tendencies for Cross-Linguistic Prosodic Tendencies: A Review and Some New Proposals",
    "volume": "main",
    "abstract": "The present talk aims first to review the literature on similar tendencies regularly observed in typologically unrelated languages. The tendencies concern the use of fundamental frequency (F0, including declination line as the reference line, the top-line, up-stepping, down-stepping, register change and range widening-reducing), lengthening-shortening maneuvers and strengthening-weakening phenomena at the glottal and supraglottic levels, for instantiating acoustically the syllable, the word, the minor and major phrases, and the utterance. Our presentation concerns only attitudinally and emotionally neutral utterances. The second part of the talk will present particular aspects: 1) the different centers of articulatory \"effort\" at the syllable level; 2) the suggestion of the existence of an unmarked strong-long pattern, neither trochaic nor iambic, at the word level in languages where natives don't have the consciousness of a \"lexical stress,\" or don't agree on its existence or position; 3) the regrouping of one or more words into a prosodic phrase by the application of two established principles: a) the \"hat-pattern\" principle (t'Hart) favoring initial high-rising and final low-falling F0, and b) the intensive or the temporal rhythmic basic tendencies (Woodrow, Fraisse) favoring a more intense, stronger, more precisely articulated beginning and a lengthened ending; 4) the existence of a multilayer rhythm at the utterance level composed by the repetition/alternation of integrated Gestalts at the levels of the syllable, word, and phrases. One or two Gestalts will prevail perceptually depending on a) the language, b) the style, and c) the rate of speech. The impressionistic evidence of a particular type of language-dependent \"rhythm\" depends on the listener's expectations, related to his maternal language and the languages he already masters, and up to a certain extent, to his pre-existing theoretical beliefs",
    "checked": true,
    "id": "8b2a863b62e66cce139c6d23a5896c882cfae25f",
    "semantic_title": "universal tendencies for cross-linguistic prosodic tendencies: a review and some new proposals",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/klejch18_interspeech.html": {
    "title": "Learning to Adapt: A Meta-learning Approach for Speaker Adaptation",
    "volume": "main",
    "abstract": "The performance of automatic speech recognition systems can be improved by adapting an acoustic model to compensate for the mismatch between training and testing conditions, for example by adapting to unseen speakers. The success of speaker adaptation methods relies on selecting weights that are suitable for adaptation and using good adaptation schedules to update these weights in order not to overfit to the adaptation data. In this paper we investigate a principled way of adapting all the weights of the acoustic model using a meta-learning. We show that the meta-learner can learn to perform supervised and unsupervised speaker adaptation and that it outperforms a strong baseline adapting LHUC parameters when adapting a DNN AM with 1.5M parameters. We also report initial experiments on adapting TDNN AMs, where the meta-learner achieves comparable performance with LHUC",
    "checked": true,
    "id": "ceaadb7011a29046437b2c862722f7b7fea7209e",
    "semantic_title": "learning to adapt: a meta-learning approach for speaker adaptation",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18o_interspeech.html": {
    "title": "Speaker Adaptation and Adaptive Training for Jointly Optimised Tandem Systems",
    "volume": "main",
    "abstract": "Speaker independent (SI) Tandem systems trained by joint optimisation of bottleneck (BN) deep neural networks (DNNs) and Gaussian mixture models (GMMs) have been found to produce similar word error rates (WERs) to Hybrid DNN systems. A key advantage of using GMMs is that existing speaker adaptation methods, such as maximum likelihood linear regression (MLLR), can be used which to account for diverse speaker variations and improve system robustness. This paper investigates speaker adaptation and adaptive training (SAT) schemes for jointly optimised Tandem systems. Adaptation techniques investigated include constrained MLLR (CMLLR) transforms based on BN features for SAT as well as MLLR and parameterised sigmoid functions for unsupervised test-time adaptation. Experiments using English multi-genre broadcast (MGB3) data show that CMLLR SAT yields a 4% relative WER reduction over jointly trained Tandem and Hybrid SI systems and further reductions in WER are obtained by system combination",
    "checked": true,
    "id": "874b355b13539526a1c1824ba61c27745a1152d5",
    "semantic_title": "speaker adaptation and adaptive training for jointly optimised tandem systems",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kitza18_interspeech.html": {
    "title": "Comparison of BLSTM-Layer-Specific Affine Transformations for Speaker Adaptation",
    "volume": "main",
    "abstract": "Bidirectional Long Short-Term Memory (BLSTM) Recurrent Neural Networks (RNN) acoustic models have demonstrated superior performance over Deep feed-forward Neural Networks (DNN) models in speech recognition and many other tasks. Although, a lot of work has been reported on DNN model adaptation, very little has been done on BLSTM model adaptation. This work presents a systematic study on the adaptation of BLSTM acoustic models by means of learning affine transformations within the neural network on small amounts of unsupervised adaptation data. Through a series of experiments on two major speech recognition benchmarks (Switchboard and CHiME-4), we investigate the significance of the position of the transformation in a BLSTM Network using a separate transformation for the forward- and backward-direction. We observe that applying affine transformations result in consistent relative word error rate reductions ranging from 6% to 11% depending on the task and the degree of mismatch between training and test data",
    "checked": true,
    "id": "7fe3531a52817a641fc4dfbcd7ac486e4c07e3de",
    "semantic_title": "comparison of blstm-layer-specific affine transformations for speaker adaptation",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sharon18_interspeech.html": {
    "title": "Correlational Networks for Speaker Normalization in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose using common representation learning(CRL) for speaker normalization in automatic speech recognition (ASR). Conventional methods like feature space maximum likelihood linear regression (fMLLR) require two pass decode and their performance is often limited by the amount of data during test. While i-vectors do not require two-pass decode, a significant number of input frames are required for estimation. Hence, as an alternative, a regression model employing correlational neural networks (CorrNet) for multi-view CRL is proposed. In this approach, the CorrNet training methodology treats normalized and un-normalized features as two parallel views of the same speech data. Once trained, this network generates frame-wise fMLLR-like features, thus overcoming the limitations of fMLLR/i-vectors. The recognition accuracy using the proposed CorrNet-generated features is comparable with the i-vector model counterparts and significantly better than the un-normalized features like filterbank. With CorrNet-features, we get an absolute improvement in word error rate of 2.5% for TIMIT, 2.69% for WSJ84 and 3.2% for Switchboard-33hour over un-normalized features",
    "checked": true,
    "id": "376373150f94ab03ce913606ef7ce1fb4a61239a",
    "semantic_title": "correlational networks for speaker normalization in automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tjandra18_interspeech.html": {
    "title": "Machine Speech Chain with One-shot Speaker Adaptation",
    "volume": "main",
    "abstract": "In previous work, we developed a closed-loop speech chain model based on deep learning, in which the architecture enabled the automatic speech recognition (ASR) and text-to-speech synthesis (TTS) components to mutually improve their performance. This was accomplished by the two parts teaching each other using both labeled and unlabeled data. This approach could significantly improve model performance within a single-speaker speech dataset, but only a slight increase could be gained in multi-speaker tasks. Furthermore, the model is still unable to handle unseen speakers. In this paper, we present a new speech chain mechanism by integrating a speaker recognition model inside the loop. We also propose extending the capability of TTS to handle unseen speakers by implementing one-shot speaker adaptation. This enables TTS to mimic voice characteristics from one speaker to another with only a one-shot speaker sample, even from a text without any speaker information. In the speech chain loop mechanism, ASR also benefits from the ability to further learn an arbitrary speaker's characteristics from the generated speech waveform, resulting in a significant improvement in the recognition rate",
    "checked": true,
    "id": "86c607fbc05217711cc03745a05178884865119a",
    "semantic_title": "machine speech chain with one-shot speaker adaptation",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sim18_interspeech.html": {
    "title": "Domain Adaptation Using Factorized Hidden Layer for Robust Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Domain robustness is a challenging problem for automatic speech recognition (ASR). In this paper, we consider speech data collected for different applications as separate domains and investigate the robustness of acoustic models trained on multi-domain data on unseen domains. Specifically, we use Factorized Hidden Layer (FHL) as a compact low-rank representation to adapt a multi-domain ASR system to unseen domains. Experimental results on two unseen domains show that FHL is a more effective adaptation method compared to selectively fine-tuning part of the network, without dramatically increasing the model parameters. Furthermore, we found that using singular value decomposition to initialize the low-rank bases of an FHL model leads to a faster convergence and improved performance",
    "checked": true,
    "id": "2b3c3066468fc9924f06c94399153fe7e45d76e7",
    "semantic_title": "domain adaptation using factorized hidden layer for robust automatic speech recognition",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wan18_interspeech.html": {
    "title": "Waveform-Based Speaker Representations for Speech Synthesis",
    "volume": "main",
    "abstract": "Speaker adaptation is a key aspect of building a range of speech processing systems, for example personalised speech synthesis. For deep-learning based approaches, the model parameters are hard to interpret, making speaker adaptation more challenging. One widely used method to address this problem is to extract a fixed length vector as speaker representation and use this as an additional input to the task-specific model. This allows speaker-specific output to be generated, without modifying the model parameters. However, the speaker representation is often extracted in a task-independent fashion. This allows the same approach to be used for a range of tasks, but the extracted representation is unlikely to be optimal for the specific task of interest. Furthermore, the features from which the speaker representation is extracted are usually pre-defined, often a standard speech representation. This may limit the available information that can be used. In this paper, an integrated optimisation framework for building a task specific speaker representation, making use of all the available information, is proposed. Speech synthesis is used as the example task. The speaker representation is derived from raw waveform, incorporating text information via an attention mechanism. This paper evaluates and compares this framework with standard task-independent forms",
    "checked": true,
    "id": "33a2bcf1c402c6883ff8e6d92419f8bcb803321e",
    "semantic_title": "waveform-based speaker representations for speech synthesis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yanagita18_interspeech.html": {
    "title": "Incremental TTS for Japanese Language",
    "volume": "main",
    "abstract": "Simultaneous lecture translation requires speech to be translated in real time before the speaker has spoken an entire sentence since a long delay will create difficulties for the listeners trying to follow the lecture. The challenge is to construct a full-fledged system with speech recognition, machine translation and text-to-speech synthesis (TTS) components that could produce high-quality speech translations on the fly. Specifically for a TTS, this poses problems as a conventional framework commonly requires the language-dependent contextual linguistics of a full sentence to produce a natural-sounding speech waveform. Several studies have proposed ways for an incremental TTS (ITTS), in which it can estimate the target prosody from only partial knowledge of the sentence. However, most investigations are being done only in French, English and German. French is a syllable-timed language and the others are stress-timed languages. The Japanese language, which is a mora-timed language, has not been investigated so far. In this paper, we evaluate the quality of Japanese synthesized speech based on various linguistic and temporal incremental units. Experimental results reveal that an accent phrase incremental unit (a group of moras) is essential for a Japanese ITTS as a trade-off between quality and synthesis units",
    "checked": true,
    "id": "56931f63d4140982cdf608789090f5fb402906dc",
    "semantic_title": "incremental tts for japanese language",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fu18_interspeech.html": {
    "title": "Transfer Learning Based Progressive Neural Networks for Acoustic Modeling in Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "The fundamental frequency and the spectrum parameters of the speech are correlated thus one of their learned mapping from the linguistic features can be leveraged to help determine the other. The conventional methods treated all the acoustic features as one stream for acoustic modeling. And the multi-task learning methods were applied to acoustic modeling with several targets in a global cost function. To improve the accuracy of the acoustic model, the progressive deep neural networks (PDNN) is applied for acoustic modeling in statistical parametric speech synthesis (SPSS) in our method. Each type of the acoustic features is modeled in different sub-networks with its own cost function and the knowledge transfers through lateral connections. Each sub-network in the PDNN can be trained step by step to reach its own optimum. Experiments are conducted to compare the proposed PDNN-based SPSS system with the standard DNN methods. The multi-task learning (MTL) method is also applied to the structure of PDNN and DNN as the contrast experiment of the transfer learning. The computational complexity, prediction sequences and quantity of hierarchies of the PDNN are investigated. Both objective and subjective experimental results demonstrate the effectiveness of the proposed technique",
    "checked": true,
    "id": "ae15563b2cfcc6a06876b48b40bad43c4e81d5d0",
    "semantic_title": "transfer learning based progressive neural networks for acoustic modeling in statistical parametric speech synthesis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hwang18_interspeech.html": {
    "title": "A Unified Framework for the Generation of Glottal Signals in Deep Learning-based Parametric Speech Synthesis Systems",
    "volume": "main",
    "abstract": "In this paper, we propose a unified training framework for the generation of glottal signals in deep learning (DL)-based parametric speech synthesis systems. The glottal vocoding-based speech synthesis system, especially the modeling-by-generation (MbG) structure that we proposed recently, significantly improves the naturalness of synthesized speech by faithfully representing the noise component of the glottal excitation with an additional DL structure. Because the MbG method introduces a multistage processing pipeline, however, its training process is complicated and inefficient. To alleviate this problem, we propose a unified training approach that directly generates speech parameters by merging all the required models, such as acoustic, glottal and noise models into a single unified network. Considering the fact that noise analysis should be performed after training the glottal model, we also propose a stochastic noise analysis method that enables noise modeling to be included in the unified training process by iteratively analyzing the noise component in every epoch. Both objective and subjective test results verify the superiority of the proposed algorithm compared to conventional methods",
    "checked": true,
    "id": "f51afd294e32f58ffd06593f1c27da6bc7a171b3",
    "semantic_title": "a unified framework for the generation of glottal signals in deep learning-based parametric speech synthesis systems",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lee18c_interspeech.html": {
    "title": "Acoustic Modeling Using Adversarially Trained Variational Recurrent Neural Network for Speech Synthesis",
    "volume": "main",
    "abstract": "In this paper, we propose a variational recurrent neural network (VRNN) based method for modeling and generating speech parameter sequences. In recent years, the performance of speech synthesis systems has been improved over conventional techniques thanks to deep learning-based acoustic models. Among the popular deep learning techniques, recurrent neural networks (RNNs) has been successful in modeling time-dependent sequential data efficiently. However, due to the deterministic nature of RNNs prediction, such models do not reflect the full complexity of highly structured data, like natural speech. In this regard, we propose adversarially trained variational recurrent neural network (AdVRNN) which use VRNN to better represent the variability of natural speech for acoustic modeling in speech synthesis. Also, we apply adversarial learning scheme in training AdVRNN to overcome oversmoothing problem. We conducted comparative experiments for the proposed VRNN with the conventional gated recurrent unit which is one of RNNs, for speech synthesis system. It is shown that the proposed AdVRNN based method performed better than the conventional GRU technique",
    "checked": true,
    "id": "b8201788e36789b84bd5d3a969858d9f6760a934",
    "semantic_title": "acoustic modeling using adversarially trained variational recurrent neural network for speech synthesis",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zheng18b_interspeech.html": {
    "title": "On the Application and Compression of Deep Time Delay Neural Network for Embedded Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "Acoustic models based on long short-term memory (LSTM) recurrent neural networks (RNNs) were applied to statistical parametric speech synthesis (SPSS) and shown significant improvements. However, the model complexity and inference time cost of RNNs are much higher than feed-forward neural networks (FNN) due to the sequential nature of the learning algorithm, thus limiting its usage in many runtime applications. In this paper, we explore a novel application of deep time delay neural network (TDNN) for embedded SPSS, which requires low disk footprint, memory and latency. The TDNN could model long short-term temporal dependencies with inference cost comparable to standard FNN. Temporal subsampling enabled by TDNN could reduce computational complexity. Then we compress deep TDNN using singular value decomposition (SVD) to further reduce model complexity, which are motivated by the goal of building embedded SPSS systems which can be run efficiently on mobile devices. Both objective and subjective experimental results show that, the proposed deep TDNN with SVD compression could generate synthesized speech with better speech quality than FNN and comparable speech quality to LSTM, while drastically reduce model complexity and speech parameter generation time",
    "checked": true,
    "id": "efbaee3b1d55fdfc773e736f029eea8f3c226ec8",
    "semantic_title": "on the application and compression of deep time delay neural network for embedded statistical parametric speech synthesis",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tzinis18_interspeech.html": {
    "title": "Integrating Recurrence Dynamics for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "We investigate the performance of features that can capture nonlinear recurrence dynamics embedded in the speech signal for the task of Speech Emotion Recognition (SER). Reconstruction of the phase space of each speech frame and the computation of its respective Recurrence Plot (RP) reveals complex structures which can be measured by performing Recurrence Quantification Analysis (RQA). These measures are aggregated by using statistical functionals over segment and utterance periods. We report SER results for the proposed feature set on three databases using different classification methods. When fusing the proposed features with traditional feature sets, e.g., [1], we show an improvement in unweighted accuracy of up to 5.7% and 10.7% on Speaker-Dependent (SD) and Speaker-Independent (SI) SER tasks, respectively, over the baseline [1]. Following a segment-based approach we demonstrate state-of-the-art performance on IEMOCAP using a Bidirectional Recurrent Neural Network",
    "checked": true,
    "id": "33626606c34621f8e04cb47f358f10a6dcb1c91a",
    "semantic_title": "integrating recurrence dynamics for speech emotion recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2018/han18c_interspeech.html": {
    "title": "Towards Temporal Modelling of Categorical Speech Emotion Recognition",
    "volume": "main",
    "abstract": "To model the categorical speech emotion recognition task in a temporal manner, the first challenge arising is how to transfer the categorical label for each utterance into a label sequence. To settle this, we make a hypothesis that an utterance is consisting of emotional and non-emotional segments and these non-emotional segments correspond to silent regions, short pauses, transitions between phonemes, unvoiced phonemes, etc. With this hypothesis, we propose to treat an utterance's label sequence as a chain of two states: the emotional state denoting the emotional frame and Null denoting the non-emotional frame. Then, we exploit a recurrent neural network based connectionist temporal classification model to automatically label and align an utterance's emotional segments with emotional labels, while non-emotional segments with Nulls. Experimental results on the IEMOCAP corpus validate our hypothesis and also demonstrate the effectiveness of our proposed method compared to the state-of-the-art algorithms",
    "checked": true,
    "id": "c661a020ada25093bd6e802d5839dc8499e07822",
    "semantic_title": "towards temporal modelling of categorical speech emotion recognition",
    "citation_count": 38
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kim18_interspeech.html": {
    "title": "Emotion Recognition from Human Speech Using Temporal Information and Deep Learning",
    "volume": "main",
    "abstract": "Emotion recognition by machine is a challenging task, but it has great potential to make empathic human-machine communications possible. In conventional approaches that consist of feature extraction and classifier stages, extensive studies have devoted their effort to developing good feature representations, but relatively little effort was made to make proper use of the important temporal information in these features. In this paper, we propose a model combining features known to be useful for emotion recognition and deep neural networks to exploit temporal information when recognizing emotion status. A benchmark evaluation on EMO-DB demonstrates that the proposed model achieves a state-of-the-art performance of 88.9% recognition rate",
    "checked": true,
    "id": "ec053f3131a9704b6ffb733b9b16d4a9cda489e9",
    "semantic_title": "emotion recognition from human speech using temporal information and deep learning",
    "citation_count": 42
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sridhar18_interspeech.html": {
    "title": "Role of Regularization in the Prediction of Valence from Speech",
    "volume": "main",
    "abstract": "Regularization plays a key role in improving the prediction of emotions using attributes such as arousal, valence and dominance. Regularization is particularly important with deep neural networks (DNNs), which have millions of parameters. While previous studies have reported competitive performance for arousal and dominance, the prediction results for valence using acoustic features are significantly lower. We hypothesize that higher regularization can lead to better results for valence. This study focuses on exploring the role of dropout as a form of regularization for valence suggesting the need for higher regularization. We analyze the performance of regression models for valence, arousal and dominance as a function of the dropout probability. We observe that the optimum dropout rates are consistent for arousal and dominance. However, the optimum dropout rate for valence is higher. To understand the need for higher regularization for valence, we perform an empirical analysis to explore the nature of emotional cues conveyed in speech. We compare regression models with speaker-dependent and speaker-independent partitions for training and testing. The experimental evaluation suggests stronger speaker dependent traits for valence. We conclude that higher regularization is needed for valence to force the network to learn global patterns that generalize across speakers",
    "checked": true,
    "id": "7e4f0425e1d8d3fa9010b6ba6c3e92035a9b04ee",
    "semantic_title": "role of regularization in the prediction of valence from speech",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mangalam18_interspeech.html": {
    "title": "Learning Spontaneity to Improve Emotion Recognition in Speech",
    "volume": "main",
    "abstract": "We investigate the effect and usefulness of spontaneity (i.e. whether a given speech is spontaneous or not) in speech in the context of emotion recognition. We hypothesize that emotional content in speech is interrelated with its spontaneity and use spontaneity classification as an auxiliary task to the problem of emotion recognition. We propose two supervised learning settings that utilize spontaneity to improve speech emotion recognition: a hierarchical model that performs spontaneity detection before performing emotion recognition and a multitask learning model that jointly learns to recognize both spontaneity and emotion. Through various experiments on the well-known IEMOCAP database, we show that by using spontaneity detection as an additional task, significant improvement can be achieved over emotion recognition systems that are unaware of spontaneity. We achieve state-of-the-art emotion recognition accuracy (4-class, 69.1%) on the IEMOCAP database outperforming several relevant and competitive baselines",
    "checked": true,
    "id": "09a42b418d4285a0047ec2c8ff046a39b390bbfb",
    "semantic_title": "learning spontaneity to improve emotion recognition in speech",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lotfian18_interspeech.html": {
    "title": "Predicting Categorical Emotions by Jointly Learning Primary and Secondary Emotions through Multitask Learning",
    "volume": "main",
    "abstract": "Detection of human emotions is an essential part of affect-aware human-computer interaction (HCI). In daily conversations, the preferred way of describing affects is by using categorical emotion labels (e.g., sad, anger, surprise). In categorical emotion classification, multiple descriptors (with different degrees of relevance) can be assigned to a sample. Perceptual evaluations have relied on primary and secondary emotions to capture the ambiguous nature of spontaneous recordings. Primary emotion is the most relevant category felt by the evaluator. Secondary emotions capture other emotional cues also conveyed in the stimulus. In most cases, the labels collected from the secondary emotions are discarded, since assigning a single class label to a sample is preferred from an application perspective. In this work, we take advantage of both types of annotations to improve the performance of emotion classification. We collect the labels from all the annotations available for a sample and generate primary and secondary emotion labels. A classifier is then trained using multitask learning with both primary and secondary emotions. We experimentally show that considering secondary emotion labels during the learning process leads to relative improvements of 7.9% in F1-score for an 8-class emotion classification task",
    "checked": true,
    "id": "d30dbdde4f25d3702348bd471d02b33806d3637a",
    "semantic_title": "predicting categorical emotions by jointly learning primary and secondary emotions through multitask learning",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2018/caudrelier18_interspeech.html": {
    "title": "Picture Naming or Word Reading: Does the Modality Affect Speech Motor Adaptation and Its Transfer?",
    "volume": "main",
    "abstract": "Auditory-motor adaptation and transfer paradigms are increasingly used to explore speech motor control as well as phonological representations underlying speech production. Auditory-motor adaptation is generally assumed to occur at the sensory-motor level. However, few studies suggested that linguistic or contextual factors such as the modality of presentation of stimuli influences adaptation. The present study investigates the influence of the modality of stimuli presentation (written word vs. a picture representing the same word) on auditory-motor adaptation and transfer. In this speech production experiment, speakers' auditory feedback was altered online, inducing adaptation. We contrasted the magnitude of adaptation in these two different modalities and we assessed transfer from /pe/ to the French word /epe/ in the same vs. different modality of presentation, using a mixed 2*2 subject design. The magnitude of adaptation was not different between modalities. This observation contrasts with recent findings showing an effect of the modality (a written word vs. a go signal) on adaptation. Moreover, transfer did occur from one modality to the other and transfer pattern depended on the modality of transfer stimuli. Overall, the results suggest that picture naming and word reading rely on sensory-motor representations that may be linked to contextual (or surface) characteristics",
    "checked": true,
    "id": "69e44278bf55b42f0dfe20536a3a040073cda048",
    "semantic_title": "picture naming or word reading: does the modality affect speech motor adaptation and its transfer?",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/du18_interspeech.html": {
    "title": "Measuring the Band Importance Function for Mandarin Chinese with a Bayesian Adaptive Procedure",
    "volume": "main",
    "abstract": "A speech intelligibility index (SII) based band importance function (BIF) for Mandarin monosyllabic words spoken by a female speaker was derived with an adaptive procedure in this work. The adaptive procedure, namely the quick-band-importance-function (qBIF) procedure, optimized the stimulus on each trial according listeners' performance on proceeding trials in an iterative fashion. This method greatly improved the efficiency of data collection. Test-retest experiments were conducted and confirmed the reliability of this adaptive procedure at a group level. The BIF derived in this work showed generally consistence with the BIF derived with the traditional paradigm with noticeable differences at certain frequencies",
    "checked": true,
    "id": "47fb57e5ff2646f40dbbf6b098cb34e3e9ef98ac",
    "semantic_title": "measuring the band importance function for mandarin chinese with a bayesian adaptive procedure",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shafaeibajestan18_interspeech.html": {
    "title": "Wide Learning for Auditory Comprehension",
    "volume": "main",
    "abstract": "Classical linguistic, cognitive and engineering models for speech recognition and human auditory comprehension posit representations for sounds and words that mediate between the acoustic signal and interpretation. Recent advances in automatic speech recognition have shown, using deep learning, that state-of-the-art performance is obtained without such units. We present a cognitive model of auditory comprehension based on wide rather than deep learning that was trained on 20 to 80 hours of TV news broadcasts. Just as deep network models, our model is an end-to-end system that does not make use of phonemes and phonological wordform representations. Nevertheless, it performs well on the difficult task of single word identification (model accuracy 11.37%, Mozilla DeepSpeech: 4.45%). The architecture of the model is a simple two-layered wide neural network with weighted connections between the acoustic frequency band features as inputs and lexical outcomes (pointers to semantic vectors) as outputs. Model performance shows hardly any degredation when trained on speech in noise rather than on clean speech. Performance was further enhanced by adding a second network to a standard wide network. The present word recognition module is designed to become part of a larger system modeling the comprehension of running speech",
    "checked": true,
    "id": "e601639249a4b2d191166084c21e2c56a9a277d5",
    "semantic_title": "wide learning for auditory comprehension",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tenbosch18_interspeech.html": {
    "title": "Analyzing Reaction Time Sequences from Human Participants in Auditory Experiments",
    "volume": "main",
    "abstract": "Sequences of reaction times (RT) produced by participants in an experiment are not only influenced by the stimuli, but by many other factors as well, including fatigue, attention, experience, IQ, handedness, etc. These confounding factors result in longterm effects (such as a participant's overall reaction capability) and in short- and medium-time fluctuations in RTs (often referred to as ‘local speed effects'). Because stimuli are usually presented in a random sequence different for each participant, local speed effects affect the underlying ‘true' RTs of specific trials in different ways across participants. To be able to focus statistical analysis on the effects of the cognitive process under study, it is necessary to reduce the effect of confounding factors as much as possible. In this paper we propose and compare techniques and criteria for doing so, with focus on reducing (‘filtering') the local speed effects. We show that filtering matters substantially for the significance analyses of predictors in linear mixed effect regression models. The performance of filtering is assessed by the average between-participant correlation between filtered RT sequences and by Akaike's Information Criterion, an important measure of the goodness-of-fit of linear mixed effect regression models",
    "checked": true,
    "id": "32a657a63253161099769262d8379f2fe96be856",
    "semantic_title": "analyzing reaction time sequences from human participants in auditory experiments",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ooster18_interspeech.html": {
    "title": "Prediction of Perceived Speech Quality Using Deep Machine Listening",
    "volume": "main",
    "abstract": "Subjective ratings of speech quality (SQ) are essential for evaluating algorithms for speech transmission and enhancement. In this paper we explore a non-intrusive model for SQ prediction based on the output of a deep neural net (DNN) from a regular automatic speech recognizer. The degradation of phoneme probabilities obtained from the net is quantified with the mean temporal distance proposed earlier for multi-stream ASR. The SQ predicted with this method is compared with average subject ratings from the TCD-VoIP speech quality database that covers several effects of SQ degradation that can occur in VoIP applications such as clipping, packet loss, echo effects, background noise and competing speakers. Our approach is tailored to speech and therefore not applicable when quality is degraded by a competing speaker, which is reflected by an insignificant correlation between model output and subjective SQ. In all other conditions mentioned above, the model reaches an average correlation of r=0.87, which is higher than the correlation achieved with the baseline ITU-T P.563 (r=0.71) and the American National Standard ANIQUE+ (r=0.75). Since the most robust ASR system is not necessarily the best model to predict SQ, we investigate the effect of the amount of training data on quality prediction",
    "checked": true,
    "id": "0ceb73fd62b3e0984f6e429b592f5c06bf602720",
    "semantic_title": "prediction of perceived speech quality using deep machine listening",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kranzusch18_interspeech.html": {
    "title": "Prediction of Subjective Listening Effort from Acoustic Data with Non-Intrusive Deep Models",
    "volume": "main",
    "abstract": "The effort of listening to spoken language is a highly important perceptive measure for the design of speech enhancement algorithms and hearing-aid processing. In previous research, we proposed a model that quantifies the phoneme output probabilities obtained from a deep neural net (DNN), which resulted in accurate predictions for unseen speech samples. However, high correlations between subjective ratings and model output were observed in known noise types, which is an unrealistic assumption in real-life scenarios. This paper explores non-intrusive listening effort prediction in unseen noisy environments. A set of different noise types are used for training a standard automatic speech recognition (ASR) system. Model predictions are produced by measuring the mean temporal distance of phoneme vectors from the DNN and compared to subjective ratings of hearing-impaired and normal-hearing listener responses group in three databases that cover a variety of noise types and signal enhancement algorithms. We obtain an average correlation of 0.88 and outperform three baseline measures in most conditions",
    "checked": true,
    "id": "087fab4f978d7cf9ebcf183d5b6b8a58a8fdad8f",
    "semantic_title": "prediction of subjective listening effort from acoustic data with non-intrusive deep models",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kotti18_interspeech.html": {
    "title": "A Case Study on the Importance of Belief State Representation for Dialogue Policy Management",
    "volume": "main",
    "abstract": "A key component of task-oriented dialogue systems is the belief state representation, since it directly affects the policy learning efficiency. In this paper, we propose a novel, binary, compact, yet scalable belief state representation. We compare the standard verbose belief state representation (268 dimensions) with the domain-independent representation (57 dimensions) and the proposed representation (13 or 4 dimensions). To test those representations, the recently introduced Advantage Actor Critic (A2C) algorithm is exploited. The latter has not been tested before for any representation apart from the verbose one. We study the effect of the belief state representation within A2C under 0%, 15%, 30% and 45% semantic error rate and conclude that the novel binary representation in general outperforms both the domain-independent and the verbose belief state representation. Further, the robustness of the binary representation is tested under more realistic scenarios with mismatched semantic error rates, within the A2C and DQN algorithms. The results indicate that the proposed compact, binary representation performs better or similarly to the other representations, being an efficient and promising alternative to the full belief",
    "checked": true,
    "id": "f35d817325485ba02f35d13ad35441d7d39f895c",
    "semantic_title": "a case study on the importance of belief state representation for dialogue policy management",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hara18_interspeech.html": {
    "title": "Prediction of Turn-taking Using Multitask Learning with Prediction of Backchannels and Fillers",
    "volume": "main",
    "abstract": "We address prediction of turn-taking considering related behaviors such as backchannels and fillers. Backchannels are used by listeners to acknowledge that the current speaker can hold the turn. On the other hand, fillers are used by prospective speakers to indicate a will to take a turn. We propose a turn-taking model based on multitask learning in conjunction with prediction of backchannels and fillers. The multitask learning of LSTM neural networks shared by these tasks allows for efficient and generalized learning and thus improves prediction accuracy. Evaluations with two kinds of dialogue corpora of human-robot interaction demonstrate that the proposed multitask learning scheme outperforms the conventional single-task learning",
    "checked": true,
    "id": "ef375fd8cb807630895f8124ca4dc988c330817b",
    "semantic_title": "prediction of turn-taking using multitask learning with prediction of backchannels and fillers",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bothe18_interspeech.html": {
    "title": "Conversational Analysis Using Utterance-level Attention-based Bidirectional Recurrent Neural Networks",
    "volume": "main",
    "abstract": "Recent approaches for dialogue act recognition have shown that context from preceding utterances is important to classify the subsequent one. It was shown that the performance improves rapidly when the context is taken into account. We propose an utterance-level attention-based bidirectional recurrent neural network (Utt-Att-BiRNN) model to analyze the importance of preceding utterances to classify the current one. In our setup, the BiRNN is given the input set of current and preceding utterances. Our model outperforms previous models that use only preceding utterances as context on the used corpus. Another contribution of our research is a mechanism to discover the amount of information in each utterance to classify the subsequent one and to show that context-based learning not only improves the performance but also achieves higher confidence in the recognition of dialogue acts. We use character- and word-level features to represent the utterances. The results are presented for character and word feature representations and as an ensemble model of both representations. We found that when classifying short utterances, the closest preceding utterances contribute to a higher degree",
    "checked": true,
    "id": "9868db4bd854ea90855abdb1d845c4ecc0327a19",
    "semantic_title": "conversational analysis using utterance-level attention-based bidirectional recurrent neural networks",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ohsugi18_interspeech.html": {
    "title": "A Comparative Study of Statistical Conversion of Face to Voice Based on Their Subjective Impressions",
    "volume": "main",
    "abstract": "Recently, various types of Voice-based User Interfaces (VUIs) including smart speakers have been developed to be on the market. However, many of the VUIs use only synthetic voices to provide information for users. To realize a more natural interface, one feasible solution will be personifying VUIs by adding visual features such as face, but what kind of face is suited to a given quality of voice or what kind of voice quality is suited to a given face? In this paper, we test methods of statistical conversion from face to voice based on their subjective impressions. To this end, six combinations of two types of face features, one type of speech features, and three types of conversion models are tested using a parallel corpus developed based on subjective mapping from face features to voice features. The experimental results show that each subject judge one specific and subject-dependent voice quality as suited to different faces and that the optimal number of mixtures of face features is different from the numbers of mixtures of voice features tested",
    "checked": true,
    "id": "1ae3a26a985fe525b23f080a9e1041ecff0509ad",
    "semantic_title": "a comparative study of statistical conversion of face to voice based on their subjective impressions",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/su18_interspeech.html": {
    "title": "Follow-up Question Generation Using Pattern-based Seq2seq with a Small Corpus for Interview Coaching",
    "volume": "main",
    "abstract": "Interview is a vital part of recruitment process and is especially challenging for the beginners. In an interactive and natural interview, the interviewers would ask follow-up questions or request further elaborations when they are not satisfied with the interviewee's initial response. In this study, as only a small interview corpus is available, a pattern-based sequence to sequence (Seq2seq) model is adopted for follow-up question generation. First, word clustering is employed to automatically transform the question/answer sentences into sentence patterns, in which each sentence pattern is composed of word classes, to decrease the complexity of the sentence structures. Next, the convolutional neural tensor network (CNTN) is used to select a target sentence in an interviewee's answer turn for follow-up question generation. In order to generate the follow-up question pattern, the selected target sentence pattern is fed to a Seq2seq model to obtain the corresponding follow-up question pattern. Then the word class positions in the generated follow-up question sentence pattern is filled in with the words using a word class table obtained from the training corpus. Finally, the n-gram language model is used to rank the candidate follow-up questions and choose the most suitable one as the response to the interviewee. This study collected 3390 follow-up question and answer sentence pairs for training and evaluation. Five-fold cross validation was employed and the experimental results show that the proposed method outperformed the traditional word-based method and achieved a more favorable performance based on a statistical significance test",
    "checked": true,
    "id": "a29e1e4b5e48bd52b0285ddcda5f5c2f14497b05",
    "semantic_title": "follow-up question generation using pattern-based seq2seq with a small corpus for interview coaching",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cervone18_interspeech.html": {
    "title": "Coherence Models for Dialogue",
    "volume": "main",
    "abstract": "Coherence across multiple turns is a major challenge for state-of-the-art dialogue models. Arguably the most successful approach to automatically learning text coherence is the entity grid, which relies on modelling patterns of distribution of entities across multiple sentences of a text. Originally applied to the evaluation of automatic summaries and the news genre, among its many extensions, this model has also been successfully used to assess dialogue coherence. Nevertheless, both the original grid and its extensions do not model intents, a crucial aspect that has been studied widely in the literature in connection to dialogue structure. We propose to augment the original grid document representation for dialogue with the intentional structure of the conversation. Our models outperform the original grid representation on both text discrimination and insertion, the two main standard tasks for coherence assessment across three different dialogue datasets, confirming that intents play a key role in modelling dialogue coherence",
    "checked": true,
    "id": "a3e80b972916c5acea0f0c7b451afaa736cbec74",
    "semantic_title": "coherence models for dialogue",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ke18_interspeech.html": {
    "title": "Indian Languages ASR: A Multilingual Phone Recognition Framework with IPA Based Common Phone-set, Predicted Articulatory Features and Feature fusion",
    "volume": "main",
    "abstract": "In this study, a multilingual phone recognition system for four Indian languages - Kannada, Telugu, Bengali and Odia - is described. International phonetic alphabets are used to derive the transcription. Multilingual Phone Recognition System (MPRS) is developed using the state-of-the-art DNNs. The performance of MPRS is improved using the Articulatory Features (AFs). DNNs are used to predict the AFs for place, manner, roundness, frontness and height AF groups. Further, the MPRS is also developed using oracle AFs and their performance is compared with that of predicted AFs. Oracle AFs are used to set the best performance realizable by AFs predicted from MFCC features by DNNs. In addition to the AFs, we have also explored the use of phone posteriors to further boost the performance of MPRS.We show that oracle AFs by feature fusion with MFCCs offer a remarkably low target of PER of 10.4%, which is 24.7% absolute reduction compared to baseline MPRS with MFCCs alone. The best performing system using predicted AFs has shown 2.8% reduction in absolute PER (8% reduction in relative PER) compared to baseline MPRS",
    "checked": true,
    "id": "93c3912b648975837d7327be4a46d930dd4061fa",
    "semantic_title": "indian languages asr: a multilingual phone recognition framework with ipa based common phone-set, predicted articulatory features and feature fusion",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2018/raza18_interspeech.html": {
    "title": "Rapid Collection of Spontaneous Speech Corpora Using Telephonic Community Forums",
    "volume": "main",
    "abstract": "We present a novel technique for rapid collection of spontaneous speech data over mobile phone channel using telephonic community forums. Our public forum allows users to post audio messages, listen to messages posted by others, post votes and audio comments and share content with friends through subsidized phone calls. The entertainment aspects and sharing features of the forum lead to its viral spread in Pakistan. Within 8 months, it reached 11,017 users and gathered 1,207 hours of speech data comprising 57,454 audio-posts and 130,685 audio-comments, spanning Urdu and 9 regional languages. We trained an ASR using just 9.5 hours of the corpus to obtain 24.19% WER. Community forums automatically overcome common spontaneous speech data collection challenges like speaker recruitment, natural speech elicitation, content diversity, informed consent, sampling real-world ambient noise and reach (for geographically remote linguistic communities). This technique is especially useful for gathering speech corpora for under-resourced languages hence enabling the development of speech recognition, keyword spotting, speaker ID and noise classification systems (among others) for such languages. It also allows rapid, automatic preservation of spoken languages and oral aspects of culture. This technique can be extended to collect speech data for endangered languages, oral cultures and linguistic minorities",
    "checked": true,
    "id": "fabf72a1381f8a67d9da98e427325b42e5a10a09",
    "semantic_title": "rapid collection of spontaneous speech corpora using telephonic community forums",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2018/murthy18_interspeech.html": {
    "title": "Effect of TTS Generated Audio on OOV Detection and Word Error Rate in ASR for Low-resource Languages",
    "volume": "main",
    "abstract": "Out-of-Vocabulary (OOV) detection and recovery is an important aspect of reducing Word Error Rate (WER) in Automatic Speech Recognition (ASR). In this paper, we evaluate the effect on WER for a low-resource language ASR system using OOV detection and recovery. We use a small seed corpus of continuous speech and improve the vocabulary by incorporating the detected OOV words. We use a syllable-model to detect and learn OOV words and, augment the word-model with these words leading to improved recognition. Our research investigates the effect on OOV detection and recovery after adding missing syllable sounds in the syllable model using a Text-to-Speech (TTS) system. Our experiments are conducted using 5 hours of continuous speech Kannada corpus. We use an already available Festival TTS for Hindi to generate Kannada speech. Our initial experiments report an improvement in OOV detection due to addition of missing syllable sounds using a cross-lingual TTS system",
    "checked": true,
    "id": "d8bc1035e0ecee4e6b88f222e7771987df041eaf",
    "semantic_title": "effect of tts generated audio on oov detection and word error rate in asr for low-resource languages",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/patel18_interspeech.html": {
    "title": "Development of Large Vocabulary Speech Recognition System with Keyword Search for Manipuri",
    "volume": "main",
    "abstract": "Research in Automatic Speech Recognition (ASR) has witnessed a steep improvement in the past decade (especially for English language) where the variety and amount of training data available is huge. In this work, we develop an ASR and Keyword Search (KWS) system for Manipuri, a low-resource Indian Language. Manipuri (also known as Meitei), is a Tibeto-Burman language spoken predominantly in Manipur (a northeastern state of India). We collect and transcribe telephonic read speech data of 90+ hours from 300+ speakers for the ASR task. Both state-of-the-art Gaussian Mixture-Hidden Markov Model (GMM-HMM) and Deep Neural Network-Hidden Markov Model (DNN-HMM) based architectures are developed as a baseline. Using the collected data, we achieve better performance using DNN-HMM systems, i.e., 13.57% WER for ASR and 7.64% EER for KWS. The KALDI speech recognition tool-kit is used for developing the systems. The Manipuri ASR system along with KWS is integrated as a visual interface for demonstration purpose. Future systems will be improved with more amount of training data and advanced forms of acoustic models and language models",
    "checked": true,
    "id": "4c381e3338260a924a99917c84219b1ba2b2fd09",
    "semantic_title": "development of large vocabulary speech recognition system with keyword search for manipuri",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dey18_interspeech.html": {
    "title": "Robust Mizo Continuous Speech Recognition",
    "volume": "main",
    "abstract": "Mizo is an under-resourced tonal language that is mainly spoken in North-East India. It has 4 canonical tones along with a tone-sandhi. In Mizo language, a majority of the words contain tone information. As a result of that, it exhibits higher acoustic variability like other tonal languages in the world. In this work, we investigate the impact of tonal information on robust Mizo continuous speech recognition (CSR). First, separate baseline CSR systems are developed employing the Mel-frequency cepstral coefficient (MFCC) based acoustic features and salient acoustic modeling paradigms. For further improvement, the tonal information has been incorporated in each of the CSR systems. For this purpose, 3-dimensional tonal features are derived which include pitch, pitch-difference and probability of voicing values. Our experimental study reveals that with the inclusion of tonal information, the robustness of Mizo CSR system gets enhanced across all acoustic modeling paradigms. This trend is attributed to lesser degradation in the fundamental frequency information than the vocal tract information under noisy conditions",
    "checked": true,
    "id": "e23e7f97e22b7f3f436f2e1e4c2d4dc0d62110c2",
    "semantic_title": "robust mizo continuous speech recognition",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chellapriyadharshini18_interspeech.html": {
    "title": "Semi-supervised and Active-learning Scenarios: Efficient Acoustic Model Refinement for a Low Resource Indian Language",
    "volume": "main",
    "abstract": "We address the problem of efficient acoustic-model refinement (continuous retraining) using semi-supervised and active learning for a low resource Indian language, wherein the low resource constraints are having i) a small labeled corpus from which to train a baseline `seed' acoustic model and ii) a large training corpus without orthographic labeling or from which to perform a data selection for manual labeling at low costs. The proposed semi-supervised learning decodes the unlabeled large training corpus using the seed model and through various protocols, selects the decoded utterances with high reliability using confidence levels (that correlate to the WER of the decoded utterances) and iterative bootstrapping. The proposed active learning protocol uses confidence level based metric to select the decoded utterances from the large unlabeled corpus for further labeling. The semi-supervised learning protocols can offer a WER reduction, from a poorly trained seed model, by as much as 50% of the best WER-reduction realizable from the seed model's WER, if the large corpus were labeled and used for acoustic-model training. The active learning protocols allow that only 60% of the entire training corpus be manually labeled, to reach the same performance as the entire data",
    "checked": true,
    "id": "d9bd82987eb058c8cb21300c9e5036325599386f",
    "semantic_title": "semi-supervised and active-learning scenarios: efficient acoustic model refinement for a low resource indian language",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dash18_interspeech.html": {
    "title": "Automatic Speech Recognition with Articulatory Information and a Unified Dictionary for Hindi, Marathi, Bengali and Oriya",
    "volume": "main",
    "abstract": "Despite the continuous progress of Automatic Speech recognition (ASR) technologies, these systems for Indian languages are still in infancy stage due to a multitude of challenges involved, including resource deficiency. This paper addressed this challenge with four Indian languages, Hindi, Marathi, Bengali and Oriya by integrating articulatory information into acoustic features, thereby compensating the low resource property of these languages for improved performance. Articulatory movements were recorded during speech production using an electromagnetic articulograph and trained together with acoustic features to build automatic speech recognizers for these languages. Both speaker-dependent and -independent recognition experiments were conducted by adopting three ASR models: Gaussian Mixture Model (GMM)-Hidden Markov Model (HMM), Deep Neural Network (DNN)-HMM and Long Short Term Memory recurrent neural network (LSTM)-HMM. A cross-language similarity was discerned in both acoustic and articulatory domains in the pairs of Oriya-Bengali and Hindi-Marathi. Based on these observations, a multi-lingual, multi-modal speech recognizer was built by constructing a unified dictionary consisting of common and unique phonemes of all the four languages, which reduced the phoneme error rates",
    "checked": true,
    "id": "54a74105c5e68d3a28646eee5fe6def60bd6e168",
    "semantic_title": "automatic speech recognition with articulatory information and a unified dictionary for hindi, marathi, bengali and oriya",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rouhe18_interspeech.html": {
    "title": "Captaina: Integrated Pronunciation Practice and Data Collection Portal",
    "volume": "main",
    "abstract": "We demonstrate Captaina, computer assisted pronunciation training portal. It is aimed at university students, who read passages aloud and receive automatic feedback based on speech recognition and phoneme classification. Later their teacher can provide more accurate feedback and comments through the portal. The system enables better independent practice. It also acts as a data collection method. We aim to gather both good quality second language speech data with segmentations and the teacher given evaluations of pronunciation",
    "checked": true,
    "id": "50c417046b20d2583f4ecdc486d7c8cbe3be72e8",
    "semantic_title": "captaina: integrated pronunciation practice and data collection portal",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sachdev18_interspeech.html": {
    "title": "auMina™ - Enterprise Speech Analytics",
    "volume": "main",
    "abstract": "This paper gives an overview of a commercially viable product, auMina™ – an Enterprise Speech Analytics solution. It details out the features and capabilities of the product and the different business outcomes which can be derived from this Speech Analytics solution",
    "checked": true,
    "id": "be5cba8cc310da984f470d0c307a38dafe202413",
    "semantic_title": "aumina™ - enterprise speech analytics",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/naresh18_interspeech.html": {
    "title": "HoloCompanion: An MR Friend for EveryOne",
    "volume": "main",
    "abstract": "Chat bots are becoming ubiquitous in our day to day life. The advent of the summer of AI has brought us all in close contact with intelligent agents such as Cortana, Siri and Alexa. We envisage a world, where these bots have there physical existence within the realm of Mixed Reality (MR). We present the first 3D chit-chat bot called the HoloCompanion. This bot has a personality, can chat with anyone and about any topic and has articulated lip, eye and head movements",
    "checked": true,
    "id": "3795ce1702d95910260dc156a7537c34caf31112",
    "semantic_title": "holocompanion: an mr friend for everyone",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sachdev18b_interspeech.html": {
    "title": "akeira™ - Virtual Assistant",
    "volume": "main",
    "abstract": "This paper gives an overview of a commercially viable product, akeira™ – Voice Virtual Assistant. It also details out the features, capabilities and benefits of this intuitive Virtual Assistant",
    "checked": true,
    "id": "03a4039f8287b9da69551e1a14cbe48bfa0f8a1d",
    "semantic_title": "akeira™ - virtual assistant",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/maruthachalam18_interspeech.html": {
    "title": "Brain-Computer Interface using Electroencephalogram Signatures of Eye Blinks",
    "volume": "main",
    "abstract": "The objective of this work is to develop a personalized eye blink based communicator device. The eye blink is detected using a single channel Electroencephalogram (EEG) system with a Bluetooth interface. Eye blinks have predominant signatures in EEG. Different patterns based on these signatures are used to map alphanumeric characters on a virtual keyboard and words are generated. Voice module is incorporated into the app on the android device for better accessibility of the device by including Text To Speech synthesizer (TTS) at the back-end to produce speech output",
    "checked": true,
    "id": "d90ada39027fe73756992d644522546fe5ea3fd5",
    "semantic_title": "brain-computer interface using electroencephalogram signatures of eye blinks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ajili18_interspeech.html": {
    "title": "Voice Comparison and Rhythm: Behavioral Differences between Target and Non-target Comparisons",
    "volume": "main",
    "abstract": "It is common to see voice recordings being presented as a forensic trace in court. Generally, a forensic expert is asked to analyze both suspect and criminal's voice samples in order to indicate whether the evidence supports the prosecution (same-speaker) or defence (different-speakers) hypotheses. This process is known as Forensic Voice Comparison (FVC). Since the emergence of the DNA typing model, the likelihood-ratio (LR) framework has become the golden standard in forensic sciences. The LR not only supports one of the hypotheses but also quantifies the strength of its support. However, the LR accepts some practical limitations due to its estimation process itself. It is particularly true when Automatic Speaker Recognition (ASpR) systems are considered as they are outputting a score in all situations regardless of the case specific conditions. Indeed, several factors are not taken into account by the estimation process like the quality and quantity of information in both voice recordings, their phonological content or also the speakers intrinsic characteristics. In our recent study, we showed the importance of the phonemic content and we highlighted interesting differences between inter-speakers effects and intra-speaker's ones. In this article, we wish to take our previous analysis a step farther and investigate the impact of rhythm variation separately on target and non-target trials",
    "checked": true,
    "id": "80c6e0da82816f2c19345d323a196f6923f3983a",
    "semantic_title": "voice comparison and rhythm: behavioral differences between target and non-target comparisons",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xu18b_interspeech.html": {
    "title": "Co-whitening of I-vectors for Short and Long Duration Speaker Verification",
    "volume": "main",
    "abstract": "An i-vector is a fixed-length and low-rank representation of a speech utterance. It has been used extensively in text-independent speaker verification. Ideally, speech utterances from the same speaker would map to an unique i-vector. However, this is not the case due to some intrinsic and extrinsic factors like physical condition of the speaker, channel difference, noise and notably the duration of speech utterances. In particular, we found that i-vectors extracted from short utterances exhibit larger variance than that of long utterances. To address the problem, we propose a co-whitening approach, taking into account the duration, while maximizing the correlation between the i-vectors of short and long duration. The proposed co-whitening method was derived based on canonical correlation analysis (CCA). Experimental results on NIST SRE 2010 show that co-whitening method is effective in compensating the duration mismatch, leading to a reduction of up to 13.07% in equal error rate (EER)",
    "checked": true,
    "id": "28405abbff887d5a5fce64041a3217430f0dab0f",
    "semantic_title": "co-whitening of i-vectors for short and long duration speaker verification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bahmaninezhad18_interspeech.html": {
    "title": "Compensation for Domain Mismatch in Text-independent Speaker Recognition",
    "volume": "main",
    "abstract": "Domain mismatch continues to be a major research challenge for speaker recognition in naturalistic audio streams. This study presents a new technique for domain mismatch compensation within a text-independent speaker recognition scenario. The proposed method is designed for the NIST speaker recognition evaluation 2016 (SRE16) task, where speakers from training, development and evaluation data belong to different sets of languages. An i-vector/PLDA speaker recognition system is adopted for this study. To address the mismatch problem, we propose to append auxiliary features to the i-vectors. These auxiliary features are adapted representations of the i-vectors to the specific in-domain data; therefore, the new feature vector has two parts: (1) i-vectors which represent speaker identity and (2) auxiliary features which are representations of i-vectors in the in-domain data feature space (and may not contain speaker identity information). This new concatenated feature vector (we call this a-vector) is then post-processed with support vector discriminant analysis (SVDA) for further domain compensation. Evaluations based on the SRE16 confirm the effectiveness of the proposed technique. In terms of minimum Cprimary cost, a-vector outperforms the i-vector consistently. Moreover, comparing to previous single systems introduced for SRE16, we achieved 8.5%-18% improvements in terms of equal error rate",
    "checked": true,
    "id": "4ffa75ca31241084d66743eec3a4e4af0f938a9c",
    "semantic_title": "compensation for domain mismatch in text-independent speaker recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shi18d_interspeech.html": {
    "title": "Joint Learning of J-Vector Extractor and Joint Bayesian Model for Text Dependent Speaker Verification",
    "volume": "main",
    "abstract": "J-vector and joint Bayesian have been proved to be very effective in text dependent speaker verification with short-duration speech. However current state-of-the-art framework often consider training the J-vector extractor and the joint Bayesian classifier separately. Such an approach will result in information loss for j-vector learning and also fail to exploit an end-to-end framework. In this paper we present a integrated approach to text dependent speaker verification, which consists of a siamese deep neural network that takes two variable length speech segments and maps them to the likelihood score and speaker/phrase labels, where the likelihood score as a loss guide is computed by a variant joint Bayesian model. The likelihood loss guide can constrain the j-vector extractor for improving the verification performance. Since the strengths of j-vector and joint Bayesian analysis appear complementary the joint learning significantly outperforms traditional separate training scheme. Our experiments on the the public RSR2015 part I data corpus demonstrate that this new training scheme can produce more discriminative j-vectors and leading to performance improvement on the speaker verification task",
    "checked": true,
    "id": "2c9816967ea5e936ebb2f9c6f70025077312ad0f",
    "semantic_title": "joint learning of j-vector extractor and joint bayesian model for text dependent speaker verification",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shi18c_interspeech.html": {
    "title": "Latent Factor Analysis of Deep Bottleneck Features for Speaker Verification with Random Digit Strings",
    "volume": "main",
    "abstract": "Speaker verification with prompted random digit strings has been a challenging task due to very short test utterance. This work investigates how to combine methods from deep bottleneck features (DBF) and latent factor analysis (LFA) to result in a new state-of-the-art approach for such task. In order to provide a wider temporal context, a stacked DBF is extracted to replace the traditional MFCC feature in the derivation of the supervector representations and leads to a significant improvement for the speaker verification. The LFA is used to model these stacked DBFs in both digit and utterance scales. Based on this learned LFA model, two kinds of supervector representations are extracted for utterance and local digits respectively. Since the strengths of DBF and LFA appear complementary, the combination significantly outperforms either of its components. Experiments have been conducted on the public RSR2015 part III data corpus, the results showed that our approach can achieve 1.40% EER and 1.55% EER on male and female respectively",
    "checked": true,
    "id": "458beb18648aeb5f7b478f5bb692fac887893d40",
    "semantic_title": "latent factor analysis of deep bottleneck features for speaker verification with random digit strings",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chung18b_interspeech.html": {
    "title": "VoxCeleb2: Deep Speaker Recognition",
    "volume": "main",
    "abstract": "The objective of this paper is speaker recognition under noisy and unconstrained conditions. We make two key contributions. First, we introduce a very large-scale speaker recognition dataset collected from open-source media. Using a fully automated pipeline, we curate VoxCeleb2 which contains over a million utterances from over 6,000 speakers. This is several times larger than any publicly available speaker recognition dataset. Second, we develop and compare Convolutional Neural Network (CNN) models and training strategies that can effectively recognise identities from voice under various conditions. The models trained on the VoxCeleb2 dataset surpass the performance of previous works on a benchmark dataset by a significant margin",
    "checked": true,
    "id": "8875ae233bc074f5cd6c4ebba447b536a7e847a5",
    "semantic_title": "voxceleb2: deep speaker recognition",
    "citation_count": 1530
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ramoji18_interspeech.html": {
    "title": "Supervised I-vector Modeling - Theory and Applications",
    "volume": "main",
    "abstract": "Over the last decade, the factor analysis based modeling of a variable length speech utterance into a fixed dimensional vector (termed as i-vector) has been prominently used for many tasks like speaker recognition, language recognition and even in speech recognition. The i-vector model is an unsupervised learning paradigm where the data is initially clustered using a Gaussian Mixture Universal Background Model (GMM-UBM). The adapted means of the Gaussian mixture components are dimensionality reduced using the Total Variability Matrix (TVM) where the latent variables are modeled with a single Gaussian distribution. In this paper, we propose to rework the theory of i-vector modeling using a supervised framework where the speech utterances are associated with a label. Class labels are introduced in the i-vector model using a mixture Gaussian prior. We show that the proposed model is a generalized i-vector model and the conventional i-vector model turns out to be a special case of this model. This model is applied for a language recognition task using the NIST Language Recognition Evaluation (LRE) 2017 dataset. In these experiments, the supervised i-vector model provides significant improvements over the conventional i-vector model (average relative improvements of 5% in terms of C_{avg}",
    "checked": true,
    "id": "bb20b0221a30d7240824ea4a3d826a015814a3c2",
    "semantic_title": "supervised i-vector modeling - theory and applications",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dmitriev18_interspeech.html": {
    "title": "LOCUST - Longitudinal Corpus and Toolset for Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, we set forth a new longitudinal corpus and a toolset in an effort to address the influence of voice-aging on speaker verification. We have examined previous longitudinal research of age-related voice changes as well as its applicability to real world use cases. Our findings reveal that scientists have treated age-related voice changes as a hindrance instead of leveraging it to the advantage of the identity validator. Additionally, we found a significant dearth of publicly available corpora related to both the time span of and the number of participants in audio recordings. We also identified a significant bias toward the development of speaker recognition technologies applicable to government surveillance systems compared to speaker verification systems used in civilian IT security systems. To solve the aforementioned issues, we built an open project with the largest publicly available longitudinal speaker database, which includes 229 speakers with an average talking time exceeding 15 hours spanning across an average of 21 years per speaker. We assembled, cleaned and normalized audio recordings and developed software tools for speech features extractions, all of which we are releasing to the public domain",
    "checked": true,
    "id": "38b0fe219e949db11c7095f6ef1f521391572f2b",
    "semantic_title": "locust - longitudinal corpus and toolset for speaker verification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/madikeri18_interspeech.html": {
    "title": "Analysis of Language Dependent Front-End for Speaker Recognition",
    "volume": "main",
    "abstract": "In Deep Neural Network (DNN) i-vector based speaker recognition systems, acoustic models trained for Automatic Speech Recognition are employed to estimate sufficient statistics for i-vector modeling. The DNN based acoustic model is typically trained on a well-resourced language like English. In evaluation conditions where the enrollment and test data are not in English, as in the NIST SRE 2016 dataset, a DNN acoustic model generalizes poorly. In such conditions, a conventional Universal Background Model/Gaussian Mixture Model (UBM/GMM) based i-vector extractor performs better than the DNN based i-vector system. In this paper, we address the scenario in which one can develop a Automatic Speech Recognizer with limited resources for a language present in the evaluation condition, thus enabling the use of a DNN acoustic model instead of UBM/GMM. Experiments are performed on the Tagalog subset of the NIST SRE 2016 dataset assuming an open training condition. With a DNN i-vector system trained for Tagalog, a relative improvement of 12.1% is obtained over a baseline system trained for English",
    "checked": true,
    "id": "c5594db022ef0cc3bc6d8f9123bec3ada285f990",
    "semantic_title": "analysis of language dependent front-end for speaker recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nandwana18b_interspeech.html": {
    "title": "Robust Speaker Recognition from Distant Speech under Real Reverberant Environments Using Speaker Embeddings",
    "volume": "main",
    "abstract": "This article focuses on speaker recognition using speech acquired using a single distant or far-field microphone in an indoors environment. This study differs from the majority of speaker recognition research, which focuses on speech acquisition over short distances, such as when using a telephone handset or mobile device or far-field microphone arrays, for which beamforming can enhance distant speech signals. We use two large-scale corpora collected by retransmitting speech data in reverberant environments with multiple microphones placed at different distances. We first characterize three different speaker recognition systems ranging from a traditional universal background model (UBM) i-vector system to a state-of-the-art deep neural network (DNN) speaker embedding system with a probabilistic linear discriminant analysis (PLDA) back-end. We then assess the impact of microphone distance and placement, background noise and loudspeaker orientation on the performance of speaker recognition system for distant speech data. We observe that the recently introduced DNN speaker embedding based systems are far more robust compared to i-vector based systems, providing a significant relative improvement of up to 54% over the baseline UBM i-vector system and 45.5% over prior DNN-based speaker recognition technology",
    "checked": true,
    "id": "2c0944003ef45190fddca06b78ac8c59759a9d27",
    "semantic_title": "robust speaker recognition from distant speech under real reverberant environments using speaker embeddings",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nidadavolu18_interspeech.html": {
    "title": "Investigation on Bandwidth Extension for Speaker Recognition",
    "volume": "main",
    "abstract": "In this work, we investigate training speaker recognition systems on wideband (WB) features and compare their performance with narrowband (NB) baselines. NIST speaker recognition evaluations have mainly driven speaker recognition research in the past years. Because of the target application of these evaluations, most data available to train speaker recognition systems is NB telephone speech. Meanwhile, WB data have been more scarce not being enough to train factor analysis and PLDA models. Thus, the usual practice when dealing with WB speech consists in downsampling the signal to 8 kHz, which implies potential loss of useful information. Instead, we experimented upsampling the training telephone data and leaving the WB data unchanged. We adopt two techniques to upsample telephone data: (1) using a feed-forward neural network, termed Bandwidth Extension (BWE) network, to predict WB features given NB features as input; and (2) using basic upsampling with a low-pass filter interpolator. While the former intends to estimate the high frequency information, the latter does not. The upsampled features are used to train state-of-the art i-vector and recently proposed x-vector models. We evaluated the systems on Speakers In The Wild (SITW) database obtaining 11.5% relative improvement in detection cost function (DCF) with x-vector model",
    "checked": true,
    "id": "204abd534d69efa728a4c2ff5d1f212431890393",
    "semantic_title": "investigation on bandwidth extension for speaker recognition",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2018/muckenhirn18_interspeech.html": {
    "title": "On Learning Vocal Tract System Related Speaker Discriminative Information from Raw Signal Using CNNs",
    "volume": "main",
    "abstract": "In a recent work, we have shown that speaker verification systems can be built where both features and classifiers are directly learned from the raw speech signal with convolutional neural networks (CNNs). In this framework, the training phase also decides the block processing through cross validation. It was found that the first convolution layer, which processes about 20 ms speech, learns to model fundamental frequency information. In the present paper, inspired from speech recognition studies, we build further on that framework to design a CNN-based system, which models sub-segmental speech (about 2ms speech) in the first convolution layer, with an hypothesis that such a system should learn vocal tract system related speaker discriminative information. Through experimental studies on Voxforge corpus and analysis on American vowel dataset, we show that the proposed system (a) indeed focuses on formant regions, (b) yields competitive speaker verification system and (c) is complementary to the CNN-based system that models fundamental frequency information",
    "checked": true,
    "id": "b74a3a83b0c2e575029bec71242971c4343abbc8",
    "semantic_title": "on learning vocal tract system related speaker discriminative information from raw signal using cnns",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kumar18b_interspeech.html": {
    "title": "On Convolutional LSTM Modeling for Joint Wake-Word Detection and Text Dependent Speaker Verification",
    "volume": "main",
    "abstract": "The task of personalized keyword detection system which also performs text dependent speaker verification (TDSV) has received substantial interest recently. Conventional approaches to this task involve the development of the TDSV and wake-up-word detection systems separately. In this paper, we show that TDSV and keyword spotting (KWS) can be jointly modeled using the convolutional long short term memory (CLSTM) model architecture, where an initial convolutional feature map is further processed by a LSTM recurrent network. Given a small amount of training data for developing the CLSTM system, we show that the model provides accurate detection of the presence of the keyword in spoken utterance. For the TDSV task, the MTL model can be well regularized using the CLSTM training examples for personalized wake up task. The experiments are performed for KWS wake up detection and TDSV using the combined speech recordings from Wall Street Journal (WSJ) and LibriSpeech corpus. In these experiments with multiple keywords, we illustrate that the proposed approach of MTL significantly improves the performance of previously proposed neural network based text dependent SV systems. We also experimentally illustrate that the CLSTM model provides significant improvements over previously proposed keyword detection systems as well (average relative improvements of 30% over previous approaches)",
    "checked": true,
    "id": "bea8c366ebc5b0afc70f1b8d39030365bbcb4e52",
    "semantic_title": "on convolutional lstm modeling for joint wake-word detection and text dependent speaker verification",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bai18_interspeech.html": {
    "title": "Cosine Metric Learning for Speaker Verification in the I-vector Space",
    "volume": "main",
    "abstract": "It is known that the equal-error-rate (EER) performance of a speaker verification system is determined by the overlap region of the decision scores of true and imposter trials. Also, the cosine similarity scores of the true or imposter trials produced by the state-of-the-art i-vector front-end approximate to a Gaussian distribution and the overlap region of the two classes of trials depends mainly on their between-class distance. Motivated by the above facts, this paper presents a cosine similarity learning (CML) framework for speaker verification, which combines classical compensation techniques and the cosine similarity scoring for improving the EER performance. CML minimizes the overlap region by enlarging the between-class distance while introducing a regularization term to control the within-class variance, which is initialized by a traditional channel compensation technique such as linear discriminant analysis. Experiments are carried out to compare the proposed CML framework with several traditional channel compensation baselines on the NIST speaker recognition evaluation data sets. The results show that CML outperforms all the studied initialization compensation techniques",
    "checked": true,
    "id": "8e37d3c776854c7d778516debaeea4f19417c1be",
    "semantic_title": "cosine metric learning for speaker verification in the i-vector space",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jati18_interspeech.html": {
    "title": "An Unsupervised Neural Prediction Framework for Learning Speaker Embeddings Using Recurrent Neural Networks",
    "volume": "main",
    "abstract": "This paper presents an unsupervised training framework for learning a speaker-specific embedding using a Neural Predictive Coding (NPC) technique. We employ a Recurrent Neural Network (RNN) trained on unlabeled audio with multiple and unknown speaker change points. We assume short-term speaker stationarity and hence that speech frames in close temporal proximity originated from a single speaker. In contrast, two random short speech segments from different audio streams are assumed to originate from two different speakers. Based on this hypothesis, a binary classification scenario of predicting whether an input pair of short speech segments comes from the same speaker or not, is developed. An RNN based deep siamese network is trained and the resulting embeddings, extracted from a hidden layer representation of the network, are employed as speaker embeddings. The experimental results on speaker change points detection show the efficacy of the proposed method to learn short-term speaker-specific features. We also show the consistency of these features via a simple statistics-based utterance-level speaker classification task. The proposed method outperforms the MFCC baseline for speaker change detection and both MFCC and i-vector baselines for speaker classification",
    "checked": true,
    "id": "c436324d5b98f6fa44d03e66525d321e714c3a79",
    "semantic_title": "an unsupervised neural prediction framework for learning speaker embeddings using recurrent neural networks",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pandey18c_interspeech.html": {
    "title": "A New Framework for Supervised Speech Enhancement in the Time Domain",
    "volume": "main",
    "abstract": "This work proposes a new learning framework that uses a loss function in the frequency domain to train a convolutional neural network (CNN) in the time domain. At the training time, an extra operation is added after the speech enhancement network to convert the estimated signal in the time domain to the frequency domain. This operation is differentiable and is used to train the system with a loss in the frequency domain. This proposed approach replaces learning in the frequency domain, i.e., short-time Fourier transform (STFT) magnitude estimation, with learning in the original time domain. The proposed method is a spectral mapping approach in which the CNN first generates a time domain signal then computes its STFT that is used for spectral mapping. This way the CNN can exploit the additional domain knowledge about calculating the STFT magnitude from the time domain signal. Experimental results demonstrate that the proposed method substantially outperforms the other methods of speech enhancement. The proposed approach is easy to implement and applicable to related speech processing tasks that require spectral mapping or time-frequency (T-F) masking",
    "checked": true,
    "id": "2f412125b71c6b0929e45024def0567b2abf6009",
    "semantic_title": "a new framework for supervised speech enhancement in the time domain",
    "citation_count": 51
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sadasivan18_interspeech.html": {
    "title": "Speech Enhancement Using the Minimum-probability-of-error Criterion",
    "volume": "main",
    "abstract": "We propose a novel speech denoising framework by minimizing the probability of error (PE), which measures the deviation probability of the estimate from its true value. To develop the minimum PE (MPE) criterion, one requires the knowledge of the noise probability density function (p.d.f.), which may not be available in a parametric form in speech denoising applications. Therefore, we adopt two approaches for modeling the noise p.d.f.: (i) Gaussian modeling based on adaptive variance estimation; and (ii) a Gaussian mixture model (GMM) in view of its approximation capabilities. We consider discrete cosine transform (DCT) domain shrinkage, where the optimum shrinkage parameter is obtained by minimizing an estimate of the PE. A performance assessment for real-world noise types shows that for input signal-to-noise ratios (SNR) greater than 5 dB, the proposed MPE-based point-wise shrinkage estimators outperform three benchmark techniques in terms of segmental SNR and short-time objective intelligibility (STOI) scores",
    "checked": true,
    "id": "11444c325334d4cf942539a55ad9df67283e2416",
    "semantic_title": "speech enhancement using the minimum-probability-of-error criterion",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/papadopoulos18_interspeech.html": {
    "title": "Exploring the Relationship between Conic Affinity of NMF Dictionaries and Speech Enhancement Metrics",
    "volume": "main",
    "abstract": "Nonnegative Matrix Factorization(NMF) has been successfully used in speech enhancement. In the training phase NMF produces speech and noise dictionaries, whose elements are non-negative, while in the testing phase it estimates a non-negative activation matrix to express the enhanced speech signal as a conic combination of those dictionaries. This nonnegativity property enables us to interpret them as convex polyhedral cones that lie in the positive orthant. Conic affinity could be useful when designing NMF-based systems for unseen noise conditions, which operate by selecting an appropriate noise dictionary amongst a pool of potential candidates. To that end, we examine two conic affinity measures, one based on cosine similarity, while the other is based on euclidean distance from a point to a cone. Moreover, we construct an algorithm to show that conic affinity correlates with speech enhancement performance metrics",
    "checked": true,
    "id": "550226ed53080f1618fc50f5b81273e4b1b2361f",
    "semantic_title": "exploring the relationship between conic affinity of nmf dictionaries and speech enhancement metrics",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18e_interspeech.html": {
    "title": "Using Shifted Real Spectrum Mask as Training Target for Supervised Speech Separation",
    "volume": "main",
    "abstract": "Deep learning-based speech separation has been widely studied in recent years. Most of these kind approaches focus on recovering the magnitude spectrum of the target speech, but ignore the phase estimation. Recently, a method called shifted real spectrum (SRS) is proposed. Unlike the short-time Fourier transform (STFT), the SRS contains only real components which encode the phase information. In this paper, we propose several SRS-based masks and use them as the training target of deep neural networks. Experimental results show that the proposed target outperforms the commonly used masks computed on STFT in general",
    "checked": true,
    "id": "bb4f91f566808017e331ce35608731b2806d8b00",
    "semantic_title": "using shifted real spectrum mask as training target for supervised speech separation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/srinivas18_interspeech.html": {
    "title": "Enhancement of Noisy Speech Signal by Non-Local Means Estimation of Variational Mode Functions",
    "volume": "main",
    "abstract": "In this paper, a speech enhancement approach exploiting the efficacy of non-local means (NLM) estimation and variational mode decomposition (VMD) is proposed. The NLMestimation is effective in removing noises whenever non-local similarities are present among the samples of the signal under consideration. However, it suffers from the issue of under-averaging in those regions where amplitude and frequency variations are abrupt. Since speech is a non-stationary signal, the magnitude and frequency vary over the time. Consequently, NLM is not that effective in removing the noise components from the speech signal as observed in the case of image enhancement. To address this issue, the noisy speech signal is first decomposed into variational mode functions (VMFs) using VMD. Each of the VMFs represents a small portion of the overall frequency components of the signal. The VMFs are then combined into different groups depending on their similarities to reduce computational cost. Next, the non-local similarity present in each group of VMFs is exploited for an effective speech enhancement through NLM estimation. The enhancement performance of the proposed method is compared with two existing speech enhancement techniques. The experimental results presented in this study show that, the proposed method provides better speech enhancement performance",
    "checked": true,
    "id": "3205e64ad60ff39a28a8e29fe197ca06fa918025",
    "semantic_title": "enhancement of noisy speech signal by non-local means estimation of variational mode functions",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pallavi18_interspeech.html": {
    "title": "Phase-locked Loop (PLL) Based Phase Estimation in Single Channel Speech Enhancement",
    "volume": "main",
    "abstract": "Conventional speech enhancement techniques are based on the modification of noisy spectral magnitude. In the reconstruction of the enhanced signal, noisy phase is combined with the modified noisy spectral magnitude. Recent studies on the importance of phase in enhancement process shows that the clean speech phase improves the quality of the enhanced signal. This work focused on Phase-Locked Loop (PLL) based time-domain approach for estimating the clean speech phase from noisy speech signal. The proposed technique is compared with the conventional approaches where noisy phase is used in the reconstruction of the enhanced signal. Here, Log-Likelihood Ratio (LLR), Weighted Spectral Slope (WSS) distance and Perceptual Evaluation of Speech Quality (PESQ) are used as performance measures. From experimental results, it is observed that the speech quality and intelligibility improved significantly with the proposed method over existing methods",
    "checked": true,
    "id": "c5d5e2375cd2642f51cd96ce0643525b0075f3f0",
    "semantic_title": "phase-locked loop (pll) based phase estimation in single channel speech enhancement",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/meng18b_interspeech.html": {
    "title": "Cycle-Consistent Speech Enhancement",
    "volume": "main",
    "abstract": "Feature mapping using deep neural networks is an effective approach for single-channel speech enhancement. Noisy features are transformed to the enhanced ones through a mapping network and the mean square errors between the enhanced and clean features are minimized. In this paper, we propose a cycle-consistent speech enhancement (CSE) in which an additional inverse mapping network is introduced to reconstruct the noisy features from the enhanced ones. A cycle-consistent constraint is enforced to minimize the reconstruction loss. Similarly, a backward cycle of mappings is performed in the opposite direction with the same networks and losses. With cycle-consistency, the speech structure is well preserved in the enhanced features while noise is effectively reduced such that the feature mapping network generalizes better to unseen data. In cases where only unparalleled noisy and clean data is available for training, two discriminator networks are used to distinguish the reconstructed clean and noisy features from the real ones. The discrimination losses are jointly optimized with reconstruction losses through adversarial multi-task learning. Evaluated on the CHiME-3 dataset, the proposed CSE achieves 19.60% and 6.69% relative word error rate improvements respectively when using or without using parallel clean and noisy speech data",
    "checked": true,
    "id": "7a11e7e2812dbe5ac72e50acef1296d98079f37c",
    "semantic_title": "cycle-consistent speech enhancement",
    "citation_count": 48
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gabbay18_interspeech.html": {
    "title": "Visual Speech Enhancement",
    "volume": "main",
    "abstract": "When video is shot in noisy environment, the voice of a speaker seen in the video can be enhanced using the visible mouth movements, reducing background noise. While most existing methods use audio-only inputs, improved performance is obtained with our visual speech enhancement, based on an audio-visual neural network. We include in the training data videos to which we added the voice of the target speaker as background noise. Since the audio input is not sufficient to separate the voice of a speaker from his own voice, the trained model better exploits the visual input and generalizes well to different noise types. The proposed model outperforms prior audio visual methods on two public lipreading datasets. It is also the first to be demonstrated on a dataset not designed for lipreading, such as the weekly addresses of Barack Obama",
    "checked": true,
    "id": "f997d69d78af086dec4462e4319c6d241f42c0c1",
    "semantic_title": "visual speech enhancement",
    "citation_count": 114
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sharma18c_interspeech.html": {
    "title": "Implementation of Digital Hearing Aid as a Smartphone Application",
    "volume": "main",
    "abstract": "Hearing aids for persons with sensorineural hearing loss aim to compensate for degraded speech perception caused by frequency-dependent elevation of hearing thresholds, reduced dynamic range, abnormal loudness growth and increased temporal and spectral masking. A digital hearing aid is implemented as a smartphone application as an alternative to ASIC-based hearing aids. The implementation provides user-configurable processing for background noise suppression and dynamic range compression. Speech enhancement technique using spectral subtraction based on geometric approach and noise spectrum estimation based on dynamic quantile tracking is implemented to improve speech perception. To compensate for reduced dynamic range and frequency-dependent elevation of hearing thresholds, a sliding-band dynamic range compression technique is used. Both processing blocks are imple­mented for real-time processing using single FFT-based analysis-synthesis. Implementation as a smartphone applica­tion has been carried out using Nexus 5X with Android 7.1 Nougat OS. A touch-controlled graphical user interface enables the user to fine tune the processing parameters in an inte­ractive and real-time mode. The audio latency is 45 ms, making it suitable for face-to-face communication",
    "checked": true,
    "id": "f43ce695eb4bc2f2d491726d516eec14a58e433a",
    "semantic_title": "implementation of digital hearing aid as a smartphone application",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lee18_interspeech.html": {
    "title": "Bone-Conduction Sensor Assisted Noise Estimation for Improved Speech Enhancement",
    "volume": "main",
    "abstract": "State-of-the-art noise power spectral density (PSD) estimation techniques for speech enhancement utilize the so-called speech presence probability (SPP). However, in highly non-stationary environments, SPP-based techniques could still suffer from inaccurate estimation, leading to significant amount of residual noise or speech distortion. In this paper, we propose to improve speech enhancement by deploying the bone-conduction (BC) sensor, which is known to be relatively insensitive to the environmental noise compared to the regular air-conduction (AC) microphone. A strategy is suggested to utilized the BC sensor characteristics for assisting the AC microphone in better SPP-based noise estimation. To our knowledge, no previous work has incorporated the BC sensor in this noise estimation aspect. Consequently, the proposed strategy can possibly be combined with other BC sensor assisted speech enhancement techniques. We show the feasibility and potential of the proposed method for improving the enhanced speech quality by both objective and subjective tests",
    "checked": true,
    "id": "5b9cc751f644b21c48c4c235964cbfd8b9e63a1a",
    "semantic_title": "bone-conduction sensor assisted noise estimation for improved speech enhancement",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bachhav18_interspeech.html": {
    "title": "Artificial Bandwidth Extension with Memory Inclusion Using Semi-supervised Stacked Auto-encoders",
    "volume": "main",
    "abstract": "Artificial bandwidth extension (ABE) algorithms have been developed to improve quality when wideband devices receive speech signals from narrowband devices or infrastructure. The utilisation of contextual information in the form of dynamic features or explicit memory captured from neighbouring frames is common to ABE research, however the use of additional cues augments complexity and can introduce latency. Previous work shows that unsupervised, linear dimensionality reduction techniques help to reduce complexity. This paper reports a semisupervised, non-linear approach to dimensionality reduction using a stacked auto-encoder. In further contrast to previous work, it operates on raw spectra from which a low dimensional narrowband representation is learned in a data-driven manner. Three different objective speech quality measures show that the new features can be used with a standard regression model to improve ABE performance. Improvements in the mutual information between learned features and missing higher frequency components are also observed whereas improvements in speech quality are corroborated by informal listening tests",
    "checked": true,
    "id": "c94480b6c6f63f5fc4953a58d021f9ec8ccba7f5",
    "semantic_title": "artificial bandwidth extension with memory inclusion using semi-supervised stacked auto-encoders",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/maiti18_interspeech.html": {
    "title": "Large Vocabulary Concatenative Resynthesis",
    "volume": "main",
    "abstract": "Traditional speech enhancement systems reduce noise by modifying the noisy signal, which suffer from two problems: under-suppression of noise and over-suppression of speech. As an alternative, in this paper, we use the recently introduced concatenative resynthesis approach where we replace the noisy speech with its clean resynthesis. The output of such a system can produce speech that is both noise-free and high quality. This paper generalizes our previous small-vocabulary system to large vocabulary. To do so, we employ efficient decoding techniques using fast approximate nearest neighbor (ANN) algorithms. Firstly, we apply ANN techniques on the original small vocabulary task and get 5X speedup. We then apply the techniques to the construction of a large vocabulary concatenative resynthesis system and scale the system up to 12X larger dictionary. We perform listening tests with five participants to measure subjective quality and intelligibility of the output speech",
    "checked": true,
    "id": "85c7e4233d24aeae77c014ddc63de6f1369dd6d4",
    "semantic_title": "large vocabulary concatenative resynthesis",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/syed18b_interspeech.html": {
    "title": "Concatenative Resynthesis with Improved Training Signals for Speech Enhancement",
    "volume": "main",
    "abstract": "Noise reduction in speech signals remains an important area of research with potential for high impact in speech processing domains such as voice communication and hearing prostheses. We extend and demonstrate significant improvements to our previous work in synthesis-based speech enhancement, which performs concatenative resynthesis of speech signals for the production of noiseless, high quality speech. Concatenative resynthesis methods perform unit selection through learned non-linear similarity functions between short chunks of clean and noisy signals. These mappings are learned using deep neural networks (DNN) trained to predict high similarity for the exact chunk of speech that is contained within a chunk of noisy speech and low similarity for all other pairings. We find here that more robust mappings can be learned with a more efficient use of the available data by selecting pairings that are not exact matches, but contain similar clean speech that matches the original in terms of acoustic, phonetic and prosodic content. The resulting output is evaluated on the small vocabulary CHiME2-GRID corpus and outperforms our original baseline system in terms of intelligibility by combining phonetic similarity with similarity of acoustic intensity, fundamental frequency and periodicity",
    "checked": true,
    "id": "e74087f632c2b6600d728c801102d51b27debe19",
    "semantic_title": "concatenative resynthesis with improved training signals for speech enhancement",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rasanen18_interspeech.html": {
    "title": "Comparison of Syllabification Algorithms and Training Strategies for Robust Word Count Estimation across Different Languages and Recording Conditions",
    "volume": "main",
    "abstract": "Word count estimation (WCE) from audio recordings has a number of applications, including quantifying the amount of speech that language-learning infants hear in their natural environments, as captured by daylong recordings made with devices worn by infants. To be applicable in a wide range of scenarios and also low-resource domains, WCE tools should be extremely robust against varying signal conditions and require minimal access to labeled training data in the target domain. For this purpose, earlier work has used automatic syllabification of speech, followed by a least-squares-mapping of syllables to word counts. This paper compares a number of previously proposed syllabifiers in the WCE task, including a supervised bi-directional long short-term memory (BLSTM) network that is trained on a language for which high quality syllable annotations are available (a \"high resource language\") and reports how the alternative methods compare on different languages and signal conditions. We also explore additive noise and varying-channel data augmentation strategies for BLSTM training and show how they improve performance in both matching and mismatching languages. Intriguingly, we also find that even though the BLSTM works on languages beyond its training data, the unsupervised algorithms can still outperform it in challenging signal conditions on novel languages",
    "checked": true,
    "id": "67de9ad28d920c22e8b5611894a2853b8da65d28",
    "semantic_title": "comparison of syllabification algorithms and training strategies for robust word count estimation across different languages and recording conditions",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kelley18_interspeech.html": {
    "title": "A Comparison of Input Types to a Deep Neural Network-based Forced Aligner",
    "volume": "main",
    "abstract": "The present paper investigates the effect of different inputs on the accuracy of a forced alignment tool built using deep neural networks. Both raw audio samples and Mel-frequency cepstral coefficients were compared as network inputs. A set of experiments were performed using the TIMIT speech corpus as training data and its accompanying test data set. The networks consisted of a series of convolutional layers followed by a series of bidirectional long short-term memory (LSTM) layers. The convolutional layers were trained first to act as feature detectors, after which their weights were frozen. Then, the LSTM layers were trained to learn the temporal relations in the data. The current results indicate that networks using raw audio perform better than those using Mel-frequency cepstral coefficients and an off-the-shelf forced aligner. Possible explanations for why the raw audio networks perform better are discussed. We then lay out potential ways to improve the results of the networks and conclude with a comparison of human cognition to network architecture",
    "checked": true,
    "id": "67d5fd8728f92fc13e492c946909eb636b7ec248",
    "semantic_title": "a comparison of input types to a deep neural network-based forced aligner",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jung18_interspeech.html": {
    "title": "Joint Learning Using Denoising Variational Autoencoders for Voice Activity Detection",
    "volume": "main",
    "abstract": "Voice activity detection (VAD) is a challenging task in very low signal-to-noise ratio (SNR) environments. To address this issue, a promising approach is to map noisy speech features to corresponding clean features and to perform VAD using the generated clean features. This can be implemented by concatenating a speech enhancement (SE) and a VAD network, whose parameters are jointly updated. In this paper, we propose denoising variational autoencoder-based (DVAE) speech enhancement in the joint learning framework. Moreover, we feed not only the enhanced feature but also the latent code from the DVAE into the VAD network. We show that the proposed joint learning approach outperforms conventional denoising autoencoder-based joint learning approach",
    "checked": true,
    "id": "bd2199c62f6349ad88d6e58c18f122aba869580f",
    "semantic_title": "joint learning using denoising variational autoencoders for voice activity detection",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dawalatabad18_interspeech.html": {
    "title": "Information Bottleneck Based Percussion Instrument Diarization System for Taniavartanam Segments of Carnatic Music Concerts",
    "volume": "main",
    "abstract": "An approach to diarize taniavartanam segments of a Carnatic music concert is proposed in this paper. Information bottleneck (IB) based approach used for speaker diarization is applied for this task. IB system initializes the segments to be clustered uniformly with fixed duration. The issue with diarization of percussion instruments in taniavartanam is that the stroke rate varies highly across the segments. It can double or even quadruple within a short duration, thus leading to variable information rate in different segments. To address this issue, the IB system is modified to use the stroke rate information to divide the audio into segments of varying durations. These varying duration segments are then clustered using the IB approach which is then followed by Kullback-Leibler hidden Markov model (KL-HMM) based realignment of the instrument boundaries. Performance of the conventional IB system and the proposed system is evaluated on standard Carnatic music dataset. The proposed technique shows a best case absolute improvement of 8.2% over the conventional IB based system in terms of diarization error rate",
    "checked": true,
    "id": "461ae1d917f5ded2115235633a2fa685dc4d0c0d",
    "semantic_title": "information bottleneck based percussion instrument diarization system for taniavartanam segments of carnatic music concerts",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ghosh18_interspeech.html": {
    "title": "Robust Voice Activity Detection Using Frequency Domain Long-Term Differential Entropy",
    "volume": "main",
    "abstract": "We propose a novel voice activity detection (VAD) scheme employing differential entropy at each frequency bin of power spectral estimates of past and present overlapping speech frames. Here, the power spectral estimate is obtained by employing the Bartlett-Welch method. Later, we add entropies across frequency bins and denote this as the frequency domain long-term differential entropy (FLDE). Long-term averaging enhances VAD performance under low signal-to-noise-ratio (SNR). We evaluate the performance of proposed FLDE scheme, considering 12 types of noises and 5 different SNRs which are artificially added to speech samples from the SWITCHBOARD corpus. We present VAD performance of FLDE and compare with existing VAD algorithms, such as ITU-T G.729B, likelihood ratio test, long-term signal variability and long-term spectral flatness measure based algorithms. Finally, we demonstrate that our FLDE-based VAD performs with best average accuracy and speech hit-rate among the VAD algorithms considered for evaluation",
    "checked": true,
    "id": "1a95913796630a31bf4b7535f5927273ad3786c8",
    "semantic_title": "robust voice activity detection using frequency domain long-term differential entropy",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mallidi18_interspeech.html": {
    "title": "Device-directed Utterance Detection",
    "volume": "main",
    "abstract": "In this work, we propose a classifier for distinguishing device-directed queries from background speech in the context of interactions with voice assistants. Applications include rejection of false wake-ups or unintended interactions as well as enabling wake-word free follow-up queries. Consider the example interaction: \"Computer, play music\", \"Computer, reduce the volume\". In this interaction, the user needs to repeat the wake-word (Computer) for the second query. To allow for more natural interactions, the device could immediately re-enter listening state after the first query (without wake-word repetition) and accept or reject a potential follow-up as device-directed or background speech. The proposed model consists of two long short-term memory (LSTM) neural networks trained on acoustic features and automatic speech recognition (ASR) 1-best hypotheses, respectively. A feed-forward deep neural network (DNN) is then trained to combine the acoustic and 1-best embeddings, derived from the LSTMs, with features from the ASR decoder. Experimental results show that ASR decoder, acoustic embeddings and 1-best embeddings yield an equal-error-rate (EER) of 9.3%, 10.9% and 20.1%, respectively. Combination of the features resulted in a 44% relative improvement and a final EER of 5.2%",
    "checked": true,
    "id": "468898123752d2848d59698e65d9f7825ffaea80",
    "semantic_title": "device-directed utterance detection",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ma18_interspeech.html": {
    "title": "Acoustic-Prosodic Features of Tabla Bol Recitation and Correspondence with the Tabla Imitation",
    "volume": "main",
    "abstract": "In the Indian classical drumming tradition, the different strokes on the tabla are named by spoken syllables(bols) in a case of onomatopoeia. The recitation of a tabla composition using vocalic syllables(bols) plays an important role in the oral tradition of pedagogy in North Indian classical music. Previous studies have considered the phonetic features of isolated bol utterances with the corresponding isolated strokes produced on the tabla. In this work, we investigate the acoustic properties of bol recitation beyond the segmental measurements related to the phones or syllables. The recitation of a tabla composition, apart from conveying the basic \"score\" in terms of the sequence of stroke name and onset times, is typically quite expressive in nature, being marked by pitch variations, loudness dynamics and voice quality variations across a sequence or phrase. Given the distinct spaces of acoustic variation of the voice and tabla, we study acoustic-prosodic variations in the recitation and investigate the corresponding (time-aligned) supra-segmental acoustic variations in the drumming. An available large dataset of recordings of selected tabla compositions by an expert tabla player, each aligned with the corresponding bol recitation, is employed in the analyses. We find that while the recitation reliably encodes intensity variations across bols in a cycle, the observed pitch variations are meaningful only for the pitch-varying drum strokes of the left drum",
    "checked": true,
    "id": "2578f29840b13cf2690bc92a8000802deb7c523d",
    "semantic_title": "acoustic-prosodic features of tabla bol recitation and correspondence with the tabla imitation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/krikke18_interspeech.html": {
    "title": "Who Said That? a Comparative Study of Non-negative Matrix Factorization Techniques",
    "volume": "main",
    "abstract": "In noisy environments it is difficult for a computer to understand what a person is saying especially when there are multiple speakers. In this paper we concentrate on separating overlapping speech. Non-negative matrix factorisation (NMF) is a method of doing source separation without needing a lot of data. The choice of cost function can have a significant impact on the performance of NMF. We evaluate NMF using three different cost functions (Euclidean, Itakura-Saito and Kullback-Leibler) including modifications using sparsity, convolution or additional information in the form of the direction of arrival. We conduct this evaluation on three different speech corpora. Adding directional information to NMF in the form of non-negative tensor factorisation (NTF) gives us the best result on the map task and vocalization corpora and the Itakura-Saito cost function performs best on the acoustic-camera corpus. In this paper, we show that the Itakura-Saito cost function is the most robust cost function when the recording contains noise. We do this by applying acoustic evaluation measurements",
    "checked": true,
    "id": "2431a8c38b2c8e33a2db2b093f406607ce14eb29",
    "semantic_title": "who said that? a comparative study of non-negative matrix factorization techniques",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chaudhuri18_interspeech.html": {
    "title": "AVA-Speech: A Densely Labeled Dataset of Speech Activity in Movies",
    "volume": "main",
    "abstract": "Speech activity detection (or endpointing) is an important processing step for applications such as speech recognition, language identification and speaker diarization. Both audio- and vision-based approaches have been used for this task in various settings, often tailored toward end applications. However, much of the prior work reports results in synthetic settings, on task-specific datasets, or on datasets that are not openly available. This makes it difficult to compare approaches and understand their strengths and weaknesses. In this paper, we describe a new dataset which we will release publicly containing densely labeled speech activity in YouTube videos, with the goal of creating a shared, available dataset for this task. The labels in the dataset annotate three different speech activity conditions: clean speech, speech co-occurring with music and speech co-occurring with noise, which enable analysis of model performance in more challenging conditions based on the presence of overlapping noise. We report benchmark performance numbers on AVA-Speech using off-the-shelf, state-of-the-art audio and vision models that serve as a baseline to facilitate future research",
    "checked": true,
    "id": "f52c83a3c6a1b7ffa8752f20e5c3bdab6001c1d2",
    "semantic_title": "ava-speech: a densely labeled dataset of speech activity in movies",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tao18_interspeech.html": {
    "title": "Audiovisual Speech Activity Detection with Advanced Long Short-Term Memory",
    "volume": "main",
    "abstract": "Speech activity detection (SAD) is a key pre-processing step for a speech-based system. The performance of conventional audio-only SAD (A-SAD) systems is impaired by acoustic noise when they are used in practical applications. An alternative approach to address this problem is to include visual information, creating audiovisual speech activity detection (AV-SAD) solutions. In our previous work, we proposed to build an AV-SAD system using bimodal recurrent neural network (BRNN). This framework was able to capture the task-related characteristics in the audio and visual inputs and model the temporal information within and across modalities. The approach relied on long short-term memory (LSTM). Although LSTM can model longer temporal dependencies with the cells, the effective memory of the units is limited to a few frames, since the recurrent connection only considers the previous frame. For SAD systems, it is important to model longer temporal dependencies to capture the semi-periodic nature of speech conveyed in acoustic and orofacial features. This study proposes to implement a BRNN-based AV-SAD system with advanced LSTMs (A-LSTMs), which overcomes this limitation by including multiple connections to frames in the past. The results show that the proposed framework can significantly outperform the BRNN system trained with the original LSTM layers",
    "checked": true,
    "id": "b2bb7e4ccdb436e65a911ba8046ca6ff00ee33be",
    "semantic_title": "audiovisual speech activity detection with advanced long short-term memory",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2018/saha18_interspeech.html": {
    "title": "Towards Automatic Speech Identification from Vocal Tract Shape Dynamics in Real-time MRI",
    "volume": "main",
    "abstract": "Vocal tract configurations play a vital role in generating distinguishable speech sounds, by modulating the airflow and creating different resonant cavities in speech production. They contain abundant information that can be utilized to better understand the underlying speech production mechanism. As a step towards automatic mapping of vocal tract shape geometry to acoustics, this paper employs effective video action recognition techniques, like Long-term Recurrent Convolutional Networks (LRCN) models, to identify different vowel-consonant-vowel (VCV) sequences from dynamic shaping of the vocal tract. Such a model typically combines a CNN based deep hierarchical visual feature extractor with Recurrent Networks, that ideally makes the network spatio-temporally deep enough to learn the sequential dynamics of a short video clip for video classification tasks. We use a database consisting of 2D real-time MRI of vocal tract shaping during VCV utterances by 17 speakers. The comparative performances of this class of algorithms under various parameter settings and for various classification tasks are discussed. Interestingly, the results show a marked difference in the model performance in the context of speech classification with respect to generic sequence or video classification tasks",
    "checked": true,
    "id": "caafb196d05a2c0215ae281fbb4db9ce986be8bf",
    "semantic_title": "towards automatic speech identification from vocal tract shape dynamics in real-time mri",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shi18_interspeech.html": {
    "title": "Structured Word Embedding for Low Memory Neural Network Language Model",
    "volume": "main",
    "abstract": "Neural network language model (NN LM), such as long short term memory (LSTM) LM, has been increasingly popular due to its promising performance. However, the model size of an uncompressed NN LM is still too large to be used in embedded or portable devices. The dominant part of memory consumption of NN LM is the word embedding matrix. Directly compressing the word embedding matrix usually leads to performance degradation. In this paper, a product quantization based structured embedding approach is proposed to significantly reduce memory consumption of word embeddings without hurting LM performance. Here, each word embedding vector is cut into partial embedding vectors which are then quantized separately. Word embedding matrix can then be represented by an index vector and a code-book tensor of the quantized partial embedding vectors. Experiments show that the proposed approach can achieve 10 to 20 times embedding parameter reduction rate with negligible performance loss",
    "checked": true,
    "id": "732804c8c319a948a6fe7dd1a1544562e5a8241a",
    "semantic_title": "structured word embedding for low memory neural network language model",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2018/masumura18_interspeech.html": {
    "title": "Role Play Dialogue Aware Language Models Based on Conditional Hierarchical Recurrent Encoder-Decoder",
    "volume": "main",
    "abstract": "We propose role play dialogue-aware language models (RPDA-LMs) that can leverage interactive contexts in role play multi-turn dialogues for estimating the generative probability of words. Our motivation is to improve automatic speech recognition (ASR) performance in role play dialogues such as contact center dialogues and service center dialogues. Although long short-term memory recurrent neural network based language models (LSTM-RNN-LMs) can capture long-range contexts within an utterance, they cannot utilize sequential interactive information between speakers in multi-turn dialogues. Our idea is to explicitly leverage speakers' roles of individual utterances, which are often available in role play dialogues, for neural language modeling. The RPDA-LMs are represented as a generative model conditioned by a role sequence of a target role play dialogue. We compose the RPDA-LMs by extending hierarchical recurrent encoder-decoder modeling so as to handle the role information. Our ASR evaluation in a contact center dialogue demonstrates that RPDA-LMs outperform LSTM-RNN-LMs and document-context LMs in terms of perplexity and word error rate. In addition, we verify the effectiveness of explicitly taking interactive contexts into consideration",
    "checked": true,
    "id": "d84743b91541907e830f902c59fcf2fd65b94069",
    "semantic_title": "role play dialogue aware language models based on conditional hierarchical recurrent encoder-decoder",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2018/myer18_interspeech.html": {
    "title": "Efficient Keyword Spotting Using Time Delay Neural Networks",
    "volume": "main",
    "abstract": "This paper describes a novel method of live keyword spotting using a two-stage time delay neural network. The model is trained using transfer learning: initial training with phone targets from a large speech corpus is followed by training with keyword targets from a smaller data set. The accuracy of the system is evaluated on two separate tasks. The first is the freely available Google Speech Commands dataset. The second is an in-house task specifically developed for keyword spotting. The results show significant improvements in false accept and false reject rates in both clean and noisy environments when compared with previously known techniques. Furthermore, we investigate various techniques to reduce computation in terms of multiplications per second of audio. Compared to recently published work, the proposed system provides up to 89% savings on computational complexity",
    "checked": true,
    "id": "58c0e480a578b692eed2b19e274033ef2919098e",
    "semantic_title": "efficient keyword spotting using time delay neural networks",
    "citation_count": 35
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yoshida18_interspeech.html": {
    "title": "Automatic DNN Node Pruning Using Mixture Distribution-based Group Regularization",
    "volume": "main",
    "abstract": "In this paper, we address a constrained training for deep neural network-based acoustic model size reduction. While the L2 regularizer is used as a modeling approach to shrinking parameters, we cannot cut down the unimportant parts because it does not assume any group structure. The Group Lasso regularizer is used for the model size reduction approach. Group Lasso can set arbitrary group parameters (e.g. the column vector norms of the parameter matrices) as unimportant parts and make the parameters sparse. Therefore, we can prune the unimportant parameters whose group parameter norm is nearly zero. However, Group Lasso does not suggest a clear rule for separating parameters close to zero and large in the group parameter space and hence is unsuitable for the model size reduction. To solve these problems, we propose a mixture distribution-based regularizer which assumes distributions of norms in the group parameter space. We evaluate our method on a NTT real recorded voice search data containing 1600 hours. Our proposal achieves 27.0% reduction compared to the pruned model by Group Lasso while keeping recognition performance",
    "checked": true,
    "id": "a1bb8b057fa17a7092029f9d92219eb905c61a33",
    "semantic_title": "automatic dnn node pruning using mixture distribution-based group regularization",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tavarone18_interspeech.html": {
    "title": "Conditional-Computation-Based Recurrent Neural Networks for Computationally Efficient Acoustic Modelling",
    "volume": "main",
    "abstract": "The first step in Automatic Speech Recognition (ASR) is a fixed-rate segmentation of the acoustic signal into overlapping windows of fixed length. Although this procedure allows to achieve excellent recognition accuracy, it is far from being computationally efficient, in that it may produce a highly redundant signal (i.e, almost identical spectral vectors may span many observation windows) that converts into computational overload. The reduction of such overload can be very beneficial for application such as offline ASR on mobile devices. In this paper we present a principled way for saving numerical operations during ASR by using conditional-computation methods in deep bidirectional Recurrent Neural Networks (RNNs) for acoustic modelling. The methods rely on learned binary neurons that allow hidden layers to be updated only when necessary or to keep their previous value. We (i) evaluate, for the first time, conditional computation-based recurrent architectures on a speech recognition task and (ii) propose a novel model specifically designed for speech data that inherently builds a multi-scale temporal structure in the hidden layers. Results on the TIMIT dataset show that conditional mechanisms in recurrent architectures can reduce hidden layer updates up to 40% at the cost of about 20% relative phone error rate increase",
    "checked": true,
    "id": "f648f0dda8cf9b3b751293d84ca44379529911fc",
    "semantic_title": "conditional-computation-based recurrent neural networks for computationally efficient acoustic modelling",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/anastasopoulos18_interspeech.html": {
    "title": "Leveraging Translations for Speech Transcription in Low-resource Settings",
    "volume": "main",
    "abstract": "Recently proposed data collection frameworks for endangered language documentation aim not only to collect speech in the language of interest, but also to collect translations into a high-resource language that will render the collected resource interpretable. We focus on this scenario and explore whether we can improve transcription quality under these extremely low-resource settings with the assistance of text translations. We present a neural multi-source model and evaluate several variations of it on three low-resource datasets. We find that our multi-source model with shared attention outperforms the baselines, reducing transcription character error rate by up to 12.3%",
    "checked": true,
    "id": "8f8a64f32b553490084e7e9b7ce86a4a05cb9cc6",
    "semantic_title": "leveraging translations for speech transcription in low-resource settings",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bruguier18_interspeech.html": {
    "title": "Sequence-to-sequence Neural Network Model with 2D Attention for Learning Japanese Pitch Accents",
    "volume": "main",
    "abstract": "Many Japanese text-to-speech (TTS) systems use word-level pitch accents as one of the prosodic features. Combination of a pronunciation dictionary including lexical pitch accents and a statistical model representing the word accent sandhi is often used to predict pitch accents from a text. However, using human transcribers to build the dictionary and training data for the model is tedious and expensive. This paper proposes a neural pitch accent recognition model. This model combines the information from audio and its transcription (word sequence in hiragana characters) via two-dimensional attention and outputs word-level pitch accents. Experimental results show a reduction in the word pitch accent prediction error rate over that with text only. It lowers the load of human annotators when building a pronunciation dictionary. As the approach is general, it can be used to do pronunciation learning in other languages as well",
    "checked": true,
    "id": "99a2fca3b8738cfff59343faaf3ce641a1e5ff9e",
    "semantic_title": "sequence-to-sequence neural network model with 2d attention for learning japanese pitch accents",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ghannay18_interspeech.html": {
    "title": "Task Specific Sentence Embeddings for ASR Error Detection",
    "volume": "main",
    "abstract": "This paper presents a study on the modeling of automatic speech recognition errors at the sentence level. We aim in this study to compensate certain phenomena highlighted by the analysis of the outputs generated by the ASR error detection system we previously proposed. We investigated three different approaches, that are based respectively on the use of sentence embeddings dedicated to ASR error detection task, a probabilistic contextual model and a bidirectional long short term memory (BLSTM) architecture. An approach to build task-specific sentence embeddings is proposed and compared to the Doc2vec approach. Experiments are performed on transcriptions generated by the LIUM ASR system applied to the ETAPE corpus. They show that the proposed sentence embeddings dedicated to ASR error detection achieve better results than generic sentence embeddings and that the integration of task-specific sentence embeddings in our system achieves better results than the probabilistic contextual model and BLSTM models",
    "checked": true,
    "id": "df9a63ac014b9b8a4f6ed46468d0c2f99f4ce76c",
    "semantic_title": "task specific sentence embeddings for asr error detection",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/niehues18_interspeech.html": {
    "title": "Low-Latency Neural Speech Translation",
    "volume": "main",
    "abstract": "Through the development of neural machine translation, the quality of machine translation systems has been improved significantly. By exploiting advancements in deep learning, systems are now able to better approximate the complex mapping from source sentences to target sentences. But with this ability, new challenges also arise. An example is the translation of partial sentences in low-latency speech translation. Since the model has only seen complete sentences in training, it will always try to generate a complete sentence, though the input may only be a partial sentence. We show that NMT systems can be adapted to scenarios where no task-specific training data is available. Furthermore, this is possible without losing performance on the original training data. We achieve this by creating artificial data and by using multi-task learning. After adaptation, we are able to reduce the number of corrections displayed during incremental output construction by 45%, without a decrease in translation quality",
    "checked": true,
    "id": "3a6c8b467167781a8570dfdfa111a70bf4046d73",
    "semantic_title": "low-latency neural speech translation",
    "citation_count": 66
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bansal18_interspeech.html": {
    "title": "Low-Resource Speech-to-Text Translation",
    "volume": "main",
    "abstract": "Speech-to-text translation has many potential applications for low-resource languages, but the typical approach of cascading speech recognition with machine translation is often impossible, since the transcripts needed to train a speech recognizer are usually not available for low-resource languages. Recent work has found that neural encoder-decoder models can learn to directly translate foreign speech in high-resource scenarios, without the need for intermediate transcription. We investigate whether this approach also works in settings where both data and computation are limited. To make the approach efficient, we make several architectural changes, including a change from character-level to word-level decoding. We find that this choice yields crucial speed improvements that allow us to train with fewer computational resources, yet still performs well on frequent words. We explore models trained on between 20 and 160 hours of data and find that although models trained on less data have considerably lower BLEU scores, they can still predict words with relatively high precision and recall-around 50% for a model trained on 50 hours of data, versus around 60% for the full 160 hour model. Thus, they may still be useful for some low-resource scenarios",
    "checked": true,
    "id": "4f33b7aeec807df6d4c611d00eaa69f9e94d842a",
    "semantic_title": "low-resource speech-to-text translation",
    "citation_count": 46
  },
  "https://www.isca-speech.org/archive/interspeech_2018/brasser18_interspeech.html": {
    "title": "VoiceGuard: Secure and Private Speech Processing",
    "volume": "main",
    "abstract": "With the advent of smart-home devices providing voice-based interfaces, such as Amazon Alexa or Apple Siri, voice data is constantly transferred to cloud services for automated speech recognition or speaker verification. While this development enables intriguing new applications, it also poses significant risks: Voice data is highly sensitive since it contains biometric information of the speaker as well as the spoken words. This data may be abused if not protected properly, thus the security and privacy of billions of end-users is at stake. We tackle this challenge by proposing an architecture, dubbed VoiceGuard, that efficiently protects the speech processing task inside a trusted execution environment (TEE). Our solution preserves the privacy of users while at the same time it does not require the service provider to reveal model parameters. Our architecture can be extended to enable user-specific models, such as feature transformations (including fMLLR), i-vectors, or model transformations (e.g., custom output layers). It also generalizes to secure on-premise solutions, allowing vendors to securely ship their models to customers. We provide a proof-of-concept implementation and evaluate it on the Resource Management and WSJ speech recognition tasks isolated with Intel SGX, a widely available TEE implementation, demonstrating even real time processing capabilities",
    "checked": true,
    "id": "1a5ba159030ad2349b5870b57697fbb45deabaf0",
    "semantic_title": "voiceguard: secure and private speech processing",
    "citation_count": 48
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hakkanitur18_interspeech.html": {
    "title": "Deep Learning based Situated Goal-oriented Dialogue Systems",
    "volume": "main",
    "abstract": "Interacting with machines in natural language has been a holy grail since the beginning of computers. Given the difficulty of understanding natural language, only in the past couple of decades, we started seeing real user applications for targeted/limited domains. More recently, advances in deep learning-based approaches enabled exciting new research frontiers for end-to-end goal-oriented conversational systems. In this talk, I'll review end-to-end dialogue systems research, with components for situated language understanding, dialogue state tracking, policy, and language generation. The talk will highlight novel approaches where dialogue is viewed as a collaborative game between a user and an agent in the presence of visual information, and will aim to summarize challenges for future research",
    "checked": true,
    "id": "3b82d4d2c68c7b36fc343d0b21059c41d64407a0",
    "semantic_title": "deep learning based situated goal-oriented dialogue systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18b_interspeech.html": {
    "title": "Single-channel Speech Dereverberation via Generative Adversarial Training",
    "volume": "main",
    "abstract": "In this paper, we propose a single-channel speech dereverberation system (DeReGAT) based on convolutional, bidirectional long short-term memory and deep feed-forward neural network (CBLDNN) with generative adversarial training (GAT). In order to obtain better speech quality instead of only minimizing a mean square error (MSE), GAT is employed to make the dereverberated speech indistinguishable form the clean samples. Besides, our system can deal with wide range reverberation and be well adapted to variant environments. The experimental results show that the proposed model outperforms weighted prediction error (WPE) and deep neural network-based systems. In addition, DeReGAT is extended to an online speech dereverberation scenario, which reports comparable performance with the offline case",
    "checked": true,
    "id": "7746b2d9402fa5860534d2c3fbbc9e600c0c3d62",
    "semantic_title": "single-channel speech dereverberation via generative adversarial training",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mack18_interspeech.html": {
    "title": "Single-Channel Dereverberation Using Direct MMSE Optimization and Bidirectional LSTM Networks",
    "volume": "main",
    "abstract": "Dereverberation is useful in hands-free communication and voice controlled devices for distant speech acquisition. Single-channel dereverberation can be achieved by applying a time-frequency (TF) mask to the short-time Fourier transform (STFT) representation of a reverberant signal. Recent approaches have used deep neural networks (DNNs) to estimate such masks. Previously proposed DNN-based mask estimation methods train a DNN to minimize the mean-squared-error (MSE) between the desired and estimated masks. Recent TF mask estimation methods for signal separation directly minimize instead the MSE between the desired and estimated STFT magnitudes. We apply this direct optimization concept to dereverberation. Moreover, as reverberation exceeds the duration of a single STFT frame, we propose to use a bidirectional long short-term memory (LSTM) network which is able to take the relation between multiple STFT frames into account. We evaluated our method for different reverberation times and source-microphone distances using simulated as well as measured room impulse responses of different rooms. An evaluation of the proposed method and a comparison with a state-of-the-art method demonstrate the superiority of our approach and its robustness to different acoustic conditions",
    "checked": true,
    "id": "a5942e4793542eebae6ebf0c310d13185ff0728b",
    "semantic_title": "single-channel dereverberation using direct mmse optimization and bidirectional lstm networks",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kodrasi18_interspeech.html": {
    "title": "Single-channel Late Reverberation Power Spectral Density Estimation Using Denoising Autoencoders",
    "volume": "main",
    "abstract": "In order to suppress the late reverberation in the spectral domain, many single-channel dereverberation techniques rely on an estimate of the late reverberation power spectral density (PSD). In this paper, we propose a novel approach to late reverberation PSD estimation using a denoising autoencoder (DA), which is trained to learn a mapping from the microphone signal PSD to the late reverberation PSD. Simulation results show that the proposed approach yields a high PSD estimation accuracy and generalizes well to unseen data. Furthermore, simulation results show that the proposed DA-based PSD estimate yields a higher PSD estimation accuracy and a similar dereverberation performance than a state-of-the-art statistical PSD estimate, which additionally also requires knowledge of the reverberation time",
    "checked": true,
    "id": "1138ec78b2b7772057ba812b30cf742dbfc1e6aa",
    "semantic_title": "single-channel late reverberation power spectral density estimation using denoising autoencoders",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mohanan18_interspeech.html": {
    "title": "A Non-convolutive NMF Model for Speech Dereverberation",
    "volume": "main",
    "abstract": "Reverberation corrupts speech recorded using distant microphones, resulting in poor speech intelligibility. We propose a single-channel, supervised non-negative matrix factorization (NMF) based dereverberation method, in contrast to the convolutive NMF (CNMF) based methods in literature. Recent supervised approaches use a CNMF model for reverberation and a NMF model for clean speech spectrogram to obtain enhanced speech by directly estimating the clean speech activations. In the proposed method, with a separability assumption on the room impulse response (RIR) spectrogram, the reverb speech can be decomposed into bases and activations using conventional NMF. Using these reverb activations, the clean speech activations are estimated to obtain enhanced speech. The proposed model (i) helps in imposing meaningful constraints on the RIR in both frequency- and time-domains to achieve improved enhancement (ii) leads to a framework that can include a NMF model for noise. (iii) gives a better interpretation of the effects of reverberation in the NMF context. We evaluate and compare the enhancement performance of the algorithm on reverb and noisy conditions, simulated using TIMIT utterances and REVERB challenge RIRs. The proposed method performs better than existing C-NMF based methods in objective measures, such as cepstral distance (CD) and speech-to-reverberation modulation energy ratio (SRMR)",
    "checked": true,
    "id": "1cfe3abec0fb152553cc5acf64a76c1259fc6739",
    "semantic_title": "a non-convolutive nmf model for speech dereverberation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/guzewich18_interspeech.html": {
    "title": "Cross-Corpora Convolutional Deep Neural Network Dereverberation Preprocessing for Speaker Verification and Speech Enhancement",
    "volume": "main",
    "abstract": "Deep neural network (DNN) dereverberation preprocessing has been shown to be a viable strategy for speech enhancement and increasing the accuracy of automatic speech recognition and automatic speaker verification. In this paper, an improved DNN technique based on convolutional neural networks is presented and compared to existing methods for speech enhancement and speaker verification in the presence of reverberation. This new technique is first shown to enhance speech quality as compared to other existing methods. Then, a more thorough set of experiments is presented that assesses cross-corpora speaker verification performance on data that contains real reverberation and noise. A discussion of the applicability and generalizability of such techniques is given",
    "checked": true,
    "id": "4cb3ab7c407f1aa3ea381bacafc65038c68c90f3",
    "semantic_title": "cross-corpora convolutional deep neural network dereverberation preprocessing for speaker verification and speech enhancement",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mosner18_interspeech.html": {
    "title": "Dereverberation and Beamforming in Robust Far-Field Speaker Recognition",
    "volume": "main",
    "abstract": "This paper deals with robust speaker verification (SV) in far-field sensing. The robustness is verified on a subset of NIST SRE 2010 corpus retransmitted in multiple real rooms of different acoustics and captured with multiple microphones. We experimented with various data preprocessing steps including different approaches to dereverberation and beamforming applied to ad-hoc microphone arrays. We found that significant improvements in accuracy can be achieved with neural network based generalized eigenvalue beamformer preceded by weighted prediction error dereverberation. We also explored the effect of data augmentation by adding various real or simulated room acoustic properties to the Probabilistic Linear Discriminant Analysis (PLDA) training dataset. As a result, we developed a speaker recognition system whose performance is stable across different room acoustic conditions. It yields 41.4% relative improvement in performance over the system without multi-channel processing tested on the cleanest microphone data. With the best combination of data preprocessing and augmentation, we obtained a performance close to the one we achieved with the original clean test data",
    "checked": true,
    "id": "c89d254a03353144fa898d4ebfcd88411c15eeb0",
    "semantic_title": "dereverberation and beamforming in robust far-field speaker recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18_interspeech.html": {
    "title": "Comparing the Max and Noisy-Or Pooling Functions in Multiple Instance Learning for Weakly Supervised Sequence Learning Tasks",
    "volume": "main",
    "abstract": "Many sequence learning tasks require the localization of certain events in sequences. Because it can be expensive to obtain strong labeling that specifies the starting and ending times of the events, modern systems are often trained with weak labeling without explicit timing information. Multiple instance learning (MIL) is a popular framework for learning from weak labeling. In a common scenario of MIL, it is necessary to choose a pooling function to aggregate the predictions for the individual steps of the sequences. In this paper, we compare the \"max\" and \"noisy-or\" pooling functions on a speech recognition task and a sound event detection task. We find that max pooling is able to localize phonemes and sound events, while noisy-or pooling fails. We provide a theoretical explanation of the different behavior of the two pooling functions on sequence learning tasks",
    "checked": true,
    "id": "df84e2de3320ec8211c8311e2af32d44cbd46847",
    "semantic_title": "comparing the max and noisy-or pooling functions in multiple instance learning for weakly supervised sequence learning tasks",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18n_interspeech.html": {
    "title": "A Simple Model for Detection of Rare Sound Events",
    "volume": "main",
    "abstract": "We propose a simple recurrent model for detecting rare sound events, when the time boundaries of events are available for training. Our model optimizes the combination of an utterance-level loss, which classifies whether an event occurs in an utterance and a frame-level loss, which classifies whether each frame corresponds to the event when it does occur. The two losses make use of a shared vectorial representation the event and are connected by an attention mechanism. We demonstrate our model on Task 2 of the DCASE 2017 challenge and achieve competitive performance",
    "checked": true,
    "id": "af0fbab670a491e01baf622b08824bee9e38bec0",
    "semantic_title": "a simple model for detection of rare sound events",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18e_interspeech.html": {
    "title": "Temporal Transformer Networks for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "Neural networks have been proven to be powerful models for acoustic scene classification tasks, but are still limited by the lack of ability to be temporally invariant to the audio data. In this paper, a novel temporal transformer module is proposed to allow the temporal manipulation of data in neural networks. This module is composed of a Fourier transform layer for feature maps and a learnable feature reduction layer and can be inserted into existing convolutional neural network (CNN) and Long short-term memory (LSTM) models. Experiments on LITIS Rouen dataset and DCASE2016 dataset show that the proposed method leads to a significant improvement when compared with the existing neural networks. Our approach is able to perform significantly better than the state-of-the-art result on LITIS Rouen dataset, obtaining a relative reduction of 23.6% on classification error",
    "checked": true,
    "id": "300ed23694963ab940be11736606c9dd45a456b4",
    "semantic_title": "temporal transformer networks for acoustic scene classification",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lu18_interspeech.html": {
    "title": "Temporal Attentive Pooling for Acoustic Event Detection",
    "volume": "main",
    "abstract": "Deep convolutional neural network (DCNN) based model has been successfully applied to acoustic event detection (AED) due to its efficiency to explore temporal-frequency structure for feature representations. In most studies, the final representation either uses a temporal average- or max-pooling algorithm to accumulate local temporal features as a global representation for event classification. The temporal pooling algorithm in the DCNN is based on the assumption that the target label is assigned to all temporal locations (average pooling) or to only one temporal location with a maximum response (max-pooling). However, the acoustic event labels are holistic descriptions in a semantic level, it is difficult or even impossible to decide features from which temporal locations contribute to the event perception. In this study, we propose a weighted temporal-pooling algorithm to accumulate local temporal features for AED. The pooling algorithm integrates global and local attention modules in a convolutional recurrent neural network to integrate temporal features. Experiments on an AED task were carried out to evaluate the proposed model. Results showed that with the global and local attentions, a large gain was obtained",
    "checked": true,
    "id": "9a5fef0cb41c77a7f8186a35f87a4b80cdc3c2fd",
    "semantic_title": "temporal attentive pooling for acoustic event detection",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kao18_interspeech.html": {
    "title": "R-CRNN: Region-based Convolutional Recurrent Neural Network for Audio Event Detection",
    "volume": "main",
    "abstract": "This paper proposes a Region-based Convolutional Recurrent Neural Network (R-CRNN) for audio event detection (AED). The proposed network is inspired by Faster-RCNN [1], a well-known region-based convolutional network framework for visual object detection. Different from the original Faster-RCNN, a recurrent layer is added on top of the convolutional network to capture the long-term temporal context from the extracted high-level features. While most of the previous works on AED generate predictions at frame level first and then use post-processing to predict the onset/offset timestamps of events from a probability sequence; the proposed method generates predictions at event level directly and can be trained end-to-end with a multi-task loss, which optimizes the classification and localization of audio events simultaneously. The proposed method is tested on DCASE 2017 Challenge dataset [2]. To the best of our knowledge, R-CRNN is the best performing single-model method among all methods without using ensembles both on development and evaluation sets. Compared to the other region-based network for AED (R-FCN [3]) with an event-based error rate (ER) of 0.18 on the development set, our method reduced the ER to half",
    "checked": true,
    "id": "c569ce5b1c2d950f41131b9efb2e8752a3442398",
    "semantic_title": "r-crnn: region-based convolutional recurrent neural network for audio event detection",
    "citation_count": 51
  },
  "https://www.isca-speech.org/archive/interspeech_2018/papayiannis18_interspeech.html": {
    "title": "Detecting Media Sound Presence in Acoustic Scenes",
    "volume": "main",
    "abstract": "Using speech to interact with electronic devices and access services is becoming increasingly common. Using such applications in our households poses new challenges for speech and audio processing algorithms as these applications should perform robustly in a number of scenarios. Media devices are very commonly present in such scenarios and can interfere with the user-device communication by contributing to the noise or simply by being mistaken as user issued voice commands. Detecting the presence of media sounds in the environment can help avoid such issues. In this work we propose a method for this task based on a parallel CNN-GRU-FC classifier architecture which relies on multi-channel information to discriminate between media and live sources. Experiments performed using 378 hours of in-house audio recordings collected by volunteers show an F1 score of 71% with a recall of 72% in detecting active media sources. The use of information from multiple channels gave a relative improvement of 16% to the F1 score when compared to using information from only a single channel",
    "checked": true,
    "id": "2ad3e25e05911b8aab00d49e32c7d6a232da7f18",
    "semantic_title": "detecting media sound presence in acoustic scenes",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2018/broux18_interspeech.html": {
    "title": "S4D: Speaker Diarization Toolkit in Python",
    "volume": "main",
    "abstract": "In this paper, we present S4D, a new open-source Python toolkit dedicated to speaker diarization. S4D provides various state-of-the-art components and the possibility to easily develop end-to-end diarization prototype systems. S4D offers a large panel of clustering, segmentation, scoring and visualization algorithms. S4D has been thought to be easily understood, installed, modified and used in order to allow fast transfers of diarization technologies to industry and facilitate development of new approaches. Examples, benchmarks on standard tasks and tutorials are provided in this paper. S4D is an extension of the open-source toolkit for speaker recognition: SIDEKIT",
    "checked": true,
    "id": "ba804aff340943dce821b3e0f7168f55e45eb2e4",
    "semantic_title": "s4d: speaker diarization toolkit in python",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2018/park18b_interspeech.html": {
    "title": "Multimodal Speaker Segmentation and Diarization Using Lexical and Acoustic Cues via Sequence to Sequence Neural Networks",
    "volume": "main",
    "abstract": "While there has been substantial amount of work in speaker diarization recently, there are few efforts in jointly employing lexical and acoustic information for speaker segmentation. Towards that, we investigate a speaker diarization system using a sequence-to-sequence neural network trained on both lexical and acoustic features. We also propose a loss function that allows for selecting not only the speaker change points but also the best speaker at any time by allowing for different speaker groupings. We incorporate Mel Frequency Cepstral Coefficients (MFCC) as an acoustic feature stream alongside lexical information that are obtained from conversations from the Fisher dataset. Thus, we show that acoustics provide complementary information to the lexical modality. The experimental results show that sequence-to-sequence system trained on both word sequences and MFCC can improve on speaker diarization result compared to the system that only relies on lexical modality or the baseline MFCC-based system. In addition, we test the performance of our proposed method with Automatic Speech Recognition (ASR) transcripts. While the performance on ASR transcripts drops, the Diarization Error Rate (DER) of our proposed method still outperforms the traditional method based on Bayesian Information Criterion (BIC)",
    "checked": true,
    "id": "fa45b9817b73be40d53a2cc0e40a9878627d4c41",
    "semantic_title": "multimodal speaker segmentation and diarization using lexical and acoustic cues via sequence to sequence neural networks",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2018/flemotomos18b_interspeech.html": {
    "title": "Combined Speaker Clustering and Role Recognition in Conversational Speech",
    "volume": "main",
    "abstract": "Speaker Role Recognition (SRR) is usually addressed either as an independent classification task, or as a subsequent step after a speaker clustering module. However, the first approach does not take speaker-specific variabilities into account, while the second one results in error propagation. In this work we propose the integration of an audio-based speaker clustering algorithm with a language-aided role recognizer into a meta-classifier which takes both modalities into account. That way, we can treat separately any speaker-specific and role-specific characteristics before combining the relevant information together. The method is evaluated on two corpora of different conditions with interactions between a clinician and a patient and it is shown that it yields superior results for the SRR task",
    "checked": true,
    "id": "64f873665ccf49d15063578222193461746af63f",
    "semantic_title": "combined speaker clustering and role recognition in conversational speech",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lefranc18_interspeech.html": {
    "title": "The ACLEW DiViMe: An Easy-to-use Diarization Tool",
    "volume": "main",
    "abstract": "We present \"DiViMe\", an open-source virtual machine aimed at packaging speech technology for real-life data and developed in the context of the \"Analyzing Children's Language Environments across the World\" Project. This first release focuses on Speech Activity Detection, Speaker Diarization and their evaluation. The present paper introduces the set of included tools and the current workflow, which is focused on making minimal assumptions regarding users' technical skills. Additionally, we show how the current DiViMe tools fare against three sets of challenging data. In a first experiment, we look at performance with samples extracted from daylong recordings gathered using the LENA{TM}, system from English-learning children. We find that the performance of the tools currently in DiViMe is not far from that achieved by the lena proprietary software. In a second experiment, we generalize to other samples of child-centered daylong files, gathered with non-LENA{TM}, hardware from non-English-learning children, showing that performance does not degrade in this condition. Finally, we report on performance in the DiHARD 2018 Challenge Test Data. Originally conceived in the \"Speech Recognition Virtual Kitchen\", DiViMe is a promising platform for packaging speech technology tools for widespread re-use, with potential impact on both fundamental and applied speech and language research",
    "checked": true,
    "id": "b149982f7d827934f1e4173e5e049ea9549f328d",
    "semantic_title": "the aclew divime: an easy-to-use diarization tool",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kazimirova18_interspeech.html": {
    "title": "Automatic Detection of Multi-speaker Fragments with High Time Resolution",
    "volume": "main",
    "abstract": "Interruptions and simultaneous talking represent important patterns of speech behavior. However, there is a lack of approaches to their automatic detection in continuous audio data. We have developed a solution for automatic labeling of multi-speaker fragments using harmonic traces analysis. Since harmonic traces in multi-speaker intervals form an irregular pattern as opposed to the structured pattern typical for a single speaker, we resorted to computer vision methods to detect multi-speaker fragments. A convolutional neural network was trained on synthetic material to differentiate between single-speaker and multi-speaker fragments. For evaluation of the proposed method the SSPNet Conflict Corpus with provided manual diarization was used. We also examined factors affecting algorithm performance. The main advantages of the proposed method are calculation simplicity and high time resolution. With our approach it is possible to detect segments with minimum duration of 0.5 seconds. The proposed method demonstrates highly accurate results and may be used for speech segmentation, speaker tracking, content analysis such as conflict detection and other practical purposes",
    "checked": true,
    "id": "4bfe63b6863e8a387fba23eb9cfb685384490595",
    "semantic_title": "automatic detection of multi-speaker fragments with high time resolution",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yin18b_interspeech.html": {
    "title": "Neural Speech Turn Segmentation and Affinity Propagation for Speaker Diarization",
    "volume": "main",
    "abstract": "Speaker diarization is the task of determining who speaks when in an audio stream. Most diarization systems rely on statistical models to address four sub-tasks: speech activity detection (SAD), speaker change detection (SCD), speech turn clustering and re-segmentation. First, following the recent success of recurrent neural networks (RNN) for SAD and SCD, we propose to address re-segmentation with Long-Short Term Memory (LSTM) networks. Then, we propose to use affinity propagation on top of neural speaker embeddings for speech turn clustering, outperforming regular Hierarchical Agglomerative Clustering (HAC). Finally, all these modules are combined and jointly optimized to form a speaker diarization pipeline in which all but the clustering step are based on RNNs. We provide experimental results on the French Broadcast dataset ETAPE where we reach state-of-the-art performance",
    "checked": true,
    "id": "337bfde02f3ac1a7992a5628547fefb2323edc7d",
    "semantic_title": "neural speech turn segmentation and affinity propagation for speaker diarization",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18i_interspeech.html": {
    "title": "Pitch or Phonation: on the Glottalization in Tone Productions in the Ruokeng Hui Chinese Dialect",
    "volume": "main",
    "abstract": "This paper examines the interplay of glottalization and tones in tonal phonology of the Ruokeng Hui Chinese. Acoustic data from 10 native speakers were analyzed in terms of pitch (F0), duration, H1-H2, H1-A1/2/3, CPP, HNR, SHR, etc. Fine-grained phonetic details reveal the interactions between phonation and tones and shed light on the ongoing tonal change from a glottalized tone to a plain high falling tone in the Ruokeng dialect",
    "checked": true,
    "id": "8c9f1184f29d57da2d0a34d30864aac6abb63a6e",
    "semantic_title": "pitch or phonation: on the glottalization in tone productions in the ruokeng hui chinese dialect",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hullebus18_interspeech.html": {
    "title": "Speaker-specific Structure in German Voiceless Stop Voice Onset Times",
    "volume": "main",
    "abstract": "Voice onset time (VOT), a primary cue for voicing in many languages including English and German, is known to vary greatly between speakers, but also displays robust within-speaker consistencies, at least in English. The current analysis extends these findings to German. VOT measures were investigated from voiceless alveolar and velar stops in CV syllables cued by a visual prompt in a cue-distractor task. Comparably to English, a considerable portion of German VOT variability can be attributed to the syllable's vowel length and the stop's place of articulation. Individual differences in VOT still remain irrespective of speech rate. However, significant correlations across places of articulation and between speaker-specific mean VOTs and standard deviations indicate that talkers employ a relatively unified VOT profile across places of articulation. This could allow listeners to more efficiently adapt to speaker-specific realisations",
    "checked": true,
    "id": "475b7a7ca640d6ef70e3b31622f2bbebb0b0954f",
    "semantic_title": "speaker-specific structure in german voiceless stop voice onset times",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/aare18_interspeech.html": {
    "title": "Creak in the Respiratory Cycle",
    "volume": "main",
    "abstract": "Creakiness is a well-known turn-taking cue and has been observed to systematically accompany phrase and turn ends in several languages. In Estonian, creaky voice is frequently used by all speakers without any obvious evidence for its systematic use as a turn-taking cue. Rather, it signals a lack of prominence and is favored by lengthening and later timing in phrases. In this paper, we analyze the occurrence of creak with respect to properties of the respiratory cycle. We show that creak is more likely to accompany longer exhalations. Furthermore, the results suggest there is little difference in lung volume values regardless of the presence of creak, indicating that creaky voice might be employed to preserve air over the course of longer utterances. We discuss the results in connection to processes of speech planning in spontaneous speech",
    "checked": true,
    "id": "a1535994b11635d819a3077a6345f45b6a7f847f",
    "semantic_title": "creak in the respiratory cycle",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18p_interspeech.html": {
    "title": "Acoustic Analysis of Whispery Voice Disguise in Mandarin Chinese",
    "volume": "main",
    "abstract": "This paper investigates the auditory and acoustical characteristics of whispery disguised voice and compares the patterns with those of normal (non-disguised) voices. It also evaluates effects of whispery disguise on forensic voice comparison. Recordings of eleven male college students' normal voices and whispery disguised voices were collected. All their normal and whisper speech was acoustically analyzed and compared. The parameters including average syllable duration, intensity, vowel formant frequencies and long term average spectrum (LTAS) were measured and statistically analyzed. The effect of whispery voice disguise on speaker recognition by auditory perception and an automatic system were evaluated. Correlation and regression analyses were made on the parameters of whispery voice and normal voice. These simple regression models can be used for parameter compensation in forensic casework",
    "checked": true,
    "id": "c7418dd554e1351c430aa171269175dddc3625b2",
    "semantic_title": "acoustic analysis of whispery voice disguise in mandarin chinese",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/maurer18_interspeech.html": {
    "title": "The Zurich Corpus of Vowel and Voice Quality, Version 1.0",
    "volume": "main",
    "abstract": "Existing databases of isolated vowel sounds or vowel sounds embedded in consonantal context generally document only limited variation of basic production parameters. Thus, concerning the possible variation range of vowel and voice quality-related sound characteristics, there is a lack of broad phenomenological and descriptive references that allow for a comprehensive understanding of vowel acoustics and for an evaluation of the extent to which corresponding existing approaches and models can be generalised. In order to contribute to the building up of such references, a novel database of vowel sounds that exceeds any existing collection by size and diversity of vocalic characteristics is presented here, comprised of c. 34 600 utterances of 70 speakers (46 non-professional speakers, children, women and men and 24 professional actors/actresses and singers of straight theatre, contemporary singing and European classical singing). The database focuses on sounds of the long Standard German vowels /i–y–e–ø–ɛ–a–o–u/ produced with varying basic production parameters such as phonation type, vocal effort, fundamental frequency, vowel context and speaking or singing style. In addition, a read text and, for professionals, songs are also included. The database is accessible for scientific use and further extensions are in progress",
    "checked": true,
    "id": "fdd3c6083ecc03342496b557eeea0b2eef9cc672",
    "semantic_title": "the zurich corpus of vowel and voice quality, version 1.0",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/penney18_interspeech.html": {
    "title": "Weighting of Coda Voicing Cues: Glottalisation and Vowel Duration",
    "volume": "main",
    "abstract": "Recent research suggests that a trading relationship may exist in speech production between vowel duration and glottalisation as cues to coda stop voicing in Australian English. Younger speakers have been shown to use glottalisation to signal voicelessness more than older speakers who instead make greater use of vowel duration. This suggests a sound change in progress for the voicing cues. In addition, the vowel duration cue to voicing is greater in inherently long vowel contexts compared to inherently short vowel contexts. We report on a perceptual study designed to examine whether the weighting of these two cues found in production is replicated in perception. Older and younger listeners were presented with audio stimuli co-varying in vowel duration and glottalisation. In accord with findings from production, the vowel duration cue was weaker for contexts containing inherently short vowels than for those containing inherently long vowels. Complementarily, glottalisation had a stronger effect on the perception of coda voicelessness in inherently short vowel contexts. Older and younger listeners did not differ in their use of glottalisation as a perceptual cue to voicelessness despite previously identified age differences in production. This finding raises questions about the link between perception and production in sound change",
    "checked": true,
    "id": "cdef42285ebbabf2487fde92bd8a816dccffd8b4",
    "semantic_title": "weighting of coda voicing cues: glottalisation and vowel duration",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhao18d_interspeech.html": {
    "title": "Revealing Spatiotemporal Brain Dynamics of Speech Production Based on EEG and Eye Movement",
    "volume": "main",
    "abstract": "To understand the neural circuitry associated with speech production in oral reading, it is essential to describe the whole-range spatiotemporal brain dynamics in the processes including visual word recognition, orthography-phonology mapping, semantic accessing, speech planning, articulation, self-monitoring, etc. This has turned out to be extremely difficult because of demanding resolution in both spatial and temporal domains and advanced algorithms to eliminate severe contamination by articulatory movements. To tackle this hard target, we recruited 16 subjects in a sentence reading task and measured multimodal signals of electroencephalography (EEG), eye movement and speech simultaneously. The onset/offset of gazing and utterance were used for segmenting brain activation stages. Cortical modeling of causal interactions among anatomical regions was conducted on EEG signals through (i) independent component analysis to identify cortical regions of interest (ROIs); (ii) multivariate autoregressive modeling of representative cortical activity from each ROI; and (iii) quantification of the dynamic causal interactions among ROIs using the Short-time direct Directed Transfer function. The resulting brain dynamic model reveals a widely connected bilateral organization with left-lateralized semantic, orthographic and phonological sub-networks, right-lateralized prosody and motor sequencing sub-networks and bi-lateralized auditory and multisensory integration sub-networks that cooperate along interlaced and paralleled temporal stages for speech processing",
    "checked": true,
    "id": "94a542a777ead3d886f5e1dcd4c620309144419e",
    "semantic_title": "revealing spatiotemporal brain dynamics of speech production based on eeg and eye movement",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bollavetisyan18_interspeech.html": {
    "title": "Neural Response Development During Distributional Learning",
    "volume": "main",
    "abstract": "We investigated online electrophysiological components of distributional learning, specifically of tones by listeners of a non-tonal language. German listeners were presented with a bimodal distribution of syllables with lexical tones from a synthesized continuum based on Cantonese level tones. Tones were presented in sets of four standards (within-category tokens) followed by a deviant (across-category token). Mismatch negativity (MMN) was measured. Earlier behavioral data showed that exposure to this bimodal distribution improved both categorical perception and perceptual acuity for level tones [1]. In the present study we present analyses of the electrophysiological response recorded during this exposure, i.e. the development of the MMN response during distributional learning. This development over time is analyzed using Generalized Additive Mixed Models and results showed that the MMN amplitude increased for both within- and across-category tokens, reflecting higher perceptual acuity accompanying category formation. This is evidence that learners zooming in on phonological categories undergo neural changes associated with more accurate phonetic perception",
    "checked": true,
    "id": "fb0f6e633f42e479b2de02a438dc7cf487993a7b",
    "semantic_title": "neural response development during distributional learning",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/maggu18b_interspeech.html": {
    "title": "Learning Two Tone Languages Enhances the Brainstem Encoding of Lexical Tones",
    "volume": "main",
    "abstract": "Auditory brainstem encoding is influenced by experience-dependent factors such as language and music. Tone language speakers exhibit more robust brainstem encoding of lexical tones than non-tone language speakers. Studies suggest that the effects of experience with a tone language generalize to the brainstem encoding of lexical tones from other tone languages. However, the effects of learning two tone languages, with different tonal systems, on brainstem encoding of lexical pitch are unknown. In the current study, we investigated whether or not the experience with two tone languages (Mandarin and Cantonese) enhances the brainstem encoding of lexical pitch, using frequency following response (FFR). Mandarin has four lexical tones- high level, rising, dipping and falling while Cantonese has a richer tone system with three level tones (high, mid, low), two rising tones (high and low) and one falling tone. We compared speakers fluent in Cantonese vs. those fluent in both Cantonese and Mandarin on their brainstem encoding of Cantonese and Mandarin lexical tones. We found that the Cantonese-Mandarin speakers exhibited more robust brainstem encoding of the lexical tones as compared to Cantonese speakers. From the current findings, we conclude that learning two tone languages may enhance lexical pitch encoding at the brainstem",
    "checked": true,
    "id": "c1a90e00abaca6ec2c056b8994c0b3b4effd7430",
    "semantic_title": "learning two tone languages enhances the brainstem encoding of lexical tones",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2018/williams18b_interspeech.html": {
    "title": "Perceptual Sensitivity to Spectral Change in Australian English Close Front Vowels: An Electroencephalographic Investigation",
    "volume": "main",
    "abstract": "Speech scientists have long noted that the qualities of naturally-produced vowels do not remain constant over their durations – regardless of being nominally \"monophthongs\" or \"diphthongs\". Recent acoustic corpora show that there are consistent patterns of first (F1) and second (F2) formant frequency change across different vowel categories. The three Australian English (AusE) close front vowels /iː, ɪ, ɪə/ provide a striking example: while their midpoint or mean F1 and F2 frequencies are virtually identical, their spectral change patterns distinctly differ. The present study utilizes a pre-attentive discrimination paradigm with electroencephalography to assess AusE listeners' perceptual sensitivity to close front vowels with different F1 × F2 trajectory lengths (TLs) and directions (TDs). When TLs are modest, there is an asymmetry in perceptual sensitivity: closing vowels, e.g., /iː/ whose trajectory terminates high in the F1 × F2 vowel space, are perceptually prominent, whereas centering vowels, e.g., /ɪ, ɪə/ whose trajectories end more centrally, are not. However, when TLs are exaggerated, the asymmetry in the perceptual sensitivity to the two TDs is substantially reduced. The results indicate that, despite the distinct patterns of spectral change of AusE /iː, ɪ, ɪə/ in production, its perceptual relevance is not uniform, but rather vowel-category dependent",
    "checked": true,
    "id": "caa25ff0330b568f1ea0d9ceb8d905db7ee359e8",
    "semantic_title": "perceptual sensitivity to spectral change in australian english close front vowels: an electroencephalographic investigation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nixon18_interspeech.html": {
    "title": "Effective Acoustic Cue Learning Is Not Just Statistical, It Is Discriminative",
    "volume": "main",
    "abstract": "A growing statistical learning literature suggests that listeners extract statistical information from the linguistic environment. However, distributional frequency may be insufficient for important but relatively low-frequency cues. Acquisition of linguistic knowledge may rely not merely on co-occurrences but on predictive relationships between cues and their outcomes. The present study investigates effects of predictive temporal cue structure on acquisition of a non-native acoustic cue dimension. During training, native English speakers saw coloured shape objects and heard spoken Min Chinese words with six different lexical tones. Tones were the only reliable cue to identifying the associated object. Words also contained a salient cue that did not discriminate between objects. Three tones occurred with high-frequency and three with low-frequency in training. The critical manipulation was the presentation order: either words, containing complex cue structure, preceded object outcomes (discriminative order) or objects preceded words (non-discriminative order). Generalised linear mixed models showed accuracy was significantly higher in the discriminative order than the non-discriminative order. These results demonstrate that predictive cue structure can facilitate acquisition of a non-native cue dimension. Feedback from prediction error drives learners to ignore salient non-discriminative cues and effectively learn to use the target cue dimension",
    "checked": true,
    "id": "51a30ce5803d8d343cd9301034f4f7e2fe07a376",
    "semantic_title": "effective acoustic cue learning is not just statistical, it is discriminative",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mulder18_interspeech.html": {
    "title": "Analyzing EEG Signals in Auditory Speech Comprehension Using Temporal Response Functions and Generalized Additive Models",
    "volume": "main",
    "abstract": "Analyzing EEG signals recorded while participants are listening to continuous speech with the purpose of testing linguistic hypotheses is complicated by the fact that the signals simultaneously reflect exogenous acoustic excitation and endogenous linguistic processing. This makes it difficult to trace subtle differences that occur in mid-sentence position. We apply an analysis based on multivariate temporal response functions to uncover subtle mid-sentence effects. This approach is based on a per-stimulus estimate of the response of the neural system to speech input. Analyzing EEG signals predicted on the basis of the response functions might then bring to light conditionspecific differences in the filtered signals. We validate this approach by means of an analysis of EEG signals recorded with isolated word stimuli. Then, we apply the validated method to the analysis of the responses to the same words in the middle of meaningful sentences",
    "checked": true,
    "id": "cefc10a31203bce04b4562d4e7cad9f83acba1ad",
    "semantic_title": "analyzing eeg signals in auditory speech comprehension using temporal response functions and generalized additive models",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tenbosch18b_interspeech.html": {
    "title": "Information Encoding by Deep Neural Networks: What Can We Learn?",
    "volume": "main",
    "abstract": "The recent advent of deep learning techniques in speech technology and in particular in automatic speech recognition has yielded substantial performance improvements. This suggests that deep neural networks (DNNs) are able to capture structure in speech data that older methods for acoustic modeling, such as Gaussian Mixture Models and shallow neural networks fail to uncover. In image recognition it is possible to link representations on the first couple of layers in DNNs to structural properties of images and to representations on early layers in the visual cortex. This raises the question whether it is possible to accomplish a similar feat with representations on DNN layers when processing speech input. In this paper we present three different experiments in which we attempt to untangle how DNNs encode speech signals and to relate these representations to phonetic knowledge, with the aim to advance conventional phonetic concepts and to choose the topology of a DNNs more efficiently. Two experiments investigate representations formed by auto-encoders. A third experiment investigates representations on convolutional layers that treat speech spectrograms as if they were images. The results lay the basis for future experiments with recursive networks",
    "checked": true,
    "id": "0e6ca3d8d32339c610032d6c5758f8f110be2236",
    "semantic_title": "information encoding by deep neural networks: what can we learn?",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hsu18_interspeech.html": {
    "title": "Scalable Factorized Hierarchical Variational Autoencoder Training",
    "volume": "main",
    "abstract": "Deep generative models have achieved great success in unsupervised learning with the ability to capture complex nonlinear relationships between latent generating factors and observations. Among them, a factorized hierarchical variational autoencoder (FHVAE) is a variational inference-based model that formulates a hierarchical generative process for sequential data. Specifically, an FHVAE model can learn disentangled and interpretable representations, which have been proven useful for numerous speech applications, such as speaker verification, robust speech recognition and voice conversion. However, as we will elaborate in this paper, the training algorithm proposed in the original paper is not scalable to datasets of thousands of hours, which makes this model less applicable on a larger scale. After identifying limitations in terms of runtime, memory and hyperparameter optimization, we propose a hierarchical sampling training algorithm to address all three issues. Our proposed method is evaluated comprehensively on a wide variety of datasets, ranging from 3 to 1,000 hours and involving different types of generating factors, such as recording conditions and noise types. In addition, we also present a new visualization method for qualitatively evaluating the performance with respect to the interpretability and disentanglement. Models trained with our proposed algorithm demonstrate the desired characteristics on all the datasets",
    "checked": true,
    "id": "3eb0a17c690978a4ce091bbaf34c7a69a5d4a364",
    "semantic_title": "scalable factorized hierarchical variational autoencoder training",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2018/verwimp18_interspeech.html": {
    "title": "State Gradients for RNN Memory Analysis",
    "volume": "main",
    "abstract": "We present a framework for analyzing what the state in RNNs remembers from its input embeddings. Our approach is inspired by backpropagation, in the sense that we compute the gradients of the states with respect to the input embeddings. The gradient matrix is decomposed with Singular Value Decomposition to analyze which directions in the embedding space are best transferred to the hidden state space, characterized by the largest singular values. We apply our approach to LSTM language models and investigate to what extent and for how long certain classes of words are remembered on average for a certain corpus. Additionally, the extent to which a specific property or relationship is remembered by the RNN can be tracked by comparing a vector characterizing that property with the direction(s) in embedding space that are best preserved in hidden state space",
    "checked": true,
    "id": "f7cab3a164fcb4e35012250c3033cb76d9dfb6ba",
    "semantic_title": "state gradients for rnn memory analysis",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bai18b_interspeech.html": {
    "title": "Exploring How Phone Classification Neural Networks Learn Phonetic Information by Visualising and Interpreting Bottleneck Features",
    "volume": "main",
    "abstract": "Neural networks have a reputation for being \"black boxes\", which it has been suggested that techniques from user interface development and visualisation in particular, could help lift. In this paper, we explore 9-dimensional bottleneck features (BNFs) that have been shown in our earlier work to well represent speech in the context of speech recognition and 2-dimensional BNFs directly extracted from bottleneck neural networks. The 9-dimensional BNFs obtained from a phone classification neural network are visualised in 2-dimensional spaces using linear discriminant analysis (LDA) and t-distributed stochastic neighbour embedding (t-SNE). The 2-dimensional BNF space is analysed with regard to phonetic features. A back-propagation method is used to create \"cardinal\" features for each phone under a particular neural network. Both the visualisations of 9-dimensional and 2-dimensional BNFs show distinctions between most phone categories. Particularly, the 2-dimensional BNF space seems to be a union of phonetic category related subspaces that preserve local structures within each subspace where the organisations of phones appear to correspond to phone production mechanisms. By applying LDA to the features of higher dimensional non-bottleneck layers, we observe a triangular pattern which may indicate that silence, friction and voicing are the three main properties learned by the neural networks",
    "checked": true,
    "id": "5c69df39bbaf77615237f2ab52fb9b20a9cf6959",
    "semantic_title": "exploring how phone classification neural networks learn phonetic information by visualising and interpreting bottleneck features",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zegers18_interspeech.html": {
    "title": "Memory Time Span in LSTMs for Multi-Speaker Source Separation",
    "volume": "main",
    "abstract": "With deep learning approaches becoming state-of-the-art in many speech (as well as non-speech) related machine learning tasks, efforts are being taken to delve into the neural networks which are often considered as a black box. In this paper it is analyzed how recurrent neural network (RNNs) cope with temporal dependencies by determining the relevant memory time span in a long short-term memory (LSTM) cell. This is done by leaking the state variable with a controlled lifetime and evaluating the task performance. This technique can be used for any task to estimate the time span the LSTM exploits in that specific scenario. The focus in this paper is on the task of separating speakers from overlapping speech. We discern two effects: A long term effect, probably due to speaker characterization and a short term effect, probably exploiting phone-size formant tracks",
    "checked": true,
    "id": "9e3b378b2622bac04c8b5a24082c23dc99713da1",
    "semantic_title": "memory time span in lstms for multi-speaker source separation",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/scharenborg18b_interspeech.html": {
    "title": "Visualizing Phoneme Category Adaptation in Deep Neural Networks",
    "volume": "main",
    "abstract": "Both human listeners and machines need to adapt their sound categories whenever a new speaker is encountered. This perceptual learning is driven by lexical information. The aim of this paper is two-fold: investigate whether a deep neural network-based (DNN) ASR system can adapt to only a few examples of ambiguous speech as humans have been found to do; investigate a DNN's ability to serve as a model of human perceptual learning. Crucially, we do so by looking at intermediate levels of phoneme category adaptation rather than at the output level. We visualize the activations in the hidden layers of the DNN during perceptual learning. The results show that, similar to humans, DNN systems learn speaker-adapted phone category boundaries from a few labeled examples. The DNN adapts its category boundaries not only by adapting the weights of the output layer, but also by adapting the implicit feature maps computed by the hidden layers, suggesting the possibility that human perceptual learning might involve a similar nonlinear distortion of a perceptual space that is intermediate between the acoustic input and the phonological categories. Comparisons between DNNs and humans can thus provide valuable insights into the way humans process speech and improve ASR technology",
    "checked": true,
    "id": "1d52a71fba0120568d03cc97717b25c1bb13f1e2",
    "semantic_title": "visualizing phoneme category adaptation in deep neural networks",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kasthuri18_interspeech.html": {
    "title": "Early Vocabulary Development Through Picture-based Software Solutions",
    "volume": "main",
    "abstract": "Assistive technology enables children with disabilities to gain access, function independently and take advantage of schooling and social opportunities[1]. The need for alternative and augmentative communication (AAC) in all children is not the same[2] but AAC can aid in expressive language and also support intelligible speakers in developing and using communication skills in varied situations. While the range and flexibility of AAC has grown over the years, making devices accessible to children at varied economic and regional backgrounds, is still a challenge. KAVI-PTS is designed as a picture-to-speech Android application and has been made available in several Asian languages. This application is conceived of as an inexpensive software alternative to communication charts. It can be easily configured to adjust contrast levels and customize selection modes, enabling children to have access to a tailor-made communication solution",
    "checked": true,
    "id": "10333333e0c8b0765722978066bc0120d5458bdc",
    "semantic_title": "early vocabulary development through picture-based software solutions",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sabu18_interspeech.html": {
    "title": "Automatic Detection of Expressiveness in Oral Reading",
    "volume": "main",
    "abstract": "We present a Computer-Aided Language Learning (CALL) system that assesses a child's oral reading skill including the prosodic aspects. With children who have otherwise achieved word decoding automaticity, prosodic fluency is a reliable predictor of comprehension. Prosody includes attributes such as pace, phrasing and expression. Based on the acoustic correlates of prosodic events, we propose and test features that discriminate expressive speech from monotonous speech and further detect whether the expression is meaningful or simply a rhythmic cadence with no relation to the underlying syntax or semantics of the text. Finally the system based on processing short samples of recorded oral reading and providing feedback on the goodness of both lexical and prosodic aspects is described",
    "checked": true,
    "id": "21cd307e99a43295c0d3d0cfe537ab6ed759d82a",
    "semantic_title": "automatic detection of expressiveness in oral reading",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pal18_interspeech.html": {
    "title": "PannoMulloKathan: Voice Enabled Mobile App for Agricultural Commodity Price Dissemination in Bengali Language",
    "volume": "main",
    "abstract": "In this work we present a voice based mobile application for dissemination of agricultural commodity procurement and consumer prices. Disbursed information is crawled at daily basis from government authorized websites of agricultural marketing departments. The app incorporates mix media multiple access means in form of touch-type-see, touch-type-listen, speak-see and speak-listen modalities and also includes a robust Automatic Speech Recognition (ASR) engine in Bengali language to support real time voice queries. Colorful interactive app based user interface and ASR incorporated core client-server architecture altogether provides an efficient framework for serving registered users of different educational and economical background including people having little or no computer knowledge, semi-literate or illiterate rural people",
    "checked": true,
    "id": "de5aaa725f13636cc59411469c1e33a2a2ff6e69",
    "semantic_title": "pannomullokathan: voice enabled mobile app for agricultural commodity price dissemination in bengali language",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/oktem18_interspeech.html": {
    "title": "Visualizing Punctuation Restoration in Speech Transcripts with Prosograph",
    "volume": "main",
    "abstract": "We have developed a neural architecture that tests the effect of lexical, morphosyntactic and prosodic features in restoring punctuation in speech transcriptions. Having outperformed a baseline model in terms of precision and recall, we further extend our performance tests by attaching it in a speech recognition pipeline. The visual and interactive testing environment that we prepared helps us observe how our models generalizes in unseen data and also plan our next steps for improvement",
    "checked": true,
    "id": "16bde9654a2e89b3ff3757cdbc2ffd4224c6a90e",
    "semantic_title": "visualizing punctuation restoration in speech transcripts with prosograph",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mathivanan18_interspeech.html": {
    "title": "CACTAS - Collaborative Audio Categorization and Transcription for ASR Systems",
    "volume": "main",
    "abstract": "We present a web based tool that allows collaborative analysis and/or transcription of audios with respect to Automatic Speech Recognition (ASR) systems. The tool presents a webpage consisting of audios and their corresponding references and hypotheses obtained offline. Several other information and features are provided that allow the audios to be categorized and references to be corrected efficiently in a collaborative way almost 10 times faster, without the need for prior knowledge on speech or ASR systems. The analysis can later be summarized and acted upon to improve or triage the ASR system",
    "checked": true,
    "id": "cfbd6b9ff5c8c8eb4cf70661f4439902f4a879eb",
    "semantic_title": "cactas - collaborative audio categorization and transcription for asr systems",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/parrell18_interspeech.html": {
    "title": "FACTS: A Hierarchical Task-based Control Model of Speech Incorporating Sensory Feedback",
    "volume": "main",
    "abstract": "We present a computational model of speech motor control that integrates vocal tract state prediction with sensory feedback. This hierarchical model, called FACTS, incorporates both a high-level and low-level controller. The high-level controller orchestrates linguistically-relevant speech tasks, which are represented as desired constrictions along the vocal tract (e.g., closure of the lips). The output of the high-level controller is passed to a low-level controller that can issue motor commands at the level of the speech articulators in order to accomplish the desired constrictions. In order to generate these articulatory motor commands, this low-level articulatory controller relies on an estimate of the current state of the vocal tract. This estimate combines internal predictions about the consequences of issued motor commands with auditory and somatosensory feedback from the vocal tract using an Unscented Kalman Filter based state estimation method. FACTS is able to replicate important aspects of human speech behavior, in that it reproduces: (i) stable speech behavior in the presence of noisy motor and sensory systems, (ii) partial acoustic compensation to auditory feedback perturbations, (iii) complete compensations to mechanical perturbations only when they interfere with current production goals and (iv) the observed relationship between sensory acuity and response to sensory perturbations",
    "checked": true,
    "id": "f13cd6b02b32054a99781124f412cf68ff688492",
    "semantic_title": "facts: a hierarchical task-based control model of speech incorporating sensory feedback",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2018/katz18_interspeech.html": {
    "title": "Sensorimotor Response to Tongue Displacement Imagery by Talkers with Parkinson's Disease",
    "volume": "main",
    "abstract": "In a previous study, we asked healthy adult speakers to produce the word head under noise-masked (visual only) conditions and while watching videos of a 3D tongue avatar that gradually morphed from producing head to had. Results indicated that during the visual mismatch phases all participants entrained to the visually presented word, head, without being aware that their vowel quality had changed. Here, we explore whether similar effects occur for individuals with presumed sensorineural processing disorders, patients with Parkinson's disease (PD). We also examine the effects of PD treatment on this entrainment behavior. Participants were 14 individuals with PD, with eight in ongoing speech/language therapy and six reporting no recent therapy. Participants heard pink noise over headphones and produced the word head under four viewing conditions: First, while viewing repetitions of head (baseline); next, during \"morphed\" videos shifting gradually from head to had (ramp); then videos of had (maximum hold); and finally videos of head (after effects). Analysis with a linear mixedeffects model indicated a significant F1 difference between baseline and maximum hold phases for the productions of the treated PD group, but not for the untreated group. Implications for the causes and treatment of PD speech disorders are discussed",
    "checked": true,
    "id": "f53b79b6bb058db96b8847589a7eaf40a7e6aa47",
    "semantic_title": "sensorimotor response to tongue displacement imagery by talkers with parkinson's disease",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gupta18_interspeech.html": {
    "title": "Automatic Pronunciation Evaluation of Singing",
    "volume": "main",
    "abstract": "In this work, we develop a strategy to automatically evaluate pronunciation of singing. We apply singing-adapted automatic speech recognizer (ASR) in a two-stage approach for evaluating pronunciation of singing. First, we force-align the lyrics with the sung utterances to obtain the word boundaries. We improve the word boundaries by a novel lexical modification technique. Second, we investigate the performance of the phonetic posteriorgram (PPG) based template independent and dependent methods for scoring the aligned words. To validate the evaluation scheme, we obtain reliable human pronunciation evaluation scores using a crowd-sourcing platform. We show that the automatic evaluation scheme offers quality scores that are close to human judgments",
    "checked": true,
    "id": "a75a3bcf9a292953e9691e4a57ba7c68736343eb",
    "semantic_title": "automatic pronunciation evaluation of singing",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bouserhal18_interspeech.html": {
    "title": "Classification of Nonverbal Human Produced Audio Events: A Pilot Study",
    "volume": "main",
    "abstract": "The accurate classification of nonverbal human produced audio events opens the door to numerous applications beyond health monitoring. Voluntary events, such as tongue clicking and teeth chattering, may lead to a novel way of silent interface command. Involuntary events, such as coughing and clearing the throat, may advance the current state-of-the-art in hearing health research. The challenge of such applications is the balance between the processing capabilities of a small intra-aural device and the accuracy of classification. In this pilot study, 10 nonverbal audio events are captured inside the ear canal blocked by an intra-aural device. The performance of three classifiers is investigated: Gaussian Mixture Model (GMM), Support Vector Machine and Multi-Layer Perceptron. Each classifier is trained using three different feature vector structures constructed using the mel-frequency cepstral (MFCC) coefficients and their derivatives. Fusion of the MFCCs with the auditory-inspired amplitude modulation features (AAMF) is also investigated. Classification is compared between binaural and monaural training sets as well as for noisy and clean conditions. The highest accuracy is achieved at 75.45% using the GMM classifier with the binaural MFCC+AAMF clean training set. Accuracy of 73.47% is achieved by training and testing the classifier with the binaural clean and noisy dataset",
    "checked": true,
    "id": "7377c60db7b84138b5cfaeb920869f6131041daf",
    "semantic_title": "classification of nonverbal human produced audio events: a pilot study",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/spreafico18_interspeech.html": {
    "title": "UltraFit: A Speaker-friendly Headset for Ultrasound Recordings in Speech Science",
    "volume": "main",
    "abstract": "UltraFit is a headset for Ultrasound Tongue Imaging (UTI) printed in Nylon; altogether, it weighs about 350 g. It was developed through an iterative process of rapid prototyping a proof of concept, asking for feedback from researchers and subjects of the experiments and instantly incorporating changes based on their feedback into the design. We evaluated the UltraFit headset by recording a speaker using an optical marker tracking system that provides sub-millimeter tracking accuracy. We show that the overall error range of the headset movement for this speaker lies within 3mm with most errors lying in a 1-2mm range. This makes the headset potentially suitable for speech science applications. Furthermore, we analyze the superior usability of the headset compared to other existing designs and describe the headsets development process",
    "checked": true,
    "id": "8fd3ffdbb5fc062274d1a6e74384e1f75a51e58f",
    "semantic_title": "ultrafit: a speaker-friendly headset for ultrasound recordings in speech science",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cortes18_interspeech.html": {
    "title": "Articulatory Consequences of Vocal Effort Elicitation Method",
    "volume": "main",
    "abstract": "Articulatory features from two datasets, Slovak and Swedish, were compared to see whether different methods of eliciting loud speech (ambient noise vs. visually presented loudness target) result in different articulatory behavior. The features studied were temporal and kinematic characteristics of lip separation within the closing and opening gestures of bilabial consonants and of the tongue body movement from /i/ to /a/ through a bilabial consonant. The results indicate larger hyper-articulation in the speech elicited with visually presented target. While individual articulatory strategies are evident, the speaker groups agree on increasing the kinematic features consistently within each gesture in response to the increased vocal effort. Another concerted strategy is keeping the tongue response considerably smaller than that of the lips, presumably to preserve acoustic prerequisites necessary for the adequate vowel identity. While the method of visually presented loudness target elicits larger span of vocal effort, the two elicitation methods achieve comparable consistency per loudness conditions",
    "checked": true,
    "id": "c7d987a91382284fde5242beb97df0c25ce1b126",
    "semantic_title": "articulatory consequences of vocal effort elicitation method",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hermes18b_interspeech.html": {
    "title": "Age-related Effects on Sensorimotor Control of Speech Production",
    "volume": "main",
    "abstract": "The current study investigates the effect of aging on the speech motor control, more specifically the labial and lingual system. We provide an acoustic and articulatory analysis comparing younger (20-30 years old) and older speakers (70-80 years old) of German, all of them recorded with electromagnetic articulography. We analyzed target words in contrastive focus condition. In the acoustic domain, target syllables were not prolonged in the productions of the older speakers. However, when looking at the articulatory domain, we found systematic modifications: Especially vocalic gestures, requiring movements of the lingual system, showed slower peak velocities for older subjects. Furthermore, we found age-related effects on the symmetry of articulatory gestures. Older subjects produce longer deceleration and shorter acceleration phases leading to a strong asymmetry of the movement components. Variability between and across speakers were considerably higher in the group of older speakers compared to younger ones. Our results on age-related effects on speech motor control are comparable with those from general motor control, where e.g. prolonged deceleration phases are an indicator for a decrease in sensory feedback control",
    "checked": true,
    "id": "2103711283c390ccdd52856ad0e7733c392ab3eb",
    "semantic_title": "age-related effects on sensorimotor control of speech production",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/percival18_interspeech.html": {
    "title": "An Ultrasound Study of Gemination in Coronal Stops in Eastern Oromo",
    "volume": "main",
    "abstract": "This study extends the use of ultrasound methodology to stops in Eastern Oromo (Cushitic; Ethiopia) to examine the link between gemination, laryngeal features and tongue shape. Ultrasound data were collected from 5 native speakers of Eastern Oromo. Tokens consisted of 12 repetitions per speaker of [tʰ, t, d, ɗ] and six of [ttʰ, tt, dd, ɗɗ] in the environment of a_a. Tongue images at the point of maximum constriction during the stop closure were traced following Kochetov et al. (2014) and their coordinates submitted to linear mixed effects models. Results indicated differences in tongue shape between singletons and geminates, especially for ejectives and implosives. Singleton ejectives displayed raised tongue bodies not found in geminate ejectives. Singleton implosives resembled voiceless stops, but geminate implosives were variably produced with tongue body raising. I suggest that the results can be attributed to fortition in geminates. Tongue body raising in singleton ejectives may be an enhancement strategy to the ejective contrast that is not necessary in longer geminates. The singleton implosive resembling a voiceless aspirated stop is predicted by Lloret (1994) while the geminate tongue body raising may be retraction, c.f. Payne (2006). The results support a link between tongue, larynx and gemination",
    "checked": true,
    "id": "f816bf74418316ce9e5955420035173b15e9abbc",
    "semantic_title": "an ultrasound study of gemination in coronal stops in eastern oromo",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nomosudro18_interspeech.html": {
    "title": "Processing Transition Regions of Glottal Stop Substituted /S/ for Intelligibility Enhancement of Cleft Palate Speech",
    "volume": "main",
    "abstract": "The speech intelligibility of cleft palate (CP) individuals is de- graded primarily due to compensatory articulation errors and hypernasality. The present work proposes a method to enhance the CP speech intelligibility, where fricatives are substituted by compensatory articulation errors. Apart from the distortion present in the sustained fricative region, the fricative-vowel and vowel-fricative regions are also deviated due to co-articulation effect. Since important perceptual cues are embedded in the transition regions. Therefore, it is necessary to enhance the transition regions for more intelligibility. Motivated by the per- ceptual significance of the transition regions, 2D-DCT based joint spectro-temporal features are exploited for the modifica- tion. The 2D-DCT coefficients of CP speech are modified by projecting them onto the singular vectors derived from the SVD analysis of normal speech. Further, for the evaluation of speech intelligibility, objective and subjective assessment is conducted. The results show significant improvement in the speech intelli- gibility of the modified speech",
    "checked": true,
    "id": "8f480c542c679b4da64db2b85aec6da11b88c851",
    "semantic_title": "processing transition regions of glottal stop substituted /s/ for intelligibility enhancement of cleft palate speech",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/n18_interspeech.html": {
    "title": "Reconstructing Neutral Speech from Tracheoesophageal Speech",
    "volume": "main",
    "abstract": "In this work, we propose a tracheoesophageal (TE) speech to neutral speech conversion system using data collected from a laryngectomee. In laryngectomees, in the absence of vocal folds, it is the vibration of the esophagus that gives rise to a low-frequency pitch during speech production. This pitch is manifested as impulse-like noise in the recorded speech. We propose a method to first ‘whisperize' the TE speech prior to the linear predictive coding (LPC) based synthesis which uses pitch derived from the energy contour. In order to perform ‘whisperization', we model the LPC residual signal as the sum of white noise and impulses introduced by the esophageal vibrations. We model these impulses and white noise using Bernoulli-Gaussian distribution and Gaussian distribution, respectively. The strength and location of the impulses are estimated using Gibbs sampling in order to remove the impulse-like noise from speech to obtain whispered speech. Subjective evaluation via listening test reveals that the ‘whisperization' step in the proposed method aids in synthesizing a more natural sounding neutral speech. A different listening test shows that the listeners prefer the synthesized speech from the proposed method ∼ 93% (absolute) times more than the best baseline scheme",
    "checked": true,
    "id": "f6f623df5487b626a141616fc34b5b3049a59f8b",
    "semantic_title": "reconstructing neutral speech from tracheoesophageal speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ochi18_interspeech.html": {
    "title": "Automatic Evaluation of Soft Articulatory Contact for Stuttering Treatment",
    "volume": "main",
    "abstract": "We describe a new method for the automatic discrimination and evaluation of phonation beginning with a consonant with soft articulatory contact, which is used in the treatment of stuttering and normal phonation. Soft articulatory contact is trained to relax articulators and remove hard contacts that occur during stuttering. We use features related to the changes in acoustic characteristics and the voice quality under the hypothesis that the slowing down of articulatory movement of the initial consonant and the relaxing of phonatory muscles co-occur with soft articulatory contact. The results of an experimental evaluation showed that high accuracy was obtained when acoustic features were related to the peaks of the first derivative of the mel frequency cepstral coefficients (MFCCs) corresponded to the slowing down of the movement of the articulators. The features of vocal quality only slightly contributed to the classification",
    "checked": true,
    "id": "152516c52c1c6e29b30148435a57524baa37d870",
    "semantic_title": "automatic evaluation of soft articulatory contact for stuttering treatment",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kim18b_interspeech.html": {
    "title": "Korean Singing Voice Synthesis Based on an LSTM Recurrent Neural Network",
    "volume": "main",
    "abstract": "Singing voice synthesis (SVS) systems generate the singing voice from a musical score. Similar to the text-to-speech synthesis (TTS) field, SVS systems have also been greatly improved since the deep neural network (DNN) framework was introduced. Although they share many parts of the framework, the main difference between TTS and SVS systems is that the feature composing method, between linguistic and musical features, is important for SVS systems. In this paper, we propose a Korean SVS system based on a long-short term memory recurrent neural network (LSTM-RNN). At the feature composing stage, we propose a novel composing method, based on Korean syllable structure. At the synthesis stage, we adopt LSTM-RNN for the SVS. According to our experiments, our composed feature improved the naturalness of the voice, specifically in any part that has to be pronounced for a long time. Furthermore, LSTM-RNN outperformed the DNN based SVS system in both quantitative and qualitative evaluations",
    "checked": true,
    "id": "53939b59153aaf7243e6efe42529466747889d6f",
    "semantic_title": "korean singing voice synthesis based on an lstm recurrent neural network",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xuanda18_interspeech.html": {
    "title": "The Trajectory of Voice Onset Time with Vocal Aging",
    "volume": "main",
    "abstract": "Vocal aging, a universal process of human aging, can largely affect one's language use, possibly including some subtle acoustic features of one's utterances like Voice Onset Time. To figure out the time effects, Queen Elizabeth's Christmas speeches are documented and analyzed in the long-term trend. We build statistical models of time dependence in Voice Onset Time, controlling a wide range of other fixed factors, to present annual variations and the simulated trajectory. It is revealed that the variation range of Voice Onset Time has been narrowing over fifty years with a slight reduction in the mean value, which, possibly, is an effect of diminishing exertion, resulting from subdued muscle contraction, transcending other non-linguistic factors in forming Voice Onset Time patterns over a long time",
    "checked": true,
    "id": "1137ba17c6f23908eaaefee19f2517ca45d7314c",
    "semantic_title": "the trajectory of voice onset time with vocal aging",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/barker18_interspeech.html": {
    "title": "The Fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset, Task and Baselines",
    "volume": "main",
    "abstract": "The CHiME challenge series aims to advance robust automatic speech recognition (ASR) technology by promoting research at the interface of speech and language processing, signal processing and machine learning. This paper introduces the 5th CHiME Challenge, which considers the task of distant multi-microphone conversational ASR in real home environments. Speech material was elicited using a dinner party scenario with efforts taken to capture data that is representative of natural conversational speech and recorded by 6 Kinect microphone arrays and 4 binaural microphone pairs. The challenge features a single-array track and a multiple-array track and, for each track, distinct rankings will be produced for systems focusing on robustness with respect to distant-microphone capture versus systems attempting to address all aspects of the task including conversational language modeling. We discuss the rationale for the challenge and provide a detailed description of the data collection procedure, the task and the baseline systems for array synchronization, speech enhancement and conventional and end-to-end ASR",
    "checked": true,
    "id": "eadf5023c90a6af8a0f8e8605bd8050cc13c23a3",
    "semantic_title": "the fifth 'chime' speech separation and recognition challenge: dataset, task and baselines",
    "citation_count": 816
  },
  "https://www.isca-speech.org/archive/interspeech_2018/richey18_interspeech.html": {
    "title": "Voices Obscured in Complex Environmental Settings (VOiCES) Corpus",
    "volume": "main",
    "abstract": "This paper introduces the Voices Obscured In Complex Environmental Settings (VOiCES) corpus, a freely available dataset under Creative Commons BY 4.0. This dataset will promote speech and signal processing research of speech recorded by far-field microphones in noisy room conditions. Publicly available speech corpora are mostly composed of isolated speech at close-range microphony. A typical approach to better represent realistic scenarios, is to convolve clean speech with noise and simulated room response for model training. Despite these efforts, model performance degrades when tested against uncurated speech in natural conditions. For this corpus, audio was recorded in furnished rooms with background noise played in conjunction with foreground speech selected from the LibriSpeech corpus. Multiple sessions were recorded in each room to accommodate for all foreground speech-background noise combinations. Audio was recorded using twelve microphones placed throughout the room, resulting in 120 hours of audio per microphone. This work is a multi-organizational effort led by SRI International and Lab41 with the intent to push forward state-of-the-art distant microphone approaches in signal processing and speech recognition",
    "checked": true,
    "id": "93122c7d659c036e9e290aebd0a28812cd71453c",
    "semantic_title": "voices obscured in complex environmental settings (voices) corpus",
    "citation_count": 93
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18d_interspeech.html": {
    "title": "Building State-of-the-art Distant Speech Recognition Using the CHiME-4 Challenge with a Setup of Speech Enhancement Baseline",
    "volume": "main",
    "abstract": "This paper describes a new baseline system for automatic speech recognition (ASR) in the CHiME-4 challenge to promote the development of noisy ASR in speech processing communities by providing 1) state-of-the-art system with a simplified single system comparable to the complicated top systems in the challenge, 2) publicly available and reproducible recipe through the main repository in the Kaldi speech recognition toolkit. The proposed system adopts generalized eigenvalue beamforming with bidirectional long short-term memory (LSTM) mask estimation. We also propose to use a time delay neural network (TDNN) based on the lattice-free version of the maximum mutual information (LF-MMI) trained with augmented all six microphones plus the enhanced data after beamforming. Finally, we use a LSTM language model for lattice and n-best re-scoring. The final system achieved 2.74% WER for the real test set in the 6-channel track, which corresponds to the 2nd place in the challenge. In addition, the proposed baseline recipe includes four different speech enhancement measures, short-time objective intelligibility measure (STOI), extended STOI (eSTOI), perceptual evaluation of speech quality (PESQ) and speech distortion ratio (SDR) for the simulation test set. Thus, the recipe also provides an experimental platform for speech enhancement studies with these performance measures",
    "checked": true,
    "id": "d15a7d00897f58a94def2a58c0cb0311851f2968",
    "semantic_title": "building state-of-the-art distant speech recognition using the chime-4 challenge with a setup of speech enhancement baseline",
    "citation_count": 57
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hsu18b_interspeech.html": {
    "title": "Unsupervised Adaptation with Interpretable Disentangled Representations for Distant Conversational Speech Recognition",
    "volume": "main",
    "abstract": "The current trend in automatic speech recognition is to leverage large amounts of labeled data to train supervised neural network models. Unfortunately, obtaining data for a wide range of domains to train robust models can be costly. However, it is relatively inexpensive to collect large amounts of unlabeled data from domains that we want the models to generalize to. In this paper, we propose a novel unsupervised adaptation method that learns to synthesize labeled data for the target domain from unlabeled in-domain data and labeled out-of-domain data. We first learn without supervision an interpretable latent representation of speech that encodes linguistic and nuisance factors (e.g., speaker and channel) using different latent variables. To transform a labeled out-of-domain utterance without altering its transcript, we transform the latent nuisance variables while maintaining the linguistic variables. To demonstrate our approach, we focus on a channel mismatch setting, where the domain of interest is distant conversational speech and labels are only available for close-talking speech. Our proposed method is evaluated on the AMI dataset, outperforming all baselines and bridging the gap between unadapted and in-domain models by over 77% without using any parallel data",
    "checked": true,
    "id": "302ea202f25c996c3ae5b9538489ad2f4ad24c20",
    "semantic_title": "unsupervised adaptation with interpretable disentangled representations for distant conversational speech recognition",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18i_interspeech.html": {
    "title": "Investigating Generative Adversarial Networks Based Speech Dereverberation for Robust Speech Recognition",
    "volume": "main",
    "abstract": "We investigate the use of generative adversarial networks (GANs) in speech dereverberation for robust speech recognition. GANs have been recently studied for speech enhancement to remove additive noises, but there still lacks of a work to examine their ability in speech dereverberation and the advantages of using GANs have not been fully established. In this paper, we provide deep investigations in the use of GAN-based dereverberation front-end in ASR. First, we study the effectiveness of different dereverberation networks (the generator in GAN) and find that LSTM leads a significant improvement as compared with feed-forward DNN and CNN in our dataset. Second, further adding residual connections in the deep LSTMs can boost the performance as well. Finally, we find that, for the success of GAN, it is important to update the generator and the discriminator using the same mini-batch data during training. Moreover, using reverberant spectrogram as a condition to discriminator, as suggested in previous studies, may degrade the performance. In summary, our GAN-based dereverberation front-end achieves 14%~19% relative CER reduction as compared to the baseline DNN dereverberation network when tested on a strong multi-condition training acoustic model",
    "checked": true,
    "id": "6d206565c4c94be3684b313c0f33e082d785eb8e",
    "semantic_title": "investigating generative adversarial networks based speech dereverberation for robust speech recognition",
    "citation_count": 36
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chang18_interspeech.html": {
    "title": "Monaural Multi-Talker Speech Recognition with Attention Mechanism and Gated Convolutional Networks",
    "volume": "main",
    "abstract": "To improve the speech recognition accuracy under the multi-talker scenario, we propose a novel model architecture that incorporates the attention mechanism and gated convolutional network (GCN) into our previously developed permutation invariant training based multi-talker speech recognition system (PIT-ASR). The new architecture has three components: an encoding transformer, an attention module and a frame-level senone predictor. The encoding transformer first transforms a mixed speech sequence into a sequence of embedding vectors. Then the attention mechanism extracts individual context vectors from this embedding sequence for different speaker sources. Finally the predictor generates the senone posteriors for all speaker sources independently with the knowledge from the context vectors. To get better embedding representations we explore gated convolutional networks in the encoding transformer. The experimental results on the artificially mixed two-talker WSJ0 corpus show that our proposed model can reduce the word error rate (WER) by more than 15% relatively compared to our previous PIT-ASR system",
    "checked": true,
    "id": "25771a5573276bbaa3bd1d09ecd1002cb8eba9b0",
    "semantic_title": "monaural multi-talker speech recognition with attention mechanism and gated convolutional networks",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2018/do18_interspeech.html": {
    "title": "Weighting Time-Frequency Representation of Speech Using Auditory Saliency for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "This paper proposes a new method for weighting two-dimensional (2D) time-frequency (T-F) representation of speech using auditory saliency for noise-robust automatic speech recognition (ASR). Auditory saliency is estimated via 2D auditory saliency maps which model the mechanism for allocating human auditory attention. These maps are used to weight T-F representation of speech, namely the 2D magnitude spectrum or spectrogram, prior to features extraction for ASR. Experiments on Aurora-4 corpus demonstrate the effectiveness of the proposed method for noise-robust ASR. In multi-stream ASR, relative word error rate (WER) reduction of up to 5.3% and 4.0% are observed when comparing the multi-stream system using the proposed method with the baseline single-stream system not using T-F representation weighting and that using conventional spectral masking noise-robust technique, respectively. Combining the multi-stream system using the proposed method and the single-stream system using the conventional spectral masking technique reduces further the WER",
    "checked": true,
    "id": "e27407af689b01edc434eccc18a78b8f29923e38",
    "semantic_title": "weighting time-frequency representation of speech using auditory saliency for automatic speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ghahremani18_interspeech.html": {
    "title": "Acoustic Modeling from Frequency Domain Representations of Speech",
    "volume": "main",
    "abstract": "In recent years, different studies have proposed new methods for DNN-based feature extraction and joint acoustic model training and feature learning from raw waveform for large vocabulary speech recognition. However, conventional pre-processed methods such as MFCC and PLP are still preferred in the state-of-the-art speech recognition systems as they are perceived to be more robust. Besides, the raw waveform methods - most of which are based on the time-domain signal - do not significantly outperform the conventional methods. In this paper, we propose a frequency-domain feature-learning layer which can allow acoustic model training directly from the waveform. The main distinctions from previous works are a new normalization block and a short-range constraint on the filter weights. The proposed setup achieves consistent performance improvements compared to the baseline MFCC and log-Mel features as well as other proposed time and frequency domain setups on different LVCSR tasks. Finally, based on the learned filters in our feature-learning layer, we propose a new set of analytic filters using polynomial approximation, which outperforms log-Mel filters significantly while being equally fast",
    "checked": true,
    "id": "476a781840a3a906cc8fdb045108c4702e089601",
    "semantic_title": "acoustic modeling from frequency domain representations of speech",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yadav18b_interspeech.html": {
    "title": "Non-Uniform Spectral Smoothing for Robust Children's Speech Recognition",
    "volume": "main",
    "abstract": "Insufficient spectral smoothing during front-end speech parametrization results in pitch-induced distortions in the short-time magnitude spectra. This, in turn, degrades the performance of an automatic speech recognition (ASR) system for high-pitched speakers. Motivated by this fact, a non-uniform spectral smoothing algorithm is proposed in this paper in order to mitigate the acoustic mismatch resulting from pitch differences. In the proposed technique, the speech utterance is first segmented into vowel and non-vowel regions. The short-time magnitude spectrum obtained by discrete Fourier transform is then processed through a single-pole low-pass filter with different pole values for vowel and non-vowel regions. Sufficiently smoothed spectra is obtained by keeping higher values for the pole in the case of vowels while lower values are chosen for non-vowel regions. The Mel-frequency cepstral coefficients computed using the derived smoothed spectra are observed to be less affected by pitch variations. In order to validate this claim, an ASR system is developed on speech from adult speakers and evaluated on a test set which consists of children's speech to simulate large pitch differences. The experimental evaluations as well as signal domain analyses presented in this paper support the claim",
    "checked": true,
    "id": "3c36761b6f31e77eb36d4f243dcef06d097ea683",
    "semantic_title": "non-uniform spectral smoothing for robust children's speech recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nicolson18_interspeech.html": {
    "title": "Bidirectional Long-Short Term Memory Network-based Estimation of Reliable Spectral Component Locations",
    "volume": "main",
    "abstract": "An accurate Ideal Binary Mask (IBM) estimate is essential for Missing Feature Theory (MFT)-based speaker identification, as incorrectly labelled spectral components (where a component is either reliable or unreliable) will degrade the performance of an Automatic Speaker Identification (ASI) system adversely in the presence of noise. In this work a Bidirectional Recurrent Neural Network (BRNN) with Long-Short Term Memory (LSTM) cells is proposed for improved IBM estimation. The proposed system had an average IBM estimate accuracy improvement of 4.5% and an average MFT-based speaker identification accuracy improvement of 3.1% over all tested SNR dB levels, when compared to the previously proposed Multilayer Perceptron (MLP)-IBM estimator. When used for speech enhancement the proposed system had an average MOS-LQO (objective quality measure) improvement of 0.32 and an average QSTI (objective intelligibility measure) improvement of 0.01 over all tested SNR dB levels, when compared to the MLP-IBM estimator. The results presented in this work highlight the effectiveness of the proposed BRNN-IBM estimator for MFT-based speaker identification and IBM-based speech enhancement",
    "checked": true,
    "id": "a7bc838ea915f07fb836df70c4903627d6ecab55",
    "semantic_title": "bidirectional long-short term memory network-based estimation of reliable spectral component locations",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/guo18c_interspeech.html": {
    "title": "Speech Emotion Recognition by Combining Amplitude and Phase Information Using Convolutional Neural Network",
    "volume": "main",
    "abstract": "Previous studies of speech emotion recognition utilize convolutional neural network (CNN) directly on amplitude spectrogram to extract features. CNN combines with bidirectional long short term memory (BLSTM) has become the state-of-the-art model. However, phase information has been ignored in this model. The importance of phase information in speech processing field is gathering attention. In this paper, we propose feature extraction of amplitude spectrogram and phase information using CNN for speech emotion recognition. The modified group delay cepstral coefficient (MGDCC) and relative phase are used as phase information. Firstly, we analyze the influence of phase information on speech emotion recognition. Then we design a CNN-based feature representation using amplitude and phase information. Finally, experiments were conducted on EmoDB to validate the effectiveness of phase information. Integrating amplitude spectrogram with phase information, the relative emotion error recognition rates are reduced by over 33% in comparison with using only amplitude-based feature",
    "checked": true,
    "id": "7533c8428eb8726472c99918849bcd536292dbc5",
    "semantic_title": "speech emotion recognition by combining amplitude and phase information using convolutional neural network",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2018/trinh18_interspeech.html": {
    "title": "Bubble Cooperative Networks for Identifying Important Speech Cues",
    "volume": "main",
    "abstract": "Predicting the intelligibility of noisy recordings is difficult and most current algorithms treat all speech energy as equally important to intelligibility. Our previous work on human perception used a listening test paradigm and correlational analysis to show that some energy is more important to intelligibility than other energy. In this paper, we propose a system called the Bubble Cooperative Network (BCN), which aims to predict important areas of individual utterances directly from clean speech. Given such a prediction, noise is added to the utterance in unimportant regions and then presented to a recognizer. The BCN is trained with a loss that encourages it to add as much noise as possible while preserving recognition performance, encouraging it to identify important regions precisely and place the noise everywhere else. Empirical evaluation shows that the BCN can obscure 97.7% of the spectrogram with noise while maintaining recognition accuracy for a simple speech recognizer that compares a noisy test utterance with a clean reference utterance. The masks predicted by a single BCN on several utterances show patterns that are similar to analyses derived from human listening tests that analyze each utterance separately, while exhibiting better generalization and less context-dependence than previous approaches",
    "checked": true,
    "id": "c21d43f47d51e2bbd01139519ce412bbecbc8be1",
    "semantic_title": "bubble cooperative networks for identifying important speech cues",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cheng18_interspeech.html": {
    "title": "Real-Time Scoring of an Oral Reading Assessment on Mobile Devices",
    "volume": "main",
    "abstract": "We discuss the real-time scoring logic for a self-administered oral reading assessment on mobile devices (Moby.Read) to measure the three components of children's oral reading fluency skills: words correct per minute, expression and comprehension. Critical techniques that make the assessment real-time on-device are discussed in detail. We propose the idea of producing comprehension scores by measuring the semantic similarity between the prompt and the retelling response utilizing the recent advance of document embeddings in natural language processing. By combining features derived from word embedding with the normalized number of common types, we achieved a human-machine correlation coefficient of 0.90 at the participant level for comprehension scores, which was better than the human inter-rater correlation 0.88. We achieved a better human-machine correlation coefficient than that of the human inter-rater in expression scores too. Experimental results demonstrate that Moby.Read can provide highly accurate words correct per minute, expression and comprehension scores in real-time and validate the use of machine scoring methods to automatically measure oral reading fluency skills",
    "checked": true,
    "id": "5a585c45ae59d462753b6d4ef802d42a790f67d0",
    "semantic_title": "real-time scoring of an oral reading assessment on mobile devices",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kyriakopoulos18_interspeech.html": {
    "title": "A Deep Learning Approach to Assessing Non-native Pronunciation of English Using Phone Distances",
    "volume": "main",
    "abstract": "The way a non-native speaker pronounces the phones of a language is an important predictor of their proficiency. In grading spontaneous speech, the pairwise distances between generative statistical models trained on each phone have been shown to be powerful features. This paper presents a deep learning alternative to model-based phone distances in the form of a tunable Siamese network feature extractor to extract distance metrics directly from the audio frame sequence. Features are extracted at the phone instance level and combined to phone-level representations using an attention mechanism. Pair-wise distances between phone features are then projected through a feed-forward layer to predict score. The extraction stage is initialised on either a binary phone instance-pair classification task, or to mimic the model-based features, then the whole system is fine-tuned end-to-end, optimising the learning of the distance metric to the score prediction task. This method is therefore more adaptable and more sensitive to phone instance level phenomena. Its performance is compared against a DNN trained on Gaussian phone model distance features",
    "checked": true,
    "id": "1800abcbd9d07ee2e13ba89ceb10fbcec54cbe17",
    "semantic_title": "a deep learning approach to assessing non-native pronunciation of english using phone distances",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xiao18b_interspeech.html": {
    "title": "Paired Phone-Posteriors Approach to ESL Pronunciation Quality Assessment",
    "volume": "main",
    "abstract": "This work proposes to incorporate paired phone-posteriors as input features into a neural net (NN) model for assessing ESL learner's pronunciation quality. In this work, posteriors of forty phones, instead of several thousand sub-phonemic senones, are used to circumvent the sparsity issues in NN training. Phone posteriors are assembled with their corresponding senone posteriors estimated via a speaker-independent, DNN-based acoustic model, trained with standard American English speech data (i.e., Wall Street Journal database). Phone posteriors of both reference(standard American English speaker) and test speaker are paired together as augmented input feature vectors to train an NN based, 2-class, i.e., native vs nonnative speaker, classiﬁer. The Goodness of Pronunciation (GOP), a proven effective measure, is used as the baseline for comparison. The binary NN classiﬁer trained with such features achieves a high classification accuracy of 89.6% on native and non-native speakers' data. The classiﬁer also shows a better equal error rate (EER) than the GOP-based baseline classiﬁer in either phone or word level pronunciation, i.e., at phone level from 18.3% to 6.2% and at word level from 12.98% to 2.54%",
    "checked": true,
    "id": "931d3b0d8b70337867024af0fdcef9c112ab697c",
    "semantic_title": "paired phone-posteriors approach to esl pronunciation quality assessment",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tu18_interspeech.html": {
    "title": "Investigating the Role of L1 in Automatic Pronunciation Evaluation of L2 Speech",
    "volume": "main",
    "abstract": "Automatic pronunciation evaluation plays an important role in pronunciation training and second language education. This field draws heavily on concepts from automatic speech recognition (ASR) to quantify how close the pronunciation of non-native speech is to native-like pronunciation. However, it is known that the formation of accent is related to pronunciation patterns of both the target language (L2) and the speaker's first language (L1). In this paper, we propose to use two native speech acoustic models, one trained on L2 speech and the other trained on L1 speech. We develop two sets of measurements that can be extracted from two acoustic models given accented speech. A new utterance-level feature extraction scheme is used to convert these measurements into a fixed-dimension vector which is used as an input to a statistical model to predict the accentedness of a speaker. On a data set consisting of speakers from 4 different L1 backgrounds, we show that the proposed system yields improved correlation with human evaluators compared to systems only using the L2 acoustic model",
    "checked": true,
    "id": "ca1bf0c471cc56faa7006cb9c53c10a6792a29a8",
    "semantic_title": "investigating the role of l1 in automatic pronunciation evaluation of l2 speech",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2018/knill18_interspeech.html": {
    "title": "Impact of ASR Performance on Free Speaking Language Assessment",
    "volume": "main",
    "abstract": "In free speaking tests candidates respond in spontaneous speech to prompts. This form of test allows the spoken language proficiency of a non-native speaker of English to be assessed more fully than read aloud tests. As the candidate's responses are unscripted, transcription by automatic speech recognition (ASR) is essential for automated assessment. ASR will never be 100% accurate so any assessment system must seek to minimise and mitigate ASR errors. This paper considers the impact of ASR errors on the performance of free speaking test auto-marking systems. Firstly rich linguistically related features, based on part-of-speech tags from statistical parse trees, are investigated for assessment. Then, the impact of ASR errors on how well the system can detect whether a learner's answer is relevant to the question asked is evaluated. Finally, the impact that these errors may have on the ability of the system to provide detailed feedback to the learner is analysed. In particular, pronunciation and grammatical errors are considered as these are important in helping a learner to make progress. As feedback resulting from an ASR error would be highly confusing, an approach to mitigate this problem using confidence scores is also analysed",
    "checked": true,
    "id": "bda119ed9161d29259bde7b554b4a501047a661e",
    "semantic_title": "impact of asr performance on free speaking language assessment",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hong18_interspeech.html": {
    "title": "Automatic Miscue Detection Using RNN Based Models with Data Augmentation",
    "volume": "main",
    "abstract": "This study proposes a method of using data augmentation to address the problem of data shortages in miscue detection tasks. Three main steps were taken. First, a phoneme classifier was developed to acquire force-aligned data, which would be used for miscue classification and data augmentation. In order to create the phoneme classifier, phonetic features of \"Seoul Reading Speech\" (SRS) corpus were extracted by using grapheme-to-phoneme (G2P) to train CNN-based models. Second, to obtain miscue labeled corpus, we performed data augmentation using the phoneme classifier output, which is artificially generated miscue corpus of SRS (modified-SRS). This miscue corpus was created by randomly deleting or modifying sound sections according to three miscue categories; extension (EXT), pause (PAU) and pre-correction (PRE). Third, the performance of the miscue classifier was tested after training three types of RNN based models (LSTM, BiLSTM, BiGRU) with the modified-SRS corpus. The results show that the BiGRU model performed best at 0.819 in F1-score on augmented data, while BiLSTM model performed best at 0.512 on real data",
    "checked": true,
    "id": "b9195f86e989d4e8638ffa1dfbd5b8a86abf6b8c",
    "semantic_title": "automatic miscue detection using rnn based models with data augmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/inoue18_interspeech.html": {
    "title": "A Study of Objective Measurement of Comprehensibility through Native Speakers' Shadowing of Learners' Utterances",
    "volume": "main",
    "abstract": "While learners desire to acquire so comprehensible pronunciations as to make themselves understood smoothly, acquisition often becomes difficult because, outside of classrooms, it is not rare that learners can hardly find chances to talk in the target language. Even when they talk to native speakers, they may receive only lenient or superficial suggestions from native speakers. How can learners know native speakers' honest perception on their utterances? In this paper, shadowing is introduced not to learners but to native listeners, who are asked to shadow learners' utterances. Since shadowing is as simultaneous repetition as possible, it is expected that native listeners' perceived comprehensibility can be measured objectively as smoothness of natives' shadowings. Experiments show that 1) shadowers' subjective assessment of learners' speech and that of their shadowings are highly correlated and that 2) the former is more correlated with the GOP scores of natives' shadowings than that of learners' speech. These results indicate that it is valid to regard comprehensible pronunciation as shadowable pronunciation",
    "checked": true,
    "id": "1f5b5b2741882abfd58932d31ac143450fbda6ef",
    "semantic_title": "a study of objective measurement of comprehensibility through native speakers' shadowing of learners' utterances",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/luo18b_interspeech.html": {
    "title": "Factorized Deep Neural Network Adaptation for Automatic Scoring of L2 Speech in English Speaking Tests",
    "volume": "main",
    "abstract": "Speaker adaptation has been shown to be effective on speech recognition and evaluation of L2 speech. However, other factors, such as environments and foreign accents, can affect the speech signal in addition to speakers. Factorizing the speaker, environment and other acoustic factors is crucial in evaluating L2 speech to effectively reduce acoustic mismatch between train and test conditions. In this study, we investigate the effects of deep neural network factorized adaptation techniques on L2 speech assessment in real speaking tests. Through recognition and automatic scoring experiments on L2 speech, we demonstrate that factorized fMLLR and iVector based DNN adaptation can better utilize adaptation data to efficiently adapt to complex speaker and environment conditions. Combining the factored components of iVectors and fMLLR transforms can further improve robustness of DNN models in speech recognition and automatic scoring of L2 speech in dynamic environments",
    "checked": true,
    "id": "dce96165dba0fc142f52c883cc1643274898a688",
    "semantic_title": "factorized deep neural network adaptation for automatic scoring of l2 speech in english speaking tests",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yeung18_interspeech.html": {
    "title": "On the Difficulties of Automatic Speech Recognition for Kindergarten-Aged Children",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) systems for children have lagged behind in performance when compared to adult ASR. The exact problems and evaluation methods for child ASR have not yet been fully investigated. Recent work from the robotics community suggests that ASR for kindergarten speech is especially difficult, even though this age group may benefit most from voice-based educational and diagnostic tools. Our study focused on ASR performance for specific grade levels (K-10) using a word identification task. Grade-specific ASR systems were evaluated, with particular attention placed on the evaluation of kindergarten-aged children (5-6 years old). Experiments included investigation of grade-specific interactions with triphone models using feature space maximum likelihood linear regression (fMLLR), vocal tract length normalization (VTLN) and subglottal resonance (SGR) normalization. Our results indicate that kindergarten ASR performs dramatically worse than even 1st grade ASR, likely due to large speech variability at that age. As such, ASR systems may require targeted evaluations on kindergarten speech rather than being evaluated under the guise of \"child ASR.\" Additionally, results show that systems trained in matched conditions on kindergarten speech may be less suitable than mismatched-grade training with 1st grade speech. Finally, we analyzed the phonetic errors made by the kindergarten ASR",
    "checked": true,
    "id": "65343c6d882cf4efccb510a96bbbfbc6e8cc6511",
    "semantic_title": "on the difficulties of automatic speech recognition for kindergarten-aged children",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nicolao18_interspeech.html": {
    "title": "Improved Acoustic Modelling for Automatic Literacy Assessment of Children",
    "volume": "main",
    "abstract": "Automatic literacy assessment of children is a complex task that normally requires carefully annotated data. This paper focuses on a system for the assessment of reading skills, aiming to detection of a range of fluency and pronunciation errors. Naturally, reading is a prompted task and thereby the acquisition of training data for acoustic modelling should be straightforward. However, given the prominence of errors in the training set and the importance of labelling them in the transcription, a lightly supervised approach to acoustic modelling has better chances of success. A method based on weighted finite state transducers is proposed, to model specific prompt corrections, such as repetitions, substitutions and deletions, as observed in real recordings. Iterative cycles of lightly-supervised training are performed in which decoding improves the transcriptions and the derived models. Improvements are due to increasing accuracy in phone-to-sound alignment and in the training data selection. The effectiveness of the proposed methods for relabelling and acoustic modelling is assessed through experiemnts on the CHOREC corpus, in terms of sequence error rate and alignment accuracy. Improvements over the baseline of up to 60% and 23.3% respectively are observed",
    "checked": true,
    "id": "dea08b8f557a369c5b3a3c0c54cf91a344573f48",
    "semantic_title": "improved acoustic modelling for automatic literacy assessment of children",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shahin18_interspeech.html": {
    "title": "Anomaly Detection Approach for Pronunciation Verification of Disordered Speech Using Speech Attribute Features",
    "volume": "main",
    "abstract": "The automatic assessment of speech is a powerful tool in computer aided speech therapy for disorders such as Childhood Apraxia of Speech (CAS). However, the lack of sufficient annotated disordered speech data seriously impedes the accurate detection of pronunciation errors. To handle this deficiency, in this paper, we used the novel approach of tackling pronunciation verification as an anomaly detection problem. We achieved this by modeling only the correct pronunciation of each individual phoneme with a one-class Support Vector Machine (SVM) trained using a set of speech attributes features, namely the manner and place of articulation. These features are extracted from a bank of pre-trained Deep Neural Network (DNN) speech attributes classifiers. The one-class SVM model classifies each phoneme production as normal (correct) or an anomaly (incorrect). We evaluated the system using both native speech with artificial errors and disordered speech collected from children with apraxia of speech and compared it with the DNN Goodness of Pronunciation (GOP) algorithm. The results show that our approach reduces the false-rejection rates by around 35% when applied to disordered speech",
    "checked": true,
    "id": "b9849ef2b7c28fa5c120c4614d50cd1d990ec3d6",
    "semantic_title": "anomaly detection approach for pronunciation verification of disordered speech using speech attribute features",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2018/afshan18_interspeech.html": {
    "title": "Effectiveness of Voice Quality Features in Detecting Depression",
    "volume": "main",
    "abstract": "Automatic assessment of depression from speech signals is affected by variabilities in acoustic content and speakers. In this study, we focused on addressing these variabilities. We used a database comprised of recordings of interviews from a large number of female speakers: 735 individuals suffering from depressive (dysthymia and major depression) and anxiety disorders (generalized anxiety disorder, panic disorder with or without agoraphobia) and 953 healthy individuals. Leveraging this unique and extensive database, we built an i-vector framework. In order to capture various aspects of speech signals, we used voice quality features in addition to conventional cepstral features. The features (F0, F1, F2, F3, H1-H2, H2-H4, H4-H2k, A1, A2, A3 and CPP) were inspired by a psychoacoustic model of voice quality [1]. An i-vector-based system using Mel Frequency Cepstral Coefficients (MFCCs) and another using voice quality features was developed. Voice quality features performed as well as MFCCs. A score-level fusion was then used to combine these two systems, resulting in a 6% relative improvement in accuracy in comparison with the i-vector system based on MFCCs alone. The system was robust even when the duration of the utterances was shortened to 10 seconds",
    "checked": true,
    "id": "6b69078206adbc8bda31c9db07d9348138abfea6",
    "semantic_title": "effectiveness of voice quality features in detecting depression",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kothalkar18_interspeech.html": {
    "title": "Fusing Text-dependent Word-level i-Vector Models to Screen ‘at Risk' Child Speech",
    "volume": "main",
    "abstract": "Speech sound disorders (SSDs) are the most prevalent type of communication disorder among preschoolers. The earlier an SSD is identified, the earlier an intervention can be provided to potentially reduce the social/academic impact of the disorder. The challenge, lies in early identification of such disorders. In this study 29 carefully selected words were produced by 165 children from 3-6 years of age. The audio recordings, were collected by parents using a mobile application /platform. \"Ground truth\" child status as 'typically developing' vs 'at risk' was based on a percentage of consonants correct-revised growth curve model. State-of-the-art speech processing/speaker recognition models were employed along with our clinical group verification framework. Results showed that text-dependent i-Vector models were superior to both text dependent and text-independent Gaussian Mixture Models (GMMs) for correct classification of children. Fusing individual word, i-Vector models provides insight into word and consonant groupings that are more indicative of 'at risk' child speech",
    "checked": false,
    "id": "b720c1287b92945e08cc69c4e2eaf4cb78350bbb",
    "semantic_title": "fusing text-dependent word-level i-vector models to screen 'at risk' child speech",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chandrashekar18_interspeech.html": {
    "title": "Testing Paradigms for Assistive Hearing Devices in Diverse Acoustic Environments",
    "volume": "main",
    "abstract": "Many individuals worldwide are at risk of hearing loss due to unsafe acoustical exposure and chronic listening experience using personal audio devices. Assistive hearing devices(AHD), such as hearing-aids(HAs) and cochlear-implants(CIs) are a common choice for the restoration and rehabilitation of the auditory function. Audio sound processors in CIs and HAs operate within limits, prescribed by audiologists, not only for acceptable sound perception but also for safety reasons. Signal processing(SP) engineers follow best design practices to ensure reliable performance and incorporate necessary safety checks within the design of SP strategies to ensure safety limits are never exceeded irrespective of acoustic environments. This paper proposes a comprehensive testing and evaluation paradigm to investigate the behavior of audio devices that addresses the safety concerns in diverse acoustic conditions. This is achieved by characterizing the performance of devices with large amounts of acoustic inputs and monitoring the output behavior. The CCi-MOBILE Research-Interface(RI) (used for CI/HA research) is used in this study as the testing paradigm. Factors such as pulse-width(PW), inter-phase gap(IPG) and a number of other parameters are estimated to evaluate the impact of AHDs on hearing comfort, subjective sound quality and characterize audio devices in terms of listening perception and biological safety",
    "checked": true,
    "id": "2388857d2879745ff681e1531dc7afd5254cd9fb",
    "semantic_title": "testing paradigms for assistive hearing devices in diverse acoustic environments",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ujiro18_interspeech.html": {
    "title": "Detection of Dementia from Responses to Atypical Questions Asked by Embodied Conversational Agents",
    "volume": "main",
    "abstract": "Detection of dementia requires examinations, such as blood tests and functional magnetic resonance imaging (fMRI), that can be very stressful for the patient. Previous studies proposed screenings for easy detection of dementia that utilized acoustic and language information derived from conversations between patients and medical staff. Although these studies demonstrated effectiveness in automatically detecting dementia, the tasks used were created based on neuropsychological tests. The effect of habituation on this limited variety of tasks might have a negative impact on routine dementia screening. We propose a method to detect dementia using responses to more atypical questions asked by embodied conversational agents. Through consultations with neuropsychologists, we created a total of 13 questions. The embodied conversational agent obtained answers to these questions from 24 participants (12 dementia and 12 non-dementia). We recorded their responses and extracted speech and language features. We classified the two groups (dementia/non-dementia) by a machine learning algorithm (support vector machines and logistic regression) using the extracted features. The results showed a 0.95 detection performance in the area under the curve of the receiver operating characteristic (AUROC). This result demonstrates that our system using atypical questions can detect dementia",
    "checked": true,
    "id": "cdf160103bf0a471a00de2724c56e0153964c8bc",
    "semantic_title": "detection of dementia from responses to atypical questions asked by embodied conversational agents",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18h_interspeech.html": {
    "title": "Acoustic Features Associated with Sustained Vowel and Continuous Speech Productions by Chinese Children with Functional Articulation Disorders",
    "volume": "main",
    "abstract": "Functional articulation disorder (FAD) is a speech disorder commonly found in preschoolers, negatively affecting their day-to-day communication and in the long run their psychological development. Current FAD research mainly focused on the perceptual aspects, but not other means such as acoustic and physiological analyses. The present study aimed to evaluate the different acoustic features associated with sustained vowels and continuous speech produced by children with FAD and their age-matched controls. Speech samples produced by 67 children with FAD and 30 typically developing children. Articulatory-acoustic vowel space features, including formant centralization ratio (FCR3), F1 range ratio (F1RR), F2 range ratio (F2RR) and triangular vowel space area (TVSA), were calculated using the first two formant frequencies from vowels /a/, /i/, /u/. Voice onset time (VOT) values associated with the stop consonants were also obtained. Results indicated that children with FAD exhibited articulatory undershooting with reduced range of articulatory movements, as well as poorer control over the release of oral occlusion when producing aspirated or unaspirated stops, when compared with normal counterparts. The findings support the notion that these acoustic features can be used to differentiate misarticulated speech from healthy speech and could be used to objectively classify and evaluate FAD speech",
    "checked": true,
    "id": "b0da707906595dfd18700ba4e4a8a3f7a79ff6e8",
    "semantic_title": "acoustic features associated with sustained vowel and continuous speech productions by chinese children with functional articulation disorders",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vikram18b_interspeech.html": {
    "title": "Estimation of Hypernasality Scores from Cleft Lip and Palate Speech",
    "volume": "main",
    "abstract": "Hypernasality refers to the perception of excessive nasal resonances in vowels and voiced consonants. Existing speech processing based approaches concentrate only on the classification of speech into normal or hypernasal, which do not give the degree of hypernasality in terms of continuous values like nasometer. Motivated by the functionality of nasometer, in this work, a method is proposed for the evaluation of hypernasality. Speech signals representing two extremely opposite cases of nasality are used to develop the acoustic models, where oral sentences (rich in vowels, stops and fricatives) of normal speakers and nasal sentences (rich in nasals and nasalized vowels) of moderate-severe hypernasal speakers represent the groups with minimum and maximum attainable degrees of nasality, respectively. The acoustic features derived from glottal activity regions are used to model the maximum and minimum nasality classes using Gaussian mixture model and deep neural network approaches. The posterior probabilities obtained for nasal sentence class are referred to as hypernasality scores. The scores show a significant correlation (p<0.01) with respect to perceptual ratings of hypernasality, provided by expert speech-language pathologists. Further, hypernasality scores are used for the detection of hypernasality and the results are compared with the nasometer based approach",
    "checked": true,
    "id": "34d08242dd6c823393acd764283a8cb9626546c8",
    "semantic_title": "estimation of hypernasality scores from cleft lip and palate speech",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2018/warnita18_interspeech.html": {
    "title": "Detecting Alzheimer's Disease Using Gated Convolutional Neural Network from Audio Data",
    "volume": "main",
    "abstract": "We propose an automatic detection method of Alzheimer's diseases using a gated convolutional neural network (GCNN) from speech data. This GCNN can be trained with a relatively small amount of data and can capture the temporal information in audio paralinguistic features. Since it does not utilize any linguistic features, it can be easily applied to any languages. We evaluated our method using Pitt Corpus. The proposed method achieved the accuracy of 73.6%, which is better than the conventional sequential minimal optimization (SMO) by 7.6 points",
    "checked": true,
    "id": "9a1fdc7dc4b41f055f28032b5eae96ac9bb6ceb8",
    "semantic_title": "detecting alzheimer's disease using gated convolutional neural network from audio data",
    "citation_count": 35
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bandini18_interspeech.html": {
    "title": "Automatic Detection of Orofacial Impairment in Stroke",
    "volume": "main",
    "abstract": "Stroke is a devastating condition that affects the ability of people to communicate through speech, leading to social isolation and poor quality of life. The quantitative evaluation of speech and orofacial movements is essential for assessing the impairment and identifying treatment targets. However, to our knowledge, a tool for the automatic orofacial assessment, which considers multiple aspects of orofacial impairment (e.g., range of motion in addition to asymmetry), has not been developed for this clinical population. In this work, we tested a video-based approach for the automatic orofacial assessment in stroke survivors, combining low-cost depth sensor and face alignment algorithms for extracting facial features. Twelve patients post-stroke and 11 control subjects were evaluated during speech and non-speech tasks. By using a small feature-set representing range of motion and asymmetry of face movements, it was possible to discriminate patients post-stroke from control subjects with high accuracy (87%). Further insights on the choice of the task and face alignment algorithm are provided, demonstrating that a non-parametric approach such as SDM can provide better results. Through this work we demonstrated the feasibility of an objective tool to support clinicians in the assessment of speech and orofacial impairment post-stroke",
    "checked": true,
    "id": "a84c54ce6d76d1e67d03093d3cb819fa01ee73ac",
    "semantic_title": "automatic detection of orofacial impairment in stroke",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2018/alhanai18_interspeech.html": {
    "title": "Detecting Depression with Audio/Text Sequence Modeling of Interviews",
    "volume": "main",
    "abstract": "Medical professionals diagnose depression by interpreting the responses of individuals to a variety of questions, probing lifestyle changes and ongoing thoughts. Like professionals, an effective automated agent must understand that responses to queries have varying prognostic value. In this study we demonstrate an automated depression-detection algorithm that models interviews between an individual and agent and learns from sequences of questions and answers without the need to perform explicit topic modeling of the content. We utilized data of 142 individuals undergoing depression screening and modeled the interactions with audio and text features in a Long-Short Term Memory (LSTM) neural network model to detect depression. Our results were comparable to methods that explicitly modeled the topics of the questions and answers which suggests that depression can be detected through sequential modeling of an interaction, with minimal information on the structure of the interview",
    "checked": true,
    "id": "5ecbf78909359343a8f9f95a2412c1886dc6f1eb",
    "semantic_title": "detecting depression with audio/text sequence modeling of interviews",
    "citation_count": 146
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18l_interspeech.html": {
    "title": "Discourse Marker Detection for Hesitation Events on Mandarin Conversation",
    "volume": "main",
    "abstract": "The occurrence of hesitation events in spontaneous conversations can be associated with the difficulties in memory recall. One indicator of hesitation in speech in Taiwanese Mandarin is the usage of discourse markers. This paper introduces an approach to the detection of discourse markers that denote hesitation events. We propose a sequential labeling model to detect discourse markers in conversations by taking information on both acoustic level and word level into account. Experimental results show the integration of word-level acoustic feature extraction network significantly enhances the detection performance. Our approach for further applications is also discussed",
    "checked": true,
    "id": "692f1812bc568e35b27816acd99b1ddd8f109233",
    "semantic_title": "discourse marker detection for hesitation events on mandarin conversation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/geng18_interspeech.html": {
    "title": "Acoustic and Perceptual Characteristics of Mandarin Speech in Homosexual and Heterosexual Male Speakers",
    "volume": "main",
    "abstract": "The present study investigated both acoustic and perceptual characteristics of Mandarin speech in homosexual and heterosexual male speakers. Acoustic analyses of monosyllabic words showed significant differences between the two groups in F0 features (including the mean, the max and the range), F1 and F2 of vowels, aspiration/frication duration of consonants and center of gravity as well as skewness for /s/. Especially, the patterns were found to be opposite between Mandarin and American English speakers, which might be due to social psychological differences between the two societies. The perceptual experiment showed that the perceived score of gayness differed significantly between the speeches of the two groups. Among those acoustic parameters showing significant differences, fricative duration may be the most salient cue for sexual orientation of Mandarin male speakers",
    "checked": true,
    "id": "282d602a01e1d6e83bed2c9b6757541697c4f79b",
    "semantic_title": "acoustic and perceptual characteristics of mandarin speech in homosexual and heterosexual male speakers",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ando18_interspeech.html": {
    "title": "Automatic Question Detection from Acoustic and Phonetic Features Using Feature-wise Pre-training",
    "volume": "main",
    "abstract": "This paper presents a novel question detection method from natural speech using acoustic and phonetic features. The conventional methods based on Recurrent Neural Networks (RNNs) use only acoustic features. However, lexical cues are essential to identify some questions such as declarative questions. To this end we propose a new RNN-based question detection model which utilizes both acoustic and lexical information. Phonetic features which are suitable to describe interrogative cues are used as lexical information. Furthermore, we also propose a new training framework named feature-wise pre-training (FP) to combine the acoustic and phonetic features effectively. FP attempts to acquire interrogative cues in individual features instead of the combination of the features, which makes the model training more stable. The estimation models of the interrogatives are then integrated and fine-tuning is applied to obtain the unified comprehensive model. Experiments show that the proposed method offers better performance than the conventional benchmarks",
    "checked": true,
    "id": "850beefa9367d6f62c17799c3e921d674a1e842a",
    "semantic_title": "automatic question detection from acoustic and phonetic features using feature-wise pre-training",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/haider18b_interspeech.html": {
    "title": "Improving Response Time of Active Speaker Detection Using Visual Prosody Information Prior to Articulation",
    "volume": "main",
    "abstract": "Natural multi-party interaction commonly involves turning one's gaze towards the speaker who has the floor. Implementing virtual agents or robots who are able to engage in natural conversations with humans therefore requires enabling machines to exhibit this form of communicative behaviour. This task is called active speaker detection. In this paper, we propose a method for active speaker detection using visual prosody (lip and head movements) information before and after speech articulation to decrease the machine response time; and also demonstrate the discriminating power of visual prosody before and after speech articulation for active speaker detection. The results show that the visual prosody information one second before articulation is helpful in detecting the active speaker. Lip movements provide better results than head movements and fusion of both improves accuracy. We have also used visual prosody information of the first second of the speech utterance and found that it provides more accurate results than one second before articulation. We conclude that the fusion of lip movements from both regions (the first one second of speech and the one second before articulation) improves the accuracy of active speaker detection",
    "checked": true,
    "id": "62f96031ee8567e6bbdead77aa141304ae47dee0",
    "semantic_title": "improving response time of active speaker detection using visual prosody information prior to articulation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/turker18_interspeech.html": {
    "title": "Audio-Visual Prediction of Head-Nod and Turn-Taking Events in Dyadic Interactions",
    "volume": "main",
    "abstract": "Head-nods and turn-taking both significantly contribute conversational dynamics in dyadic interactions. Timely prediction and use of these events is quite valuable for dialog management systems in human-robot interaction. In this study, we present an audio-visual prediction framework for the head-nod and turn-taking events that can also be utilized in real-time systems. Prediction systems based on Support Vector Machines (SVM) and Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN) are trained on human-human conversational data. Unimodal and multimodal classification performances of head-nod and turn-taking events are reported over the IEMOCAP dataset",
    "checked": true,
    "id": "d0e41594572b5c96172e419f25a1798f1ced9397",
    "semantic_title": "audio-visual prediction of head-nod and turn-taking events in dyadic interactions",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wu18c_interspeech.html": {
    "title": "Analyzing Effect of Physical Expression on English Proficiency for Multimodal Computer-Assisted Language Learning",
    "volume": "main",
    "abstract": "English proficiency is important for communication in English. Computer-Assisted Language Learning (CALL) systems are introduced to provide a convenient and low-cost language learning environment. Most of the conventional speech-based CALL systems concentrate on developing verbal fluency of the learners. However, actual English communication involves not only verbal expressions but also facial expressions and gestures, which could affect the perceived proficiency. The objective of our research is to develop a CALL system that can evaluate fluency of physical expressions as well as the verbal fluency of English. However, it is not clear how physical expressions affect the overall proficiency of English. Therefore, this study investigates the relationship between the proficiency of English and the fluency of the physical expression by analyzing the dialog data of the multimodal CALL system",
    "checked": true,
    "id": "8ff7efb36f6d8ef6680b4da79aa424c851af53b4",
    "semantic_title": "analyzing effect of physical expression on english proficiency for multimodal computer-assisted language learning",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dumpala18_interspeech.html": {
    "title": "Analysis of the Effect of Speech-Laugh on Speaker Recognition System",
    "volume": "main",
    "abstract": "A robust speaker recognition system should be able to recognize a speaker despite all the possible variations in speaker's speech. A common variation of the neutral speech is speech-laugh, which occurs when a person is speaking and laughing, simultaneously. In this paper, we show that speech-laugh significantly degrades the performance of an i-vector based speaker recognition system. Further, we show that laughter and neutral speech contain complementary speaker information, which can be combined to improve the performance of the speaker recognition system for speech-laugh scenarios. Using AMI meeting corpus database, we show that by including neutral speech and laughter in enrollment phase, the performance of the system in the speech-laugh scenarios can be relatively improved by 36% in EER",
    "checked": true,
    "id": "28a511f693db49ad06fa0c34cc6ad50a808ea67b",
    "semantic_title": "analysis of the effect of speech-laugh on speaker recognition system",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sloboda18_interspeech.html": {
    "title": "Vocal Biomarkers for Cognitive Performance Estimation in a Working Memory Task",
    "volume": "main",
    "abstract": "The ability to non-invasively estimate cognitive fatigue and workload as contributing factors to cognitive performance has value for planning and decision making surrounding human participation in cognitively demanding situations and environments. Growing evidence supports the use of speech as an effective modality for assessing cognitive fatigue and workload, while also being operationally appropriate in a wide variety of environments. To assess ability to discriminate changes in cognitive fatigue and load from speech, features that measure speech onset time, speaking rate, voice quality and vocal tract coordination from the delta-mel-cepstrum are evaluated on two independent data sets that employ the same auditory working memory task. Feature effect sizes due to fatigue were generally larger than those due to load. Speech onset time, speaking rate and vocal tract coordination features show strong potential for speech-based fatigue estimation",
    "checked": true,
    "id": "7586304d8ca55224bc43c646f0047770c03ac49b",
    "semantic_title": "vocal biomarkers for cognitive performance estimation in a working memory task",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2018/an18_interspeech.html": {
    "title": "Lexical and Acoustic Deep Learning Model for Personality Recognition",
    "volume": "main",
    "abstract": "Deep learning has been very successful on labeling tasks such as image classification and neural network modeling, but there has not yet been much work on using deep learning for automatic personality recognition. In this study, we propose two deep learning structures for the task of personality recognition using acoustic-prosodic, psycholinguistic and lexical features and present empirical results of several experimental configurations, including a cross-corpus condition to evaluate robustness. Our best models match or outperform state-of-the-art on the well-known myPersonality corpus and also set a new state-of-the-art performance on the more difficult CXD corpus",
    "checked": true,
    "id": "281d814c51935dda96655fe2698af82703246d84",
    "semantic_title": "lexical and acoustic deep learning model for personality recognition",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ramabhadran18_interspeech.html": {
    "title": "Open Problems in Speech Recognition",
    "volume": "main",
    "abstract": "In this talk, I will focus on the evolution of ideas in speech recognition over the last couple of decades, with emphasis on the key breakthroughs over the last ten years, its impact across spoken language processing in several languages, recent trends and open challenges that remain to be addressed. One such breakthrough is the use of several neural network model variants, which has had an enormous impact on the performance of state-of-the-art large vocabulary speech recognition systems. They have also had impact on keyword search which is the task of localizing an orthographic query in a speech corpus, and is typically performed through analysis of automatic speech recognition (ASR). Using the recently concluded IARPA funded Babel program as an example of a well-benchmarked task that focussed on the rapid development of speech recognition capability for keyword search in a previously unstudied language, I will present the successes and challenges that persist with limited amounts of transcription. Interpreting and understanding the hidden representations of various models remains a challenge today. I will also discuss current research taking advantage of such interpretations to improve robustness to noisy environments, speaker/domain adaptation algorithms, and dialects/accents. I will conclude with relevant metrics to measure speech recognition performance today that include and ignore the bigger picture of end to end user experience",
    "checked": true,
    "id": "3cc5c8bcbd11f4169eacbffba2313342ae919926",
    "semantic_title": "open problems in speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bourlard18_interspeech.html": {
    "title": "Evolution of Neural Network Architectures for Speech Recognition",
    "volume": "main",
    "abstract": "Over these last few years, the use of Artificial Neural Networks (ANNs), now often referred to as deep learning or Deep Neural Networks (DNNs), has significantly reshaped research and development in a variety of signal and information processing tasks. While further boosting the state-of-the-art in Automatic Speech Recognition (ASR), recent progresses in the field have also allowed for more flexible and faster developments in emerging markets and multilingual societies (e.g., under-resourced languages). In this talk, we will provide a historical account of ANN architectures used for ASR since the mid-1980's, and now used in most ASR and spoken language understanding applications. We will start by recalling/revisiting key links between ANNs and statistical inference, discriminant analysis, and linear/nonlinear algebra. Finally, we will briefly discuss more recent trends towards novel DNN-based ASR approaches, including complex hierarchical systems, sparse recovery modeling, and \"end-to-end systems.\" However, and in spite of the recent progress in the area, we still lack basic understanding of the problems in hands. Although more and more tools are now available, in association with basically \"unlimited\" processing and data resources, we still fail in building principled ASR models and theories. Alternatively, we are still relying on \"ignorance-based\" models, often exposing limitations of our understanding, rather than enriching the field of ASR. Discussion of these limitations will underpin all of our overview",
    "checked": true,
    "id": "fe552783a86ae3bae3a84c92de1983bfdf906e67",
    "semantic_title": "evolution of neural network architectures for speech recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18i_interspeech.html": {
    "title": "Layer Trajectory LSTM",
    "volume": "main",
    "abstract": "It is popular to stack LSTM layers to get better modeling power, especially when large amount of training data is available. However, an LSTM-RNN with too many vanilla LSTM layers is very hard to train and there still exists the gradient vanishing issue if the network goes too deep. This issue can be partially solved by adding skip connections between layers, such as residual LSTM. In this paper, we propose a layer trajectory LSTM (ltLSTM) which builds a layer-LSTM using all the layer outputs from a standard multi-layer time-LSTM. This layer-LSTM scans the outputs from time-LSTMs and uses the summarized layer trajectory information for final senone classification. The forward-propagation of time-LSTM and layer-LSTM can be handled in two separate threads in parallel so that the network computation time is the same as the standard time-LSTM. With a layer-LSTM running through layers, a gated path is provided from the output layer to the bottom layer, alleviating the gradient vanishing issue. Trained with 30 thousand hours of EN-US Microsoft internal data, the proposed ltLSTM performed significantly better than the standard multi-layer LSTM and residual LSTM, with up to 9.0% relative word error rate reduction across different tasks",
    "checked": true,
    "id": "dfedbbd87b39b7e95cf6c46aa925d2e5d3f2dd03",
    "semantic_title": "layer trajectory lstm",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18n_interspeech.html": {
    "title": "Semi-tied Units for Efficient Gating in LSTM and Highway Networks",
    "volume": "main",
    "abstract": "Gating is a key technique used for integrating information from multiple sources by long short-term memory (LSTM) models and has recently also been applied to other models such as the highway network. Although gating is powerful, it is rather expensive in terms of both computation and storage as each gating unit uses a separate full weight matrix. This issue can be severe since several gates can be used together in e.g. an LSTM cell. This paper proposes a semi-tied unit (STU) approach to solve this efficiency issue, which uses one shared weight matrix to replace those in all the units in the same layer. The approach is termed \"semi-tied\" since extra parameters are used to separately scale each of the shared output values. These extra scaling factors are associated with the network activation functions and result in the use of parameterised sigmoid, hyperbolic tangent and rectified linear unit functions. Speech recognition experiments using British English multi-genre broadcast data showed that using STUs can reduce the calculation and storage cost by a factor of three for highway networks and four for LSTMs, while giving similar word error rates to the original models",
    "checked": true,
    "id": "2bd589e56bc0b0a7faeb7bd0876ef62281ea2ead",
    "semantic_title": "semi-tied units for efficient gating in lstm and highway networks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lam18_interspeech.html": {
    "title": "Gaussian Process Neural Networks for Speech Recognition",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) play an important role in state-of-the-art speech recognition systems. One important issue associated with DNNs and artificial neural networks in general is the selection of suitable model structures, for example, the form of hidden node activation functions to use. Due to lack of automatic model selection techniques, the choice of activation functions has been largely empirically based. In addition, the use of deterministic, fixed-point parameter estimates is prone to over-fitting when given limited training data. In order to model both models' structural and parametric uncertainty, a novel form of DNN architecture using non-parametric activation functions based on Gaussian process (GP), Gaussian process neural networks (GPNN), is proposed in this paper. Initial experiments conducted on the ARPA Resource Management task suggest that the proposed GPNN acoustic models outperformed the baseline sigmoid activation based DNN by 3.40% to 24.25% relatively in terms of word error rate. Consistent performance improvements over the DNN baseline were also obtained by varying the number of hidden nodes and the number of spectral basis functions",
    "checked": true,
    "id": "adfa0de9b6bd25115d9171ebdfaf2364333f8c79",
    "semantic_title": "gaussian process neural networks for speech recognition",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tang18_interspeech.html": {
    "title": "Acoustic Modeling with Densely Connected Residual Network for Multichannel Speech Recognition",
    "volume": "main",
    "abstract": "Motivated by recent advances in computer vision research, this paper proposes a novel acoustic model called Densely Connected Residual Network (DenseRNet) for multichannel speech recognition. This combines the strength of both DenseNet and ResNet. It adopts the basic \"building blocks\" of ResNet with different convolutional layers, receptive field sizes and growth rates as basic components that are densely connected to form socalled denseR blocks. By concatenating the feature maps of all preceding layers as inputs, DenseRNet can not only strengthen gradient back-propagation for the vanishing-gradient problem, but also exploit multi-resolution feature maps. Preliminary experimental results on CHiME 3 have shown that DenseRNet achieves a word error rate (WER) of 7.58% on beamforming-enhanced speech with six channel real test data by cross entropy criteria training while WER is 10.23% for the official baseline. Besides, additional experimental results are also presented to demonstrate that DenseRNet exhibits the robustness to beamforming-enhanced speech as well as near and far-field speech",
    "checked": true,
    "id": "9621d3648bbbf3ee82db1191b344015eea8af9a3",
    "semantic_title": "acoustic modeling with densely connected residual network for multichannel speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18k_interspeech.html": {
    "title": "Gated Recurrent Unit Based Acoustic Modeling with Future Context",
    "volume": "main",
    "abstract": "The use of future contextual information is typically shown to be helpful for acoustic modeling. However, for the recurrent neural network (RNN), it's not so easy to model the future temporal context effectively, meanwhile keep lower model latency. In this paper, we attempt to design a RNN acoustic model that being capable of utilizing the future context effectively and directly, with the model latency and computation cost as low as possible. The proposed model is based on the minimal gated recurrent unit (mGRU) with an input projection layer inserted in it. Two context modules, temporal encoding and temporal convolution, are specifically designed for this architecture to model the future context. Experimental results on the Switchboard task and an internal Mandarin ASR task show that, the proposed model performs much better than long short-term memory (LSTM) and mGRU models, whereas enables online decoding with a maximum latency of 170 ms. This model even outperforms a very strong baseline, TDNN-LSTM, with smaller model latency and almost half less parameters",
    "checked": true,
    "id": "75eafc0c30af2f1a53698072fec49a5850628731",
    "semantic_title": "gated recurrent unit based acoustic modeling with future context",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cheng18b_interspeech.html": {
    "title": "Output-Gate Projected Gated Recurrent Unit for Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we describe the work on accelerating decoding speed while improving the decoding accuracy. Firstly, we propose an architecture which we call Projected Gated Recurrent Unit (PGRU) for automatic speech recognition (ASR) tasks and show that the PGRU could outperform the standard GRU consistently. Secondly, in order to improve the PGRU's generalization, especially for large-scale ASR task, the Output-gate PGRU (OPGRU) is proposed. Finally, time delay neural network (TDNN) and normalization skills are found to be beneficial to the proposed projected-based GRU. The finally proposed unidirectional TDNN-OPGRU acoustic model achieves 3.3% / 4.5% relative reduction in word error rate (WER) compared with bidirectional projected LSTM (BLSTMP) on Eval2000 / RT03 test sets. Meanwhile, TDNN-OPGRU acoustic model speeds up the decoding speed by around 2.6 times compared with BLSTMP",
    "checked": true,
    "id": "b199f4815db1110a4c27bec1303f930b8b8da618",
    "semantic_title": "output-gate projected gated recurrent unit for speech recognition",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sadjadi18_interspeech.html": {
    "title": "Performance Analysis of the 2017 NIST Language Recognition Evaluation",
    "volume": "main",
    "abstract": "The 2017 NIST Language Recognition Evaluation (LRE) was held in the autumn of 2017. Similar to past LREs, the basic task in LRE17 was language detection, with an emphasis on discriminating closely related languages (14 in total) selected from 5 language clusters. LRE17 featured several new aspects including: audio data extracted from online videos; a development set for system training and development use; log-likelihood system output submissions; a normalized cross-entropy performance measure as an alternative metric; and, the release of a baseline system developed using the NIST Speaker and Language Recognition Evaluation (SLRE) toolkit for participant use. A total of 18 teams from 25 academic and industrial organizations participated in the evaluation and submitted 79 valid systems under fixed and open training conditions first introduced in LRE15. In this paper, we report an in-depth analysis of system performance broken down by multiple factors such as data source and gender, as well as a cross-year performance comparison of leading systems from LRE15 and LRE17 to measure progress over the 2-year period. In addition, we present a comparison of primary versus \"single best\" submissions to understand the effect of fusion on overall performance",
    "checked": true,
    "id": "04b3aa2a7a4906a1be08a35b47d21e22beeb24a2",
    "semantic_title": "performance analysis of the 2017 nist language recognition evaluation",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mateju18_interspeech.html": {
    "title": "Using Deep Neural Networks for Identification of Slavic Languages from Acoustic Signal",
    "volume": "main",
    "abstract": "This paper investigates the use of deep neural networks (DNNs) for the task of spoken language identification. Various feed-forward fully connected, convolutional and recurrent DNN architectures are adopted and compared against a baseline i-vector based system. Moreover, DNNs are also utilized for extraction of bottleneck features from the input signal. The dataset used for experimental evaluation contains utterances belonging to languages that are all related to each other and sometimes hard to distinguish even for human listeners: it is compiled from recordings of the 11 most widespread Slavic languages. We also released this Slavic dataset to the general public, because a similar collection is not publicly available through any other source. The best results were yielded by a bidirectional recurrent DNN with gated recurrent units that was fed by bottleneck features. In this case, the baseline ER was reduced from 4.2% to 1.2% and Cavg from 2.3% to 0.6%",
    "checked": true,
    "id": "bcee563937901bae3194dace328f85631808e603",
    "semantic_title": "using deep neural networks for identification of slavic languages from acoustic signal",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2018/taitelbaum18_interspeech.html": {
    "title": "Adding New Classes without Access to the Original Training Data with Applications to Language Identification",
    "volume": "main",
    "abstract": "In this study we address the problem of adding new classes to an existing neural network classifier. We assume that new training data with the new classes is available. In many applications, dataset used to train machine learning algorithms contain confidential information that cannot be accessed during the process of extending the class set. We propose a method for training an extended class-set classifier using only examples with labels from the new classes while avoiding the problem of forgetting the original classes. This incremental training method is applied to the problem of language identification. We report results on the 50 languages NIST 2015 dataset where we were able to classify all the languages even though only part of the classes was available during the first training phase and the other languages were only available during the second phase",
    "checked": true,
    "id": "4341717cb5a90bd94426d791d3767f564c4aed25",
    "semantic_title": "adding new classes without access to the original training data with applications to language identification",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shen18b_interspeech.html": {
    "title": "Feature Representation of Short Utterances Based on Knowledge Distillation for Spoken Language Identification",
    "volume": "main",
    "abstract": "The performance of spoken language identification (LID) on short utterances is drastically degraded even though model is completely trained on short utterance data set. The degradation is because of the large pattern confusion caused by the large variation of feature representation on short utterances. In this paper, we propose a teacher-student network learning algorithm to explore discriminative features for short utterances. With the teacher-student network learning, the feature representation for short utterances (explored by the student network) are normalized to their representations corresponding to long utterances (provided by the teacher network). With this learning algorithm, the feature representation on short utterances is supposed to reduce pattern confusion. Experiments on a 10-language LID task were carried out to test the algorithm. Our results showed the proposed algorithm significantly improved the performance",
    "checked": true,
    "id": "273cfe6b6bc058af386658f58ec412cc005c5f50",
    "semantic_title": "feature representation of short utterances based on knowledge distillation for spoken language identification",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fernando18_interspeech.html": {
    "title": "Sub-band Envelope Features Using Frequency Domain Linear Prediction for Short Duration Language Identification",
    "volume": "main",
    "abstract": "Mismatch between training and testing utterances can significantly degrade the performance of language identification (LID) systems, especially in the case of short duration utterances. This work explores the hypothesis that long-term trends are less affected by this mismatch compared to short-term features. In particular, it proposes the use of features based on temporal envelopes within sub-bands. In this work, the temporal envelopes are obtained using linear prediction in the frequency domain. These envelopes are then transformed into cepstral features. The proposed features are then used as a front-end to a bidirectional long short term memory recurrent neural network to identify languages. Experimental evaluations on the AP17-OLR dataset under different conditions indicate that the proposed features exhibit substantially greater robustness under different noise and mismatch conditions, compared to baseline features. Specifically, the proposed features outperform state-of-the-art bottleneck features and show a relative improvement of 38.4% averaged across the test set",
    "checked": true,
    "id": "610d40ab05c603cce92523e2ff7cc1206472457d",
    "semantic_title": "sub-band envelope features using frequency domain linear prediction for short duration language identification",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2018/frederiksen18_interspeech.html": {
    "title": "Effectiveness of Single-Channel BLSTM Enhancement for Language Identification",
    "volume": "main",
    "abstract": "This paper proposes to apply deep neural network (DNN)-based single-channel speech enhancement (SE) to language identification. The 2017 language recognition evaluation (LRE17) introduced noisy audios from videos, in addition to the telephone conversation from past challenges. Because of that, adapting models from telephone speech to noisy speech from the video domain was required to obtain optimum performance. However, such adaptation requires knowledge of the audio domain and availability of in-domain data. Instead of adaptation, we propose to use a speech enhancement step to clean up the noisy audio as preprocessing for language identification. We used a bi-directional long short-term memory (BLSTM) neural network, which given log-Mel noisy features predicts a spectral mask indicating how clean each time-frequency bin is. The noisy spectrogram is multiplied by this predicted mask to obtain the enhanced magnitude spectrogram and it is transformed back into the time domain by using the unaltered noisy speech phase. The experiments show significant improvement to language identification of noisy speech, for systems with and without domain adaptation, while preserving the identification performance in the telephone audio domain. In the best adapted state-of-the-art bottleneck i-vector system the relative improvement is 11.3% for noisy speech",
    "checked": true,
    "id": "0115d5d37f7cdc7b8d2147c0bb348e714432e899",
    "semantic_title": "effectiveness of single-channel blstm enhancement for language identification",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gold18b_interspeech.html": {
    "title": "Articulation Rate as a Speaker Discriminant in British English",
    "volume": "main",
    "abstract": "Identifying speech parameters that have both a low level of intra-speaker variability and a high level of inter-speaker variability is key when discriminating between individuals in forensic speaker comparison cases. A substantial amount of research in the field of forensic phonetics has been devoted to identifying highly discriminant speaker parameters. To this end, the vast majority of the existing literature has focused solely on vowels and constants. However, the discriminant power of speaking tempo has yet to be examined, despite its broad use in practice and it having been recognized. This paper examines, for the first time, the discriminant power of articulation rate (AR) in British English. Approximately 3000 local ARs were measured in this study for 100 Southern Standard British English male speakers. In order to assess the evidential value of AR, likelihood ratios were calculated. The results suggest that AR performs well for same speaker comparisons. However, for different speaker comparisons, the system is performing just worse than chance. Overall, it appears that AR may not be the best speaker discriminant, although it is important to still consider AR in forensic speaker comparisons as there may be some individuals for which AR is highly idiosyncratic",
    "checked": true,
    "id": "1b39d26f5d1d9b0171f78e88ebaad5b951dc87cd",
    "semantic_title": "articulation rate as a speaker discriminant in british english",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yu18d_interspeech.html": {
    "title": "Truncation and Compression in Southern German and Australian English",
    "volume": "main",
    "abstract": "Nuclear pitch accents are realized differently when there is little sonorant material (as in monosyllabic compared to disyllabic words): Southern British English speakers compress rises and falls, while Northern German speakers truncate falls and compress rises [1] (Grabe 1998). This leads to different phonetic surface patterns for final falls. Within these languages, dialectal variation affects alignment and the frequency of occurrence of nuclear tunes. We test whether the differences in compression and truncation use are a stable cross-linguistic phenomenon (and occur in other varieties of English and German) or whether they are limited to the varieties tested in [1]. Here, we investigated productions of rises and falls in Australian English and Southern German in words with different proportions of sonorant material. Australian English speakers compressed rises and falls, while Southern German speakers only compressed rises but truncated falls, consistent with Grabe's findings for Southern British English and Northern German. This indicates consistent use of strategies within a language, even though the varieties under investigation display other phonetic differences from previous varieties tested. We discuss implications of these findings for automatic labelling",
    "checked": true,
    "id": "fd26d00276f299d9002d767c3badae4bc6e26325",
    "semantic_title": "truncation and compression in southern german and australian english",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kallio18_interspeech.html": {
    "title": "Prominence-based Evaluation of L2 Prosody",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ridouane18_interspeech.html": {
    "title": "Length Contrast and Covarying Features: Whistled Speech as a Case Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chodroff18_interspeech.html": {
    "title": "Information Structure, Affect and Prenuclear Prominence in American English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/novakiii18_interspeech.html": {
    "title": "Effects of User Controlled Speech Rate on Intelligibility in Noisy Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kondo18_interspeech.html": {
    "title": "Binaural Speech Intelligibility Estimation Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yamamoto18_interspeech.html": {
    "title": "Multi-resolution Gammachirp Envelope Distortion Index for Intelligibility Prediction of Noisy Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pv18_interspeech.html": {
    "title": "Speech Intelligibility Enhancement Based on a Non-causal Wavenet-like Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fu18c_interspeech.html": {
    "title": "Quality-Net: An End-to-End Non-intrusive Speech Quality Assessment Model Based on BLSTM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/aralikatti18_interspeech.html": {
    "title": "Global SNR Estimation of Speech Signals Using Entropy and Uncertainty Estimates from Dropout Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mittag18_interspeech.html": {
    "title": "Detecting Packet-Loss Concealment Using Formant Features and Decision Tree Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/eshky18_interspeech.html": {
    "title": "UltraSuite: A Repository of Ultrasound and Acoustic Data from Child Speech Therapy Sessions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mirheidari18_interspeech.html": {
    "title": "Detecting Signs of Dementia Using Word Vector Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/perez18_interspeech.html": {
    "title": "Classification of Huntington Disease Using Acoustic and Lexical Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/khorram18_interspeech.html": {
    "title": "The PRIORI Emotion Dataset: Linking Mood to Emotion Detected In-the-Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/flemotomos18_interspeech.html": {
    "title": "Language Features for Automated Evaluation of Cognitive Behavior Psychotherapy Sessions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/an18c_interspeech.html": {
    "title": "Automatic Early Detection of Amyotrophic Lateral Sclerosis from Intelligible Speech Using Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rao18_interspeech.html": {
    "title": "A Study of Lexical and Prosodic Cues to Segmentation in a Hindi-English Code-switched Discourse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ylmaz18c_interspeech.html": {
    "title": "Building a Unified Code-Switching ASR System for South African Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/guo18b_interspeech.html": {
    "title": "Study of Semi-supervised Approaches to Improving English-Mandarin Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ylmaz18_interspeech.html": {
    "title": "Acoustic and Textual Data Augmentation for Improved ASR of Code-Switching Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/soto18_interspeech.html": {
    "title": "The Role of Cognate Words, POS Tags and Entrainment in Code-Switching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/srivastava18_interspeech.html": {
    "title": "Homophone Identification and Merging for Code-switched Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/thomas18_interspeech.html": {
    "title": "Code-switching in Indic Speech Synthesisers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ganji18_interspeech.html": {
    "title": "A Novel Approach for Effective Recognition of the Code-Switched Data on Monolingual Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/viswanathan18_interspeech.html": {
    "title": "Hierarchical Accent Determination and Application in a Large Scale ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ramanarayanan18_interspeech.html": {
    "title": "Toward Scalable Dialog Technology for Conversational Language Learning: Case Study of the TOEFL® MOOC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/freitas18_interspeech.html": {
    "title": "Machine Learning Powered Data Platform for High-Quality Speech and NLP Workflows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cohen18_interspeech.html": {
    "title": "Fully Automatic Speaker Separation System, with Automatic Enrolling of Recurrent Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ayyavu18_interspeech.html": {
    "title": "Online Speech Translation System for Tamil",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shah18c_interspeech.html": {
    "title": "Unsupervised Vocal Tract Length Warped Posterior Features for Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhou18b_interspeech.html": {
    "title": "Voice Conversion with Conditional SampleRNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sisman18_interspeech.html": {
    "title": "A Voice Conversion Framework with Tandem Feature Sparse Representation and Speaker-Adapted WaveNet Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18_interspeech.html": {
    "title": "WaveNet Vocoder with Limited Training Data for Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wu18_interspeech.html": {
    "title": "Collapsed Speech Segment Detection and Suppression for WaveNet Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18h_interspeech.html": {
    "title": "High-quality Voice Conversion Using Spectrogram-Based WaveNet Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bonafonte18_interspeech.html": {
    "title": "Spanish Statistical Parametric Speech Synthesis Using a Neural Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/podsiado18_interspeech.html": {
    "title": "Experiments with Training Corpora for Statistical Text-to-speech Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gu18_interspeech.html": {
    "title": "Multi-task WaveNet: A Multi-task Generative Model for Statistical Parametric Speech Synthesis without Fundamental Frequency Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/juvela18_interspeech.html": {
    "title": "Speaker-independent Raw Waveform Model for Glottal Excitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cui18_interspeech.html": {
    "title": "A New Glottal Neural Vocoder for Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/watts18_interspeech.html": {
    "title": "Exemplar-based Speech Waveform Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kawahara18_interspeech.html": {
    "title": "Frequency Domain Variants of Velvet Noise and Their Application to Speech Processing and Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chung18_interspeech.html": {
    "title": "Joint Learning of Interactive Spoken Content Retrieval and Trainable User Simulator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shan18_interspeech.html": {
    "title": "Attention-based End-to-End Models for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/m18_interspeech.html": {
    "title": "Prediction of Aesthetic Elements in Karnatic Music: A Machine Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18e_interspeech.html": {
    "title": "Topic and Keyword Identification for Low-resourced Speech Using Cross-Language Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wiesner18_interspeech.html": {
    "title": "Automatic Speech Recognition and Topic Identification from Speech for Almost-Zero-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xiao18_interspeech.html": {
    "title": "Play Duration Based User-Entity Affinity Modeling in Spoken Dialog System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lee18e_interspeech.html": {
    "title": "Empirical Analysis of Score Fusion Application to Combined Neural Networks for Open Vocabulary Spoken Term Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/asaei18_interspeech.html": {
    "title": "Phonological Posterior Hashing for Query by Example Spoken Term Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kucza18_interspeech.html": {
    "title": "Term Extraction via Neural Sequence Labeling a Comparative Evaluation of Strategies Using Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kannan18_interspeech.html": {
    "title": "Semi-supervised Learning for Information Extraction from Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shin18_interspeech.html": {
    "title": "Slot Filling with Delexicalized Sentence Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ghosal18_interspeech.html": {
    "title": "Music Genre Recognition Using Deep Neural Networks and Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sigtia18_interspeech.html": {
    "title": "Efficient Voice Trigger Detection for Low Resource Hardware",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lin18_interspeech.html": {
    "title": "A Novel Normalization Method for Autocorrelation Function for Pitch Detection and for Speech Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ananthapadmanabha18_interspeech.html": {
    "title": "Estimation of the Vocal Tract Length of Vowel Sounds Based on the Frequency of the Significant Spectral Valley",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/himawan18_interspeech.html": {
    "title": "Deep Learning Techniques for Koala Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/matousek18_interspeech.html": {
    "title": "Glottal Closure Instant Detection from Speech Signal Using Voting Classifier and Recursive Feature Elimination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yousefi18_interspeech.html": {
    "title": "Assessing Speaker Engagement in 2-Person Debates: Overlap Detection in United States Presidential Debates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pankajakshan18_interspeech.html": {
    "title": "All-Conv Net for Bird Activity Detection: Significance of Learned Pooling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/thakur18_interspeech.html": {
    "title": "Deep Convex Representations: Feature Representations for Bioacoustics Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dasgupta18_interspeech.html": {
    "title": "Detection of Glottal Excitation Epochs in Speech Signal Using Hilbert Envelope",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18m_interspeech.html": {
    "title": "Analyzing Thai Tone Distribution through Functional Data Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/merkx18_interspeech.html": {
    "title": "Articulatory Feature Classification Using Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lin18c_interspeech.html": {
    "title": "A New Frequency Coverage Metric and a New Subband Encoding Model, with an Application in Pitch Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gowri18_interspeech.html": {
    "title": "Improved Epoch Extraction from Telephonic Speech Using Chebfun and Zero Frequency Filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kohn18_interspeech.html": {
    "title": "An Empirical Analysis of the Correlation of Syntax and Prosody",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/baumann18_interspeech.html": {
    "title": "Analysing the Focus of a Hierarchical Attention Network: the Importance of Enjambments When Classifying Post-modern Poetry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kocharov18_interspeech.html": {
    "title": "Language-Dependent Melody Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jia18_interspeech.html": {
    "title": "Stress Distribution of Given Information in Chinese Reading Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cabarrao18_interspeech.html": {
    "title": "Acoustic-prosodic Entrainment in Structural Metadata Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tabain18_interspeech.html": {
    "title": "Formant Measures of Vowels Adjacent to Alveolar and Retroflex Consonants in Arrernte: Stressed and Unstressed Position",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/truong18_interspeech.html": {
    "title": "Automatic Assessment of L2 English Word Prosody Using Weighted Distances of F0 and Intensity Contours",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/maxwell18_interspeech.html": {
    "title": "Homogeneity vs Heterogeneity in Indian English: Investigating Influences of L1 on f0 Range",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18_interspeech.html": {
    "title": "Emotional Prosody Perception in Mandarin-speaking Congenital Amusics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shochi18_interspeech.html": {
    "title": "Cultural Differences in Pattern Matching: Multisensory Recognition of Socio-affective Prosody",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mesgarani18_interspeech.html": {
    "title": "Speech Processing in the Human Brain Meets Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/watanabe18_interspeech.html": {
    "title": "ESPnet: End-to-End Speech Processing Toolkit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18f_interspeech.html": {
    "title": "A GPU-based WFST Decoder with Exact Lattice Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ragni18_interspeech.html": {
    "title": "Automatic Speech Recognition System Development in the \"Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/velikovich18_interspeech.html": {
    "title": "Semantic Lattice Processing in Contextual Automatic Speech Recognition for Google Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/williams18_interspeech.html": {
    "title": "Contextual Speech Recognition in End-to-end Neural Network Systems Using Beam Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mimura18_interspeech.html": {
    "title": "Forward-Backward Attention Decoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yadav18_interspeech.html": {
    "title": "Learning Discriminative Features for Speaker Identification and Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/novoselov18_interspeech.html": {
    "title": "Triplet Loss Based Cosine Similarity Metric Learning for Text-independent Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18b_interspeech.html": {
    "title": "Speaker Embedding Extraction with Phonetic Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/okabe18_interspeech.html": {
    "title": "Attentive Statistics Pooling for Deep Speaker Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/le18_interspeech.html": {
    "title": "Robust and Discriminative Speaker Embedding via Intra-Class Distance Variance Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18l_interspeech.html": {
    "title": "Deep Discriminative Embeddings for Duration Robust Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/simantiraki18_interspeech.html": {
    "title": "Impact of Different Speech Types on Listening Effort",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/huet18_interspeech.html": {
    "title": "Who Are You Listening to? Towards a Dynamic Measure of Auditory Attention to Speech-on-speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kim18d_interspeech.html": {
    "title": "Investigating the Role of Familiar Face and Voice Cues in Speech Processing in Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/scharenborg18_interspeech.html": {
    "title": "The Conversation Continues: the Effect of Lyrics and Music Complexity of Background Music on Spoken-Word Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/meyer18b_interspeech.html": {
    "title": "Loud and Shouted Speech Perception at Variable Distances in a Forest",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/docarmoblanco18_interspeech.html": {
    "title": "Phoneme Resistance and Phoneme Confusion in Noise: Impact of Dyslexia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/haque18_interspeech.html": {
    "title": "Conditional End-to-End Audio Transforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/aneeja18_interspeech.html": {
    "title": "Detection of Glottal Closure Instants in Degraded Speech Using Single Frequency Filtering Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lugosch18_interspeech.html": {
    "title": "Tone Recognition Using Lifters and CTC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vikram18_interspeech.html": {
    "title": "Epoch Extraction from Pathological Children Speech Using Single Pole Filtering Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bt18_interspeech.html": {
    "title": "Automated Classification of Vowel-Gesture Parameters Using External Broadband Excitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kadiri18_interspeech.html": {
    "title": "Estimation of Fundamental Frequency from Singing Voice Using Harmonics of Impulse-like Excitation Source",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/weiner18_interspeech.html": {
    "title": "Investigating the Effect of Audio Duration on Dementia Detection Using Acoustic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lin18b_interspeech.html": {
    "title": "An Interlocutor-Modulated Attentional LSTM for Differentiating between Subgroups of Autism Spectrum Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/amiriparian18_interspeech.html": {
    "title": "Recognition of Echolalic Autistic Child Vocalisations Utilising Convolutional Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nallanchakravarthula18_interspeech.html": {
    "title": "Modeling Interpersonal Influence of Verbal Behavior in Couples Therapy Dyadic Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ramakrishna18_interspeech.html": {
    "title": "Computational Modeling of Conversational Humor in Psychotherapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/garcia18_interspeech.html": {
    "title": "Multimodal I-vectors to Detect and Evaluate Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/baur18_interspeech.html": {
    "title": "Overview of the 2018 Spoken CALL Shared Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/julg18_interspeech.html": {
    "title": "The CSU-K Rule-Based System for the 2nd Edition Spoken CALL Shared Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nguyen18_interspeech.html": {
    "title": "Liulishuo's System for the Spoken CALL Shared Task 2018",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ateeq18_interspeech.html": {
    "title": "An Optimization Based Approach for Solving Spoken CALL Shared Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/qian18_interspeech.html": {
    "title": "The University of Birmingham 2018 Spoken CALL Shared Task Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/evanini18_interspeech.html": {
    "title": "Improvements to an Automated Content Scoring System for Spoken CALL Responses: the ETS Submission to the Second Spoken CALL Shared Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/goel18_interspeech.html": {
    "title": "Extracting Speaker's Gender, Accent, Age and Emotional State from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/narayanamurthy18_interspeech.html": {
    "title": "Determining Speaker Location from Speech in a Practical Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/patel18b_interspeech.html": {
    "title": "An Automatic Speech Transcription System for Manipuri Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yarra18_interspeech.html": {
    "title": "SPIRE-SST: An Automatic Web-based Self-learning Tool for Syllable Stress Tutoring (SST) to the Second Language Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chakraborty18_interspeech.html": {
    "title": "Glotto Vibrato Graph: A Device and Method for Recording, Analysis and Visualization of Glottal Activity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/renduchintala18_interspeech.html": {
    "title": "Multi-Modal Data Augmentation for End-to-end ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/moriya18_interspeech.html": {
    "title": "Multi-task Learning with Augmentation Strategy for Acoustic-to-word Attention-based Encoder-decoder Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sun18_interspeech.html": {
    "title": "Training Augmentation with Adversarial Examples for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fukuda18_interspeech.html": {
    "title": "Data Augmentation Improves Recognition of Foreign Accented Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tomashenko18_interspeech.html": {
    "title": "Speaker Adaptive Training and Mixup Regularization for Neural Network Acoustic Models in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/muller18_interspeech.html": {
    "title": "Neural Language Codes for Multilingual Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ueno18_interspeech.html": {
    "title": "Encoder Transfer for Attention-based Acoustic-to-word Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18j_interspeech.html": {
    "title": "Empirical Evaluation of Speaker Adaptation on DNN Based Acoustic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/das18c_interspeech.html": {
    "title": "Improving DNNs Trained with Non-Native Transcriptions Using Knowledge Distillation and Target Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/feng18b_interspeech.html": {
    "title": "Improving Cross-Lingual Knowledge Transferability Using Multilingual TDNN-BLSTM with Language-Dependent Pre-Final Layer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/delcroix18_interspeech.html": {
    "title": "Auxiliary Feature Based Adaptation of End-to-end ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ghorbani18_interspeech.html": {
    "title": "Leveraging Native Language Information for Improved Accented Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jain18_interspeech.html": {
    "title": "Improved Accented Speech Recognition Using Accent Embeddings and Multi-task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tong18_interspeech.html": {
    "title": "Fast Language Adaptation Using Phonological Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/murakami18_interspeech.html": {
    "title": "Naturalness Improvement Algorithm for Reconstructed Glossectomy Patient's Speech Using Spectral Differential Modification in Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tamura18_interspeech.html": {
    "title": "Audio-visual Voice Conversion Using Deep Canonical Correlation Analysis for Deep Bottleneck Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/baljekar18_interspeech.html": {
    "title": "An Investigation of Convolution Attention Based Models for Multilingual Speech Synthesis of Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/websdale18_interspeech.html": {
    "title": "The Effect of Real-Time Constraints on Automatic Speech Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/greenwood18_interspeech.html": {
    "title": "Joint Learning of Facial Expression and Head Pose from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vythelingum18_interspeech.html": {
    "title": "Acoustic-dependent Phonemic Transcription for Text-to-speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/luong18b_interspeech.html": {
    "title": "Multimodal Speech Synthesis Architecture for Unsupervised Speaker Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/taguchi18_interspeech.html": {
    "title": "Articulatory-to-speech Conversion Using Bi-directional Long Short-term Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tanihara18_interspeech.html": {
    "title": "Implementation of Respiration in Articulatory Synthesis Using a Pressure-Volume Lung Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhou18c_interspeech.html": {
    "title": "Learning and Modeling Unit Embeddings for Improving HMM-based Unit Selection Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fu18b_interspeech.html": {
    "title": "Deep Metric Learning for the Target Cost in Unit-Selection Speech Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sone18_interspeech.html": {
    "title": "DNN-based Speech Synthesis for Small Data Sets Considering Bidirectional Speech-Text Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gerazov18_interspeech.html": {
    "title": "A Weighted Superposition of Functional Contours Model for Modelling Contextual Prominence of Elementary Prosodic Contours",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nakashika18_interspeech.html": {
    "title": "LSTBM: A Novel Sequence Representation of Speech Spectra Using Restricted Boltzmann Machine with Long Short-Term Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bullock18_interspeech.html": {
    "title": "Should Code-switching Models Be Asymmetric?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tsukada18_interspeech.html": {
    "title": "Cross-language Perception of Mandarin Lexical Tones by Mongolian-speaking Bilinguals in the Inner Mongolia Autonomous Region, China",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fontan18_interspeech.html": {
    "title": "Automatically Measuring L2 Speech Fluency without the Need of ASR: A Proof-of-concept Study with Japanese Learners of French",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sun18c_interspeech.html": {
    "title": "Analysis of L2 Learners' Progress of Distinguishing Mandarin Tone 2 and Tone 3",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18o_interspeech.html": {
    "title": "Unsupervised Discovery of Non-native Phonetic Patterns in L2 English Speech for Mispronunciation Detection and Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18m_interspeech.html": {
    "title": "Wuxi Speakers' Production and Perception of Coda Nasals in Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dyrenko18_interspeech.html": {
    "title": "The Diphthongs of Formal Nigerian English: A Preliminary Acoustic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/davis18_interspeech.html": {
    "title": "Characterizing Rhythm Differences between Strong and Weak Accented L2 Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fringi18_interspeech.html": {
    "title": "Analysis of Phone Errors Attributable to Phonological Effects Associated With Language Acquisition Through Bottleneck Feature Visualisations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/koreman18_interspeech.html": {
    "title": "Category Similarity in Multilingual Pronunciation Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cristia18_interspeech.html": {
    "title": "Talker Diarization in the Wild: the Case of Child-centered Daylong Audio-recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18o_interspeech.html": {
    "title": "Automated Classification of Children's Linguistic versus Non-Linguistic Vocalisations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yuan18b_interspeech.html": {
    "title": "Pitch Characteristics of L2 English Speech by Chinese Speakers: A Large-scale Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/garg18_interspeech.html": {
    "title": "Dual Language Models for Code Switched Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/biswas18_interspeech.html": {
    "title": "Multilingual Neural Network Acoustic Modelling for ASR of Under-Resourced English-isiZulu Code-Switched Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/menon18_interspeech.html": {
    "title": "Fast ASR-free and Almost Zero-resource Keyword Spotting Using DTW and CNNs for Humanitarian Monitoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yu18c_interspeech.html": {
    "title": "Text-Dependent Speech Enhancement for Small-Footprint Robust Keyword Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/he18_interspeech.html": {
    "title": "Improved ASR for Under-resourced Languages through Multi-task Learning with Acoustic Landmarks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chibuye18_interspeech.html": {
    "title": "Cross-language Phoneme Mapping for Low-resource Languages: An Exploration of Benefits and Trade-offs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tundik18_interspeech.html": {
    "title": "User-centric Evaluation of Automatic Punctuation in ASR Closed Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zelasko18_interspeech.html": {
    "title": "Punctuation Prediction Model for Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/karafiat18_interspeech.html": {
    "title": "BUT OpenSAT 2017 Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18h_interspeech.html": {
    "title": "Visual Recognition of Continuous Cued Speech Using a Tandem CNN-HMM Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/thangthai18_interspeech.html": {
    "title": "Building Large-vocabulary Speaker-independent Lipreading Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gupta18b_interspeech.html": {
    "title": "CRIM's System for the MGB-3 English Multi-Genre Broadcast Media Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/riad18_interspeech.html": {
    "title": "Sampling Strategies in Siamese Networks for Unsupervised Speech Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18c_interspeech.html": {
    "title": "Compact Feedforward Sequential Memory Networks for Small-footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hermann18_interspeech.html": {
    "title": "Multilingual Bottleneck Features for Subword Modeling in Zero-resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/feng18_interspeech.html": {
    "title": "Exploiting Speaker and Phonetic Diversity of Mismatched Language Resources for Unsupervised Subword Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/godard18_interspeech.html": {
    "title": "Unsupervised Word Segmentation from Speech with Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/holzenberger18_interspeech.html": {
    "title": "Learning Word Embeddings: Unsupervised Methods for Fixed-size Representations of Variable-length Speech Segments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/glarner18_interspeech.html": {
    "title": "Full Bayesian Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/milde18_interspeech.html": {
    "title": "Unspeech: Unsupervised Speech Context Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gong18b_interspeech.html": {
    "title": "Impact of Aliasing on Deep CNN-Based End-to-End Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sivasankaran18_interspeech.html": {
    "title": "Keyword Based Speaker Localization: Localizing a Target Speaker in a Multi-speaker Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18f_interspeech.html": {
    "title": "End-to-End Speech Separation with Unfolded Iterative Phase Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/takahashi18_interspeech.html": {
    "title": "PhaseNet: Discretized Phase Modeling with Deep Neural Networks for Audio Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18k_interspeech.html": {
    "title": "Integrating Spectral and Spatial Features for Multi-Channel Speaker Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gogate18_interspeech.html": {
    "title": "DNN Driven Speaker Independent Audio-Visual Mask Estimation for Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vasilescu18_interspeech.html": {
    "title": "Exploring Temporal Reduction in Dialectal Spanish: A Large-scale Study of Lenition of Voiced Stops and Coda-s",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rose18_interspeech.html": {
    "title": "Dialect-geographical Acoustic-Tonetics: Five Disyllabic Tone Sandhi Patterns in Cognate Words from the Wu Dialects of ZhèJiāNg Province",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/leemann18_interspeech.html": {
    "title": "Regional Variation of /r/ in Swiss German Dialects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/earnshaw18_interspeech.html": {
    "title": "Variation in the FACE Vowel across West Yorkshire: Implications for Forensic Speaker Comparisons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gold18_interspeech.html": {
    "title": "The ‘West Yorkshire Regional English Database': Investigations into the Generalizability of Reference Populations for Forensic Speaker Comparison Casework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wottawa18_interspeech.html": {
    "title": "Studying Vowel Variation in French-Algerian Arabic Code-switched Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hansen18_interspeech.html": {
    "title": "Fearless Steps: Apollo-11 Corpus Advancements for Speech Technologies from Earth to the Moon",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kumar18_interspeech.html": {
    "title": "A Knowledge Driven Structural Segmentation Approach for Play-Talk Classification During Autism Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/james18_interspeech.html": {
    "title": "An Open Source Emotional Speech Corpus for Human Robot Interaction Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lapidot18_interspeech.html": {
    "title": "Speech Database and Protocol Validation Using Waveform Entropy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/terissi18_interspeech.html": {
    "title": "A French-Spanish Multimodal Speech Communication Corpus Incorporating Acoustic Data, Facial, Hands and Arms Gestures Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhao18b_interspeech.html": {
    "title": "L2-ARCTIC: A Non-native English Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zajic18_interspeech.html": {
    "title": "ZCU-NTIS Speaker Diarization System for the DIHARD 2018 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sun18b_interspeech.html": {
    "title": "Speaker Diarization with Enhancing Speech for the First DIHARD Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/diez18_interspeech.html": {
    "title": "BUT System for DIHARD Speech Diarization Challenge 2018",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vinals18_interspeech.html": {
    "title": "Estimation of the Number of Speakers with Variational Bayesian PLDA in the DIHARD Diarization Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sell18_interspeech.html": {
    "title": "Diarization is Hard: Some Experiences and Lessons Learned for the JHU Team in the Inaugural DIHARD Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/patino18_interspeech.html": {
    "title": "The EURECOM Submission to the First DIHARD Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/miasatofilho18_interspeech.html": {
    "title": "Joint Discriminative Embedding Learning, Speech Activity and Overlap Detection for the DIHARD Speaker Diarization Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ni18_interspeech.html": {
    "title": "Multilingual Grapheme-to-Phoneme Conversion with Global Character Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/roy18_interspeech.html": {
    "title": "A Hybrid Approach to Grapheme to Phoneme Conversion in Assamese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mohammadi18_interspeech.html": {
    "title": "Investigation of Using Disentangled and Interpretable Representations for One-shot Cross-lingual Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/govender18_interspeech.html": {
    "title": "Using Pupillometry to Measure the Cognitive Load of Synthetic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/govender18b_interspeech.html": {
    "title": "Measuring the Cognitive Load of Synthetic Speech Using a Dual Task Paradigm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/orife18_interspeech.html": {
    "title": "Attentive Sequence-to-Sequence Learning for Diacritic Restoration of YorùBá Language Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18_interspeech.html": {
    "title": "Gated Convolutional Neural Network for Sentence Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sharma18b_interspeech.html": {
    "title": "On Training and Evaluation of Grapheme-to-Phoneme Mappings with Limited Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/baird18_interspeech.html": {
    "title": "The Perception and Analysis of the Likeability and Human Likeness of Synthesized Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mass18_interspeech.html": {
    "title": "Word Emphasis Prediction for Expressive Text to Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lee18b_interspeech.html": {
    "title": "A Comparison of Speaker-based and Utterance-based Data Selection for Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/toman18_interspeech.html": {
    "title": "Data Requirements, Selection and Augmentation for DNN-based Speech Synthesis from Crowdsourced Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vesely18_interspeech.html": {
    "title": "Lightly Supervised vs. Semi-supervised Training of Acoustic Model on Luxembourgish for Low-resource Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wenjie18_interspeech.html": {
    "title": "Investigation on the Combination of Batch Normalization and Dropout in BLSTM-based Acoustic Modeling for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/suzuki18_interspeech.html": {
    "title": "Inference-Invariant Transformation of Batch Normalization for Domain Adaptation of Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/long18_interspeech.html": {
    "title": "Active Learning for LF-MMI Trained Neural Networks in ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/medennikov18_interspeech.html": {
    "title": "An Investigation of Mixup Training Strategies for Acoustic Models in ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/agrawal18_interspeech.html": {
    "title": "Comparison of Unsupervised Modulation Filter Learning Methods for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kim18f_interspeech.html": {
    "title": "Improved Training for Online End-to-end Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/haider18c_interspeech.html": {
    "title": "Combining Natural Gradient with Hessian Free Methods for Sequence Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kanda18_interspeech.html": {
    "title": "Lattice-free State-level Minimum Bayes Risk Training of Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tang18b_interspeech.html": {
    "title": "A Study of Enhancement, Augmentation and Autoencoder Methods for Domain Adaptation in Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kirkedal18_interspeech.html": {
    "title": "Multilingual Deep Neural Network Training Using Cyclical Learning Rate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yu18b_interspeech.html": {
    "title": "Development of the CUHK Dysarthric Speech Recognition System for the UA Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/laaridh18b_interspeech.html": {
    "title": "Automatic Evaluation of Speech Intelligibility Based on I-vectors in the Context of Head and Neck Cancers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kim18e_interspeech.html": {
    "title": "Dysarthric Speech Recognition Using Convolutional LSTM Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/laaridh18_interspeech.html": {
    "title": "Perceptual and Automatic Evaluations of the Intelligibility of Speech Degraded by Noise Induced Hearing Loss Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ylmaz18b_interspeech.html": {
    "title": "Articulatory Features for ASR of Pathological Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/correia18_interspeech.html": {
    "title": "Mining Multimodal Repositories for Speech Affecting Diseases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/qin18_interspeech.html": {
    "title": "Long Distance Voice Channel Diagnosis Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chiu18_interspeech.html": {
    "title": "Speech Recognition for Medical Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/farah18_interspeech.html": {
    "title": "Prosodic Focus Acquisition in French Early Cochlear Implanted Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fezza18_interspeech.html": {
    "title": "The Role of Temporal Variation in Narrative Organization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/murtola18_interspeech.html": {
    "title": "Interaction Mechanisms between Glottal Source and Vocal Tract in Pitch Glides",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/singh18b_interspeech.html": {
    "title": "Relating Articulatory Motions in Different Speaking Rates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cabral18_interspeech.html": {
    "title": "Estimation of the Asymmetry Parameter of the Glottal Flow Waveform Using the Electroglottographic Signal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mandal18_interspeech.html": {
    "title": "Classification of Disorders in Vocal Folds Using Electroglottographic Signal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/raomv18_interspeech.html": {
    "title": "Automatic Glottis Localization and Segmentation in Stroboscopic Videos Using Deep Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/iseijaakkola18_interspeech.html": {
    "title": "Respiratory and Respiratory Muscular Control in JL1's and JL2's Text Reading Utilizing 4-RSTs and a Soft Respiratory Mask with a Two-Way Bulb",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hao18_interspeech.html": {
    "title": "A Preliminary Study on Tonal Coarticulation in Continuous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/meng18d_interspeech.html": {
    "title": "Speech and Language Processing for Learning and Wellbeing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ganapathy18_interspeech.html": {
    "title": "Far-Field Speech Recognition Using Multivariate Autoregressive Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/kim18g_interspeech.html": {
    "title": "Efficient Implementation of the Room Simulator for Training Deep Neural Network Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18b_interspeech.html": {
    "title": "Stream Attention for Distributed Multi-Microphone Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yoshioka18_interspeech.html": {
    "title": "Recognizing Overlapped Speech in Meetings: A Multichannel Separation Approach Using Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/drude18_interspeech.html": {
    "title": "Integrating Neural Network Based Beamforming and Weighted Prediction Error Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bu18_interspeech.html": {
    "title": "A Probability Weighted Beamformer for Noise Robust ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yokoyama18_interspeech.html": {
    "title": "Effects of Dimensional Input on Paralinguistic Information Perceived from Synthesized Dialogue Speech with Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/oraby18_interspeech.html": {
    "title": "Neural MultiVoice Models for Expressing Novel Personalities in Dialog",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jauk18_interspeech.html": {
    "title": "Expressive Speech Synthesis Using Sentiment Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/akuzawa18_interspeech.html": {
    "title": "Expressive Speech Synthesis via Modeling Expressions with Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wu18d_interspeech.html": {
    "title": "Rapid Style Adaptation Using Residual Error Embedding for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18j_interspeech.html": {
    "title": "EMPHASIS: An Emotional Phoneme-based Acoustic Model for Speech Synthesis System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/han18_interspeech.html": {
    "title": "Bags in Bag: Generating Context-Aware Bags for Tracking Emotions from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18c_interspeech.html": {
    "title": "An Attention Pooling Based Representation Learning Method for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yang18c_interspeech.html": {
    "title": "Predicting Arousal and Valence from Waveforms and Spectrograms Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sarma18_interspeech.html": {
    "title": "Emotion Identification from Raw Speech Signals Using DNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18f_interspeech.html": {
    "title": "Encoding Individual Acoustic Features Using Dyad-Augmented Deep Variational Representations for Dialog-level Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/latif18_interspeech.html": {
    "title": "Variational Autoencoders for Learning Latent Representations of Speech Emotion: A Preliminary Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/biasuttolervat18_interspeech.html": {
    "title": "Phoneme-to-Articulatory Mapping Using Bidirectional Gated RNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/su18c_interspeech.html": {
    "title": "Tongue Segmentation with Geometrically Constrained Snake Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/illa18_interspeech.html": {
    "title": "Low Resource Acoustic-to-articulatory Inversion Using Bi-directional Long Short Term Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/s18_interspeech.html": {
    "title": "Automatic Visual Augmentation for Concatenation Based Synthesized Articulatory Videos from Real-time MRI Data for Spoken Language Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ca18_interspeech.html": {
    "title": "Air-Tissue Boundary Segmentation in Real-Time Magnetic Resonance Imaging Video Using Semantic Segmentation with Fully Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/seneviratne18_interspeech.html": {
    "title": "Noise Robust Acoustic to Articulatory Speech Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ahmadi18_interspeech.html": {
    "title": "Designing a Pneumatic Bionic Voice Prosthesis - A Statistical Approach for Source Excitation Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/schnell18_interspeech.html": {
    "title": "A Neural Model to Predict Parameters for a Generalized Command Response Model of Intonation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cao18b_interspeech.html": {
    "title": "Articulation-to-Speech Synthesis Using Articulatory Flesh Point Sensors' Orientation Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shah18b_interspeech.html": {
    "title": "Effectiveness of Generative Adversarial Network for Non-Audible Murmur-to-Whisper Speech Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/diener18_interspeech.html": {
    "title": "Investigating Objective Intelligibility in Real-Time EMG-to-Speech Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wand18_interspeech.html": {
    "title": "Domain-Adversarial Training for Session Independent EMG-based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/toth18_interspeech.html": {
    "title": "Multi-Task Learning of Speech Recognition and Speech Synthesis Parameters for Ultrasound-based Silent Speech Interfaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jprakash18_interspeech.html": {
    "title": "Transcription Correction for Indian Languages Using Acoustic Signatures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pulugundla18_interspeech.html": {
    "title": "BUT System for Low Resource Indian Language ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sailor18b_interspeech.html": {
    "title": "DA-IICT/IIITV System for Low Resource Speech Recognition Challenge 2018",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/vydana18_interspeech.html": {
    "title": "An Exploration towards Joint Acoustic Modeling for Indian Languages: IIIT-H Submission for Low Resource Speech Recognition Challenge for Indian Languages, INTERSPEECH 2018",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/fathima18_interspeech.html": {
    "title": "TDNN-based Multilingual Speech Recognition System for Low Resource Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shetty18_interspeech.html": {
    "title": "Articulatory and Stacked Bottleneck Features for Low Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/billa18_interspeech.html": {
    "title": "ISI ASR System for the Low Resource Speech Recognition Challenge for Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/finley18_interspeech.html": {
    "title": "An Automated Assistant for Medical Scribes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dey18c_interspeech.html": {
    "title": "AGROASSAM: A Web Based Assamese Speech Recognition Application for Retrieving Agricultural Commodity Price and Weather Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/aharon18_interspeech.html": {
    "title": "Voice-powered Solutions with Cloud AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sivaraman18_interspeech.html": {
    "title": "Speech Synthesis in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nie18_interspeech.html": {
    "title": "Deep Noise Tracking Network: A Hybrid Signal Processing/Deep Learning Approach to Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ouyang18_interspeech.html": {
    "title": "A Deep Neural Network Based Harmonic Noise Model for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tan18_interspeech.html": {
    "title": "A Convolutional Recurrent Neural Network for Real-Time Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18h_interspeech.html": {
    "title": "All-Neural Multi-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18g_interspeech.html": {
    "title": "Deep Learning for Acoustic Echo Cancellation in Noisy and Double-Talk Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/afouras18_interspeech.html": {
    "title": "The Conversation: Deep Audio-Visual Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/subramanian18_interspeech.html": {
    "title": "Student-Teacher Learning for BLSTM Mask-based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/karjol18_interspeech.html": {
    "title": "Speech Enhancement Using Deep Mixture of Experts Based on Hard Expectation Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/meng18c_interspeech.html": {
    "title": "Adversarial Feature-Mapping for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/baby18_interspeech.html": {
    "title": "Biophysically-inspired Features Improve the Generalizability of Neural Network-based Speech Enhancement Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chai18_interspeech.html": {
    "title": "Error Modeling via Asymmetric Laplace Distribution for Deep Neural Network Based Single-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xia18_interspeech.html": {
    "title": "A Priori SNR Estimation Based on a Recurrent Neural Network for Robust Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tseng18_interspeech.html": {
    "title": "Multiple Instance Deep Learning for Weakly Supervised Small-Footprint Audio Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18f_interspeech.html": {
    "title": "Unsupervised Temporal Feature Learning Based on Sparse Coding Embedded BoAW for Acoustic Event Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/teng18_interspeech.html": {
    "title": "Data Independent Sequence Augmentation Method for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/song18_interspeech.html": {
    "title": "A Compact and Discriminative Feature Based on Auditory Summary Statistics for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sharma18_interspeech.html": {
    "title": "ASe: Acoustic Scene Embedding Using Deep Archetypal Analysis and GMM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18g_interspeech.html": {
    "title": "Deep Convolutional Neural Network with Scalogram for Audio Scene Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/joshi18_interspeech.html": {
    "title": "Time Aggregation Operators for Multi-label Audio Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mcloughlin18_interspeech.html": {
    "title": "Early Detection of Continuous and Partial Audio Events Using CNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/mulimani18_interspeech.html": {
    "title": "Robust Acoustic Event Classification Using Bag-of-Visual-Words",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/waldekar18_interspeech.html": {
    "title": "Wavelet Transform Based Mel-scaled Features for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18d_interspeech.html": {
    "title": "Multi-modal Attention Mechanisms in LSTM and Its Application to Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/raju18_interspeech.html": {
    "title": "Contextual Language Model Adaptation for Conversational Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/chen18b_interspeech.html": {
    "title": "Active Memory Networks for Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/khassanov18_interspeech.html": {
    "title": "Unsupervised and Efficient Vocabulary Expansion for Recurrent Neural Network Language Models in ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18c_interspeech.html": {
    "title": "Improving Language Modeling with an Adversarial Critic for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/deng18_interspeech.html": {
    "title": "Training Recurrent Neural Network through Moment Matching for NLP Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tuske18_interspeech.html": {
    "title": "Investigation on LSTM Recurrent N-gram Language Models for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hu18_interspeech.html": {
    "title": "Online Incremental Learning for Speaker-Adaptive Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/andresferrer18_interspeech.html": {
    "title": "Efficient Language Model Adaptation with Noise Contrastive Estimation and Kullback-Leibler Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18e_interspeech.html": {
    "title": "Recurrent Neural Network Language Model Adaptation for Conversational Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/levit18_interspeech.html": {
    "title": "What to Expect from Expected Kneser-Ney Smoothing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/benes18_interspeech.html": {
    "title": "i-Vectors in Language Modeling: An Efficient Way of Domain Adaptation for Feed-Forward Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rathner18_interspeech.html": {
    "title": "How Did You like 2017? Detection of Language Markers of Depression and Narcissism in Personal Narratives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/huang18e_interspeech.html": {
    "title": "Depression Detection from Short Utterances via Diverse Smartphones in Natural Environmental Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ozkanca18_interspeech.html": {
    "title": "Multi-Lingual Depression-Level Assessment from Conversational Speech Using Acoustic and Text Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/np18_interspeech.html": {
    "title": "Dysarthric Speech Classification Using Glottal Features Computed from Non-words, Words and Sentences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gosztolya18b_interspeech.html": {
    "title": "Identifying Schizophrenia Based on Temporal Parameters in Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/singla18_interspeech.html": {
    "title": "Using Prosodic and Lexical Information for Learning Utterance-level Behaviors in Psychotherapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/qin18b_interspeech.html": {
    "title": "Automatic Speech Assessment for People with Aphasia Using TDNN-BLSTM with Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nasir18_interspeech.html": {
    "title": "Towards an Unsupervised Entrainment Distance in Conversational Speech Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/teixeira18_interspeech.html": {
    "title": "Patient Privacy in Paralinguistic Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/alharbi18_interspeech.html": {
    "title": "A Lightly Supervised Approach to Detect Stuttering in Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18d_interspeech.html": {
    "title": "Learning Conditional Acoustic Latent Representation with Gender and Age Attributes for Automatic Pain Level Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ganapathy18b_interspeech.html": {
    "title": "Speaker and Language Recognition -- From Laboratory Technologies to the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/wang18e_interspeech.html": {
    "title": "A Deep Reinforcement Learning Based Multimodal Coaching Model (DCM) for Slot Filling in Spoken Language Understanding(SLU)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bechet18_interspeech.html": {
    "title": "Is ATIS Too Shallow to Go Deeper for Benchmarking Spoken Language Understanding Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ray18_interspeech.html": {
    "title": "Robust Spoken Language Understanding via Paraphrasing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/lee18d_interspeech.html": {
    "title": "Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/shen18_interspeech.html": {
    "title": "User Information Augmented Semantic Frame Parsing Using Progressive Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gupta18c_interspeech.html": {
    "title": "An Efficient Approach to Encoding Context for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hetherly18_interspeech.html": {
    "title": "Deep Speech Denoising with Vector Space Projections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xu18_interspeech.html": {
    "title": "A Shifted Delta Coefficient Objective for Monaural Speech Separation Using Multi-task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/tan18b_interspeech.html": {
    "title": "A Two-Stage Approach to Noisy Cochannel Speech Separation with Gated Residual Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/pandey18b_interspeech.html": {
    "title": "Monoaural Audio Source Separation Using Variational Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gang18_interspeech.html": {
    "title": "Towards Automated Single Channel Source Separation Using Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/erdogan18_interspeech.html": {
    "title": "Investigations on Data Augmentation and Loss Functions for Deep Learning Based Speech-Background Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hantke18_interspeech.html": {
    "title": "Annotator Trustability-based Cooperative Learning Solutions for Intelligent Audio Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/su18b_interspeech.html": {
    "title": "Semi-supervised Cross-domain Visual Feature Learning for Audio-Visual Broadcast Speech Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/afouras18b_interspeech.html": {
    "title": "Deep Lip Reading: A Comparison of Models and an Online Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/srinivasamurthy18_interspeech.html": {
    "title": "Iterative Learning of Speech Recognition Models for Air Traffic Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sari18_interspeech.html": {
    "title": "Speaker Adaptive Audio-Visual Fusion for the Open-Vocabulary Section of AVICAR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hruz18_interspeech.html": {
    "title": "Multimodal Name Recognition in Live TV Subtitling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/backstrom18_interspeech.html": {
    "title": "Dithered Quantization for Frequency-Domain Speech and Audio Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/das18_interspeech.html": {
    "title": "Postfiltering with Complex Spectral Correlations for Speech and Audio Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/das18b_interspeech.html": {
    "title": "Postfiltering Using Log-Magnitude Spectrum for Speech and Audio Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/biswas18b_interspeech.html": {
    "title": "Temporal Noise Shaping with Companding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18p_interspeech.html": {
    "title": "Multi-frame Quantization of LSF Parameters Using a Deep Autoencoder and Pyramid Vector Quantizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18q_interspeech.html": {
    "title": "Multi-frame Coding of LSF Parameters Using Block-Constrained Trellis Coded Vector Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/park18_interspeech.html": {
    "title": "Training Utterance-level Embedding Networks for Speaker Identification and Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nandwana18_interspeech.html": {
    "title": "Analysis of Complementary Information Sources in the Speaker Embeddings Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhu18_interspeech.html": {
    "title": "Self-Attentive Speaker Embeddings for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/gao18_interspeech.html": {
    "title": "An Improved Deep Embedding Learning Method for Short Duration Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/jung18b_interspeech.html": {
    "title": "Avoiding Speaker Overfitting in End-to-End DNNs Using Raw Waveform for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bhattacharya18_interspeech.html": {
    "title": "Deeply Fused Speaker Embeddings for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/rahman18_interspeech.html": {
    "title": "Employing Phonetic Information in DNN Speaker Embeddings to Improve Speaker Recognition Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dey18b_interspeech.html": {
    "title": "End-to-end Text-dependent Speaker Verification Using Novel Distance Measures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dubey18_interspeech.html": {
    "title": "Robust Speaker Clustering using Mixtures of von Mises-Fisher Distributions for Naturalistic Audio Streams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/song18b_interspeech.html": {
    "title": "Triplet Network with Attention for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18j_interspeech.html": {
    "title": "I-vector Transformation Using Conditional Generative Adversarial Networks for Short Utterance Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cai18_interspeech.html": {
    "title": "Analysis of Length Normalization in End-to-End Speaker Verification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/huang18c_interspeech.html": {
    "title": "Angular Softmax for Short-Duration Text-independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ji18_interspeech.html": {
    "title": "An End-to-End Text-Independent Speaker Identification System on Short Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ding18_interspeech.html": {
    "title": "MTGAN: Speaker Verification through Multitasking Triplet Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/paradacabaleiro18_interspeech.html": {
    "title": "Categorical vs Dimensional Perception of Italian Emotional Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18m_interspeech.html": {
    "title": "A Three-Layer Emotion Perception Model for Valence and Arousal-Based Detection from Multilingual Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/desplanques18_interspeech.html": {
    "title": "Cross-lingual Speech Emotion Recognition through Factor Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cheng18c_interspeech.html": {
    "title": "Modeling Self-Reported and Observed Affect from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/huang18_interspeech.html": {
    "title": "Stochastic Shake-Shake Regularization for Affective Learning from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/avila18_interspeech.html": {
    "title": "Investigating Speech Enhancement and Perceptual Quality for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/atcheson18_interspeech.html": {
    "title": "Demonstrating and Modelling Systematic Time-varying Annotator Disagreement in Continuous Emotion Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/huang18b_interspeech.html": {
    "title": "Speech Emotion Recognition from Variable-Length Inputs with Triplet Loss Function",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/zhang18k_interspeech.html": {
    "title": "Imbalance Learning-based Framework for Fear Recognition in the MediaEval Emotional Impact of Movies Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ma18b_interspeech.html": {
    "title": "Emotion Recognition from Variable-Length Speech Segments Using Deep Learning on Spectrograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/yenigalla18_interspeech.html": {
    "title": "Speech Emotion Recognition Using Spectrogram & Phoneme Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sahu18_interspeech.html": {
    "title": "On Enhancing Speech Emotion Recognition Using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/parthasarathy18_interspeech.html": {
    "title": "Ladder Networks for Emotion Recognition: Using Unsupervised Auxiliary Tasks to Improve Predictions of Emotional Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/huang18d_interspeech.html": {
    "title": "Knowledge Distillation for Sequence Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/li18h_interspeech.html": {
    "title": "Improving CTC-based Acoustic Model with Very Deep Residual Time-delay Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/guo18_interspeech.html": {
    "title": "Filter Sampling and Combination CNN (FSC-CNN): A Compact CNN Model for Small-footprint ASR Acoustic Modeling Using Raw Waveforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/ravanelli18_interspeech.html": {
    "title": "Twin Regularization for Online Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sperber18_interspeech.html": {
    "title": "Self-Attentional Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/park18d_interspeech.html": {
    "title": "Hierarchical Recurrent Neural Networks for Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/bruguier18b_interspeech.html": {
    "title": "Dictionary Augmented Sequence-to-Sequence Neural Network for Grapheme to Phoneme Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/raj18_interspeech.html": {
    "title": "Leveraging Second-Order Log-Linear Model for Improved Deep Learning Based ASR Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/povey18_interspeech.html": {
    "title": "Semi-Orthogonal Low-Rank Matrix Factorization for Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/liu18g_interspeech.html": {
    "title": "Completely Unsupervised Phoneme Recognition by Adversarially Learning Mapping Relationships from Audio Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/qian18b_interspeech.html": {
    "title": "Phone Recognition Using a Non-Linear Manifold with Broad Phone Class Dependent DNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/hosseiniasl18_interspeech.html": {
    "title": "A Multi-Discriminator CycleGAN for Unsupervised Non-Parallel Speech Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/cao18_interspeech.html": {
    "title": "Interactions between Vowels and Nasal Codas in Mandarin Speakers' Perception of Nasal Finals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/meng18_interspeech.html": {
    "title": "Weighting Pitch Contour and Loudness Contour in Mandarin Tone Perception in Cochlear Implant Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/nenadic18_interspeech.html": {
    "title": "Implementing DIANA to Model Isolated Auditory Word Recognition in English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/sharma18d_interspeech.html": {
    "title": "Effects of Homophone Density on Spoken Word Recognition in Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/xie18_interspeech.html": {
    "title": "Visual Timing Information in Audiovisual Speech Perception: Evidence from Lexical Tone Contour",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/barnaud18_interspeech.html": {
    "title": "COSMO SylPhon: A Bayesian Perceptuo-motor Model to Assess Phonological Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/maggu18_interspeech.html": {
    "title": "Experience-dependent Influence of Music and Language on Lexical Pitch Learning Is Not Additive",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2018/dellwo18_interspeech.html": {
    "title": "Influences of Fundamental Oscillation on Speaker Identification in Vocalic Utterances by Humans and Computers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}