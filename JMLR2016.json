{
  "https://jmlr.org/papers/v17/kaufman16a.html": {
    "title": "On the Complexity of Best-Arm Identification in Multi-Armed Bandit Models",
    "abstract": "The stochastic multi-armed bandit model is a simple abstraction that has proven useful in many different contexts in statistics and machine learning. Whereas the achievable limit in terms of regret minimization is now well known, our aim is to contribute to a better understanding of the performance in terms of identifying the $m$ best arms. We introduce generic notions of complexity for the two dominant frameworks considered in the literature: fixed-budget and fixed-confidence settings. In the fixed-confidence setting, we provide the first known distribution-dependent lower bound on the complexity that involves information-theoretic quantities and holds when $m\\geq 1$ under general assumptions. In the specific case of two armed- bandits, we derive refined lower bounds in both the fixed- confidence and fixed-budget settings, along with matching algorithms for Gaussian and Bernoulli bandit models. These results show in particular that the complexity of the fixed- budget setting may be smaller than the complexity of the fixed- confidence setting, contradicting the familiar behavior observed when testing fully specified alternatives. In addition, we also provide improved sequential stopping rules that have guaranteed error probabilities and shorter average running times. The proofs rely on two technical results that are of independent interest : a deviation lemma for self-normalized sums (Lemma 7) and a novel change of measure inequality for bandit models (Lemma 1)",
    "volume": "main",
    "checked": true,
    "id": "4dc0e6599d543a5ad3c32b9127c9dac04e34021b",
    "citation_count": 902
  },
  "https://jmlr.org/papers/v17/maggioni16a.html": {
    "title": "Multiscale Dictionary Learning: Non-Asymptotic Bounds and Robustness",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2c9c9969384b03e341abe754e9a060f8ca36df10",
    "citation_count": 32
  },
  "https://jmlr.org/papers/v17/khaleghi16a.html": {
    "title": "Consistent Algorithms for Clustering Time Series",
    "abstract": "The problem of clustering is considered for the case where every point is a time series. The time series are either given in one batch (offline setting), or they are allowed to grow with time and new time series can be added along the way (online setting). We propose a natural notion of consistency for this problem, and show that there are simple, computationally efficient algorithms that are asymptotically consistent under extremely weak assumptions on the distributions that generate the data. The notion of consistency is as follows. A clustering algorithm is called consistent if it places two time series into the same cluster if and only if the distribution that generates them is the same. In the considered framework the time series are allowed to be highly dependent, and the dependence can have arbitrary form. If the number of clusters is known, the only assumption we make is that the (marginal) distribution of each time series is stationary ergodic. No parametric, memory or mixing assumptions are made. When the number of clusters is unknown, stronger assumptions are provably necessary, but it is still possible to devise nonparametric algorithms that are consistent under very general conditions. The theoretical findings of this work are illustrated with experiments on both synthetic and real data",
    "volume": "main",
    "checked": true,
    "id": "3fc4ed04c9356b5a384373be465745608c724915",
    "citation_count": 42
  },
  "https://jmlr.org/papers/v17/blaser16a.html": {
    "title": "Random Rotation Ensembles",
    "abstract": "In machine learning, ensemble methods combine the predictions of multiple base learners to construct more accurate aggregate predictions. Established supervised learning algorithms inject randomness into the construction of the individual base learners in an effort to promote diversity within the resulting ensembles. An undesirable side effect of this approach is that it generally also reduces the accuracy of the base learners. In this paper, we introduce a method that is simple to implement yet general and effective in improving ensemble diversity with only modest impact on the accuracy of the individual base learners. By randomly rotating the feature space prior to inducing the base learners, we achieve favorable aggregate predictions on standard data sets compared to state of the art ensemble methods, most notably for tree-based ensembles, which are particularly sensitive to rotation",
    "volume": "main",
    "checked": true,
    "id": "00107f0f32068ee03e9e254f712dd0358449e119",
    "citation_count": 116
  },
  "https://jmlr.org/papers/v17/benavoli16a.html": {
    "title": "Should We Really Use Post-Hoc Tests Based on Mean-Ranks?",
    "abstract": "The statistical comparison of multiple algorithms over multiple data sets is fundamental in machine learning. This is typically carried out by the Friedman test. When the Friedman test rejects the null hypothesis, multiple comparisons are carried out to establish which are the significant differences among algorithms. The multiple comparisons are usually performed using the mean-ranks test. The aim of this technical note is to discuss the inconsistencies of the mean-ranks post-hoc test with the goal of discouraging its use in machine learning as well as in medicine, psychology, etc.. We show that the outcome of the mean-ranks test depends on the pool of algorithms originally included in the experiment. In other words, the outcome of the comparison between algorithms $A$ and $B$ depends also on the performance of the other algorithms included in the original experiment. This can lead to paradoxical situations. For instance the difference between $A$ and $B$ could be declared significant if the pool comprises algorithms $C,D,E$ and not significant if the pool comprises algorithms $F,G,H$. To overcome these issues, we suggest instead to perform the multiple comparison using a test whose outcome only depends on the two algorithms being compared, such as the sign-test or the Wilcoxon signed-rank test",
    "volume": "main",
    "checked": true,
    "id": "1435c09304f24a1310e6ede11bf5dfb4dbe78c3d",
    "citation_count": 263
  },
  "https://jmlr.org/papers/v17/collier16a.html": {
    "title": "Minimax Rates in Permutation Estimation for Feature Matching",
    "abstract": "The problem of matching two sets of features appears in various tasks of computer vision and can be often formalized as a problem of permutation estimation. We address this problem from a statistical point of view and provide a theoretical analysis of the accuracy of several natural estimators. To this end, the minimax rate of separation is investigated and its expression is obtained as a function of the sample size, noise level and dimension of the features. We consider the cases of homoscedastic and heteroscedastic noise and establish, in each case, tight upper bounds on the separation distance of several estimators. These upper bounds are shown to be unimprovable both in the homoscedastic and heteroscedastic settings. Interestingly, these bounds demonstrate that a phase transition occurs when the dimension $d$ of the features is of the order of the logarithm of the number of features $n$. For $d=O(\\log n)$, the rate is dimension free and equals $\\sigma (\\log n)^{1/2}$, where $\\sigma$ is the noise level. In contrast, when $d$ is larger than $c\\log n$ for some constant $c>0$, the minimax rate increases with $d$ and is of the order of $\\sigma(d\\log n)^{1/4}$. We also discuss the computational aspects of the estimators and provide empirical evidence of their consistency on synthetic data. Finally, we show that our results extend to more general matching criteria",
    "volume": "main",
    "checked": true,
    "id": "7fb3979e37699cf792113f3db4d4f5360d06e627",
    "citation_count": 36
  },
  "https://jmlr.org/papers/v17/teh16a.html": {
    "title": "Consistency and Fluctuations For Stochastic Gradient Langevin Dynamics",
    "abstract": "Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data sets is computationally expensive. Both the calculation of the acceptance probability and the creation of informed proposals usually require an iteration through the whole data set. The recently proposed stochastic gradient Langevin dynamics (SGLD) method circumvents this problem by generating proposals which are only based on a subset of the data, by skipping the accept-reject step and by using decreasing step-sizes sequence $(\\delta_m)_{m \\geq 0}$. We provide in this article a rigorous mathematical framework for analysing this algorithm. We prove that, under verifiable assumptions, the algorithm is consistent, satisfies a central limit theorem (CLT) and its asymptotic bias-variance decomposition can be characterized by an explicit functional of the step-sizes sequence $(\\delta_m)_{m \\geq 0}$. We leverage this analysis to give practical recommendations for the notoriously difficult tuning of this algorithm: it is asymptotically optimal to use a step-size sequence of the type $\\delta_m \\asymp m^{-1/3}$, leading to an algorithm whose mean squared error (MSE) decreases at rate $\\mathcal{O}(m^{-1/3})$",
    "volume": "main",
    "checked": true,
    "id": "aaee41270a6397024a68188dbf61614d87f891fb",
    "citation_count": 214
  },
  "https://jmlr.org/papers/v17/gulchere16a.html": {
    "title": "Knowledge Matters: Importance of Prior Information for Optimization",
    "abstract": "We explored the effect of introducing prior knowledge into the intermediate level of deep supervised neural networks on two tasks. On a task we designed, all black-box state-of-the-art machine learning algorithms which we tested, failed to generalize well. We motivate our work from the hypothesis that, there is a training barrier involved in the nature of such tasks, and that humans learn useful intermediate concepts from other individuals by using a form of supervision or guidance using a curriculum. Our results provide a positive evidence in favor of this hypothesis. In our experiments, we trained a two- tiered MLP architecture on a dataset for which each input image contains three sprites, and the binary target class is $1$ if all of three shapes belong to the same category and otherwise the class is $0$. In terms of generalization, black-box machine learning algorithms could not perform better than chance on this task. Standard deep supervised neural networks also failed to generalize. However, using a particular structure and guiding the learner by providing intermediate targets in the form of intermediate concepts (the presence of each object) allowed us to solve the task efficiently. We obtained much better than chance, but imperfect results by exploring different architectures and optimization variants. This observation might be an indication of optimization difficulty when the neural network trained without hints on this task. We hypothesize that the learning difficulty is due to the composition of two highly non-linear tasks. Our findings are also consistent with the hypotheses on cultural learning inspired by the observations of training of neural networks sometimes getting stuck, even though good solutions exist, both in terms of training and generalization error",
    "volume": "main",
    "checked": true,
    "id": "523b12db4004b89284387f978c2af8ae0e79d54b",
    "citation_count": 155
  },
  "https://jmlr.org/papers/v17/rieck16a.html": {
    "title": "Harry: A Tool for Measuring String Similarity",
    "abstract": "Comparing strings and assessing their similarity is a basic operation in many application domains of machine learning, such as in information retrieval, natural language processing and bioinformatics. The practitioner can choose from a large variety of available similarity measures for this task, each emphasizing different aspects of the string data. In this article, we present Harry, a small tool specifically designed for measuring the similarity of strings. Harry implements over 20 similarity measures, including common string distances and string kernels, such as the Levenshtein distance and the Subsequence kernel. The tool has been designed with efficiency in mind and allows for multi-threaded as well as distributed computing, enabling the analysis of large data sets of strings. Harry supports common data formats and thus can interface with analysis environments, such as Matlab, Pylab and Weka",
    "volume": "main",
    "checked": true,
    "id": "e6e9701935d6d3626834769300bd0c188dc22ffe",
    "citation_count": 17
  },
  "https://jmlr.org/papers/v17/chen16a.html": {
    "title": "Herded Gibbs Sampling",
    "abstract": "The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an $O(1/T)$ convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem",
    "volume": "main",
    "checked": false,
    "id": "24f52c902a7ce4fefa49a6133bf95a986df78b49",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v17/yuille16a.html": {
    "title": "Complexity of Representation and Inference in Compositional Models with Part Sharing",
    "abstract": "This paper performs a complexity analysis of a class of serial and parallel compositional models of multiple objects and shows that they enable efficient representation and rapid inference. Compositional models are generative and represent objects in a hierarchically distributed manner in terms of parts and subparts, which are constructed recursively by part-subpart compositions. Parts are represented more coarsely at higher level of the hierarchy, so that the upper levels give coarse summary descriptions (e.g., there is a horse in the image) while the lower levels represents the details (e.g., the positions of the legs of the horse). This hierarchically distributed representation obeys the executive summary principle, meaning that a high level executive only requires a coarse summary description and can, if necessary, get more details by consulting lower level executives. The parts and subparts are organized in terms of hierarchical dictionaries which enables part sharing between different objects allowing efficient representation of many objects. The first main contribution of this paper is to show that compositional models can be mapped onto a parallel visual architecture similar to that used by bio- inspired visual models such as deep convolutional networks but more explicit in terms of representation, hence enabling part detection as well as object detection, and suitable for complexity analysis. Inference algorithms can be run on this architecture to exploit the gains caused by part sharing and executive summary. Effectively, this compositional architecture enables us to perform exact inference simultaneously over a large class of generative models of objects. The second contribution is an analysis of the complexity of compositional models in terms of computation time (for serial computers) and numbers of nodes (e.g., \"neurons\") for parallel computers. In particular, we compute the complexity gains by part sharing and executive summary and their dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level of the hierarchy, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can enable linear processing on parallel computers",
    "volume": "main",
    "checked": true,
    "id": "1be859a41cfbbb39a7212b765431fe20b181917b",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v17/13-354.html": {
    "title": "Noisy Sparse Subspace Clustering",
    "abstract": "This paper considers the problem of subspace clustering under noise. Specifically, we study the behavior of Sparse Subspace Clustering (SSC) when either adversarial or random noise is added to the unlabeled input data points, which are assumed to be in a union of low-dimensional subspaces. We show that a modified version of SSC is provably effective in correctly identifying the underlying subspaces, even with noisy data. This extends theoretical guarantee of this algorithm to more practical settings and provides justification to the success of SSC in a class of real applications",
    "volume": "main",
    "checked": true,
    "id": "91e5b3c7750f554bf1626c303beb9ce9f29a3483",
    "citation_count": 192
  },
  "https://jmlr.org/papers/v17/14-335.html": {
    "title": "Learning the Variance of the Reward-To-Go",
    "abstract": "In Markov decision processes (MDPs), the variance of the reward- to-go is a natural measure of uncertainty about the long term performance of a policy, and is important in domains such as finance, resource allocation, and process control. Currently however, there is no tractable procedure for calculating it in large scale MDPs. This is in contrast to the case of the expected reward-to-go, also known as the value function, for which effective simulation-based algorithms are known, and have been used successfully in various domains. In this paper we extend temporal difference (TD) learning algorithms to estimating the variance of the reward-to- go for a fixed policy. We propose variants of both TD(0) and LSTD($\\lambda$) with linear function approximation, prove their convergence, and demonstrate their utility in an option pricing problem. Our results show a dramatic improvement in terms of sample efficiency over standard Monte-Carlo methods, which are currently the state-of-the-art",
    "volume": "main",
    "checked": true,
    "id": "14b92218c91b7d8eb5155a8d4624403405dba96f",
    "citation_count": 63
  },
  "https://jmlr.org/papers/v17/14-316.html": {
    "title": "Convex Calibration Dimension for Multiclass Loss Matrices",
    "abstract": "We study consistency properties of surrogate loss functions for general multiclass learning problems, defined by a general multiclass loss matrix. We extend the notion of classification calibration, which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems), to the general multiclass setting, and derive necessary and sufficient conditions for a surrogate loss to be calibrated with respect to a loss matrix in this setting. We then introduce the notion of convex calibration dimension of a multiclass loss matrix, which measures the smallest \"size\" of a prediction space in which it is possible to design a convex surrogate that is calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, we apply our framework to study various subset ranking losses, and use the convex calibration dimension as a tool to show both the existence and non-existence of various types of convex calibrated surrogates for these losses. Our results strengthen recent results of Duchi et al. (2010) and CalauzÃÂ¨nes et al. (2012) on the non-existence of certain types of convex calibrated surrogates in subset ranking. We anticipate the convex calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems",
    "volume": "main",
    "checked": true,
    "id": "26a7aa26974cbbd52f06cd7742d3e762efa7a9af",
    "citation_count": 36
  },
  "https://jmlr.org/papers/v17/14-301.html": {
    "title": "LLORMA: Local Low-Rank Matrix Approximation",
    "abstract": "Matrix approximation is a common tool in recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is low-rank. In this paper, we propose, analyze, and experiment with two procedures, one parallel and the other global, for constructing local matrix approximations. The two approaches approximate the observed matrix as a weighted sum of low-rank matrices. These matrices are limited to a local region of the observed matrix. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy over classical approaches for recommendation tasks",
    "volume": "main",
    "checked": true,
    "id": "e8cedf9f96aa266818642ffbc9e6fbeda626f3ff",
    "citation_count": 91
  },
  "https://jmlr.org/papers/v17/14-231.html": {
    "title": "A Consistent Information Criterion for Support Vector Machines in Diverging Model Spaces",
    "abstract": "Information criteria have been popularly used in model selection and proved to possess nice theoretical properties. For classification, Claeskens et al. (2880) proposed support vector machine information criterion for feature selection and provided encouraging numerical evidence. Yet no theoretical justification was given there. This work aims to fill the gap and to provide some theoretical justifications for support vector machine information criterion in both fixed and diverging model spaces. We first derive a uniform convergence rate for the support vector machine solution and then show that a modification of the support vector machine information criterion achieves model selection consistency even when the number of features diverges at an exponential rate of the sample size. This consistency result can be further applied to selecting the optimal tuning parameter for various penalized support vector machine methods. Finite-sample performance of the proposed information criterion is investigated using Monte Carlo studies and one real-world gene selection problem",
    "volume": "main",
    "checked": true,
    "id": "407568ddf123f350bd13afaa9d86dccbe03845d3",
    "citation_count": 19
  },
  "https://jmlr.org/papers/v17/15-135.html": {
    "title": "Extremal Mechanisms for Local Differential Privacy",
    "abstract": "Local differential privacy has recently surfaced as a strong measure of privacy in contexts where personal information remains private even from data analysts. Working in a setting where both the data providers and data analysts want to maximize the utility of statistical analyses performed on the released data, we study the fundamental trade-off between local differential privacy and utility. This trade-off is formulated as a constrained optimization problem: maximize utility subject to local differential privacy constraints. We introduce a combinatorial family of extremal privatization mechanisms, which we call staircase mechanisms, and show that it contains the optimal privatization mechanisms for a broad class of information theoretic utilities such as mutual information and $f$-divergences. We further prove that for any utility function and any privacy level, solving the privacy-utility maximization problem is equivalent to solving a finite-dimensional linear program, the outcome of which is the optimal staircase mechanism. However, solving this linear program can be computationally expensive since it has a number of variables that is exponential in the size of the alphabet the data lives in. To account for this, we show that two simple privatization mechanisms, the binary and randomized response mechanisms, are universally optimal in the low and high privacy regimes, and well approximate the intermediate regime",
    "volume": "main",
    "checked": true,
    "id": "b6b1d641a832d54144653493e3f364fbec040c25",
    "citation_count": 436
  },
  "https://jmlr.org/papers/v17/14-273.html": {
    "title": "Loss Minimization and Parameter Estimation with Heavy Tails",
    "abstract": "This work studies applications and generalizations of a simple estimation technique that provides exponential concentration under heavy-tailed distributions, assuming only bounded low- order moments. We show that the technique can be used for approximate minimization of smooth and strongly convex losses, and specifically for least squares linear regression. For instance, our $d$-dimensional estimator requires just $\\tilde{O}(d\\log(1/\\delta))$ random samples to obtain a constant factor approximation to the optimal least squares loss with probability $1-\\delta$, without requiring the covariates or noise to be bounded or subgaussian. We provide further applications to sparse linear regression and low-rank covariance matrix estimation with similar allowances on the noise and covariate distributions. The core technique is a generalization of the median-of-means estimator to arbitrary metric spaces",
    "volume": "main",
    "checked": true,
    "id": "ef438d93a852ba818da1f51f243abe8e8e4e291f",
    "citation_count": 161
  },
  "https://jmlr.org/papers/v17/10-364.html": {
    "title": "Analysis of Classification-based Policy Iteration Algorithms",
    "abstract": "We introduce a variant of the classification-based approach to policy iteration which uses a cost-sensitive loss function weighting each classification mistake by its actual regret, that is, the difference between the action- value of the greedy action and of the action chosen by the classifier. For this algorithm, we provide a full finite-sample analysis. Our results state a performance bound in terms of the number of policy improvement steps, the number of rollouts used in each iteration, the capacity of the considered policy space (classifier), and a capacity measure which indicates how well the policy space can approximate policies that are greedy with respect to any of its members. The analysis reveals a tradeoff between the estimation and approximation errors in this classification-based policy iteration setting. Furthermore it confirms the intuition that classification-based policy iteration algorithms could be favorably compared to value-based approaches when the policies can be approximated more easily than their corresponding value functions. We also study the consistency of the algorithm when there exists a sequence of policy spaces with increasing capacity",
    "volume": "main",
    "checked": false,
    "id": "bd308b2cee1d0ff63c9f492611e3a17528c431ef",
    "citation_count": 32
  },
  "https://jmlr.org/papers/v17/11-315.html": {
    "title": "Operator-valued Kernels for Learning from Functional Response Data",
    "abstract": "In this paper (This is a combined and expanded version of previous conference papers Kadri et al., 2010, 2011c) we consider the problems of supervised classification and regression in the case where attributes and labels are functions: a data is represented by a set of functions, and the label is also a function. We focus on the use of reproducing kernel Hilbert space theory to learn from such functional data. Basic concepts and properties of kernel-based learning are extended to include the estimation of function-valued functions. In this setting, the representer theorem is restated, a set of rigorously defined infinite-dimensional operator-valued kernels that can be valuably applied when the data are functions is described, and a learning algorithm for nonlinear functional data analysis is introduced. The methodology is illustrated through speech and audio signal processing experiments",
    "volume": "main",
    "checked": true,
    "id": "59cebd0fd733078c3b664f891ccc1a72f6046301",
    "citation_count": 100
  },
  "https://jmlr.org/papers/v17/12-164.html": {
    "title": "MEKA: A Multi-label/Multi-target Extension to WEKA",
    "abstract": "Multi-label classification has rapidly attracted interest in the machine learning literature, and there are now a large number and considerable variety of methods for this type of learning. We present MEKA: an open-source Java framework based on the well-known WEKA library. MEKA provides interfaces to facilitate practical application, and a wealth of multi-label classifiers, evaluation metrics, and tools for multi-label experiments and development. It supports multi-label and multi-target data, including in incremental and semi- supervised contexts",
    "volume": "main",
    "checked": true,
    "id": "40063d384be8d0a3a8ade0794e64426fef9d3566",
    "citation_count": 202
  },
  "https://jmlr.org/papers/v17/13-351.html": {
    "title": "Gradients Weights improve Regression and Classification",
    "abstract": "In regression problems over $\\mathbb{R}^d$, the unknown function $f$ often varies more in some coordinates than in others. We show that weighting each coordinate $i$ according to an estimate of the variation of $f$ along coordinate $i$ -- e.g. the $L_1$ norm of the $i$th-directional derivative of $f$ -- is an efficient way to significantly improve the performance of distance-based regressors such as kernel and $k$-NN regressors. The approach, termed Gradient Weighting (GW), consists of a first pass regression estimate $f_n$ which serves to evaluate the directional derivatives of $f$, and a second-pass regression estimate on the re-weighted data. The GW approach can be instantiated for both regression and classification, and is grounded in strong theoretical principles having to do with the way regression bias and variance are affected by a generic feature-weighting scheme. These theoretical principles provide further technical foundation for some existing feature-weighting heuristics that have proved successful in practice. We propose a simple estimator of these derivative norms and prove its consistency. The proposed estimator computes efficiently and easily extends to run online. We then derive a classification version of the GW approach which evaluates on real-worlds datasets with as much success as its regression counterpart",
    "volume": "main",
    "checked": true,
    "id": "1ab85d420e9b8089a38edb37181ff1fee347a697",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v17/13-533.html": {
    "title": "A Closer Look at Adaptive Regret",
    "abstract": "For the prediction with expert advice setting, we consider methods to construct algorithms that have low adaptive regret. The adaptive regret of an algorithm on a time interval $[t_1,t_2]$ is the loss of the algorithm minus the loss of the best expert over that interval. Adaptive regret measures how well the algorithm approximates the best expert locally, and so is different from, although closely related to, both the classical regret, measured over an initial time interval $[1,t]$, and the tracking regret, where the algorithm is compared to a good sequence of experts over $[1,t]$. We investigate two existing intuitive methods for deriving algorithms with low adaptive regret, one based on specialist experts and the other based on restarts. Quite surprisingly, we show that both methods lead to the same algorithm, namely Fixed Share, which is known for its tracking regret. We provide a thorough analysis of the adaptive regret of Fixed Share. We obtain the exact worst-case adaptive regret for Fixed Share, from which the classical tracking bounds follow. We prove that Fixed Share is optimal for adaptive regret: the worst-case adaptive regret of any algorithm is at least that of an instance of Fixed Share",
    "volume": "main",
    "checked": true,
    "id": "83f3a6aff7433c7a1233ad07de67baac4a161a5a",
    "citation_count": 88
  },
  "https://jmlr.org/papers/v17/13-589.html": {
    "title": "Learning Using Anti-Training with Sacrificial Data",
    "abstract": "Traditionally the machine-learning community has viewed the No Free Lunch (NFL) theorems for search and optimization as a limitation. We review, analyze, and unify the NFL theorem with the perspectives of \"blind\" search and meta-learning to arrive at necessary conditions for improving black-box optimization. We survey meta-learning literature to determine when and how meta- learning can benefit machine learning. Then, we generalize meta- learning in the context of the NFL theorems, to arrive at a novel technique called anti-training with sacrificial data (ATSD). Our technique applies at the meta level to arrive at domain specific algorithms. We also show how to generate sacrificial data. An extensive case study is presented along with simulated annealing results to demonstrate the efficacy of the ATSD method",
    "volume": "main",
    "checked": true,
    "id": "04150ee5796fa1fb89c3d79b7976e34dcee1d7b7",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v17/14-036.html": {
    "title": "A Unifying Framework in Vector-valued Reproducing Kernel Hilbert Spaces for Manifold Regularization and Co-Regularized Multi-view Learning",
    "abstract": "This paper presents a general vector-valued reproducing kernel Hilbert spaces (RKHS) framework for the problem of learning an unknown functional dependency between a structured input space and a structured output space. Our formulation encompasses both Vector-valued Manifold Regularization and Co-regularized Multi- view Learning, providing in particular a unifying framework linking these two important learning approaches. In the case of the least square loss function, we provide a closed form solution, which is obtained by solving a system of linear equations. In the case of Support Vector Machine (SVM) classification, our formulation generalizes in particular both the binary Laplacian SVM to the multi-class, multi-view settings and the multi-class Simplex Cone SVM to the semi-supervised, multi-view settings. The solution is obtained by solving a single quadratic optimization problem, as in standard SVM, via the Sequential Minimal Optimization (SMO) approach. Empirical results obtained on the task of object recognition, using several challenging data sets, demonstrate the competitiveness of our algorithms compared with other state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "fdffbd57eebd5b4f0164d64125c867151d6b3676",
    "citation_count": 58
  },
  "https://jmlr.org/papers/v17/14-168.html": {
    "title": "Quantifying Uncertainty in Random Forests via Confidence Intervals and Hypothesis Tests",
    "abstract": "This work develops formal statistical inference procedures for predictions generated by supervised learning ensembles. Ensemble methods based on bootstrapping, such as bagging and random forests, have improved the predictive accuracy of individual trees, but fail to provide a framework in which distributional results can be easily determined. Instead of aggregating full bootstrap samples, we consider predicting by averaging over trees built on subsamples of the training set and demonstrate that the resulting estimator takes the form of a U-statistic. As such, predictions for individual feature vectors are asymptotically normal, allowing for confidence intervals to accompany predictions. In practice, a subset of subsamples is used for computational speed; here our estimators take the form of incomplete U-statistics and equivalent results are derived. We further demonstrate that this setup provides a framework for testing the significance of features. Moreover, the internal estimation method we develop allows us to estimate the variance parameters and perform these inference procedures at no additional computational cost. Simulations and illustrations on a real data set are provided",
    "volume": "main",
    "checked": true,
    "id": "4b0ab58b83c9d141c8009f130d596def83b35ce3",
    "citation_count": 250
  },
  "https://jmlr.org/papers/v17/14-330.html": {
    "title": "Statistical-Computational Tradeoffs in Planted Problems and Submatrix Localization with a Growing Number of Clusters and Submatrices",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a5118a777b310eb7109d95e1c8fba187609b7349",
    "citation_count": 202
  },
  "https://jmlr.org/papers/v17/14-375.html": {
    "title": "Non-linear Causal Inference using Gaussianity Measures",
    "abstract": "We provide theoretical and empirical evidence for a type of asymmetry between causes and effects that is present when these are related via linear models contaminated with additive non- Gaussian noise. Assuming that the causes and the effects have the same distribution, we show that the distribution of the residuals of a linear fit in the anti-causal direction is closer to a Gaussian than the distribution of the residuals in the causal direction. This Gaussianization effect is characterized by reduction of the magnitude of the high-order cumulants and by an increment of the differential entropy of the residuals. The problem of non-linear causal inference is addressed by performing an embedding in an expanded feature space, in which the relation between causes and effects can be assumed to be linear. The effectiveness of a method to discriminate between causes and effects based on this type of asymmetry is illustrated in a variety of experiments using different measures of Gaussianity. The proposed method is shown to be competitive with state-of-the-art techniques for causal inference",
    "volume": "main",
    "checked": true,
    "id": "462dc72c5ac670747fed704a98a1f7000928cfa8",
    "citation_count": 19
  },
  "https://jmlr.org/papers/v17/14-441.html": {
    "title": "Consistent Distribution-Free $K$-Sample and Independence Tests for Univariate Random Variables",
    "abstract": "A popular approach for testing if two univariate random variables are statistically independent consists of partitioning the sample space into bins, and evaluating a test statistic on the binned data. The partition size matters, and the optimal partition size is data dependent. While for detecting simple relationships coarse partitions may be best, for detecting complex relationships a great gain in power can be achieved by considering finer partitions. We suggest novel consistent distribution-free tests that are based on summation or maximization aggregation of scores over all partitions of a fixed size. We show that our test statistics based on summation can serve as good estimators of the mutual information. Moreover, we suggest regularized tests that aggregate over all partition sizes, and prove those are consistent too. We provide polynomial-time algorithms, which are critical for computing the suggested test statistics efficiently. We show that the power of the regularized tests is excellent compared to existing tests, and almost as powerful as the tests based on the optimal (yet unknown in practice) partition size, in simulations as well as on a real data example",
    "volume": "main",
    "checked": true,
    "id": "6bb991868726317ba459d2306cdcadf0295bb3c6",
    "citation_count": 77
  },
  "https://jmlr.org/papers/v17/14-486.html": {
    "title": "A Gibbs Sampler for Learning DAGs",
    "abstract": "We propose a Gibbs sampler for structure learning in directed acyclic graph (DAG) models. The standard Markov chain Monte Carlo algorithms used for learning DAGs are random-walk Metropolis-Hastings samplers. These samplers are guaranteed to converge asymptotically but often mix slowly when exploring the large graph spaces that arise in structure learning. In each step, the sampler we propose draws entire sets of parents for multiple nodes from the appropriate conditional distribution. This provides an efficient way to make large moves in graph space, permitting faster mixing whilst retaining asymptotic guarantees of convergence. The conditional distribution is related to variable selection with candidate parents playing the role of covariates or inputs. We empirically examine the performance of the sampler using several simulated and real data examples. The proposed method gives robust results in diverse settings, outperforming several existing Bayesian and frequentist methods. In addition, our empirical results shed some light on the relative merits of Bayesian and constraint- based methods for structure learning",
    "volume": "main",
    "checked": true,
    "id": "3829c97d8df64c24710e5c1b1ceed4daa69a00b0",
    "citation_count": 22
  },
  "https://jmlr.org/papers/v17/14-501.html": {
    "title": "Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning",
    "abstract": "Learning probabilistic models over strings is an important issue for many applications. Spectral methods propose elegant solutions to the problem of inferring weighted automata from finite samples of variable-length strings drawn from an unknown target distribution $p$. These methods rely on a singular value decomposition of a matrix $\\v{H}_S$, called the empirical Hankel matrix, that records the frequencies of (some of) the observed strings $S$. The accuracy of the learned distribution depends both on the quantity of information embedded in $\\v{H}_S$ and on the distance between $\\v{H}_S$ and its mean $\\v{H}_p$. Existing concentration bounds seem to indicate that the concentration over $\\v{H}_p$ gets looser with its dimensions, suggesting that it might be necessary to bound the dimensions of $\\v{H}_S$ for learning. We prove new dimension-free concentration bounds for classical Hankel matrices and several variants, based on prefixes or factors of strings, that are useful for learning. Experiments demonstrate that these bounds are tight and that they significantly improve existing (dimension-dependent) bounds. One consequence of these results is that the spectral learning approach remains consistent even if all the observations are recorded within the empirical matrix",
    "volume": "main",
    "checked": true,
    "id": "9750292d666fa7d2ca489443f9c0a10429fb44a5",
    "citation_count": 19
  },
  "https://jmlr.org/papers/v17/14-518.html": {
    "title": "Distinguishing Cause from Effect Using Observational Data: Methods and Benchmarks",
    "abstract": "The discovery of causal relationships from purely observational data is a fundamental problem in science. The most elementary form of such a causal discovery problem is to decide whether $X$ causes $Y$ or, alternatively, $Y$ causes $X$, given joint observations of two variables $X,Y$. An example is to decide whether altitude causes temperature, or vice versa, given only joint measurements of both variables. Even under the simplifying assumptions of no confounding, no feedback loops, and no selection bias, such bivariate causal discovery problems are challenging. Nevertheless, several approaches for addressing those problems have been proposed in recent years. We review two families of such methods: methods based on Additive Noise Models (ANMs) and Information Geometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs that consists of data for 100 different cause-effect pairs selected from 37 data sets from various domains (e.g., meteorology, biology, medicine, engineering, economy, etc.) and motivate our decisions regarding the ground truth causal directions of all pairs. We evaluate the performance of several bivariate causal discovery methods on these real-world benchmark data and in addition on artificially simulated data. Our empirical results on real-world data indicate that certain methods are indeed able to distinguish cause from effect using only purely observational data, although more benchmark data would be needed to obtain statistically significant conclusions. One of the best performing methods overall is the method based on Additive Noise Models that has originally been proposed by Hoyer et al. (2009), which obtains an accuracy of 63 $\\pm$ 10 % and an AUC of 0.74 $\\pm$ 0.05 on the real-world benchmark. As the main theoretical contribution of this work we prove the consistency of that method",
    "volume": "main",
    "checked": true,
    "id": "025a1a10674d6a1712e59f1fc4800da3b27cdf94",
    "citation_count": 396
  },
  "https://jmlr.org/papers/v17/15-215.html": {
    "title": "Multi-task Sparse Structure Learning with Gaussian Copula Models",
    "abstract": "Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously. While sometimes the underlying task relationship structure is known, often the structure needs to be estimated from data at hand. In this paper, we present a novel family of models for MTL, applicable to regression and classification problems, capable of learning the structure of tasks relationship. In particular, we consider a joint estimation problem of the tasks relationship structure and the individual task parameters, which is solved using alternating minimization. The task relationship revealed by structure learning is founded on recent advances in Gaussian graphical models endowed with sparse estimators of the precision (inverse covariance) matrix. An extension to include flexible Gaussian copula models that relaxes the Gaussian marginal assumption is also proposed. We illustrate the effectiveness of the proposed model on a variety of synthetic and benchmark data sets for regression and classification. We also consider the problem of combining Earth System Model (ESM) outputs for better projections of future climate, with focus on projections of temperature by combining ESMs in South and North America, and show that the proposed model outperforms several existing methods for the problem",
    "volume": "main",
    "checked": true,
    "id": "f97d8554b3a42e155be1688a8699e46439ac441d",
    "citation_count": 58
  },
  "https://jmlr.org/papers/v17/15-237.html": {
    "title": "MLlib: Machine Learning in Apache Spark",
    "abstract": "Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open- source distributed machine learning library. MLlib provides efficient functionality for a wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed",
    "volume": "main",
    "checked": true,
    "id": "3784b73a1f392160523400ec0309191c0a96d86f",
    "citation_count": 1665
  },
  "https://jmlr.org/papers/v17/15-317.html": {
    "title": "OLPS: A Toolbox for On-Line Portfolio Selection",
    "abstract": "On-line portfolio selection is a practical financial engineering problem, which aims to sequentially allocate capital among a set of assets in order to maximize long-term return. In recent years, a variety of machine learning algorithms have been proposed to address this challenging problem, but no comprehensive open-source toolbox has been released for various reasons. This article presents the first open-source toolbox for \"On-Line Portfolio Selection\" (OLPS), which implements a collection of classical and state-of-the-art strategies powered by machine learning algorithms. We hope that OLPS can facilitate the development of new learning methods and enable the performance benchmarking and comparisons of different strategies. OLPS is an open-source project released under Apache License (version 2.0), which is available at github.com/OLPS/OLPS or OLPS.stevenhoi.org",
    "volume": "main",
    "checked": true,
    "id": "8a27bdc8505da32c72b2fadf71213795ae2debd3",
    "citation_count": 46
  },
  "https://jmlr.org/papers/v17/15-319.html": {
    "title": "A Bounded p-norm Approximation of Max-Convolution for Sub-Quadratic Bayesian Inference on Additive Factors",
    "abstract": "Max-convolution is an important problem closely resembling standard convolution; as such, max-convolution occurs frequently across many fields. Here we extend the method with fastest known worst-case runtime, which can be applied to nonnegative vectors by numerically approximating the Chebyshev norm $\\| \\cdot \\|_\\infty$, and use this approach to derive two numerically stable methods based on the idea of computing $p$-norms via fast convolution: The first method proposed, with runtime in $O( k \\log(k) \\log(\\log(k)) )$ (which is less than $18 k \\log(k)$ for any vectors that can be practically realized), uses the $p$-norm as a direct approximation of the Chebyshev norm. The second approach proposed, with runtime in $O( k \\log(k) )$ (although in practice both perform similarly), uses a novel null space projection method, which extracts information from a sequence of $p$-norms to estimate the maximum value in the vector (this is equivalent to querying a small number of moments from a distribution of bounded support in order to estimate the maximum). The $p$-norm approaches are compared to one another and are shown to compute an approximation of the Viterbi path in a hidden Markov model where the transition matrix is a Toeplitz matrix; the runtime of approximating the Viterbi path is thus reduced from $O( n k^2 )$ steps to $O( n k \\log(k))$ steps in practice, and is demonstrated by inferring the U.S. unemployment rate from the S&P 500 stock index",
    "volume": "main",
    "checked": true,
    "id": "f395e87ff3bdd3846b846c7cba8fa13c254be5b2",
    "citation_count": 16
  },
  "https://jmlr.org/papers/v17/15-335.html": {
    "title": "Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Learn Neural Networks",
    "abstract": "In this paper, we propose a novel model for high-dimensional data, called the Hybrid Orthogonal Projection and Estimation (HOPE) model, which combines a linear orthogonal projection and a finite mixture model under a unified generative modeling framework. The HOPE model itself can be learned unsupervised from unlabelled data based on the maximum likelihood estimation as well as discriminatively from labelled data. More interestingly, we have shown the proposed HOPE models are closely related to neural networks (NNs) in a sense that each hidden layer can be reformulated as a HOPE model. As a result, the HOPE framework can be used as a novel tool to probe why and how NNs work, more importantly, to learn NNs in either supervised or unsupervised ways. In this work, we have investigated the HOPE framework to learn NNs for several standard tasks, including image recognition on MNIST and speech recognition on TIMIT. Experimental results have shown that the HOPE framework yields significant performance gains over the current state-of-the-art methods in various types of NN learning problems, including unsupervised feature learning, supervised or semi-supervised learning",
    "volume": "main",
    "checked": true,
    "id": "157ef36227804431b8d2794a0d04f6feb070bb78",
    "citation_count": 14
  },
  "https://jmlr.org/papers/v17/15-389.html": {
    "title": "The Optimal Sample Complexity of PAC Learning",
    "abstract": "This work establishes a new upper bound on the number of samples sufficient for PAC learning in the realizable case. The bound matches known lower bounds up to numerical constant factors. This solves a long-standing open problem on the sample complexity of PAC learning. The technique and analysis build on a recent breakthrough by Hans Simon",
    "volume": "main",
    "checked": true,
    "id": "7860192bb790b487384e3f4e8984f4305b13ad20",
    "citation_count": 114
  },
  "https://jmlr.org/papers/v17/15-522.html": {
    "title": "End-to-End Training of Deep Visuomotor Policies",
    "abstract": "Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods",
    "volume": "main",
    "checked": true,
    "id": "b6b8a1b80891c96c28cc6340267b58186157e536",
    "citation_count": 2904
  },
  "https://jmlr.org/papers/v17/zhang16a.html": {
    "title": "On Quantile Regression in Reproducing Kernel Hilbert Spaces with the Data Sparsity Constraint",
    "abstract": "For spline regressions, it is well known that the choice of knots is crucial for the performance of the estimator. As a general learning framework covering the smoothing splines, learning in a Reproducing Kernel Hilbert Space (RKHS) has a similar issue. However, the selection of training data points for kernel functions in the RKHS representation has not been carefully studied in the literature. In this paper we study quantile regression as an example of learning in a RKHS. In this case, the regular squared norm penalty does not perform training data selection. We propose a data sparsity constraint that imposes thresholding on the kernel function coefficients to achieve a sparse kernel function representation. We demonstrate that the proposed data sparsity method can have competitive prediction performance for certain situations, and have comparable performance in other cases compared to that of the traditional squared norm penalty. Therefore, the data sparsity method can serve as a competitive alternative to the squared norm penalty method. Some theoretical properties of our proposed method using the data sparsity constraint are obtained. Both simulated and real data sets are used to demonstrate the usefulness of our data sparsity constraint",
    "volume": "main",
    "checked": true,
    "id": "99a9bb2b80c29b855f365d074412ac999e048934",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v17/luttinen16a.html": {
    "title": "BayesPy: Variational Bayesian Inference in Python",
    "abstract": "BayesPy is an open-source Python software package for performing variational Bayesian inference. It is based on the variational message passing framework and supports conjugate exponential family models. By removing the tedious task of implementing the variational Bayesian update equations, the user can construct models faster and in a less error-prone way. Simple syntax, flexible model construction and efficient inference make BayesPy suitable for both average and expert Bayesian users. It also supports some advanced methods such as stochastic and collapsed variational inference",
    "volume": "main",
    "checked": true,
    "id": "1ec8427c723626f6554e6f2e4b4ce0dbad16926b",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v17/damianou16a.html": {
    "title": "Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes",
    "abstract": "The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximised over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximising an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from i.i.d. observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the non-linear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain or partially missing inputs. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data",
    "volume": "main",
    "checked": true,
    "id": "d1b5a6f7b908b34105fea8ce302a03a840ee3496",
    "citation_count": 136
  },
  "https://jmlr.org/papers/v17/ariascastro16a.html": {
    "title": "On the Estimation of the Gradient Lines of a Density and the Consistency of the Mean-Shift Algorithm",
    "abstract": "We consider the problem of estimating the gradient lines of a density, which can be used to cluster points sampled from that density, for example via the mean-shift algorithm of Fukunaga and Hostetler (1975). We prove general convergence bounds that we then specialize to kernel density estimation",
    "volume": "main",
    "checked": true,
    "id": "fb4fee6e4602cc49ddb5e4b9cf49be4f2db06d57",
    "citation_count": 93
  },
  "https://jmlr.org/papers/v17/martinez16a.html": {
    "title": "Scalable Learning of Bayesian Network Classifiers",
    "abstract": "Ever increasing data quantity makes ever more urgent the need for highly scalable learners that have good classification performance. Therefore, an out-of-core learner with excellent time and space complexity, along with high expressivity (that is, capacity to learn very complex multivariate probability distributions) is extremely desirable. This paper presents such a learner. We propose an extension to the $k$-dependence Bayesian classifier (KDB) that discriminatively selects a sub- model of a full KDB classifier. It requires only one additional pass through the training data, making it a three-pass learner. Our extensive experimental evaluation on $16$ large data sets reveals that this out-of-core algorithm achieves competitive classification performance, and substantially better training and classification time than state-of-the-art in-core learners such as random forest and linear and non-linear logistic regression",
    "volume": "main",
    "checked": true,
    "id": "2635d7bf037029b94d16d75fd6310955d9a409c9",
    "citation_count": 55
  },
  "https://jmlr.org/papers/v17/11-229.html": {
    "title": "A Unified View on Multi-class Support Vector Classification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2eb8183efaf4fd239fb5c1374f182bbc717e6a4c",
    "citation_count": 90
  },
  "https://jmlr.org/papers/v17/14-037.html": {
    "title": "Addressing Environment Non-Stationarity by Repeating Q-learning Updates",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f9c8cd4874b19b33e28010bac2a3025d9bb4c9fb",
    "citation_count": 35
  },
  "https://jmlr.org/papers/v17/14-148.html": {
    "title": "Large Scale Online Kernel Learning",
    "abstract": "In this paper, we present a new framework for large scale online kernel learning, making kernel methods efficient and scalable for large-scale online learning applications. Unlike the regular budget online kernel learning scheme that usually uses some budget maintenance strategies to bound the number of support vectors, our framework explores a completely different approach of kernel functional approximation techniques to make the subsequent online learning task efficient and scalable. Specifically, we present two different online kernel machine learning algorithms: (i) Fourier Online Gradient Descent (FOGD) algorithm that applies the random Fourier features for approximating kernel functions; and (ii) NystrÃÂ¶m Online Gradient Descent (NOGD) algorithm that applies the NystrÃÂ¶m method to approximate large kernel matrices. We explore these two approaches to tackle three online learning tasks: binary classification, multi-class classification, and regression. The encouraging results of our experiments on large-scale datasets validate the effectiveness and efficiency of the proposed algorithms, making them potentially more practical than the family of existing budget online kernel learning approaches",
    "volume": "main",
    "checked": true,
    "id": "3091689115f308d4c3f7f4421a284af1d5119cb7",
    "citation_count": 144
  },
  "https://jmlr.org/papers/v17/14-195.html": {
    "title": "Kernel Mean Shrinkage Estimators",
    "abstract": "A mean function in a reproducing kernel Hilbert space (RKHS), or a kernel mean, is central to kernel methods in that it is used by many classical algorithms such as kernel principal component analysis, and it also forms the core inference step of modern kernel methods that rely on embedding probability distributions in RKHSs. Given a finite sample, an empirical average has been used commonly as a standard estimator of the true kernel mean. Despite a widespread use of this estimator, we show that it can be improved thanks to the well-known Stein phenomenon. We propose a new family of estimators called kernel mean shrinkage estimators (KMSEs), which benefit from both theoretical justifications and good empirical performance. The results demonstrate that the proposed estimators outperform the standard one, especially in a \"large $d$, small $n$\" paradigm",
    "volume": "main",
    "checked": true,
    "id": "6ed00c17139e8c75d4ec48879b4e4a5e9d034339",
    "citation_count": 46
  },
  "https://jmlr.org/papers/v17/14-199.html": {
    "title": "SPSD Matrix Approximation vis Column Selection: Theories, Algorithms, and Extensions",
    "abstract": "Symmetric positive semidefinite (SPSD) matrix approximation is an important problem with applications in kernel methods. However, existing SPSD matrix approximation methods such as the NystrÃÂ¶m method only have weak error bounds. In this paper we conduct in-depth studies of an SPSD matrix approximation model and establish strong relative-error bounds. We call it the prototype model for it has more efficient and effective extensions, and some of its extensions have high scalability. Though the prototype model itself is not suitable for large- scale data, it is still useful to study its properties, on which the analysis of its extensions relies. This paper offers novel theoretical analysis, efficient algorithms, and a highly accurate extension. First, we establish a lower error bound for the prototype model and improve the error bound of an existing column selection algorithm to match the lower bound. In this way, we obtain the first optimal column selection algorithm for the prototype model. We also prove that the prototype model is exact under certain conditions. Second, we develop a simple column selection algorithm with a provable error bound. Third, we propose a so-called spectral shifting model to make the approximation more accurate when the eigenvalues of the matrix decay slowly, and the improvement is theoretically quantified. The spectral shifting method can also be applied to improve other SPSD matrix approximation models",
    "volume": "main",
    "checked": true,
    "id": "e6c3a4711fd1f6034f2f432f6875f50284c32641",
    "citation_count": 35
  },
  "https://jmlr.org/papers/v17/14-298.html": {
    "title": "Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically Triggered Arms",
    "abstract": "We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where subsets of base arms with unknown distributions form super arms. In each round, a super arm is played and the base arms contained in the super arm are played and their outcomes are observed. We further consider the extension in which more base arms could be probabilistically triggered based on the outcomes of already triggered arms. The reward of the super arm depends on the outcomes of all played arms, and it only needs to satisfy two mild assumptions, which allow a large class of nonlinear reward instances. We assume the availability of an offline $(\\alpha,\\beta)$-approximation oracle that takes the means of the outcome distributions of arms and outputs a super arm that with probability $\\beta$ generates an $\\alpha$ fraction of the optimal expected reward. The objective of an online learning algorithm for CMAB is to minimize $(\\alpha,\\beta)$-approximation regret, which is the difference in total expected reward between the $\\alpha\\beta$ fraction of expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide CUCB algorithm that achieves $O(\\log n)$ distribution-dependent regret, where $n$ is the number of rounds played, and we further provide distribution-independent bounds for a large class of reward functions. Our regret analysis is tight in that it matches the bound of UCB1 algorithm (up to a constant factor) for the classical MAB problem, and it significantly improves the regret bound in an earlier paper on combinatorial bandits with linear rewards. We apply our CMAB framework to two new applications, probabilistic maximum coverage (PMC) for online advertising and social influence maximization for viral marketing, both having nonlinear reward structures. In particular, application to social influence maximization requires our extension on probabilistically triggered arms",
    "volume": "main",
    "checked": true,
    "id": "c40640d997dab261d37743e1bffc09273c9937a0",
    "citation_count": 240
  },
  "https://jmlr.org/papers/v17/14-388.html": {
    "title": "Differentially Private Data Releasing for Smooth Queries",
    "abstract": "In the past few years, differential privacy has become a standard concept in the area of privacy. One of the most important problems in this field is to answer queries while preserving differential privacy. In spite of extensive studies, most existing work on differentially private query answering assumes the data are discrete (i.e., in $\\{0,1\\}^d$) and focuses on queries induced by \\emph{Boolean} functions. In real applications however, continuous data are at least as common as binary data. Thus, in this work we explore a less studied topic, namely, differential privately query answering for continuous data with continuous function. As a first step towards the continuous case, we study a natural class of linear queries on continuous data which we refer to as smooth queries. A linear query is said to be $K$-smooth if it is specified by a function defined on $[-1,1]^d$ whose partial derivatives up to order $K$ are all bounded. We develop two $\\epsilon$-differentially private mechanisms which are able to answer all smooth queries. The first mechanism outputs a summary of the database and can then give answers to the queries. The second mechanism is an improvement of the first one and it outputs a synthetic database. The two mechanisms both achieve an accuracy of $O (n^{-\\frac{K}{2d+K}}/\\epsilon )$. Here we assume that the dimension $d$ is a constant. It turns out that even in this parameter setting (which is almost trivial in the discrete case), using existing discrete mechanisms to answer the smooth queries is difficult and requires more noise. Our mechanisms are based on $L_{\\infty}$-approximation of (transformed) smooth functions by low-degree even trigonometric polynomials with uniformly bounded coefficients. We also develop practically efficient variants of the mechanisms with promising experimental results",
    "volume": "main",
    "checked": true,
    "id": "fdfef6f1298f4c6cb88da550bc3e12349374c497",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v17/14-443.html": {
    "title": "Subspace Learning with Partial Information",
    "abstract": "The goal of subspace learning is to find a $k$-dimensional subspace of $\\mathbb{R}^d$, such that the expected squared distance between instance vectors and the subspace is as small as possible. In this paper we study subspace learning in a partial information setting, in which the learner can only observe $r \\le d$ attributes from each instance vector. We propose several efficient algorithms for this task, and analyze their sample complexity",
    "volume": "main",
    "checked": true,
    "id": "0c54fa252289ad3fdcace8278ce503df669cfc2a",
    "citation_count": 36
  },
  "https://jmlr.org/papers/v17/14-460.html": {
    "title": "Iterative Hessian Sketch: Fast and Accurate Solution Approximation for Constrained Least-Squares",
    "abstract": "We study randomized sketching methods for approximately solving least-squares problem with a general convex constraint. The quality of a least-squares approximation can be assessed in different ways: either in terms of the value of the quadratic objective function (cost approximation), or in terms of some distance measure between the approximate minimizer and the true minimizer (solution approximation). Focusing on the latter criterion, our first main result provides a general lower bound on any randomized method that sketches both the data matrix and vector in a least-squares problem; as a surprising consequence, the most widely used least-squares sketch is sub-optimal for solution approximation. We then present a new method known as the iterative Hessian sketch, and show that it can be used to obtain approximations to the original least-squares problem using a projection dimension proportional to the statistical complexity of the least-squares minimizer, and a logarithmic number of iterations. We illustrate our general theory with simulations for both unconstrained and constrained versions of least-squares, including $\\ell_1$-regularization and nuclear norm constraints. We also numerically demonstrate the practicality of our approach in a real face expression classification experiment",
    "volume": "main",
    "checked": true,
    "id": "cfcb1645bb6abe972e935d03a8c855f53a718116",
    "citation_count": 186
  },
  "https://jmlr.org/papers/v17/14-479.html": {
    "title": "Estimating Causal Structure Using Conditional DAG Models",
    "abstract": "This paper considers inference of causal structure in a class of graphical models called conditional DAGs. These are directed acyclic graph (DAG) models with two kinds of variables, primary and secondary. The secondary variables are used to aid in the estimation of the structure of causal relationships between the primary variables. We prove that, under certain assumptions, such causal structure is identifiable from the joint observational distribution of the primary and secondary variables. We give causal semantics for the model class, put forward a score-based approach for estimation and establish consistency results. Empirical results demonstrate gains compared with formulations that treat all variables on an equal footing, or that ignore secondary variables. The methodology is motivated by applications in biology that involve multiple data types and is illustrated here using simulated data and in an analysis of molecular data from the Cancer Genome Atlas",
    "volume": "main",
    "checked": true,
    "id": "423ee341652d9990ffc40de0c27a28cd4aaff5a1",
    "citation_count": 16
  },
  "https://jmlr.org/papers/v17/15-021.html": {
    "title": "Adaptive Lasso and group-Lasso for functional Poisson regression",
    "abstract": "High dimensional Poisson regression has become a standard framework for the analysis of massive counts datasets. In this work we estimate the intensity function of the Poisson regression model by using a dictionary approach, which generalizes the classical basis approach, combined with a Lasso or a group-Lasso procedure. Selection depends on penalty weights that need to be calibrated. Standard methodologies developed in the Gaussian framework can not be directly applied to Poisson models due to heteroscedasticity. Here we provide data-driven weights for the Lasso and the group-Lasso derived from concentration inequalities adapted to the Poisson case. We show that the associated Lasso and group-Lasso procedures satisfy fast and slow oracle inequalities. Simulations are used to assess the empirical performance of our procedure, and an original application to the analysis of Next Generation Sequencing data is provided",
    "volume": "main",
    "checked": true,
    "id": "df1b5b183c0120757124b7b014a82e64829f9778",
    "citation_count": 40
  },
  "https://jmlr.org/papers/v17/15-130.html": {
    "title": "Causal Inference through a Witness Protection Program",
    "abstract": "One of the most fundamental problems in causal inference is the estimation of a causal effect when treatment and outcome are confounded. This is difficult in an observational study, because one has no direct evidence that all confounders have been adjusted for. We introduce a novel approach for estimating causal effects that exploits observational conditional independencies to suggest Ã¢ÂÂweakÃ¢ÂÂ paths in an unknown causal graph. The widely used faithfulness condition of Spirtes et al. is relaxed to allow for varying degrees of Ã¢ÂÂpath cancellationsÃ¢ÂÂ that imply conditional independencies but do not rule out the existence of confounding causal paths. The output is a posterior distribution over bounds on the average causal effect via a linear programming approach and Bayesian inference. We claim this approach should be used in regular practice as a complement to other tools in observational studies",
    "volume": "main",
    "checked": true,
    "id": "d1393e305bc20fa3ad3a83a0282a8bddce29ade6",
    "citation_count": 16
  },
  "https://jmlr.org/papers/v17/15-140.html": {
    "title": "Structure Discovery in Bayesian Networks by Sampling Partial Orders",
    "abstract": "We present methods based on Metropolis-coupled Markov chain Monte Carlo (MC3) and annealed importance sampling (AIS) for estimating the posterior distribution of Bayesian networks. The methods draw samples from an appropriate distribution of partial orders on the nodes, continued by sampling directed acyclic graphs (DAGs) conditionally on the sampled partial orders. We show that the computations needed for the sampling algorithms are feasible as long as the encountered partial orders have relatively few down-sets. While the algorithms assume suitable modularity properties of the priors, arbitrary priors can be handled by dividing the importance weight of each sampled DAG by the number of topological sorts it has---we give a practical dynamic programming algorithm to compute these numbers. Our empirical results demonstrate that the presented partial-order- based samplers are superior to previous Markov chain Monte Carlo methods, which sample DAGs either directly or via linear orders on the nodes. The results also suggest that the convergence rate of the estimators based on AIS are competitive to those of MC3. Thus AIS is the preferred method, as it enables easier large- scale parallelization and, in addition, supplies good probabilistic lower bound guarantees for the marginal likelihood of the model",
    "volume": "main",
    "checked": true,
    "id": "ccffab833f5d804a2064a3653b2e4801334a510b",
    "citation_count": 40
  },
  "https://jmlr.org/papers/v17/15-189.html": {
    "title": "Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology Dependence",
    "abstract": "Data in the form of pairwise comparisons arises in many domains, including preference elicitation, sporting competitions, and peer grading among others. We consider parametric ordinal models for such pairwise comparison data involving a latent vector $w^* \\in \\mathbb{R}^d$ that represents the Ã¢ÂÂqualitiesÃ¢ÂÂ of the $d$ items being compared; this class of models includes the two most widely used parametric models---the Bradley-Terry-Luce (BTL) and the Thurstone models. Working within a standard minimax framework, we provide tight upper and lower bounds on the optimal error in estimating the quality score vector $w^*$ under this class of models. The bounds depend on the topology of the comparison graph induced by the subset of pairs being compared, via the spectrum of the Laplacian of the comparison graph. Thus, in settings where the subset of pairs may be chosen, our results provide principled guidelines for making this choice. Finally, we compare these error rates to those under cardinal measurement models and show that the error rates in the ordinal and cardinal settings have identical scalings apart from constant pre- factors",
    "volume": "main",
    "checked": true,
    "id": "da1f22dd6d834e031eb733d2b70320f34ef9458f",
    "citation_count": 124
  },
  "https://jmlr.org/papers/v17/15-239.html": {
    "title": "Domain-Adversarial Training of Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1d5972b32a9b5a455a6eef389de5b7fca25771ad",
    "citation_count": 6043
  },
  "https://jmlr.org/papers/v17/15-273.html": {
    "title": "Probabilistic Low-Rank Matrix Completion from Quantized Measurements",
    "abstract": "We consider the recovery of a low rank real-valued matrix $M$ given a subset of noisy discrete (or quantized) measurements. Such problems arise in several applications such as collaborative filtering, learning and content analytics, and sensor network localization. We consider constrained maximum likelihood estimation of $M$, under a constraint on the entry- wise infinity-norm of $M$ and an exact rank constraint. We provide upper bounds on the Frobenius norm of matrix estimation error under this model. Previous theoretical investigations have focused on binary (1-bit) quantizers, and been based on convex relaxation of the rank. Compared to the existing binary results, our performance upper bound has faster convergence rate with matrix dimensions when the fraction of revealed observations is fixed. We also propose a globally convergent optimization algorithm based on low rank factorization of $M$ and validate the method on synthetic and real data, with improved performance over previous methods",
    "volume": "main",
    "checked": true,
    "id": "694ea051a41203cddc7dc69ea0d0c59ff72f4216",
    "citation_count": 38
  },
  "https://jmlr.org/papers/v17/15-292.html": {
    "title": "DSA: Decentralized Double Stochastic Averaging Gradient Algorithm",
    "abstract": "This paper considers optimization problems where nodes of a network have access to summands of a global objective. Each of these local objectives is further assumed to be an average of a finite set of functions. The motivation for this setup is to solve large scale machine learning problems where elements of the training set are distributed to multiple computational elements. The decentralized double stochastic averaging gradient (DSA) algorithm is proposed as a solution alternative that relies on: (i) The use of local stochastic averaging gradients. (ii) Determination of descent steps as differences of consecutive stochastic averaging gradients. Strong convexity of local functions and Lipschitz continuity of local gradients is shown to guarantee linear convergence of the sequence generated by DSA in expectation. Local iterates are further shown to approach the optimal argument for almost all realizations. The expected linear convergence of DSA is in contrast to the sublinear rate characteristic of existing methods for decentralized stochastic optimization. Numerical experiments on a logistic regression problem illustrate reductions in convergence time and number of feature vectors processed until convergence relative to these other alternatives",
    "volume": "main",
    "checked": true,
    "id": "80609bbeefdc1d5e37c4b1fb40c3d64a97844242",
    "citation_count": 153
  },
  "https://jmlr.org/papers/v17/15-346.html": {
    "title": "The Statistical Performance of Collaborative Inference",
    "abstract": "The statistical analysis of massive and complex data sets will require the development of algorithms that depend on distributed computing and collaborative inference. Inspired by this, we propose a collaborative framework that aims to estimate the unknown mean $\\theta$ of a random variable $X$. In the model we present, a certain number of calculation units, distributed across a communication network represented by a graph, participate in the estimation of $\\theta$ by sequentially receiving independent data from $X$ while exchanging messages via a stochastic matrix $A$ defined over the graph. We give precise conditions on the matrix $A$ under which the statistical precision of the individual units is comparable to that of a (gold standard) virtual centralized estimate, even though each unit does not have access to all of the data. We show in particular the fundamental role played by both the non-trivial eigenvalues of $A$ and the Ramanujan class of expander graphs, which provide remarkable performance for moderate algorithmic cost",
    "volume": "main",
    "checked": true,
    "id": "1450e2c43c2c7f33b6d4a266c629b8bd575d88d4",
    "citation_count": 2
  }
}